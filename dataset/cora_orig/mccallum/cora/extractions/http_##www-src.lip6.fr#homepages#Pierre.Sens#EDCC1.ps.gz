URL: http://www-src.lip6.fr/homepages/Pierre.Sens/EDCC1.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/yasushi/porc-internal/LMM.html
Root-URL: 
Title: GATOSTAR: A Fault Tolerant Load Sharing Facility for Parallel Applications  
Author: Bertil Folliot Pierre Sens 
Affiliation: MASI Lab CNRS 818, IBP, University Paris VI  
Date: October 1994,  
Address: Berlin, Germany,  4 place Jussieu 75252 Paris Cedex 05 (France)  
Note: In proc. of the First European Dependable Computing Conference,  Lecture Notes in Computer Science, 852 Springer-Verlag  
Abstract: This paper presents how and why to unify load sharing and fault tolerance facilities. A realization of a fault tolerant load sharing facility, GATOSTAR, is presented and discussed. It is based on the integration of two applications developed on top of Unix: G ATOS and STAR. G ATOS is a load sharing manager which automatically distributes parallel applications among heterogeneous hosts according to multicriteria allocation algorithms. STAR is a software fault tolerance manager which automatically recovers processes of faulty machines based on checkpointing and message logging. The main advantage of this approach is to increase fault tolerant performance by taking advantage of the load sharing policies when allocating or recovering processes. This unification not only improves the efficiency of both facilities but avoids many redundancies mechanisms between them. Indeed, each facility needs to manage at least three common features: global knowledge of the running processors, a crash detection mechanism and remote process management. The backbone of this unification is a logical ring of communication for host crash detection and for host related information transfer. Thus, all necessary information is acquired with a relatively low cost of messages com pared to the two systems taken independently.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Alonso and L.L. Cova. </author> <title> Sharing Jobs Among Independently Owned Processors. </title> <booktitle> In Proc. of the 8th International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, California, </address> <pages> pp. 365-372, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: A number of simulation studies on load sharing have been performed. Eager et al. [14] show that the overload threshold depends on the average load of the system and is between 1 and 2. For Alonso and Cova <ref> [1] </ref> this threshold depends on the number of users and is between 0.4 and 1.6. Bernard and Simatic have chosen the value of 0.8 for their Radio system [4]. <p> Like in case of the overload threshold (the one used when a host is initially chosen), good values must be set for the migration and reception thresholds. Alonso and Cova <ref> [1] </ref> and Bernard and Simatic [4] empirically give the same value to the migration threshold, respectively 1.75 and 1.8, while Harbus [21] uses values between 2.5 and 5! This threshold seems to depend on the behavior of the migrated processes, on the ratio (available processing power / mean number of processes),
Reference: 2. <author> O. Babaoglu, L. Alvisi, A. Amoroso, and R. Davoli. </author> <title> Paralex: An Environment for Parallel Programming in Distributed Systems. </title> <booktitle> In Proc. of International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <pages> pp. 178-187, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The major ones are the systems REM (Remote Execution Manager) [32], Paralex <ref> [2] </ref>, Condor [26], and DAWGS (Distributed Automated Workload sharing System) [11]. These four systems have been developed essentially as load sharing manager, since fault tolerance is a limited extension. To our knowledge, no work has compared and unified the two concepts.
Reference: 3. <author> A. </author> <title> Barak and O.G. Paradise. MOS - A Load-balancing Unix. </title> <booktitle> In Proc. of the Usenix Technical Conference - Summer 1986, </booktitle> <pages> pp. 414-418, </pages> <year> 1986. </year>
Reference-contexts: They are divided in two groups, dynamic placement (as Radio [4], REM [32], Utopia [37]) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor [26], Mosix <ref> [3] </ref>, Sprite [13]) where processes can move according to overload conditions or the reactivation of workstations by their owners. Utopia is the closest to GATOS. It is based on previous simulation experiments of Ferrari and Zhou [16] about load thresholds values.
Reference: 4. <author> G. Bernard and M. Simatic. </author> <title> A Decentralized and Efficient Algorithm for Networks of Workstations. </title> <booktitle> In Proc. of the European Conference for Open Systems Spring 91, Troms, Norway, </booktitle> <pages> pp. 139-148, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In such an environment, * Authors affiliation: UFR dInformatique, University Paris VII, Paris, France workstations are free from 33% to 90% of the time [35]. By using a load sharing management, users applications can take advantage of all the hosts' processing power and particularly of the idle ones <ref> [4, 26] </ref>. Thus, not only the application response time greatly decreases when there is no fault but also the software fault tolerant overhead becomes acceptable. There are several common mechanisms in the implementation of a load sharing manager (LSM) and of a fault tolerant manager (FTM). <p> It may also lead to unacceptable interference with hosts supporting interactive users. Most of the previous works on load sharing deal only with processor load and are not adapted for the allocation of entire parallel applications allocation. They are divided in two groups, dynamic placement (as Radio <ref> [4] </ref>, REM [32], Utopia [37]) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor [26], Mosix [3], Sprite [13]) where processes can move according to overload conditions or the reactivation of workstations by their owners. <p> For Alonso and Cova [1] this threshold depends on the number of users and is between 0.4 and 1.6. Bernard and Simatic have chosen the value of 0.8 for their Radio system <ref> [4] </ref>. These results and our experience with the GATOS system within the MASI lab. lead us to set the overload threshold value to 1. Another problem is to get global state information. <p> Like in case of the overload threshold (the one used when a host is initially chosen), good values must be set for the migration and reception thresholds. Alonso and Cova [1] and Bernard and Simatic <ref> [4] </ref> empirically give the same value to the migration threshold, respectively 1.75 and 1.8, while Harbus [21] uses values between 2.5 and 5! This threshold seems to depend on the behavior of the migrated processes, on the ratio (available processing power / mean number of processes), and on users behavior. <p> cost The previous measures show that the total cost of our fault management is low for the specific studied applications, i.e. long-running ones with small message exchanges. 7.1 Performance comparison with other systems The mean time for a placement request is 5 seconds in REM [32], 500 ms in Radio <ref> [4] </ref>, and 12 seconds in DAWGS [11]. These high allocation times are essentially due to highly communicating algorithms, while GATOS (recall its 100 ms allocation time) does not need any extra communication when allocating hosts.
Reference: 5. <author> P. A. Bernstein and N. Goodman. </author> <title> Concurrency Control in Distributed Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 13(2) </volume> <pages> 185-221, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: When a process rolls back, old versions of files in use replace current ones. Shared files management has not been considered. Such management is complex, and requires a distributed database manager <ref> [5] </ref>. Another extension would be to integrate an existing and open log manager, as in Camelot [12], Clio [17] or KitLog [30]. 5 Process Allocation The process allocation in G ATOSTAR is not limited to independent programs. It is mostly geared towards parallel applications and takes into account processes behavior.
Reference: 6. <author> B. Bhargava, S-R. Lian, and P-J. Leu. </author> <title> Experimental Evaluation of Concurrent Checkpointing and Rollback-Recovery Algorithms. </title> <booktitle> In Proc. of the International Conference on Data Engineering, </booktitle> <pages> pp 182-189, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: With consistent checkpointing, processes coordinate their checkpointing actions such that the collection of checkpoints represents a consistent state of the whole system [10]. When a failure occurs, the system restarts from these checkpoints. According to the results presented in <ref> [6] </ref> and [15], the main drawback of this approach is that the messages used for synchronizing a checkpoint are an important source of overhead. Moreover, after a failure, surviving processes may have to rollback to their latest checkpoint in order to remain consistent with recovering processes [25]. <p> Reliable message logging avoids this classical domino effect. A substantial body of work has been published regarding fault tolerance by means of checkpointing. The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint <ref> [6, 15, 36] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging [8, 23, 34]. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. <p> To reduce the checkpoint overhead, the authors recommend a copy-on-write method. This technique implies a consequent increase in performance.With a 2 minutes checkpoint interval, their checkpointing increased the running time of an application by about 1%. The worst overhead measured was 5.8 %. Bhargava et al. <ref> [6] </ref> give some performance measurements of consistent checkpointing. In their environment, the messages needed for synchronizing a checkpoint implied an important overhead. The authors have limited their study to small size programs (4 to 48 kilobytes).
Reference: 7. <author> K.P. Birman and T. Joseph. </author> <title> Reliable Communication in the Presence of Failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5 </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: This number is set by the network administrator or by the application designer according to the fault-tolerance and performance requirements. To ensure coherence of all copies, the file manager performs a reliable broadcast protocol <ref> [7] </ref>. A file update is reliably broadcast to all managers having a copy. A read operation is locally done whenever possible. The file manager also manages versions of replicated files. Since each process needs only one checkpoint, only one old (replicated) version is needed for each file in use.
Reference: 8. <author> A. Borg, W. Blau, and W. Craetsch, F. Herrmann, and W. Oberle. </author> <title> Fault Tolerance under Unix. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint [6, 15, 36], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging <ref> [8, 23, 34] </ref>. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. The major ones are the systems REM (Remote Execution Manager) [32], Paralex [2], Condor [26], and DAWGS (Distributed Automated Workload sharing System) [11]. <p> The worst overhead measured was 5.8 %. Bhargava et al. [6] give some performance measurements of consistent checkpointing. In their environment, the messages needed for synchronizing a checkpoint implied an important overhead. The authors have limited their study to small size programs (4 to 48 kilobytes). Borg et al. <ref> [8] </ref> have implemented a fault-tolerant version of Unix based on three-way atomic message transmission: the TARGON/32 system. They measured the performance on only two machines. According to the authors, the performance turns out to be 1.6 times slower than on a standard Unix.
Reference: 9. <author> R. Boutaba and B. Folliot. </author> <title> Load Balancing in Local Area Networks. </title> <booktitle> In Proc. of the Networks92 International Conference on Computer Networks, Architecture and Applications, Trivandrum, India, </booktitle> <pages> pp. 73-89, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: He specifies the set of hosts involved in the load distribution and provides different versions for each program in order to deal with system heterogeneity. The following section describes the algorithm for the first of the criteria. For a complete description, see <ref> [9, 19] </ref>. Selecting the Less Loaded Host. Allocating processes according to the hosts' load allows to benefit from the idle workstations power.
Reference: 10. <author> K.M. Chandy and L. Lamport. </author> <title> Distributed Snapshots: Determining Global States of Distributed Systems. </title> <journal> ACM Transactions on Computer Systems , 3(1) </journal> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: With consistent checkpointing, processes coordinate their checkpointing actions such that the collection of checkpoints represents a consistent state of the whole system <ref> [10] </ref>. When a failure occurs, the system restarts from these checkpoints. According to the results presented in [6] and [15], the main drawback of this approach is that the messages used for synchronizing a checkpoint are an important source of overhead.
Reference: 11. <author> H. Clark and B. McMillin. </author> <title> DAWGS - A Distributed Compute Server Utilizing Idle Workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 175-186, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Furthermore, LSM needs some additional information, such as the processors speed, the hosts load and the amount of available memory. The crash detection mechanism needs not be efficient for the LSM, and is often very basic (as in DAWGS <ref> [11] </ref> and Utopia [37]), but is crucial for the FTM. It must be very efficient, in order to react to failures as fast as possible, while its implementation must not overload the network or the workstations. The last feature concerns distributed process management. <p> The major ones are the systems REM (Remote Execution Manager) [32], Paralex [2], Condor [26], and DAWGS (Distributed Automated Workload sharing System) <ref> [11] </ref>. These four systems have been developed essentially as load sharing manager, since fault tolerance is a limited extension. To our knowledge, no work has compared and unified the two concepts. REM and Paralex are load sharing managers that support cooperative applications. <p> Thus, the fault tolerant property is gained at the expense of computing power. Moreover, failure of the local user's machine implies a failure of the whole computation. As the failure rate in a local area network is low <ref> [11] </ref>, the process replication mechanism is not adapted to a load sharing scheme. Furthermore, the target applications of software fault tolerance are long-lived ones, and users are not supposed to always be connected with the same local machine. Paralex provides a package for distributing cooperative computation across a network. <p> It works on a set of workstations connected by a local area network. In this environment, failures are uncommon events. Clark and McMillin measured the average crash time in a local area network to be once every 2.7 days <ref> [11] </ref>. Moreover, most of the Unix systems do not allow real time programming and thus, very short recovery delays can not be provided. Therefore, we favor solutions with a low overhead under normal operation, possibly to the detriment of an increase in recovery time. <p> the total cost of our fault management is low for the specific studied applications, i.e. long-running ones with small message exchanges. 7.1 Performance comparison with other systems The mean time for a placement request is 5 seconds in REM [32], 500 ms in Radio [4], and 12 seconds in DAWGS <ref> [11] </ref>. These high allocation times are essentially due to highly communicating algorithms, while GATOS (recall its 100 ms allocation time) does not need any extra communication when allocating hosts.
Reference: 12. <author> D.S. Daniels. </author> <title> Distributed Logging for Transaction Processing. </title> <type> PhD Thesis, </type> <institution> Technical Report CMU-CS-89-114 , Carnegie-Mellon University, Pittsburg, </institution> <address> PA (USA), </address> <month> December </month> <year> 1988 </year>
Reference-contexts: When a process rolls back, old versions of files in use replace current ones. Shared files management has not been considered. Such management is complex, and requires a distributed database manager [5]. Another extension would be to integrate an existing and open log manager, as in Camelot <ref> [12] </ref>, Clio [17] or KitLog [30]. 5 Process Allocation The process allocation in G ATOSTAR is not limited to independent programs. It is mostly geared towards parallel applications and takes into account processes behavior.
Reference: 13. <author> F. Douglis and J. Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <year> 1991. </year>
Reference-contexts: They are divided in two groups, dynamic placement (as Radio [4], REM [32], Utopia [37]) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor [26], Mosix [3], Sprite <ref> [13] </ref>) where processes can move according to overload conditions or the reactivation of workstations by their owners. Utopia is the closest to GATOS. It is based on previous simulation experiments of Ferrari and Zhou [16] about load thresholds values. This system runs on heterogeneous operating systems and applies load sharing.
Reference: 14. <author> D. L. Eager, E. D. Lazoska, and J. Zahorjan. </author> <title> Adaptative Load Sharing in Homogeneous Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE12(5):662-675, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: In such a situation, running processes on remote hosts would be even more expensive than running them on their local one. The main difficulty is how to find a good overload threshold value. A number of simulation studies on load sharing have been performed. Eager et al. <ref> [14] </ref> show that the overload threshold depends on the average load of the system and is between 1 and 2. For Alonso and Cova [1] this threshold depends on the number of users and is between 0.4 and 1.6.
Reference: 15. <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> The Performance of Consistent Checkpointing. </title> <booktitle> In Proc. of the 11th Symposium on Reliable Distributed Systems, </booktitle> <address> Houston, Texas, </address> <month> October 92. </month>
Reference-contexts: With consistent checkpointing, processes coordinate their checkpointing actions such that the collection of checkpoints represents a consistent state of the whole system [10]. When a failure occurs, the system restarts from these checkpoints. According to the results presented in [6] and <ref> [15] </ref>, the main drawback of this approach is that the messages used for synchronizing a checkpoint are an important source of overhead. Moreover, after a failure, surviving processes may have to rollback to their latest checkpoint in order to remain consistent with recovering processes [25]. <p> Reliable message logging avoids this classical domino effect. A substantial body of work has been published regarding fault tolerance by means of checkpointing. The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint <ref> [6, 15, 36] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging [8, 23, 34]. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. <p> The checkpoint time in DAWGS is quite high: 1.84 seconds for a 25 Kbytes process.It manages only a replication of degree one and, furthermore, processes running on a faulty host must wait for the host to be restarted before being recovered. Elnozahy et al. <ref> [15] </ref> have implemented a consistent checkpointing on an Ethernet network of Sun 3/60 workstations with only replication of degree one. Their performance results depend on the application complexity.
Reference: 16. <author> D. Ferrari and S. Zhou. </author> <title> An Empirical Investigation of Load Indices for Load Balancing Applications. </title> <type> Performances 87, </type> <institution> Bruxelles, </institution> <address> Belgium, </address> <pages> pp. 515-528, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Utopia is the closest to GATOS. It is based on previous simulation experiments of Ferrari and Zhou <ref> [16] </ref> about load thresholds values. This system runs on heterogeneous operating systems and applies load sharing.
Reference: 17. <author> R.S. Finlayson. </author> <title> A Log File Service Exploiting Write-once Storage. </title> <type> PhD Thesis, Technical Report STAN-CS-89-1272, </type> <institution> Stanford University, Stanford, </institution> <address> CA (USA), </address> <month> July </month> <year> 1989. </year>
Reference-contexts: When a process rolls back, old versions of files in use replace current ones. Shared files management has not been considered. Such management is complex, and requires a distributed database manager [5]. Another extension would be to integrate an existing and open log manager, as in Camelot [12], Clio <ref> [17] </ref> or KitLog [30]. 5 Process Allocation The process allocation in G ATOSTAR is not limited to independent programs. It is mostly geared towards parallel applications and takes into account processes behavior.
Reference: 18. <author> B. Folliot. </author> <title> Distributed Applications in Heterogeneous Environments. </title> <booktitle> In Proc. of The European Forum for Open Systems , Troms, Norway, </booktitle> <pages> pp. 149-159, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: GATOSTAR is based on two independent systems realized at the MASI lab: GATOS and STAR. G AT OS is a load sharing manager which automatically distributes parallel applications among heterogeneous hosts <ref> [18, 19] </ref>. STAR is a fault tolerance manager which automatically recovers processes of faulty machines based on message logging [31]. They are implemented on a set of Sun3 and Sun4 workstations connected by a LAN (Ethernet) and they run on top of Unix (SunOS). <p> section an overview of a multicriteria load sharing algorithm and how these algorithms can be adapted to react to unexpected load variation by means of a dynamic migration mechanism. 5.1 Process placement To launch new processes and to restart processes of faulty hosts, GATOSTAR uses the GATOS load sharing manager <ref> [18, 19] </ref>. This manager automatically distributes parallel applications among heterogeneous hosts. To be executed, application processes need resources (CPU, memory, file access, communication) that vary from one process to another.
Reference: 19. <author> B. </author> <month> Folliot. </month> <institution> Mthodes et Outils de Partage de Charge pour la Conception et la Mise en uvre dApplications dans les Systmes Rpartis Htrognes. </institution> <type> PhD Thesis, </type> <institution> Research Report 93-27 , IBP, University Paris 6, France, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: GATOSTAR is based on two independent systems realized at the MASI lab: GATOS and STAR. G AT OS is a load sharing manager which automatically distributes parallel applications among heterogeneous hosts <ref> [18, 19] </ref>. STAR is a fault tolerance manager which automatically recovers processes of faulty machines based on message logging [31]. They are implemented on a set of Sun3 and Sun4 workstations connected by a LAN (Ethernet) and they run on top of Unix (SunOS). <p> section an overview of a multicriteria load sharing algorithm and how these algorithms can be adapted to react to unexpected load variation by means of a dynamic migration mechanism. 5.1 Process placement To launch new processes and to restart processes of faulty hosts, GATOSTAR uses the GATOS load sharing manager <ref> [18, 19] </ref>. This manager automatically distributes parallel applications among heterogeneous hosts. To be executed, application processes need resources (CPU, memory, file access, communication) that vary from one process to another. <p> He specifies the set of hosts involved in the load distribution and provides different versions for each program in order to deal with system heterogeneity. The following section describes the algorithm for the first of the criteria. For a complete description, see <ref> [9, 19] </ref>. Selecting the Less Loaded Host. Allocating processes according to the hosts' load allows to benefit from the idle workstations power.
Reference: 20. <author> G. A. Geist and V. S. Sunderam. </author> <title> Network Based Concurrent Computing on the PVM System. </title> <journal> Journal of Concurrency: Practice and Experience , 4(4) </journal> <pages> 293-311, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Earlier works concentrated on static allocation, adapted for multiprocessor systems without interference between applications' components [27, 33] or adapted for specific given applications [24, 29]. A well-known parallel application manager for local area networks of workstations is the PVM system <ref> [20] </ref>. It uses a user-managed host file indicating a list of hosts to run an application. This scheme is adequate for a dedicated environment, but may lead to overutilization of resources if different users use overlapping sets of hosts.
Reference: 21. <author> R.S. Harbus. </author> <title> Dynamic Process Migration: To Migrate or Not To Migrate. </title> <type> Technical Note CSRI-42, </type> <institution> University of Toronto, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: Alonso and Cova [1] and Bernard and Simatic [4] empirically give the same value to the migration threshold, respectively 1.75 and 1.8, while Harbus <ref> [21] </ref> uses values between 2.5 and 5! This threshold seems to depend on the behavior of the migrated processes, on the ratio (available processing power / mean number of processes), and on users behavior. There are few studies on the reception threshold.
Reference: 22. <author> S. Israel and D. Morris. </author> <title> A Nonintrusive Checkpointing Protocol. </title> <booktitle> In Proc. of the Phoenix Conference on Communications and Computers, </booktitle> <pages> pp. 413-421, </pages> <year> 1989. </year>
Reference-contexts: The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint [6, 15, 36], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [22, 25] </ref>, or using message logging [8, 23, 34]. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism.
Reference: 23. <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in Distributed Systems Using Optimistic Message Logging and Checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint [6, 15, 36], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging <ref> [8, 23, 34] </ref>. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. The major ones are the systems REM (Remote Execution Manager) [32], Paralex [2], Condor [26], and DAWGS (Distributed Automated Workload sharing System) [11].
Reference: 24. <author> C. Kim and H. Kameda. </author> <title> Optimal Static Load Balancing of Multi-class Jobs in a Distributed Computer System. </title> <booktitle> In Proc. of the 10th International Conference on Distributed Computing Systems, Paris, France, </booktitle> <pages> pp. 562-569, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Finally, we describe four existing unified load sharing and fault tolerant facilities. 2.1 Parallel application placement in LAN Parallel program placement has been widely studied. Earlier works concentrated on static allocation, adapted for multiprocessor systems without interference between applications' components [27, 33] or adapted for specific given applications <ref> [24, 29] </ref>. A well-known parallel application manager for local area networks of workstations is the PVM system [20]. It uses a user-managed host file indicating a list of hosts to run an application.
Reference: 25. <author> R. Koo and S. Toueg. </author> <title> Checkpointing and Rollback-Recovery for Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering , SE-13(1):23-21, </journal> <month> January </month> <year> 1987. </year>
Reference-contexts: Moreover, after a failure, surviving processes may have to rollback to their latest checkpoint in order to remain consistent with recovering processes <ref> [25] </ref>. In independent checkpointing, each process saves its state without external synchronization.This technique is simple, but since the set of checkpoints may not define a consistent global state, the failure of one process leads to the rollback of other processes. Reliable message logging avoids this classical domino effect. <p> The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint [6, 15, 36], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [22, 25] </ref>, or using message logging [8, 23, 34]. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. <p> In that case, it can independently be recovered. Difficulties arise when a process communicates with others and/or accesses files. The recovery method uses checkpoint/rollback management <ref> [25] </ref>. By checkpointing, user's pro - cesses save their states to survive their local host failures. We adopt independent checkpointing with pessimistic message logging. This technique is tailored to appli - cations consisting of processes exchanging small streams of data.
Reference: 26. <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka. Condor - A Hunter of Idle Workstations. </title> <booktitle> In Proc. of the 8th International Conference on Distributed Computing Systems, </booktitle> <address> San Jos, California, </address> <pages> pp. 104-111, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: In such an environment, * Authors affiliation: UFR dInformatique, University Paris VII, Paris, France workstations are free from 33% to 90% of the time [35]. By using a load sharing management, users applications can take advantage of all the hosts' processing power and particularly of the idle ones <ref> [4, 26] </ref>. Thus, not only the application response time greatly decreases when there is no fault but also the software fault tolerant overhead becomes acceptable. There are several common mechanisms in the implementation of a load sharing manager (LSM) and of a fault tolerant manager (FTM). <p> They are divided in two groups, dynamic placement (as Radio [4], REM [32], Utopia [37]) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor <ref> [26] </ref>, Mosix [3], Sprite [13]) where processes can move according to overload conditions or the reactivation of workstations by their owners. Utopia is the closest to GATOS. It is based on previous simulation experiments of Ferrari and Zhou [16] about load thresholds values. <p> The major ones are the systems REM (Remote Execution Manager) [32], Paralex [2], Condor <ref> [26] </ref>, and DAWGS (Distributed Automated Workload sharing System) [11]. These four systems have been developed essentially as load sharing manager, since fault tolerance is a limited extension. To our knowledge, no work has compared and unified the two concepts. REM and Paralex are load sharing managers that support cooperative applications.
Reference: 27. <author> V.M. Lo. </author> <title> Task Assignement to Minimize Completion Time. </title> <booktitle> In Proc. of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 239-336, </pages> <year> 1985. </year>
Reference-contexts: Finally, we describe four existing unified load sharing and fault tolerant facilities. 2.1 Parallel application placement in LAN Parallel program placement has been widely studied. Earlier works concentrated on static allocation, adapted for multiprocessor systems without interference between applications' components <ref> [27, 33] </ref> or adapted for specific given applications [24, 29]. A well-known parallel application manager for local area networks of workstations is the PVM system [20]. It uses a user-managed host file indicating a list of hosts to run an application.
Reference: 28. <author> D. Powell, G. Bonn, D. Seaton, P. Verissimo, and F. Waeselynck. </author> <title> The Delta-4 Approach to Dependability in Open Distributed Computing Systems. </title> <booktitle> In Proc. of the 18th International Symposium on Fault-Tolerant Computing Systems, </booktitle> <address> Tokyo, Japan, </address> <pages> pp. 246-251, </pages> <year> 1988. </year>
Reference-contexts: Therefore, we favor solutions with a low overhead under normal operation, possibly to the detriment of an increase in recovery time. We consider that the system is composed of failsilent processors <ref> [28] </ref>, where the faulty nodes simply stop and the remote nodes are not notified.
Reference: 29. <author> C.C. Price and S. Krishnaprasad. </author> <title> Software Allocation Models for Distributed Computing Systems. </title> <booktitle> In Proc. of the 4th International Conference on Distributed Computing Systems, San Fransisco, </booktitle> <pages> pp. 40-48, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Finally, we describe four existing unified load sharing and fault tolerant facilities. 2.1 Parallel application placement in LAN Parallel program placement has been widely studied. Earlier works concentrated on static allocation, adapted for multiprocessor systems without interference between applications' components [27, 33] or adapted for specific given applications <ref> [24, 29] </ref>. A well-known parallel application manager for local area networks of workstations is the PVM system [20]. It uses a user-managed host file indicating a list of hosts to run an application.
Reference: 30. <author> M. Ruffin. KITLOG: </author> <title> a Generic Logging Service. </title> <booktitle> In Proc. of the 11h Symposium on Reliable Distributed Systems , Houston, Texas, </booktitle> <pages> pp. 139-146, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Shared files management has not been considered. Such management is complex, and requires a distributed database manager [5]. Another extension would be to integrate an existing and open log manager, as in Camelot [12], Clio [17] or KitLog <ref> [30] </ref>. 5 Process Allocation The process allocation in G ATOSTAR is not limited to independent programs. It is mostly geared towards parallel applications and takes into account processes behavior.
Reference: 31. <author> P. Sens and B. Folliot. </author> <title> Star: A Fault Tolerant System for Distributed Applications. </title> <booktitle> In Proc of the 5th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, Texas, </address> <pages> pp. 656-660, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: G AT OS is a load sharing manager which automatically distributes parallel applications among heterogeneous hosts [18, 19]. STAR is a fault tolerance manager which automatically recovers processes of faulty machines based on message logging <ref> [31] </ref>. They are implemented on a set of Sun3 and Sun4 workstations connected by a LAN (Ethernet) and they run on top of Unix (SunOS).
Reference: 32. <author> G. C. Shoja, G. Clarke, and T. Taylor. REM: </author> <title> A Distributed Facility For Utilizing Idle Processing Power of Workstations. </title> <booktitle> In Proc. of the IFIP Conference on Distributed Processing, </booktitle> <address> Amsterdam, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: It may also lead to unacceptable interference with hosts supporting interactive users. Most of the previous works on load sharing deal only with processor load and are not adapted for the allocation of entire parallel applications allocation. They are divided in two groups, dynamic placement (as Radio [4], REM <ref> [32] </ref>, Utopia [37]) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor [26], Mosix [3], Sprite [13]) where processes can move according to overload conditions or the reactivation of workstations by their owners. <p> The major ones are the systems REM (Remote Execution Manager) <ref> [32] </ref>, Paralex [2], Condor [26], and DAWGS (Distributed Automated Workload sharing System) [11]. These four systems have been developed essentially as load sharing manager, since fault tolerance is a limited extension. To our knowledge, no work has compared and unified the two concepts. <p> Checkpoint cost The previous measures show that the total cost of our fault management is low for the specific studied applications, i.e. long-running ones with small message exchanges. 7.1 Performance comparison with other systems The mean time for a placement request is 5 seconds in REM <ref> [32] </ref>, 500 ms in Radio [4], and 12 seconds in DAWGS [11]. These high allocation times are essentially due to highly communicating algorithms, while GATOS (recall its 100 ms allocation time) does not need any extra communication when allocating hosts.
Reference: 33. <author> H. S. Stone. </author> <title> Multiprocessor Scheduling with the Aid of Network Flow Algorithms. </title> <journal> IEEE Transactions on Software Engineering , SE-3(1):85-93, </journal> <month> January </month> <year> 1977. </year>
Reference-contexts: Finally, we describe four existing unified load sharing and fault tolerant facilities. 2.1 Parallel application placement in LAN Parallel program placement has been widely studied. Earlier works concentrated on static allocation, adapted for multiprocessor systems without interference between applications' components <ref> [27, 33] </ref> or adapted for specific given applications [24, 29]. A well-known parallel application manager for local area networks of workstations is the PVM system [20]. It uses a user-managed host file indicating a list of hosts to run an application.
Reference: 34. <author> R.E. Strom and S.A. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint [6, 15, 36], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging <ref> [8, 23, 34] </ref>. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism. The major ones are the systems REM (Remote Execution Manager) [32], Paralex [2], Condor [26], and DAWGS (Distributed Automated Workload sharing System) [11].
Reference: 35. <author> M. M. Theimer and K. A. Lantz. </author> <title> Finding Idle Machines in a Workstation-based Distributed System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-1458, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Indeed, studies have shown that many hosts are unused or idle at a given time in local area networks of workstations. In such an environment, * Authors affiliation: UFR dInformatique, University Paris VII, Paris, France workstations are free from 33% to 90% of the time <ref> [35] </ref>. By using a load sharing management, users applications can take advantage of all the hosts' processing power and particularly of the idle ones [4, 26]. Thus, not only the application response time greatly decreases when there is no fault but also the software fault tolerant overhead becomes acceptable.
Reference: 36. <author> Z. Tong, R.Y. Kain, and W.T. Tsai. </author> <title> A Lower Overhead Checkpointing and Rollback Recovery Scheme for Distributed Systems. </title> <booktitle> In Proc. of the 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 12-20, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Reliable message logging avoids this classical domino effect. A substantial body of work has been published regarding fault tolerance by means of checkpointing. The main issues that have been covered are reducing the number of messages required to synchronize a checkpoint <ref> [6, 15, 36] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [22, 25], or using message logging [8, 23, 34]. 2.3 Existing load sharing fault tolerant facility Few experiences have been reported on load sharing systems with a fault tolerance mechanism.
Reference: 37. <author> S. Zhou, X. Zheng, J. Wang, and P. Delisle. </author> <title> Utopia: A Load Sharing Facility for Large, Heterogeneous Distributed Computer Systems. </title> <type> Technical Report257, </type> <institution> Computer Systems Research Institute, Toronto University, Canada, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Furthermore, LSM needs some additional information, such as the processors speed, the hosts load and the amount of available memory. The crash detection mechanism needs not be efficient for the LSM, and is often very basic (as in DAWGS [11] and Utopia <ref> [37] </ref>), but is crucial for the FTM. It must be very efficient, in order to react to failures as fast as possible, while its implementation must not overload the network or the workstations. The last feature concerns distributed process management. <p> Most of the previous works on load sharing deal only with processor load and are not adapted for the allocation of entire parallel applications allocation. They are divided in two groups, dynamic placement (as Radio [4], REM [32], Utopia <ref> [37] </ref>) where pro - cesses are allocated at startup and stay on the same location, or dynamic process mi - gration (as Condor [26], Mosix [3], Sprite [13]) where processes can move according to overload conditions or the reactivation of workstations by their owners. Utopia is the closest to GATOS. <p> Allocating processes according to the hosts' load allows to benefit from the idle workstations power. To take into account the heterogeneous character of system hosts, the load of a host is generally considered as directly proportional to CPU utilization and inversely proportional to its CPU speed <ref> [37] </ref>. On each host, the load sharing manager locally computes an average load and uses failure detection messages to exchange load information. Thus, no extra communication is needed when allocating programs.
References-found: 37

