URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-24.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: fvibby,edwardo,ramag@cc.gatech.edu  
Title: The Sensible Sharing Approach to a Scalable, High-Performance Database System  
Author: Vibby Gottemukkala Edward Omiecinski Umakishore Ramachandran 
Note: This work is supported in part by an NSF grant CCR-8619886 and by an NSF PYI Award MIP-9058430.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: March 1993  
Pubnum: GIT-CC-93-24  
Abstract: Exploiting parallelism has become the key to building high-performance database systems. Several approaches to building database systems that support both inter-and intra-query parallelism have been proposed. These approaches can be broadly classified as either Shared Nothing (SN) or Shared Everything (SE). Although the SN approach is highly scalable, it requires complex data partitioning and tuning to achieve good performance whereas the SE approach suffers from non-scalability. We propose a sensible sharing approach which combines the advantages of both SN and SE. We propose an architecture, and data partitioning and scheduling strategies that promote sensible sharing. We analyze the performance and scalability of our approach and compare with that of a SN system. We find that for a variety of workloads and data skew our approach performs and scales at least as well as a SN system that uses the best possible data partitioning strategy. 
Abstract-found: 1
Intro-found: 1
Reference: [BAC + 90] <author> H. Boral, W. Alexander, L. Clay, G. Copeland, S. Danforth, M. Franklin, B. Hart, M. Smith, and P. Valduriez. </author> <title> Prototyping Bubba, a highly parallel database system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 4-23, </pages> <month> March </month> <year> 1990. </year> <month> 18 </month>
Reference-contexts: In the remainder 1 2 of this section we will describe previous work related to each of these components and the innovations and extensions we propose. 2.1 Architecture Several parallel database architectures have been proposed and built. These include both SN systems (e.g. BUBBA and GAMMA) <ref> [BAC + 90, DGS + 90] </ref> and SE systems (e.g. XPRS and DBS3) [SKPO88, BCV91]. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. <p> Logical partitioning divides a relation into a collection of disjoint subsets of tuples. Physical partitioning maps these subsets to processors. 2.2.1 Logical Partitioning A relation can be declustered by applying a partitioning function (e.g. hash, range) to an attribute to decide the mapping between a tuple and a fragment <ref> [DGS + 90, BAC + 90] </ref>. However, when a single attribute is used as the basis for declustering, only queries that involve the 4 declustering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering [GDQ92, LSR92].
Reference: [BCV91] <author> B. Bergsten, M. Couprie, and P. Valduriez. </author> <title> Prototyping DBS3, a shared-memory parallel database system. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 226-234, </pages> <year> 1991. </year>
Reference-contexts: These include both SN systems (e.g. BUBBA and GAMMA) [BAC + 90, DGS + 90] and SE systems (e.g. XPRS and DBS3) <ref> [SKPO88, BCV91] </ref>. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. Then the system can be viewed as a set of interconnected but independent database systems, each managing its own subset of the data.
Reference: [BHT90] <author> M. Bellew, M. Hsu, and V. Tam. </author> <title> Update propagation in distributed memory hierarchy. </title> <booktitle> In Proceedings of the 6th International Conference on Data Engineering, </booktitle> <pages> pages 521-28, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The SN and SE approaches represent the extremities of the spectrum of possible approaches. We propose a Distributed Shared Memory (DSM) based architecture that provides the flexibility of a SE system and the scalability of a SN system (see Fig. 1). In <ref> [BHT90] </ref>, the authors present a DSM based database architecture. However, the thrust of their work was efficient coherence maintenance and concurrency control. DSM was also used in [SN93] to enhance join strategies to efficiently handle data skew.
Reference: [BJR88] <author> M. Balakrishnan, R. Jain, and C. S. Raghavendra. </author> <title> On array storage for conflict-free memory access for parallel processors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 103-107, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: To overcome this deficiency of the diagonal assignment strategy we could use the Magic Squares strategy which guarantees that even the diagonal fragments get assigned to different processors <ref> [BJR88] </ref>. The result of applying this assignment strategy to the grid in Figure 2 (a) can be seen graphically in Figure 3.
Reference: [CBZ91] <author> J. B. Carter, J. K. Bennet, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [LH89, RK91, CBZ91] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture we propose is an extension to the architecture of a commercially available shared-memory multiprocessor [Ken92]. The architecture provides shared-memory in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [LH89, RK91, CBZ91] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [DG90] <author> D. J. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of database processing or a passing fad? SIGMOD RECORD, </title> <booktitle> 19(4) </booktitle> <pages> 104-112, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the key advantages of the SN and SE approaches. From the discussions in the literature <ref> [Sto86, DG90, DG92] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bottlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3.
Reference: [DG92] <author> D. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of high performance database systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the key advantages of the SN and SE approaches. From the discussions in the literature <ref> [Sto86, DG90, DG92] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bottlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3. <p> The effect of the per relation degree of parallelism on transaction response time. 2. The effect of skew on the scheduling strategies (load balancing vs no load balancing). 3. Scalability of the different schemes, both in terms of scaleup and speedup <ref> [DG92] </ref>. 4. The effect of the cost of sharing on the SS system. 5. The effectiveness of the assignment strategies. We have developed a simulation model for our experiments based on CSIM. CSIM is process-oriented, discrete-event simulation package for use with C or C++ programs [Sch90].
Reference: [DGS + 90] <author> D. J. DeWitt, S. Ghandeharizadeh, D. A. Schneider, A. Bricker, H. Hsiao, and R. Rasmussen. </author> <title> The Gamma Database Machine Project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-61, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In the remainder 1 2 of this section we will describe previous work related to each of these components and the innovations and extensions we propose. 2.1 Architecture Several parallel database architectures have been proposed and built. These include both SN systems (e.g. BUBBA and GAMMA) <ref> [BAC + 90, DGS + 90] </ref> and SE systems (e.g. XPRS and DBS3) [SKPO88, BCV91]. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. <p> Logical partitioning divides a relation into a collection of disjoint subsets of tuples. Physical partitioning maps these subsets to processors. 2.2.1 Logical Partitioning A relation can be declustered by applying a partitioning function (e.g. hash, range) to an attribute to decide the mapping between a tuple and a fragment <ref> [DGS + 90, BAC + 90] </ref>. However, when a single attribute is used as the basis for declustering, only queries that involve the 4 declustering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering [GDQ92, LSR92].
Reference: [GD92] <author> S. Ghandeharizadeh and D. J. DeWitt. </author> <title> MAGIC amulti-attribute declustering mechanism for multiprocessor database machines. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Southern California, </institution> <address> Los Angeles, </address> <year> 1992. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of partitions to processors <ref> [HL91, OL92, GD92] </ref> such that each node gets roughly the same number of partitions. Furthermore, these strategies are static in nature. For example, in [GD92] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors. <p> In SN systems the basic technique for skew avoidance is to tune the assignment of partitions to processors [HL91, OL92, GD92] such that each node gets roughly the same number of partitions. Furthermore, these strategies are static in nature. For example, in <ref> [GD92] </ref> the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors. This strategy, however, works only for data skew resulting from high correlation between declustering attributes.
Reference: [GDQ92] <author> S. Ghandeharizadeh, D. J. DeWitt, and W. Qureshi. </author> <title> A performance ananlysis of alternative multi-attribute declustering strategies. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD, </booktitle> <pages> pages 29-38, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: However, when a single attribute is used as the basis for declustering, only queries that involve the 4 declustering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering <ref> [GDQ92, LSR92] </ref>. First, all the attributes to be used in the declustering are identified. Then each dimension (the domain of values of a particular attribute) is divided into a set of sub-domains and tuples are assigned to sub-domains based on their attribute values. <p> The number of sub-domains along each dimension is dependent on the degree of parallelism that is desired. For example, the MAGIC declustering method <ref> [GDQ92] </ref> applies range partitioning to each of the attributes involved in the declustering to create a multi-attribute grid structure. Attribute values of tuples are used as coordinates to assign tuples to elements in the grid. <p> Attribute values of tuples are used as coordinates to assign tuples to elements in the grid. The desired degree of parallelism, P , can be determined when the following cost model is assumed <ref> [WFA92, GDQ92] </ref>: R = P where R is the response time for a query, c t is the cost of processing a tuple, T ave is the average number of tuples accessed by a query and c p is the overhead per unit of parallelism. <p> However, in the case where the number of fragments is greater than the number of processors this strategy will no longer work and a more complex assignment strategy is required. In <ref> [GDQ92] </ref> the authors present a set of complex heuristics to assign the fragments to processors. The complexity of the assignment strategy comes from two sources: 1. <p> fragments (1, 7, 13, 19, 25) in Figure 2 (a) form one slice and are assigned to node 1 (Figure 2 (b)). 1 2 3 4 5 11 12 13 14 15 21 22 23 24 25 =) 2 1 5 4 3 4 3 2 1 5 (b) In <ref> [GDQ92] </ref> the number of ranges in each dimension (N i ) is determined using the frequency with which each dimension is accessed and the degree of parallelism for each dimension (P i ) serves as a lower bound for this number.
Reference: [HL91] <author> K. A. Hua and C. Lee. </author> <title> Handling data skew in multiprocessor database computer using partition tuning. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Databases, </booktitle> <pages> pages 525-535, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of partitions to processors <ref> [HL91, OL92, GD92] </ref> such that each node gets roughly the same number of partitions. Furthermore, these strategies are static in nature. For example, in [GD92] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors.
Reference: [HSIT91] <author> Y. Hirano, T. Satoh, U. Inoue, and K. Teranaka. </author> <title> Load balancing algorithms for parallel database processing on shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 210-217. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Since all processors are uniform in a SE system the sub-queries can be executed on any processor. Several strategies have been proposed to determine when and what a node chooses to process. For example, in <ref> [HSIT91] </ref> the authors present a processor-initiated load-balancing algorithm. In this approach, queries are decomposed into work-units and placed in a central queue. Each idle processor picks some number of work-units and does the required processing.
Reference: [Ken92] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: The DSM architecture differs from the SE architecture in that all nodes are not identical with respect to the cost of accessing a particular data item. The architecture we propose is an extension to the architecture of a commercially available shared-memory multiprocessor <ref> [Ken92] </ref>. The architecture provides shared-memory in hardware and not as a software abstraction built on top of a message-passing environment such as those in [LH89, RK91, CBZ91].
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [LH89, RK91, CBZ91] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture we propose is an extension to the architecture of a commercially available shared-memory multiprocessor [Ken92]. The architecture provides shared-memory in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [LH89, RK91, CBZ91] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [LSR92] <author> J. Li, J. Srivastava, and D. Rotem. CMD: </author> <title> A multidimensional declustering method for parallel database systems. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 3-14, </pages> <year> 1992. </year> <month> 19 </month>
Reference-contexts: However, when a single attribute is used as the basis for declustering, only queries that involve the 4 declustering attribute gain the full benefits of the available parallelism. This problem can be overcome by using multi-attribute declustering <ref> [GDQ92, LSR92] </ref>. First, all the attributes to be used in the declustering are identified. Then each dimension (the domain of values of a particular attribute) is divided into a set of sub-domains and tuples are assigned to sub-domains based on their attribute values.
Reference: [ML92] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: For our experiments we use the diagonal assignment strategy because of our assumptions about the data skew (see Section 3). 1 3 5 2 4 3 5 2 4 1 5 2 4 1 3 2.3 Scheduling Following the classification of scheduling given in <ref> [ML92] </ref> the scheduling strategies in SN and SE systems can be classified as data-affinity and load-balancing scheduling, respectively. In SN 7 systems, queries are decomposed into sub-queries based on the the degree of declustering of the data being accessed. <p> Our approach to scheduling decomposes queries into sub-queries and is processor-initiated. However, our approach takes data-affinity into consideration and thus is similar to the scheduling approach advocated in <ref> [ML92] </ref>. Each node has a private work queue, similar to a SN scheduling strategy. However, any node can access any other node's work queue through DSM. When a query is submitted to the DBMS the declustering directory is consulted to determine the fragments that need to be accessed.
Reference: [OL92] <author> E. Omiecinski and E. Lin. </author> <title> The adaptive hash join algorithm for a Hypercube Multicomputer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 334-49, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Furthermore, our strategy greatly simplifies the process of assigning fragments to nodes in the system. Another problem in physical partitioning is the presence of data skew. In SN systems the basic technique for skew avoidance is to tune the assignment of partitions to processors <ref> [HL91, OL92, GD92] </ref> such that each node gets roughly the same number of partitions. Furthermore, these strategies are static in nature. For example, in [GD92] the authors present a heuristic to avoid imbalances in the amount of data assigned to different processors.
Reference: [Omi91] <author> E. Omiecinski. </author> <title> Performance analysis of a load-balancing hash -join algorithm for a shared memory multiprocessor. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Databases, </booktitle> <pages> pages 375-85, </pages> <year> 1991. </year>
Reference-contexts: The objective of the algorithm is to optimize in two dimensions, namely, load balancing time and idle time. To achieve this objective the number of work-units that are acquired by a processor are reduced in each iteration. In <ref> [Omi91] </ref> a load-balancing scheme is proposed for hash-based join processing in the presence of data skew. The strategy takes advantage of two-phase join processing. In the first phase tuples are assigned to buckets. The size of the buckets can vary widely due to data skew.
Reference: [PMC + 90] <author> H. Pirahesh, C. Mohan, J. Cheng, T. S. Liu, and P. Selinger. </author> <title> Parallelism in relational database systems: </title> <booktitle> Architectural issues and design approaches. In Proceedings of the 2nd International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The rise in the complexity of databases in terms of physical size, query complexity and query volume demands enormous amounts of processing power which can only be satisfied using parallel systems <ref> [PMC + 90] </ref>. An area of debate in parallel database design is the Shared Nothing (SN) approach versus the Shared Everything (SE) approach. Both approaches have their pros and cons. The SN approach lends itself well to large, scalable systems but load-balancing requires complex data partitioning and assignment strategies.
Reference: [RK91] <author> U. Ramachandran and M. Y. A. Khalidi. </author> <title> An implementation of distributed shared memory. </title> <journal> Software Practice & Experience, </journal> <volume> 21(5) </volume> <pages> 443-64, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In the SE approach the global accessibility of data allows uniform utilization of resources. However, the uniformity of a SE system makes communication a bottleneck which hinders the scalability of such a system. Distributed Shared Memory (DSM) <ref> [LH89, RK91, CBZ91] </ref> can be used to overcome the drawbacks of both SN and SE. In a DSM system, memory is physically distributed among the processors. When data is partitioned among processors a DSM system essentially behaves as a SN system. <p> The architecture we propose is an extension to the architecture of a commercially available shared-memory multiprocessor [Ken92]. The architecture provides shared-memory in hardware and not as a software abstraction built on top of a message-passing environment such as those in <ref> [LH89, RK91, CBZ91] </ref>. The reason for choosing a hardware implementation of DSM, despite the ease of implementation of the software-based DSM, is the higher cost of sharing in a software-based DSM. Our design choice is justified by the results in Section 4.
Reference: [RSRM93] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <type> Technical Report GIT-CC-93/03, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA, </address> <month> January </month> <year> 1993. </year> <booktitle> To appear in the Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: The scalability of such an architecture has been studied in <ref> [RSRM93] </ref>. Before we discuss how to build a scalable database system given the proposed architecture, we have to recognize that the SN mode of functioning is acceptable as long as all the nodes are busy processing assigned tasks that access data local to the nodes.
Reference: [Sch90] <author> H. Schwetman. </author> <title> CSIM Users' Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: The effect of the cost of sharing on the SS system. 5. The effectiveness of the assignment strategies. We have developed a simulation model for our experiments based on CSIM. CSIM is process-oriented, discrete-event simulation package for use with C or C++ programs <ref> [Sch90] </ref>. The simulation model consists of a transaction generator, transactions, processors and work queues (see Figure 4). The transaction generator, transactions and processors are modelled as CSIM processes. The transaction generator process generates transactions with a frequency that models the inter-arrival time between transactions.
Reference: [SKPO88] <author> M. Stonebraker, R. Katz, D. Patterson, and J. Ousterhout. </author> <booktitle> The design of XPRS. In Proceedings of the 14th VLDB Conference, </booktitle> <pages> pages 318-330, </pages> <year> 1988. </year>
Reference-contexts: These include both SN systems (e.g. BUBBA and GAMMA) [BAC + 90, DGS + 90] and SE systems (e.g. XPRS and DBS3) <ref> [SKPO88, BCV91] </ref>. SN systems are typically built by interconnecting nodes of processor/disk pairs. Data is split into disjoint sets and assigned to different nodes in the system. Then the system can be viewed as a set of interconnected but independent database systems, each managing its own subset of the data.
Reference: [SN93] <author> A. Shatdal and J. F. Naughton. </author> <title> Using shared virtual memory for parallel join processing. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD, </booktitle> <pages> pages 119-128, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In [BHT90], the authors present a DSM based database architecture. However, the thrust of their work was efficient coherence maintenance and concurrency control. DSM was also used in <ref> [SN93] </ref> to enhance join strategies to efficiently handle data skew. However, these studies did not examine the impact of the architectural assumptions on the scalability of the database system. The DSM architecture we propose consists of interconnected processors, each with a large amount of main-memory and a disk.
Reference: [Sto86] <author> M. Stonebraker. </author> <title> The case for shared nothing. </title> <journal> IEEE Database Engineering, </journal> <volume> 9(1), </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: In Section 4 we present the results of our simulation experiments and discuss their implications. Concluding remarks are given in Section 5. 2 Overview Our objective is to build a database system that captures the key advantages of the SN and SE approaches. From the discussions in the literature <ref> [Sto86, DG90, DG92] </ref> we know that the main drawback to sharing is scalability. We identify the following simple guidelines to building a scalable system: 1. Eliminate central resources that can become bottlenecks. 2. Minimize communication and thus avoid making the interconnect a bottleneck. 3.
Reference: [WFA92] <author> A. N. Wilschut, J. Flokstra, and P. M. G. Apers. </author> <title> Parallelism in a main-memory DBMS: </title> <booktitle> The performance of PRISMA/DB. In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 521-532, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Attribute values of tuples are used as coordinates to assign tuples to elements in the grid. The desired degree of parallelism, P , can be determined when the following cost model is assumed <ref> [WFA92, GDQ92] </ref>: R = P where R is the response time for a query, c t is the cost of processing a tuple, T ave is the average number of tuples accessed by a query and c p is the overhead per unit of parallelism.
References-found: 26

