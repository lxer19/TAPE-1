URL: ftp://info.mcs.anl.gov/pub/ADOLC/PAPERS/sf_col_ver.ps.gz
Refering-URL: http://www.mcs.anl.gov/Projects/autodiff/AD_Tools/adolc.anl/adolc.html
Root-URL: http://www.mcs.anl.gov
Title: Chapter 13 Structure and Efficient Jacobian Calculation  
Author: Thomas F. Coleman Arun Verma 
Keyword: Newton step, Jacobian structure, Jacobian sparsity.  
Date: May 9, 1996  
Abstract: Many computational tasks require the determination of the Jacobian matrix, at a given argument, for a large nonlinear system of equations. Calculation or approximation of a Newton step is a related task. The development of robust automatic differentiation (AD) software allows for "painless" and accurate calculation of these quantities; however, straightforward application of AD software on large-scale problems can require an inordinate amount of computation. Fortunately, large-scale systems of nonlinear equations typically exhibit either sparsity or structure in their Jacobian matrices. In this paper, we proffer general approaches for exploiting sparsity and structure to yield efficient ways to determine Jacobian matrices (and Newton steps) via automatic differentiation. 
Abstract-found: 1
Intro-found: 1
Reference: [Averick1994a] <author> B. M. Averick, J. J. Mor e, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J, via AD, have been developed <ref> [Averick1994a] </ref>, [Coleman1995a], [Hossain1995a]. <p> corresponding Newton equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(3) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., <ref> [Averick1994a] </ref>, [Coleman1995a], [Hossain1995a], can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . <p> these ideas can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of Sparse Jacobians via Bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in <ref> [Averick1994a] </ref>, [Coleman1995a], [Hossain1995a]. In this section we briefly highlight the approach proposed in [Coleman1995a], coined "bi-coloring".
Reference: [Bischof1995g] <author> C. H. Bischof, A. Bouaricha, P. M. Khademi, and J. J. Mor e, </author> <title> Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This special case, the efficient determination of a gradient of a partially separable function via the forward-mode of AD, is studied in detail in <ref> [Bischof1995g] </ref>. 4 Examples We discuss three common classes of structured nonlinear systems. In each case the Jacobian matrix is potentially dense; however, by differentiating the natural high-level program to compute F , as discussed in Section 3, underlying sparsity can be exploited in the use of AD tools.
Reference: [Broyden1965a] <author> C. </author> <title> Broyden, A class of methods for solving nonlinear simultaneous equations, </title> <journal> Mathematics of Computation, </journal> <volume> 19 (1965), </volume> <pages> pp. 577-593. </pages>
Reference-contexts: For example the extended system (3) can be solved directly. This can afford significant savings. To illustrate, consider the following experiment. We define a composite function F (x) following the form described above. The functions ~ F and F are defined to be the Broyden <ref> [Broyden1965a] </ref> function (the Jacobian is tridiagonal). The structure of A is based on the 5-point Laplacian defined on a regular p p n grid.
Reference: [Coleman1986a] <author> T. F. Coleman and J. Y. Cai, </author> <title> The cyclic coloring problem and estimation of sparse Hessian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7 (1986), </volume> <pages> pp. 221-235. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature <ref> [Coleman1986a] </ref>, [Coleman1984a], [Coleman1985a], [Coleman1984b], [Coleman1984c], [Curtis1974a], where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product JV .
Reference: [Coleman1984a] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 329-345. </month> <title> [Coleman1985a] , Software for estimating sparse Hessian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 (1985), </volume> <pages> pp. 363-377. 12 </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature [Coleman1986a], <ref> [Coleman1984a] </ref>, [Coleman1985a], [Coleman1984b], [Coleman1984c], [Curtis1974a], where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product JV .
Reference: [Coleman1984b] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Hessian matrices and graph coloring problems, </title> <journal> Math. Programming, </journal> <volume> 28 (1984), </volume> <pages> pp. </pages> <month> 243-270. </month> <title> [Coleman1984c] , Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. on Numerical Analysis, </journal> <volume> 20 (1984), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature [Coleman1986a], [Coleman1984a], [Coleman1985a], <ref> [Coleman1984b] </ref>, [Coleman1984c], [Curtis1974a], where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product JV .
Reference: [Coleman1995a] <author> T. F. Coleman and A. Verma, </author> <title> The efficient computation of sparse Jacobian matrices using automatic differentiation, </title> <type> Technical report TR95-1557, </type> <institution> Computer Science Department, Cornell University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J, via AD, have been developed [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a]. The bi-coloring approach of Coleman and Verma [Coleman1995a], as discussed in Section 2, rests on the observation that is is usually possible to define "thin" matrices V; W such that the nonzero fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office <p> Recently, techniques for the efficient determination of sparse Jacobian matrices J, via AD, have been developed [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a]. The bi-coloring approach of Coleman and Verma [Coleman1995a], as discussed in Section 2, rests on the observation that is is usually possible to define "thin" matrices V; W such that the nonzero fl This research was partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research of the U.S. <p> Newton equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(3) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a], can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . For example, the work required by the bi-coloring technique developed in [Coleman1995a] is O !(F E ) = O !(F ) <p> E is sparse and clearly sparse AD-techniques, e.g., [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a], can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . For example, the work required by the bi-coloring technique developed in [Coleman1995a] is O !(F E ) = O !(F ) where O is a "bi-chromatic number" dependent on the sparsity of J E . Typically, O &lt;< min (m; n). <p> ideas can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of Sparse Jacobians via Bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a]. In this section we briefly highlight the approach proposed in [Coleman1995a], coined "bi-coloring". <p> program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of Sparse Jacobians via Bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in [Averick1994a], <ref> [Coleman1995a] </ref>, [Hossain1995a]. In this section we briefly highlight the approach proposed in [Coleman1995a], coined "bi-coloring". <p> Again, it is very easy to construct examples, where defining thin W is not possible : consider the case where J has a single dense column. Bi-coloring circumvents the dense row/dense column problem as illustrated in (1). In <ref> [Coleman1995a] </ref> bi-coloring approaches are proposed that allow for both the direct calculation of J the non-zero elements of J are are extracted for the AD-computed products (W T J; J V ) directly, without further computations and determination by substitution where the the non-zero elements of J are are extracted for <p> Hence, the computed Jacobian matrix is usually less accurate than the directly determined Jacobian. Nevertheless, as discussed in <ref> [Coleman1995a] </ref>, the loss of accuracy is usually minimal. Computational experiments are reported in [Coleman1995a] which illustrate the effectiveness of bi-coloring as opposed to strictly 1-sided schemes. <p> Hence, the computed Jacobian matrix is usually less accurate than the directly determined Jacobian. Nevertheless, as discussed in <ref> [Coleman1995a] </ref>, the loss of accuracy is usually minimal. Computational experiments are reported in [Coleman1995a] which illustrate the effectiveness of bi-coloring as opposed to strictly 1-sided schemes. A 1-sided scheme may be column-based: partition the columns of J to define a thin matrix V such that the non-zeroes of J can be determined form the product J V (computed via forward mode AD). <p> Clearly on this set of sparsity structures bi-coloring is a significant win over 1-sided calculations . Moreover, bi-coloring combined with determination by substitution represents the least-cost approach. Additional experiments, with more detail, and further related discussion is given in <ref> [Coleman1995a] </ref>. Table 1 Totals for LP Collection (http://www.netlib.org/lp/data). Bi-coloring 1-sided Coloring Direct Substitution column row 337 270 1753 452 3 Structure Our thesis can be summarized as follows. Large-scale nonlinear systems F (x) = 0 often exhibit a natural lower Hessenberg form. <p> It is clear that the computation of J E will, in general, be considerably more economical than than the straightforward application of AD to detemine J = i=1:p ~ J i J i via AD. For example, Jacobians ~ J, J can be determined by applying the bi-coloring technique <ref> [Coleman1995a] </ref> to ~ F and F respectively. And of course if F is simple enough, J will be constant (and trivially available). If the function F is itself non-trivial, it may be advantageous to exploit this by expanding the 2-line program. <p> j = ^ F j (w j ) P t Clearly the evaluation of this "group partially separable" function is easily expressed in the lower Hessenberg form given in Figure 2; the corresponding Jacobian matrix J E is likely to be sparse and economically computable via the AD techniques in <ref> [Coleman1995a] </ref> 4.2.1 Product Function A special case of the generalized partially separable form is a product function: Suppose that F : &lt; n ! &lt; n is a component-wise product of functions: F (x) = F 1 (x): fl F 2 (x): fl : : : : fl F p (x); <p> i : Clearly, it is entirely possible that J is relatively dense even when each component Jacobian J i is very sparse; hence, direct determination of J via AD is usually unattractive compared to the calculation of the extended Jacobian, J E , via AD (e.g., using the bi-coloring approach <ref> [Coleman1995a] </ref>). 4.3 A Remark Our two main examples, composite functions and generalized partially separable functions, are complementary in a structural sense. The evaluation of a composite function is a depth computation: each subsequent intermediate variable y i depends on the previous intermediate y i1 .
Reference: [Curtis1974a] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature [Coleman1986a], [Coleman1984a], [Coleman1985a], [Coleman1984b], [Coleman1984c], <ref> [Curtis1974a] </ref>, where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product JV .
Reference: [Griewank1990a] <author> A. Griewank, </author> <title> Direct calculation of Newton steps without accumulating Jaco-bians, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM, </publisher> <address> Philadel-phia, Penn., </address> <year> 1990, </year> <pages> pp. </pages> <month> 115-137. </month> <title> [Griewank1993a] , Some bounds on the complexity of gradients, Jacobians, and Hessians, in Complexity in Nonlinear Optimization, </title> <editor> P. Pardalos, ed., </editor> <publisher> World Scientific Publishers, </publisher> <year> 1993, </year> <pages> pp. 128-161. </pages>
Reference-contexts: Given a computer code to evaluate F (x), the techniques of automatic differentiation (AD) can be used to compute J (x). There are two basic modes of automatic differentiation, forward and reverse, e.g., <ref> [Griewank1990a] </ref>, [Griewank1993a]. Forward mode AD yields J in time proportional to n !(F ), where !(F ) is the number of flops to evaluate F (x). Alternatively, reverse mode AD yields J (x) in time proportional to m !(F ). <p> Given an arbitrary n-by-t V matrix V , product J V can be directly calculated using automatic differentiation in the "forward mode"; given an arbitrary m-by-t W matrix W , the product W T J can be calculated using automatic differentiation in the "reverse mode", e.g., <ref> [Griewank1990a] </ref>, [Griewank1993a]. <p> For example, consider the case where f is a partially separable function, f = f 1 + f 2 + + f p , where f i : &lt; n ! &lt;, i = 1 : p, and each component function f i depends on only a 1 Griewank <ref> [Griewank1990a] </ref> has proposed a similar idea in a more extreme form: F E is defined with respect to all intermediate variables. The resulting extended Jacobian matrix J E is huge, but very sparse. 7 few components of x. <p> In many cases this is practical. However, an interesting question is, given an arbitrary (but correct!) program to evaluate F , is it possible to automatically recover the lower Hessenberg form F E ? Of course a fine-grained lower Hessenberg structure is always available from the compiled program <ref> [Griewank1990a] </ref>; however, we are concerned with the natural high-level (coarse-grained) lower-Hessenberg form. This is an open question.
Reference: [Hossain1995a] <author> A. K. M. Hossain and T. Steihaug, </author> <title> Computing a sparse Jacobian matrix by rows and columns, </title> <type> Technical Report 109, </type> <institution> Department of Informatics, University of Bergen,, Bergen, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J, via AD, have been developed [Averick1994a], [Coleman1995a], <ref> [Hossain1995a] </ref>. <p> equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(3) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., [Averick1994a], [Coleman1995a], <ref> [Hossain1995a] </ref>, can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . <p> can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of Sparse Jacobians via Bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in [Averick1994a], [Coleman1995a], <ref> [Hossain1995a] </ref>. In this section we briefly highlight the approach proposed in [Coleman1995a], coined "bi-coloring".
References-found: 10

