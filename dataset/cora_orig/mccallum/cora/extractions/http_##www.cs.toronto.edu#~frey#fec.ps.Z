URL: http://www.cs.toronto.edu/~frey/fec.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/index.html
Root-URL: 
Title: Free energy coding  In  
Author: Brendan J. Frey and Geoffrey E. Hinton J. A. Storer and M. Cohn 
Keyword: Index Terms source coding, data compression, bits-back, free energy, expecta tion maximization, Boltzmann distribution.  
Note: (editors), Proceedings of the Data Compression Conference 1996, IEEE Computer Society Press,  
Address: 10 King's College Road Toronto, Canada, M5S 1A4  Los Alamitos, CA.  
Affiliation: Department of Electrical Engineering University of Toronto  
Email: E-mail: frey@cs.toronto.edu and hinton@cs.toronto.edu  
Phone: Phone: +1-416-978-7391 Fax: +1-416-978-1455  
Abstract: In this paper, we introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed, the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baum, L. E., and Eagon, J. A. </author> <year> 1967. </year> <title> An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. </title> <journal> Bull. </journal> <volume> AMS 73, </volume> <pages> 360-363. </pages>
Reference-contexts: Only then can the parameters be adjusted to increase or decrease the probability of the symbol. There are many algorithms for accomplishing this incomplete data task: the expectation maximization (EM) algorithm [2], the generalized EM algorithms [6], the Baum-Welch algorithm <ref> [1] </ref>, the expectation-conditional maximization algorithms [6], and the incremental EM algorithm [7]. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3].
Reference: [2] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm (with discussion). </title> <journal> Journal of the Royal Statistical Society B 39, </journal> <pages> 1-38. </pages>
Reference-contexts: Only then can the parameters be adjusted to increase or decrease the probability of the symbol. There are many algorithms for accomplishing this incomplete data task: the expectation maximization (EM) algorithm <ref> [2] </ref>, the generalized EM algorithms [6], the Baum-Welch algorithm [1], the expectation-conditional maximization algorithms [6], and the incremental EM algorithm [7]. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3].
Reference: [3] <author> Ferguson, J. D. </author> <year> 1980. </year> <title> Hidden Markov analysis: An introduction. In Proceedings of the Symposium on the Applications of Hidden Markov Models to Text and Speech, </title> <address> IDA-CRD, Princeton, NJ. </address>
Reference-contexts: The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models <ref> [3] </ref>. Neal and Hinton [7] show that all of these algorithms can be viewed as variants of the EM algorithm that attempt to optimize a single cost function in different ways. The cost function is the free energy given in equation 4, averaged over the data set. <p> In its most basic form, the HMM is a probabilistic model of discrete-time sequences. Details of HMM operation and parameter estimation are not given here; a short, clear description of HMM basics is given by Ferguson <ref> [3] </ref>. It is sufficient to note that the model parameters can be estimated from a data set using the Baum-Welch algorithm (a variant of the EM algorithm) and also that for a given symbol sequence the model can be used to produce a codeword. <p> The letter output table is set in the same fashion, except 0.975 and 0.001 are used instead of 0.962 and 0.002. Using a method described in <ref> [3] </ref> we find the entropy of this data source is 9.32 bits/string. To estimate the parameters of a HMM with the same architecture as above, we first initialize the probabilities to be uniform plus 2% peak-to-peak uniform noise (to break symmetry).
Reference: [4] <author> Hinton, G. E., and Zemel, R. S. </author> <year> 1994. </year> <title> Autoencoders, minimum description length, and Helmholtz free energy. </title> <editor> In J. K. Cowan, G. Tesauro and J. Alspector (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Bits-back The approach we take to solve this problem is motivated by the "bits-back" argument of Hinton and Zemel <ref> [4] </ref>. It turns out that by randomly selecting codewords, we can communicate extra auxiliary information along with the data we are coding. The bits communicated in the auxiliary data will more than make up for the excess codeword lengths that result from not always using the shortest codeword.
Reference: [5] <author> Huffman, D. A. </author> <year> 1952. </year> <title> A method for the construction of minimum redundancy codes. </title> <booktitle> Proceedings of the Institute of Radio Engineers 40, </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: In the example given above, there were two codewords and each was selected equally often so that a single bit could be used for bits-back. If the codeword selection distribution is dyadic 1 , Huffman coding <ref> [5] </ref> may be used. Here, we consider the case of an arbitrary codeword selection distribution. In this more general case, it is not so easy to see how random codeword choices can be made without losing auxiliary data information.
Reference: [6] <author> Meng, X. L., and Rubin, D. B. </author> <year> 1992. </year> <title> Recent extensions of the EM algorithm (with discussion). </title> <editor> In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (editors), </editor> <booktitle> Bayesian Statistics 4, </booktitle> <publisher> Clarendon Press, </publisher> <address> Oxford England. </address>
Reference-contexts: Only then can the parameters be adjusted to increase or decrease the probability of the symbol. There are many algorithms for accomplishing this incomplete data task: the expectation maximization (EM) algorithm [2], the generalized EM algorithms <ref> [6] </ref>, the Baum-Welch algorithm [1], the expectation-conditional maximization algorithms [6], and the incremental EM algorithm [7]. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3]. <p> Only then can the parameters be adjusted to increase or decrease the probability of the symbol. There are many algorithms for accomplishing this incomplete data task: the expectation maximization (EM) algorithm [2], the generalized EM algorithms <ref> [6] </ref>, the Baum-Welch algorithm [1], the expectation-conditional maximization algorithms [6], and the incremental EM algorithm [7]. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3].
Reference: [7] <author> Neal, R. M., and Hinton, G. E. </author> <year> 1996. </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted to Biometrika. </note>
Reference-contexts: There are many algorithms for accomplishing this incomplete data task: the expectation maximization (EM) algorithm [2], the generalized EM algorithms [6], the Baum-Welch algorithm [1], the expectation-conditional maximization algorithms [6], and the incremental EM algorithm <ref> [7] </ref>. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3]. Neal and Hinton [7] show that all of these algorithms can be viewed as variants of the EM algorithm that attempt to optimize a single cost function in different ways. <p> this incomplete data task: the expectation maximization (EM) algorithm [2], the generalized EM algorithms [6], the Baum-Welch algorithm [1], the expectation-conditional maximization algorithms [6], and the incremental EM algorithm <ref> [7] </ref>. The Baum-Welch algorithm, for example, is known 6 for its application in training hidden Markov models [3]. Neal and Hinton [7] show that all of these algorithms can be viewed as variants of the EM algorithm that attempt to optimize a single cost function in different ways. The cost function is the free energy given in equation 4, averaged over the data set.
Reference: [8] <author> Rissanen, J., and Langdon, G. G. </author> <year> 1976. </year> <title> Arithmetic coding. </title> <journal> IBM Journal of Research and Development 23, </journal> <pages> 149-162. </pages>
Reference: [9] <author> Thompson, C. J. </author> <year> 1988. </year> <title> Classical Equilibrium Statistical Mechanics. </title> <publisher> Clarendon Press, </publisher> <address> Oxford England. </address>
Reference-contexts: the information content (entropy) of the distribution used to select codewords: H (x) y The difference between equations 2 and 3 gives the effective average codeword length, F (x): F (x) E (x) H (x): (4) Since this quantity is analogous to the variational Helmholtz free energy from statistical physics <ref> [9] </ref>, we refer to the effective average codeword length as the free energy. Free energy coding makes gains over shortest codeword selection by taking into account the existence of multiple codewords of similar length.
Reference: [10] <author> Viterbi, A. J., and Omura, J. K. </author> <year> 1979. </year> <title> Principles of Digital Communication and Coding. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: Further, it is apparent that a large body of popular methods for fitting models with hidden variables | the EM variants | produce source models for which shortest codeword selection is 2 To find the shortest codeword for a given string, we use the Viterbi algorithm <ref> [10] </ref>. 8 suboptimal in compression efficiency whereas free energy coding is optimal. We have shown that a practical free energy coding algorithm can be implemented and that it does indeed outperform a shortest codeword selection algorithm in compression performance.
Reference: [11] <author> Witten, I. H., Neal R. M., and Cleary J. G. </author> <year> 1987. </year> <title> Fast arithmetic coding using low-precision division. </title> <journal> Communications of the ACM 30, </journal> <pages> 520-540. 9 </pages>
References-found: 11

