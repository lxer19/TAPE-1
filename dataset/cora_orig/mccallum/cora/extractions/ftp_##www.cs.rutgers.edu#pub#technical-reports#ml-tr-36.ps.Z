URL: ftp://www.cs.rutgers.edu/pub/technical-reports/ml-tr-36.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Email: hirsh@cs.rutgers.edu  
Title: The Computational Complexity of the Candidate-Elimination Algorithm  
Author: Haym Hirsh 
Address: New Brunswick, NJ 08903  
Affiliation: Computer Science Department Hill Center for the Mathematical Science Busch Campus Rutgers University  
Abstract: Mitchell's original work on version spaces ( Mitchell, 1982 ) presented an analysis of the computational complexity of version spaces. However, this analysis proved somewhat coarse, as it was parameterized by s and g, the maximum sizes that the S and G sets reach during learning. As has been pointed out by Haussler ( 1988 ) , g can be exponential in the number of examples processed. This paper presents a more fine-grained analysis of the computational complexity of version spaces, demonstrates its equivalence to Mitchell's analysis, and instantiates it for two commonly used conjunctive concept description languages. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> T. G. Dietterich, B. London, K. Clarkson, and G. Dromey. </author> <title> Learning and inductive inference. </title> <editor> In P. Cohen and E. A. Feigenbaum, editors, </editor> <booktitle> The Handbook of Artificial Intelligence, Volume III. </booktitle> <publisher> William Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1982. </year>
Reference-contexts: For positive data, removing elements of a G set that don't cover an instance takes O (jGjc above? ) time, since each G-set element must be tested to check whether it 1 The analyses of this paper assume that learning uses a concept description language for which the single-representation trick <ref> ( Dietterich et al., 1982 ) </ref> holds, that is, that for each instance there is a concept definition that covers only that example and no others. <p> This section considers what happens to the computational complexity across multiple examples, and shows that the more fine-grained analysis given here is equivalent to Mitchell's <ref> ( 1982 ) </ref> original analysis.
Reference: 2. <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 26(2) </volume> <pages> 177-221, </pages> <month> Sept. </month> <year> 1988. </year>
Reference: 3. <author> H. Hirsh. </author> <title> Polynomial-time learning with version spaces. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference: 4. <author> T. M. Mitchell. </author> <title> Version Spaces: An Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1978. </year>
Reference: 5. <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <month> March </month> <year> 1982. </year>
Reference: 6. <author> B. D. Smith and P. S. Rosenbloom. </author> <title> Incremental non-backtracking focusing: A polynomially bounded generalization algorithm for version spaces. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 848-853, </pages> <address> Boston, MA, </address> <month> August </month> <year> 1990. </year> <month> 7 </month>
References-found: 6

