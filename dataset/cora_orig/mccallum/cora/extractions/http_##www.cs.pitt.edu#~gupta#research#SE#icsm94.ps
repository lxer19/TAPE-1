URL: http://www.cs.pitt.edu/~gupta/research/SE/icsm94.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/testing.html
Root-URL: 
Email: fgupta,soffag@cs.pitt.edu  
Title: A Framework for Partial Data Flow Analysis  
Author: Rajiv Gupta and Mary Lou Soffa 
Address: Pittsburgh Pittsburgh, Pa, 15260  
Affiliation: Department of Computer Science University of  
Abstract: Although data flow analysis was first developed for use in compilers, its usefulness is now recognized in many software tools. Because of its compiler origins, the computation of data flow for software tools is based on the traditional exhaustive data flow framework. However, although this framework is useful for computing data flow for compilers, it is not the most appropriate for software tools, particularly those used in the maintenance stage. In maintenance, testing and debugging is typically performed in response to program changes. As such, the data flow required is demand driven from the changed program points. Rather than compute the data flow exhaustively using the traditional data flow framework, we present a framework for partial analysis. The framework includes a specification language enabling the specification of the demand driven data flow desired by a user. From the specification, a partial analysis algorithm is automatically generated using an L-attributed definition for the grammar of the specification language. A specification of a demand driven data flow problem expresses characteristics that define the kind of traversal needed in the partial analysis and the type of dependencies to be captured. The partial analyses algorithms are efficient in that only as much of the program is analyzed as actually needed, thus reducing the time and space requirements over exhaustively computing the data flow information. The algorithms are shown to be useful when debugging and testing programs during maintenance. Keywords control flow graph (CFG), program debugging, program testing, code optimization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Burke, </author> <title> "An interval analysis approach toward exhaustive and incremental interprocedural data flow analysis," </title> <type> Technical Report RC 12702, </type> <institution> IBM Thomas J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: When changes are made to code, the data flow has to be recomputed exhaustively and compared to previous data flow or has to be incrementally updated, under the assumption that exhaustive data flow has already been computed <ref> [1, 18, 22] </ref>. A major problem with computing data flow information exhaustively is the high cost both in execution time and memory demands. Experimental studies show that performing analyses even over small or medium size programs can take several hours [13]. <p> Related to partial analysis is incremental analysis in terms of overall goals. However, the focus of incremental data flow analysis is to maintain a global data flow solution by incrementally updating the solution in response to small changes in the program <ref> [1, 18, 21, 22, 26] </ref>. It may be too expensive to fully reanalyze a program from scratch each time a small change is made to the program.
Reference: [2] <author> D. Callahan and J. Subhlok, </author> <title> "Static analysis of low-level synchronization," </title> <booktitle> Proceeding of the ACM SIG-PLAN/SIGOPS Workshop on Parallel and Distributed Debugging, SIGPLAN Notices, </booktitle> <volume> Vol. 24, No. 1, </volume> <pages> pages 100-111, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers [3, 9, 19] program integration [10], and parallel program analyzers <ref> [2, 4] </ref>. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8]. Although compilers and software tools utilize static analysis to improve their capabilities and performances, there are important differences in the data flow information needed between these two classes of software.
Reference: [3] <author> E. Duesterwald, R. Gupta and M.L. Soffa, </author> <title> "Rigorous data flow testing through output influences," </title> <booktitle> Proc. 2nd Irvine Software Symposium, </booktitle> <pages> pages 131-145, </pages> <address> Irvine, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers <ref> [3, 9, 19] </ref> program integration [10], and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8].
Reference: [4] <author> E. Duesterwald and M.L. Soffa, </author> <title> "Static concurrency analysis in the presence of procedures using a data-flow framework," </title> <booktitle> Proc. ACM Symposium on Testing, Analysis, and Verification, </booktitle> <address> Victoria, British Columbia, </address> <pages> pages 36-48, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers [3, 9, 19] program integration [10], and parallel program analyzers <ref> [2, 4] </ref>. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8]. Although compilers and software tools utilize static analysis to improve their capabilities and performances, there are important differences in the data flow information needed between these two classes of software.
Reference: [5] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren, </author> <title> "The program dependence graph and its use in optimization," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 9, No. 3, </volume> <pages> pages 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Its use in parallelizing compilers is invaluable, as code must be transformed using data dependency information in order to fully exploit the parallel architectures <ref> [5, 17] </ref>. <p> Our algorithms use the control flow graph as the program representation. Another type of representation that has been used to compute static program slice, a type of demand driven data flow, is the program dependence graph <ref> [5, 11] </ref>. However, this representation needs to have the data flow computed exhaustively and then selects the information to present to the user, using the program dependence graph.
Reference: [6] <author> R. Gupta and M.L. Soffa, </author> <title> "Employing static information in the generation of test cases," </title> <journal> Journal of Software Testing, Verification and Reliability, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pages 29-48, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Data flow has been proven to be especially useful in tools for the maintenance stage <ref> [6, 8] </ref>. Although compilers and software tools utilize static analysis to improve their capabilities and performances, there are important differences in the data flow information needed between these two classes of software. <p> test case generation. 4.1 Test Case Generation and Regression Testing In order to reduce the number of test cases generated when retesting a program after changes using data flow testing, statically determinable properties of a program, namely postdominance and dominance, can be used to guide the test case generation process <ref> [6, 7] </ref>. Although these properties can be efficiently determined exhaustively, program changes would require repeated computations of the properties. Our partial analysis can be used to avoid these computations after program changes.
Reference: [7] <author> R. Gupta, </author> <title> "Generalized dominators and postdomina-tors," </title> <booktitle> Proc. 19th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 246-257, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: test case generation. 4.1 Test Case Generation and Regression Testing In order to reduce the number of test cases generated when retesting a program after changes using data flow testing, statically determinable properties of a program, namely postdominance and dominance, can be used to guide the test case generation process <ref> [6, 7] </ref>. Although these properties can be efficiently determined exhaustively, program changes would require repeated computations of the properties. Our partial analysis can be used to avoid these computations after program changes.
Reference: [8] <author> R. Gupta, M.J. Harrold, </author> <title> and M.L. Soffa, "An approach to regression testing using slicing," </title> <booktitle> Proc. Conference on Software Maintenance, </booktitle> <address> Orlando, Florida, </address> <pages> pages 299-308, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Data flow has been proven to be especially useful in tools for the maintenance stage <ref> [6, 8] </ref>. Although compilers and software tools utilize static analysis to improve their capabilities and performances, there are important differences in the data flow information needed between these two classes of software. <p> Construct DOMS: Sdep forward closure all Compute DOMS :- ( S ) We have also demonstrated the utility of partial data flow algorithms in regression testing to find definition-use pairs that must be tested after a program change <ref> [8] </ref>. After changes are made to a previously tested program, regression testing attempts to retest only the changed portion of the code. We developed a data flow regression testing technique based on partial analysis.
Reference: [9] <author> M.J. Harrold and M.L. Soffa, </author> <title> "Interprocedural data flow testing," </title> <booktitle> Proc. ACM Symposium on Software Testing, Analysis and Verification, </booktitle> <pages> pages 158-167, </pages> <address> Key West, Florida, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers <ref> [3, 9, 19] </ref> program integration [10], and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8].
Reference: [10] <author> S. Horwitz, J. Prins and T. Reps, </author> <title> "Integrating non-interfering versions of programs," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 11, No. 3, </volume> <pages> pages 345-387, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers [3, 9, 19] program integration <ref> [10] </ref>, and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8].
Reference: [11] <author> S. Horwitz, T. Reps, and D. Binkley, </author> <title> "Interprocedural slicing using dependence graphs," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 12, No. 1, </volume> <pages> pages 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Our algorithms use the control flow graph as the program representation. Another type of representation that has been used to compute static program slice, a type of demand driven data flow, is the program dependence graph <ref> [5, 11] </ref>. However, this representation needs to have the data flow computed exhaustively and then selects the information to present to the user, using the program dependence graph.
Reference: [12] <author> J.B. Kam and J.D. Ullman, </author> <title> "Monotone data flow analysis frameworks," </title> <journal> Acta Informatica, </journal> <volume> Vol. 7, </volume> <pages> pages 305-317, </pages> <year> 1977. </year>
Reference-contexts: Compilers require information about the flow of data for an entire program, as global optimizations are typically applicable to all code in the program. As such, data flow information is computed exhaustively using the traditional data flow framework <ref> [12] </ref> and is computed before optimizations are applied. The types of data flow or data dependency information needed are based on the kinds of optimizations and paralleliz-ing transformations to be applied and are thus known beforehand. And lastly, optimizations are applied in many cases after a program has been debugged. <p> Also, as was indicated, slices from program points other than where a variable is used are not available (e.g., in Fig. 7, slice at S4 on Y). 5 Related Work The framework presented in this paper differs from the usual data flow framework <ref> [12] </ref> in a number of important ways. First of all, the number and complexity of parameters are increased due to the selection of the starting point and variables that is an inherent property of demand driven data flow.
Reference: [13] <author> W. Landi and B. Ryder, </author> <title> "A safe approximation algorithm for interprocedural pointer aliasing," </title> <booktitle> Proc. of the SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 235-248, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: A major problem with computing data flow information exhaustively is the high cost both in execution time and memory demands. Experimental studies show that performing analyses even over small or medium size programs can take several hours <ref> [13] </ref>. In order to provide more flexibility and efficiency in the data flow computation for software tools, we present a framework for the computation of demand driven data flow using partial analysis algorithms.
Reference: [14] <author> J.R. Lyle and M. Weiser, </author> <title> "Automatic program bug location by program slicing," </title> <booktitle> Proc. Second IEEE Symposium on Computers and Applications, </booktitle> <pages> pages 877-883, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The parameters that are actually needed in the algorithm are derived from these characteristics. One type of demand driven data flow that has been developed and used extensively is the static slice <ref> [14] </ref>. Slicing as defined by Weiser is a data flow analysis technique that computes the set of statements contributing to the dependency information desired for a given slicing criterion.
Reference: [15] <author> K.M. Olender and L.J. Osterweil, </author> <title> "Interprocedural static analysis of sequencing constraints," </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> Vol. 1, No. 1, </volume> <pages> pages 21-52, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Olender and Osterweil have developed a tool that automatically performs static interprocedural sequencing analysis from programmable constraint specifications <ref> [15, 16] </ref>. Sequencing analysis enables the detection of data anomalies in a program. In order to efficiently carry out sequencing analysis for various objects, a program slice is computed for each object.
Reference: [16] <author> K.M. Olender and L.J. Osterweil, "Cecil: </author> <title> a sequencing constraint language for automatic static analysis generation," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 16, No. 3, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: Olender and Osterweil have developed a tool that automatically performs static interprocedural sequencing analysis from programmable constraint specifications <ref> [15, 16] </ref>. Sequencing analysis enables the detection of data anomalies in a program. In order to efficiently carry out sequencing analysis for various objects, a program slice is computed for each object.
Reference: [17] <author> D. Padua and M.J. Wolfe, </author> <title> "Advanced compiler optimizations for supercomputers," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 22, No. 12, </volume> <pages> pages 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Its use in parallelizing compilers is invaluable, as code must be transformed using data dependency information in order to fully exploit the parallel architectures <ref> [5, 17] </ref>.
Reference: [18] <author> L.L. Pollock and M.L. Soffa, </author> <title> "An incremental version of iterative data flow analysis," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 15, No. 12, </volume> <pages> pages 1537-1549, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: When changes are made to code, the data flow has to be recomputed exhaustively and compared to previous data flow or has to be incrementally updated, under the assumption that exhaustive data flow has already been computed <ref> [1, 18, 22] </ref>. A major problem with computing data flow information exhaustively is the high cost both in execution time and memory demands. Experimental studies show that performing analyses even over small or medium size programs can take several hours [13]. <p> Related to partial analysis is incremental analysis in terms of overall goals. However, the focus of incremental data flow analysis is to maintain a global data flow solution by incrementally updating the solution in response to small changes in the program <ref> [1, 18, 21, 22, 26] </ref>. It may be too expensive to fully reanalyze a program from scratch each time a small change is made to the program.
Reference: [19] <author> S. Rapps and E. Weyuker, </author> <title> "Selecting software test data using data flow information," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-11, No. 4, </volume> <pages> pages 367-375, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers [25], software testers <ref> [3, 9, 19] </ref> program integration [10], and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8].
Reference: [20] <author> T. Reps, T. Teitelbaum and A. Demers, </author> <title> "Incremental context-dependent analysis for language-based editors," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 5, No. 3, </volume> <pages> pages 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors <ref> [20] </ref>, debuggers [25], software testers [3, 9, 19] program integration [10], and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8].
Reference: [21] <author> B. Rosen, </author> <title> "Linear cost is sometimes quadratic," </title> <booktitle> Proc. Eighth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 117-124, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Related to partial analysis is incremental analysis in terms of overall goals. However, the focus of incremental data flow analysis is to maintain a global data flow solution by incrementally updating the solution in response to small changes in the program <ref> [1, 18, 21, 22, 26] </ref>. It may be too expensive to fully reanalyze a program from scratch each time a small change is made to the program.
Reference: [22] <author> B.G. Ryder and M. C. Paull, </author> <title> "Incremental data flow analysis algorithms," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 10, No. 1, </volume> <pages> pages 1-50, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: When changes are made to code, the data flow has to be recomputed exhaustively and compared to previous data flow or has to be incrementally updated, under the assumption that exhaustive data flow has already been computed <ref> [1, 18, 22] </ref>. A major problem with computing data flow information exhaustively is the high cost both in execution time and memory demands. Experimental studies show that performing analyses even over small or medium size programs can take several hours [13]. <p> Related to partial analysis is incremental analysis in terms of overall goals. However, the focus of incremental data flow analysis is to maintain a global data flow solution by incrementally updating the solution in response to small changes in the program <ref> [1, 18, 21, 22, 26] </ref>. It may be too expensive to fully reanalyze a program from scratch each time a small change is made to the program.
Reference: [23] <author> S.W.K. Tjiang and J.L. Hennessy, </author> <title> "Sharlit a tool for building optimizers," </title> <booktitle> Proc. ACM Sigplan Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 82-93, </pages> <year> 1992. </year>
Reference-contexts: Furthermore, the above approach requires the precomputation of slices for all objects. Work related to the construction of data flow algorithms has been addressed in the Sharlit tool that was developed to help compiler writers develop opti-mizers and data flow analyzers <ref> [23] </ref>. Abstractions are presented that enable compiler writers to program in a modular fashion. Our technique is oriented toward demand driven data flow computation (although exhaustive data flow can be computed), employs specifications to automatically produce algorithms and is oriented to software engineering tools as well as compilers.
Reference: [24] <author> A. Venkatesh and C.N. Fischer, "SPARE: </author> <title> a development environment for program analysis algorithms," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 18, No. 4, </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: Our technique is oriented toward demand driven data flow computation (although exhaustive data flow can be computed), employs specifications to automatically produce algorithms and is oriented to software engineering tools as well as compilers. SPARE is another tool that facilitates the development of program analysis algorithms <ref> [24] </ref>. This tool supports a high-level specification language through which analysis algorithms are expressed. The denota-tional nature of the specifications enables automatic implementation as well as verification of the algorithms.
Reference: [25] <author> M. Weiser, </author> <title> "Program slicing," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-10, No. 4, </volume> <pages> pages 352-357, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: In addition, static analysis has also become a primary component fl Partially supported by National Science Foundation Presidential Young Investigator Award CCR-9157371 and Grant CCR-9109089 to the University of Pittsburgh. of many software tools, such as editors [20], debuggers <ref> [25] </ref>, software testers [3, 9, 19] program integration [10], and parallel program analyzers [2, 4]. Data flow has been proven to be especially useful in tools for the maintenance stage [6, 8]. <p> Based on the type of change in the program, these two walks are used to find the def-use pairs that must be tested after a change. 4.2 Debugging Various types of demand driven data flow information are particularly suitable for bug localization, including slicing <ref> [25] </ref>.

References-found: 25

