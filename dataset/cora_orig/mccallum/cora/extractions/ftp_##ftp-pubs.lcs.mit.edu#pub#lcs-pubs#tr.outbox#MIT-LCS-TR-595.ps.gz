URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-595.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr500.html
Root-URL: 
Title: Compiler Analysis to Implement Point-to-Point Synchronization in Parallel Programs  
Author: by John Nguyen Stephen A. Ward 
Degree: Submitted to the Department of ELECTRICAL ENGINEERING AND COMPUTER SCIENCE in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY at the  All rights reserved Signature of Author:  Certified by:  Professor of Computer Science and Engineering Thesis Supervisor Accepted by: Frederic R. Morgenthaler Chairman, Department Committee on Graduate Students  
Note: c 1993  
Date: (1987)  (1989)  August 1993  August 13, 1993  
Affiliation: S.B., Computer Science S.B., Mathematics Massachusetts Institute of Technology  S.M., Electrical Engineering and Computer Science Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [Aga91] <author> Anant Agarwal, et al. </author> <title> The MIT alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: SECTION 2.3: MACHINE MODEL 33 2.3 Machine model This thesis assumes a cache-coherent shared-memory interface found on machines such as Alewife <ref> [Aga91] </ref> and Dash [Len92]. Such a multiprocessor can be modeled as a collection of processors P = fp 1 ; : : : p n g and a shared pool of memory units that can be accessed through a network. <p> Since these broadcasts may be very inefficient, the shared-memory interface provides a better solution in those cases. Implementing synchronization through message-passing is only applicable to machines that provide support for both the shared-memory and message-passing models such as the MIT Alewife multiprocessor <ref> [Aga91] </ref>. On machines that only support message-passing, additional program transformations must be done to manage data sharing through explicit communication. Synchronization can be accomplished implicitly in such cases since processors are specifically aware of data sharing with other processors. <p> The imaginary machine is composed of 64 nodes arranged in a 8 fi 8 mesh with bidirectional links between nearest neighbors on the mesh. Each node contains a processor, a memory unit, and a hardware-supported coherent cache. The simulation uses the Alewife <ref> [Aga91] </ref> cache coherence protocol and also allows for explicit message sending between processors. Although most communication is accomplished through the shared-memory interface, some operations such as software-supported barrier synchronization are implemented using explicit messages.
Reference: [AH91] <author> Santosh G. Abraham and David E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Data partitioning involves splitting and aligning data to minimize communication distance between processors and the data they access [KLS90][LC91][GB92][RS91]. In loop partitioning, nested loops can be mapped to processors to minimize non-local memory accesses <ref> [AH91] </ref>. In these techniques, constraints between arrays are formed from flow dependences and occurrences in common statements. A partitioning algorithm then performs heuristics to resolve cyclic constraints and produce a partitioning scheme. Using array flow analysis, more accurate flow dependences can be computed to produce improved partitioning results.
Reference: [AJ87] <author> N. S. Arenstorf and H. F. Jordan. </author> <title> Comparing barrier algorithms. </title> <type> Technical Report ICASE 87-65, </type> <institution> ICASE, Nasa Langley Research Center, </institution> <month> September </month> <year> 1987. </year>
Reference: [AK87] <author> Randy Allen and Ken Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The markExt function introduced earlier can be used to mark the fact that the subarray propagated from the previous iteration of i is external to the loop j. In the terminology of <ref> [AK87] </ref>, this field is equivalent to specifying that the dependence is loop-independent rather than loop-carried with respect to an outer loop. The application of this external field also corresponds to the different cases of dependence checking for different data direction vectors of [BC86] and [Wol89].
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: The calculation of such data dependence information can be adapted primarily from two areas of research: sequential data-flow analysis and array-dependence analysis for parallelizing DO loops. Standard data-flow analysis techniques <ref> [ASU86] </ref> can provide definition-use chains for computing flow dependences. The algorithms can also be adapted to generate information necessary for calculating output and anti-dependences. Unfortunately, these techniques are primarily concerned with scalar variables and pay little attention to flow information on individual array elements. <p> If it is possible that execution of statement S 1 can be followed by statement S 2 , then a directed edge exists from S 1 to S 2 . Figure 2-3 shows how a flow graph can be constructed from sequential constructs in the language. From <ref> [ASU86] </ref>, an edge in the control-flow graph is a forward edge if it is part of a spanning tree formed from a particular depth-first traversal of the graph. Forward edges SECTION 2.2: CONTROL FLOW GRAPH 31 are represented by solid arrows in the figure. <p> This knowledge can be derived from array data flow analysis, an adaptation of conventional scalar data flow analysis. Since this thesis focuses primarily on the domain of array and loop-based data-parallel programs, it is very important that accurate information on array usage be obtained. Conventional data flow analysis techniques <ref> [ASU86] </ref> tend to treat arrays as single variables. A reference to any element of an array is considered a reference to the entire array. Clearly, such conservative analysis cannot be used to derive dependences needed for point-to-point synchronization.
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference: [BC86] <author> Michael Burke and Ron Cytron. </author> <title> Interprocedural dependence analysis and par-allelization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 162-175, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: In the terminology of [AK87], this field is equivalent to specifying that the dependence is loop-independent rather than loop-carried with respect to an outer loop. The application of this external field also corresponds to the different cases of dependence checking for different data direction vectors of <ref> [BC86] </ref> and [Wol89].
Reference: [Bor90] <author> Shekhar Borkar, et al. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81, </pages> <year> 1990. </year>
Reference-contexts: In the conventional scheme of dynamic routing, a message is routed by examining its header which identifies the destination processor for the message. In situations where two messages need to access the same resource, one message must be either blocked or buffered. Architectures such as iWarp <ref> [Bor90] </ref> or NuMesh [War93] seek to alleviate contention costs by introducing the idea of static routing. When destinations of messages are known at compilation, then routing can be scheduled statically to avoid unnecessary contentions [SA91].
Reference: [Bou72] <editor> W. J. Bouknight, et al. </editor> <booktitle> The Illiac IV system. Proceedings of the IEEE, </booktitle> <volume> 60(4) </volume> <pages> 369-388, </pages> <month> April </month> <year> 1972. </year>
Reference-contexts: Introduction The concept of devoting many processing elements to one task in order to increase performance has existed for several decades. Implementations of this concept vary from early array processors such as the Illiac IV <ref> [Bou72] </ref> to the more decoupled MIMD machines of today [Smi78][Sei85][Thi91]. Early array processors and SIMD machines allow parallelism through repeated application of a single computation or instruction to different data.
Reference: [BP90] <author> Micah Beck and Keshav Pingali. </author> <title> From control flow to dataflow. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> Volume II, </volume> <pages> pages 43-52, </pages> <year> 1990. </year> <note> 166 BIBLIOGRAPHY </note>
Reference: [Bre91] <author> Eric A. Brewer, et al. Proteus: </author> <title> a high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: On a real machine, the actual communication overhead can be much higher and can in turn affect execution time much more drastically. This issue is discussed in more detail later in this chapter. 6.2 Simulation environment The multiprocessor simulations for this thesis are done using Proteus <ref> [Bre91] </ref>. Although this simulation tool allows varying many architectural parameters, the figures here are obtained for a fixed hardware model. The imaginary machine is composed of 64 nodes arranged in a 8 fi 8 mesh with bidirectional links between nearest neighbors on the mesh.
Reference: [Cap87] <author> Peter R. Cappello. </author> <title> Space time transformation of cellular algorithms. </title> <editor> In Jr. Earl E. Swartzlander, editor, </editor> <booktitle> Systolic Signal Processing Systems, </booktitle> <pages> pages 161-207. </pages> <publisher> Dekker, </publisher> <year> 1987. </year>
Reference: [CC77] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract interpretation: a unified lattice model for static analysis of programs by construction and approximation of fixpoints. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <year> 1977. </year>
Reference: [CF87] <author> Ron Cytron and Jeanne Ferrante. </author> <title> What's in a name? the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <year> 1987. </year>
Reference: [CFKA90] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory based cache-coherence in large-scale multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In addition, each processor is associated with a data cache to reduce memory-access latency. The resolution and maintenance of multiply-cached copies of data is performed by a cache-coherence protocol <ref> [CFKA90] </ref>. Processors are completely independent from each other in the sense that they are able to execute completely different programs from each other. However, the execution model assumed here is one in which all processors execute the same program, although on different data.
Reference: [CH78] <author> Patrick Cousot and Nicolas Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-96, </pages> <year> 1978. </year>
Reference: [Che86] <author> Marina C. Chen. </author> <title> A design methodology for synthesizing parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 461-491, </pages> <year> 1986. </year>
Reference: [CHH89] <author> Ron Cytron, Michael Hind, and Wilson Hsieh. </author> <title> Automatic generation of DAG parallelism. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 54-68, </pages> <year> 1989. </year>
Reference-contexts: ... = a [i-5]; (a) doall (j=1,50) a [i,j] = ...; doall (j=1,50) - synch with iteration i-5,j ... = a [i-5,j]; - While synchronization for DOACROSS loops requires study of dependences across loop iterations, dependences within a loop iteration or within a general sequence of statements are considered in <ref> [CHH89] </ref>. A sequence of statements can be mapped into a directed acyclic graph of code blocks with edges representing dependences between blocks. Since each block can be executed by a different processor, synchronization must be performed for each edge in the graph. <p> Consequently, the program shown in Figure 2-2 is incorrect since other iterations can be started and finished between the fetch and assignment of sum in a particular iteration. Although alternate models of executing DOALL loops exist in the literature which allow atomicity of iterations or copy-in/copy-out semantics <ref> [CHH89] </ref>, this thesis focuses only on the simpler semantics presented above.
Reference: [CK88] <author> David Callahan and Ken Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <year> 1988. </year>
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year> <note> BIBLIOGRAPHY 167 </note>
Reference-contexts: Solutions exist for these problems whose running times are exponential in the length of the integers but polynomial in the value of the integers. The above subset-sum problem can be solved by a dynamic-programming algorithm that is polynomial in max (b; n; log (max u i )) <ref> [CLR90] </ref>. Furthermore, since the exponential growth in the example involves managing processor offsets, such quantities are limited by the number of processors on a machine.
Reference: [Cro78] <author> W. P. Crowley, et al. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: grid using DOACROSS loops. 19 WaTor Ecological simulation presented by Fox, et al on 32 fi 32 array. [Fox88] 219 Shallow Weather prediction based on finite difference models of the shallow-water equations [Sad75] on 32 fi 20 array. 155 Simple Fluid flow simulation adapted to a 24 fi 24 array. <ref> [Cro78] </ref> 829 991 MICCG3D Preconditioned conjugate gradient using modified incomplete Cholesky factoriza tion on an 8 fi 8 fi 8 array. [YA93] 527 4270 A list of applications used to derive the results in this chapter is shown in the table above.
Reference: [Cyt86] <author> Ron G. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <year> 1986. </year>
Reference-contexts: However, in some cases, one may wish for a loop construct that exhibits the behavior of both types. The DOACROSS loop construct commonly used in the literature <ref> [Cyt86] </ref> satisfies these characteristics. The semantics of DOACROSS loop execution follows that of sequential loops, but loop iterations can be partitioned among many processors. Consequently, data dependences can exist between iterations on different processors.
Reference: [Cyt91] <author> Ron Cytron, et al. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The term single assignment is typically used to represent an execution model where only one assignment is done for each variable during the entire program execution. Similarly, a program is in static single assignment form when each variable is assigned by only one statement <ref> [Cyt91] </ref>. Note that each variable can still be assigned many times dynamically due to the presence of loops. However, each of those assignments is done in the same statement. A program in SSA form has at most one assignment statement for each variable. <p> Each use of a variable is renamed to the name of the definition of the variable that reaches it. This definition is unique since assignments are inserted at every join point where multiple reaching definitions can arise. Algorithms for computing SSA form in general are given in <ref> [Cyt91] </ref> and for structured programs in [RWZ88]. An illustration of the SSA transformation is shown in A definition-use graph can be defined as a directed graph with statements as vertices and edges from definitions of variables to their uses.
Reference: [Dal92] <author> William J. Dally, et al. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference: [DGS93] <author> Evelyn Duesterwald, Rajiv Gupta, and Mary Lou Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 68-77, </pages> <year> 1993. </year>
Reference-contexts: Gross and Steenkiste [GS90] and Granston and Veidenbaum [GV91] rely on the structure of scalar flow analysis, but use array regions as flow elements. Unfortunately, as shown in the above examples, a more effective representation is needed to compute accurate dependence information. Rau [Rau91] and Duesterwald, et al <ref> [DGS93] </ref> use the linear induction variables themselves as indices of flow elements. <p> Such optimizations include parallelism detection, private variable detection, data and loop SECTION 3.6: OTHER APPLICATIONS OF ARRAY FLOW ANALYSIS 61 partitioning, and static data routing. Other optimizations that can benefit from array flow analysis are shown in <ref> [DGS93] </ref>. 3.6.1 Parallelism detection In compiling programs for multiprocessors, a very useful optimization involves the detection of parallelism in sequential DO loops [AK87][Wol89].
Reference: [DH88] <author> Helen Davis and John Hennessy. </author> <title> Characterizing the synchronization behavior of parallel programs. </title> <booktitle> In Proceedings of Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pages 198-211, </pages> <year> 1988. </year>
Reference-contexts: If parallel loop iterations possess fairly dynamic control flow, this can result in unnecessary idling and imply that the time required to execute each loop is equal to the maximum time required by any processor <ref> [DH88] </ref>. With a more decoupled synchronization scheme, consecutive loops can be allowed to stagger, thus providing higher processor utilization.
Reference: [DSB88] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> Febru-ary </month> <year> 1988. </year>
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <editor> In U. Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing-Fourth International Workshop, </booktitle> <pages> pages 65-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [Ell85] <author> John R. Ellis. Bulldog: </author> <title> A compiler for VLIW architectures. </title> <type> Technical Report YALEU/DCS RR # 364, </type> <institution> Yale University Department of Computer Science, </institution> <month> February </month> <year> 1985. </year>
Reference: [Fea88] <author> Paul Featrier. </author> <title> Array expansion. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 429-441, </pages> <year> 1988. </year> <note> 168 BIBLIOGRAPHY </note>
Reference: [Fea91] <author> Paul Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1) </volume> <pages> 23-53, </pages> <year> 1991. </year>
Reference-contexts: In order to delve much further into this question, a formal execution model of parallel loops and processor partitioning must be introduced. 4.5.4 Related work Analysis to compute dependence relationships between instances have been introduced with the goal of privatizing arrays to improve parallelization. Feautrier <ref> [Fea91] </ref> uses a method which computes constraints on the set of source instances to form a bounded polyhedron. Finding the maximum coordinate in the polyhedron can then be viewed as a parametric integer programming problem. <p> One can observe from the above example that the derivations to compute synchronization relationships must be changed to support such array references. In particular, the space of filtered instances is no longer necessarily orthogonal to the loop index axes. One can apply the more general algorithms of Feautrier <ref> [Fea91] </ref> or Maydan [MAL93] to compute the needed results. In adapting these algorithms, however, it is important to remember the desired goal. To perform synchronization, we only need to compute the processors represented by the filtered space and some reasonable estimate of the upper bound of its timestamps.
Reference: [Fel68] <author> William Feller. </author> <title> An Introduction to Probability Theory and Its Applications. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: As n increases, the variance of the fish population decreases, which in turn reduces the penalty for global synchronization. Indeed, for any distribution, the standard deviation of the average of n identical events scales as 1= n <ref> [Fel68] </ref>. One would expect the relative overhead due to unnecessary idling to be related to this ratio. SECTION 6.3: AN EXAMPLE 145 shows the effects of different synchronization schemes on varying problem size.
Reference: [FERN84] <author> Joseph A. Fisher, John R. Ellis, John C. Ruttenberg, and Alexandru Nicolau. </author> <title> Parallel processing: A smart compiler and a dumb machine. </title> <booktitle> In ACM SIG-PLAN Symposium on Compiler Construction, </booktitle> <pages> pages 37-47, </pages> <year> 1984. </year>
Reference: [FOW87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference: [Fox88] <author> Geoffrey C. Fox, et al. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Al though one can execute the compiler on such examples, the resulting code would be no different than if one were to employ a simplistic barrier scheme. Application Description Stmts Inlined Jacobi Jacobi algorithm for solving Laplace's equation on a 48 fi 48 grid. <ref> [Fox88] </ref> 23 Red-black SOR Solution to Laplace's equation using a checkerboard 48 fi 48 grid. [Fox88] 32 Gaussian Gaussian elimination on a 32fi32 matrix. 25 Median Repeated 3 fi 3 median filter on a 24 fi 24 image. [Lim90] 42 Doacross SOR Successive over-relaxation on a 64 fi 64 grid using <p> Application Description Stmts Inlined Jacobi Jacobi algorithm for solving Laplace's equation on a 48 fi 48 grid. <ref> [Fox88] </ref> 23 Red-black SOR Solution to Laplace's equation using a checkerboard 48 fi 48 grid. [Fox88] 32 Gaussian Gaussian elimination on a 32fi32 matrix. 25 Median Repeated 3 fi 3 median filter on a 24 fi 24 image. [Lim90] 42 Doacross SOR Successive over-relaxation on a 64 fi 64 grid using DOACROSS loops. 19 WaTor Ecological simulation presented by Fox, et al on 32 fi 32 <p> Gaussian Gaussian elimination on a 32fi32 matrix. 25 Median Repeated 3 fi 3 median filter on a 24 fi 24 image. [Lim90] 42 Doacross SOR Successive over-relaxation on a 64 fi 64 grid using DOACROSS loops. 19 WaTor Ecological simulation presented by Fox, et al on 32 fi 32 array. <ref> [Fox88] </ref> 219 Shallow Weather prediction based on finite difference models of the shallow-water equations [Sad75] on 32 fi 20 array. 155 Simple Fluid flow simulation adapted to a 24 fi 24 array. [Cro78] 829 991 MICCG3D Preconditioned conjugate gradient using modified incomplete Cholesky factoriza tion on an 8 fi 8 fi <p> Although this example is by no means representative of the benchmarks, it can be used to illustrate some strengths as well as weaknesses of the approach. The WaTor program is adapted from an ecological simulation that appears in <ref> [Fox88] </ref>. Given a population of predators and prey with defined behavior, we wish to simulate the dynamics of the population in time. In this particular example, sharks form the predators and minnows form the prey. Both species inhabit a rectangular lake which is represented by a two-dimensional array.
Reference: [GB92] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, New York, </address> <year> 1979. </year>
Reference-contexts: doall (j=m,4m) ... = a1 [j-u 1 ]; ... doall (j=m,4m) an [j] = ...; doall (j=m,4m) ... = an [j-u n ]; doall (j=2m,3m) ... = c [j-b]; /* S2 */ Fortunately, the above problem is not NP-complete in the strong sense and can be computed in pseudo-polynomial time <ref> [GJ79] </ref>. Solutions exist for these problems whose running times are exponential in the length of the integers but polynomial in the value of the integers. The above subset-sum problem can be solved by a dynamic-programming algorithm that is polynomial in max (b; n; log (max u i )) [CLR90].
Reference: [Gra91] <author> Philippe Granger. </author> <title> Static analysis of linear congruence equalities among variables of a program. </title> <booktitle> In Proceedings of the Internal Joint Conference on Theory and Practice of Software Development, </booktitle> <pages> pages 169-192, </pages> <year> 1991. </year>
Reference: [GS90] <author> Thomas Gross and Peter Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software-Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 135-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The topic of array flow analysis has not been explored until very recently, when it has suddenly become somewhat popular. Initial efforts at array flow analysis focused on the goal of detecting loop-based parallelism. Gross and Steenkiste <ref> [GS90] </ref> and Granston and Veidenbaum [GV91] rely on the structure of scalar flow analysis, but use array regions as flow elements. Unfortunately, as shown in the above examples, a more effective representation is needed to compute accurate dependence information.
Reference: [Gup89] <author> Rajiv Gupta. </author> <title> The fuzzy barrier: a mechanism for high-speed synchronization of processors. </title> <booktitle> In Third International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <year> 1989. </year>
Reference-contexts: Consequently, many efforts have been made to reduce the potentially high expense of this operation [Pol88][AJ87]. However, many of these schemes still rely on global propagation and do not address the problem of processor idling at barrier points. ``Fuzzy'' barriers <ref> [Gup89] </ref> reduce idling by breaking barrier synchronization into two phases: signaling and waiting. In conventional execution, a processor arrives at the barrier point, signals that it has arrived at that point, then waits until all other processors have signaled their arrival. <p> In the above example, collection can be done immediately after the first loop nest, while distribution is not required until the beginning of the second loop nest. This separation forms the exact mechanism touted by the fuzzy barrier schemes <ref> [Gup89] </ref>. In summary, the above discussion shows that synchronization mechanisms other than purely global or local schemes may be useful.
Reference: [Gup90] <author> Rajiv Gupta. </author> <title> A fresh look at optimizing array bound checking. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages BIBLIOGRAPHY 169 272-282, </pages> <year> 1990. </year>
Reference: [GV91] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 854-865, </pages> <year> 1991. </year>
Reference-contexts: The topic of array flow analysis has not been explored until very recently, when it has suddenly become somewhat popular. Initial efforts at array flow analysis focused on the goal of detecting loop-based parallelism. Gross and Steenkiste [GS90] and Granston and Veidenbaum <ref> [GV91] </ref> rely on the structure of scalar flow analysis, but use array regions as flow elements. Unfortunately, as shown in the above examples, a more effective representation is needed to compute accurate dependence information.
Reference: [Har77] <author> William H. Harrison. </author> <title> Compiler analysis of the value ranges for variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 3(3) </volume> <pages> 243-250, </pages> <month> May </month> <year> 1977. </year>
Reference: [HBCM] <author> Michael Hind, Michael Burke, Paul Carini, and Sam Midkiff. </author> <title> Interprocedural array analysis: how much precision do we need? In Proceedings of the Third Workshop on Compilers for parallel computers, </title> <booktitle> volume 2. </booktitle>
Reference: [Hey91] <author> A.J.G. Hey. </author> <title> The GENESIS distributed memory benchmarks. </title> <journal> Parallel Computing, </journal> <volume> 17(10 </volume> & 11):1275-1283, 1991. 
Reference-contexts: First, the parallel machine model used here is one that employs the shared-memory semantics rather than message-passing for interprocessor communication. Rather than being a limitation, this feature actually allows easier porting of sequential code to a parallel machine. However, some available benchmark suites <ref> [Hey91] </ref> that rely on message-passing semantics cannot be used. Second, the derivations of this thesis assume that the input program contains fine-grained data parallelism.
Reference: [HJ91] <author> John L. Hennessy and Norman P. Jouppi. </author> <title> Computer technology and architecture: An evolving interaction. </title> <journal> Computer, </journal> <volume> 24 </volume> <pages> 18-29, </pages> <year> 1991. </year>
Reference: [HKT92] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference: [Jay88] <author> Doddaballapur Narasimha-Murthy Jayasimha. </author> <title> Communication and synchronization in parallel computation. </title> <type> Technical Report CSRD-819, </type> <institution> UIUC Center for Supercomputing Research & Development, </institution> <month> September </month> <year> 1988. </year>
Reference: [Jor78] <author> Harry F. Jordan. </author> <title> A special purpose architecture for finite element analysis. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 263-266, </pages> <year> 1978. </year>
Reference: [Kar72] <author> Richard M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85-103. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference: [Kar76] <author> Michael Karr. </author> <title> Affine relationships among variables of a program. </title> <journal> Acta Informatica, </journal> <volume> 6 </volume> <pages> 133-151, </pages> <year> 1976. </year>
Reference-contexts: In this section, an algorithm is shown for propagation of linear induction variables on scalars using a sparse flow graph representation. Compiler analysis to discover linear relationships among variables was first studied by Karr <ref> [Kar76] </ref>. Linear relationships among variables can also be shown to be derivable in the general framework of abstract interpretation [CH78][CC77].
Reference: [KK91] <author> A. Kallis and D. Klappholz. </author> <title> Extending conventional flow analysis to deal with array references. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, ed 170 BIBLIOGRAPHY itors, </editor> <booktitle> Languages and Compilers for Parallel Computing-Fourth International Workshop, </booktitle> <pages> pages 251-265. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [KLS90] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference: [Kra93] <author> David Kranz, et al. </author> <title> Integrating message-passing and shared-memory: early experience. </title> <booktitle> In ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <year> 1993. </year>
Reference: [KS91] <author> V. P. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in DOACROSS loops with constant dependences. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 281-289, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus when two processors are already synchronized due to other dependences, then a dependence between the two processors is redundant and can be eliminated. Reduction of redundant synchronization has been studied in the context of DOACROSS loops in [MP87] and <ref> [KS91] </ref>. In these works, redundant dependences can be defined as duplicate edges in the transitive closure of the dependence graph. Again, as applied to this thesis, the analysis is required to be more complex due to interactions between data dependences and control flow.
Reference: [Kuc81] <author> D. J. Kuck, et al. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <year> 1981. </year>
Reference: [Kum87] <author> Manoj Kumar. </author> <title> Effect of storage allocation/reclamation methods on parallelism and storage requirements. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 197-205, </pages> <year> 1987. </year>
Reference-contexts: In an imperative language, variables can be renamed or replicated to avoid these dependences in certain circumstances, although at a cost in memory usage <ref> [Kum87] </ref>. An example is presented here to illustrate variable replication as well as removal of redundant dependences. Consider a transformation of the program of Figure 1-3 as illustrated by Figure 1-8.
Reference: [Kun82] <author> H.T. Kung. </author> <title> Why systolic architectures? Computer, </title> <booktitle> 15(1) </booktitle> <pages> 37-46, </pages> <month> January </month> <year> 1982. </year>
Reference: [Lam87] <author> Monica Sin-Ling Lam. </author> <title> A systolic array optimizing compiler. </title> <type> Technical Report CMU-CS-87-187, </type> <institution> Carnegie-Mellon Univ. Computer Science Dept., </institution> <month> May </month> <year> 1987. </year>
Reference: [LAs85] <author> Zhiyuan Li and Walid Abu-sufah. </author> <title> A technique for reducing synchronization overhead in large scale multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 284-291, </pages> <year> 1985. </year>
Reference: [LC91] <author> Jingke Li and Marina Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference: [Lei92] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures. </title> <publisher> Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1992. </year> <note> BIBLIOGRAPHY 171 </note>
Reference: [Len92] <author> D. Lenoski, et al. </author> <title> The Stanford Dash multiprocessor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: SECTION 2.3: MACHINE MODEL 33 2.3 Machine model This thesis assumes a cache-coherent shared-memory interface found on machines such as Alewife [Aga91] and Dash <ref> [Len92] </ref>. Such a multiprocessor can be modeled as a collection of processors P = fp 1 ; : : : p n g and a shared pool of memory units that can be accessed through a network. <p> Synchronization can be accomplished implicitly in such cases since processors are specifically aware of data sharing with other processors. Other shared-memory multiprocessors also contain mechanisms to overcome the inefficiencies of supporting cache-coherent protocols. The Stanford Dash multiprocessor <ref> [Len92] </ref> allows processors to write values directly to caches of other processors.
Reference: [Lim90] <author> Jae S. Lim. </author> <title> Two-Dimensional Signal and Image Processing. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: Jacobi algorithm for solving Laplace's equation on a 48 fi 48 grid. [Fox88] 23 Red-black SOR Solution to Laplace's equation using a checkerboard 48 fi 48 grid. [Fox88] 32 Gaussian Gaussian elimination on a 32fi32 matrix. 25 Median Repeated 3 fi 3 median filter on a 24 fi 24 image. <ref> [Lim90] </ref> 42 Doacross SOR Successive over-relaxation on a 64 fi 64 grid using DOACROSS loops. 19 WaTor Ecological simulation presented by Fox, et al on 32 fi 32 array. [Fox88] 219 Shallow Weather prediction based on finite difference models of the shallow-water equations [Sad75] on 32 fi 20 array. 155 Simple
Reference: [MAL93] <author> Dror E. Maydan, Saman P. Amarasinghe, and Monica S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 2-15, </pages> <year> 1993. </year>
Reference-contexts: The approach used here can be viewed as solving the 80 CHAPTER 4: PROCESSOR DEPENDENCES AND SYNCHRONIZATION problem posed by Feautrier, but for the particular case where all constraint surfaces are orthogonal to axes in the iteration space. Recently, Maydan, et al <ref> [MAL93] </ref> have introduced a new scheme which solves problems that are almost as general as those of Feautrier, but promises to be more efficient. <p> In particular, the space of filtered instances is no longer necessarily orthogonal to the loop index axes. One can apply the more general algorithms of Feautrier [Fea91] or Maydan <ref> [MAL93] </ref> to compute the needed results. In adapting these algorithms, however, it is important to remember the desired goal. To perform synchronization, we only need to compute the processors represented by the filtered space and some reasonable estimate of the upper bound of its timestamps.
Reference: [MCM82] <author> Victoria Markstein, John Cocke, and Peter Markstein. </author> <title> Optimization of range checking. </title> <booktitle> In ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 114-119, </pages> <year> 1982. </year>
Reference: [MCS91] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Synchronization without contention. </title> <booktitle> In Fourth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <year> 1991. </year>
Reference: [MJ81] <author> Steven S. Muchnick and Neil D. Jones. </author> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference: [MP87] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1485-1495, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If accesses that require the barrier cannot be moved very far apart at compilation, then processors still spend a large amount of time idle. A point-to-point synchronization scheme for DOACROSS loops is presented in <ref> [MP87] </ref>. Even though all iterations of a DOACROSS loop can be executed in parallel, dependences can exist between iterations. In Figure 1-1a, the definition and use of elements of array a in different iterations imply that synchronization must be performed between those iterations. <p> Consequently, the analysis done in this thesis must deal with all the issues that arise in synchronization within DOACROSS loops. In addition, synchronization across DOALL loops requires consideration of dependences between separate loops, which is not considered in <ref> [MP87] </ref>. doacross (i=1,100) - a [i] = ...; synch with iteration i-5 ... = a [i-5]; (a) doall (j=1,50) a [i,j] = ...; doall (j=1,50) - synch with iteration i-5,j ... = a [i-5,j]; - While synchronization for DOACROSS loops requires study of dependences across loop iterations, dependences within a loop <p> The realization of this goal involves careful study of the topics outlined below. SECTION 1.3: APPROACH 21 Barrier synchronization Point-to-point synchronization 1.3.1 Synchronization variables Point-to-point synchronization can be implemented by the use of a shared variable which indicates the current loop iteration of each processor as in <ref> [MP87] </ref>. <p> Thus when two processors are already synchronized due to other dependences, then a dependence between the two processors is redundant and can be eliminated. Reduction of redundant synchronization has been studied in the context of DOACROSS loops in <ref> [MP87] </ref> and [KS91]. In these works, redundant dependences can be defined as duplicate edges in the transitive closure of the dependence graph. Again, as applied to this thesis, the analysis is required to be more complex due to interactions between data dependences and control flow. <p> The semantics of DOACROSS loop execution follows that of sequential loops, but loop iterations can be partitioned among many processors. Consequently, data dependences can exist between iterations on different processors. Even though synchronization for DOACROSS loops has been studied by <ref> [MP87] </ref>, a discussion is presented here to show how a synchronization scheme for such a loop fits into the current general framework that allows for arbitrary loop usage. Semantically, iterations of a DOACROSS are executed in sequential order. <p> Proof: The proof is based on reduction from the subset-sum problem: Given a set integers U = fu 1 ; : : : ; u n g and an integer b, the question of whether a subset U 0 U exists y Midkiff and Padua <ref> [MP87] </ref> mention that finding redundant dependences in DOACROSS loops is NP hard. Their proof is probably similar to the one given here. SECTION 5.3: REDUNDANT DEPENDENCES 119 such that P u2U 0 u = b is NP-hard [GJ79][Kar72].
Reference: [MP91] <author> Sam P. Midkiff and David A. Padua. </author> <title> A comparison of four synchronization optimization techniques. </title> <type> Technical Report CSRD-1135, </type> <institution> UIUC Center for Supercomputing Research & Development, </institution> <month> June </month> <year> 1991. </year>
Reference: [OD90] <author> Matthew T. O'Keefe and Henry G. Dietz. </author> <title> Hardware barrier synchronization: static barrier MIMD (SBM). </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> Volume I, </volume> <pages> pages 35-42, </pages> <year> 1990. </year>
Reference: [PKL80] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 29(9) </volume> <pages> 763-776, </pages> <month> September </month> <year> 1980. </year>
Reference: [Pol88] <author> Constantine D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year> <note> 172 BIBLIOGRAPHY </note>
Reference: [Pol89] <author> Constantine D. Polychronopolous. </author> <title> Compiler optimizations for enhancing parallelism and their impact on architecture design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 991-1004, </pages> <month> August </month> <year> 1989. </year>
Reference: [PW86] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Furthermore, the value of the array index k is always equal to 2i+5. However, it is not clear how this information can be deduced in a general manner. Fortunately, existing value propagation algorithms can be adapted to propagate linear loop induction variables. In the literature <ref> [PW86] </ref>, this optimization is known as forward substitution . 36 CHAPTER 3: STATEMENT DEPENDENCES for (i=1,100) - if (a [i]&gt;0) else k=j+4; - 3.2.1 Value lattices For each lexical expression in a program, let the value set of the expression be the set of values that it can take on during
Reference: [Rau91] <author> B. R. Rau. </author> <title> Data flow and dependence analysis for instruction level parallelism. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing-Fourth International Workshop, </booktitle> <pages> pages 236-250. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Gross and Steenkiste [GS90] and Granston and Veidenbaum [GV91] rely on the structure of scalar flow analysis, but use array regions as flow elements. Unfortunately, as shown in the above examples, a more effective representation is needed to compute accurate dependence information. Rau <ref> [Rau91] </ref> and Duesterwald, et al [DGS93] use the linear induction variables themselves as indices of flow elements.
Reference: [RL86] <author> John H. Reif and Harry R. Lewis. </author> <title> Efficient symbolic analysis of programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 32 </volume> <pages> 280-314, </pages> <year> 1986. </year>
Reference-contexts: in a real system by incorporating relevant SECTION 3.2: PROPAGATION OF LINEAR INDUCTION VARIABLES 39 techniques in the literature. 3.2.2 Propagation of linear induction variables The algorithm presented here for the propagation of linear induction variables is based on previous work on symbolic value propagation done by Reif and Lewis <ref> [RL86] </ref>. Wegman and Zadeck [WZ91] show that in the context of constant propagation, derived constant information can aid in the flow analysis as well. In this section, an algorithm is shown for propagation of linear induction variables on scalars using a sparse flow graph representation. <p> = b [i*2]; Original program doall (i1 = 1,99,2) - j1 = i1 * 2; j2 = j1 + 1; b [j3] = b [j3] / 3; doall (i2 = 2,100,2) c [i2] = b [i2*2]; Transformed program E is the number of edges in the program control flow graph <ref> [RL86] </ref>. The def-use graph of a program in SSA form can thus be viewed as a sparse representation of the general definition-use graph. 3.2.2.2 Propagation algorithm After a program has been transformed into SSA form, linear induction variables can be propagated over the definition-use graph. <p> Overall, the number of times that an edge can be placed in the work-list corresponds to the number of def-use edges in the SSA graph times H. From <ref> [RL86] </ref>, there are at most E def-use edges for each variable in an SSA graph where E is the number of edges in the control flow graph.
Reference: [RS91] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference: [RWZ88] <author> Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <year> 1988. </year>
Reference-contexts: This definition is unique since assignments are inserted at every join point where multiple reaching definitions can arise. Algorithms for computing SSA form in general are given in [Cyt91] and for structured programs in <ref> [RWZ88] </ref>. An illustration of the SSA transformation is shown in A definition-use graph can be defined as a directed graph with statements as vertices and edges from definitions of variables to their uses.
Reference: [SA91] <author> Shridhar B. Shukla and Dharma P. Agrawal. </author> <title> Scheduling pipelined communication in distributed memory multiprocessors for real-time applications. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 222-231, </pages> <year> 1991. </year>
Reference-contexts: Architectures such as iWarp [Bor90] or NuMesh [War93] seek to alleviate contention costs by introducing the idea of static routing. When destinations of messages are known at compilation, then routing can be scheduled statically to avoid unnecessary contentions <ref> [SA91] </ref>. Furthermore, hardware which supports static routing can avoid the latency associated with examining headers as in dynamic routing. In loop-based parallel programs, communication between processors arises primarily from flow dependences between different processors.
Reference: [Sad75] <author> R. Sadourny. </author> <title> The dynamics of finite-difference models of the shallow-water equations. </title> <journal> Journal of Atmospheric Science, </journal> <volume> 32(4), </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: filter on a 24 fi 24 image. [Lim90] 42 Doacross SOR Successive over-relaxation on a 64 fi 64 grid using DOACROSS loops. 19 WaTor Ecological simulation presented by Fox, et al on 32 fi 32 array. [Fox88] 219 Shallow Weather prediction based on finite difference models of the shallow-water equations <ref> [Sad75] </ref> on 32 fi 20 array. 155 Simple Fluid flow simulation adapted to a 24 fi 24 array. [Cro78] 829 991 MICCG3D Preconditioned conjugate gradient using modified incomplete Cholesky factoriza tion on an 8 fi 8 fi 8 array. [YA93] 527 4270 A list of applications used to derive the results
Reference: [Sal90] <author> Joel Saltz, et al. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference: [Sar87] <author> Vivek Sarkar. </author> <title> Partitioning and scheduling parallel programs for execution on multiprocessors. </title> <type> Technical Report STANFORD CSL-TR-87-328, </type> <institution> Stanford BIBLIOGRAPHY 173 Univ. Computer Systems Lab., </institution> <month> April </month> <year> 1987. </year>
Reference: [Sch86] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley, </publisher> <address> Chich-ester, Great Britain, </address> <year> 1986. </year>
Reference-contexts: Instead, the problem can be viewed as one of general integer linear programming which can also be solved in pseudo-polynomial time by dynamic programming <ref> [Sch86] </ref>. However, rather than computing whether each individual dependence is redundant, we seek to consolidate the intermediate dependence computations through an algorithm which computes redundancy information for all dependences at the same time. <p> From <ref> [Sch86] </ref>, each component of ~x is bound by ms where s is the maximum absolute value of d i and d . Consequently, one only needs to traverse m 2 s back edges to find all redundant dependences even with very large sequential loop bounds.
Reference: [Sei85] <author> Charles L. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 22-33, </pages> <month> January </month> <year> 1985. </year>
Reference: [SLY89] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An empirical study on array subscripts and data dependencies. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> Volume II, </volume> <pages> pages 145-152, </pages> <year> 1989. </year>
Reference-contexts: However, many loop-based programs contain array references that are generally linear functions of loop indices, thus providing statically-obtainable dependence information between individual elements <ref> [SLY89] </ref>.
Reference: [Smi78] <author> Burton J. Smith. </author> <title> A pipelined, shared resource MIMD computer. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year>
Reference: [SWG91] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stan-ford parallel applications for shared-memory. </title> <type> Technical Report STANFORD CSL-TR-91-469, </type> <institution> Stanford Univ. Computer Systems Lab., </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In other words, 138 CHAPTER 6: RESULTS although each program is meant to be executed on a multiprocessor, its top-level orga nization is sequential, with parallelism occurring at lower levels. This assumption elim inates applications with high-level coarse-grain parallelism such as those in the Splash benchmarks <ref> [SWG91] </ref>. Finally, since the analysis only performs optimization on array indices that are linear functions of loop indices, applications are chosen whose array accesses predominantly fit such characteristics.
Reference: [SY89] <author> Hong-Men Su and Pen-Chung Yew. </author> <title> On data synchronization for multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 416-423, </pages> <year> 1989. </year>
Reference: [Thi91] <author> Thinking Machines Corporation. </author> <title> The connection machine CM-5 technical summary. </title> <type> Technical report, </type> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference: [War93] <author> Steve Ward, et al. </author> <title> The NuMesh: A scalable, modular, 3D interconnect. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: In the conventional scheme of dynamic routing, a message is routed by examining its header which identifies the destination processor for the message. In situations where two messages need to access the same resource, one message must be either blocked or buffered. Architectures such as iWarp [Bor90] or NuMesh <ref> [War93] </ref> seek to alleviate contention costs by introducing the idea of static routing. When destinations of messages are known at compilation, then routing can be scheduled statically to avoid unnecessary contentions [SA91]. Furthermore, hardware which supports static routing can avoid the latency associated with examining headers as in dynamic routing.
Reference: [WF91] <author> Min-You Wu and Geoffrey Fox. </author> <title> Compiling Fortran90 programs for distributed memory MIMD parallel computers. </title> <type> Technical Report CRPC-TR91126, </type> <note> Center for Research on Parallel Computation, </note> <month> January </month> <year> 1991. </year>
Reference: [WM91] <author> Ko-Yang Wang and Piyush Mehrotra. </author> <title> Optimizing data synchronizations on distributed memory architectures. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> Volume II, </volume> <pages> pages 76-82, </pages> <year> 1991. </year>
Reference: [Wol89] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <note> 174 BIBLIOGRAPHY </note>
Reference-contexts: In the terminology of [AK87], this field is equivalent to specifying that the dependence is loop-independent rather than loop-carried with respect to an outer loop. The application of this external field also corresponds to the different cases of dependence checking for different data direction vectors of [BC86] and <ref> [Wol89] </ref>. <p> The second involves linearization of the array reference by using known array bounds to map the multiple-dimensional index space into a one-dimensional space. Unfortunately, each approach has cases in which the other approach produces a more accurate answer <ref> [Wol89] </ref>. For the best solution, both approaches can be used to test each dependence.
Reference: [Wol92] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <year> 1992. </year>
Reference-contexts: Note that functions of only one loop index are present in the linear induction variable lattice. Linear functions of multiple loop indices are not supported. Also, detection of induction variables that are not defined directly from loop indices <ref> [Wol92] </ref> is not done. These topics are viewed as somewhat orthogonal to the approach outlined here.
Reference: [WZ91] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Wegman and Zadeck <ref> [WZ91] </ref> show that in the context of constant propagation, derived constant information can aid in the flow analysis as well. In this section, an algorithm is shown for propagation of linear induction variables on scalars using a sparse flow graph representation. <p> Hence, the worst-case running time for the propagation algorithm is O (H fi N fi E) where N is the number of variables in a program. From <ref> [WZ91] </ref>, empirical evidence suggests that constant propagation runs in time linear to program size. The typical running time of linear induction variable propagation is expected to also be linear but with an additional multiplicative factor of H.
Reference: [YA93] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with fine-grain synchronization on MIMD machines for preconditioned conjugate gradient. </title> <booktitle> In ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 187-197, </pages> <year> 1993. </year>
Reference-contexts: Weather prediction based on finite difference models of the shallow-water equations [Sad75] on 32 fi 20 array. 155 Simple Fluid flow simulation adapted to a 24 fi 24 array. [Cro78] 829 991 MICCG3D Preconditioned conjugate gradient using modified incomplete Cholesky factoriza tion on an 8 fi 8 fi 8 array. <ref> [YA93] </ref> 527 4270 A list of applications used to derive the results in this chapter is shown in the table above.
Reference: [YTL87] <author> P. C. Yew, N. F. Tzeng, and D. H. Lawrie. </author> <title> Distributing hot-spot addressing in large scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(4), </volume> <month> April </month> <year> 1987. </year>

References-found: 98

