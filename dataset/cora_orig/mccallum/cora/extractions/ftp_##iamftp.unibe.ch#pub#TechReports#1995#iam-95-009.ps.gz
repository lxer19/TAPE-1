URL: ftp://iamftp.unibe.ch/pub/TechReports/1995/iam-95-009.ps.gz
Refering-URL: 
Root-URL: 
Title: An Optimum Decision Rule for Pattern Recognition  
Author: Thien M. HA 
Keyword: CR Categories and Subject Descriptors: I.5.0 [Pattern Recognition]: General; I.5.1 [Pattern Recognition]: Models; I.5.2 [Pattern Recognition]: Design Methodol ogy; I.5.m [Pattern Recognition]: Decision. Key Words: classification, decision rule, Bayes rule, selective rejection, man machine interface.  
Address: Neubruckstr. 10, CH-3012 Berne, Switzerland  
Affiliation: University of Berne Institut fur Informatik und Angewandte Mathematik  
Email: E-Mail: haminh@iam.unibe.ch  
Phone: Phone: +41 31 631 86 99 Fax.: +41 31 631 39 65  
Date: November 30, 1995  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: through the Bayes formula: P i (x) = P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, and 1 p (x) = j=1 is the absolute probability density function <ref> [1, 2] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) Based on the posterior probabilities, the Bayes decision rule assigns to sample x the class that has the highest posterior probability. <p> This is obvious for n fl = 1. For n fl &gt; 1, suppose that Q n fl (x) t, then 9k (= n fl 1) such that Q k+1 (=n fl ) t, k (= n fl 1) 1 ) k 2 <ref> [1; ::; N ] </ref>, and k (= n fl 1) &lt; n fl , which means that the optimum decision rule given by Eq. (12) had not been used (n fl is not the minimum value possible). * Case b: Q 1 (x) t: We have Q n fl (x) t <p> Finally, let us consider the range of t 2 [0; 1 2 ]. Since the decision rule involves the comparison between t and posterior probabilities, it makes sense only for t 2 <ref> [0; 1] </ref>. On the other hand, when t 1 2 , it can be easily seen that our rule is identical to the Bayes rule, i.e., choose the single best class.
Reference: [2] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, second edition, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: through the Bayes formula: P i (x) = P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, and 1 p (x) = j=1 is the absolute probability density function <ref> [1, 2] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) Based on the posterior probabilities, the Bayes decision rule assigns to sample x the class that has the highest posterior probability.
Reference: [3] <author> C.K. Chow, </author> <title> "An Optimum Character Recognition System Using Decision Functions," </title> <journal> Institute of Radio Engineers (IRE) Transactions on Electronic Computers, </journal> <volume> Vol. EC-6, No. 4, </volume> <pages> pp. 247-254, </pages> <month> December </month> <year> 1957. </year>
Reference-contexts: Fig. 2a illustrates the partition of the pattern space X into three regions, each of which corresponds to a single class, when the Bayes rule is used. The Bayes rule has also been modified by Chow to cope with a reject option <ref> [3, 4] </ref>. The reject option is desirable in those applications where it is more costly to make a wrong decision than to withhold making a decision. In such situations, the optimality espouses another meaning, that of a tradeoff between the error rate and the reject rate (reject probability).
Reference: [4] <author> C.K. Chow, </author> <title> "On Optimum Recognition Error and Reject Tradeoff," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-16, No. 1, </volume> <pages> pp. 41-46, </pages> <month> January </month> <year> 1970. </year>
Reference-contexts: Fig. 2a illustrates the partition of the pattern space X into three regions, each of which corresponds to a single class, when the Bayes rule is used. The Bayes rule has also been modified by Chow to cope with a reject option <ref> [3, 4] </ref>. The reject option is desirable in those applications where it is more costly to make a wrong decision than to withhold making a decision. In such situations, the optimality espouses another meaning, that of a tradeoff between the error rate and the reject rate (reject probability).
Reference: [5] <author> M.E. Hellman, </author> <title> "The Nearest Neighbor Classification Rule with a Reject Option," </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, </journal> <volume> Vol. SSC-6, No. 3, </volume> <pages> pp. 179-185, </pages> <month> July </month> <year> 1970. </year> <month> 18 </month>
Reference-contexts: An adaptation of Chow's results to the k-nearest neighbour rule was achieved by Hell-man leading to the (k,k')-nearest neighbor rule <ref> [5] </ref>. Interestingly, the outcomes of Chow's rule are also singletons, like in the Bayes rule, but augmented by the empty subset f;g, which represents the reject option; see Figs. 1 and 2b.
References-found: 5

