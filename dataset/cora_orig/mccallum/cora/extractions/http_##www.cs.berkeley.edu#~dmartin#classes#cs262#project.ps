URL: http://www.cs.berkeley.edu/~dmartin/classes/cs262/project.ps
Refering-URL: http://www.cs.berkeley.edu/~dmartin/
Root-URL: 
Title: A Scalable Non-Blocking Concurrent Hash Table Implementation with Incremental Rehashing  
Author: David R. Martin Richard C. Davis 
Date: December 15, 1997  
Abstract: Highly concurrent systems often require high-throughput shared data structures to achieve high performance. Traditional lock-based implementations suffer from the problems of convoying, priority inversion, and deadlock. Furthermore, locks introduce overhead in the common case. Non-blocking concurrent data structures have been proposed to solve all of these problems, as they allow concurrent atomic updates without using locks. There exist general methods to transform any lock-based data structure into a lock-free, non-blocking version. However, these methods invariably yield low performance implementations. Common practice is to design a custom lock-free version of the data structure. We present a fully non-blocking concurrent hash table implementation with incremental rehashing. Our fault model allows a thread to suspend or die at any point in our code without compromising the consistency of the table. Furthermore, our design is portable and is shown to achieve high throughput on workloads with high contention. Two implementation notes are of interest. First, we use open addressing rather than the traditional bucket-and-chain data structure. This design choice was critical in enabling a relatively simple design and proving it correct. Second, our strong fault model requires garbage collection. Even a weaker fault model would benefit greatly from garbage collection, as it greatly simplifies analysis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: The requirement that the table be scalable (to a large number of threads) precludes serializing the rehashing code. Since the number of rehashes is likely to increase with the number of client threads, if the rehashing is not parallelized across threads, Amdahl's Law <ref> [1] </ref> dictates that rehashing time would limit speedup quite severely. It follows that many threads must cooperatively rehash the table. Our implementation effectively parallelizes rehashing across all available threads, and even performs rehashing incrementally (i.e. concurrently) with table operations.
Reference: [2] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: The benefits of lock-free synchronization are clear, but they do come at a cost. Lock-free concurrent programming is not as well understood as lock-based concurrent programming. Programming languages such as Modula-3 [5] and Java <ref> [2] </ref> have made lock-based programming quite painless as far as correctness is concerned. Consequently, the technique of lock-free synchronization is relevant only when complexity can be traded for performance. Highly-concurrent systems such as databases and web servers may we willing to make this trade-off.
Reference: [3] <author> Brian N. Bershad, David D. Redell, and John R. Ellis. </author> <title> Fast mutual exclusion for uniprocessors. </title> <booktitle> Fifth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: A building block for atomicity is required. Assuming an atomic memory primitive such as compare-and-swap (as provided by the x86 and Sparc v9 [14] architectures) or load-linked/store-conditional (as provided by the MIPS [10] and Alpha [12] architectures), one can build restartable atomic sequences <ref> [3] </ref>. For the purposes of building lock-free structures, any update must be structured as a restartable atomic sequence of code such that the update either completes atomically or can be safely aborted at any instruction in the sequence.
Reference: [4] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: When searching for (or inserting) a key, a probe sequence is generated for the key that is a permutation of all slots in the array. The slots are visited in this order until either they key is found or until an empty slot is found. As described in <ref> [4] </ref>, open-addressed hash tables have several problems. First, the table has a finite capacity, unlike the bucket-and-chain approach. Second, a slot left by a deleted key cannot be entirely vacated, since subsequent searches must know to skip vacated slots.
Reference: [5] <author> Samuel P. Harbison. </author> <title> MODULA-3. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The benefits of lock-free synchronization are clear, but they do come at a cost. Lock-free concurrent programming is not as well understood as lock-based concurrent programming. Programming languages such as Modula-3 <ref> [5] </ref> and Java [2] have made lock-based programming quite painless as far as correctness is concerned. Consequently, the technique of lock-free synchronization is relevant only when complexity can be traded for performance. Highly-concurrent systems such as databases and web servers may we willing to make this trade-off.
Reference: [6] <author> M. P. Herlihy and J. M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 463-492, </pages> <year> 1990. </year>
Reference-contexts: This leaves room for a version, so that both a pointer and an associated version can be updated atomically. With a 32-bit pointer and a 32-bit version, this is a satisfactory solution to the ABA problem [13]. An important invariant to maintain is one of linearizability <ref> [6] </ref>. A concurrent object is linearizable if it appears to all threads as if the operations of all threads were executed in some linear sequence. This definition is nearly identical to the definition of sequential consistency in a multiprocessor system.
Reference: [7] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 745-770, </pages> <year> 1993. </year>
Reference-contexts: Non-blocking implementations are often sufficient; if supplemented with exponential back-off, they can be made as robust as a wait-free implementation. 2 2 Background There are well-known methods for transforming any lock-based data structure implementation into a wait-free implementation <ref> [7] </ref>. These methods invariably yield low performance implementations, since they are oblivious to application-specific concurrency. Without knowledge about how a data structure is used by an application, an automatic method must be conservative with respect to what sort of concurrency is possible by the application.
Reference: [8] <author> Maurice Herlihy and J. Eliot B. Moss. </author> <title> Transactional memory: Architectural support for lock-free data structures. </title> <booktitle> Proceedings of the 1993 International Symposium in Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Fine-grained locking trades space for supporting more concurrency. Even if this trade-off is acceptable, the overhead of acquiring and releasing the lock is always present and could be significant. 1 Apart from overhead, there are at least three other significant problems associated with lock-based synchronization <ref> [8] </ref>: 1. Convoying is the queuing of threads behind a lock that is currently held by another thread. If convoys form, then they can persist after the lock has been passed. This creates hot-spots and reduces concurrency. 2.
Reference: [9] <author> Amos Israeli and Lihu Rappoport. </author> <title> Efficient wait-free implementation of a concurrent priority queue. </title> <booktitle> In Lecture Notes in Computer Science 725, </booktitle> <pages> pages 1-17. </pages> <publisher> Springer Verlag, </publisher> <month> September </month> <year> 1993. </year>
Reference-contexts: The ideal system for lock-free design would include a lock-free concurrent allocator and garbage collection. The literature contains several implementations of simple lock-free concurrent data structures such as stacks and queues [11]. More interesting data structures such as lists [13], binary trees [13], and priority queues <ref> [9] </ref> can be found. It is not clear, however, if the designs for these more complex data structures are feasible, as experimental results are either not emphasized or not present.
Reference: [10] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: A design not proven correct has very little chance of being correct. A building block for atomicity is required. Assuming an atomic memory primitive such as compare-and-swap (as provided by the x86 and Sparc v9 [14] architectures) or load-linked/store-conditional (as provided by the MIPS <ref> [10] </ref> and Alpha [12] architectures), one can build restartable atomic sequences [3]. For the purposes of building lock-free structures, any update must be structured as a restartable atomic sequence of code such that the update either completes atomically or can be safely aborted at any instruction in the sequence.
Reference: [11] <author> Maged M. Michael and Michael L. Scott. </author> <title> Simple, fast, and practial non-blocking and blocking concurrent queue algorithms. </title> <booktitle> Proceedings of the 15th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 267-276, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: It is a direct consequence of our fault model that a language with garbage collection is a requirement. The ideal system for lock-free design would include a lock-free concurrent allocator and garbage collection. The literature contains several implementations of simple lock-free concurrent data structures such as stacks and queues <ref> [11] </ref>. More interesting data structures such as lists [13], binary trees [13], and priority queues [9] can be found. It is not clear, however, if the designs for these more complex data structures are feasible, as experimental results are either not emphasized or not present.
Reference: [12] <author> Richard L. </author> <title> Sites. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: A design not proven correct has very little chance of being correct. A building block for atomicity is required. Assuming an atomic memory primitive such as compare-and-swap (as provided by the x86 and Sparc v9 [14] architectures) or load-linked/store-conditional (as provided by the MIPS [10] and Alpha <ref> [12] </ref> architectures), one can build restartable atomic sequences [3]. For the purposes of building lock-free structures, any update must be structured as a restartable atomic sequence of code such that the update either completes atomically or can be safely aborted at any instruction in the sequence.
Reference: [13] <author> John D. Valois. </author> <title> Lock-free linked lists using compare-and-swap. </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 214-222, </pages> <year> 1995. </year>
Reference-contexts: Highly-concurrent systems such as databases and web servers may we willing to make this trade-off. There are two classes of lock-free data structures. A lock-free implementation can be either non-blocking or wait-free. According to <ref> [13] </ref>, * A data structure is non-blocking if some thread will always complete its operation in finite time, regardless of the actions of other threads. * A data structure is wait-free if every non-faulty thread is guaranteed to complete its operation in finite time. <p> This leaves room for a version, so that both a pointer and an associated version can be updated atomically. With a 32-bit pointer and a 32-bit version, this is a satisfactory solution to the ABA problem <ref> [13] </ref>. An important invariant to maintain is one of linearizability [6]. A concurrent object is linearizable if it appears to all threads as if the operations of all threads were executed in some linear sequence. This definition is nearly identical to the definition of sequential consistency in a multiprocessor system. <p> A weaker fault 3 model that prohibits threads from evaporating enables reference counting as a solution. Effectively, the data structure performs its own lock-free allocation and reference counting garbage collection. This solution is attempted in <ref> [13] </ref> in a lock-free linked-list algorithm. A stronger fault model that allows thread evaporation precludes reference counting as a viable solution, since reference counts could not be maintained consistently. It is a direct consequence of our fault model that a language with garbage collection is a requirement. <p> The ideal system for lock-free design would include a lock-free concurrent allocator and garbage collection. The literature contains several implementations of simple lock-free concurrent data structures such as stacks and queues [11]. More interesting data structures such as lists <ref> [13] </ref>, binary trees [13], and priority queues [9] can be found. It is not clear, however, if the designs for these more complex data structures are feasible, as experimental results are either not emphasized or not present. <p> The ideal system for lock-free design would include a lock-free concurrent allocator and garbage collection. The literature contains several implementations of simple lock-free concurrent data structures such as stacks and queues [11]. More interesting data structures such as lists <ref> [13] </ref>, binary trees [13], and priority queues [9] can be found. It is not clear, however, if the designs for these more complex data structures are feasible, as experimental results are either not emphasized or not present.
Reference: [14] <author> David L. Weaver and Tom Germond. </author> <title> The SPARC Architecture Manual. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year> <month> 23 </month>
Reference-contexts: A design not proven correct has very little chance of being correct. A building block for atomicity is required. Assuming an atomic memory primitive such as compare-and-swap (as provided by the x86 and Sparc v9 <ref> [14] </ref> architectures) or load-linked/store-conditional (as provided by the MIPS [10] and Alpha [12] architectures), one can build restartable atomic sequences [3].
References-found: 14

