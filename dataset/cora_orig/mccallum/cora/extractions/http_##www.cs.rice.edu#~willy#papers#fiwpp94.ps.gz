URL: http://www.cs.rice.edu/~willy/papers/fiwpp94.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Title: An Integrated Approach to Distributed Shared Memory  
Author: Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Willy Zwaenepoel 
Address: Houston, TX, U.S.A.  
Affiliation: Department of Computer Science Rice University,  
Abstract: We are developing an integrated user-compiler-runtime approach to programming distributed memory machines. Our approach is to provide a shared-memory programming paradigm through a global address space since shared memory is easier to program than message passing. We integrate the ability for user input through the use of high-level synchronization operations, runtime efficiency through the use of lazy release consistency and multiple writer protocols, and compiler support to assist in prefetching data and removing consistency overhead by switching to message passing directly when the communication patterns are predictable. Our optimizations allows us to take advantage of the benefits of bulk data transfer afforded by the underlying message-passing system and of combining data communication with synchronization. Our base system, which does not include any message-passing support or compiler analysis, achieves good speedups on 8 processors for several applications, including Successive Over-Relaxation (7.4), the Traveling Salesman Problem (7.2), and Genetic Linkage Analysis (5.9). We expect performance and/or usability to improve significantly with the specified optimizations for a wider range of applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.V. Adve, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Replacing locks by higher-level primitives. </title> <type> Technical Report TR94-237, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: We therefore identify a small number of high-level operations often used by programmers, and provide support for these. Examples of these include the above-mentioned task queues and accumulation operations (see <ref> [1] </ref> for further details). Access to shared data in a shared memory program is mediated through synchronization operations. Locks, barriers, and condition variables are the most common primitives provided to the user. One example of the use of locks is to implement work queues. <p> Therefore, this program is not directly amenable to automatic parallelization. The use of a single global address space and shared memory greatly improved the ease of parallelization of this program. Using accumulation operations further improves efficiency and ease of use <ref> [1] </ref>. TSP is a program that uses a work queue approach to generating and performing its tasks. The program has a shared, global queue of partial tours. Each process gets a partial tour from the queue, extends the tour, and returns the results back to the queue. <p> The simpler approach of updating the contents of the record directly can be used without loss in performance by using the high-level accumulate primitive <ref> [1] </ref>. Using compiler-analyzed prefetch-ing would also benefit this application by replacing multiple miss messages with a single update. Molecular dynamics programs such as Water can be automatically parallelized using runtime support such as CHAOS [4].
Reference: [2] <author> S. Ahuja, N. Carreiro, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Section 6 describes related work. Section 7 summarizes our conclusions. 2 Single Global Address Space Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces <ref> [2] </ref>, and message passing [19]. TreadMarks is a distributed shared memory (DSM) system [17]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 2).
Reference: [3] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN `89 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: This results in a reduction in the number of miss messages as well as provides the ability to overlap the communication and computation requirements of an application. Mechanisms such as the data access descriptor <ref> [3] </ref> provide compact representations of data access and traversal information, and will be used to efficiently determine the required access information. However, irregular access patterns can result in complex runtime analysis of the pages required to be prefetched.
Reference: [4] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI run-time primitives. </title> <type> Technical Report 13, </type> <institution> ICASE, Hampton, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Using compiler-analyzed prefetch-ing would also benefit this application by replacing multiple miss messages with a single update. Molecular dynamics programs such as Water can be automatically parallelized using runtime support such as CHAOS <ref> [4] </ref>. However, depending on the dynamic nature of the irregular data access pattern, there is a tradeoff between the use of the default runtime virtual memory support and compiler-based approaches. These examples show that no single approach to parallelization is suitable for all programs.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Performance is improved through an integrated user-compiler-runtime approach, as shown in Figure 1. Specifically, we integrate the ability for user input through the use of high-level synchronization operations, run-time efficiency through the use of lazy release consistency [10] and multiple writer protocols <ref> [5] </ref>, and compiler support to assist in prefetching data and removing consistency overhead by switching to message passing directly when the communication patterns are predictable. The user programs use a global address space and a shared memory interface. <p> Tread-Marks uses a lazy implementation of release consistency to allow aggregation of data communication and to limit the communication of memory consistency information to synchronizing processes. Our implementation also limits the effects of false sharing by allowing multiple writers to a page <ref> [5] </ref>. Our experimental platform is a network of 8 DECStation-5000/240s running Ultrix and using a 100Mbps ATM network. We achieved good speedups on 8 processors for several applications including Successive Over-Relaxation (SOR - 7.4), the Traveling Salesman Problem (TSP - 7.2), and Genetic Linkage Analysis (ILINK - 5.9) [11].
Reference: [6] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: One solution to this is to keep track of all data requested by any processor, and based on the knowledge of what the processor has accessed in the past, update the relevant pages. This hybrid protocol <ref> [6] </ref> is an attempt to achieve most of the advantages of both a pure update and a pure invalidate protocol. Updates are sent only for the modifications present at the releaser, and in-validations for the remaining modifications.
Reference: [7] <author> S. Dwarkadas, A.A. Schaffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: No kernel modifications are necessary. ILINK, SOR, TSP, and Water. ILINK, from the LINKAGE package [16], is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. We present results for the CLP input data set (see <ref> [7] </ref> for more details). Red-Black Successive Over-Relaxation (SOR) is a method for solving partial differential equations. We ran SOR for 100 iterations on a 2000 fi 1000 matrix. TSP solves the traveling salesman problem using a branch-and-bound algorithm. We used a 19-city problem as input.
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The disadvantage of this model is that it can result in an excessively large amount of communication since modifications to shared data must be sent out immediately in the form of invalidates or updates. The model we provide to the user is release consistency (RC) <ref> [8] </ref> and TreadMarks uses a lazy release consistent (LRC) implementation [10]. Further details on TreadMarks may be found in [11]. RC is a relaxed memory consistency model that addresses the above-mentioned efficiency problems of sequential consistency.
Reference: [9] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Different programs benefit from different optimizations and dif ferent parallelization strategies. With our integrated approach, we will combine the advantages of several of these strategies in a single framework. 6 Related Work Implicit parallelism, as in HPF <ref> [9] </ref>, relies on user-provided data distributions which are then used by the compiler to generate message passing code. This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework.
Reference: [10] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Performance is improved through an integrated user-compiler-runtime approach, as shown in Figure 1. Specifically, we integrate the ability for user input through the use of high-level synchronization operations, run-time efficiency through the use of lazy release consistency <ref> [10] </ref> and multiple writer protocols [5], and compiler support to assist in prefetching data and removing consistency overhead by switching to message passing directly when the communication patterns are predictable. The user programs use a global address space and a shared memory interface. <p> The model we provide to the user is release consistency (RC) [8] and TreadMarks uses a lazy release consistent (LRC) implementation <ref> [10] </ref>. Further details on TreadMarks may be found in [11]. RC is a relaxed memory consistency model that addresses the above-mentioned efficiency problems of sequential consistency.
Reference: [11] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: These techniques reduce latency by cutting down on the number of misses incurred. The fundamental principle behind the efficiency of our runtime environment is the use of a relaxed consistency model. We have a working runtime system in TreadMarks <ref> [11] </ref>, a distributed shared memory system that runs on top of a network of workstations. Tread-Marks uses a lazy implementation of release consistency to allow aggregation of data communication and to limit the communication of memory consistency information to synchronizing processes. <p> Our experimental platform is a network of 8 DECStation-5000/240s running Ultrix and using a 100Mbps ATM network. We achieved good speedups on 8 processors for several applications including Successive Over-Relaxation (SOR - 7.4), the Traveling Salesman Problem (TSP - 7.2), and Genetic Linkage Analysis (ILINK - 5.9) <ref> [11] </ref>. The TreadMarks implementation is portable: it runs on Alpha/OSF/1, SPARC/SunOS 4.x, and RS/6000/AIX 3.x as well. The rest of the paper is organized as follows. Section 2 discusses the advantages of using a single global address space and provides an overview of the consistency model used in our system. <p> The model we provide to the user is release consistency (RC) [8] and TreadMarks uses a lazy release consistent (LRC) implementation [10]. Further details on TreadMarks may be found in <ref> [11] </ref>. RC is a relaxed memory consistency model that addresses the above-mentioned efficiency problems of sequential consistency. In RC, synchronization operations are made explicit and categorized into acquires (corresponding to getting access to data) and releases (corresponding to providing access to data).
Reference: [12] <author> Povl T. Koch and Robert J. Fowler. </author> <title> Integrating message-passing with lazy release consistent distributed shared memory. </title> <booktitle> To appear in the First Operating Systems Design and Implementation Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework. Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife [13] and FLASH [14]. Both try to exploit the advantages of bulk data transfer afforded by message passing. Carlos <ref> [12] </ref> is a distributed shared memory system that integrates support for message passing with and without coherence.
Reference: [13] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the 1993 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework. Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife <ref> [13] </ref> and FLASH [14]. Both try to exploit the advantages of bulk data transfer afforded by message passing. Carlos [12] is a distributed shared memory system that integrates support for message passing with and without coherence.
Reference: [14] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> To appear in Proceedings of the 21st Annual International Conference on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework. Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife [13] and FLASH <ref> [14] </ref>. Both try to exploit the advantages of bulk data transfer afforded by message passing. Carlos [12] is a distributed shared memory system that integrates support for message passing with and without coherence.
Reference: [15] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The consistency model seen by an application or user determines when modifications to data may be expected to be seen at a given processor. Traditionally, sequential consistency <ref> [15] </ref> has been the model provided to users of shared memory machines. Sequential consistency, in essence, requires that writes to shared memory are visible "immediately" at all processors. The IVY system [17] implemented this model.
Reference: [16] <author> G.M. Lathrop, J.M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for multilocus linkage analysis in humans. </title> <booktitle> Proceedings of National Academy of Science, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The workstations run the Ul-trix version 4.3 operating system. TreadMarks is implemented as a user-level library that is linked in with the application program. No kernel modifications are necessary. ILINK, SOR, TSP, and Water. ILINK, from the LINKAGE package <ref> [16] </ref>, is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. We present results for the CLP input data set (see [7] for more details). Red-Black Successive Over-Relaxation (SOR) is a method for solving partial differential equations.
Reference: [17] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Section 6 describes related work. Section 7 summarizes our conclusions. 2 Single Global Address Space Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces [2], and message passing [19]. TreadMarks is a distributed shared memory (DSM) system <ref> [17] </ref>. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 2). <p> Traditionally, sequential consistency [15] has been the model provided to users of shared memory machines. Sequential consistency, in essence, requires that writes to shared memory are visible "immediately" at all processors. The IVY system <ref> [17] </ref> implemented this model. The disadvantage of this model is that it can result in an excessively large amount of communication since modifications to shared data must be sent out immediately in the form of invalidates or updates.
Reference: [18] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We ran SOR for 100 iterations on a 2000 fi 1000 matrix. TSP solves the traveling salesman problem using a branch-and-bound algorithm. We used a 19-city problem as input. Water, a modified version of the program from the SPLASH suite <ref> [18] </ref>, is a molecular dynamics simulation. We present results for Water for a run with 288 molecules and 5 time steps. 5.1 Discussion The speedups presented in Figure 4 indicate that the base system performs reasonably well for most of the applications presented.
Reference: [19] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency:Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Section 6 describes related work. Section 7 summarizes our conclusions. 2 Single Global Address Space Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces [2], and message passing <ref> [19] </ref>. TreadMarks is a distributed shared memory (DSM) system [17]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 2).
References-found: 19

