URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/ml95w1/jain.ps.gz
Refering-URL: http://www.cs.wisc.edu/~shavlik/ml95w1/procs.html
Root-URL: 
Email: sanjay@iscs.nus.sg  arun@cse.unsw.edu.au  
Title: Team Learning of Formal Languages learning of computer programs for computable functions from their graphs
Author: Sanjay Jain Arun Sharma 
Note: Team  has been studied extensively. However,  this is that  Scenarios which can be modeled by team learning are also presented.  
Address: Singapore 0511, Republic of Singapore  Sydney, NSW 2052, Australia  
Affiliation: Dept. of Info. Systems Computer Science National University of Singapore  School of Computer Science and Engineering The University of New South Wales  
Abstract: Some theoretical results about learnability of formal languages by teams of algorithmic machines are surveyed. Some new results about restricted classes of languages are presented. These results are mainly about two issues: redundancy and aggregation. The issue of redundancy deals with the impact of increasing the size of a team and increasing the number of machines required to be successful. The issue of aggregation deals with conditions under which a team may be replaced by a single machine without any loss in learning ability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 46-62, </pages> <year> 1980. </year>
Reference-contexts: In other words, there is a uniform decision proce dure for languages in the class. Angluin <ref> [1] </ref> was the first researcher to restrict investigations to indexed families of recursive languages; she was motivated by the fact that most language families of practical interest are indexed families (e.g., the collection of pattern languages). We now report on redundancy and aggregation issues for these classes.
Reference: [2] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: Before we formally define learning by a team, it is worth considering the origins of team learning. Consider the following theorem for TxtEx-identification. Theorem 1 <ref> [2] </ref> There are collections of languages L 1 and L 2 such that (a) L 1 2 TxtEx, (b) L 2 2 TxtEx, but (c) (L 1 [ L 2 ) 62 TxtEx. <p> The formal definitions for team identification of languages are presented next. J. Case first suggested the notion of team identification for functions based on the non-union theorem of the Blums <ref> [2] </ref>, and it was extensively investigated by C. Smith [17]. The general case of m out of n teams is due to Osherson, Stob, and Wienstein [13]. Jain and Sharma [8] first investigated team learning for languages.
Reference: [3] <author> R. P. Daley, B. Kalyanasundaram, and M. Velau-thapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <booktitle> In Proceedings of the Fifth An--nual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <pages> pages 203-217. </pages> <editor> A. C. M. </editor> <publisher> Press, </publisher> <year> 1992. </year>
Reference-contexts: Recently, learning of functions by teams of learning machines has become a very active area of research and has been suggested as a theoretical model for multi-agent learning (for example, see <ref> [17, 16, 7, 4, 3, 12] </ref>). We argue that the utility of function learning as a model for machine learning is somewhat limited. Data available to most learning systems are of two kinds: positive data and complete (both positive and negative) data. Function learning models only the later.
Reference: [4] <author> R. P. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In L. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 228-239. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference-contexts: Recently, learning of functions by teams of learning machines has become a very active area of research and has been suggested as a theoretical model for multi-agent learning (for example, see <ref> [17, 16, 7, 4, 3, 12] </ref>). We argue that the utility of function learning as a model for machine learning is somewhat limited. Data available to most learning systems are of two kinds: positive data and complete (both positive and negative) data. Function learning models only the later.
Reference: [5] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: M denotes a typical variable for learning machines. We now consider what it means for a learning machine to successfully learn languages. The criterion of success considered in the present paper is Gold's <ref> [5] </ref> identification in the limit . We first introduce it for learning from positive data. Definition 3 [5] (a) M TxtEx-identifies an r.e. language L just in case M, fed any text for L, converges to a grammar for L. <p> We now consider what it means for a learning machine to successfully learn languages. The criterion of success considered in the present paper is Gold's <ref> [5] </ref> identification in the limit . We first introduce it for learning from positive data. Definition 3 [5] (a) M TxtEx-identifies an r.e. language L just in case M, fed any text for L, converges to a grammar for L. <p> We now define identification from both positive and negative data. Definition 4 <ref> [5] </ref> (a) M InfEx-identifies an r.e. language L just in case M, fed any informant for L, converges to a grammar for L. <p> For example, the cardinality of set f2; 2; 3g is 2, but the cardinality of the multiset f2; 2; 3g is 3. 2 Taking L 1 = fN g and L 2 = FIN yields a proof because of Gold's <ref> [5] </ref> result that no collection of languages closed under union. In other words, there are collec-tions of languages that are identifiable, but the union of these collections is not identifiable. <p> The following definition formalizes this notion. (Notation: A characteristic function of a language A is the function which is 1 on elements of A and 0 on non-elements of A.) Definition 6 <ref> [5] </ref> (a) M TxtExCI-identifies a recursive language L just in case M, fed any text for L, converges to a program that computes the characteristic function of L.
Reference: [6] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: As already noted our domain is the collection of recursively enumerable languages over N . A grammar for a recursively enumerable language L is a computer program that accepts L (or, equivalently, generates L <ref> [6] </ref>). For any recursively enumerable language L, the elements of L constitute its positive data and the elements of the complement, N L, constitute its negative data. We next describe notions that capture the presentation of positive data and presentation of both positive and negative data.
Reference: [7] <author> S. Jain and A. Sharma. </author> <title> Finite learning by a team. </title> <editor> In M. Fulk and J. Case, editors, </editor> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <address> Rochester, New York, </address> <pages> pages 163-177. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Recently, learning of functions by teams of learning machines has become a very active area of research and has been suggested as a theoretical model for multi-agent learning (for example, see <ref> [17, 16, 7, 4, 3, 12] </ref>). We argue that the utility of function learning as a model for machine learning is somewhat limited. Data available to most learning systems are of two kinds: positive data and complete (both positive and negative) data. Function learning models only the later.
Reference: [8] <author> S. Jain and A. Sharma. </author> <title> Language learning by a team. </title> <editor> In M. S. Paterson, editor, </editor> <booktitle> Proceedings of the 17th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 153-166. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1990. </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <pages> 443. </pages>
Reference-contexts: J. Case first suggested the notion of team identification for functions based on the non-union theorem of the Blums [2], and it was extensively investigated by C. Smith [17]. The general case of m out of n teams is due to Osherson, Stob, and Wienstein [13]. Jain and Sharma <ref> [8] </ref> first investigated team learning for languages. <p> The results that we present here are about redundancy and aggregation. We direct the reader to <ref> [8, 10, 9, 11] </ref> for additional results. First, it is easy to see the following proposition.
Reference: [9] <author> S. Jain and A. Sharma. </author> <title> Computational limits on team identification of languages. </title> <type> Technical Report 9301, </type> <institution> School of Computer Science and Engineering; University of New South Wales, </institution> <year> 1993. </year>
Reference-contexts: The results that we present here are about redundancy and aggregation. We direct the reader to <ref> [8, 10, 9, 11] </ref> for additional results. First, it is easy to see the following proposition. <p> The complete picture is actually quite complicated. The status of teams with success ratio 1 2 is completely known, but only partial results are known for other team ratios ( 1 k ; k &gt; 2); we direct the reader to <ref> [9] </ref>. The next result sheds light on when a team learning languages from texts can be aggregated into a single machine without loss in learning ability.
Reference: [10] <author> S. Jain and A. Sharma. </author> <title> Probability is more powerful than team for language identification. </title> <booktitle> In Proceedings of the Sixth Annual Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <publisher> Califor-nia, </publisher> <pages> pages 192-198. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: The results that we present here are about redundancy and aggregation. We direct the reader to <ref> [8, 10, 9, 11] </ref> for additional results. First, it is easy to see the following proposition.
Reference: [11] <author> S. Jain and A. Sharma. </author> <title> On aggregating teams of learning machines. </title> <journal> Theoretical Computer Science A, </journal> <volume> 137(1) </volume> <pages> 85-108, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The results that we present here are about redundancy and aggregation. We direct the reader to <ref> [8, 10, 9, 11] </ref> for additional results. First, it is easy to see the following proposition.
Reference: [12] <author> S. Jain, A. Sharma, and M. Velauthapillai. </author> <title> Finite identification of function by teams with success ratio 1/2 and above. </title> <journal> Information and Computation, </journal> <note> 1995. To Appear. </note>
Reference-contexts: Recently, learning of functions by teams of learning machines has become a very active area of research and has been suggested as a theoretical model for multi-agent learning (for example, see <ref> [17, 16, 7, 4, 3, 12] </ref>). We argue that the utility of function learning as a model for machine learning is somewhat limited. Data available to most learning systems are of two kinds: positive data and complete (both positive and negative) data. Function learning models only the later.
Reference: [13] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Aggregating inductive expertise. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 69-95, </pages> <year> 1986. </year>
Reference-contexts: J. Case first suggested the notion of team identification for functions based on the non-union theorem of the Blums [2], and it was extensively investigated by C. Smith [17]. The general case of m out of n teams is due to Osherson, Stob, and Wienstein <ref> [13] </ref>. Jain and Sharma [8] first investigated team learning for languages.
Reference: [14] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: It should be noted that the hypothesis space of the learner in the above definition is still the set of all programs; it is only required that the final converged program compute the characteristic function of the language being learned. Osherson and Weinstein <ref> [14] </ref> observed the following fact which implies that there are collections of recursive languages for which a grammar can be identified from texts, but for which a decision procedure cannot be identified from texts. Theorem 9 [14] TxtExCI (TxtEx " 2 REC ). <p> Osherson and Weinstein <ref> [14] </ref> observed the following fact which implies that there are collections of recursive languages for which a grammar can be identified from texts, but for which a decision procedure cannot be identified from texts. Theorem 9 [14] TxtExCI (TxtEx " 2 REC ). We next consider team identification of decision procedures for recursive languages from texts. Clearly, the class Team m n TxtExCI can be defined.
Reference: [15] <author> L. Pitt. </author> <title> Probabilistic inductive inference. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 383-433, </pages> <year> 1989. </year>
Reference-contexts: In the sequel, we refer to such cutoff points as aggregation ratios. Theorem 3 (a) (8m; n j m n &gt; 1 2 )[Team m n InfEx = InfEx]. (b) InfEx Team 1 2 InfEx. A proof of the above result can be worked out using techniques from Pitt <ref> [15] </ref>. 4.2 TEAM LEARNING FROM TEXTS Surprisingly, introducing redundancy in the team does help sometimes in the context of learning from only positive data.
Reference: [16] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: Recently, learning of functions by teams of learning machines has become a very active area of research and has been suggested as a theoretical model for multi-agent learning (for example, see <ref> [17, 16, 7, 4, 3, 12] </ref>). We argue that the utility of function learning as a model for machine learning is somewhat limited. Data available to most learning systems are of two kinds: positive data and complete (both positive and negative) data. Function learning models only the later.

References-found: 16

