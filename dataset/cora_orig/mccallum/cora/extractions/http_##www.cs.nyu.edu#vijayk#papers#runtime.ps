URL: http://www.cs.nyu.edu/vijayk/papers/runtime.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: fvijayk,achieng@cs.uiuc.edu  
Title: Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware  
Author: Vijay Karamcheti and Andrew Chien 
Date: November 15-19, 1993.  
Note: In Proceedings of Supercomputing'93, Portland, Oregon,  
Address: 1304 West Springfield Ave., Urbana, IL 61801  
Affiliation: University of Illinois at Urbana-Champaign  
Abstract: Inefficient implementations of global namespaces, message passing, and thread scheduling on stock multicomput-ers have prevented concurrent object-oriented programming (COOP) languages from gaining widespread acceptance. Recognizing that the architectures of stock mul-ticomputers impose a hierarchy of costs for these operations, we have described a runtime system which provides different versions of each primitive, exposing performance distinctions for optimization. We confirm the advantages of a cost-hierarchy based runtime system organization by showing a variation of two orders of magnitude in version costs for a CM5 implementation. Frequency measurements based on COOP application programs demonstrate that a 39% invocation cost reduction is feasible by simply selecting cheaper versions of runtime operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha, </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Fine-grained COOP languages are particularly promising since they expose a great deal of concurrency by allowing its expression at the object level. Numerous researchers have been exploring fine-grained object-oriented approaches <ref> [1, 2, 3, 4, 5] </ref>. <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in <ref> [1] </ref>.
Reference: [2] <author> P. America, POOL-T: </author> <title> A parallel object-oriented language, in Object-Oriented Concurrent Programming(A. </title> <editor> Yonezawa and M. Tokoro, </editor> <booktitle> eds.), </booktitle> <pages> pp. 199-220, </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Fine-grained COOP languages are particularly promising since they expose a great deal of concurrency by allowing its expression at the object level. Numerous researchers have been exploring fine-grained object-oriented approaches <ref> [1, 2, 3, 4, 5] </ref>. <p> First, we briefly describe an abstract computation model which embodies characteristics common to many COOP languages proposed so far <ref> [2, 3, 4, 9, 5] </ref>. We then present the Concert runtime interface which provides the runtime operations required by the abstract model. 2.1 Computation model of COOP languages COOP languages express computation as a set of autonomous communicating entities (objects). <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in [1]. A wide variety of COOP languages <ref> [2, 3, 4, 9, 5] </ref> can be modeled by specializing the message acceptance and scheduling policies of this abstract computation model. 2.2 Components of the Concert runtime interface The dynamic nature of the abstract computation model and a distributed implementation make runtime support for the following operations essential. * name translation
Reference: [3] <author> A. A. Chien, </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Fine-grained COOP languages are particularly promising since they expose a great deal of concurrency by allowing its expression at the object level. Numerous researchers have been exploring fine-grained object-oriented approaches <ref> [1, 2, 3, 4, 5] </ref>. <p> First, we briefly describe an abstract computation model which embodies characteristics common to many COOP languages proposed so far <ref> [2, 3, 4, 9, 5] </ref>. We then present the Concert runtime interface which provides the runtime operations required by the abstract model. 2.1 Computation model of COOP languages COOP languages express computation as a set of autonomous communicating entities (objects). <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in [1]. A wide variety of COOP languages <ref> [2, 3, 4, 9, 5] </ref> can be modeled by specializing the message acceptance and scheduling policies of this abstract computation model. 2.2 Components of the Concert runtime interface The dynamic nature of the abstract computation model and a distributed implementation make runtime support for the following operations essential. * name translation <p> stored in central scheduler queue. 3:27 108 sched grp scheduling queue associated with object: all waiting computations scheduled together. 4:06 134 Table 1: Cost of different versions of runtime operations. 4 Implementation and performance results The Concert runtime is currently being used as an execution environment for Concurrent Aggregates (CA) <ref> [3] </ref> programs on both the Thinking Machines CM5 [10] and a thread-based uniprocessor simulator. We use these implementations to measure runtime operation costs, and CA program statistics to conservatively estimate opportunities for using low cost versions of runtime operations. <p> To demonstrate the extent to which the cost distinctions are exploitable, we present frequency measurements of optimization opportunities for a set of CA applications <ref> [3] </ref>, and characterize the effective reduction in costs of runtime operations. The applications include pcbroute which uses concurrent A fl search to route nets around obstacles on a board, logicsim which is an event-driven logic simulation, and multigrid which implements the multigrid relaxation technique.
Reference: [4] <author> W. Dally and A. Chien, </author> <title> Object oriented concurrent programming in cst, </title> <booktitle> in Proceedings of the Third Conference on Hypercube Computers, </booktitle> <address> (Pasadena, California), </address> <pages> pp. 434-9, </pages> <publisher> SIAM, </publisher> <year> 1988. </year>
Reference-contexts: Fine-grained COOP languages are particularly promising since they expose a great deal of concurrency by allowing its expression at the object level. Numerous researchers have been exploring fine-grained object-oriented approaches <ref> [1, 2, 3, 4, 5] </ref>. <p> First, we briefly describe an abstract computation model which embodies characteristics common to many COOP languages proposed so far <ref> [2, 3, 4, 9, 5] </ref>. We then present the Concert runtime interface which provides the runtime operations required by the abstract model. 2.1 Computation model of COOP languages COOP languages express computation as a set of autonomous communicating entities (objects). <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in [1]. A wide variety of COOP languages <ref> [2, 3, 4, 9, 5] </ref> can be modeled by specializing the message acceptance and scheduling policies of this abstract computation model. 2.2 Components of the Concert runtime interface The dynamic nature of the abstract computation model and a distributed implementation make runtime support for the following operations essential. * name translation
Reference: [5] <editor> A. Yonezawa, ed., </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: Fine-grained COOP languages are particularly promising since they expose a great deal of concurrency by allowing its expression at the object level. Numerous researchers have been exploring fine-grained object-oriented approaches <ref> [1, 2, 3, 4, 5] </ref>. <p> First, we briefly describe an abstract computation model which embodies characteristics common to many COOP languages proposed so far <ref> [2, 3, 4, 9, 5] </ref>. We then present the Concert runtime interface which provides the runtime operations required by the abstract model. 2.1 Computation model of COOP languages COOP languages express computation as a set of autonomous communicating entities (objects). <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in [1]. A wide variety of COOP languages <ref> [2, 3, 4, 9, 5] </ref> can be modeled by specializing the message acceptance and scheduling policies of this abstract computation model. 2.2 Components of the Concert runtime interface The dynamic nature of the abstract computation model and a distributed implementation make runtime support for the following operations essential. * name translation
Reference: [6] <author> W. Horwat, </author> <title> Concurrent smalltalk on the message-driven processor, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: This claim is reinforced by the use of custom hardware in the highest performance implementations of COOP languages. Implementations of CST on the J-machine <ref> [6] </ref> and ABCL on the EM-4 [7] exploit hardware support for message passing, method dispatch, global name translation, and thread scheduling to achieve high performance. Such custom hardware efforts focus on minimizing the cost of all runtime operations by supporting a few general-purpose primitives efficiently.
Reference: [7] <author> M. Yasugi, S. Matsuoka, and A. Yonezawa, ABCL/onEM-4: </author> <title> A new software/hardware architecture for object-oriented concurrent computing on an extended dataflow supercomputer, </title> <booktitle> in Proceedings of the ACM Conference on Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: This claim is reinforced by the use of custom hardware in the highest performance implementations of COOP languages. Implementations of CST on the J-machine [6] and ABCL on the EM-4 <ref> [7] </ref> exploit hardware support for message passing, method dispatch, global name translation, and thread scheduling to achieve high performance. Such custom hardware efforts focus on minimizing the cost of all runtime operations by supporting a few general-purpose primitives efficiently.
Reference: [8] <author> A. Chien, V. Karamcheti, and J. Plevyak, </author> <title> The concert system compiler and runtime support for efficient fine-grained concurrent object-oriented programs, </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-93-1815, Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Such static selection is particularly important for object-oriented languages which have high procedure call frequencies because it enables interprocedural optimizations. The Concert runtime is part of the Concert project <ref> [8] </ref> whose goal is to achieve efficient and portable execution of concurrent object-oriented languages on a variety of hardware platforms. The Concert system focuses on execution grain size tuning relying on aggressive program analysis and careful information management at every stage from the compiler to the runtime system. <p> Dynamic statistics show that less expensive versions can be used in many cases, dramatically reducing effective cost of runtime operations. We are constructing a compiler which will automatically optimize execution by selecting cheaper versions of runtime operations <ref> [8] </ref>. 4.1 Costs of primitive runtime operations The costs of various runtime operations are given in Table 1. Operation latency in seconds and CM5 processor cycles was measured using a synthetic program which repeatedly performed the runtime operation of interest.
Reference: [9] <author> C. Houck and G. Agha, HAL: </author> <title> A high-level actor language and its distributed implementation, </title> <booktitle> in Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <address> (St. Charles, IL), </address> <pages> pp. 158-165, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: First, we briefly describe an abstract computation model which embodies characteristics common to many COOP languages proposed so far <ref> [2, 3, 4, 9, 5] </ref>. We then present the Concert runtime interface which provides the runtime operations required by the abstract model. 2.1 Computation model of COOP languages COOP languages express computation as a set of autonomous communicating entities (objects). <p> Invocation requests can behave like remote procedure calls, requiring reply messages which are processed as invocations on the activation frame. A theoretical basis for reasoning about COOP languages can be found in [1]. A wide variety of COOP languages <ref> [2, 3, 4, 9, 5] </ref> can be modeled by specializing the message acceptance and scheduling policies of this abstract computation model. 2.2 Components of the Concert runtime interface The dynamic nature of the abstract computation model and a distributed implementation make runtime support for the following operations essential. * name translation
Reference: [10] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, </address> <month> Mas-sachusetts, </month> <title> Connection Machine CM-5, </title> <type> Technical Summary, </type> <month> November </month> <year> 1992. </year>
Reference-contexts: The different data transfer versions reflect two basic performance distinctions: local versus remote transfers, and register versus memory access costs. Remote transfers cost more than corresponding local transfers due to routing and network interface delays. Local memory transfers are more 3 The CM5 <ref> [10] </ref> provides a register-to-register primitive, while the Paragon [11] provides a memory-to-memory transfer operation. <p> grp scheduling queue associated with object: all waiting computations scheduled together. 4:06 134 Table 1: Cost of different versions of runtime operations. 4 Implementation and performance results The Concert runtime is currently being used as an execution environment for Concurrent Aggregates (CA) [3] programs on both the Thinking Machines CM5 <ref> [10] </ref> and a thread-based uniprocessor simulator. We use these implementations to measure runtime operation costs, and CA program statistics to conservatively estimate opportunities for using low cost versions of runtime operations. Performance measurements validate the existence of a cost hierarchy for each operation, justifying its reflection in the run-time interface. <p> Clearly, register-to-register versions give significant benefits for small transfers; however memory-to-memory transfers are required for large amounts of data which cannot be buffered in registers. In many machines, transfer cost increases with message size, and both the CM5 <ref> [10] </ref> and AP1000 [16] incorporate special mechanisms for sending small messages. Scheduling and initiation Dynamic binding using C++ like dispatch tables [17] costs only a few instructions more than static binding.
Reference: [11] <author> Intel Corporation, </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Remote transfers cost more than corresponding local transfers due to routing and network interface delays. Local memory transfers are more 3 The CM5 [10] provides a register-to-register primitive, while the Paragon <ref> [11] </ref> provides a memory-to-memory transfer operation. Situation Version Description local lsend register ! register destination lsend mem memory ! memory remote rsend register ! register destination rsend mem memory ! memory expensive than register movements because memory data must be moved to and from registers in load/store architectures.
Reference: [12] <author> T. Baba, et al., </author> <title> A parallel object-oriented total architecture: </title> <booktitle> A-NET, in Proceedings of IEEE Supercomputing '90, </booktitle> <pages> pp. 276-285, </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year>
Reference-contexts: Data transfer versions are listed in Figure 4. Since most data transfers are accompanied by a handler invocation, all versions integrate handler invocation with actual data transfer. This integration reduces the overhead from data reception to dynamic method dispatching <ref> [12] </ref> and has been used in the J-machine [13] and the active message facility [14]. The different data transfer versions reflect two basic performance distinctions: local versus remote transfers, and register versus memory access costs. Remote transfers cost more than corresponding local transfers due to routing and network interface delays.
Reference: [13] <author> W. J. Dally, A. Chien, S. Fiske, W. Horwat, J. Keen, M. Lar-ivee, R. Lethin, P. Nuth, S. Wills, P. Carrick, and G. Fyler, </author> <title> The J-Machine: A fine-grain concurrent computer, </title> <booktitle> in Information Processing 89, Proceedingsof the IFIP Congress, </booktitle> <pages> pp. 1147-1153, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Data transfer versions are listed in Figure 4. Since most data transfers are accompanied by a handler invocation, all versions integrate handler invocation with actual data transfer. This integration reduces the overhead from data reception to dynamic method dispatching [12] and has been used in the J-machine <ref> [13] </ref> and the active message facility [14]. The different data transfer versions reflect two basic performance distinctions: local versus remote transfers, and register versus memory access costs. Remote transfers cost more than corresponding local transfers due to routing and network interface delays.
Reference: [14] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser, </author> <title> Active Messages: a mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Since most data transfers are accompanied by a handler invocation, all versions integrate handler invocation with actual data transfer. This integration reduces the overhead from data reception to dynamic method dispatching [12] and has been used in the J-machine [13] and the active message facility <ref> [14] </ref>. The different data transfer versions reflect two basic performance distinctions: local versus remote transfers, and register versus memory access costs. Remote transfers cost more than corresponding local transfers due to routing and network interface delays.
Reference: [15] <author> T. von Eicken and D. E. Culler, </author> <title> Building Communication Paradigms with the CM-5 Active Message layer (CMAM). </title> <institution> University of California, Berkeley, 2.4 ed., </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: hashtable of associations. 0:91 30 xlate pub distributed hashtable (with local caching). min max 3:59 11:70 536 Data Transfer (Local) lsend reg!reg register arguments of integrated invocation. 0:15 5 lsend mem mem!mem argument copying into preallocated buffer. 2:88 96 Data Transfer (Remote) rsend reg!reg built with CMAM 4 () from <ref> [15] </ref>. 7:05 235 rsend mem mem!mem built with CMAM () from [15]. 15:65 520 Scheduling & s bind known at compile time. 0:00 0 Initiation d bind C++-like dispatch table. 0:09 3 alloc stack modify stack pointer. 0:03 1 alloc frame binned memory allocator. 3:72 124 sched now frame setup and <p> caching). min max 3:59 11:70 536 Data Transfer (Local) lsend reg!reg register arguments of integrated invocation. 0:15 5 lsend mem mem!mem argument copying into preallocated buffer. 2:88 96 Data Transfer (Remote) rsend reg!reg built with CMAM 4 () from <ref> [15] </ref>. 7:05 235 rsend mem mem!mem built with CMAM () from [15]. 15:65 520 Scheduling & s bind known at compile time. 0:00 0 Initiation d bind C++-like dispatch table. 0:09 3 alloc stack modify stack pointer. 0:03 1 alloc frame binned memory allocator. 3:72 124 sched now frame setup and procedure call. 0:14 5 sched ind computation stored in central scheduler
Reference: [16] <author> T. Shimizu, T. Horie, and H. Ishihata, </author> <title> Low-latency message communication support for the AP1000, </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 288-297, </pages> <year> 1992. </year>
Reference-contexts: Clearly, register-to-register versions give significant benefits for small transfers; however memory-to-memory transfers are required for large amounts of data which cannot be buffered in registers. In many machines, transfer cost increases with message size, and both the CM5 [10] and AP1000 <ref> [16] </ref> incorporate special mechanisms for sending small messages. Scheduling and initiation Dynamic binding using C++ like dispatch tables [17] costs only a few instructions more than static binding. However, the cost of frame allocation for blocking invocations is significant implying that non-blocking invocations are worth dealing with specially.
Reference: [17] <author> B. Stroustrup, </author> <title> The C++ Programming Language. </title> <publisher> Addison Wesley, </publisher> <editor> second ed., </editor> <year> 1991. </year>
Reference-contexts: In many machines, transfer cost increases with message size, and both the CM5 [10] and AP1000 [16] incorporate special mechanisms for sending small messages. Scheduling and initiation Dynamic binding using C++ like dispatch tables <ref> [17] </ref> costs only a few instructions more than static binding. However, the cost of frame allocation for blocking invocations is significant implying that non-blocking invocations are worth dealing with specially.
Reference: [18] <author> K. Taura, S. Matsuoka, and A. Yonezawa, </author> <title> An efficient implementation scheme of concurrent object-oriented languages on stock multicomputers, </title> <booktitle> in Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: Table 3 also shows the costs of corresponding operations in the ABCL/onAP1000 implementation <ref> [18] </ref> which focuses on improving the worst-case efficiency of runtime primitives. The numbers show the Concert runtime to be as fast as the ABCL/onAP1000 implementation for comparable invocation sequences, implying that the observed cost-hierarchy is not merely an artifact of poor implementation of the worst-case version. <p> It differs from runtime systems for coarse-grained object-oriented languages such as COOL [21] and Mentat [22], by focusing on fine-grained object-level concurrency. The ABCL/onAP1000 implementation <ref> [18] </ref> is most similar with our work but adopts a traditional design, emphasizing techniques for reducing access latency and object-creation costs. The general versions in Concert are as efficient as in the ABCL system, while the cheaper versions cost significantly less.
Reference: [19] <author> M. C. Carlisle, A. Rogers, J. H. Reppy, and L. J. Hendren, </author> <title> Early experiences with olden, </title> <booktitle> in Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Machines, </booktitle> <year> 1993. </year>
Reference-contexts: name # indiv. # group % redn. scheduled scheduled pcbroute 119262 66692 44% logicsim 36877 15460 58% multigrid 53972 13816 74% Table 8: Scheduling statistics in CA programs. 5 Related work Our work is related to many efforts focusing on run-time support for efficiently executing irregular computations on stock hardware <ref> [19, 20] </ref>. It differs from runtime systems for coarse-grained object-oriented languages such as COOL [21] and Mentat [22], by focusing on fine-grained object-level concurrency. The ABCL/onAP1000 implementation [18] is most similar with our work but adopts a traditional design, emphasizing techniques for reducing access latency and object-creation costs.
Reference: [20] <author> W. C. Hsieh, P. Wang, and W. E. Weihl, </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems, </title> <booktitle> in Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 239-248, </pages> <year> 1993. </year>
Reference-contexts: name # indiv. # group % redn. scheduled scheduled pcbroute 119262 66692 44% logicsim 36877 15460 58% multigrid 53972 13816 74% Table 8: Scheduling statistics in CA programs. 5 Related work Our work is related to many efforts focusing on run-time support for efficiently executing irregular computations on stock hardware <ref> [19, 20] </ref>. It differs from runtime systems for coarse-grained object-oriented languages such as COOL [21] and Mentat [22], by focusing on fine-grained object-level concurrency. The ABCL/onAP1000 implementation [18] is most similar with our work but adopts a traditional design, emphasizing techniques for reducing access latency and object-creation costs.
Reference: [21] <author> R. Chandra, A. Gupta, and J. L. Hennessy, </author> <title> Data locality and load balancing in COOL, </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: It differs from runtime systems for coarse-grained object-oriented languages such as COOL <ref> [21] </ref> and Mentat [22], by focusing on fine-grained object-level concurrency. The ABCL/onAP1000 implementation [18] is most similar with our work but adopts a traditional design, emphasizing techniques for reducing access latency and object-creation costs.
Reference: [22] <author> A. Grimshaw, </author> <title> Easy-to-use object-oriented parallel processing with Mentat, </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: It differs from runtime systems for coarse-grained object-oriented languages such as COOL [21] and Mentat <ref> [22] </ref>, by focusing on fine-grained object-level concurrency. The ABCL/onAP1000 implementation [18] is most similar with our work but adopts a traditional design, emphasizing techniques for reducing access latency and object-creation costs.
Reference: [23] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine, </title> <booktitle> in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <pages> pp. 164-75, </pages> <year> 1991. </year>
Reference-contexts: The general versions in Concert are as efficient as in the ABCL system, while the cheaper versions cost significantly less. Our work complements the ABCL work by distinguishing low-cost operations from expensive ones, exposing the distinctions for optimization. Our work is also similar to the Threaded Abstract Machine (TAM) <ref> [23] </ref>. Although TAM supports a different language model (extended hybrid dataflow), both systems enable optimization by providing the compiler with a range of mechanisms for managing synchronization, scheduling and storage and an associated cost hierarchy.
References-found: 23

