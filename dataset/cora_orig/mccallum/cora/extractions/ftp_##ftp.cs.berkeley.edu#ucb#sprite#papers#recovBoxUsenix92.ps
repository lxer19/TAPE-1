URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/recovBoxUsenix92.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Title: The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment  
Author: Mary Baker, Mark Sullivan 
Address: Berkeley  
Affiliation: University of California,  
Abstract: As organizations with high system availability requirements move to UNIX, the elimination of down-time in the UNIX environment becomes a more important issue. Designing for fast recovery, rather than crash prevention, can provide low-cost highly-available systems without sacrificing performance or simplicity. In Sprite, a UNIX-like distributed operating system, we accomplish this fast recovery in part through the use of a recovery box: a stable area of memory in which the system stores carefully selected pieces of system state, and from which the system can be regenerated quickly. Error detection using checksums allows the system to revert to its traditional reboot sequence if the recovery box data is corrupted during system failure. Recent statistics about the types and frequencies of operating system failures indicate that fast recovery using the recovery box will be possible most of the time. Using our recovery box implementation, a Sprite file server recovers in 26 seconds and a database manager with ten remote client processes recovers in six seconds fast enough that many users and applications will not care that the system crashed. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Accetta, M., Baron, R., Bolosky, W., Golub, D., Rashid, R., Tevanian, A. and Young, M., </author> <title> Mach: A New Kernel Foundation for UNIX Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <month> June </month> <year> 1986. </year> <title> 2. anon, A Measure of Transaction Processing Power, </title> <booktitle> in Readings in Database Systems, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> 300-312. </pages>
Reference-contexts: It should also be possible to apply it to user-level servers, such as those running on top of the Mach microkernel <ref> [1] </ref>, and to other distributed application programs. Acknowledgements We would like to thank John Ousterhout, Men-del Rosenblum, Margo Seltzer, Jim Mott-Smith, Joe Hellerstein, Mike Olson, Jeff Mogul, and Adam Glass for their very helpful comments on drafts of this paper.
Reference: 3. <author> Auslander, M., Larkin, D. and Scherr, A., </author> <title> Evolution of MVS, </title> <journal> Vol. </journal> <volume> 25, </volume> <month> September </month> <year> 1981. </year>
Reference-contexts: Other fault-tolerant systems provide high availability through recovery schemes that are significantly more complex than the usual UNIX recovery path. Integrity-S2 [10] detects system errors as they occur and attempts to correct affected internal data structures while the system is running. MVS <ref> [3] </ref> uses a multi-level recovery scheme in which different portions of the system can fail and recover independently.
Reference: 4. <author> Baker, M. and Ousterhout, J., </author> <title> Availability in the Sprite Distributed File System, </title> <booktitle> Operating Systems Review 25, </booktitle> <month> 2 (April </month> <year> 1991). </year>
Reference-contexts: Summer '92 USENIX - June 8-June 12, 1992 - San Antonio, TX 33 The Recovery Box Baker, Sullivan Handling this recovery information from the clients can overwhelm the server's processing capabilities, resulting in a recovery storm <ref> [4] </ref>. For a single Sun 4/280 file server with 40 clients, it currently takes over two minutes for all the clients to recover their distributed state. Using the new recovery box eliminates this recovery-related client/server communication.
Reference: 5. <author> Bartlett, J., </author> <title> A NonStop Kernel, </title> <booktitle> Proceedings of the 8th Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1981. </year>
Reference-contexts: Traditional fault tolerant systems strive for high availability by eliminating or masking failures, so that the system never goes down. In doing so, they sacrifice some combination of the traditional strengths of the UNIX environment: low hardware cost, high performance during normal execution, and simplicity. For example, Tandem <ref> [5] </ref> and Stratus [23] provide non-stop processing through hardware redundancy and sometimes software redundancy. Auragen [7] applies redundant hardware and a modified process pair scheme to the UNIX environment. The hardware support is costly, and redundant software techniques either reduce normal performance or increase implementation complexity.
Reference: 6. <author> Bhide, A., Elnozahy, E. and Morgan, S., </author> <title> Implicit Replication in a Network File Server, </title> <booktitle> IEEE Workshop on Management of Replicated Data, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: The hardware support is costly, and redundant software techniques either reduce normal performance or increase implementation complexity. Harp's [11] application of replicated hardware and software to NFS file servers suffers no performance degradation, but it increases system complexity. HA-NFS <ref> [6] </ref> improves the availability of NFS file servers through specialized, redundant hardware. Other fault-tolerant systems provide high availability through recovery schemes that are significantly more complex than the usual UNIX recovery path.
Reference: 7. <author> Borg, A., Blau, W., Graetsch, W., Herrman, F. and Oberle, W., </author> <title> Fault Tolerance Under UNIX, </title> <journal> ACM Transactions on Computer Systems 7, </journal> <month> 1 (February </month> <year> 1989). </year>
Reference-contexts: In doing so, they sacrifice some combination of the traditional strengths of the UNIX environment: low hardware cost, high performance during normal execution, and simplicity. For example, Tandem [5] and Stratus [23] provide non-stop processing through hardware redundancy and sometimes software redundancy. Auragen <ref> [7] </ref> applies redundant hardware and a modified process pair scheme to the UNIX environment. The hardware support is costly, and redundant software techniques either reduce normal performance or increase implementation complexity.
Reference: 8. <author> Douglis, F. and Ousterhout, J., </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation, </title> <journal> Software Practice& Experience 21, </journal> <month> 8 (July </month> <year> 1991). </year>
Reference-contexts: This second handle is called a stream handle. The ability of processes in Sprite to migrate to idle machines <ref> [8] </ref> means that two processes on different machines may share the same reference to an open file. The server maintains the shared file offset in the stream handle and must be able to regenerate this information after a reboot.
Reference: 9. <author> Gray, J., </author> <title> A Census of Tandem System Availability between 1985 and 1990, </title> <journal> IEEE Transactions on Reliability 39, </journal> <month> 4 (October </month> <year> 1990). </year>
Reference-contexts: After a failure, the system first tries to recover quickly from backup data that it stored in main memory during regular execution. If this fast recovery fails, the system just reverts to the traditional disk-based hard reboot. Existing failure statistics <ref> [9, 21, 22] </ref> show that most common failures will not corrupt the main memory backup data, so the fast recovery path should be successful most of the time. As long as corrupted data is detected, the traditional recovery path will allow us to retain current reliability. <p> Published data about the frequency of different kinds of outages is scarce, but a study of Tandem systems shows that faulty software is responsible for most failures <ref> [9] </ref>. Over time, Tandem systems have experienced fewer outages caused by hardware failures, environment failures, and operator errors. Software failures, on the other hand, have remained constant. Table 1 shows the percentages of each source of outage.
Reference: 10. <author> Jewett, D., </author> <title> Integrity-S2 A Fault-tolerant UNIX Platform, Field Failures in Operating Systems, </title> <booktitle> Digest 21st International Symposium on Fault Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: HA-NFS [6] improves the availability of NFS file servers through specialized, redundant hardware. Other fault-tolerant systems provide high availability through recovery schemes that are significantly more complex than the usual UNIX recovery path. Integrity-S2 <ref> [10] </ref> detects system errors as they occur and attempts to correct affected internal data structures while the system is running. MVS [3] uses a multi-level recovery scheme in which different portions of the system can fail and recover independently.
Reference: 11. <author> Liskov, B., Ghemawat, S., Gruber, R., Johnson, P., Shrira, L. and Williams, M., </author> <title> Replication in the Harp File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Auragen [7] applies redundant hardware and a modified process pair scheme to the UNIX environment. The hardware support is costly, and redundant software techniques either reduce normal performance or increase implementation complexity. Harp's <ref> [11] </ref> application of replicated hardware and software to NFS file servers suffers no performance degradation, but it increases system complexity. HA-NFS [6] improves the availability of NFS file servers through specialized, redundant hardware.
Reference: 12. <author> Long, D., Carroll, J. and Park, C., </author> <title> A Study of the Reliability of Internet Sites, </title> <booktitle> Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: Introduction Increasing workstation performance is making UNIX and related operating systems more attractive in environments that also value high system availability. Unfortunately, measurements from Internet sites <ref> [12] </ref> indicate that UNIX machines fail on average once every two weeks. To maintain high availability, these systems must either reduce the failure rate substantially or recover very quickly after errors.
Reference: 13. <author> Mckusick, M. K., Joy, W. N., Leffler, S. J. and Fabry, R. S., </author> <title> A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984). </year>
Reference-contexts: Without LFS, a hard reboot of a file server with 5 gigabytes of storage using a traditional UNIX file system such as the 4.2 BSD Fast File System <ref> [13] </ref> can take up to 40 minutes to restore the consistency of file system metadata. LFS does not require a file system check (fsck) during system initialization, because it leaves its disk-resident file system metadata consistent even after a crash.
Reference: 14. <author> Mogul, J. C., </author> <title> A Recovery Protocol for Spritely NFS, </title> <booktitle> To appear in the Proceedings of the USENIX Workshop on File Systems, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Although a group commit would amortize the cost of the disk write across several opens and closes, it would still greatly increase the latency of some of the opens and closes. Another approach, used in Spritely-NFS <ref> [14] </ref>, is to store information about active clients on the server's disk. After a failure, the server only needs to contact these active clients to regenerate the distributed file system state.
Reference: 15. <author> Ousterhout, J., Cherenson, A., Douglis, F., Nelson, M. and Welch, B., </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988). </year>
Reference-contexts: As long as corrupted data is detected, the traditional recovery path will allow us to retain current reliability. In order to preserve system state across failures, we have designed and implemented a recovery box in Sprite <ref> [15] </ref>, a distributed UNIX-compatible operating system. The recovery box is Summer '92 USENIX - June 8-June 12, 1992 - San Antonio, TX 31 The Recovery Box Baker, Sullivan an area of memory used to store recovery information. It can be implemented using non-volatile RAM for protection from power failures.
Reference: 16. <author> Rosenblum, M. and Ousterhout, J., </author> <title> The Design and Implementation of a Log-structured File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Storing these items shortens our module initialization from about 7 seconds to less than 2. (3) Check disks We have converted our disks over to use the Log-Structured File System (LFS) <ref> [16] </ref>. Without LFS, a hard reboot of a file server with 5 gigabytes of storage using a traditional UNIX file system such as the 4.2 BSD Fast File System [13] can take up to 40 minutes to restore the consistency of file system metadata.
Reference: 17. <author> Sandberg, R., Goldberg, D., Kleiman, S., Walsh, D. and Lyon, B., </author> <title> Design and Implementation of the Sun Network File System, </title> <booktitle> Proceedings of the the Summer 1985 USENIX Technical Conference, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: In 24% of MVS error reports studied, this information was not evident. How Sprite Uses the Recovery Box Although the Sprite distributed operating system is UNIX-compatible at the system call interface, Sprite differs in some important ways from an NFS-style distributed UNIX system <ref> [17] </ref>. Unlike NFS file servers, Sprite file servers are not stateless. To achieve higher file system performance than NFS, Sprite caches file data on both client workstations and file servers.
Reference: 18. <author> Seltzer, M. I., </author> <type> Personal Communication, </type> <month> April </month> <year> 1992. </year>
Reference-contexts: While LFS does not need a file system check to make its metadata consistent after a failure, it could use one to check that its directory structure has not been damaged by errors. A version of LFS to be included in a future BSD UNIX release <ref> [18] </ref> uses fsck to check the consistency of essential directories on reboot, such as those on the root file system. (4) Recover distributed state The Sprite file servers must regenerate the distributed file system state they were using prior to a crash.
Reference: 19. <author> Stonebraker, M. and Rowe, L., </author> <title> The Design of POSTGRES, </title> <booktitle> Proceedings of the 5th ACM SIGMOD Conference, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: We have provided a data insertion and retrieval interface that allows the recovery box to be used by both the operating system and application programs. The recovery box has been used for fast recovery in the Sprite distributed file system and in an experimental version of POSTGRES <ref> [19] </ref>, a database management system (DBMS) with clients running on different machines. Using the recovery box and other techniques, we have reduced the combined recovery time for a Sprite file server and the POSTGRES DBMS from many minutes to a total of 32 seconds.
Reference: 20. <author> Stonebraker, M., </author> <title> The POSTGRES Storage System, </title> <booktitle> Proceedings of the 13th International Conference on Very Large Data Bases, </booktitle> <month> September </month> <year> 1987. </year>
Reference-contexts: In a conventional database management system, recovery includes the cost of write-ahead log processing (recovering disk state) in addition to client connection reestablishment. POSTGRES has an unconventional storage manager that maintains consistency of data on disk without requiring write-ahead log processing <ref> [20] </ref>, so it does not need the recovery box to avoid this costly recovery step. Without the recovery box, connection reestablishment is driven by the clients. When the database manager fails, all transactions executing on behalf of clients are aborted and all connection state held at the server is lost. <p> Authentication of the client is reverified when the DBMS receives the next message from the client. POSTGRES does not use the recovery box to store any of the state associated with its storage system. Storage system performance optimizations requiring non-volatile RAM are discussed in <ref> [20] </ref>; for example, to reduce commit latency, committed data can be stored in non-volatile RAM instead of on disk. But this technique requires the operating system to guarantee that data stored in non-volatile memory be permanent.
Reference: 21. <author> Sullivan, M., </author> <title> Unpublished survey of software errors reported in 4.1 and 4.2BSD UNIX, </title> <year> 1990. </year>
Reference-contexts: After a failure, the system first tries to recover quickly from backup data that it stored in main memory during regular execution. If this fast recovery fails, the system just reverts to the traditional disk-based hard reboot. Existing failure statistics <ref> [9, 21, 22] </ref> show that most common failures will not corrupt the main memory backup data, so the fast recovery path should be successful most of the time. As long as corrupted data is detected, the traditional recovery path will allow us to retain current reliability. <p> The table shows the distribution of types of outages occurring in Tandem systems between 1985-1990. Environment failures are caused by floods, fires, and long power outages. Two studies that categorize the types of operating system software errors indicate that most such errors will not corrupt the recovery box <ref> [21, 22] </ref>. These two studies, summarized in Table 2, focus on the frequency of addressing errors, which are the errors that cause programs to corrupt memory. The BSD UNIX study divides errors into synchronization (47%), exception-handling (12%), addressing (12%), and miscellaneous (29%) errors.
Reference: 22. <author> Sullivan, M. and Chillarege, R., </author> <title> Software Defects and Their Impact on System Availability A Study of Field Failures in Operating Systems, </title> <booktitle> Digest 21st International Symposium on Fault Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: After a failure, the system first tries to recover quickly from backup data that it stored in main memory during regular execution. If this fast recovery fails, the system just reverts to the traditional disk-based hard reboot. Existing failure statistics <ref> [9, 21, 22] </ref> show that most common failures will not corrupt the main memory backup data, so the fast recovery path should be successful most of the time. As long as corrupted data is detected, the traditional recovery path will allow us to retain current reliability. <p> The table shows the distribution of types of outages occurring in Tandem systems between 1985-1990. Environment failures are caused by floods, fires, and long power outages. Two studies that categorize the types of operating system software errors indicate that most such errors will not corrupt the recovery box <ref> [21, 22] </ref>. These two studies, summarized in Table 2, focus on the frequency of addressing errors, which are the errors that cause programs to corrupt memory. The BSD UNIX study divides errors into synchronization (47%), exception-handling (12%), addressing (12%), and miscellaneous (29%) errors.
Reference: 23. <editor> Webber, S. and Beirne, J., </editor> <booktitle> The Stratus Architecture, Digest 21st International Symposium on Fault Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: In doing so, they sacrifice some combination of the traditional strengths of the UNIX environment: low hardware cost, high performance during normal execution, and simplicity. For example, Tandem [5] and Stratus <ref> [23] </ref> provide non-stop processing through hardware redundancy and sometimes software redundancy. Auragen [7] applies redundant hardware and a modified process pair scheme to the UNIX environment. The hardware support is costly, and redundant software techniques either reduce normal performance or increase implementation complexity.

References-found: 22

