URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-13.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Sample compression, learnability, and the Vapnik-Chervonenkis dimension.  
Author: Sally Floyd Manfred Warmuth 
Note: Address:  This author was supported in part by the Director, Office of Energy Research, Scientific Computing Staff, of the U.S. Department of Energy under Contract No. DE-AC03-76SF00098. Address:  Thus author was supported by ONR grants N00014-K-86-K-0454 and NO0014-91-J-1162 and NSF grant IRI-9123692  
Address: Santa Cruz, CA 95064 USA  1 Cyclotron Road, Berkeley, CA 94720,  Santa Cruz, CA 95064,  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  Lawrence Berkeley Laboratory,  Department of Computer Science, University of California,  
Pubnum: UCSC-CRL-93-13  
Email: floyd@ee.lbl.gov.  
Date: March 30, 1993  
Abstract: Within the framework of pac-learning, we explore the learnability of concepts from samples using the paradigm of sample compression schemes. A sample compression scheme of size d for a concept class C 2 X consists of a compression function and a reconstruction function. The compression function, given a finite sample set consistent with some concept in C, chooses a subset of k examples as the compression set. The reconstruction function, given a compression set of k examples, reconstructs a hypothesis on X. Given a compression set produced by the compression function from a sample of a concept in C, the reconstruction function must be able to reproduce a hypothesis consistent with that sample. We demonstrate that the existence of a fixed-size sample compression scheme for a class C is sufficient to ensure that the class C is learnable. We define maximum and maximal classes of VC dimension d. For every maximum class of VC dimension d, there is a sample compression scheme of size d, and for sufficiently-large maximum classes there is no sample compression scheme of size less than d. We discuss briefly classes of VC dimension d that are maximal but not maximum, and we give a sample compression scheme of size d that applies to some maximal and nonmaximum classes. It is unknown whether there is a sample compression scheme of size d for every class of VC dimension d. 
Abstract-found: 1
Intro-found: 1
Reference: [CBFH+93] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> Proceedings of the 25th ACM Symposium on the Theory of Computation, </booktitle> <year> 1993. </year> <title> To appear. 6 The compression schemes given in sections 3 and 4 can be modified so that the compression set consists of unlabeled examples. However, we don't know how modify the compression scheme for maximum classes. </title> <type> 20 </type>
Reference-contexts: Choosing fi = 1=2 gives simple bounds. The bounds can be marginally improved by optimizing the choice of fi as done in 5 <ref> [CBFH+93] </ref>. Note that the upper bounds have the form O ( 1 * (d log 1 ffi )) where d is either the size of a compression scheme or the VC dimension for the concept class. <p> We define a randomly-generated maximal class by the following procedure for randomly generating such classes. Procedure for generating a random maximal class of VC dimension d. 5 In <ref> [CBFH+93] </ref> a bound was optimized which had 2 ln 1 1+fi in the denominator. Similar techniques can be used to optimize a bound with 1 fi in the denominator 16 1.
Reference: [BEHW87] <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M., </author> <title> Occam's Razor, </title> <journal> Inf. Proc. Let., </journal> <volume> 24, </volume> <year> 1987, </year> <pages> pp. 377-380. </pages>
Reference-contexts: This upper bound is linear in lnjCj. Theorem 2.1: ([V82], <ref> [BEHW87] </ref>, [BEHW89]): Let C 2 X be any finite concept class. Then for sample size greater than 1 * ln ffi , any algorithm that chooses a hypothesis from C consistent with the examples is a learning algorithm for C.
Reference: [BEHW89] <author> Blumer, A., A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <journal> JACM, </journal> <volume> 36(4), pp.929-965, </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: This research on sample compression schemes has several distinct motivations. One motivation is to demonstrate that the existence of an appropriate sample compression scheme is sufficient to ensure learnability. This approach provides an alternative to that of <ref> [BEHW89] </ref>, which uses the Vapnik-Chervonenkis dimension to classify learnable geometric concepts. A second motivation of this work is to explore the combinatorial properties of concept classes of VC dimension d. We give a sample compression scheme of size log jCj for any finite concept class C 2 X . <p> This upper bound is linear in lnjCj. Theorem 2.1: ([V82], [BEHW87], <ref> [BEHW89] </ref>): Let C 2 X be any finite concept class. Then for sample size greater than 1 * ln ffi , any algorithm that chooses a hypothesis from C consistent with the examples is a learning algorithm for C. <p> For these classes, a parameter of the class called the Vapnik-Chervonenkis dimension is used to give upper and lower bounds on the sample complexity ([VC71], <ref> [BEHW89] </ref>, [EHKV87]). For a concept class C on X, and for S X, let CjS denote the restriction of concept class C to the set S. If CjS = 2 S , then the set S is shattered by C. <p> Note that a class C with one concept is of VC dimension 0. 2 If the class C 2 X has VC dimension d, then for all Y X , the restriction CjY has VC dimension at most d. Theorem 2.2 from Blumer, Ehrenfeucht, Haussler, and Warmuth in <ref> [BEHW89] </ref> gives an upper bound on the sample complexity of learning algorithms in terms of the VC dimension of the class. This result in [BEHW89] is adapted from Vapnik and Chervonenkis in [VC71]. Theorem 2.2 (BEHW89): : Let C be a well-behaved 2 concept class. <p> Theorem 2.2 from Blumer, Ehrenfeucht, Haussler, and Warmuth in <ref> [BEHW89] </ref> gives an upper bound on the sample complexity of learning algorithms in terms of the VC dimension of the class. This result in [BEHW89] is adapted from Vapnik and Chervonenkis in [VC71]. Theorem 2.2 (BEHW89): : Let C be a well-behaved 2 concept class. <p> In this paper the size of a compression set is defined as simply the number of examples in the compression set, not the number of bits used to encode those examples. 2 2 This is a measure-theoretic condition given in <ref> [BEHW89] </ref>. <p> The output of the compression algorithm is a labeled compression set A 0 Y 0 of cardinality d that represents some concept in C consistent with the labeled set Y 0 . Definitions (the consistency oracle): The consistency problem for a particular concept class C is defined in <ref> [BEHW89] </ref> as the problem of determining whether there is a concept in C consistent with a particular set of labeled examples on X. We define a consistency oracle as a procedure for deciding the consistency problem. 2 From [BEHW89], if the consistency problem for C is NP-hard and RP 6= NP <p> The consistency problem for a particular concept class C is defined in <ref> [BEHW89] </ref> as the problem of determining whether there is a concept in C consistent with a particular set of labeled examples on X. We define a consistency oracle as a procedure for deciding the consistency problem. 2 From [BEHW89], if the consistency problem for C is NP-hard and RP 6= NP then C is not polynomially learnable by an algorithm that produces hypotheses from C. <p> Proof: Let Y be any set of d unlabeled examples. There are at most d=5 X i 2 i F d=5 (d)2 d=5 compression sets of size at most d=5 from Y . Since F k (m) em k <ref> [BEHW89] </ref>, the number of compression sets is bounded above by (10e) d=5 &lt; 32 d=5 = 2 d : Thus if Y is shattered by the class C, then there are not enough compression sets for the 2 d labelings of Y . 2 6 Batch learning algorithms using sample compression <p> 0: d we get d 1 + d By substituting ln m into the left hand side of equation (6.1) we get 1 ln ffi d (1 + ln m ln d) m 1 + d (1 + ln m ln d) *(m d) d e *(md) ffi: Since from <ref> [BEHW89] </ref> F d (m) em d we have d X i (1 *) mi F d (m)(1 *) md em d 2 Theorem 6.3: Let C 2 X be any concept class with a sample compression scheme of size at most d. <p> Proof: This follows from Theorem 6.1 and Lemma 6.2. 2 For maximum classes of VC dimension d, Theorem 6.3 slightly improves the sample complexity of batch learning from the previously known results from <ref> [BEHW89] </ref> and [SAB89] given in Theorem 2.2. Choosing fi = 1=2 gives simple bounds. The bounds can be marginally improved by optimizing the choice of fi as done in 5 [CBFH+93]. <p> Because we have given a suitable sample compression scheme of size d for maximum classes of VC dimension d, this result applies to all maximum classes of VC dimension d. This approach improves on the previously-known sample complexity for pac-learning for maximum classes of VC dimension d <ref> [BEHW89] </ref> [SAB89]. It is an open question whether there is a sample compression scheme of size d, or of size O (d), for every maximal class of VC dimension d.
Reference: [BL89] <author> Blumer, A., and Littlestone, N., </author> <title> Learning Faster than Promised by the Vapnik-Chervonenkis Dimension, </title> <note> Discrete Applied Mathematics 24, 1989, p.47-53. </note>
Reference-contexts: For example, the VC dimension of the class of arbitrary halfspaces in the plane is three, but there exists a sample compression scheme of size two for this class <ref> [BL89] </ref>. The class of arbitrary halfspaces in the plane is neither maximum nor maximal; for some sets of four points in the plane there are less than F 3 (4) = 15 ways to label those four points consistently with some arbitrary halfspace [F89].
Reference: [EHKV87] <author> Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L., </author> <title> A General Lower Bound on the Number of Examples Needed for Learning, </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988, </year> <pages> p. 139-154. </pages>
Reference-contexts: For these classes, a parameter of the class called the Vapnik-Chervonenkis dimension is used to give upper and lower bounds on the sample complexity ([VC71], [BEHW89], <ref> [EHKV87] </ref>). For a concept class C on X, and for S X, let CjS denote the restriction of concept class C to the set S. If CjS = 2 S , then the set S is shattered by C. <p> There are also general lower bounds <ref> [EHKV87] </ref> of W ( 1 * (d + log 1 ffi )) for learning any concept class of VC dimension d.
Reference: [HLW88] <author> Haussler, D., Littlestone, N., and Warmuth, M. K., </author> <title> Lower Bounds on PAC learning and the Size of Epsilon-Nets, </title> <note> unpublished notes. </note>
Reference-contexts: classes of VC dimension d for which there are learning algorithms that produce consistent hypotheses from the same class that require sample size W ( 1 * (d log 1 ffi )). (This essentially follows from lower bounds on the size of *-nets for concept classes of VC dimension d <ref> [PW90, HLW88] </ref>.) Similarly one can show [HLW88] that there are concept classes of VC dimension d with a learning algorithm using a compression scheme of size d that requires the same sample size. <p> which there are learning algorithms that produce consistent hypotheses from the same class that require sample size W ( 1 * (d log 1 ffi )). (This essentially follows from lower bounds on the size of *-nets for concept classes of VC dimension d [PW90, HLW88].) Similarly one can show <ref> [HLW88] </ref> that there are concept classes of VC dimension d with a learning algorithm using a compression scheme of size d that requires the same sample size.
Reference: [F89] <author> Floyd, S., </author> <title> On Space-bounded Learning and the Vapnik-Chervonenkis Dimension, </title> <institution> International Computer Science Institute Technical Report TR-89-061, </institution> <year> 1989. </year>
Reference-contexts: Batch learning algorithms are explored briefly in this paper; on-line learning algorithms are explored in more detail in <ref> [F89] </ref>. The paper is organized as follows. Section 2 reviews pac-learning and the VC dimension. In Section 3 we define the sample compression schemes of size at most k used in this paper. <p> In this paper we ignore computational concerns. Our focuses here are sample size bounds and the combinatorics of concept classes of VC dimension d. Computational issues regarding compression schemes are discussed in <ref> [F89] </ref> and [LSW93]. 3 Sample compression schemes In this section we define a sample compression scheme of size at most k for a concept class C, and we give several examples of such a sample compression scheme. <p> Note that a concept class that is maximum on a finite domain X is also maximal on that set [WW87, pg. 53]. 2 not maximum. More examples can be found in [WW87] and <ref> [F89] </ref>. Recall that a concept c in a class C can be thought of either as a subset S of positive examples from the set X, or as the characteristic function of S on X. <p> Corollary 5.3 extends one part of Corollary 5.2 to a maximum class on an infinite domain X. Corollary 5.2 is extended to any maximum and maximal class on an infinite domain X in <ref> [F89, p.25] </ref>. Corollary 5.2 (W87, p. 10): : Let C 2 X be a maximum concept class of VC dimension d 1 on the finite domain X. Then for x 2X, C fxg is a maximum class of VC dimension d 1 on X-fxg. <p> This class is not maximal, because the concept with no positive examples could be added to C without increasing 4 <ref> [F89] </ref> shows that every maximum class C of VC dimension d on an infinite domain X has a unique extension to a maximum and maximal class of VC dimension d on X. [F89] also shows that if C is both maximum and maximal of VC dimension d on the infinite domain <p> is not maximal, because the concept with no positive examples could be added to C without increasing 4 <ref> [F89] </ref> shows that every maximum class C of VC dimension d on an infinite domain X has a unique extension to a maximum and maximal class of VC dimension d on X. [F89] also shows that if C is both maximum and maximal of VC dimension d on the infinite domain X, then C fxg is maximum and maximal of VC dimension d 1 on X-fxg. 10 the VC dimension of the class. <p> More efficient algorithms for the VC Compression Scheme are explored in <ref> [F89] </ref>. 5.3 A lower bound on the size of a sample compression scheme In this section we show that for a maximum class C 2 X of VC dimension d, if the cardinality of the domain X is exponential in d, then there can be no sample compression scheme of size <p> The class of arbitrary halfspaces in the plane is neither maximum nor maximal; for some sets of four points in the plane there are less than F 3 (4) = 15 ways to label those four points consistently with some arbitrary halfspace <ref> [F89] </ref>. Theorem 5.10: For an arbitrary concept class C of VC dimension d, there is no sample compression scheme of size at most d=5 for sample sets of size at least d. Proof: Let Y be any set of d unlabeled examples. <p> We show that for VC dimensions 2 and 3, a large number of randomly-generated maximal classes are not maximum. There are many natural examples of maximum classes <ref> [F89] </ref>. In spite of the abundance of classes that are maximal but not maximum, we are not aware of a natural example from the literature of a class that is maximal but not maximum. We define a randomly-generated maximal class by the following procedure for randomly generating such classes. <p> The Subset Compression Scheme is motivated by a combinatorial characterization of maximal classes of VC dimension d by forbidden labels on subsets of d + 1 elements that is given in <ref> [F89] </ref>. It is an open question whether the subset compression scheme gives a sample compression scheme of size d for every maximal class of VC dimension d; the subset 18 compression scheme has worked correctly for all of the maximal classes that we have examined. <p> The structure of maximal classes of VC dimension d is discussed further in <ref> [F89] </ref>. <p> Each time that the compression set is changed, the size of the hypothesized axis-parallel rectangle is increased. As a more interesting application of the iterative compression algorithm, <ref> [F89] </ref> discusses classes defined by n-dimensional vector spaces of real functions on some domain X. Such classes include balls in E n1 , positive halfspaces in E n , and positive sets in the plane defined by polynomials of degree at most n 1. <p> Such classes include balls in E n1 , positive halfspaces in E n , and positive sets in the plane defined by polynomials of degree at most n 1. With appropriate restrictions to the domain X <ref> [F89, p.102] </ref>, each of these classes is a maximum class of VC dimension n, and the iterative compression set for each class saves at most n examples at a time. <p> For these classes the iterative compression algorithm is acyclic; there is a partial order on the set of all possible compression sets, and each change of the compression set is to a compression set that is higher in the partial order. <ref> [F89] </ref> contains many open questions concerning the use of iterative compression algorithms for pac-learning for maximum and maximal classes. Finally, there are other definitions of compression schemes that one might consider.
Reference: [F90] <author> Freund, Y., </author> <title> Boosting a weak learning algorithm by majority, </title> <booktitle> Proceedings of the 1990 Workshop on Computational Learning Theory, p. </booktitle> <pages> 202-231., </pages> <month> August </month> <year> 1990. </year>
Reference: [HSW89] <author> Haussler, D., Sloan, R., and Warmuth, M., </author> <title> Learning Nested Differences of Intersection Closed Concept Classes, </title> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1989, </year> <month> p.41-56. </month>
Reference-contexts: The reconstruction function has as a hypothesis the smallest axis-parallel rectangle consistent with these points. This hypothesis is guaranteed to be consistent with the original set of examples. This class is of VC dimension four. 2 Rectangles are one example of an intersection closed concept class. The results of <ref> [HSW89] </ref> lead to compression schemes of size at most d for any intersection closed class of VC dimension d.
Reference: [LSW93] <author> Littlestone, N, Schapire, and Warmuth, M., </author> <title> Hypothesis Schemas, </title> <booktitle> in progress. </booktitle>
Reference-contexts: We give a sample compression scheme that applies for some concept classes that are maximal but not maximum. Recently a compression scheme of size O (d log m) for classes of VC dimension d was presented in <ref> [LSW93] </ref>; this result uses a more general definition of a sample compression scheme, and relies on the boosting schemes of weak learning algorithms [F90][S90]. It remains an open question [LW86] whether there is a sample compression scheme of size O (d) for every class of VC dimension d. <p> We let lnx denote log e x and we let log x denote log 2 x. 1 The original sample complexity bounds of [LW86] are slightly weaker; similar proofs for extended schemes appear in <ref> [LSW93] </ref>. A domain is any set X. The elements of X fi f0; 1g are called examples. Positive examples are examples labeled 1 and negative examples are labeled 0. The elements of X are sometimes called unlabeled examples. <p> In this paper we ignore computational concerns. Our focuses here are sample size bounds and the combinatorics of concept classes of VC dimension d. Computational issues regarding compression schemes are discussed in [F89] and <ref> [LSW93] </ref>. 3 Sample compression schemes In this section we define a sample compression scheme of size at most k for a concept class C, and we give several examples of such a sample compression scheme. <p> It is not likely to exclude any concept class considered in the context of machine learning applications. 3 This paper essentially uses the simplest version of the compression schemes introduced in [LW86]; various more sophisticated schemes are discussed in <ref> [LSW93] </ref>. 3 Example (rectangles): Consider the class of axis-parallel rectangles in E 2 . Each concept cor-responds to an axis-parallel rectangle; the points within the axis-parallel rectangle are labeled `1' (positive), and the points outside the rectangle are labeled `0' (negative). <p> At this point, the discussion of these algorithms is of combinatorial interest, apart from questions of efficiency. Neither the one-pass nor the multiple-pass halving compression algorithm is applicable for an infinite class C. <ref> [LSW93] </ref> gives a compression scheme of size O (d log m) for any (possibly infinite) class of VC dimension d.
Reference: [LW86] <author> Littlestone, N, and Warmuth, M., </author> <title> Relating Data Compression and Learnability, </title> <type> unpublished manuscript, </type> <year> 1986. </year>
Reference-contexts: 1 Introduction In this paper we discuss the use of sample compression schemes within computational learning theory. We define a sample compression scheme of size k for a concept class, consisting of a compression function and a reconstruction function; this formulation of a sample compression scheme was first introduced in <ref> [LW86] </ref>. Given a finite set of labeled examples, the compression function selects a compression set of at most k examples. The reconstruction function uses this compression set to construct a hypothesis for the concept to be learned. <p> Recently a compression scheme of size O (d log m) for classes of VC dimension d was presented in [LSW93]; this result uses a more general definition of a sample compression scheme, and relies on the boosting schemes of weak learning algorithms [F90][S90]. It remains an open question <ref> [LW86] </ref> whether there is a sample compression scheme of size O (d) for every class of VC dimension d. <p> Notation: A-B is used to denote the difference of sets, so A-B is defined as fa 2 A: a 62 Bg. We let lnx denote log e x and we let log x denote log 2 x. 1 The original sample complexity bounds of <ref> [LW86] </ref> are slightly weaker; similar proofs for extended schemes appear in [LSW93]. A domain is any set X. The elements of X fi f0; 1g are called examples. Positive examples are examples labeled 1 and negative examples are labeled 0. The elements of X are sometimes called unlabeled examples. <p> Computational issues regarding compression schemes are discussed in [F89] and [LSW93]. 3 Sample compression schemes In this section we define a sample compression scheme of size at most k for a concept class C, and we give several examples of such a sample compression scheme. Definitions (sample compression schemes) <ref> [LW86] </ref> 3 : A sample compression scheme of size at most k for a concept class C on X consists of two functions, a compression function and a reconstruction function. <p> It is not likely to exclude any concept class considered in the context of machine learning applications. 3 This paper essentially uses the simplest version of the compression schemes introduced in <ref> [LW86] </ref>; various more sophisticated schemes are discussed in [LSW93]. 3 Example (rectangles): Consider the class of axis-parallel rectangles in E 2 . Each concept cor-responds to an axis-parallel rectangle; the points within the axis-parallel rectangle are labeled `1' (positive), and the points outside the rectangle are labeled `0' (negative). <p> The reconstruction function maps the compression set to a hypothesis on X which is the hypothesis of the learning algorithm. Note that this hypothesis is guaranteed to be consistent with all of the examples in the original sample set. Littlestone and Warmuth <ref> [LW86] </ref> gave an upper bound on the sample size needed for a batch learning algorithm for the class C that uses a sample compression scheme of size at most d. 14 Theorem 6.1 (LW86): Let P be any probability distribution on a domain X , c be any concept on X, <p> Finally, there are other definitions of compression schemes that one might consider. In the definition used in this paper the compression function maps every finite set of labeled examples to a subset of at most k labeled examples. (In the original paper <ref> [LW86] </ref> the compression function mapped every finite sequence of labeled examples to a subsequence of at most k labeled examples. The alternate definition is essentially the same.) From the combinatorial point of view the following definition of compression function might be the most interesting. <p> This work developed from results in the unpublished manuscripts <ref> [LW86] </ref> and [W87]; we would like to acknowledge again these contributions from Nick Littlestone and Emo Welzl.
Reference: [PW90] <author> Pach, J., and Woeginger, G., </author> <title> Soem New Bounds for Epsilon-Nets, </title> <booktitle> Proceedings of the Sixth Annual Symposium on Computational Geometry, </booktitle> <address> Berkeley, California, </address> <month> June 6-8, </month> <pages> pp. 10-15, </pages> <year> 1990. </year>
Reference-contexts: classes of VC dimension d for which there are learning algorithms that produce consistent hypotheses from the same class that require sample size W ( 1 * (d log 1 ffi )). (This essentially follows from lower bounds on the size of *-nets for concept classes of VC dimension d <ref> [PW90, HLW88] </ref>.) Similarly one can show [HLW88] that there are concept classes of VC dimension d with a learning algorithm using a compression scheme of size d that requires the same sample size.
Reference: [S72] <author> Sauer, N., </author> <title> On the Density of Families of Sets, </title> <journal> Journal of Comb. Th. </journal> <note> (A) 13, </note> <author> p. </author> <month> 145-147. </month>
Reference-contexts: A concept class is called maximal if adding any concept to the class increases the VC dimension of the class. Let F d (m) be defined as P d i for m d, and as 2 m for m &lt; d. From [VC71], <ref> [S72] </ref>, the cardinality of C is at most F d (m) for any class C of VC dimension d on a domain X of cardinality m.
Reference: [S90] <author> Schapire, R., </author> <title> The strength of weak learnability, </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference: [SAB89] <author> Shawe-Taylor, J., Anthony, M., and Biggs, N., </author> <title> Bounding Sample Size with the Vapnik-Chervonenkis Dimension, </title> <month> November </month> <year> 1989. </year>
Reference-contexts: sample size at least max 4 log ffi 8d log * ; C is learnable by any algorithm that finds a concept c from C consistent with the sample sequence. 2 In Theorem 2.2, the VC dimension essentially replaces lnjCj as a measure of the size of the class C. <ref> [SAB89] </ref> improves the sample size in Theorem 2.2 to 1 ln ffi d 1 + 2 (ln 2 + ln ff for 0 &lt; ff &lt; 1. This is equivalent to 1 * 2 + * d ln *ff 2 (to facilitate comparison with bounds derived later in the paper). <p> Proof: Let 1 * 1 + d + * 1 for 0 &lt; fi &lt; 1, which is equivalent to 1 ln ffi d (1 + ln fi* fi* m ln d) m: (6:1) We use the fact from <ref> [SAB89] </ref> that ln ff 1 + ffm ln m for all ff &gt; 0: d we get d 1 + d By substituting ln m into the left hand side of equation (6.1) we get 1 ln ffi d (1 + ln m ln d) m 1 + d (1 + <p> Proof: This follows from Theorem 6.1 and Lemma 6.2. 2 For maximum classes of VC dimension d, Theorem 6.3 slightly improves the sample complexity of batch learning from the previously known results from [BEHW89] and <ref> [SAB89] </ref> given in Theorem 2.2. Choosing fi = 1=2 gives simple bounds. The bounds can be marginally improved by optimizing the choice of fi as done in 5 [CBFH+93]. <p> Because we have given a suitable sample compression scheme of size d for maximum classes of VC dimension d, this result applies to all maximum classes of VC dimension d. This approach improves on the previously-known sample complexity for pac-learning for maximum classes of VC dimension d [BEHW89] <ref> [SAB89] </ref>. It is an open question whether there is a sample compression scheme of size d, or of size O (d), for every maximal class of VC dimension d.
Reference: [V84] <author> Valiant, L.G., </author> <title> A theory of the learnable, </title> <journal> Comm. ACM, </journal> <volume> 27(11), </volume> <year> 1984, </year> <pages> pp. 1134-42. </pages>
Reference-contexts: In <ref> [V84] </ref>, Valiant introduced a model of learning concepts from examples taken from an unknown distribution.
Reference: [V82] <author> Vapnik, V.N., </author> <title> Estimation of Dependencies based on Empirical Data, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [VC71] <author> Vapnik, V.N. and Chervonenkis, A.Ya., </author> <title> On the Uniform Convergence of Relative Frequencies of Events to their Probabilities, </title> <journal> Th. Prob. and its Appl., </journal> <volume> 16(2), </volume> <year> 1971, </year> <pages> pp. 264-280. </pages>
Reference-contexts: Theorem 2.2 from Blumer, Ehrenfeucht, Haussler, and Warmuth in [BEHW89] gives an upper bound on the sample complexity of learning algorithms in terms of the VC dimension of the class. This result in [BEHW89] is adapted from Vapnik and Chervonenkis in <ref> [VC71] </ref>. Theorem 2.2 (BEHW89): : Let C be a well-behaved 2 concept class. <p> A concept class is called maximal if adding any concept to the class increases the VC dimension of the class. Let F d (m) be defined as P d i for m d, and as 2 m for m &lt; d. From <ref> [VC71] </ref>, [S72], the cardinality of C is at most F d (m) for any class C of VC dimension d on a domain X of cardinality m.
Reference: [W87] <author> Welzl, E., </author> <title> Complete Range Spaces, </title> <note> unpublished notes, </note>
Reference-contexts: Maximal concept classes are classes where no concept can be added without increasing the VC dimension of the class. Maximum classes are in some sense the largest concept classes <ref> [W87] </ref>. We give a sample compression scheme of size d for any maximum concept class of VC dimension d. Further, we show that for any sufficiently large maximum class of VC dimension d, there can be no sample compression scheme of size less than d. <p> Section 5.3 shows that for a maximum class C 2 X of VC dimension d for X sufficiently large, there is no sample compression scheme of size less than d. Definitions (maximum and maximal classes): We use the definitions from <ref> [W87] </ref> of maximum and maximal concept classes. A concept class is called maximal if adding any concept to the class increases the VC dimension of the class. Let F d (m) be defined as P d i for m d, and as 2 m for m &lt; d. <p> Because (CjZ) fxg = (C fxg)jZ, C fxg is maximum of VC dimension d on every finite subset of X-fxg of cardinality at least d. Therefore, C fxg is maximum of VC dimension d on X-fxg. 2 Definitions (the class C A ) <ref> [W87] </ref>: Let C 2 X be a maximum concept class of VC dimension d. <p> fx 1 ,..., x k g, A X, C A is defined as the class (((C fx 1 g ) fx 2 g ):::) fx k g . 2 It is easy to see that for any distinct x,y in X, (C fxg ) fyg = (C fyg ) fxg <ref> [W87, p. 8] </ref>. Therefore for any A X , the class C A is well-defined. Corollary 5.4 (W87): : Let C 2 X be a maximum concept class of VC dimension d on the finite domain X. Let A be any subset of X of cardinality d. <p> Proof: This follows from repeated application of Corollary 5.2. The class C A contains the single concept c on X-A such that c remains a concept in C for any labeling of the elements of A. 2 Definitions (the concept c A ) <ref> [W87] </ref>: For any maximum concept class C 2 X of VC dimension d on the finite domain X, and for any set A X of cardinality d, let c A denote the unique concept in the class C A on the domain X-A. 2 8 Example (at most two positive examples): <p> Theorem 5.6 is also stated, although not with this proof, by Welzl in <ref> [W87, p. 27] </ref>. <p> This work developed from results in the unpublished manuscripts [LW86] and <ref> [W87] </ref>; we would like to acknowledge again these contributions from Nick Littlestone and Emo Welzl.
Reference: [WW87] <author> Welzl, E., and Woeginger, G., </author> <title> On Vapnik-Chervonenkis Dimension One, </title> <type> unpublished manuscript, </type> <year> 1987. </year>
Reference-contexts: Thus a maximum class C restricted to a finite set Y is of maximum size, given the VC dimension of the class. Note that a concept class that is maximum on a finite domain X is also maximal on that set <ref> [WW87, pg. 53] </ref>. 2 not maximum. More examples can be found in [WW87] and [F89]. Recall that a concept c in a class C can be thought of either as a subset S of positive examples from the set X, or as the characteristic function of S on X. <p> Note that a concept class that is maximum on a finite domain X is also maximal on that set [WW87, pg. 53]. 2 not maximum. More examples can be found in <ref> [WW87] </ref> and [F89]. Recall that a concept c in a class C can be thought of either as a subset S of positive examples from the set X, or as the characteristic function of S on X. <p> Any maximum class C on a finite domain X is also a maximal class. However for an infinite domain X for any d 1 there are concept classes of VC dimension d that are maximum but not maximal <ref> [WW87, p. 53] </ref>. This occurs because a maximum class C is defined only as being maximum, and therefore maximal, on finite subsets of X. A maximum class C on X is not required to be maximal on the infinite domain X. <p> Every maximal class of VC dimension 1 is also a maximum class <ref> [WW87] </ref>, but for classes of VC dimension greater than 1, there are maximal classes that are not maximum. (Figures 5.1 and 7.2 show two different classes of VC dimension 2 that are maximal but not maximum.) In this section we discuss randomly-generated maximal classes, and we give a sample compression scheme
References-found: 20

