URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-01/MP-TR-95-01.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-01/
Root-URL: http://www.cs.wisc.edu
Title: Optimization in Machine Learning  
Author: O. L. Mangasarian 
Date: January 1995  
Pubnum: Mathematical Programming Technical Report 95-01  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illi-nois, </address> <year> 1992. </year>
Reference-contexts: In its general form, this problem is again an extremely difficult and nonconvex problem. However, greedy sequential constructions of the planes determining the various polyhedral regions <ref> [22, 25, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm [32, 18, 26]. <p> a zero minimum for (13) with the following solution: (w 1 ; 1 ) = ((2 2); 1); (w 2 ; 2 ) = ((2 2); 1) (14) It is interesting to note that the same solution for the XOR example is given by the greedy multisurface method tree (MSMT) <ref> [1] </ref>. MSMT attempts to separate as many points of A and B as possible by a first plane obtained by solving (6), and then repeats the process for each of the ensuing halfspaces, until adequate separation is obtained.
Reference: [2] <author> K. P. Bennett and E.J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. </title> <note> Department of Mathematical Sciences Report No. 217, </note> <institution> Rensse-laer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1994. </year>
Reference-contexts: research supported by Air Force Office of Scientific Research Grant F49620-94-1-000036 and National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wis-consin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. ber of misclassified points turns out to be NP-complete [11, 17], but we shall indicate effective approaches <ref> [24, 2] </ref> that render it more tractable. For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination [6, 7]. Let A and B be two disjoint point sets in R n with cardinali-ties m and k respectively. <p> In order to circumvent this difficulty, a parametric implicitly exact penalty function was proposed for solving (4) in [24] and implemented successfully in <ref> [2] </ref> by an approach that also identifies outlying misclassified points. A fast hybrid algorithm for approximately solving the misclas-sification minimization problem is also given in [11].
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Hol-land. </publisher>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms <ref> [22, 3, 5, 29, 31] </ref> and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems. <p> Effective methods for solving these problems have been briefly described. For more details, the reader is referred to <ref> [3, 4, 23] </ref>.
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year> <month> 4 </month>
Reference-contexts: We will show that the first leads to an LPEC (linear program with equilibrium constraints) [24, 20] while the second leads to a single linear program <ref> [21, 4] </ref>. <p> Unfortunately, all these attempts [22, 16, 15] contained ad hoc ways for excluding the null solution (w = 0) that plagued a linear programming formulation for linearly inseparable sets. However, the robust model proposed in <ref> [4] </ref>, which 2 consists of minimizing the average of the 1-norm of the distances of misclassified points from the separating plane, completely overcame this difficulty. The linear program [4] proposed is this: minimize w;;y;z m + ez subject to Aw + y = e + e y; z = 0 The <p> However, the robust model proposed in <ref> [4] </ref>, which 2 consists of minimizing the average of the 1-norm of the distances of misclassified points from the separating plane, completely overcame this difficulty. The linear program [4] proposed is this: minimize w;;y;z m + ez subject to Aw + y = e + e y; z = 0 The key property of (6) is that it gives the null solution w = 0 if and only if eA = k in which case w = 0 is <p> MSMT attempts to separate as many points of A and B as possible by a first plane obtained by solving (6), and then repeats the process for each of the ensuing halfspaces, until adequate separation is obtained. For this example, the first plane obtained <ref> [4] </ref> is (w 1 ; 1 ) = ((2 2); 1), which separates f (1; 0)g from f (0; 0); (0; 1); (1; 1)g. <p> Effective methods for solving these problems have been briefly described. For more details, the reader is referred to <ref> [3, 4, 23] </ref>.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilin--ear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms <ref> [22, 3, 5, 29, 31] </ref> and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [6] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Mul-ticategory separation via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 27-39, </pages> <year> 1993. </year>
Reference-contexts: For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination <ref> [6, 7] </ref>. Let A and B be two disjoint point sets in R n with cardinali-ties m and k respectively. Let the m points of A be represented by the m fi p matrix A, while the k points of B be represented by the k fi p matrix B.
Reference: [7] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Serial and parallel multicategory discrimination. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 722-734, </pages> <year> 1994. </year>
Reference-contexts: For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination <ref> [6, 7] </ref>. Let A and B be two disjoint point sets in R n with cardinali-ties m and k respectively. Let the m points of A be represented by the m fi p matrix A, while the k points of B be represented by the k fi p matrix B.
Reference: [8] <author> A. Charnes. </author> <title> Some fundamental theorems of perceptron theory and their geometry. </title> <editor> In J. T. Lou and R. H. Wilcox, editors, </editor> <booktitle> Computer and Information Sciences, </booktitle> <pages> pages 67-74, </pages> <address> Washington, D.C., 1964. </address> <publisher> Spartan Books. </publisher>
Reference-contexts: An important application of the misclassification error (3), is its use in constructing the more complex nonlinear neural network classifier of Section 3 below. 2.2 Minimization of Average Distance of Misclassifications from Separating Plane As early as 1964 <ref> [8, 21] </ref>, the distance of misclassified points from a separating plane was utilized to generate a linear programming problem for obtaining a separating plane (1) that approximately satisfied (2) by minimizing some measure of distance of misclassified points from the plane (1).
Reference: [9] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <type> Technical Report 1191, </type> <institution> Computer Sciences Department, University of Wiscon-sin, Madison, Wisconsin, </institution> <month> November </month> <year> 1993. </year> <note> Mathematical Programming, to appear. Available from ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1191.ps.Z. </note>
Reference-contexts: A fast hybrid algorithm for approximately solving the misclas-sification minimization problem is also given in [11]. Another approach to solving (3) is by utilizing the highly effective smoothing technique <ref> [9, 10] </ref> that has been used to solve many mathematical programs and related problems.
Reference: [10] <author> Chunhui Chen and O. L. Mangasar-ian. </author> <title> A class of smoothing functions for nonlinear and mixed complementarity problems. </title> <type> Technical Report 94-11, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1994. </year> <note> Computational Optimization and Applications, to appear. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-11.ps.Z. </note>
Reference-contexts: A fast hybrid algorithm for approximately solving the misclas-sification minimization problem is also given in [11]. Another approach to solving (3) is by utilizing the highly effective smoothing technique <ref> [9, 10] </ref> that has been used to solve many mathematical programs and related problems.
Reference: [11] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <type> Technical Report 95-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madi-son, Wisconsin, </institution> <month> February </month> <year> 1995. </year> <note> Advances in Computational Mathematics, submitted. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: the num fl This material is based on research supported by Air Force Office of Scientific Research Grant F49620-94-1-000036 and National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wis-consin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. ber of misclassified points turns out to be NP-complete <ref> [11, 17] </ref>, but we shall indicate effective approaches [24, 2] that render it more tractable. For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination [6, 7]. <p> In order to circumvent this difficulty, a parametric implicitly exact penalty function was proposed for solving (4) in [24] and implemented successfully in [2] by an approach that also identifies outlying misclassified points. A fast hybrid algorithm for approximately solving the misclas-sification minimization problem is also given in <ref> [11] </ref>. Another approach to solving (3) is by utilizing the highly effective smoothing technique [9, 10] that has been used to solve many mathematical programs and related problems.
Reference: [12] <author> L. DeSilets, B. Golden, Q. Wang, and R. Kumar. </author> <title> Predicting salinity in the Chesa-peake Bay using backpropagation. </title> <journal> Computers & Operations Research, </journal> <volume> 19 </volume> <pages> 277-285, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound <ref> [12, 30, 18, 13] </ref> . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [13] <author> S.I. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound <ref> [12, 30, 18, 13] </ref> . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [14] <author> G.M. Georgiou. </author> <title> Comments on hidden nodes in neural nets. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 38:1410, </volume> <year> 1991. </year>
Reference-contexts: The h separating planes (7) divide R p into at most t polyhedral regions, where <ref> [14] </ref> t := i=0 h ! We shall assume that F (A) and F (B) are contained in the interiors of two mutually exclusive subsets of these regions.
Reference: [15] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference-contexts: Unfortunately, all these attempts <ref> [22, 16, 15] </ref> contained ad hoc ways for excluding the null solution (w = 0) that plagued a linear programming formulation for linearly inseparable sets.
Reference: [16] <author> R.C. Grinold. </author> <title> Mathematical methods for pattern classification. </title> <journal> Management Science, </journal> <volume> 19 </volume> <pages> 272-289, </pages> <year> 1972. </year>
Reference-contexts: Unfortunately, all these attempts <ref> [22, 16, 15] </ref> contained ad hoc ways for excluding the null solution (w = 0) that plagued a linear programming formulation for linearly inseparable sets.
Reference: [17] <author> David Heath. </author> <title> A geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Johns Hopkins University-Baltimore, Maryland, </institution> <year> 1992. </year>
Reference-contexts: the num fl This material is based on research supported by Air Force Office of Scientific Research Grant F49620-94-1-000036 and National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wis-consin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. ber of misclassified points turns out to be NP-complete <ref> [11, 17] </ref>, but we shall indicate effective approaches [24, 2] that render it more tractable. For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination [6, 7].
Reference: [18] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, Cal-ifornia, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound <ref> [12, 30, 18, 13] </ref> . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems. <p> Another approach to solving (3) is by utilizing the highly effective smoothing technique [9, 10] that has been used to solve many mathematical programs and related problems. In this approach, the step function s () is replaced by the classical sigmoid function of neural networks <ref> [18] </ref>: 1 where ff is a positive real number that approaches +1 for more accurate representation of the step function. With this approximation, the unconstrained discontinuous minimization problem is reduced to an unconstrained continuous optimization problem, that is however nonconvex. <p> Computationally, the LP (6) is very robust, rarely giving rise to the null solution, even in contrived examples where eA = k parlance of machine learning <ref> [18] </ref>, the separating plane (1) is referred to as a "perceptron", "linear threshold unit" or simply "unit", with threshold and incoming arc weight w. <p> However, greedy sequential constructions of the planes determining the various polyhedral regions [22, 25, 1] have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm <ref> [32, 18, 26] </ref>. Online BP is often erroneously referred to as a descent algorithm, which it is not. In this section of the paper we relate the polyhedral regions into which R p is divided, to a neural network with one hidden layer of linear threshold units.
Reference: [19] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: However, any two disjoint point sets in R p can be discriminated between by some polyhedral partition that corresponds to a neural network with one hidden layer with a sufficient number of hidden units <ref> [19, 25] </ref>. We describe now precisely when a specific partition of R p by h separating planes xw i = i ; i = 1; : : : ; h; (7) corresponds to a neural network with h hidden units.
Reference: [20] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.- Q. Wu. </author> <title> Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. </title> <type> Technical Report 275, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton, </institution> <address> Ontario, Hamilton, Ontario L8S 4K1, Canada, </address> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: We shall propose two error measures: one will merely count the number of misclassified points, while the other will measure the average distance of misclassified points from a separating plane. We will show that the first leads to an LPEC (linear program with equilibrium constraints) <ref> [24, 20] </ref> while the second leads to a single linear program [21, 4].
Reference: [21] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: We will show that the first leads to an LPEC (linear program with equilibrium constraints) [24, 20] while the second leads to a single linear program <ref> [21, 4] </ref>. <p> In the simplest model p = n and F is the identity map. However, more complex separation, say by quadratic surfaces <ref> [21] </ref>, can be effected if one resorts to more general maps. (Note that complex separation, like fitting with high degree polynomials, is not always desirable, since it may lead to merely "memorizing" the training set.) The simplest and one of the most effective classifiers in R p is the plane xw <p> An important application of the misclassification error (3), is its use in constructing the more complex nonlinear neural network classifier of Section 3 below. 2.2 Minimization of Average Distance of Misclassifications from Separating Plane As early as 1964 <ref> [8, 21] </ref>, the distance of misclassified points from a separating plane was utilized to generate a linear programming problem for obtaining a separating plane (1) that approximately satisfied (2) by minimizing some measure of distance of misclassified points from the plane (1).
Reference: [22] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms <ref> [22, 3, 5, 29, 31] </ref> and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems. <p> Unfortunately, all these attempts <ref> [22, 16, 15] </ref> contained ad hoc ways for excluding the null solution (w = 0) that plagued a linear programming formulation for linearly inseparable sets. <p> In its general form, this problem is again an extremely difficult and nonconvex problem. However, greedy sequential constructions of the planes determining the various polyhedral regions <ref> [22, 25, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm [32, 18, 26].
Reference: [23] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks <ref> [23] </ref>. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . <p> a neural network with h hidden linear threshold units (with thresholds i , incoming arc weights w i ; i = 1; : : : ; h) and output linear threshold unit (with threshold t and incoming arc weights v i ; i = 1; : : : ; h <ref> [23] </ref>) . This condition is necessary and sufficient for the polyhedral partition 3 of R p in order for it to correspond to a neu-ral network with one layer of hidden units. For more detail and graphical depiction of the neural network, see [23]. "Training" a neural network consists of determining <p> = 1; : : : ; h <ref> [23] </ref>) . This condition is necessary and sufficient for the polyhedral partition 3 of R p in order for it to correspond to a neu-ral network with one layer of hidden units. For more detail and graphical depiction of the neural network, see [23]. "Training" a neural network consists of determining (w i ; i ) 2 R p+1 ; i = 1; : : : ; h; (v; t ) 2 R h+1 ; such that the following nonlinear inequalities are satisfied as best as possible: h X s (Aw i e i <p> Effective methods for solving these problems have been briefly described. For more details, the reader is referred to <ref> [3, 4, 23] </ref>.
Reference: [24] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: We shall propose two error measures: one will merely count the number of misclassified points, while the other will measure the average distance of misclassified points from a separating plane. We will show that the first leads to an LPEC (linear program with equilibrium constraints) <ref> [24, 20] </ref> while the second leads to a single linear program [21, 4]. <p> research supported by Air Force Office of Scientific Research Grant F49620-94-1-000036 and National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wis-consin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. ber of misclassified points turns out to be NP-complete [11, 17], but we shall indicate effective approaches <ref> [24, 2] </ref> that render it more tractable. For the sake of simplicity we shall limit ourselves to discriminating between two sets, although optimization models apply readily to multicategory discrimination [6, 7]. Let A and B be two disjoint point sets in R n with cardinali-ties m and k respectively. <p> The sets F (A) and F (B) are linearly separable in R p , if and only if the minimum of (3) is zero, and no points are misclassified, otherwise the minimum of (3) "counts" the number of misclassified points if the 1-norm is used. In <ref> [24] </ref> it was shown that (3) with the 1-norm is equivalent to the following LPEC: minimize w;;r;u;s;v subject to u + Aw e e = 0 r (u + Aw e e) = 0 u = 0 v Bw + e e = 0 s (v Bw + e e) = <p> In order to circumvent this difficulty, a parametric implicitly exact penalty function was proposed for solving (4) in <ref> [24] </ref> and implemented successfully in [2] by an approach that also identifies outlying misclassified points. A fast hybrid algorithm for approximately solving the misclas-sification minimization problem is also given in [11].
Reference: [25] <author> O. L. Mangasarian, R. Setiono, and W. H. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 22-31, </pages> <address> Philadelphia, Penn-sylvania, </address> <year> 1990. </year> <title> SIAM. </title> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989. </year>
Reference-contexts: In its general form, this problem is again an extremely difficult and nonconvex problem. However, greedy sequential constructions of the planes determining the various polyhedral regions <ref> [22, 25, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm [32, 18, 26]. <p> However, any two disjoint point sets in R p can be discriminated between by some polyhedral partition that corresponds to a neural network with one hidden layer with a sufficient number of hidden units <ref> [19, 25] </ref>. We describe now precisely when a specific partition of R p by h separating planes xw i = i ; i = 1; : : : ; h; (7) corresponds to a neural network with h hidden units.
Reference: [26] <author> O. L. Mangasarian and M.V. Solodov. </author> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2) </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: However, greedy sequential constructions of the planes determining the various polyhedral regions [22, 25, 1] have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm <ref> [32, 18, 26] </ref>. Online BP is often erroneously referred to as a descent algorithm, which it is not. In this section of the paper we relate the polyhedral regions into which R p is divided, to a neural network with one hidden layer of linear threshold units. <p> is used in (13) instead of the 1-norm, and if the step function s is replaced by the sigmoid function in (13), we obtain an error function similar to the error function that BP attempts to find a stationary point for, and for which a convergence proof is given in <ref> [26] </ref>, and stability analysis in [33].
Reference: [27] <author> O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <type> Technical Report 94-10, </type> <institution> Computer Sciences Department, University of Wis-consin, Madison, Wisconsin 53706, </institution> <year> 1994. </year> <note> Operations Research, to appear. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-10.ps.Z. </note>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis <ref> [34, 35, 27] </ref>. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [28] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1969. </year>
Reference-contexts: We note that the clas sical exclusive-or (XOR) example <ref> [28] </ref> for which F is the identity map and A = " 0 1 ; B = 0 0 # , gives a zero minimum for (13) with the following solution: (w 1 ; 1 ) = ((2 2); 1); (w 2 ; 2 ) = ((2 2); 1) (14) It
Reference: [29] <author> S. Mukhopadhyay, A. Roy, and S. Govil. </author> <title> A polynomial time algorithm for generating neural networks for pattern classification: Its stability properties and some test results. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 317-330, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms <ref> [22, 3, 5, 29, 31] </ref> and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [30] <author> K.E. Nygard, P. Juell, and N. Kadaba. </author> <title> Neural networks for selecting vehicle routing heuristics. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4 </volume> <pages> 353-364, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound <ref> [12, 30, 18, 13] </ref> . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [31] <author> A. Roy, L.S. Kim, and S. Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multilayer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-545, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms <ref> [22, 3, 5, 29, 31] </ref> and practical applications in medical diagnosis and prognosis [34, 35, 27]. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [32] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cam-bridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, greedy sequential constructions of the planes determining the various polyhedral regions [22, 25, 1] have been quite successful in obtaining very effective algorithms for training neural networks much faster than the classical online (that is training on one point at a time) backpropagation (BP) gradient algorithm <ref> [32, 18, 26] </ref>. Online BP is often erroneously referred to as a descent algorithm, which it is not. In this section of the paper we relate the polyhedral regions into which R p is divided, to a neural network with one hidden layer of linear threshold units.
Reference: [33] <author> M.V. Solodov and S.K. Zavriev. </author> <title> Stability properties of the gradient projection method with applications to the backpropagation algorithm. </title> <institution> Computer Sciences Department, </institution> <note> Mathematical Programming Technical Report 94-05, </note> <institution> University of Wis-consin, Madison, Wisconsin, </institution> <month> June </month> <year> 1994. </year> <note> SIAM Journal on Optimization, submitted. </note>
Reference-contexts: of the 1-norm, and if the step function s is replaced by the sigmoid function in (13), we obtain an error function similar to the error function that BP attempts to find a stationary point for, and for which a convergence proof is given in [26], and stability analysis in <ref> [33] </ref>.
Reference: [34] <author> W. H. Wolberg and O. L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis <ref> [34, 35, 27] </ref>. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
Reference: [35] <author> W. H. Wolberg, W. N. Street, and O. L. Mangasarian. </author> <title> Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates. </title> <journal> Cancer Letters, </journal> <volume> 77 </volume> <pages> 163-171, </pages> <year> 1994. </year> <month> 6 </month>
Reference-contexts: 1 Introduction Optimization has played a significant role in training neural networks [23]. This has resulted in a number of efficient algorithms [22, 3, 5, 29, 31] and practical applications in medical diagnosis and prognosis <ref> [34, 35, 27] </ref>. Other applications of neural networks abound [12, 30, 18, 13] . In this brief work we focus on a number of problems of machine learning and pose them as optimization problems.
References-found: 35

