URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-96-13.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Email: yair@scp.caltech.edu  
Title: The Message Driven File System: a Network Accessible File System for Fine-Grain Message Passing Multicomputers
Author: Yair Zadik 
Degree: California Institute of Technology In Partial Fulfillment of the Requirements for the Degree of Master of Science  
Date: 1 March 1996  
Abstract: 1 The research described in this report is sponsored primarily by the Advanced Research Projects Agency, ARPA Order number 8176, and monitored by the Office of Naval Research under contract number N00014-91-J-1986. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cao, P., Lim, S. B., Venkataraman, S., and Wilkes, J. </author> <title> The TickerTAIP parallel RAID architecture. </title> <journal> ACM Transactions on Computer Systems 12, </journal> <month> 3 (August </month> <year> 1994), </year> <pages> 236-269. </pages>
Reference-contexts: Thus, redundancy for reliability is not present in the current MDFS design, but it could easily be incorporated by introducing a new layer either in software above the block I/O (like in <ref> [1] </ref>) or cache layers, or in hardware by replacing each disk with a RAID array and a faster driver implementation. The chief difficulty in a software implementation would be minimizing the impact of generating redundancy codes on write performance, both by optimizing communications and processor use.
Reference: [2] <author> Chen, P. M., Lee, E. K., Gibson, G. A., Katz, R. H., and Patterson, D. A. </author> <title> RAID: high-performance, reliable secondary storage. </title> <journal> ACM Computing Surveys 26, </journal> <month> 2 (June </month> <year> 1994), </year> <pages> 145-185. </pages>
Reference-contexts: More recently, efforts have been made to increase disk bandwidth by exploiting 39 40 CHAPTER 5. RELATED RESEARCH parallelism in arrays of disks. The basic idea, called striping, involves distributing requests among disks so that multiple requests and multiple portions of large requests are serviced simultaneously <ref> [2] </ref>. MDFS uses this technique to improve block I/O bandwidth. Gibson [8] noted that one could exploit striping in disk arrays to improve reliability by writing redundant stripes to disks. The resulting array is known as a Redundant Array of Inexpensive Disks (RAID). <p> The resulting array is known as a Redundant Array of Inexpensive Disks (RAID). He presents various methods for accomplishing this, each with its own performance, cost, and reliability trade-offs and labels each with a RAID level. An updated version of this discussion is present in <ref> [2] </ref>. RAID techniques increase space requirements to improve reliability. Additionally, some RAID methods can exploit redundancy to improve disk bandwidth for reads. However, without hardware support, RAID techniques have lower write performance than simple striping.
Reference: [3] <author> Corbett, P., Feitelson, D., Hsu, Y., Prost, J.-P., Snir, M., Fineberg, S., Nitzberg, B., Traversat, B., and Wong, P. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Tech. Rep. </type> <institution> NAS-95-002, NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: These attempts range from simple extensions to the Unix model (as in [6]) to complete reworking of the system model, as in ELFS [9], the Alloc Stream Facility [12], and the forthcoming MPI-IO standard <ref> [3] </ref>. ELFS replaces the Unix's untyped byte stream files with typed object based files.
Reference: [4] <author> Dahlin, M., Anderson, T., Patterson, D., and Wang., R. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings 1994 Operating Systems: Design and Implementation Conference (November 1994), </booktitle> <pages> pp. 267-280. </pages>
Reference-contexts: Also, caching must be done at the block level since file replication requires local disk. Some work has been done on distributed caches, similar to the one in MDFS, but in a network of workstations framework. Dahlin, et. al. <ref> [4] </ref> assume a fast communications substrate and simulate various distributed cache algorithms using synthetic workloads. Such simulations prove the value of a distributed cache over strictly local caches when communication is fast.
Reference: [5] <author> Dally, W. J., et al. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Information Processing 89 (North Holland, IFIP, </booktitle> <year> 1989), </year> <editor> G. X. Ritter, Ed., </editor> <publisher> Elsevier Science Publishers B.V. </publisher>
Reference-contexts: Introduction This thesis describes an experimental message-driven file system and its implementation on a 512-computer J-machine with thirty-two, 402Mb SCSI disks. The J-machine <ref> [5] </ref> is an architectural experiment which focuses on the evaluation of hardware support for concurrent execution and active messages. The hardware is designed to scale to many thousands of fine-grain nodes, each of which has a relatively small local memory (1Mb) but fast communications.
Reference: [6] <author> DeBenedictis, E., and del Rosario, J. M. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC) (April 1992), </booktitle> <pages> pp. 0117-0124. </pages>
Reference-contexts: This work is based on the assumption that Unix and/or C Standard Library file models no longer match the hardware models adequately, especially in the case of parallel computing. These attempts range from simple extensions to the Unix model (as in <ref> [6] </ref>) to complete reworking of the system model, as in ELFS [9], the Alloc Stream Facility [12], and the forthcoming MPI-IO standard [3]. ELFS replaces the Unix's untyped byte stream files with typed object based files.
Reference: [7] <author> Deitel, H. M. </author> <title> An Introduction to Operating Systems, 2nd ed. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1990. </year>
Reference-contexts: A more extensive discussion of disk-arm scheduling algorithms can be found in most good operating systems texts (for example <ref> [7] </ref>). MDFS does not use these techniques since the request queues for each disk are relatively short. For disk arm scheduling to be effective, a significant number of requests for one disk must accumulate during the processing of a request for that disk.
Reference: [8] <author> Gibson, G. A. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> ACM Distinguished Dissertations. MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: RELATED RESEARCH parallelism in arrays of disks. The basic idea, called striping, involves distributing requests among disks so that multiple requests and multiple portions of large requests are serviced simultaneously [2]. MDFS uses this technique to improve block I/O bandwidth. Gibson <ref> [8] </ref> noted that one could exploit striping in disk arrays to improve reliability by writing redundant stripes to disks. The resulting array is known as a Redundant Array of Inexpensive Disks (RAID).
Reference: [9] <author> Grimshaw, A. S., and Loyot, Jr., E. C. </author> <title> ELFS: object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems (1991), p. </booktitle> <volume> 177. 45 46 BIBLIOGRAPHY </volume>
Reference-contexts: These attempts range from simple extensions to the Unix model (as in [6]) to complete reworking of the system model, as in ELFS <ref> [9] </ref>, the Alloc Stream Facility [12], and the forthcoming MPI-IO standard [3]. ELFS replaces the Unix's untyped byte stream files with typed object based files.
Reference: [10] <author> Hewlett Packard. </author> <title> HP C2230 Series 3.5-inch SCSI Disk Drives Technical Reference Manual, </title> <type> 2 ed., </type> <month> June </month> <year> 1991. </year>
Reference-contexts: The disk drives used have average seek time of 12.6 milliseconds <ref> [10] </ref>. Given the 64k stripes used, random disk seeks should limit transfer rate to approximately 5Mb/s when transferring whole stripes. Sustained transfer rates through the internal disk controller vary from 1Mb/s to 1.6Mb/s depending on the location accessed on disk, and maximum SCSI bus transfer rates are quoted at 10Mb/s.
Reference: [11] <author> Howard, J. H., et al. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Science 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: This makes a variety of applications immediately available. Many of the issues confronting implementation of a parallel file system are also relevant to distributed file systems. For instance, the caching issues we confronted are also present in the Andrew File System <ref> [11] </ref>, Sprite [16], and LOCUS [17]. Most of this work assumes workstation-like nodes with large memories, slow communications, and local disks. Thus, many of the choices are not appropriate to a fine-grain environment like the J-machine. <p> The NFS layer of MDFS could be extended to support this interface, or replaced with an alternate interface (such as AFS <ref> [11] </ref>) as needed. 42 CHAPTER 5. RELATED RESEARCH Chapter 6 Conclusions This work demonstrates that it is possible to construct scalable fine-grain parallel code in a structured, easily understood manner by using the techniques described.
Reference: [12] <author> Krieger, O., Stumm, M., and Unrau, R. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <booktitle> IEEE Computer 27, </booktitle> <month> 3 (March </month> <year> 1994), </year> <pages> 75-82. </pages>
Reference-contexts: These attempts range from simple extensions to the Unix model (as in [6]) to complete reworking of the system model, as in ELFS [9], the Alloc Stream Facility <ref> [12] </ref>, and the forthcoming MPI-IO standard [3]. ELFS replaces the Unix's untyped byte stream files with typed object based files.
Reference: [13] <author> Maskit, D., and Taylor, S. </author> <title> A message-driven programming system for fine-grain multicomputers. </title> <booktitle> Software Practice and Experience 24 (1994), </booktitle> <pages> 953-980. </pages>
Reference-contexts: Having multiple disk nodes also leads to a higher bandwidth as each disk node has its own connection to the main mesh. 2.2 Message Driven C More complete information on the Message Driven C (MDC) programming system is available in <ref> [13] </ref> and [14]. MDC makes available to the C programmer many of the hardware features of the MDP. MDC allows multiple processes per node with all processes on one MDP node sharing a global address space. MDC does not provide for address space sharing among nodes. <p> Thus, compared to modern chips, it runs at a low clock rate and does not have enough registers or on-chip cache. MDC increases these liabilities somewhat by attempting to support multiple processes without a memory management unit: MDC adopts a heap-based rather than stack based architecture <ref> [13] </ref>, thus making the implementation of dynamic memory allocation very important. Even with these deficiencies, we would still expect that a highly tuned SCSI implementation should achieve at least 50% of peek bandwidth. 4.3 Block I/O performance results Performance was measured using a 512-node J-machine running at 20MHz.
Reference: [14] <author> Maskit, D., Taylor, S., and Zadik, Y. </author> <title> System tools for the J-Machine. </title> <institution> Department of Computer Science Technical Report CS-TR-93-12, California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Having multiple disk nodes also leads to a higher bandwidth as each disk node has its own connection to the main mesh. 2.2 Message Driven C More complete information on the Message Driven C (MDC) programming system is available in [13] and <ref> [14] </ref>. MDC makes available to the C programmer many of the hardware features of the MDP. MDC allows multiple processes per node with all processes on one MDP node sharing a global address space. MDC does not provide for address space sharing among nodes.
Reference: [15] <author> NCR Microelectronics Division. </author> <title> NCR 53C90 Enhanced SCSI Processor Data Manual, </title> <editor> revision 3.0 ed., </editor> <month> March </month> <year> 1989. </year>
Reference-contexts: Sustained transfer rates through the internal disk controller vary from 1Mb/s to 1.6Mb/s depending on the location accessed on disk, and maximum SCSI bus transfer rates are quoted at 10Mb/s. The SCSI controller used in the disk interface cards can achieve a maximum transfer rate of 5Mb/s <ref> [15] </ref>. Thus sustained disk transfer should peak at 4Mb/s if all disks can be kept busy. Due to the architecture of the MDP, interprocessor communications are actually limited by memory bandwidth.
Reference: [16] <author> Nelson, M., et al. </author> <title> Caching in the sprite network file system. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988). </year>
Reference-contexts: This makes a variety of applications immediately available. Many of the issues confronting implementation of a parallel file system are also relevant to distributed file systems. For instance, the caching issues we confronted are also present in the Andrew File System [11], Sprite <ref> [16] </ref>, and LOCUS [17]. Most of this work assumes workstation-like nodes with large memories, slow communications, and local disks. Thus, many of the choices are not appropriate to a fine-grain environment like the J-machine.
Reference: [17] <author> Popek, G., and Walker, B. J. </author> <title> The LOCUS distributed system architecture. </title> <publisher> MIT Press series in computer systems. MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1985. </year>
Reference-contexts: This makes a variety of applications immediately available. Many of the issues confronting implementation of a parallel file system are also relevant to distributed file systems. For instance, the caching issues we confronted are also present in the Andrew File System [11], Sprite [16], and LOCUS <ref> [17] </ref>. Most of this work assumes workstation-like nodes with large memories, slow communications, and local disks. Thus, many of the choices are not appropriate to a fine-grain environment like the J-machine.
Reference: [18] <author> Sun Microsystems, Inc. NFS: </author> <title> Network file system protocol specification. Request For Comment 1094, </title> <month> March </month> <year> 1989. </year>
Reference-contexts: Ideally, the user could manipulate files and transfer them to and from the J-machine using familiar commands implemented using industry standard protocols. To meet these goals, MDFS implements a Network File System (NFS) <ref> [18] </ref> compatible interface. MDFS is written so as to be optimized for another common characteristic of scientific codes: checkpointing. Typical scientific code will write intermediate results to disk from time to time so that long jobs can be stopped and later restarted. <p> The server needs to maintain no implicit client state to fulfill a request. For efficiency, servers may cache information to optimize common requests, but such behavior is hidden from the client. This approach is similar to the one employed in NFS <ref> [18] </ref>. While NFS uses this approach to increase reliability, MDFS uses it to minimize interdependent requests: requests can be issued in parallel without concern for their affects on hidden state.
Reference: [19] <author> Sun Microsystems, Inc. </author> <title> NFS version 3 protocol specification. Request For Comment 1813, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: Our choice of NFS as a network protocol was largely based on its availability on all of our workstation platforms. There has been recent work on improving the NFS protocol for greater client cache efficiency, file lengths of greater than two gigabytes, and better support for non-Unix file systems <ref> [19] </ref>. The NFS layer of MDFS could be extended to support this interface, or replaced with an alternate interface (such as AFS [11]) as needed. 42 CHAPTER 5.
References-found: 19

