URL: http://www.ai.mit.edu/people/cohn/SAL95/Contributions/ring.ps
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Mark.Ring@gmd.de  
Title: Finding Promising Exploration Regions by Weighting Expected Navigation Costs continuous environments, some first-order approximations to
Author: Mark B. Ring 
Keyword: Tasks with Distance Relationships  Weighting Actions by Exploration Benefit and Navigation Costs  
Note: In  has been done (Cohn 1994; Linden Weber 1993; Schmidhuber 1991; Thrun Moller 1992). In these  Associated  
Address: Schlo Birlinghoven D-53 754 Sankt Augustin Germany  
Affiliation: Research Group for Adaptive Systems GMD German National Research Center for Information Technology  
Abstract: In many learning tasks, data-query is neither free nor of constant cost. Often the cost of a query depends on the distance from the current location in state space to the desired query point. This is easiest to visualize in robotics environments where a robot must physically move to a location in order to learn something there. The cost of this learning is the time and effort it takes to reach the new location. Furthermore, this cost is characterized by a distance relationship: When the robot moves as directly as possible from a source state to a destination state, the states through which it passes are closer (i.e., cheaper to reach) than is the destination state. Distance relationships hold in many real-world non-robotics tasks also | any environment where states are not immediately accessible. Optimizing the performance of a chemical plant, for example, requires the adjustment of analog controls which have a continuum of intermediate states. Querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility. In discrete environments with small numbers of states, it's possible to keep track of precisely where and to what degree learning has already been done sufficiently and where it still needs to be done. It is also possible to keep best known estimates of the distances from each state to each other (see Kaelbling, 1993). Kael-bling's DG-learning algorithm is based on Floyd's all-pairs shortest-path algorithm (Aho, Hopcroft, & Ull-man 1983) and is just slightly different from that used here. These "all-goals" algorithms (after Kaelbling) can provide a highly satisfying representation of the distance/benefit tradeoff. where E x is the exploration value of state x (the potential benefit of exploring state x), D xy is the distance to state y, and A xy is the action to take in state x to move most cheaply to state y. This information can be learned incrementally and completely : That is, it can be guaranteed that if a path from any state x to any state y is deducible from the state transitions seen so far, then (1) the algorithm will have a non-null entry for S xy (i.e., the algorithm will know a path from x to y), and (2) The current value for D xy will be the best deducible value from all data seen so far. With this information, decisions about which areas to explore next can be based on not just the amount to be gained from such exploration but also on the cost of reaching each area together with the benefit of incidental exploration done on the way. Though optimal exploration is NP-hard (i.e., it's at least as difficult as TSP) good approximations are easily computable. One such good approximation is to take the action at each state that leads in the direction of greatest accumulated exploration benefit: 
Abstract-found: 1
Intro-found: 0
Reference: <author> Aho, A. V.; Hopcroft, J. E.; and Ullman, J. D. </author> <year> 1983. </year> <title> Data Structures and Algorithms. </title> <booktitle> Addison-Wesley Series in Computer Science and Information Processing. </booktitle> <publisher> Addison-Wesley. </publisher>
Reference: <author> Cohn, D. </author> <year> 1994. </year> <title> Neural network exploration using optimal experiment design. </title> <editor> In Cowan, J. D.; Tesauro, G.; and Alspector, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> 679-686. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Kaelbling, L. P. </author> <year> 1993. </year> <title> Learning to achieve goals. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1094-1098. </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Linden, A., and Weber, F. </author> <year> 1993. </year> <title> Implementing inner drive through competence reflection. </title> <editor> In Meyer, J. A.; Roitblat, H.; and Wilson, S., eds., </editor> <booktitle> From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> 321-326. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> 1991. </year> <title> Adaptive confidence and adaptive curiosity. </title> <type> Technical Report FKI-149-91 (revised), </type> <institution> Technische Universitat Munchen, Institut fu Informatik. </institution>
Reference: <author> Thrun, S. B., and Moller, K. </author> <year> 1992. </year> <title> Active exploration in dynamic environments. </title> <editor> In Moody, J. E.; Hanson, S. J.; and Lippman, R. P., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 531-538. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
References-found: 6

