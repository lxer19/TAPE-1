URL: http://polaris.cs.uiuc.edu/~hoefling/thesis.ps
Refering-URL: http://polaris.cs.uiuc.edu/~hoefling/pubs.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Jay Philip Hoeflinger 
Date: 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: In Fortran, it is written as: A (expr, expr, , expr), while in C or C++, it is written as: A [expr][expr] [expr]. A program typically uses loops nested around array references. Each loop typically has a basic induction variable <ref> [1] </ref> associated with it, which from now on will be called the loop index. <p> It is safe to assume a single loop index for each loop, because if there are multiple induction variables being incremented within a loop, one can always be designated as the basic induction variable, and then used to express the values of all other induction variables in the loop <ref> [1] </ref>. If no induction variable exists within a loop in the original program, one can always be added by assigning a value of zero to it immediately before the loop entry point and incrementing it by 1 immediately after the loop entry point (inside the loop). <p> Identifying loops in the program may be done with interval analysis <ref> [1] </ref>. Many languages have explicit looping constructs, such as the do loop in Fortran, which makes identifying those loops easy. <p> When this is the case, it is always possible to transform such a program into a program with a nested structure by a technique known as node-splitting <ref> [1] </ref>. Programs with an irreducible flow graph are believed to be very rare [1], so this is not expected to be a problem. After interval analysis, the program is a collection of nested loops and if statements. <p> When this is the case, it is always possible to transform such a program into a program with a nested structure by a technique known as node-splitting <ref> [1] </ref>. Programs with an irreducible flow graph are believed to be very rare [1], so this is not expected to be a problem. After interval analysis, the program is a collection of nested loops and if statements. <p> The program situations requiring proof of logical implication, which Tu [38] proposed specifically for array privatization, are now handled as a natural part of MCA, to the benefit of all analysis. The analysis necessary to determine whether last value assignment needs to be done is essentially liveness analysis <ref> [1] </ref>. It must be determined whether a privatized variable is live immediately after the loop. 6.2.4 Reduction Analysis The general idea of reduction recognition is discussed in Section 2.2.2. <p> This is essentially the same as the traditional liveness <ref> [1] </ref> calculation. The purpose of the ReadNext summary set is to determine which private locations need to have a last value copy inserted for them.
Reference: [2] <author> S. Amarasinghe. </author> <title> Parallelizing Compiler Techniques Based on Linear Inequalities. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: The PIPS compiler also uses an algorithm to handle some of the simpler array reshaping situations. Work on the SUIF system at Stanford uses a very similar representation <ref> [2, 23, 28] </ref> to that of PIPS, and has a more sophisticated algorithm to handle the general problem of array reshaping.
Reference: [3] <author> J. Backus. </author> <title> The History of FORTRAN I, II, and III. </title> <journal> Annals of the History of Computing, </journal> <volume> 1(1), </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: A pass was defined as a traversal of the source program, or its machine representation. The first Fortran compiler <ref> [3] </ref> was developed at IBM in this way in the mid-1950s. At that time it was largely unknown what techniques would be necessary to develop the compiler, so they broke up the task to make it manageable.
Reference: [4] <author> V. Balasundaram and K. Kennedy. </author> <title> A Technique for Summarizing Data Access and its Use in Parallelism Enhancing Transformations. </title> <booktitle> Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Both the PIPS and SUIF projects use the linear constraint-based representation as the basis for doing operations such as array privatization and locality analysis, in addition to dependence analysis. Balasundaram and Kennedy <ref> [4] </ref> proposed a technique, involving Data Access Descriptors (DADs), which uses a set of linear constraints applied to an array reference, but only allows certain forms for the constraints.
Reference: [5] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachussats, </address> <year> 1993. </year>
Reference-contexts: The dimension-indices act as the loop indices of a set of nested loops, generating the subscripting offset sequence for the array reference. The original dimension-indices represent a normalized form for the loop indices in the program. Loop normalization <ref> [5] </ref> is a well-known technique used to produce a new loop index which starts at 0 and proceeds to an upper bound in steps of 1.
Reference: [6] <author> Utpal Banerjee. </author> <title> Dependence Analysis. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1997. </year>
Reference-contexts: Dependence analysis was turned into an equation-solving activity by researchers such as David Kuck, Yochi Muraoka, Ross Towle, and Utpal Banerjee <ref> [6] </ref> at Illinois in the early 1970s. They proposed equating the subscript expressions of two array reference sites, solving the Diophantine equations which result, and then applying the constraints derived from the loop bounds to determine whether the solutions were feasible.
Reference: [7] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, W. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The use of this combination of filters was found by Blume to be essentially as powerful, on a set of benchmark codes, as was the same filters with the Omega Test substituted for the Range Test. 2.4 Polaris Results The results (shown in Figure 2.1) of studies <ref> [7] </ref> using the Polaris compiler has proven that the combination of transformation and analysis techniques implemented in it are sufficient to get good speedups on a set of benchmark codes. This is used as evidence that those techniques form a good basis for a parallelizing compiler.
Reference: [8] <author> William Blume. </author> <title> Symbolic Analysis Techniques for Effective Automatic Parallelization. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Sophisticated comparison routines make use of the range dictionaries to determine whether one expression is less than, equal to, or greater than another expression. 2.3.2 Interprocedural Symbolic Analysis Interprocedural value propagation <ref> [8] </ref> is used within Polaris. It is a version of a standard constant propagation algorithm in which the value being propagated is a symbolic expression. <p> (I-1 + (J-1)*N) . . . end do The information can be recovered by clever use of range information for the program variables, but the code is definitely not as easy to parallelize in its inlined form. 2.3.4 Improved Data Dependence Analysis A data dependence test called the Range Test <ref> [8] </ref> was implemented in Polaris. It uses the symbolic expression manipulation package and the symbolic range information heavily. The Range Test tests a pair of array references within a loop nest to determine whether they access the same memory locations. It uses two tests to determine this. <p> The elimination of dead code is always useful in that it cuts down the amount of processing which must be done. Interprocedural value propagation <ref> [8] </ref> propagates expressions from definition points to use points interprocedu-rally. <p> A conversion to Static Single Assignment (SSA) form [15] embeds the def-use chains for scalar variables in the names of the variables, which supports the symbolic analysis process by making clear whether two uses of the same variable hold the same value or may not. Range propagation <ref> [8] </ref> stores value ranges for scalar variables, to aid in symbolic analysis. 86 6.2.3 Privatization As described in Section 2.2.1, privatization is the transformation by which dependences involving certain variables are removed, by making one copy of the variable per processor. <p> This is the algorithm which uses the summary sets of ARDs produced by MCA to parallelize programs by combining privatization, reduction and induction analysis. It is shown, through argument and experiment, to be more powerful than a combination of the Range Test <ref> [8] </ref> and the GCD Test. The various components of MCA are able to generate conditions when they cannot be certain of their analysis. These conditions can be combined within the code generation pass for doing deeper, demand-driven analysis, or for generating a runtime dependence test.
Reference: [9] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic Range Propagation. </title> <booktitle> Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The interprocedural component is split into interprocedural value propagation and subroutine inlining. This is complemented by intrapro-cedural range propagation and powerful expression simplification and comparison routines. 2.3.1 Intraprocedural Symbolic Analysis The intraprocedural range propagation algorithm <ref> [9] </ref> uses a data-flow based technique to discover lower and upper bounds on the values of program variables through the use of widening and narrowing operators. The variable ranges are stored in range dictionaries, which relate the value ranges to statements in the program.
Reference: [10] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Prior to this thesis, only Blume's Range Test <ref> [10] </ref> could handle non-affine subscript expressions. Subscripted-subscript expressions are well-known as being difficult for a compiler to analyze. They fit in the above classification in category two, unless the subscripting array is read from an input file. <p> The Polaris project at the University of Illinois has used full symbolic triplet notation to represent array accesses within its array privatization analysis [36] and its dependence analysis <ref> [10] </ref>. Researchers at the University of Minnesota have used Guarded Array Regions [20] which are equivalent to Bounded RSDs, with an additional predicate (guard).
Reference: [11] <author> M. Burke and R. Cytron. </author> <title> Interprocedural Dependence Analysis and Parallelization. </title> <booktitle> Pro--ceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The typical set operations (intersection, union, etc) are defined on DADs. 3.2 Reference List-based Techniques Li and Yew proposed Atom Images [26] and Burke and Cytron proposed the conversion of all arrays to a single declared dimension and all subscript expressions to a linearized form <ref> [11] </ref>. The reference-list techniques rely on making a list of each individual array reference in the code section. These methods retain all the precision of the original program because they keep all relevant information about each access, but they were not designed to summarize this 18 information.
Reference: [12] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of Interprocedural Side Effects in a Parallel Programming Environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: This observation implies that most references in a program are simple, and for these references the representation is exact. When the representation is exact, a compiler can use simple set operations (union, intersection, etc) without losing any precision. Researchers at Rice University <ref> [12, 24] </ref> have devised several variants of regular section descriptors (RSDs), with operations defined on a lattice. RSDs are of the form I + ff, where I is a loop index and ff is a loop invariant.
Reference: [13] <author> B. Creusillet and F. Irigoin. </author> <title> Interprocedural Array Region Analyses. </title> <booktitle> In Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, New York, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Similar studies have been done at Stanford [21]. Researchers at Stanford [22], and Minnesota [20], plus the PIPS group at Ecole des mines de Paris <ref> [13, 14] </ref> and the Parafrase-2 group at Illinois [31], have implemented compilers including the same basic transformations found to be important in the Cedar Project. The results have been similar enough to form a general consensus as to which analysis techniques are important to provide in a parallelizing compiler. <p> It also eliminates the requirement that the linear constraints form a convex hull, avoiding one source of precision loss. The PIPS project at Ecole des mines de Paris <ref> [13, 14] </ref> has added an indicator of the accuracy of the representation to the representation itself. When linear system manipulations must approximate, sometimes it is appropriate to under-approximate (MUST analysis), while at other times it is appropriate to over-approximate (MAY analysis).
Reference: [14] <author> B. Creusillet and F. Irigoin. </author> <title> Exact vs. Approximate Array Region Analyses. </title> <booktitle> In Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Similar studies have been done at Stanford [21]. Researchers at Stanford [22], and Minnesota [20], plus the PIPS group at Ecole des mines de Paris <ref> [13, 14] </ref> and the Parafrase-2 group at Illinois [31], have implemented compilers including the same basic transformations found to be important in the Cedar Project. The results have been similar enough to form a general consensus as to which analysis techniques are important to provide in a parallelizing compiler. <p> It also eliminates the requirement that the linear constraints form a convex hull, avoiding one source of precision loss. The PIPS project at Ecole des mines de Paris <ref> [13, 14] </ref> has added an indicator of the accuracy of the representation to the representation itself. When linear system manipulations must approximate, sometimes it is appropriate to under-approximate (MUST analysis), while at other times it is appropriate to over-approximate (MAY analysis).
Reference: [15] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently Computing Static Single Assignment Form and the Control Dependence Graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The elimination of dead code is always useful in that it cuts down the amount of processing which must be done. Interprocedural value propagation [8] propagates expressions from definition points to use points interprocedu-rally. A conversion to Static Single Assignment (SSA) form <ref> [15] </ref> embeds the def-use chains for scalar variables in the names of the variables, which supports the symbolic analysis process by making clear whether two uses of the same variable hold the same value or may not.
Reference: [16] <author> G. Dantzig and B.Eaves. </author> <title> Fourier-Motzkin Elimination and its Dual. </title> <journal> Journal of Combinatorial Theory, </journal> <pages> pages 288-297, </pages> <year> 1973. </year>
Reference-contexts: When a potential dependence between two array references was being tested, the linear inequalities associated with the two references were combined to form a linear system and the feasibility of the system was tested using Fourier-Motzkin elimination <ref> [16] </ref> techniques. The other principle technique for doing interprocedural analysis at that time was inline-expansion of subroutines (subroutine inlining) [25].
Reference: [17] <author> R. Eigenmann, J. Hoeflinger, and D. Padua. </author> <title> On the Automatic Parallelization of the Perfect Benchmarks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 5-23, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: The conditions can also be used to assist in generating conditional prefetch commands. 5 2 EXISTING COMPILER ANALYSIS TECHNIQUES Much research has focused on the question of what techniques are important to implement in a compiler. One of the first such studies <ref> [17] </ref> was done at Illinois as part of the Cedar project, by a group of people of which the author was a member. Similar studies have been done at Stanford [21]. <p> The Perfect Benchmark codes were hand-parallelized in a mechanical manner, using only information and techniques which might realistically be available to a compiler. The results of this study, presented in <ref> [17] </ref>, were that the transformation techniques with the most overall impact across all codes were * array privatization * parallel reductions 6 * generalized induction variable (GIV) analysis, and * transformations which map loops to the machine. <p> This new compiler was called Polaris. After the Polaris compiler was built, studies of the performance resulting from its par-allelization <ref> [17] </ref> confirmed what the hand analysis had predicted significant speedups were possible from automatic analysis alone. 2.2 The Transformation Techniques The following subsections describe the transformation techniques, implemented in Polaris, which are relevant to this thesis. 2.2.1 Array Privatization The term privatization refers to the transformation of certain variables from being
Reference: [18] <author> Dennis Gannon and Ko-Yang Wang. </author> <title> Using AI Techniques to Resturcture Programs for Different Parallel Architectures. </title> <editor> In Kai Hwang; Douglas Degroot, editor, </editor> <booktitle> AI and Supercomputing Systems, </booktitle> <year> 1987. </year>
Reference-contexts: This can lead to two different routines to do the same job, or duplicated code which is run repeatedly for each pass. Some attempts have been made to take an "artificial intelligence" approach to combining compiler analyses <ref> [18] </ref>, allowing the form of the program to direct the application of the compiler's techniques.
Reference: [19] <author> J. Grout. </author> <title> Inline Expansion for the Polaris Research Compiler. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Second, because code is copied, the size of the source code increases, sometimes becoming very large. For these reasons, inlining has not been a practical option in general. Recently, as computers became faster, main memory and disk larger, inlining was investigated again <ref> [19] </ref>, but much the same conclusions were reached. Even for moderately-sized programs, the memory and compile time requirements can grow too large. <p> Whenever the values for formal parameters are determined for a particular subroutine, assignment statements are inserted inside the routine, at the subroutine entry point, assigning those values to the formal parameters. 11 2.3.3 Subroutine Inlining Subroutine inlining <ref> [19] </ref> is the general technique of copying the body of a subroutine to its call sites, translating references to formal parameters into references to its actual parameters.
Reference: [20] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Symbolic Array Dataflow Analysis for Array Privatization and Program Parallelization. </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: One of the first such studies [17] was done at Illinois as part of the Cedar project, by a group of people of which the author was a member. Similar studies have been done at Stanford [21]. Researchers at Stanford [22], and Minnesota <ref> [20] </ref>, plus the PIPS group at Ecole des mines de Paris [13, 14] and the Parafrase-2 group at Illinois [31], have implemented compilers including the same basic transformations found to be important in the Cedar Project. <p> The Polaris project at the University of Illinois has used full symbolic triplet notation to represent array accesses within its array privatization analysis [36] and its dependence analysis [10]. Researchers at the University of Minnesota have used Guarded Array Regions <ref> [20] </ref> which are equivalent to Bounded RSDs, with an additional predicate (guard). More information can be added to the guard to sharpen the accuracy in a given situation. 19 4 THE ACCURACY OF TRIPLET NOTATION Descriptors representing memory accesses are the raw material used by compiler algorithms.
Reference: [21] <author> M. Hall, J. Anderson, S. Amarasinghe, B. Murphy, S. Liao, E. Bugnion, and M. Lam. </author> <title> Maximizing Multiprocessor Performance with the SUIF Compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year> <month> 137 </month>
Reference-contexts: One of the first such studies [17] was done at Illinois as part of the Cedar project, by a group of people of which the author was a member. Similar studies have been done at Stanford <ref> [21] </ref>. Researchers at Stanford [22], and Minnesota [20], plus the PIPS group at Ecole des mines de Paris [13, 14] and the Parafrase-2 group at Illinois [31], have implemented compilers including the same basic transformations found to be important in the Cedar Project.
Reference: [22] <author> M. Hall, B. Murphy, S. Amarasinghe, S. Liao, and M. Lam. </author> <title> Interprocedural Analysis for Parallelization. </title> <booktitle> Proceedings of 8th Workshop on Language and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: One of the first such studies [17] was done at Illinois as part of the Cedar project, by a group of people of which the author was a member. Similar studies have been done at Stanford [21]. Researchers at Stanford <ref> [22] </ref>, and Minnesota [20], plus the PIPS group at Ecole des mines de Paris [13, 14] and the Parafrase-2 group at Illinois [31], have implemented compilers including the same basic transformations found to be important in the Cedar Project.
Reference: [23] <author> M. Hall, B. Murphy, S. Amarasinghe, S. Liao, and M. Lam. </author> <title> Detecting Coarse-grain Parallelism Using An Interprocedural Parallelizing Compiler. </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The PIPS compiler also uses an algorithm to handle some of the simpler array reshaping situations. Work on the SUIF system at Stanford uses a very similar representation <ref> [2, 23, 28] </ref> to that of PIPS, and has a more sophisticated algorithm to handle the general problem of array reshaping.
Reference: [24] <author> P. Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This observation implies that most references in a program are simple, and for these references the representation is exact. When the representation is exact, a compiler can use simple set operations (union, intersection, etc) without losing any precision. Researchers at Rice University <ref> [12, 24] </ref> have devised several variants of regular section descriptors (RSDs), with operations defined on a lattice. RSDs are of the form I + ff, where I is a loop index and ff is a loop invariant.
Reference: [25] <author> Chris Huson. </author> <title> An Inline Subroutine Expander For Parafrase. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> December </month> <year> 1982. </year>
Reference-contexts: The other principle technique for doing interprocedural analysis at that time was inline-expansion of subroutines (subroutine inlining) <ref> [25] </ref>.
Reference: [26] <author> Z. Li and P. Yew. </author> <title> Efficient Interprocedural Analysis for Program Parallelization and Restructuring. </title> <booktitle> Proceedings of the SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: The typical set operations (intersection, union, etc) are defined on DADs. 3.2 Reference List-based Techniques Li and Yew proposed Atom Images <ref> [26] </ref> and Burke and Cytron proposed the conversion of all arrays to a single declared dimension and all subscript expressions to a linearized form [11]. The reference-list techniques rely on making a list of each individual array reference in the code section.
Reference: [27] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> An Efficient Data Dependence Analysis for Parallelizing Compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 26-34, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: when one of the following situations occurs for an array reference: * an array reference is used in a subscripting expression (subscripted-subscripts). * non-affine subscript expressions are used. * the array reference occurs inside a triangular loop. * a loop index appears in more than one subscript position (coupled subscripts <ref> [27] </ref>). * more than one loop index appears in a single subscript position (multi-index subscripts). 21 4.1.1 Subscripted-subscripts do I = 1, N end do In most cases, it is unknown what values are in the array B, so the best way to make use of triplet notation is to make
Reference: [28] <author> D. Maydan, S. Amarasinghe, and M. Lam. </author> <title> Array Data-Flow Analysis and its Use in Array Privatization. </title> <booktitle> Proceedings of ACM SIGPLAN Symposium on Principles of Programming Languges, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: The PIPS compiler also uses an algorithm to handle some of the simpler array reshaping situations. Work on the SUIF system at Stanford uses a very similar representation <ref> [2, 23, 28] </ref> to that of PIPS, and has a more sophisticated algorithm to handle the general problem of array reshaping.
Reference: [29] <author> Y. Paek. </author> <title> Automatic Parallelization for Distributed Memory Machines Based on Access Region Analysis. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Such array references are precisely representable in triplet notation. 1 Rectangular loops have no loop indices in the loop bound expressions for that loop. 24 5 THE ACCESS REGION DESCRIPTOR A hybrid form of memory access representation, combining a generalized triplet notation with constraints, was first described in <ref> [29] </ref>. It was further refined in [30] and called a Linear Memory Access Descriptor (LMAD). This thesis refines it yet again. The Linear Memory Access Descriptor contains the basic information that is needed to describe a memory access pattern.
Reference: [30] <author> Y. Paek, J. Hoeflinger, and D. Padua. </author> <title> Simplification of Array Access Patterns for Compiler Optimizations. </title> <booktitle> In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: It was further refined in <ref> [30] </ref> and called a Linear Memory Access Descriptor (LMAD). This thesis refines it yet again. The Linear Memory Access Descriptor contains the basic information that is needed to describe a memory access pattern. However, to use the LMAD for flow-sensitive program analysis, additional information must be attached to it.
Reference: [31] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B Leung, and D. Schouten. </author> <title> The Structure of Parafrase-2: An Advanced Parallelizing Compiler for C and Fortran. </title> <booktitle> In Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, New York, </address> <month> August </month> <year> 1990. </year> <month> 138 </month>
Reference-contexts: Similar studies have been done at Stanford [21]. Researchers at Stanford [22], and Minnesota [20], plus the PIPS group at Ecole des mines de Paris [13, 14] and the Parafrase-2 group at Illinois <ref> [31] </ref>, have implemented compilers including the same basic transformations found to be important in the Cedar Project. The results have been similar enough to form a general consensus as to which analysis techniques are important to provide in a parallelizing compiler.
Reference: [32] <author> W. Pottenger. </author> <title> Induction Variable Substitution and Reduction Recognition in the Polaris Parallelizing Compiler. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: + . . . do J = 1, M end do Subscripted-subscripts can even be involved in this same general form: do I = 1, N A (B (J)) = A (B (J)) + . . . do J = 1, M end do 9 A reduction can be parallelized <ref> [32] </ref> in a number of ways, all of them potentially allowing the original operations to be done in a different order from that of the original loop. <p> The dependence is removed by the compiler computing a closed form for the variable, calculated in each iteration separately. The loops surrounding the induction assignments can be triangular 1 as well as rectangular. Triangular loops cause a more complicated closed form expression for the induction variable <ref> [32] </ref>. 1 an inner loop uses the loop index of an outer loop in one of its bounds expressions 10 2.3 The Symbolic Analysis Techniques Symbolic analysis takes several forms inside Polaris. The interprocedural component is split into interprocedural value propagation and subroutine inlining.
Reference: [33] <author> W. Pugh. </author> <title> A Practical Algorithm for Exact Array Dependence Analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8), </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Work on constraint-based techniques since 1986 has attempted to address all of these drawbacks and make the representation useful for analyses other than just dependence analysis. The advent of Pugh's Omega Solver <ref> [33] </ref> made the use of Fourier-Motzkin techniques practical. The algorithms built-in to the Omega Solver optimized it so as to avoid exponential running times for the constraints derived from most practical program situations.
Reference: [34] <author> Z. Shen, Z. Li, and P. Yew. </author> <title> An Empirical Study of Fortran Programs for Parallelizing Compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 350-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: When one loop index appears in the subscript expression of more than one array dimension of an array reference, the subscripts are said to be coupled. According to the study of Shen et al <ref> [34] </ref>, 80% of subscript expressions are non-coupled. This observation implies that most references in a program are simple, and for these references the representation is exact. When the representation is exact, a compiler can use simple set operations (union, intersection, etc) without losing any precision.
Reference: [35] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct Parallelization of Call Statements. </title> <booktitle> Proceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <year> 1986. </year>
Reference-contexts: done on representing the regions within an array which are accessed by a section of code has proceeded in three general directions: linear constraint-based forms, reference-list forms, and triplet-notation based forms. 3.1 Linear Constraint-based Techniques Using linear constraint-based techniques to represent array accesses was first proposed by Trio-let, et al <ref> [35] </ref>. Their representation was called a region and the overall parallelization technique was called direct parallelization.
Reference: [36] <author> P. Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Restricted RSDs were devised to handle diagonal access, and then Bounded RSDs were devised to express triplet notation with full symbolic expressions. The Polaris project at the University of Illinois has used full symbolic triplet notation to represent array accesses within its array privatization analysis <ref> [36] </ref> and its dependence analysis [10]. Researchers at the University of Minnesota have used Guarded Array Regions [20] which are equivalent to Bounded RSDs, with an additional predicate (guard).
Reference: [37] <author> P. Tu and D. Padua. </author> <title> Automatic Array Privatization. </title> <month> August </month> <year> 1993. </year>
Reference-contexts: This copy is called a last value assignment. The same idea may be applied to arrays <ref> [37] </ref>, but the subscripting expressions, used to assign array elements, complicate the analysis. It must be determined that some part (or all) of the array is assigned within each iteration before being used. Flow-sensitive array privatization adds further difficulties.

References-found: 37

