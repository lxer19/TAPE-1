URL: http://www.cs.washington.edu/research/lis/rapid/pubs/fccm97.ps.Z
Refering-URL: http://www.cs.washington.edu/homes/sgberg/
Root-URL: 
Title: Mapping Applications to the RaPiD Configurable Architecture  
Author: Carl Ebeling, Darren C. Cronquist, Paul Franklin, Jason Secosky, and Stefan G. Berg 
Address: Box 352350 Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: The goal of the RaPiD (Reconfigurable Pipelined Datapath) architecture is to provide high performance configurable computing for a range of computationally-intensive applications that demand special-purpose hardware. This is accomplished by mapping the computation into a deep pipeline using a configurable array of coarse-grained computational units. A key feature of RaPiD is the combination of static and dynamic control. While the underlying computational pipelines are configured statically, a limited amount of dynamic control is provided which greatly increases the range and capability of applications that can be mapped to RaPiD. This paper illustrates this mapping and configuration for several important applications including a FIR filter, 2-D DCT, motion estimation, and parametric curve generation; it also shows how static and dynamic control are used to perform complex computations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Arnold et al. </author> <title> The Splash 2 processor and applications. </title> <booktitle> In Proceedings IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 482-5. </pages> <publisher> IEEE Com-put. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Unfortunately, the promise of configurable computing has yet to be realized in spite of some very successful examples <ref> [1, 9] </ref>. There are two main reasons for this. First, configurable computing platforms are currently implemented using commercial FPGAs. These FPGAs are necessarily very fine-grained so they can be used to implement arbitrary circuits, but the overhead of this generality exacts a high price in density and performance.
Reference: [2] <author> T. DeRose et al. Apex: </author> <title> two architectures for generating parametric curves and surfaces. </title> <journal> Visual Computer, </journal> <volume> 5 </volume> <pages> 264-76, </pages> <year> 1989. </year>
Reference-contexts: be preloaded 576=16 times and a preload takes 16 fl 32 cycles, resulting in 18; 432 cycles. 4 With very little additional effort this can be changed to Bezier curves of arbitrary dimension and with up to six control points on a 16-cell RaPiD array. of parametric curves and surfaces <ref> [2] </ref>. Apex differs from the previous applications in that it maps a triangular data-flow onto RaPiD as shown in Figure 16. Each node in the tree performs a weighted average on the two inputs values and passes the result to the parent node.
Reference: [3] <author> C. Ebeling, D. C. Cronquist, and P. Franklin. </author> <title> RaPiD|reconfigurable pipelined datapath. </title> <editor> In R. Hartenstein and M. Glesner, editors, </editor> <booktitle> 6th International Workshop on Field-Programmable Logic and Compilers, Lecture Notes in Computer Science, </booktitle> <pages> pages 126-135. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: However, RaPiD is not limited to implementing systolic algorithms; a pipeline can be constructed which comprises different computations at different stages and at different times. We begin with an overview of the RaPiD architecture; for a more detailed description see <ref> [3] </ref>.
Reference: [4] <author> H. Kung. </author> <title> Let's design algorithms for VLSI systems. </title> <type> Technical Report CMU-CS-79-151, </type> <institution> Carnegie-Mellon University, </institution> <month> January </month> <year> 1979. </year>
Reference-contexts: RaPiD provides a large number of ALUs, multipliers, registers and memory modules that can be configured into the appropriate pipelined datapath. The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms <ref> [4] </ref>, for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays [5, 7].
Reference: [5] <author> P. Lee and Z. M. Kedem. </author> <title> Synthesizing linear array algorithms from nested FOR loop algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1578-98, </pages> <year> 1988. </year>
Reference-contexts: The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms [4], for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays <ref> [5, 7] </ref>. However, RaPiD is not limited to implementing systolic algorithms; a pipeline can be constructed which comprises different computations at different stages and at different times. We begin with an overview of the RaPiD architecture; for a more detailed description see [3].
Reference: [6] <author> C. E. Leierson and J. B. Saxe. </author> <title> Retiming synchronous circuitry. </title> <journal> Algorithmica, </journal> <volume> 6 </volume> <pages> 5-35, </pages> <year> 1991. </year>
Reference-contexts: This becomes particularly evident in circuits with more complicated control, and when more aggressive steps, such as using the pipeline stage available in RaPiD's multiplier, are needed to achieve the desired performance. Therefore, the RaPiD B compiler retimes the resulting netlist based on <ref> [6] </ref>. All of the applications presented in the following sections have been specified in a preliminary version of RaPiD B and simulated to validate the implementations described and the accompanying cycle count.
Reference: [7] <author> D. I. Moldovan and J. A. B. Fortes. </author> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(1):1-12, </volume> <year> 1986. </year>
Reference-contexts: The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms [4], for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays <ref> [5, 7] </ref>. However, RaPiD is not limited to implementing systolic algorithms; a pipeline can be constructed which comprises different computations at different stages and at different times. We begin with an overview of the RaPiD architecture; for a more detailed description see [3].
Reference: [8] <author> K. A. Vissers et al. </author> <title> Architecture and programming of two generations video signal processors. </title> <journal> Micro-processing & Microprogramming, </journal> <volume> 41(5-6):373-90, </volume> <year> 1995. </year>
Reference-contexts: Each local memory has a specialized datapath register used as an address register; one of the bus inputs to this address register is replaced by an incrementing feedback path. Like the SILOs found in the Philips VSP <ref> [8] </ref>, this supports the common case of sequential memory accesses. More complex addressing patterns can be generated using registers and ALUs in the data-path. Input and output data enter and exit RaPiD via I/O streams at each end of the datapath.
Reference: [9] <author> J. E. Vuillemin et al. </author> <title> Programmable active memories: reconfigurable systems come of age. </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> 4(1) </volume> <pages> 56-69, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately, the promise of configurable computing has yet to be realized in spite of some very successful examples <ref> [1, 9] </ref>. There are two main reasons for this. First, configurable computing platforms are currently implemented using commercial FPGAs. These FPGAs are necessarily very fine-grained so they can be used to implement arbitrary circuits, but the overhead of this generality exacts a high price in density and performance.
References-found: 9

