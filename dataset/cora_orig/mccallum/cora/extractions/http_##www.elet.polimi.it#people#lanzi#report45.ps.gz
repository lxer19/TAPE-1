URL: http://www.elet.polimi.it/people/lanzi/report45.ps.gz
Refering-URL: http://www.elet.polimi.it/people/lanzi/listpub.html
Root-URL: 
Title: Environments with Classifier Systems (Experiments on Adding Memory to XCS)  
Note: Solving Problems in Partially Observable  
Address: Piazza Leonardo da Vinci 32 I-20133 Milano Italia  
Affiliation: Dipartimento di Elettronica e Informazione Politecnico di Milano  
Abstract: Pier Luca Lanzi Technical Report N. 97.45 October 17 th , 1997 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cliff, D., and Ross, S. </author> <title> Adding memory to ZCS. Adaptive Behaviour 3, </title> <booktitle> 2 (1994), </booktitle> <pages> 101-150. </pages>
Reference-contexts: The same extension was proposed [13] for ZCS the "zeroth level" classifier system from which XCS was derived; the proposal was validated for ZCS in <ref> [1, 9] </ref> where experimental results were presented which showed that (i) ZCS with internal memory can solve problems in non-Markovian environments when the size of internal state is 1 limited [1]; while (ii) when size internal memory grows the learning become unstable [9]. <p> [13] for ZCS the "zeroth level" classifier system from which XCS was derived; the proposal was validated for ZCS in [1, 9] where experimental results were presented which showed that (i) ZCS with internal memory can solve problems in non-Markovian environments when the size of internal state is 1 limited <ref> [1] </ref>; while (ii) when size internal memory grows the learning become unstable [9]. Wilson's proposal has never been implemented for XCS and in the literature no results have been presented for extending XCS with other memory mechanisms. In this paper we validate Wilson's proposal for adding internal state to XCS. <p> In the following, we present an example of Class 2 environment, while we address the interested reader to [8] for more examples. One of the most popular Class 2 environments that has been proposed in the literature is Woods101 <ref> [1] </ref> shown in Figure 1, that is also known as McCallum's Maze [8]. <p> This policy is an efficient stochastic solution for the Woods101 problem, and is very similar to the one 7 found for the same environment with ZCS <ref> [1] </ref>. In order to evolve an optimal solution, XCS needs some sort of memory mechanism. 6 Adding Internal Memory to XCS We now extend XCS with internal memory as done for ZCS in [1]. <p> Woods101 problem, and is very similar to the one 7 found for the same environment with ZCS <ref> [1] </ref>. In order to evolve an optimal solution, XCS needs some sort of memory mechanism. 6 Adding Internal Memory to XCS We now extend XCS with internal memory as done for ZCS in [1]. An internal register with b bits is added to XCS architecture; classifiers are extended with an internal condition and an internal action that are employed to "sense" and modify the contents of the internal register. Internal condition/action consist of b characters in the ternary alphabet f0,1,#g. <p> above requires 540 classifier, thus XCSM successfully evolves a compact representation of the 11 the population (higher peak) that is successfully recovered by the generalization mechanism of XCS (M). function which maps state/action pairs in predicted rewards. 7.2 XCSM in Woods102 As a second experiment, we test XCSM in Woods102 <ref> [1] </ref>, a more difficult environment shown in four different positions in the environment, while the latter, see 10 (c), is at one of two different positions in the environment. <p> At this point is worth noticing that even three bits of internal memory may appear only a few, most of the environments presented in literature requires only one or two bits to disambiguate aliasing situations <ref> [14, 1] </ref>. 17 9 Sequences of Actions in the Internal Memory In the previous sections we applied XCSM to environments in which the optimal solution requires the agent to visit at most one aliasing state before it reaches the food, and the goal state is very near to the aliasing cells. <p> Because of (i) the animat have to perform a sequence of actions in the internal memory instead of a single action; while, as shown in <ref> [1] </ref>, the longer the sequence of action that the agent needs to reach the goal state is, the more difficult is the problem to solve.
Reference: [2] <author> Holland, J. H. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: 1 Introduction XCS is a classifier system proposed by Wilson [14] that differs from Holland's framework <ref> [2] </ref> in that (i) classifier fitness is based on the accuracy of the prediction instead of the prediction itself and (ii) XCS has a very basic architecture with respect to the traditional framework.
Reference: [3] <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research 4 (1996). </journal>
Reference-contexts: The agent is then said to suffer from the hidden state problem or from the perceptual aliasing problem, while the environment is said to be partially observable with respect to the agent <ref> [3] </ref>. Since optimal actions cannot be determined only looking at the current inputs, the agent needs some sort of memory of past states in order to develop an optimal policy. Such environments are non-Markovian, also Class 2 environments according to [12], and form the most general class of environments.
Reference: [4] <author> Kovacs, T. </author> <title> Evolving optimal populations with XCS classifier systems. </title> <type> Tech. Rep. </type> <institution> CSR-96-17 and CSRP-96-17, School of Computer Science, University of Birmingham, Birmingham, U.K., </institution> <year> 1996. </year> <note> Avaiable from the technical report archive at http://www/system/tech-reports/tr.html. </note>
Reference-contexts: Following Littman's classification Woods101 is thus a (h = 1; fi &gt; 1)-environment. 3 The XCS Classifier System We now give a brief review of XCS in its most recent version [15]. We refer the interested reader to [14] for the original XCS description or to Kovacs's report <ref> [4] </ref> for a more detailed discussion for implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . Prediction p j gives an estimate of what is the reward that the classifier is expected to gain.
Reference: [5] <author> Lanzi, P. L. </author> <title> A Model of the Environment to Avoid Local Learning with XCS in Animat Problems (An Analysis of the Generalization Mechanism of Wilson's Classifier System). </title> <type> Tech. Rep. N. </type> <institution> 97.46, Dipartimento di Elettronica e Informazione Politecnico di Milano, </institution> <year> 1997. </year> <note> Available at http://www.elet.polimi.it/ lanzi/listpub.html. 22 </note>
Reference-contexts: We already analyzed the behavior of XCS for Markovian problems in <ref> [5] </ref> where we showed that XCS fails to converge to an optimal solution when the agent does not visit all the areas of the environment frequently. Accordingly, we proposed a new exploration strategy called teletransportation which 12 population that the system is not able to recover. <p> This strategy guarantees, for small M es values, that the animat visits all the areas of the environment with the same frequency. We now extend the results presented in <ref> [5] </ref> to non-Markovian environments applying teletrans-portation for solving Woods102 with XCSM2. Teletransportation for XCSM is implemented as done for XCS except for the fact that in XCSM the internal register is reset every time the an-imat is teletransported. <p> Most important the system fails to converge to optimal performance when, due to the structure of the environment, the agent is not able to visit all the areas of the environment uniformly <ref> [5] </ref>. Accordingly, the exploration strategy called teletransportation, introduced in [5] for Markovian environments, can be employed to guarantee an uniform exploration in non-Markovian environments in order to evolve an optimal policy, as the experiments with Woods102 show. 8 Stability of Learning in XCSM Results presented in [9], pag. 20, for ZCS <p> Most important the system fails to converge to optimal performance when, due to the structure of the environment, the agent is not able to visit all the areas of the environment uniformly <ref> [5] </ref>. Accordingly, the exploration strategy called teletransportation, introduced in [5] for Markovian environments, can be employed to guarantee an uniform exploration in non-Markovian environments in order to evolve an optimal policy, as the experiments with Woods102 show. 8 Stability of Learning in XCSM Results presented in [9], pag. 20, for ZCS with internal memory show increasing instability in performance for
Reference: [6] <author> Lanzi, P. L. </author> <title> A Study on the Generalization Capabilities of XCS. </title> <booktitle> In Proceedings of the Seventh International Conference on Genetic Algorithms (1997), </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Macroclassifiers are essentially a programming technique that speeds up the learning process reducing the number of real, macro, classifiers XCS has to deal with. 5 Since XCS was presented, two genetic operators have been proposed as extensions to the original system: Subsumption deletion [15] and Specify <ref> [6] </ref>. Subsumption deletion has been introduced to improve generalization capabilities of XCS. Subsumption deletion acts when classifiers created by the genetic component have to be inserted in the population. <p> Unfortunately there are cases in which, due to a strong genetic pressure, the generalization mechanism is too slow for recovering from a corrupted population, and the system is not able to evolve an optimal solution, as shown in Figure 7. This phenomenon was already reported in <ref> [6] </ref> for Markovian environments where the Specify operator was proposed for recovering potentially dangerous situations.
Reference: [7] <author> Littman, M. L. </author> <title> An optimization-based categorization of reinforcement learning environments. </title> <booktitle> In In From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior (1992), </booktitle> <editor> J.-A. M. H. Roitblat and S. W. eds., Eds., </editor> <publisher> The MIT Press/Bradford Books. </publisher>
Reference-contexts: Class 2 environments are said to be partially observable with respect to the agent, or equivalently are non-Markovian with respect to agent's actions. Accordingly, the agent is said to suffer from the hidden state problem. Littman in <ref> [7] </ref> presents a more formal classification of reinforcement learning environments, that is based on the simplest agent that can achieve optimal performance. Two parameters h and fi characterize the complexity of an agent.
Reference: [8] <author> McCallum, R. A. </author> <title> Hidden state and reinforcement learning with instance-based state identification. </title> <journal> IEEE Transations on Systems, Man and Cybernetics Part B (Special issue on Learning Autonomous Robots) 26, </journal> <month> 3 </month> <year> (1996). </year>
Reference-contexts: Class 2 or (h &gt; 0; fi &gt; 1) environments, for which the agent needs a sort of memory mechanism to evolve an optimal solution. In the following, we present an example of Class 2 environment, while we address the interested reader to <ref> [8] </ref> for more examples. One of the most popular Class 2 environments that has been proposed in the literature is Woods101 [1] shown in Figure 1, that is also known as McCallum's Maze [8]. <p> the following, we present an example of Class 2 environment, while we address the interested reader to <ref> [8] </ref> for more examples. One of the most popular Class 2 environments that has been proposed in the literature is Woods101 [1] shown in Figure 1, that is also known as McCallum's Maze [8]. An agent must learn how to reach food, F symbol; it sense the environment by means of eight sensors that tells him the content of the corresponding adjacent cells: food, obstacle or empty if the cell is free.
Reference: [9] <author> Ross, S. </author> <title> Accurate reaction or reflective action? experiments in adding memory to wilson's ZCS. </title> <institution> University of Sussex, </institution> <year> 1994. </year>
Reference-contexts: The same extension was proposed [13] for ZCS the "zeroth level" classifier system from which XCS was derived; the proposal was validated for ZCS in <ref> [1, 9] </ref> where experimental results were presented which showed that (i) ZCS with internal memory can solve problems in non-Markovian environments when the size of internal state is 1 limited [1]; while (ii) when size internal memory grows the learning become unstable [9]. <p> derived; the proposal was validated for ZCS in [1, 9] where experimental results were presented which showed that (i) ZCS with internal memory can solve problems in non-Markovian environments when the size of internal state is 1 limited [1]; while (ii) when size internal memory grows the learning become unstable <ref> [9] </ref>. Wilson's proposal has never been implemented for XCS and in the literature no results have been presented for extending XCS with other memory mechanisms. In this paper we validate Wilson's proposal for adding internal state to XCS. <p> Accordingly, the exploration strategy called teletransportation, introduced in [5] for Markovian environments, can be employed to guarantee an uniform exploration in non-Markovian environments in order to evolve an optimal policy, as the experiments with Woods102 show. 8 Stability of Learning in XCSM Results presented in <ref> [9] </ref>, pag. 20, for ZCS with internal memory show increasing instability in performance for increasing internal memory sizes. We now apply XCSM to a series of environments using different size of internal memory to test the stability of the system.
Reference: [10] <author> Watkins, C. </author> <title> Learning from delayed reward. </title> <type> PhD Thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Several common features of Holland's classifiers have in fact been removed in XCS, in order to simplify the study of the mechanism of learning. This has led to some interesting results: [14], see also [13], presents an analysis of the similarities between XCS and the Q-learning technique <ref> [10] </ref>, while in [15] experimental results are presented showing that XCS can learn a more compact representation than that learned by tabular Q-learning. According to the original proposal, XCS does not include an internal message list, as Holland's classifier system does, and no other memory mechanism either.
Reference: [11] <author> Widrow, B., and Hoff, M. </author> <title> Adaptive switching circuits. </title> <booktitle> In Western Electronic Show and Convention (1960), </booktitle> <volume> vol. 4, </volume> <booktitle> Institute of Radio Engineers (now IEEE), </booktitle> <pages> pp. 96-104. </pages>
Reference-contexts: First, the Q-learning-like payoff P is computed as the sum of the reward received at the previous time step and the maximum system prediction discounted by a factor fl (0 fl &lt; 1). P is used to update the prediction p j by the Widrow-Hoff delta rule <ref> [11] </ref> with learning rate fi (0 &lt; fi 1): p j p j + fi (P p j ). Likewise, the prediction error " j is adjusted with the formula: " j " j + fi (jP p j j " j ). Fitness update is slightly more complex.
Reference: [12] <author> Wilson, S. W. </author> <title> The animat path to AI. </title> <booktitle> In From Animals to Animats: Proceedings of the First International Conference on the Simulation of Adaptive Behavior (1991), </booktitle> <publisher> The MIT Press/Bradford Books. </publisher>
Reference-contexts: Since optimal actions cannot be determined only looking at the current inputs, the agent needs some sort of memory of past states in order to develop an optimal policy. Such environments are non-Markovian, also Class 2 environments according to <ref> [12] </ref>, and form the most general class of environments. When in Class 2 environments XCS can only develop a suboptimal policy, in order to learn an optimal policy in such domains, XCS would require a sort of memory mechanism or local storage. <p> Recently, many authors have addressed the problem of studying the interaction agent/environment rather than studying a specific agent and/or a particular environment separately. In the literature, there have been proposed some classifications for the possible interactions between an agent and its environment. Wilson in <ref> [12] </ref> proposes a scheme to classify reinforcement learning environments with respect to the sensory capabilities of the agent.
Reference: [13] <author> Wilson, S. W. ZCS: </author> <title> a zeroth level order classifier system. </title> <booktitle> Evolutionary Computation 1, 2 (1994), </booktitle> <pages> 1-18. </pages>
Reference-contexts: Several common features of Holland's classifiers have in fact been removed in XCS, in order to simplify the study of the mechanism of learning. This has led to some interesting results: [14], see also <ref> [13] </ref>, presents an analysis of the similarities between XCS and the Q-learning technique [10], while in [15] experimental results are presented showing that XCS can learn a more compact representation than that learned by tabular Q-learning. <p> The proposal consists of (i) adding to XCS an internal memory register, and (ii) extending classifiers with an internal condition and an internal action, employed to sense and act on the internal register. The same extension was proposed <ref> [13] </ref> for ZCS the "zeroth level" classifier system from which XCS was derived; the proposal was validated for ZCS in [1, 9] where experimental results were presented which showed that (i) ZCS with internal memory can solve problems in non-Markovian environments when the size of internal state is 1 limited [1];
Reference: [14] <author> Wilson, S. W. </author> <title> Classifier fitness based on accuracy. </title> <booktitle> Evolutionary Computation 3, 2 (1995), </booktitle> <pages> 149-175. </pages>
Reference-contexts: 1 Introduction XCS is a classifier system proposed by Wilson <ref> [14] </ref> that differs from Holland's framework [2] in that (i) classifier fitness is based on the accuracy of the prediction instead of the prediction itself and (ii) XCS has a very basic architecture with respect to the traditional framework. <p> Several common features of Holland's classifiers have in fact been removed in XCS, in order to simplify the study of the mechanism of learning. This has led to some interesting results: <ref> [14] </ref>, see also [13], presents an analysis of the similarities between XCS and the Q-learning technique [10], while in [15] experimental results are presented showing that XCS can learn a more compact representation than that learned by tabular Q-learning. <p> When in Class 2 environments XCS can only develop a suboptimal policy, in order to learn an optimal policy in such domains, XCS would require a sort of memory mechanism or local storage. An extension to XCS was proposed <ref> [14] </ref> by which an internal state could be added to XCS like a sort of "system's internal memory". <p> Following Littman's classification Woods101 is thus a (h = 1; fi &gt; 1)-environment. 3 The XCS Classifier System We now give a brief review of XCS in its most recent version [15]. We refer the interested reader to <ref> [14] </ref> for the original XCS description or to Kovacs's report [4] for a more detailed discussion for implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> Accordingly, XCSM performance drops, higher peak in Figure 6, but then the generalization mechanism of XCS (M) recovers the dangerous situation and the system finally 1 Some of these parameters have not been presented in the XCS overview but are reported here for completeness. We refer the reader to <ref> [14] </ref> for a complete discussion of those parameters. 10 converges to optimal performance. <p> As shown in <ref> [14] </ref>, XCS builds a complete mapping for the function X fi A ) P from states/actions pairs to predicted rewards. <p> At this point is worth noticing that even three bits of internal memory may appear only a few, most of the environments presented in literature requires only one or two bits to disambiguate aliasing situations <ref> [14, 1] </ref>. 17 9 Sequences of Actions in the Internal Memory In the previous sections we applied XCSM to environments in which the optimal solution requires the agent to visit at most one aliasing state before it reaches the food, and the goal state is very near to the aliasing cells.
Reference: [15] <author> Wilson, S. W. </author> <title> Generalization in XCS. Unpublished contribution to the ICML '96 Workshop on Evolutionary Computing and Machine Learning. </title> <note> Avaiable at http://netq.rowland.org/sw/swhp.html, 1996. 23 </note>
Reference-contexts: This has led to some interesting results: [14], see also [13], presents an analysis of the similarities between XCS and the Q-learning technique [10], while in <ref> [15] </ref> experimental results are presented showing that XCS can learn a more compact representation than that learned by tabular Q-learning. According to the original proposal, XCS does not include an internal message list, as Holland's classifier system does, and no other memory mechanism either. <p> Following Littman's classification Woods101 is thus a (h = 1; fi &gt; 1)-environment. 3 The XCS Classifier System We now give a brief review of XCS in its most recent version <ref> [15] </ref>. We refer the interested reader to [14] for the original XCS description or to Kovacs's report [4] for a more detailed discussion for implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> Macroclassifiers are essentially a programming technique that speeds up the learning process reducing the number of real, macro, classifiers XCS has to deal with. 5 Since XCS was presented, two genetic operators have been proposed as extensions to the original system: Subsumption deletion <ref> [15] </ref> and Specify [6]. Subsumption deletion has been introduced to improve generalization capabilities of XCS. Subsumption deletion acts when classifiers created by the genetic component have to be inserted in the population.
References-found: 15

