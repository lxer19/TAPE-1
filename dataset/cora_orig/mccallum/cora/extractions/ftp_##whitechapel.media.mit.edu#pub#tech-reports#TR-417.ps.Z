URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-417.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Three-Dimensional Model of Human Lip Motion  
Author: by Sumit Basu Alex P. Pentland Arthur C. Smith 
Degree: (1995) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Engineering in Electrical Engineering and Computer Science at the  MCMXCVII. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Author  Certified by  Department Head/Associate Professor, Media Arts and Sciences Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Theses  
Date: February 1997  January 31, 1997  
Affiliation: S.B. in Electrical Engineering, Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology,  Department of Electrical Engineering and Computer Science  
Note: A  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 417 also appears as MIT EECS Master's Thesis February 1997 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adjoudani and J. Benoit. </author> <title> "On the Integration of Auditory and Visual Parameters in an HMM-based ASR". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: level features to form a parametrization of the lip shape: Petajan et al. use several image features to estimate an overal lip contour [13]; Adjoudani et al. relate a small set of observed features (such as lip opening width and height, etc.) to the controls of a polygonal lip model <ref> [1] </ref>. Still others have a trained model of lip variations and attempt to fit the observations to this model. <p> There has been some work done taking information from two known views <ref> [1] </ref>, but this requires the head to remain fairly static. We feel that in order to capture interesting lip data during natural speech and gesture, it will be necessary to robustly track the lips from any pose.
Reference: [2] <author> Ali Azarbayejani and Alex Pentland. </author> <title> Camera self-calibration from one point correspondence. </title> <type> Technical Report 341, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1995. </year> <note> submitted IEEE Symposium on Computer Vision. </note>
Reference-contexts: Methods to continue the training using other forms of input data will be discussed in a later section. The 3D camera calibration algorithm of Azarbayejani and Pentland <ref> [2] </ref> was then used to calibrate the real and virtual cameras using pinhole camera models. Given this calibration, the 3D point location for a given point was estimated by computing the closest point between the projective rays from the camera COP's (centers of projection) corresponding to that point.
Reference: [3] <author> Sumit Basu, Irfan Essa, and Alex Pentland. </author> <title> "Motion Regularization for Model-Based Head Tracking". In To Appear in: </title> <booktitle> Proceedings of 13th Int'l. Conf. on Pattern Recognition, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: In addition, in order to fully train this model, it will be necessary to apply the observations from an arbitrary pose. Prior work has shown that the rigid position of the head can be robustly and accurately tracked <ref> [3] </ref>, so it is feasible that we can apply the observations from any pose to the correct degrees of freedom of the model. As a result, our goal has been to create a model that can cover the full 3D variations of the lips.
Reference: [4] <author> Klaus-Jurgen Bathe. </author> <title> Finite Element Procedure in Engineering Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: They are thus assembled into the total k e as shown in block-matrix form below: k e = k xy # I built the 2D k xy using the formulation as described by Zienkiewicz [17] and Bathe <ref> [4] </ref>. This formulation has the following stress modes: * = 6 * x fl xy 7 2 4 @x @y @y + @v 3 5 (2:10) where u and v correspond to displacements along the x and y dimensions of the local 12 coordinate system.
Reference: [5] <author> Christoph Bregler and Stephen M. Omohundro. </author> <title> "Nonlinear Image Interpolation using Manifold Learning". </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: Still others have a trained model of lip variations and attempt to fit the observations to this model. Some of the most interesting work done in this area has been along these lines: Bregler and Omohundro's work, for example <ref> [5] </ref>, models the non 7 linear subspace of valid lip poses within the image space and can thus be used for both analysis and synthesis. Similarly, Luettin's system learns the subspace of variations for 2D contours surrounding the lips [11].
Reference: [6] <author> Tarcisio Coianiz, Lorenzo Torresani, and Bruno Caprile. </author> <title> "2D Deformable Models for Visual Speech Analysis". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: The underlying assumption behind most of these models is that the head will be viewed from only one known pose. As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. <ref> [6] </ref> and Kass et al. [9] model the lips with contours along the outer edge, while Duchnowski et al. [7] feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [7] <author> Paul Duchnowski, Uwe Meier, and Alex Waibel. </author> <title> "See Me, Hear Me: Integrating Automatic Speech Recognition and Lip-Reading". </title> <booktitle> In Int'l Conf. on Spoken Language Processing, </booktitle> <year> 1994. </year>
Reference-contexts: As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. [6] and Kass et al. [9] model the lips with contours along the outer edge, while Duchnowski et al. <ref> [7] </ref> feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [8] <author> Irfan A. Essa. </author> <title> "Analysis, Interpretation, and Synthesis of Facial Expressions". </title> <type> PhD thesis, </type> <institution> MIT Department of Media Arts and Sciences, </institution> <year> 1995. </year>
Reference: [9] <author> Michael Kass, Andrew Witkin, and Demetri Terzopoulous. "Snakes: </author> <title> Active Contour Models". </title> <journal> International Journal of Computer Vision, </journal> <pages> pages 321-331, </pages> <year> 1988. </year>
Reference-contexts: The underlying assumption behind most of these models is that the head will be viewed from only one known pose. As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. [6] and Kass et al. <ref> [9] </ref> model the lips with contours along the outer edge, while Duchnowski et al. [7] feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [10] <author> Y. Lee, D. Terzopoulos, and K. Waters. </author> <title> "Realistic Modeling for Facial Animation". </title> <booktitle> In Proceedings of SIGGRAPH, </booktitle> <pages> pages 55-62, </pages> <year> 1995. </year>
Reference-contexts: The other category of lip models are those designed for synthesis and facial animation. These lip models are usually part of a larger facial animation system, and the lips themselves often have a limited repertoire of motions <ref> [10] </ref>. To their credit, these models are mostly in 3D. For many of the models, though, the control parameters are defined by hand. A few are based on the actual physics of the lips: they attempt to model the physical material and musculature in the mouth region [8],[16].
Reference: [11] <author> J. Luettin, N. Thacker, and S. Beet. </author> <title> Visual speech recognition using active shape models and hidden markov models. </title> <booktitle> In ICASSP96, </booktitle> <pages> pages 817-820. </pages> <booktitle> IEEE Signal Processing Society, </booktitle> <year> 1996. </year> <month> 33 </month>
Reference-contexts: Similarly, Luettin's system learns the subspace of variations for 2D contours surrounding the lips <ref> [11] </ref>. However, in order for these 2D models to be robust, they have to allow for at least small rotations of the head. The changes in the apparent lip shape due to rigid rotations, then, have to be modeled as changes in the actual lip pose.
Reference: [12] <author> John Martin, Alex Pentland, and Ron Kikinis. </author> <title> Shape analysis of brain structures using physical and experimental modes. In CVPR94. </title> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: We began with the default physics (i.e., fairly uniform stiffness, only adjacent nodes connected) and have now observed how the model actually deforms. This new information can be used to form a new, "learned" K matrix. Martin et. al. <ref> [12] </ref> described the connection between the strain matrix and the covariance of the displacements.
Reference: [13] <author> E.D. Petajan. </author> <title> "Automatic Lipreading to Enhance Speech Recognition". </title> <booktitle> In Proc. IEEE Communications Society Global Telecom. Conf., </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: Others use such low level features to form a parametrization of the lip shape: Petajan et al. use several image features to estimate an overal lip contour <ref> [13] </ref>; Adjoudani et al. relate a small set of observed features (such as lip opening width and height, etc.) to the controls of a polygonal lip model [1]. Still others have a trained model of lip variations and attempt to fit the observations to this model.
Reference: [14] <author> Henry Stark and John W. Woods. </author> <title> Probability, Random Processes, and Estimation Theory for Engineers. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Furthermore, noise in the observations makes it unreasonable to estimate even this many modes. I thus take only the 10 observed modes that account for the greatest amount of variance in the input data. These modes are found by performing principal components analysis (PCA) on the sample covariance matrix <ref> [14] </ref>, i.e., taking the eigenvectors and eigenvalues. Finding the eigenvalues and eigenvectors of the expected covariance matrix (which is 612 by 612) would take a great deal of computation. We can find the desired vectors by taking the eigenvectors of a much smaller matrix and then appropriately transforming the results. <p> The unbiased estimate of the covariance matrix <ref> [14] </ref> is given by K s = E [OO T ] = n 1 where A is an m fi n matrix in which each column is an observation. Finding the eigenvectors of K s (which is m fi m) can be computationally expensive when m is large.
Reference: [15] <author> Gilbert Strang. </author> <title> An Introduction to Applied Mathematics. </title> <publisher> Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: To find this solution, we express the problem in a LaGrange multiplier formulation: we want to minimize the quantity in equation A.2 while satisfying the constraint in equation A.1. Using the method of LaGrange as described in Strang <ref> [15] </ref>, we then have to minimize the following quantity: f = 2 where is an n fi 1 vector of LaGrange multipliers for each of the rows of Ax b. In other words, we are placing the constraint that each row of Ax b must be zero.
Reference: [16] <author> K. Waters and J. Frisbie. </author> <title> "A Coordinated Muscle Model for Speech Animation". </title> <booktitle> In Graphics Interface, </booktitle> <pages> pages 163-170, </pages> <year> 1995. </year>
Reference-contexts: Some models, as in the work by Frisbie and Waters, have tried to approximate this subspace by modeling key lip positions (visemes) and then interpolating between them <ref> [16] </ref>. However, This limits the "correct" set of lip shapes to those fit by hand, without modeling how the lips really move between them. I hope to fill the gap in these approaches with a 3D model that can be used for both analysis and synthesis.
Reference: [17] <author> O.C. Zienkiewicz. </author> <title> The Finite Element Method in Structural and Continuum Mechanics. </title> <publisher> McGraw-Hill, </publisher> <year> 1967. </year> <month> 34 </month>
Reference-contexts: I constructed the model by beginning with a 2D plane-strain isotropic material formulation <ref> [17] </ref> and adding a strain relationship for the out-of-plane components. For each triangular element, then, the six in-plane degrees of freedom are related with a six-by-six matrix k xy , while the out-of-plane degrees of freedom are related by the three-by-three k z . <p> They are thus assembled into the total k e as shown in block-matrix form below: k e = k xy # I built the 2D k xy using the formulation as described by Zienkiewicz <ref> [17] </ref> and Bathe [4]. This formulation has the following stress modes: * = 6 * x fl xy 7 2 4 @x @y @y + @v 3 5 (2:10) where u and v correspond to displacements along the x and y dimensions of the local 12 coordinate system.
References-found: 17

