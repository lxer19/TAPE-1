URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-434.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: -dkroy, nitin, geek, sandy-@media.mit.edu  
Title: Wearable Audio Computing: A Survey of Interaction Techniques  
Author: Deb Roy, Nitin Sawhney, Chris Schmandt and Alex Pentland 
Keyword: Wearable computing, mobile computing, speech recognition, speech synthesis, audio interfaces, information retrieval, audio indexing  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Perceptual Computing Group and Speech Interface Group MIT Media Laboratory,  
Abstract: We consider wearable computing applications which rely on audio as a primary medium of the interface. This paper surveys a range of interaction techniques which may be applied to the design of wearable audio computers (WACs). A summary of several speech and audio processing technologies which can be used in the interface of WACs are reviewed. We present several usage scenarios and focus on two specific systems which we are currently implementing. Nomadic Radio is a 3-D audio application which provides the user with a personalized and dynamic audio-only information environment. We are also developing an adaptive speech recognition system; an application based on this system enables hearing impaired users to visualize speech. Future research areas include adaptive interfaces, automatic situational awareness and focus of attention in wearable audio computing. 
Abstract-found: 1
Intro-found: 1
Reference: [Arons91] <author> Barry Arons. Hyperspeech: </author> <title> Navigating in Speech-only Hypermedia. </title> <booktitle> Proceedings of Hypertext '91, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: We note that in order to have nonlinear access the interface needs some structural information about the audio which may be derived automatically (Section 4.3) or manually [Degen92]. Audio content represented as hyperlinked nodes can be browsed using spatial and temporal techniques. Hyperspeech <ref> [Arons91] </ref> is a speech-only hypermedia application that uses speech recognition to maneuver in a database of digitally recorded hyperlinked speech segments without a visual display. Espace 2 [Sawhney96] is an audio-only environment that consists of a hierarchy of hyperlinked ambient spaces containing voice conversations.
Reference: [Arons93] <author> Barry Arons. SpeechSkimmer: </author> <title> Interactively Skimming Recorded Speech. </title> <booktitle> Proceedings of UIST 93, </booktitle> <publisher> ACM Press, </publisher> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Short-time energy can be used to find pauses in recordings [Rabiner93]. A variety of methods based on autocorrelation based peak picking may be used to estimate the fundamental frequency of speech. Arons reports that indexing into speech recordings based on pause and pitch structure increased access efficiency <ref> [Arons93] </ref>. Speaker identification is the process of identifying which member of a previously known set of speakers is most likely to have generated a speech sample [Furui96].
Reference: [Bederson96] <author> Bederson, Benjamin B. </author> <title> Audio Augmented Reality: A Prototype Automated Tour Guide. </title> <booktitle> Proceedings of CHI 95, </booktitle> <month> May </month> <year> 1996, </year> <pages> pp. 210-211. </pages>
Reference-contexts: A similar methodology can be utilized in wearable audio system to index captured audio with contextual information such as user location, time, and current activity for later retrieval as structured views into the users sonic memories. A prototype audio augmented reality-based tour guide <ref> [Bederson96] </ref> presented digital audio recordings indexed by the spatial location of visitors in a museum. This is a early implementation of a wearable audio system which provides only fixed information and does not consider the listeners usage history or model their preferences.
Reference: [Bregman90] <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Speech and audio interfaces are considered to be sequential, while visual interfaces are simultaneous. The cocktail-party effect provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background <ref> [Bregman90] </ref>. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [Wenzel92].
Reference: [Cohen93] <author> Cohen, J. </author> <title> Monitoring background activities. Auditory Display: Sonification, Audification, and Auditory Interfaces. </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Ambient sound textures can be specially designed or algorithmically generated (as earcons) to deliver a perception of object persistence or subtle changes in object attributes. Continuous audio can indicate the presence of background activity <ref> [Cohen93] </ref> or the sense of location within an audio environment. Speech and audio interfaces are considered to be sequential, while visual interfaces are simultaneous.
Reference: [Degen92] <author> Degen, L., Mander, R. and Salomon, G. </author> <title> (1992) "Working with Audio: Integrating Personal Tape Recorders and Desktop Computers". </title> <booktitle> Proceedings of CHI '92, </booktitle> <pages> pp. 413-418, </pages> <publisher> ACM. </publisher>
Reference-contexts: This leads to an active area of research in interaction techniques for nonlinear audio access. We note that in order to have nonlinear access the interface needs some structural information about the audio which may be derived automatically (Section 4.3) or manually <ref> [Degen92] </ref>. Audio content represented as hyperlinked nodes can be browsed using spatial and temporal techniques. Hyperspeech [Arons91] is a speech-only hypermedia application that uses speech recognition to maneuver in a database of digitally recorded hyperlinked speech segments without a visual display.
Reference: [Edwards94] <author> W. Keith Edwards, Elizabeth D. Mynatt, and Kathryn Stockton. </author> <title> Providing Access to Graphical User Interfaces Not Graphical Screens. </title> <booktitle> ACM Proceedings on ASSETS 94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This approach reduces cognitive load on the user and optimizes the nature of information conveyed. Several attempts have been made to represent GUIs (Graphic User Interfaces) by mapping speech and auditory cues to visual interface artifacts, applications and window events (the first was Sonic Finder [Gaver89]). The Mercator <ref> [Edwards94] </ref> system provided visually-impaired users access to GUI applications on the X-Windows platform. The interface transparently maps the graphical and textual output of existing X-applications to audio and synthesized speech output within the Mercator environment.
Reference: [Feiten94] <author> Feiten, B. and S. Gunzel, </author> <title> Automatic Indexing of a Sound Database using Self-Organizing Neural Nets. </title> <journal> Computer Music Journal, </journal> <volume> 18:3, </volume> <pages> pp. 53-65, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Sound classification techniques can be used to automatically segment audio. For example speech and music can be separated by characterizing statistical patterns inherent in each class of acoustics. A variety of methods can be used to classify sound patterns including clustering techniques [Wold96] or neural networks <ref> [Feiten94] </ref>. Similar to speech recognition, the performance of all the technologies mentioned in this section are dependent how clean the recordings are made. 4.3 Digital Audio Recording and Playback Simple digital audio recording and playback may be used in a variety of ways in the audio interface.
Reference: [Furnas87] <author> Furnas, G., Landauer, T., Gomez, L. and Dumais, S. </author> <year> (1987). </year> <title> The vocabulary problem in human-system communications. </title> <journal> Communications of the ACM, </journal> <volume> 30: </volume> <pages> 964-971. </pages>
Reference-contexts: On the other hand it the WAC is to be a natural extension of the users clothes, the recognizer must adapt to the vocabulary and pronunciation habits of each user. Take for example the "large vocabulary problem" described by <ref> [Furnas87] </ref>. Furnas created a simple application with 25 commands and provided subjects with a list of names to be assigned to each command. Furnas found the likelihood that two people chose the same term meant the same command by it was only 15%.
Reference: [Furui96] <author> Furui, S. </author> <year> (1996). </year> <title> An Overview of Speaker Recognition Technology. In: Automatic Speech and Speaker Recognition. Edited by Lee, </title> <editor> C., Soong, F., Paliwal, K. </editor> <publisher> Kluwer Academic Press. </publisher>
Reference-contexts: Arons reports that indexing into speech recordings based on pause and pitch structure increased access efficiency [Arons93]. Speaker identification is the process of identifying which member of a previously known set of speakers is most likely to have generated a speech sample <ref> [Furui96] </ref>. Although we should not be too worried about this in the long run; people once thought that telephones would not succeed since no one would feel comfortable talking into a machine.
Reference: [Gaver89] <author> William W. Gaver. </author> <title> The Sonic Finder: An interface that uses auditory icons. </title> <booktitle> Human Computer Interaction, </booktitle> <volume> 4 </volume> <pages> 67-94, </pages> <year> 1989. </year>
Reference-contexts: This approach reduces cognitive load on the user and optimizes the nature of information conveyed. Several attempts have been made to represent GUIs (Graphic User Interfaces) by mapping speech and auditory cues to visual interface artifacts, applications and window events (the first was Sonic Finder <ref> [Gaver89] </ref>). The Mercator [Edwards94] system provided visually-impaired users access to GUI applications on the X-Windows platform. The interface transparently maps the graphical and textual output of existing X-applications to audio and synthesized speech output within the Mercator environment. <p> Thus simply adding auditory cues and speech synthesis to existing GUIs is not guaranteed to be an adequate means for providing nonvisual access to information. Auditory icons are short nonspeech sounds which are generated to convey the nature of a digital artifact or event and its dynamically changing attributes <ref> [Gaver89] </ref>. In some situations, objects and events are better represented with continuous patterns of sound rather than discreet auditory icons. Data changing over time or the presence of persistent objects in the environment require the use of continuous audio.
Reference: [Gish91] <author> Gish, H., Siu, M., Rohlicek, R. </author> <year> (1991). </year> <title> Segregation of Speakers for Speech Recognition and Speaker Identification. </title> <booktitle> Proc. Int. Conf. Acoustics, Speech and Signal Processing. </booktitle> <volume> Vol. </volume> <pages> 2 (pp. 873-876). </pages>
Reference-contexts: Speaker indexing is the process of finding and indexing (assigning unique indices) to every unique speaker in a recording <ref> [Gish91, Wilcox94, Roy97b] </ref>. Both speaker identification and indexing can be used to index into recordings at speaker change locations for efficient nonlinear access. Sound classification techniques can be used to automatically segment audio.
Reference: [Hayes83] <author> Hayes, P. And D. Reddy. </author> <title> Steps towards graceful interaction in spoken and written man-machine communication., </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 19, </volume> <pages> pp. 231-284, </pages> <year> 1983. </year>
Reference-contexts: A spoken conversation metaphor allows a system to create a shared context and protocol with a user to communicate its abilities, constraints, and level of understanding <ref> [Hayes83] </ref>. It allows a system to gracefully deal with speech recognition errors due to out-of vocabulary utterances and unpredictable phenomenon like background noise. An effective conversational approach [Schmandt94a] utilizes directive and timeout prompts, implicit, explicit and context-free confirmations, and modeling of dialogue states and transitions to better represent conversational flow.
Reference: [Herman95] <author> Herman, Jeffery Allen. NewsTalk: </author> <title> A Speech Interface to a Personalized Information Agent. M.S. </title> <type> Thesis, </type> <institution> MIT Media Lab, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: It allowed users to place calls, take voice messages, record reminders, schedule meetings, and access traffic information. A key feature of the system was to engage users in a sub-dialogue to obtain missing information or correct recognition errors. NewsTalk <ref> [Herman95] </ref> is a speech-only conversational interface that enables interactive retrieval of personalized news over the telephone, using speech recognition, synthesis and digitized news broadcasts. The MIT Media Labs Nomadic Computing Environment [Schmandt94b] enables subscribers to manage personal information via fax, pagers and telephony access to digitized and synthesized audio.
Reference: [Hernamsky94] <author> Hermansky, H. and Morgan, N. </author> <title> (1993) RASTA Processing of Speech. </title> <journal> IEEE Trans. Speech and Audio Proc. </journal> <pages> 1(1) pp. 39-49, </pages> <month> Jan., </month> <year> 1994. </year>
Reference: [Kimber95] <author> Kimber, D., Wilcox, L., Chen, F., Moran, T. </author> <title> "Speaker Segmentation for Browsing Recorded Audio". </title> <booktitle> Proceedings. of CHI `95, </booktitle> <pages> pp. 212-213, </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: A simple button interfaces lets the user navigate through audio news based on the structural information. Kimber et.al. have built a system which automatically divides a multi-speaker recording into speaker segments and displays the information in a graphical browsing tool <ref> [Kimber95] </ref>. Roy and Malamud have made audio proceedings of the US House of Representatives publicly available on the Internet by making the recordings (comprising several hundred hours of captured speech) searchable by speaker identity [Roy97a].
Reference: [Klatt87] <author> Klatt, D.H. </author> <year> (1987), </year> <title> "Review of text-to-speech conversion for English,''. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 82(3): </volume> <pages> 737-793. </pages>
Reference-contexts: Speech synthesis is flexible since arbitrary prompts may be generated at run time by an application. The principal weakness of synthesized speech is that the resulting prosody lacks naturalness. Speech may be synthesized by concatenating sub-word segments of prerecorded speech or by controlling parameters of a vocal tract model <ref> [Klatt87] </ref>. The results of the two methods are roughly comparable although is it easier to create new voices using concatenation. 5. Applications Under Development In this section we present two WAC applications which are in the early stages of development.
Reference: [Marx96] <author> Marx, Matthew and Chris Schmandt. </author> <title> CLUES: Dynamic Personalized Message Filtering. </title> <booktitle> Proceedings of CSCW 96, </booktitle> <pages> pp. 113-121, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Nomadic Radio is an attempt towards a personalized and dynamic audio-only information environment. It uses the rich metaphor of Radio to structure simultaneous and spatialized audio streams as radio broadcasts of timely information <ref> [Marx96] </ref>. The wearable audio system will allow a mobile user (a nomadic listener) to access broadcasts such as voice-mail, news, appointments, weather information, traffic reports, and music. The information is delivered via digitized audio streams previously downloaded by the system from an audio server. <p> Such asynchronous audio broadcasts allows an effective means of both transmitting and communicating information to users at their own listening pace. In a synthesized email application, MailCall <ref> [Marx96] </ref>, timely messages were filtered based on the priority and state of the information being presented as well as the context of the users tasks and related information.
Reference: [Nagao95] <author> Nagao, Katashi and Jun Rekimoto. </author> <title> Ubiquitous Talker: Spoken Language Interaction with real world objects, </title> <institution> SONY Computer Science Lab, </institution> <year> 1995. </year>
Reference-contexts: SpeechWear [Rudnicky96] is a mobile speech system developed at CMU which enables users to perform data entry and retrieval using an interface based on speech recognition and synthesis. A speech-enabled web browser allows users to access local and remote documents through a wireless link. Ubiquitous Talker <ref> [Nagao95] </ref> is a camera-enabled system developed at Sony Research Labs, that provides the user information related to a recognized physical object via display and synthesized voice. It accepts queries through speech input.
Reference: [Rabiner93] <author> Rabiner, L.R. and Juang, B.H. </author> <year> (1993). </year> <title> Fundamentals of speech recognition. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Speech recognizers may be trained on a specific users voice (speaker-dependent) or it may be speaker-independent. Recognizers can either process isolated-word speech requiring the user to pause after each word or can deal with natural continuous speech (see <ref> [Rabiner93] </ref> for a thorough treatment of speech recognition fundamentals). Keyword spotting is the process of detecting specific words or phrases in a stream of speech (for an overview see [Rose96]). Speech recognition technologies can play an important role in the interface of WACs. <p> This section describes a variety of speech and audio processing technologies which extract structural information from audio. Short-time energy can be used to find pauses in recordings <ref> [Rabiner93] </ref>. A variety of methods based on autocorrelation based peak picking may be used to estimate the fundamental frequency of speech. Arons reports that indexing into speech recordings based on pause and pitch structure increased access efficiency [Arons93].
Reference: [Raman96] <author> Raman, T. V. </author> <title> Emacspeak A Speech Interface. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: In contrast to most screen-readers designed for visually impaired users which attempt to convey the entire contents of the visual display via speech synthesis, Emacspeak <ref> [Raman96] </ref> integrates spoken feedback into the application itself. It does so by having the user interface components of the application communicate directly with the speech subsystem. This enables Emacspeak to provide rich, contextsensitive spoken feedback. to Emacs functions such as document editing, calendar, and WWW access.
Reference: [Rose96] <author> Rose, </author> <title> R.C. (1996) "Word spotting - Extracting partial information from continuous utterances". In: Automatic Speech and Speaker Recognition: Advanced Topics. </title> <publisher> Kluwer Academic Publishers: </publisher> <pages> 303-330. </pages>
Reference-contexts: Keyword spotting is the process of detecting specific words or phrases in a stream of speech (for an overview see <ref> [Rose96] </ref>). Speech recognition technologies can play an important role in the interface of WACs. It is useful to review some practical issues in using ASR with WACs. Commercial speech recognizers usually contain acoustic models trained on speech from either headset microphones, desktop microphones, or telephone speech.
Reference: [Roy96] <author> Roy, Deb K. and Chris Schmandt. NewsComm: </author> <title> A HandHeld Interface for Interactive Access to Structured Audio. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 173-180. </pages>
Reference-contexts: The interface combines speech recognition and button input, allowing the user to combine input modalities depending on what is convenient. Several systems use speaker segmentation information to provide nonlinear audio access. NewsComm <ref> [Roy96] </ref> delivers personalized news and audio programs to mobile listeners through a handheld playback device. Structural descriptions of the news programs are generated by locating speaker changes and analyzing pause structure. A simple button interfaces lets the user navigate through audio news based on the structural information.
Reference: [Roy97a] <author> Roy, Deb and Carl Malamud. </author> <title> (1997) "Speaker identification based test to audio alignment for an audio retrieval system". </title> <booktitle> 1997. To appear in the Proc. of the Int. Conf. Acoustics, Speech and Signal Processing, Munich, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 1099-1103. </pages>
Reference-contexts: Roy and Malamud have made audio proceedings of the US House of Representatives publicly available on the Internet by making the recordings (comprising several hundred hours of captured speech) searchable by speaker identity <ref> [Roy97a] </ref>. Some recent mobile applications use spatial memory in handwritten notes to access temporal indices in audio recordings. The Audio Notebook [Stifelman96] is a paper-based augmented audio system that allows a user to capture and access an audio recording synchronized with handwritten notes and page turns.
Reference: [Roy97b] <author> Roy, Deb. </author> <title> "Speaker indexing using neural network clustering of vowel spectra". </title> <journal> International Journal of Speech Technology, </journal> <volume> Vol. 1, No. 2 </volume> <pages> 143-149. </pages> <year> 1997. </year>
Reference-contexts: Speaker indexing is the process of finding and indexing (assigning unique indices) to every unique speaker in a recording <ref> [Gish91, Wilcox94, Roy97b] </ref>. Both speaker identification and indexing can be used to index into recordings at speaker change locations for efficient nonlinear access. Sound classification techniques can be used to automatically segment audio.
Reference: [Rudnicky96] <author> Rudnicky, Alexander, Stephen Reed and Eric Thayer. SpeechWear: </author> <title> A mobile speech system, </title> <address> CMU, </address> <year> 1996. </year>
Reference-contexts: This is a early implementation of a wearable audio system which provides only fixed information and does not consider the listeners usage history or model their preferences. SpeechWear <ref> [Rudnicky96] </ref> is a mobile speech system developed at CMU which enables users to perform data entry and retrieval using an interface based on speech recognition and synthesis. A speech-enabled web browser allows users to access local and remote documents through a wireless link.
Reference: [Sawhney96] <author> Sawhney, Nitin. and Arthur Murphy. </author> <title> ESPACE 2: An Experimental HyperAudio Environment, </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Audio content represented as hyperlinked nodes can be browsed using spatial and temporal techniques. Hyperspeech [Arons91] is a speech-only hypermedia application that uses speech recognition to maneuver in a database of digitally recorded hyperlinked speech segments without a visual display. Espace 2 <ref> [Sawhney96] </ref> is an audio-only environment that consists of a hierarchy of hyperlinked ambient spaces containing voice conversations. Temporal audio cues indicate the presence of hyper-links, during the playback of audio content.
Reference: [Schmandt85] <author> Schmandt, C., B. Arons, C. Simmons. </author> <title> Voice Interaction in an Integrated Office and Telecommunications Environment. </title> <booktitle> Proceedings of the 1985 American Voice I/O Society Conference, </booktitle> <pages> pp. 51-57. </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: SpeechActs [Yankelovich95] uses a spoken conversation metaphor and a speech-only interface to provide users with access to a variety of data, including email, calendar, weather, and information on publicly-traded stocks. The Conversational Desktop <ref> [Schmandt85] </ref> facilitated integrated office telecommunication and message handling using a telephony and desktop speech interface. It allowed users to place calls, take voice messages, record reminders, schedule meetings, and access traffic information.
Reference: [Schmandt94a] <author> Schmandt, Chris. </author> <title> Voice Communication with Computers. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1994. </year>
Reference-contexts: It allows a system to gracefully deal with speech recognition errors due to out-of vocabulary utterances and unpredictable phenomenon like background noise. An effective conversational approach <ref> [Schmandt94a] </ref> utilizes directive and timeout prompts, implicit, explicit and context-free confirmations, and modeling of dialogue states and transitions to better represent conversational flow. Higher levels of linguistic knowledge based on syntactic and semantic constraints as well as discourse structure can aid in conversational modeling for speech interfaces.
Reference: [Schmandt94b] <author> Schmandt, Chris. </author> <title> Multimedia Nomadic Services on Todays Hardware. </title> <journal> IEEE Network, </journal> <note> September/October 1994, pp12-21. </note>
Reference-contexts: NewsTalk [Herman95] is a speech-only conversational interface that enables interactive retrieval of personalized news over the telephone, using speech recognition, synthesis and digitized news broadcasts. The MIT Media Labs Nomadic Computing Environment <ref> [Schmandt94b] </ref> enables subscribers to manage personal information via fax, pagers and telephony access to digitized and synthesized audio. These services are integrated via Phoneshell, which provides remote telephony access to voice mail, email, calendar, rolodex, and a variety of news, weather and traffic information.
Reference: [Schmandt95] <author> Schmandt, Chris and Atty Mullins. AudioStreamer: </author> <title> Exploiting Simultaneity for Listening. </title> <booktitle> Proceedings of CHI 95, </booktitle> <pages> pp. 218-219, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: A spatial sound system can provide a strong metaphor by placing individual voices in particular spatial locations or allowing the user to create a personalized map of the audio recordings in space. The AudioStreamer <ref> [Schmandt95] </ref> detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source, allowing simultaneous browsing and listening of several news articles. Speaker differentiation and pauses were used to find story boundaries (indicated to the listener by changes in tone).
Reference: [Schroeder85] <author> Schroeder, M.R. and Atal, B.S. </author> <year> (1985). </year> <title> "Code-excited linear prediction (CELP): High-quality speech at very low bit rates," </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. 937-940. </pages>
Reference-contexts: As noted earlier, recording may be used to capture the users acoustic environment as a form of augmented memory. Applications with high storage demands can use compression algorithms to reduce storage without significantly degrading quality (see <ref> [Schroeder85] </ref> for a popular method to compress speech). Prerecorded digital speech may be used to create output prompts for an audio interface. Although the result sounds natural, the main disadvantage is that prompts must be planned in advance; the application designer must predict every necessary prompt and prerecord them.
Reference: [Starner95] <author> Starner, T. and Pentland, A. </author> <title> (1995) Real-Time American Sign Language Recognition from Video Using Hidden Markov Models. </title> <type> MIT Technical Report 375. </type>
Reference: [Starner97] <author> Starner, T. </author> <year> (1997). </year> <title> Lizzy Wearable Computer Assembly Instructions. </title> <publisher> MIT Media Laboratory. </publisher> <address> http://wearables.www.media.mit.edu/projects/wearables/ </address>
Reference-contexts: We are now porting the applications to the WAC platform and also continuing to develop the applications. 5.1 The Prototype Hardware The current applications will primarily run on the Lizzy wearable computing platform <ref> [Starner97] </ref> pictured in figure 1. The system currently consists of a 486 100 MHz CPU connected to a variety of I/O devices and memory.
Reference: [Stifelman93] <author> Stifelman, Lisa J., Barry Arons, Chris Schmandt, Eric A. Hulteen. VoiceNotes: </author> <title> A Speech Interface for Hand Held Voice Notetaker. </title> <booktitle> Proceedings of INTERCHI 93, </booktitle> <address> New York, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Espace 2 [Sawhney96] is an audio-only environment that consists of a hierarchy of hyperlinked ambient spaces containing voice conversations. Temporal audio cues indicate the presence of hyper-links, during the playback of audio content. VoiceNotes <ref> [Stifelman93] </ref> was designed as an application for a voice-controlled handheld computer which allows the creation, management and retrieval of user-authored voice notes. The interface combines speech recognition and button input, allowing the user to combine input modalities depending on what is convenient.
Reference: [Stifelman96] <author> Stifelman, Lisa J. </author> <title> Augmenting Real-World Objects: A Paper-Based Audio Notebook. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Some recent mobile applications use spatial memory in handwritten notes to access temporal indices in audio recordings. The Audio Notebook <ref> [Stifelman96] </ref> is a paper-based augmented audio system that allows a user to capture and access an audio recording synchronized with handwritten notes and page turns. Filochat [Whittaker94] also coindexes speech recording to pen-strokes of handwritten notes taken with a digital notebook.
Reference: [Wenzel92] <author> Wenzel, </author> <title> E.M. Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, 80, </volume> <year> 1992. </year>
Reference-contexts: The cocktail-party effect provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background [Bregman90]. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources <ref> [Wenzel92] </ref>. A spatial sound system can provide a strong metaphor by placing individual voices in particular spatial locations or allowing the user to create a personalized map of the audio recordings in space.
Reference: [Whittaker94] <author> Whittaker, S., Hyland, P., Wiley M. "Filochat: </author> <title> Handwritten Notes Provide Access to Recorded Conversations". </title> <booktitle> Proceedings of CHI '94, </booktitle> <year> 1994. </year>
Reference-contexts: Some recent mobile applications use spatial memory in handwritten notes to access temporal indices in audio recordings. The Audio Notebook [Stifelman96] is a paper-based augmented audio system that allows a user to capture and access an audio recording synchronized with handwritten notes and page turns. Filochat <ref> [Whittaker94] </ref> also coindexes speech recording to pen-strokes of handwritten notes taken with a digital notebook. Users can access a portion of audio data by gesturing at an associated note. Dynomite [Wilcox96] is a portable electronic notebook for the capture, search and organization of handwritten and digital audio notes.
Reference: [Wilcox94] <author> Wilcox, L., Kimber, D., Chen, F. </author> <year> (1994). </year> <title> Audio Indexing using Speaker Identification. </title> <note> Xerox PARC ISTL Technical Report No. ISTL-QCA-1994-05-04. </note>
Reference-contexts: Speaker indexing is the process of finding and indexing (assigning unique indices) to every unique speaker in a recording <ref> [Gish91, Wilcox94, Roy97b] </ref>. Both speaker identification and indexing can be used to index into recordings at speaker change locations for efficient nonlinear access. Sound classification techniques can be used to automatically segment audio.
Reference: [Wilcox97] <author> Wilcox, Lynn D., Bill N. Schilit, Nitin Sawhney. Dynomite: </author> <title> A Dynamically Organized Ink and Audio Notebook. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997, </year> <pages> pp. 186-193. </pages>
Reference: [Wold96] <author> Wold, E., T. Blum, D. Keislar, and J. Wheaton. </author> <title> Content-based Classification Search and Retrieval of Audio. </title> <journal> IEEE Multimedia Magazine, </journal> <month> Fall </month> <year> 1996. </year>
Reference-contexts: Sound classification techniques can be used to automatically segment audio. For example speech and music can be separated by characterizing statistical patterns inherent in each class of acoustics. A variety of methods can be used to classify sound patterns including clustering techniques <ref> [Wold96] </ref> or neural networks [Feiten94].
Reference: [Yankelovich95] <author> Yankelovich, N., G. Levow, and M. Marx. </author> <title> Designing SpeechActs: Issues in Speech User Interfaces. </title> <booktitle> Proceedings of CHI 95. ACM, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Higher levels of linguistic knowledge based on syntactic and semantic constraints as well as discourse structure can aid in conversational modeling for speech interfaces. SpeechActs <ref> [Yankelovich95] </ref> uses a spoken conversation metaphor and a speech-only interface to provide users with access to a variety of data, including email, calendar, weather, and information on publicly-traded stocks. The Conversational Desktop [Schmandt85] facilitated integrated office telecommunication and message handling using a telephony and desktop speech interface.
References-found: 42

