URL: http://www.cs.iastate.edu/~balakris/papers/masters.ps
Refering-URL: http://www.cs.iastate.edu/~balakris/research.html
Root-URL: http://www.cs.iastate.edu
Title: Faster Learning Approximations of Back-Propagation by Handling Flat-Spots  
Author: Karthik Balakrishnan 
Address: Ames  
Affiliation: Department of Computer Science Iowa State University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Balakrishnan, K. and Honavar, V. </author> <title> Improving Convergence of Back-Propagation by Handling Flat-Spots in the Output Layer. </title> <booktitle> In Proceedings of the Third International Conference on Artificial Neural Networks - ICANN'92, </booktitle> <address> Birmingham, UK, Septem-ber, </address> <year> 1992. </year>
Reference-contexts: Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see [7, 18, 20]). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in <ref> [1, 2, 12] </ref> 1 of learning that is motivated by pragmatic considerations. Thus, a program is said to learn over time if its performance on a given task or a range of tasks (e.g., pattern classification) improves (with respect to some suitable metric) as a result of experience.
Reference: [2] <author> Balakrishnan, K. and Honavar, V. </author> <title> Faster Learning in Multi-Layer Networks by Handling Output Layer Flat-Spots. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks - IJCNN'92, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference-contexts: Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see [7, 18, 20]). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in <ref> [1, 2, 12] </ref> 1 of learning that is motivated by pragmatic considerations. Thus, a program is said to learn over time if its performance on a given task or a range of tasks (e.g., pattern classification) improves (with respect to some suitable metric) as a result of experience.
Reference: [3] <author> Becker, S. and LeCun, Y. </author> <title> The Feasibility of Applying Numerical Optimization Techniques to Back-Propagation. </title> <editor> In D. Touretzky, G. E. Hinton, and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 29-37, </publisher> <year> 1988. </year>
Reference-contexts: With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure [22]. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications <ref> [3, 4, 13, 16, 21] </ref>. This paper focuses on methods to speed up BP by effectively dealing with the so-called flat-spot regions of the sigmoid activation function (computed by individual network nodes) where its derivative (with respect to its net input) approaches zero. <p> With the goal of fast learning, this study suggests flat-spot handling as another dimension along which improvement could be achieved. Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates <ref> [3, 4, 13] </ref>, dynamic learning rates [8], better gradient estimates [16] and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques.
Reference: [4] <author> Fahlman, S. E. </author> <title> Faster-learning variations of backpropagation : An empirical study. </title> <editor> In D. Touretzky, G. E. Hinton, and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 38-51, </publisher> <year> 1988. </year>
Reference-contexts: With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure [22]. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications <ref> [3, 4, 13, 16, 21] </ref>. This paper focuses on methods to speed up BP by effectively dealing with the so-called flat-spot regions of the sigmoid activation function (computed by individual network nodes) where its derivative (with respect to its net input) approaches zero. <p> o i = f (net i ) = 1 + e net i where, o i is the output of unit i and net i is its net input given by: net i = j 4 The sigmoid has the property that its derivative, also called sigmoid prime by Fahlman <ref> [4] </ref>, given by : o 0 approaches zero when the output of the corresponding unit approaches 0 or 1. This phenomenon is known as flat-spots (see fig 2 ). <p> This paper investigates the usefulness of flat spot handling as a method for speeding up the convergence of BP. 4 Fahlman's Sigmoid-Prime Offset Since the sigmoid-prime, o 0 i is 0, when the corresponding unit i is in a flat-spot, Fahlman <ref> [4] </ref> suggested adding a small constant, say 0.1, to the sigmoid-prime function given in eqn ( 7), thereby preventing the sigmoid-prime function from ever going to zero as shown in fig 3. <p> To do so, we borrow a technique suggested by Fahlman <ref> [4] </ref>, in which a margin is defined around each unit, and if the output of the unit is greater than the margin or less than (1-margin), then the unit is assigned to state `1' or `0' respectively. <p> loose encoder/decoder :- this problem is also like the 8-3-8, but differs in that the hidden-layer has more units than it needs to come-up with a strict binary-encoding, and hence the network has more leeway in coming up with a hidden-layer representation These problems have been extensively studied by Fahlman <ref> [4] </ref>. The two real-world data sets used in this study are : * Iris data-set :- this data-set consists of 150 examples of iris plants belonging to one of 3 species. <p> In such regions, the weight updates are negligible, even though the error may be considerable. In this study, we looked at a simple method of handling flat-spots suggested by Fahlman <ref> [4] </ref>, and also proposed two simple modifications of BP both of which handle flat-spots occurring in the output-layer. Our simulations indicate that over the range of problems studied, flat-spot handling seems a good proposition for quicker learning. <p> With the goal of fast learning, this study suggests flat-spot handling as another dimension along which improvement could be achieved. Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates <ref> [3, 4, 13] </ref>, dynamic learning rates [8], better gradient estimates [16] and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques.
Reference: [5] <author> Fahlman, S. E., and Lebiere, C. </author> <title> The Cascade-Correlation Learning Architecture. </title> <editor> In Touretzky, D. </editor> <booktitle> (Ed) Advance in Neural Information Processing Systems Vol 2, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 524-532, </pages> <year> 1990. </year>
Reference-contexts: On the other hand networks with too few units may never learn. Unfortunately, there is no proven method for choosing the network architecture and the design of algorithms that adaptively assume minimal network topologies necessary for a given task is still an area of active research <ref> [6, 7, 5] </ref>. Table 1 lists the benchmarks and their corresponding network architectures.
Reference: [6] <author> Honavar, V., and Uhr, L. </author> <title> A Network of Neuron-Like Units that Learns to Perceive by Generation as well as Reweighting of its Links. </title> <editor> In D. Touretzky, G. E. Hinton, 18 and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 472-484, </publisher> <year> 1988. </year>
Reference-contexts: On the other hand networks with too few units may never learn. Unfortunately, there is no proven method for choosing the network architecture and the design of algorithms that adaptively assume minimal network topologies necessary for a given task is still an area of active research <ref> [6, 7, 5] </ref>. Table 1 lists the benchmarks and their corresponding network architectures. <p> Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates [3, 4, 13], dynamic learning rates [8], better gradient estimates [16] and generative learning of networks <ref> [6, 7] </ref>. It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar [19] and Janakiraman & Honavar [8].
Reference: [7] <author> Honavar, V. and Uhr, L. </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <booktitle> it Information Science (Special issue on Artificial Intelligence and Neural Networks) (in press) 1992. </booktitle>
Reference-contexts: 1 Introduction The ability to learn from experience is an essential attribute of an intelligent entity. Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see <ref> [7, 18, 20] </ref>). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in [1, 2, 12] 1 of learning that is motivated by pragmatic considerations. <p> Numerous criteria can be used to evaluate learning algorithms <ref> [7] </ref> and the choice of criteria is influenced (among other things) by practical constraints of the application domain. In this paper we focus on two measures for evaluting the performance of learning algorithms that are commonly employed in the neural network literature (e.g., [17]): 1. <p> Efficacy of learning (e.g., the accuracy attained by the program on novel examples in the training set). Connectionist networks or artificial neural networks | massively parallel, highly interconnected networks of relatively simple computing elements offer an attractive and versatile paradigm for exploring inductive learning <ref> [7, 15] </ref>. <p> On the other hand networks with too few units may never learn. Unfortunately, there is no proven method for choosing the network architecture and the design of algorithms that adaptively assume minimal network topologies necessary for a given task is still an area of active research <ref> [6, 7, 5] </ref>. Table 1 lists the benchmarks and their corresponding network architectures. <p> Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates [3, 4, 13], dynamic learning rates [8], better gradient estimates [16] and generative learning of networks <ref> [6, 7] </ref>. It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar [19] and Janakiraman & Honavar [8].
Reference: [8] <author> Janakiraman, J. and Honavar, V. </author> <title> Adaptive Learning Rate for Increasing Learning Speed in Backpropagation Networks. </title> <booktitle> In Proceedings of SPIE Conference on Artificial Neural Networks, </booktitle> <address> Orlando, FL, </address> <year> 1993. </year>
Reference-contexts: The step size problem can be handled in many different ways, e.g, using dynamic learning rates <ref> [8] </ref>, but these are beyond the scope of this paper. <p> With the goal of fast learning, this study suggests flat-spot handling as another dimension along which improvement could be achieved. Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates [3, 4, 13], dynamic learning rates <ref> [8] </ref>, better gradient estimates [16] and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar [19] and Janakiraman & Honavar [8]. <p> dynamic learning rates <ref> [8] </ref>, better gradient estimates [16] and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar [19] and Janakiraman & Honavar [8]. These avenues have to be explored if BP is to be used in learning complex, real-world problems. 11 Acknowledgements I would like to thank G.
Reference: [9] <author> Kibler, D. and Langley, P. </author> <title> Machine Learning as an Experimental Science. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <address> Glasgow, Scotland, </address> <publisher> Pitman, </publisher> <year> 1988. </year>
Reference-contexts: Experiments also yield a basis for physically comparing and evaluating different learning techniques in environments where the simplifying assumptions of the underlying analytical model are violated. While there is no doubt that carefully designed experiments play a vital role in machine learning research <ref> [9] </ref>, extreme care is needed in the design, and conduct of such experiments and the interpretation and reporting of their results. Comparisons based on experimental results, in our case, computer simulations, are very susceptible to misinterpretation and misrepresentation.
Reference: [10] <author> Michalski, R. S. and Chilausky, R. L. </author> <title> Learning by being told and learning from examples: An experimental comparison of two methods of knowledge acaquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> In Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 125-160, </pages> <year> 1980. </year>
Reference-contexts: In total there are 289 examples. This has been used in a number of studies, like Michalski & Chilausky <ref> [10] </ref>, Reinke [14] and Shavlik et al. [17] 7.3 Network Architecture The network architecture plays a crucial role in both, the speed of learning and also the accuracy (generalization ability). Larger networks obviously require more work per 11 epoch.
Reference: [11] <author> Moody, J. and Darken, C. </author> <title> Learning with Localized Receptive Fields. </title> <editor> In D. Touretzky, G. E. Hinton, and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 133-143, </publisher> <year> 1988. </year>
Reference-contexts: Local activation functions like the radial basis functions, on the other hand are active only in the immediate neighborhood of a pattern that is encoded by the weights, and hence such units tend to perform better at approximating functions that lend themselves to such local representations <ref> [11] </ref>.
Reference: [12] <author> Parekh, R., Balakrishnan, K. and Honavar, V. </author> <title> An Empirical Comparison of Flat-Spot Elimination Techniques in Back-Propagation Networks. </title> <booktitle> In Proceedings of SIMTEC/WNN'92, </booktitle> <address> Clearlake, TX, </address> <month> November, </month> <year> 1992. </year>
Reference-contexts: Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see [7, 18, 20]). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in <ref> [1, 2, 12] </ref> 1 of learning that is motivated by pragmatic considerations. Thus, a program is said to learn over time if its performance on a given task or a range of tasks (e.g., pattern classification) improves (with respect to some suitable metric) as a result of experience.
Reference: [13] <author> Parker, D. B. </author> <title> Optimal Algorithms for Adaptive Networks: Second Order Back Propagation, Second Order Direct Propagation, and Second Order Hebbian Learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 593-600. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure [22]. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications <ref> [3, 4, 13, 16, 21] </ref>. This paper focuses on methods to speed up BP by effectively dealing with the so-called flat-spot regions of the sigmoid activation function (computed by individual network nodes) where its derivative (with respect to its net input) approaches zero. <p> With the goal of fast learning, this study suggests flat-spot handling as another dimension along which improvement could be achieved. Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates <ref> [3, 4, 13] </ref>, dynamic learning rates [8], better gradient estimates [16] and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques.
Reference: [14] <author> Reinke, R. </author> <title> Knowledge Acquisition and Refinement Tools for the ADVISE Meta-Expert System. </title> <type> Master's Thesis, </type> <institution> University of Illinois, Department of Computer Science, Urbana, IL, </institution> <year> 1984. </year>
Reference-contexts: In total there are 289 examples. This has been used in a number of studies, like Michalski & Chilausky [10], Reinke <ref> [14] </ref> and Shavlik et al. [17] 7.3 Network Architecture The network architecture plays a crucial role in both, the speed of learning and also the accuracy (generalization ability). Larger networks obviously require more work per 11 epoch.
Reference: [15] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <title> Learning Internal Representations by Error-Propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L. (eds), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, chapter 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, and London, England, </address> <year> 1986. </year>
Reference-contexts: Efficacy of learning (e.g., the accuracy attained by the program on novel examples in the training set). Connectionist networks or artificial neural networks | massively parallel, highly interconnected networks of relatively simple computing elements offer an attractive and versatile paradigm for exploring inductive learning <ref> [7, 15] </ref>. <p> paper focuses on a particular class of such networks, namely multi-layer feed-forward networks (i.e., the flow of input is unidirectional from the input layer to the output layer) of non-linear nodes that are trained using variants of an inductive learning algorithm called generalized delta rule (popularly known as backpropagation (BP) <ref> [15, 22] </ref>). The function computed by a feed-forward network depends on the weights on the links between its computing elements and also on their node functions. <p> Intuitively, large steps would help find a local minimum faster. However, if the steps are very large, the system could end up oscillating about a local minimum, maybe even never reaching the minimum. This is the step size problem. Rumelhart et al. <ref> [15] </ref> suggested a simple way out of this dilemma. They used a momentum term which factors in a fraction of the previous weight change into the current weight change.
Reference: [16] <author> Samad, T. </author> <title> Back Propagation With Expected Source Values. </title> <booktitle> In Neural Networks, </booktitle> <address> Vol.4,pp.615-618, </address> <year> 1991. </year> <month> 19 </month>
Reference-contexts: With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure [22]. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications <ref> [3, 4, 13, 16, 21] </ref>. This paper focuses on methods to speed up BP by effectively dealing with the so-called flat-spot regions of the sigmoid activation function (computed by individual network nodes) where its derivative (with respect to its net input) approaches zero. <p> Most of the work, so far, on improving the speed of BP has focussed on other aspects, like second-order gradient estimates [3, 4, 13], dynamic learning rates [8], better gradient estimates <ref> [16] </ref> and generative learning of networks [6, 7]. It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar [19] and Janakiraman & Honavar [8].
Reference: [17] <author> Shavlik, J. W., Mooney, R. J. and Towell, G. G. </author> <title> Symbolic and Neural Learning Algorithms : An Empirical Comparison. </title> <journal> Machine Learning, </journal> <volume> Vol 6, </volume> <year> 1991. </year>
Reference-contexts: In this paper we focus on two measures for evaluting the performance of learning algorithms that are commonly employed in the neural network literature (e.g., <ref> [17] </ref>): 1. Efficiency of learning (e.g., learning time, measured (say) by the number of passes through the training set required to learn the underlying input-output mapping to some desired accuracy). 2. Efficacy of learning (e.g., the accuracy attained by the program on novel examples in the training set). <p> In total there are 289 examples. This has been used in a number of studies, like Michalski & Chilausky [10], Reinke [14] and Shavlik et al. <ref> [17] </ref> 7.3 Network Architecture The network architecture plays a crucial role in both, the speed of learning and also the accuracy (generalization ability). Larger networks obviously require more work per 11 epoch. <p> We chose 4 hidden units for iris based on some comprehensive simulations conducted by Yang & Honavar [23]. The choice of 23 hidden units for Soybean was suggested by the work of Shavlik et al. <ref> [17] </ref>. 7.4 Parameter Settings To find an optimal choice of learning parameters one has to exhaustively search an n-dimensional space defined by the n parameters. <p> For classification problems there is another question that we have to address, namely how to certify a pattern as being correctly classified. We have adopted a technique in which we compare the highest output obtained with the desired output, a method used by Shavlik et al. <ref> [17] </ref>. In our simulations we have used a 99% correct classification convergence criterion. 7.6 Methodology for Interpreting and Reporting Results Since the random initial weights may bias the system in favor of, or against a particular run, we need to average out the effect of initial weight settings.
Reference: [18] <author> Simon, H. A. </author> <title> Why Should Machines Learn? In: </title> <editor> Michalski, R. S., Carbonell, J. G. and Mitchell, T. M. (eds), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol 1. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction The ability to learn from experience is an essential attribute of an intelligent entity. Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see <ref> [7, 18, 20] </ref>). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in [1, 2, 12] 1 of learning that is motivated by pragmatic considerations.
Reference: [19] <author> Spartz, R. and Honavar, V. </author> <title> An Empirical Analysis of the Expected Source Values Rule. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference-contexts: It is possible to use flat-spot handling in conjunction with these techniques. Such hybrid techniques may lead to faster variations of BP, as was 17 shown by Spartz & Honavar <ref> [19] </ref> and Janakiraman & Honavar [8]. These avenues have to be explored if BP is to be used in learning complex, real-world problems. 11 Acknowledgements I would like to thank G.
Reference: [20] <author> Uhr, L. </author> <title> Pattern Recognition, Learning, </title> <booktitle> and Thought. </booktitle> <address> New York, NY: </address> <publisher> Prentice Hall, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction The ability to learn from experience is an essential attribute of an intelligent entity. Learning, like intelligence, is easier to recognize than define (for examples of such definitions, see <ref> [7, 18, 20] </ref>). Researchers in machine learning generally use a working definition 1 Most of the results included in this report were published in [1, 2, 12] 1 of learning that is motivated by pragmatic considerations.
Reference: [21] <author> Watrous, R. L. </author> <title> Learning Algorithms for Connectionist Networks : Applied Gradient Methods for Non-Linear Optimization. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 619-627. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure [22]. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications <ref> [3, 4, 13, 16, 21] </ref>. This paper focuses on methods to speed up BP by effectively dealing with the so-called flat-spot regions of the sigmoid activation function (computed by individual network nodes) where its derivative (with respect to its net input) approaches zero.
Reference: [22] <author> Werbos, P. J. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis , Harvard University, </type> <year> 1974. </year>
Reference-contexts: paper focuses on a particular class of such networks, namely multi-layer feed-forward networks (i.e., the flow of input is unidirectional from the input layer to the output layer) of non-linear nodes that are trained using variants of an inductive learning algorithm called generalized delta rule (popularly known as backpropagation (BP) <ref> [15, 22] </ref>). The function computed by a feed-forward network depends on the weights on the links between its computing elements and also on their node functions. <p> Thus, BP searches the weight space using gradient descent. With suitable technical assumptions, it can be shown that such a procedure finds a set of weights that correspond to a local minimum of the error measure <ref> [22] </ref>. There have been many attempts to speed-up BP to make it more feasible for complex real-world applications [3, 4, 13, 16, 21].
Reference: [23] <author> Yang, J and Honavar, V. </author> <title> Experiments with the Cascade-Correlation Algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: The two real-world data sets used in this study are : * Iris data-set :- this data-set consists of 150 examples of iris plants belonging to one of 3 species. This was used in Yang & Honavar <ref> [23] </ref> * Soybean data-set :- this data-set describes 17 different soybean diseases each described by 50 features such as weather, time of year and descriptions of leaves and stems. In total there are 289 examples. <p> We chose 4 hidden units for iris based on some comprehensive simulations conducted by Yang & Honavar <ref> [23] </ref>. The choice of 23 hidden units for Soybean was suggested by the work of Shavlik et al. [17]. 7.4 Parameter Settings To find an optimal choice of learning parameters one has to exhaustively search an n-dimensional space defined by the n parameters.
References-found: 23

