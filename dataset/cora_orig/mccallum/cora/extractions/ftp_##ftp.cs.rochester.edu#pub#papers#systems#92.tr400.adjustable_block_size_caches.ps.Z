URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/92.tr400.adjustable_block_size_caches.ps.Z
Refering-URL: http://www.cs.rochester.edu/trs/systems-trs.html
Root-URL: 
Title: Reconciling Sharing and Spatial Locality Using Adjustable Block Size Coherent Caches  
Author: Cezary Dubnicki and Thomas J. LeBlanc 
Note: This research was supported by the National Science Foundation under grant CDA-8822724.  
Date: November 1991 (revised February 1992)  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: Technical Report 400  
Abstract: Several studies have shown that the performance of coherent caches depends on the relationship between the cache block size and the granularity of sharing and locality exhibited by the program. Large cache blocks exploit processor and spatial locality, but may cause unnecessary cache invalidations due to false sharing. Small cache blocks can reduce the number of cache invalidations, but increase the number of bus or network transactions required to load data into the cache. To reduce the performance impact of a mismatch between the cache block size and the sharing pattern exhibited by a given application, we propose to adjust the amount of data stored in a cache line dynamically according to recent reference patterns. In this scheme, cache blocks are split across cache lines when false sharing occurs, and merged back into a single cache line to exploit spatial locality. Results of simulations of a scalable multiprocessor indicate that, over a range of applications, an adjustable block size cache performs better than every fixed block size alternative. Moreover, for a given program, the adjustable block size cache is comparable in performance to the best fixed block size cache for that program. We conclude that adjusting the block size in response to reference behavior can significantly improve performance, especially when there is variability in the granularity of sharing exhibited by applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under Mach. </title> <booktitle> In Proc. of the 1988 ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: fraction of private objects F private = 0:8 * fraction of writes F writes = 0:2 * average number of references to each word of data N ref = 5 The value of the fraction of shared objects (20%) is consistent with measurements of real programs found in the literature <ref> [1] </ref>. The value chosen for the fraction of writes (20%) has been used in earlier studies [19], and is consistent with values reported in the literature. The default value for the number of references was chosen so that, on average, each word would be written once.
Reference: [2] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation. Both programs, which were written for a bus-based multi processor, exhibit fine-grain sharing and poor processor locality. * Presto applications Four programs were implemented in Presto <ref> [2] </ref>. P-life is a parallel implementation of Conway's game of life, p-qsort implements parallel quicksort, p-matmult is a matrix multiplication program, and p-gauss is a Gaussian elimination program. Like the SPLASH programs, these applications were written for a bus-based multiprocessor, and exhibit fine-grain sharing and poor processor locality.
Reference: [3] <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA policies and their relation to memory architecture. </title> <booktitle> Proc. 4th ASPLOS, </booktitle> <pages> pages 212-221, </pages> <year> 1991. </year>
Reference-contexts: In addition to our work, these traces have been used by others to study memory management policies and the performance benefits of disabling coherence to minimize short-term false sharing <ref> [3] </ref>. The programs in our trace suite fall into three different classes: * C-threads applications The majority of the programs were implemented using the C-threads package on a non-uniform memory access (NUMA) multiprocessor. Sorbyr and sorbyc implement red-black successive over-relaxation.
Reference: [4] <author> L. Bomans, D. Roose, and R. Hempel. </author> <title> The Argonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the intel iPSC/2. </title> <journal> Journal of Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-132, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Gauss is a Gaussian elimination program. All of these programs exhibit coarse-grain parallelism and good processor locality. * SPLASH applications Two programs were taken from the SPLASH suite [17]. These programs were written in C using the Argonne macros for parallel constructs <ref> [4] </ref>. S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation. Both programs, which were written for a bus-based multi processor, exhibit fine-grain sharing and poor processor locality. * Presto applications Four programs were implemented in Presto [2].
Reference: [5] <author> L.M. Censier and P. Feautier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 27 </volume> <pages> 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Each processing node contains a processor, a processor cache, and a memory module. Data coherence is maintained in hardware using a directory protocol <ref> [5] </ref>. There are four levels in the memory hierarchy. The first level consists of the local processor caches. Local memory is the second level. The third level includes the caches of all other nodes. The fourth level consists of all nonlocal memory modules.
Reference: [6] <author> D.R. Cheriton, H.A. Goosen, and P. Machanick. </author> <title> Restructuring a parallel simulation to improve cache behavior in a shared-memory mutiprocessor: A first experience. </title> <booktitle> In Intl. Symp. on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Using a variety of program restructuring tech-niques, Cheriton et al <ref> [6] </ref> were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
Reference: [7] <author> D.R. Cheriton, A. Gupta, P.D. Boyle, and H.A. Goosen. </author> <title> The VMP multiprocessor: Initial experience, refinements, and performance evaluation. </title> <booktitle> In Proc. 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 410-421, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Simulation results of three parallel applications on the VMP multiprocessor also show that no single block size is best for all programs <ref> [7] </ref>. The best block size for the three programs considered varied between 32 and 132 bytes.
Reference: [8] <author> M. Dubois and J. Wang. </author> <title> Shared data contention in a cache coherence protocol. </title> <booktitle> In Proc. 1988 Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 146-155, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our workload model is similar to the models used in <ref> [8; 15; 19] </ref>. There are a few differences between the experiments with a synthetic workload and our earlier experiments with traces from real applications including: * Merging of cache blocks was not implemented, and the initial block size was set to 32 instead of 16. <p> To measure this effect, the experiment from figure 3 was repeated, applying the fraction of writes to a burst of references to objects, as done in <ref> [8] </ref>. In this new experiment, the burst of references to an object contains all writes with probability F writes, and all reads with probability 1F writes. Thus, we significantly decrease the number of large objects that are modified, and thereby decrease the number of invalidations.
Reference: [9] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proc. 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [9; 10; 13] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> The mismatch between block size and program sharing patterns has been studied both in the context of bus-based machines and scalable multiprocessors. Eggers and Katz <ref> [9; 10] </ref> examined the effect of sharing on cache and bus performance using traces from four application programs. They showed that miss ratios for some parallel programs increase when the cache line size increases, while for other programs the miss ratio decreases with an increase in line size. <p> In the applications discussed in [13], shared writes constituted 17% (Maxflow), 5.6% (SA-TSP), 19% (MP3D), 6.8% (PTHOR) and 5% (LocusRoute) of all shared references. In <ref> [9] </ref> the figures are 7% (PLOVER), 22% (PSPICE), 10% (PUPPY), and 2% (TOPOPT). For Splash applications executed with 32 processors the figures are 18% (Ocean), 12% (Water), 40% (MP3D), 12% (LocusRoute), 7% (PTHOR) and 14% (Cholesky).
Reference: [10] <author> S.J. Eggers and R.H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proc. 3rd ASPLOS, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [9; 10; 13] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> However, if cache blocks are too large, then false sharing [18] is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations <ref> [10] </ref>. Since the granularity of sharing can vary widely among programs, the likelihood of a mismatch between the block size and the object sizes used by some programs is high. <p> The mismatch between block size and program sharing patterns has been studied both in the context of bus-based machines and scalable multiprocessors. Eggers and Katz <ref> [9; 10] </ref> examined the effect of sharing on cache and bus performance using traces from four application programs. They showed that miss ratios for some parallel programs increase when the cache line size increases, while for other programs the miss ratio decreases with an increase in line size. <p> is to structure the application based on knowledge of the cache organization, allocating separate objects to separate cache lines [18], grouping objects according to sharing patterns [6; 1 The size of an invalidation is the number of caches containing the data to be invalidated. 1 11], and avoiding fine-grain sharing <ref> [10] </ref>. Using a variety of program restructuring tech-niques, Cheriton et al [6] were able to achieve an order of magnitude improvement in the cache miss ratio for shared data in large (128 bytes) cache lines.
Reference: [11] <author> S.J. Eggers and Jeremiassen T.E. </author> <title> Eliminating false sharing. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> December </month> <year> 1990. </year>
Reference: [12] <author> A. Gupta, J. Hennessy, K. Gharachorloo, Todd Mowry, and W.D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proc. 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The incremental cost of sending an additional word in a message is A. This parameter is equal to 10 in our simulations. These parameter values are representative of the costs we would expect to incur in a scalable multiprocessor. As in the DASH multiprocessor <ref> [12] </ref>, a cache fill from a remote node can be expensive. <p> It is interesting, therefore, to understand how this fraction varies across applications. The fraction of writes to shared data for the 3 benchmarks on 16 processors discussed in <ref> [12] </ref> are 31% (MP3D), 33% (LU), and 11% (PTHOR). In the applications discussed in [13], shared writes constituted 17% (Maxflow), 5.6% (SA-TSP), 19% (MP3D), 6.8% (PTHOR) and 5% (LocusRoute) of all shared references. In [9] the figures are 7% (PLOVER), 22% (PSPICE), 10% (PUPPY), and 2% (TOPOPT).
Reference: [13] <author> A. Gupta and W.D. Weber. </author> <title> Analysis of cache invalidation patterns in shared-memory multiprocessors. </title> <booktitle> In Cache and Interconnect Architectures in Multiprocessors, </booktitle> <pages> pages 83-108. </pages> <publisher> Kulwer Academic Publishers, </publisher> <year> 1990. </year> <month> 40 </month>
Reference-contexts: 1 Introduction Several studies have shown that the performance of coherent caches depends on the relationship between the granularity of sharing and locality exhibited by a program and the cache block size (the amount of data stored in a single cache line) <ref> [9; 10; 13] </ref>. Large cache blocks improve application performance when there is substantial processor and spatial locality (i.e. all words allocated to one cache line are accessed by one processor in some window of time). <p> If cache blocks are smaller than the data objects used on a per-processor basis, then accessing a single object can generate references to multiple cache lines, increasing the number of invalidations and decreasing the beneficial effects of prefetching <ref> [13] </ref>. However, if cache blocks are too large, then false sharing [18] is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations [10]. <p> Simulation results of three parallel applications on the VMP multiprocessor also show that no single block size is best for all programs [7]. The best block size for the three programs considered varied between 32 and 132 bytes. Gupta and Weber <ref> [13] </ref> examined the effect of cache block size on the number and size of invalidations in a multiprocessor system with a directory-based cache coherency protocol. 1 With four-byte blocks, most shared writes generated one invalidation. <p> It is interesting, therefore, to understand how this fraction varies across applications. The fraction of writes to shared data for the 3 benchmarks on 16 processors discussed in [12] are 31% (MP3D), 33% (LU), and 11% (PTHOR). In the applications discussed in <ref> [13] </ref>, shared writes constituted 17% (Maxflow), 5.6% (SA-TSP), 19% (MP3D), 6.8% (PTHOR) and 5% (LocusRoute) of all shared references. In [9] the figures are 7% (PLOVER), 22% (PSPICE), 10% (PUPPY), and 2% (TOPOPT).
Reference: [14] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory--based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, each tag cache entry contains a field for the size of the data block. Both processor caches and tag caches are fully associative. 4 3.2 Coherency Protocol Data coherence is maintained using a variant of the write-invalidate, ownership protocol for the DASH multiprocessor <ref> [14] </ref>. Under this protocol, each data block is assigned to a memory module. The node containing the memory module to which a given data block is assigned is called the home node for the data block.
Reference: [15] <author> H.E. Mizrahi, J. Baer, E.D. Lazowska, and J. Zahorjan. </author> <title> Introducing memory into the switching elements of multiprocessor interconnection networks. </title> <booktitle> In Proc. 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 158-166, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Our workload model is similar to the models used in <ref> [8; 15; 19] </ref>. There are a few differences between the experiments with a synthetic workload and our earlier experiments with traces from real applications including: * Merging of cache blocks was not implemented, and the initial block size was set to 32 instead of 16. <p> In all of the previous experiments, the fraction of writes was applied to each individual reference, rather than to a burst of references within an object. A similar assumption has been used in other studies <ref> [15; 19] </ref>.
Reference: [16] <author> B.W. O'Krafka and R.A. </author> <title> Newton. An empirical evaluation of two memory-efficient directory methods. </title> <booktitle> In Proc. 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The first level consists of the local processor caches. Local memory is the second level. The third level includes the caches of all other nodes. The fourth level consists of all nonlocal memory modules. Each memory module contains random access memory and a tag cache <ref> [16] </ref>, which is used to implement a distributed directory. Each tag cache is indexed by block address. A tag cache line maintains a bit vector of nodes that currently have a copy of the data block indexed by the tag cache line.
Reference: [17] <author> J.P. Singh, W-D. Weber, and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <type> Technical report, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Gauss is a Gaussian elimination program. All of these programs exhibit coarse-grain parallelism and good processor locality. * SPLASH applications Two programs were taken from the SPLASH suite <ref> [17] </ref>. These programs were written in C using the Argonne macros for parallel constructs [4]. S-cholesky performs parallel factorization of a sparse matrix; S-mp3d solves a problem in fluid flow simulation.
Reference: [18] <author> J. Torrellas, M.S. Lam, and J.L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proc. 1990 Intl. Conf. on Parallel Processing Volume II, </booktitle> <pages> pages 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: If cache blocks are smaller than the data objects used on a per-processor basis, then accessing a single object can generate references to multiple cache lines, increasing the number of invalidations and decreasing the beneficial effects of prefetching [13]. However, if cache blocks are too large, then false sharing <ref> [18] </ref> is introduced (wherein two data items not being shared happen to reside in the same cache line), which also increases the number of invalidations [10]. <p> One general technique to achieve such a match is to structure the application based on knowledge of the cache organization, allocating separate objects to separate cache lines <ref> [18] </ref>, grouping objects according to sharing patterns [6; 1 The size of an invalidation is the number of caches containing the data to be invalidated. 1 11], and avoiding fine-grain sharing [10].
Reference: [19] <author> Q. Yiang, G. Thangadurai, and L. Bhuyan. </author> <title> An adaptive cache coherence scheme for hierarchical shared-memory multiprocessors. </title> <booktitle> In Proc. 2nd IEEE Symp. on Parallel and Distributed Systems, </booktitle> <pages> pages 318-325, </pages> <month> December </month> <year> 1990. </year> <month> 41 </month>
Reference-contexts: Our workload model is similar to the models used in <ref> [8; 15; 19] </ref>. There are a few differences between the experiments with a synthetic workload and our earlier experiments with traces from real applications including: * Merging of cache blocks was not implemented, and the initial block size was set to 32 instead of 16. <p> The value chosen for the fraction of writes (20%) has been used in earlier studies <ref> [19] </ref>, and is consistent with values reported in the literature. The default value for the number of references was chosen so that, on average, each word would be written once. <p> In all of the previous experiments, the fraction of writes was applied to each individual reference, rather than to a burst of references within an object. A similar assumption has been used in other studies <ref> [15; 19] </ref>.
References-found: 19

