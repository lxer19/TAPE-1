URL: http://www.cs.nyu.edu/vijayk/papers/stack-sc95.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: fjplevyak,vijayk,zhang,achieng@cs.uiuc.edu  
Title: A Hybrid Execution Model for Fine-Grained Languages on Distributed Memory Multicomputers  
Author: John Plevyak Vijay Karamcheti Xingbin Zhang Andrew A. Chien 
Keyword: Acknowledgements  
Address: 1304 W. Springfield Avenue Urbana, IL 61801  
Affiliation: Department of Computer Science  
Abstract: While fine-grained concurrent languages can naturally capture concurrency in many irregular and dynamic problems, their flexibility has generally resulted in poor execution efficiency. In such languages the computation consists of many small threads which are created dynamically and synchronized implicitly. In order to minimize the overhead of these operations, we propose a hybrid execution model which dynamically adapts to runtime data layout, providing both sequential efficiency and low overhead parallel execution. This model uses separately optimized sequential and parallel versions of code. Sequential efficiency is obtained by dynamically coalescing threads via stack-based execution and parallel efficiency through latency hiding and cheap synchronization using heap-allocated activation frames. Novel aspects of the stack mechanism include handling return values for futures and executing forwarded messages (the responsibility to reply is passed along, like call/cc in Scheme) on the stack. In addition, the hybrid execution model is expressed entirely in C, and therefore is easily portable to many systems. Experiments with function-call intensive programs show that this model achieves sequential efficiency comparable to C programs. Experiments with regular and irregular application kernels on the CM-5 and T3D demonstrate that it can yield 1.5 to 3 times better performance than code optimized The authors recognize the contributions of the other members of the Illinois Concert project: Hao-Hua Chu, Julian Dolby and Mark Gardner. We also thank the anonymous reviewers for their useful comments. The research described in this paper was supported in part by NSF grants CCR-92-09336, MIP-92-23732 and CDA-94-01124, ONR grants N00014-92-J-1961 and N00014-93-1-1086, NASA grant NAG 1-613, and a special-purpose grant from the AT&T Foundation. Andrew Chien is supported in part by NSF Young Investigator Award CCR-94-57809. Xingbin Zhang is supported by Computational Science and Engineering program of the University of Illinois at Urbana-Champaign. The experiments reported in this paper were conducted on the CM-5 at the National Center for Supercomputing Applications and the T3D at the Pittsburgh Supercomputing Center. for parallel execution alone.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: While the mechanisms described in this paper were developed for concurrent object-oriented languages [20, 33] based 15 on the Actor model <ref> [18, 9, 1] </ref>, we believe they are applicable to other programming models that sup-port implicit synchronization and communication. In particular, this model is useful as a compiler target.
Reference: [2] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <type> Technical report, </type> <institution> The Institute for Advanced Study, Princeton, </institution> <address> New Jersey, </address> <year> 1986. </year>
Reference-contexts: In addition, modern algorithms for such problems depend increasingly on sophisticated data structures to achieve high efficiency <ref> [2, 13, 4] </ref>. In this domain, a program implementation must adapt itself to the irregular and even dynamic structure of the data (exploiting locality where available) to achieve high performance. This is particularly important when good data distributions are used which clump parts of data structures together.
Reference: [3] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, Andrew Shaw, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <note> To appear in PPoPP '95, July 1995. Available via anonymous FTP from theory.lcs.mit.edu in /pub/cilk/cilkpaper.ps.Z. </note>
Reference-contexts: However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization. More recently, lazy task creation [25], leapfrogging [32] and other schemes <ref> [22, 3] </ref> have addressed load balancing problems resulting from serialization by stealing work from previously deferred stack frames. However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines.
Reference: [4] <author> D. A. </author> <title> Case. Computer simulations of protein dynamics and thermodynamics. </title> <journal> IEEE Computer, </journal> <volume> 26:47, </volume> <year> 1993. </year>
Reference-contexts: In addition, modern algorithms for such problems depend increasingly on sophisticated data structures to achieve high efficiency <ref> [2, 13, 4] </ref>. In this domain, a program implementation must adapt itself to the irregular and even dynamic structure of the data (exploiting locality where available) to achieve high performance. This is particularly important when good data distributions are used which clump parts of data structures together.
Reference: [5] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert system compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Technical Report UIUCDCS-R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In addition, our implementation is portable; written entirely in C, it is not specific to the stack structure on any particular machine. This execution model is used by the Illinois Concert system <ref> [5] </ref>, a compiler and runtime which achieves efficient execution of concurrent object-oriented programs. <p> The flexible parallel-sequential execution model presented in this paper dynamically adapts for parallel or sequential execution and provides a hierarchy of calling schemas of increasingly power and cost. This model is part of the Illinois Concert system <ref> [5, 7] </ref> which consists of a globally optimizing compiler [28] and runtime [23]. The compiler is capable of resolving interprocedural control and data flow information [27] which enables the use of specialized calling conventions based on the synchronization features required by the called method. <p> These checks determine whether or not a invocation can complete immediately, and are used to suspend the caller and to speculatively inline <ref> [5] </ref> invocations on local and 2 On a SPARC with register windows, a C funtion call costs 5 instructions but it is more likely to be between 10-15 instructions on other processors. 3 The relative performance of fib and tak is a result of the comparatively aggressive inlining of our compiler.
Reference: [6] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: The fallback code creates the callee's context, saves local state into it, and propagates the fall back by returning this context to its caller which then sets up the linkage. 3.2.3 Continuation Passing: Lazy Continuation Creation Explicit continuation passing can improve the composability of concurrent programs <ref> [33, 6] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit.
Reference: [7] <author> Andrew A. Chien and Julian Dolby. </author> <title> The Illinois Concert system: A problem-solving environment for irregular applications. </title> <booktitle> In Proceedings of DAGS'94, The Symposium on Parallel Computation and Problem Solving Environments., </booktitle> <year> 1994. </year>
Reference-contexts: The flexible parallel-sequential execution model presented in this paper dynamically adapts for parallel or sequential execution and provides a hierarchy of calling schemas of increasingly power and cost. This model is part of the Illinois Concert system <ref> [5, 7] </ref> which consists of a globally optimizing compiler [28] and runtime [23]. The compiler is capable of resolving interprocedural control and data flow information [27] which enables the use of specialized calling conventions based on the synchronization features required by the called method.
Reference: [8] <author> Andrew A. Chien, Vijay Karamcheti, John Plevyak, and Xingbin Zhang. </author> <title> Concurrent aggregates language report 2.0. </title> <note> Available via anonymous ftp from cs.uiuc.edu in /pub/csag or from http://www-csag.cs.uiuc.edu/, September 1993. </note>
Reference-contexts: This execution model is used by the Illinois Concert system [5], a compiler and runtime which achieves efficient execution of concurrent object-oriented programs. This system compiles ICC++, a parallel dialect of C++ [14], and Concurrent Aggregates (CA) <ref> [8] </ref> for execution on workstations, the TMC CM5 [31] and the Cray T3D [10] simply by recompiling and linking with a machine specific version of the runtime. <p> Moreover, it can adapt to runtime data layout reducing the penalty for imperfect compile time information, irregular computations and poor programmer data distribution. As the target for the Concert system it supports both the ICC++ [14] and Concurrent Aggregates <ref> [8] </ref> languages. Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency [16, 24]. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization.
Reference: [9] <author> William D. Clinger. </author> <title> Foundations of actor semantics. </title> <type> Technical Report AI-TR-633, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1981. </year>
Reference-contexts: While the mechanisms described in this paper were developed for concurrent object-oriented languages [20, 33] based 15 on the Actor model <ref> [18, 9, 1] </ref>, we believe they are applicable to other programming models that sup-port implicit synchronization and communication. In particular, this model is useful as a compiler target.
Reference: [10] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: This system compiles ICC++, a parallel dialect of C++ [14], and Concurrent Aggregates (CA) [8] for execution on workstations, the TMC CM5 [31] and the Cray T3D <ref> [10] </ref> simply by recompiling and linking with a machine specific version of the runtime. We have evaluated the performance of the execution model for both sequential and parallel execution on a suite of programs written by different authors with a variety of programming styles.
Reference: [11] <editor> A. Krishnamuthy et al. </editor> <booktitle> Parallel programming in Split-C. In Proceedings of Supercomputing, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: Thus, these remote invocations avoid the overheads of context creation and thread scheduling which are otherwise present. 4.3.3 Irregular Parallel Code: EM3D EM3D is an application kernel which models propagation of electromagnetic waves <ref> [11] </ref>. The data structure is a graph containing nodes for the electric field and for the magnetic field with edges between nodes of different types. A simple linear function is computed at each node based on the 14 values carried along the edges.
Reference: [12] <author> Seth Copen Goldstein, Klaus Eric Schauser, and David Culler. </author> <title> Lazy threads, stacklets, and synchronizers: Enabling primitives for parallel languages. </title> <booktitle> In Proceedings of POOMA'94, </booktitle> <year> 1994. </year>
Reference-contexts: However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines. Several recent thread management systems have been targeted to distributed memory machines. Two of them, Olden [29] and Stacklets <ref> [12] </ref> use the same mechanism for work generation and communication. Furthermore, they require specialized calling conventions limiting their portability. StackThreads [30] used by ABCL/f has a portable implementation. However, this system also uses a single calling convention, and allocates futures separate from the context.
Reference: [13] <author> L. Greengard and V Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73 </volume> <pages> 325-48, </pages> <year> 1987. </year>
Reference-contexts: In addition, modern algorithms for such problems depend increasingly on sophisticated data structures to achieve high efficiency <ref> [2, 13, 4] </ref>. In this domain, a program implementation must adapt itself to the irregular and even dynamic structure of the data (exploiting locality where available) to achieve high performance. This is particularly important when good data distributions are used which clump parts of data structures together. <p> Thus, those portions of the program which do not require the extra power of the programming model are not penalized. 6 3.2.2 May-block: Lazy Context Allocation callee_context = may_block_method (&return_val,...); if (callee_context != NULL) - // fallback code context = create_context (); callee_context-&gt;continuation = make_continuation (context <ref> [13] </ref>); save_state_to_heap (context); return context; // propagate blocking - while the figure on the right shows the stack unwinding when the call cannot be completed. In the may-block case, the calling schema distinguishs the two outcomes successful completion and a blocked method.
Reference: [14] <institution> The Concurrent Systems Architecture Group. </institution> <note> The ICC++ reference manual, version 1.0. Technical report, </note> <institution> University of Illinois, Department of Computer Science, 1304 W. Springfield Avenue, Urbana, Illinois, </institution> <year> 1995. </year> <note> Also available from http://www-csag.cs.uiuc.edu/. </note>
Reference-contexts: This execution model is used by the Illinois Concert system [5], a compiler and runtime which achieves efficient execution of concurrent object-oriented programs. This system compiles ICC++, a parallel dialect of C++ <ref> [14] </ref>, and Concurrent Aggregates (CA) [8] for execution on workstations, the TMC CM5 [31] and the Cray T3D [10] simply by recompiling and linking with a machine specific version of the runtime. <p> Moreover, it can adapt to runtime data layout reducing the penalty for imperfect compile time information, irregular computations and poor programmer data distribution. As the target for the Concert system it supports both the ICC++ <ref> [14] </ref> and Concurrent Aggregates [8] languages. Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency [16, 24]. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization.
Reference: [15] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1992. </year>
Reference-contexts: In Section 2, we give the relevant background, describing irregular computational structures as well as the basic programming and execution models. Section 3 presents our hybrid stack-heap 1 While data layout for such languages is an important issue and an area of much research <ref> [15] </ref>, in this paper we focus on efficient execution with respect to a data placement. 2 execution mechanism. The effectiveness of this mechanism is demonstrated on a number of fine--grained concurrent programs in Section 4.
Reference: [16] <author> Robert H. Halstead Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: These tools are implicit synchronization, 3 fine-grained concurrency, location independence, implicit locking, and first class continuations. Pro--grammers are encouraged to expose concurrency (non-binding parallelism) in both blocks of statements and loops. Implicit synchronization is provided via implicit futures <ref> [16] </ref>, which synchronize lazily on a value or a set of values in the case of a parallel loop. Introducing the futures implicitly solves the problem of future placement, but dramatically increases the density of futures over models where they are inserted manually. <p> As the target for the Concert system it supports both the ICC++ [14] and Concurrent Aggregates [8] languages. Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency <ref> [16, 24] </ref>. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization.
Reference: [17] <author> J. Hermans and M. Carson. </author> <title> Cedar documentation. Unpublished manual for CEDAR, </title> <year> 1985. </year>
Reference-contexts: Our measured value of 2.3 comes close to this maximum. 4.3.2 Irregular Parallel Code: MD-Force MD-Force is the kernel of the nonbonded force computation phase of a molecular dynamics simulation of proteins <ref> [17] </ref>. The computation iterates over a set of atom pairs that are within a spatial cutoff radius. Each iteration updates the force fields of neighboring atoms using their current coordinates, resulting in irregular data access patterns because of the spatial nature of data sharing.
Reference: [18] <author> C. Hewitt and H. Baker. </author> <title> Actors and continuous functionals. </title> <booktitle> In Proceedings of the IFIP Working Conference on Formal Description of Programming Concepts, </booktitle> <pages> pages 367-87, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: While the mechanisms described in this paper were developed for concurrent object-oriented languages [20, 33] based 15 on the Actor model <ref> [18, 9, 1] </ref>, we believe they are applicable to other programming models that sup-port implicit synchronization and communication. In particular, this model is useful as a compiler target.
Reference: [19] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: Fine-grained concurrent languages which provide a shared name space, implicit synchronization and implicit concurrency control can dramatically simplify the programming of irregular applications. Such languages typically express computations as a collection of light-weight threads executing local to the data they compute (the owner computes rule <ref> [19] </ref>). However, the cost of creating and synchronizing these threads can be high. Given a data layout 1 we propose an execution model which adapts to exploit runtime locality by merging threads, eliminating unnecessary concurrency overhead.
Reference: [20] <author> W. Horwat, A. Chien, and W. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-9. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit. Since one of our goals is to execute forwarded invocations <ref> [20] </ref> on the stack, lazy allocation of the continuation is essential. <p> While the mechanisms described in this paper were developed for concurrent object-oriented languages <ref> [20, 33] </ref> based 15 on the Actor model [18, 9, 1], we believe they are applicable to other programming models that sup-port implicit synchronization and communication. In particular, this model is useful as a compiler target.
Reference: [21] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the message-driven processor. </title> <type> Master's thesis, </type> <institution> Mas-sachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: Our programming model supports a wide variety of synchronization and communication structures including: synchronous (RPC), data (object) parallel, reactive and even custom communication and synchronization structures constructed as convenient for the application. In addition, continuations (the right to determine a future) can be forwarded <ref> [21] </ref> to another call, passed as arguments and stored in data structures. Graphical representations of these structures, all of which are supported by the ICC++ and CA langauges, appear in Figure 3. The flexibility of this programming model enables the programmer to select the mechanisms most appropriate for the application.
Reference: [22] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A foundation for an efficient multi-threaded Scheme system. </title> <booktitle> In Proc. 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 345-357, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization. More recently, lazy task creation [25], leapfrogging [32] and other schemes <ref> [22, 3] </ref> have addressed load balancing problems resulting from serialization by stealing work from previously deferred stack frames. However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines.
Reference: [23] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year> <month> 17 </month>
Reference-contexts: The flexible parallel-sequential execution model presented in this paper dynamically adapts for parallel or sequential execution and provides a hierarchy of calling schemas of increasingly power and cost. This model is part of the Illinois Concert system [5, 7] which consists of a globally optimizing compiler [28] and runtime <ref> [23] </ref>. The compiler is capable of resolving interprocedural control and data flow information [27] which enables the use of specialized calling conventions based on the synchronization features required by the called method. <p> The remaining overhead is due to parallelization. Seq-opt shows the performance when this overhead is eliminated. Since the generated executables can be run directly on parallel machines, they include paral-lelization overhead in the form of name translation, locality and concurrency checks; the cost of which are discussed in <ref> [23] </ref>.
Reference: [24] <author> D. Kranz, R. Halstead Jr., and E. Mohr. Mul-T: </author> <title> A high-performance parallel lisp. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <year> 1989. </year>
Reference-contexts: As the target for the Concert system it supports both the ICC++ [14] and Concurrent Aggregates [8] languages. Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency <ref> [16, 24] </ref>. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization.
Reference: [25] <author> E. Mohr, D. Kranz, and R. Halstead Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency [16, 24]. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization. More recently, lazy task creation <ref> [25] </ref>, leapfrogging [32] and other schemes [22, 3] have addressed load balancing problems resulting from serialization by stealing work from previously deferred stack frames. However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines.
Reference: [26] <author> John Plevyak and Andrew Chien. </author> <title> Efficient cloning to eliminate dynamic dispatch in object-oriented languages. </title> <note> Submitted for Publication, </note> <year> 1995. </year>
Reference-contexts: Because choosing the correct schema can depend on non-local (transitive) properties, our compiler performs a global flow analysis which conservatively determines the blocking and continuation requirements of methods and uses that information to select the appropriate schema <ref> [27, 26] </ref>. Since only one sequential version of each method is generated, this classification of the methods determines the calling convention which must be used when the method is invoked.
Reference: [27] <author> John Plevyak and Andrew A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA, </booktitle> <year> 1994. </year>
Reference-contexts: This model is part of the Illinois Concert system [5, 7] which consists of a globally optimizing compiler [28] and runtime [23]. The compiler is capable of resolving interprocedural control and data flow information <ref> [27] </ref> which enables the use of specialized calling conventions based on the synchronization features required by the called method. <p> Because choosing the correct schema can depend on non-local (transitive) properties, our compiler performs a global flow analysis which conservatively determines the blocking and continuation requirements of methods and uses that information to select the appropriate schema <ref> [27, 26] </ref>. Since only one sequential version of each method is generated, this classification of the methods determines the calling convention which must be used when the method is invoked.
Reference: [28] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The flexible parallel-sequential execution model presented in this paper dynamically adapts for parallel or sequential execution and provides a hierarchy of calling schemas of increasingly power and cost. This model is part of the Illinois Concert system [5, 7] which consists of a globally optimizing compiler <ref> [28] </ref> and runtime [23]. The compiler is capable of resolving interprocedural control and data flow information [27] which enables the use of specialized calling conventions based on the synchronization features required by the called method.
Reference: [29] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1995. </year>
Reference-contexts: However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines. Several recent thread management systems have been targeted to distributed memory machines. Two of them, Olden <ref> [29] </ref> and Stacklets [12] use the same mechanism for work generation and communication. Furthermore, they require specialized calling conventions limiting their portability. StackThreads [30] used by ABCL/f has a portable implementation. However, this system also uses a single calling convention, and allocates futures separate from the context.
Reference: [30] <author> Kenjiro Taura, Satoshi Matsuoka, and Akinori Yonezawa. StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock cpus. </title> <booktitle> In Joint Symposium on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Several recent thread management systems have been targeted to distributed memory machines. Two of them, Olden [29] and Stacklets [12] use the same mechanism for work generation and communication. Furthermore, they require specialized calling conventions limiting their portability. StackThreads <ref> [30] </ref> used by ABCL/f has a portable implementation. However, this system also uses a single calling convention, and allocates futures separate from the context. Thus, an additional memory reference is required to touch futures.
Reference: [31] <institution> Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This execution model is used by the Illinois Concert system [5], a compiler and runtime which achieves efficient execution of concurrent object-oriented programs. This system compiles ICC++, a parallel dialect of C++ [14], and Concurrent Aggregates (CA) [8] for execution on workstations, the TMC CM5 <ref> [31] </ref> and the Cray T3D [10] simply by recompiling and linking with a machine specific version of the runtime. We have evaluated the performance of the execution model for both sequential and parallel execution on a suite of programs written by different authors with a variety of programming styles.
Reference: [32] <author> David B. Wagner and Bradley G. Calder. </author> <title> Leapfrogging: A portable technique for implementing efficient futures. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 208-217, </pages> <year> 1993. </year>
Reference-contexts: Several languages supporting explicit futures on shared-memory machines have focused on restricting concurrency for efficiency [16, 24]. However, unlike our programming model where almost all values define futures, futures occur less frequently in these systems, decreasing the importance of their optimization. More recently, lazy task creation [25], leapfrogging <ref> [32] </ref> and other schemes [22, 3] have addressed load balancing problems resulting from serialization by stealing work from previously deferred stack frames. However, none of these approaches deals with locality constraints arising from data placements and local address spaces on distributed memory machines.
Reference: [33] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: The fallback code creates the callee's context, saves local state into it, and propagates the fall back by returning this context to its caller which then sets up the linkage. 3.2.3 Continuation Passing: Lazy Continuation Creation Explicit continuation passing can improve the composability of concurrent programs <ref> [33, 6] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit. <p> While the mechanisms described in this paper were developed for concurrent object-oriented languages <ref> [20, 33] </ref> based 15 on the Actor model [18, 9, 1], we believe they are applicable to other programming models that sup-port implicit synchronization and communication. In particular, this model is useful as a compiler target.
References-found: 33

