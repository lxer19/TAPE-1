URL: ftp://eivind.imm.dtu.dk/pub/cyril/chap5.ps.gz
Refering-URL: http://eivind.imm.dtu.dk/staff/goutte/PUBLIS/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: 5 Bayesian estimation 5.1 Introduction  
Note: 73  
Abstract: This chapter takes a different standpoint to address the problem of learning. We will here reason only in terms of probability, and make extensive use of the chain rule known as "Bayes' rule". A fast definition of the basics in probability is provided in appendix A for quick reference. Most of this chapter is a review of the methods of Bayesian learning applied to our modelling purposes. Some original analyses and comments are also provided in section 5.8, 5.11 and 5.12. There is a latent rivalry between "Bayesian" and "Orthodox" statistics. It is by no means our intention to enter this kind of controversy. We are perfectly willing to accept orthodox as well as unorthodox methods, as long as they are scientifically sound and provide good results when applied to learning tasks. The same disclaimer applies to the two frameworks presented here. They have been the object of heated controversy in the past 3 years in the neural networks community. We will not take side, but only present both frameworks, with their strong points and their weaknesses. In the context of this work, the "Bayesian frameworks" are especially interesting as the provide some continuous update rules that can be used during regularised cost minimisation to yield an automatic selection of the regularisation level. Unlike the methods presented in chapter 3, it is not necessary to try several regularisation levels and perform as many optimisations. The Bayesian framework is the only one in which training is achieved through a one-pass optimisation procedure. 
Abstract-found: 1
Intro-found: 0
Reference: <author> Efron, B. </author> <year> (1986). </year> <title> Why isn't everyone a bayesian? The American Statistician, 40(1) 1-11. with comments. </title>
Reference-contexts: In chapter 6, we carry out full Bayesian calculation on a simple example, using both frameworks. COMMENTS 5.1 The philosophical as well as practical differences between the Bayesian and the frequentist (or Fisherian) approaches to statistics is well illustrated in e.g. <ref> (Efron, 1986) </ref>, and the comments and references therein. 5.2 For a rather general (if partial) presentation of Bayesian reasoning, see Jaynes (1985) and enclosed references.
Reference: <author> Jaynes, E. T. </author> <year> (1985). </year> <title> Bayesian methods: general background. </title> <editor> In Justice, J., editor, </editor> <booktitle> Maximum Entropy and Bayesian Methods in Applied Statistics, </booktitle> <pages> pages 1-25. </pages> <publisher> Cambridge University Press. </publisher>
Reference: <author> MacKay, D. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447. </pages>
Reference: <author> MacKay, D. </author> <year> (1992b). </year> <title> A practical bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: <author> MacKay, D. </author> <year> (1993). </year> <title> Hyperparameters: Optimize or integrate out? In Heidbreder, </title> <editor> G., editor, </editor> <title> Maximum entropy and Bayesian Methods. </title> <publisher> Kluwer, Dordrecht. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Bayesian training of backpropagation network by the hybrid monte carlo method. </title> <type> Technical Report CRG-TR-92-1, </type> <institution> Connectionist Research Group, Department of Computer Science, University of Toronto. </institution>
Reference: <author> Scales, J. A. and Smith, M. L. </author> <year> (1994). </year> <title> Introductory Geophysical Inverse Theory. </title> <note> Samizdat Press, available via FTP form hilbert.mines.colorado.edu. </note>
Reference-contexts: We have used the same prior, and demon strated its effect on a toy problem in chapter 6. 5.10 It is surprisingly hard to find a source that provides a convincing treatment of the non-informative prior. In the context of inverse problems, let us men tion <ref> (Scales and Smith, 1994, page 49) </ref>. 5.11 Several authors have argued against the use of the MAP method. A simple example of incorrect inference produced by MAP is presented by MacKay (1993), although it is not clear whether it is really relevant.
Reference: <author> Thodberg, H. H. </author> <year> (1993). </year> <title> Ace of Bayes: application of neural network with pruning. </title> <type> Technical Report 1132-E, </type> <institution> Danish meat research institute, Roskilde, Danmark. </institution>
Reference: <author> Thodberg, H. H. </author> <year> (1996). </year> <title> A review of bayesian neural networks with an application to near infrared spectroscopy. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1) </volume> <pages> 56-72. </pages>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference-contexts: Thodberg (1996) also briefly argues that one should "integrate over as many parameters as possible". 5.12 The quote from V. Vapnik is in section 4.11.2 of <ref> (Vapnik, 1995) </ref>.
Reference: <author> Williams, P. M. </author> <year> (1995). </year> <title> Bayesian regularization and pruning using a Laplace prior. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 117-143. </pages> <address> c flC. Goutte 1996 86 Bayesian estimation Wolpert, D. </address> <year> (1995). </year> <title> What Bayes has to say about the evidence procedure. </title> <editor> In Hei-dbreder, G., editor, </editor> <title> 1993 Maximum Entropy and Bayesian Methods Conference. </title> <publisher> Kluwer. </publisher>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On the use of evidence in neural networks. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 5 in NIPS, </booktitle> <pages> pages 539-546. </pages> <publisher> Morgan Kaufmann. </publisher> <address> c flC. </address> <month> Goutte </month> <year> 1996 </year>
References-found: 12

