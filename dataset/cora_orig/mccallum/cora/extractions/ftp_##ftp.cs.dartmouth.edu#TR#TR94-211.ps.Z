URL: ftp://ftp.cs.dartmouth.edu/TR/TR94-211.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR94-211/
Root-URL: http://www.cs.dartmouth.edu
Email: fdfk,nilsg@cs.dartmouth.edu  
Title: Production Parallel Scientific Workload  
Author: David Kotz Nils Nieuwejaar 
Date: Revised: August 3, 1994  
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Pubnum: PCS-TR94-211  
Abstract: Available at URL ftp://ftp.cs.dartmouth.edu/pub/CS-techreports/TR94-211.ps.Z Dynamic File-Access Characteristics of a Abstract Multiprocessors have permitted astounding increases in computational performance, but many cannot meet the intense I/O requirements of some scientific applications. An important component of any solution to this I/O bottleneck is a parallel file system that can provide high-bandwidth access to tremendous amounts of data in parallel to hundreds or thousands of processors. Most successful systems are based on a solid understanding of the expected workload, but thus far there have been no comprehensive workload characterizations of multiprocessor file systems. This paper presents the results of a three week tracing study in which all file-related activity on a massively parallel computer was recorded. Our instrumentation differs from previous efforts in that it collects information about every I/O request and about the mix of jobs running in a production environment. We also present the results of a trace-driven caching simulation and recommendations for designers of multiprocessor file systems.
Abstract-found: 1
Intro-found: 1
Reference: [BCR93] <author> Rajesh Bordawekar, Alok Choudhary, and Juan Miguel Del Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware <ref> [Nit92, BCR93] </ref>. Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache [MK91]. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [Cro89, Pie89, Roy93, BGST93, Kot93] </ref>. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93].
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [Flo86, FE89] and Ousterhout et al. [OCH + 85] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system <ref> [BHK + 91] </ref>. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [dC94]. <p> Although these files were larger than those in a general-purpose file system <ref> [BHK + 91] </ref>, they were smaller than we would expect to see in a scientific supercomputing environment [MK91]. <p> request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other workloads using a similar file-system interface. 10 data transferred by request size. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [OCH + 85, BHK + 91, MK91] </ref>. <p> It is concurrently shared if the opens overlap in time. It is write-shared if one of the opens involves writing the file. In uniprocessor and distributed-system workloads, concurrent sharing is known to be uncommon, and concurrent write sharing rare <ref> [BHK + 91] </ref>. In a parallel file system, of course, concurrent file sharing among processes within a job is presumably the norm, while concurrent file sharing between jobs is likely to be rare.
Reference: [CCFN92] <author> Russell Carter, Bob Ciotti, Sam Fineberg, and Bill Nitzberg. </author> <title> NHT-1 I/O benchmarks. </title> <type> Technical Report RND-92-016, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Simple benchmarking of the instrumented library revealed that the overhead added by our instrumentation was virtually undetectable in many cases. The worst case we found was a 7% increase in execution time on one run of the NAS NHT-1 Application-I/O Benchmark <ref> [CCFN92] </ref>. After the instrumented library was put into production use, anecdotal evidence suggests that there was no noticeable performance loss. 3.2 Analysis The raw trace files required some simple postprocessing before they could be easily analyzed. This postprocessing included data realignment, clock synchronization, and chronological sorting.
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [Cro89, Pie89, Roy93, BGST93, Kot93]. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file <ref> [CFPB93, DdR92] </ref>. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [CFPB93, DdR92, Kot93] </ref>. For some applications, collective I/O requests can lead to even better performance [Kot94]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [CHKM93] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. <ref> [CHKM93] </ref> studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Galbreath et al. [GGL93] present a useful high-level characterization based on anecdotal evidence. 2.2 Existing file systems To increase parallelism, all large multiprocessor file systems decluster blocks of a file across many disks, which are accessed in parallel.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised from Dartmouth PCS-TR93-188. </note>
Reference-contexts: All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications. Crockett [Cro89] and Kotz [KE93b] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz <ref> [CK93] </ref> discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [RB90].
Reference: [Cro89] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94]. All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications. Crockett <ref> [Cro89] </ref> and Kotz [KE93b] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [CK93] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [RB90]. <p> Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [Cro89, Pie89, Roy93, BGST93, Kot93] </ref>. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93].
Reference: [dC94] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications <ref> [dC94] </ref>. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [Cro89, Pie89, Roy93, BGST93, Kot93]. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file <ref> [CFPB93, DdR92] </ref>. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [CFPB93, DdR92, Kot93] </ref>. For some applications, collective I/O requests can lead to even better performance [Kot94]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. PIFS (Bridge) <ref> [Dib90] </ref> allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [FE89] <author> Richard Allen Floyd and Carla Schlatter Ellis. </author> <title> Directory reference patterns in hierarchical file systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [Flo86, FE89] </ref> and Ousterhout et al. [OCH + 85] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [BHK + 91]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads.
Reference: [Flo86] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [Flo86, FE89] </ref> and Ousterhout et al. [OCH + 85] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [BHK + 91]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. <p> Note also that there were extremely few files that were read and written in the same open. This latter behavior is common in Unix file systems <ref> [Flo86] </ref> and may be accentuated here by the difficulty in coordinating concurrent reads and writes to the same file (note the CFS file-access modes are of little help for read-write access). Table 2: Among traced jobs, the number of files opened by jobs was often small (1-4).
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Pratt and French found that the caching and prefetching supplied with Intel's Concurrent File System (CFS) does improve performance <ref> [FPD93] </ref>. Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware [Nit92, BCR93]. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files.
Reference: [Fre89] <author> James C. </author> <title> French. A global time reference for hypercube multiprocessors. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 217-220, </pages> <year> 1989. </year>
Reference-contexts: Ordering the records was complicated by the lack of synchronized clocks on the iPSC/860. Each node maintains its own clock; the clocks are synchronized at system startup but each drifts significantly and differently after that <ref> [Fre89] </ref>. We partially compensated for the asynchrony by timestamping each block of records when it left the node and again when it was received at the data collector.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year> <month> 18 </month>
Reference-contexts: Cypher et al. [CHKM93] studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Galbreath et al. <ref> [GGL93] </ref> present a useful high-level characterization based on anecdotal evidence. 2.2 Existing file systems To increase parallelism, all large multiprocessor file systems decluster blocks of a file across many disks, which are accessed in parallel. <p> Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers <ref> [TMC87, Mas92, GGL93] </ref>. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [KE93a] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: The implications of this fact for our study are discussed in Section 5. 2.3 Caching in multiprocessor file systems In our previous work, we found that caching and prefetching are successful in multiprocessor file systems <ref> [KE93a, KE93b] </ref>. Pratt and French found that the caching and prefetching supplied with Intel's Concurrent File System (CFS) does improve performance [FPD93]. <p> Replacement policies other than LRU or FIFO should be developed (e.g., <ref> [KE93a] </ref>), to optimize for sequential access and interprocess locality rather than traditional spatial and temporal locality. Ultimately, we believe that the file-system interface must change. The current interface forces the programmer to break down large parallel I/O activities into small, non-contiguous requests.
Reference: [KE93b] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94]. All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications. Crockett [Cro89] and Kotz <ref> [KE93b] </ref> hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [CK93] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [RB90]. <p> The implications of this fact for our study are discussed in Section 5. 2.3 Caching in multiprocessor file systems In our previous work, we found that caching and prefetching are successful in multiprocessor file systems <ref> [KE93a, KE93b] </ref>. Pratt and French found that the caching and prefetching supplied with Intel's Concurrent File System (CFS) does improve performance [FPD93].
Reference: [Kot93] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [Cro89, Pie89, Roy93, BGST93, Kot93] </ref>. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [CFPB93, DdR92, Kot93] </ref>. For some applications, collective I/O requests can lead to even better performance [Kot94]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Strided requests are available in some file-system interfaces [CFPB93, DdR92, Kot93]. For some applications, collective I/O requests can lead to even better performance <ref> [Kot94] </ref>. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [KS93] <author> Orran Krieger and Michael Stumm. </author> <title> HFS: a flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [Cro89, Pie89, Roy93, BGST93, Kot93]. Some are based on a memory-mapped interface <ref> [KSR92, KS93] </ref>. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality.
Reference: [KSR92] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [Cro89, Pie89, Roy93, BGST93, Kot93]. Some are based on a memory-mapped interface <ref> [KSR92, KS93] </ref>. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality.
Reference: [Mas92] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers <ref> [TMC87, Mas92, GGL93] </ref>. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [dC94]. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns <ref> [MK91] </ref>, focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94]. <p> Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache <ref> [MK91] </ref>. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine. The compute nodes are based on the Intel i860 processor and are connected by a hypercube network. <p> Although these files were larger than those in a general-purpose file system [BHK + 91], they were smaller than we would expect to see in a scientific supercomputing environment <ref> [MK91] </ref>. <p> request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other workloads using a similar file-system interface. 10 data transferred by request size. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [OCH + 85, BHK + 91, MK91] </ref>. <p> This further suggests that most of the hits in the I/O node cache were indeed a result of interprocess locality because, as Figure 10 shows, the limited intraprocess locality was filtered out by the compute-node cache. Note the contrast with Miller and Katz's tracing study <ref> [MK91] </ref>, which found little benefit from caching. (They did notice a benefit from prefetching and write-behind.) Both their workload and ours involve sequential access patterns; the difference is that the small requests in our access pattern lead to intraprocess spatial locality, and the distribution of a sequential pattern across parallel compute
Reference: [MK93] <author> Ethan L. Miller and Randy H. Katz. </author> <title> An analysis of file migration in a UNIX supercomputing environment. </title> <booktitle> In Proceedings of the 1993 Winter USENIX Conference, </booktitle> <pages> pages 421-434, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [Pow77]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray <ref> [MK93] </ref>, giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [PP93, PP94]. All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications.
Reference: [NAS93] <institution> NASA Ames Research Center, Moffet Field, CA. NAS User Guide, </institution> <address> 6.1 edition, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle Ethernet connections or interactive shells <ref> [NAS93] </ref>. Intel's Concurrent File System (CFS) [Pie89, FPD93, Nit92] provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files. <p> Their iPSC has 128 compute nodes, each with 8 MB of memory, and 10 I/O nodes, each with 4 MB of memory and a single 760 MB disk drive <ref> [NAS93] </ref>. There is also a single service node that handles a 10-Mbit Ethernet connection to the host computer. The total I/O capacity is 7.6 GB and the total bandwidth is less than 10 MB/s.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware <ref> [Nit92, BCR93] </ref>. Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache [MK91]. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files. <p> Similarly, 89.4% of all writes were for fewer than 4000 bytes, but those writes transferred only 3% of all data written. The number of small requests is surprising due to their poor performance in CFS <ref> [Nit92] </ref>. The jump at 4 KB indicates that some users have optimized for the file-system block size, but it appears that most users prefer ease of programming over performance.
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [Flo86, FE89] and Ousterhout et al. <ref> [OCH + 85] </ref> measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [BHK + 91]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. <p> request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other workloads using a similar file-system interface. 10 data transferred by request size. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [OCH + 85, BHK + 91, MK91] </ref>.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [Cro89, Pie89, Roy93, BGST93, Kot93] </ref>. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93]. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle Ethernet connections or interactive shells [NAS93]. Intel's Concurrent File System (CFS) <ref> [Pie89, FPD93, Nit92] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files.
Reference: [Pow77] <author> Michael L. Powell. </author> <title> The DEMOS File System. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 33-42, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [dC94]. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system <ref> [Pow77] </ref>. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [MK91], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns.
Reference: [PP93] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [PP93, PP94] </ref>. All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications. Crockett [Cro89] and Kotz [KE93b] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [CK93] discuss the needs of parallel-I/O algorithms.
Reference: [PP94] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A case study of a scientific application I/O behavior. </title> <booktitle> In Proceedings of the International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 101-106, </pages> <year> 1994. </year>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [MK93], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [PP93, PP94] </ref>. All of these studies are limited to uniprocess applications on vector supercomputers. 2 Scientific parallel applications. Crockett [Cro89] and Kotz [KE93b] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [CK93] discuss the needs of parallel-I/O algorithms.
Reference: [RB90] <author> A. L. Narasimha Reddy and Prithviraj Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: Crockett [Cro89] and Kotz [KE93b] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [CK93] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns <ref> [RB90] </ref>. This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. [CHKM93] studied individual parallel scientific applications, measuring temporal patterns in I/O rates.
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [Cro89, Pie89, Roy93, BGST93, Kot93] </ref>. Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers [TMC87, Mas92, GGL93].
Reference: [TMC87] <institution> Connection Machine model CM-2 technical summary. </institution> <type> Technical Report HA87-4, </type> <institution> Thinking Machines, </institution> <month> April </month> <year> 1987. </year> <month> 20 </month>
Reference-contexts: Some are based on a memory-mapped interface [KSR92, KS93]. Some provide a way for the user to specify per-process logical views of the file [CFPB93, DdR92]. Some provide SIMD-style transfers <ref> [TMC87, Mas92, GGL93] </ref>. PIFS (Bridge) [Dib90] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
References-found: 35

