URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/oz/web/papers/CMU-CS-96-138-1sided.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/wsr/Web/research/oz-papers.html
Root-URL: http://www.cs.cmu.edu
Author: Joseph Bates, Chair Jaime Carbonell Reid Simmons Aaron Sloman, W. Scott Neal Reilly 
Degree: Thesis Committee:  Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy.  
Date: May 1996  
Address: Pittsburgh, PA 15213-3890  England  
Affiliation: School of Computer Science Carnegie Mellon University  University of Birmingham,  Believable Social and Emotional Agents  
Abstract: This research was partially supported by Fujitsu Laboratories, Mitsubishi Electric Research Labs, and Justsystem Corporation. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of Fujitsu Laboratories, Mitsubishi Electric Research Labs, or Justsystem Corporation. 
Abstract-found: 1
Intro-found: 1
Reference: [Agre90] <author> Agre, P. and Chapman, R. </author> <title> What are plans for? In Robotics and Autonomous Systems. </title> <publisher> Elsevier Science Publishers. </publisher> <year> 1990. </year>
Reference: [Aristotle87b] <author> Aristotle. Poetics. </author> <title> In Introduction to Aristotle. Translated by Richard Jenko. The Modern Library. </title> <address> New York. </address> <year> 1987. </year>
Reference-contexts: Some of these ideas can be traced as far back as Aristotles Poetics <ref> [Aristotle87b] </ref> and artists in other media (such as screenplays [Horton94], novels [Gardner91], and even comic books [McCloud91]) make similar claims. To summarize, the four important lessons to draw from the arts are: Emotions are important for creating believable characters.
Reference: [Bates92a] <author> Bates, J., Loyall, </author> <title> A.B., Reilly, W.S. An Architecture for Action, Emotion, and Social Behavior. </title> <booktitle> In Proceedings of the Fourth European Workshop on Modeling Autonomous Agents in a Multi-Agent World. </booktitle> <editor> S. Martino al Cimino, </editor> <address> Italy. </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Logarithmic combination. This is the default option in the Em system and the one used in all of the Tok agents created to date (except the first Oz agent, Lyotard the cat <ref> [Bates92a] </ref>, which used option #2). In this case, emotions are added together logarithmically, so to combine intensity I and J would be computed as log 2 (2 I J ). Combining structures of intensity 3, 3, and 4 results in a combined intensity of 5.
Reference: [Bates92b] <author> Bates, J., Loyall, </author> <title> A.B., Reilly, W.S. Integrating Reactivity, Goals, and Emotion in a Broad Agent. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. Bloomington, IN. </booktitle> <month> July </month> <year> 1992. </year>
Reference: [Bates92c] <author> Bates, J. </author> <title> Virtual Reality, Art, and Entertainment. In Presence: </title> <journal> The Journal of Teleoperators and Virtual Environments. </journal> <volume> Vol. 1. No. 1. </volume> <publisher> MIT Press. </publisher> <month> Winter </month> <year> 1992. </year>
Reference: [Bates91] <author> Bates, J., Loyall, </author> <title> A.B., Reilly, W.S. Broad Agents. </title> <journal> In SIGART Bulletin. </journal> <volume> Vol. 2. No. 4. </volume> <month> August </month> <year> 1991. </year>
Reference: [Beaudoin94] <author> Beaudoin, L. </author> <title> Goal Processing in Autonomous Agents. </title> <type> Ph.D. Thesis. </type> <institution> University of Birmingham, Birmingham, </institution> <address> England. </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Sloman calls these perturbance states emotional. (Note that he uses the word emotional, but he does not use the word emotion because of the confusion surrounding the definition of that word.) Beaudoin <ref> [Beaudoin94] </ref> has also explored this model in depth and built a simulated environment to test and demonstrate some of these ideas. <p> In particular, I will show how integrating emotion with the perception and motivation systems enables the creation of models of emotions that are often simpler and more complete than the cognitive models of Ortony et al. that I started with. 1. For an exception, see [Sloman86] and <ref> [Beaudoin94] </ref>. Sloman and Beaudoin hypothesize that states of certain kinds of broad mental architectures can be called emotional. Beaudoin has looked, in particular, at the relationship of motive processing and emotional states. <p> 3.3, is that breadth often allows us to create simpler and more complete models of how emotions are generated by using parts of the architecture such as the motivation and perception systems. 3.1.3 An Explicit Emotion Module Some work in emotion modeling, such as that of Sloman [Sloman86] and Beau-doin <ref> [Beaudoin94] </ref>, does not use an explicit emotion system to model emotions. In Slomans model, emotions are states of complex, resource-limited, goal-processing systems. That is, there is no explicit emotion systememotional reactions emerge from the complex processing of certain kinds of goal-processing systems in certain environments. <p> Another possible area to explore is robotics. Some initial work has already been done in the creation of emotional robots (e.g., [Pfeifer93], [Yamamoto94]), though with very simple tasks and emotion models. I also expect work like that of Sloman and Beaudoin <ref> [Sloman94, Beaudoin94] </ref> to be useful in this area, even though their goal is not explicitly a robotics one. Because my architecture has been designed to be exible, it could be useful as a testbed for the development of emotion systems for robots that are not necessarily artistic.
Reference: [Blank80] <author> Blank, M. and Lebling, D. Zork I: </author> <title> The Great Underground Empire. </title> <booktitle> Infocom. </booktitle> <year> 1980. </year>
Reference: [Blumberg94] <author> Blumberg, B. </author> <title> Building Believable Animals. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Believable Agents. </booktitle> <address> Stanford, CA. </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Allowing goals to affect other goals. In languages (like Hap) that dont provide many mechanisms for inter-behavior communication, the behavioral features can provide a crude mechanism for accomplishing such interactions. This is similar in spirit to Blumbergs work <ref> [Blumberg94] </ref> which uses a blackboard mechanism for inter-goal communication. In Tok, a goal can create a feature that is used to affect processing of other goals.
Reference: [Brooks86] <author> Brooks, R. </author> <title> A Robust Layered Control System for a Mobile Robot. </title> <journal> In IEEE Journal of Robotics and Automation. </journal> <volume> Vol. 2. No. 1. </volume> <publisher> IEEE Press. </publisher> <month> March </month> <year> 1986. </year>
Reference: [Brooks91] <author> Brooks, R. </author> <title> Intelligence Without Representation. </title> <journal> Artificial Intelligence. </journal> <volume> Vol. 47. </volume> <year> 1991. </year> <title> BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 282 </title>
Reference-contexts: Deliberative approaches (e.g., [Newell76, Fikes72]) have tended to rely on representation as an important part of being able to solve difficult problems. Behavior-based approaches (e.g. <ref> [Brooks91] </ref>) have pointed out that relying on good models of the world can be dangerous given how hard it is to create and maintain such models in complex worlds. I believe that there is an important and useful middle ground between the two camps. <p> I believe that there is an important and useful middle ground between the two camps. I sympathize with the behavior-based warnings of the difficulty of doing representation well (e.g., <ref> [Brooks91] </ref>); on the other hand, I dont see how to solve some complex social problems without some representation. <p> I found the combination of goal-directed behavior and reactivity to be well-suited to social domains. 1 Brookss behavior-based methodology of rejecting representation <ref> [Brooks91] </ref> also played a large role in helping define my approach to representation in social behaviors. I am sympathetic to the warnings about trying to use rich representations in complex and dynamic environments.
Reference: [Carbonell79] <author> Carbonell, J. </author> <title> Computer Models of Human Personality Traits. </title> <type> Technical Report CMU-CS-79-154, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA. </address> <month> November </month> <year> 1979. </year>
Reference-contexts: I have found useful ideas and inspiration in the work of Meehan [Meehan76], Carbonell <ref> [Carbonell79] </ref>, Dyer [Dyer83], and Lebowitz [Lebowitz84,Lebowitz85]. Each has provided some insight into the problem of building believable emotional characters. Dyers BORIS system understands stories about emotional episodes, such as divorces. BORIS does not generate stories, nor is it interactive, so on the surface it may not appear especially similar. <p> In an architecture like Tok, though, I believe that these mechanisms provide the primary ways of creating the kinds of emotional expression that artist say are needed. These ideas come from a variety of sources, including the emotion literature (e.g., [Elliott92], [Oatley92], [Frijda86]), AI (e.g., <ref> [Carbonell79] </ref>), the arts (e.g., [Thomas81]), and personal experience. The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here.
Reference: [Cesta93] <author> Cesta, A. and Miceli, M. </author> <title> In Search of Help. </title> <booktitle> In Proceedings of the 12th International Workshop on Distributed Artificial Intelligence. </booktitle> <address> Hidden Valley, PA. </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Hell always obey his boss, Mary, hell never obey Sarah, who he doesnt like, and hell obey the player when the player has a deadline to meet. Asking for help. This behavior, like negotiation, has received some attention from the AI community <ref> [Cesta93] </ref>. The AI approach to this problem is typically concerned with creating agents that ask for help intelligently. My approach is to incorporate personality and make the behavior believable even if it isnt especially competent.
Reference: [Dyer83] <author> Dyer, M. </author> <title> In-Depth Computer Understanding. </title> <publisher> MIT Press. </publisher> <address> Cambridge, MA. </address> <year> 1983. </year>
Reference-contexts: I have found useful ideas and inspiration in the work of Meehan [Meehan76], Carbonell [Carbonell79], Dyer <ref> [Dyer83] </ref>, and Lebowitz [Lebowitz84,Lebowitz85]. Each has provided some insight into the problem of building believable emotional characters. Dyers BORIS system understands stories about emotional episodes, such as divorces. BORIS does not generate stories, nor is it interactive, so on the surface it may not appear especially similar. <p> Social roles. The role that an agent plays in society can affect any number of behaviors, but mainly affects those behaviors particular to that role. Roles are often occupations, like police officer or waitress. Dyer <ref> [Dyer83] </ref> shows that thinking of characters in terms of the roles they are playing is an effective way to understand Believable Social Agents BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 148 stories. This implies that social role can be important to consider when generating stories as well. Other Goals. <p> Artists, will, however, need to use representations for building specific characters and behaviors and I expect that the approaches of Meehan and Leb-owitz to be useful in some of these cases. Dyers BORIS <ref> [Dyer83] </ref> is not a story-generation system, but a story-understanding system. Because of this, his goals are rather different from mine, but many of the aspects of stories and characters that his system needs to understand, my methodology should be able to create.
Reference: [Elliott92] <author> Elliot, C. </author> <title> The Affective Reasoner: A Process Model of Emotions in a Multi-agent System. </title> <type> Ph.D. Thesis. Technical Report No. 32, </type> <institution> Institute for the Learning Sciences, Northwestern University. </institution> <address> Evanston, IL. </address> <month> May </month> <year> 1992. </year>
Reference-contexts: They only need to be able to help artists build believable emotional characters. I chose as a basis for my work the emotion theories of Ortony, Clore and Collins (OCC) [Ortony88] and Gilboa and Ortony <ref> [Elliott92] </ref> 1 . The first describes when people are emotional and the second describes how people express emotions. One reason for choosing these models is that they were designed to be implemented computationally. Other researchers (e.g., [Elliott92] and [Warner91]) have also implemented versions of these models. <p> the emotion theories of Ortony, Clore and Collins (OCC) [Ortony88] and Gilboa and Ortony <ref> [Elliott92] </ref> 1 . The first describes when people are emotional and the second describes how people express emotions. One reason for choosing these models is that they were designed to be implemented computationally. Other researchers (e.g., [Elliott92] and [Warner91]) have also implemented versions of these models. Another reason for adopting these models is that they are reasonably simple to understand. <p> I will describe my decisions and how I made them in this thesis but I leave it as future work to determine if other models are more useful and easily understood by artists. 1. Gilboa and Ortony never published this theory, though it is described in <ref> [Elliott92] </ref>. Foundation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 29 I will, however, briey describe two of the models that I did not choose as a basis for my work: the basic emotion model of Oatley [Oatley92] and the emergent emotion model of Sloman [Sloman86]. <p> Other AI researchers study emotions apart from important parts of the architecture, such as a real motivation system, or, when they do have such components, they are often overly shallow or not well integrated with the emotion system. For instance, Elliotts Affective Reasoner <ref> [Elliott92] </ref> allows emotions about goals, but only permanent goals (i.e., no subgoals). In Tok, emotions can arise in response to the full set of the agents subgoals. Also, systems with weak motivation systems are limited in the ways that emotions can be expressed through goals, behaviors, and actions. <p> Emotion System Previous work in the area of computational emotions has taken a psychological model of human emotions and put it into code. For example, Elliotts Affective Reasoner <ref> [Elliott92] </ref> is a fairly true model of the Ortony, Clore and Collins model of human emotions [Ortony88]. Similarly, Frijda and Swagermans AI work [Frijda87] is based on Frijdas previous theoretical work [Frijda86]. <p> Even when researchers have considered other aspects of the mind, they have treated those other problems rather shallowly. For example, Elliotts Affective Reasoner <ref> [Elliott92] </ref> was designed, as its name implies, to reason about emotions. His system incorporates a motivation/action module but this module is not modeled very deeply since his goal is to understand emotions and not motivation. <p> I will discuss my approach in more detail throughout section 3.3. Goal Processing Information Having information about the current goals is important, as I just discussed, but it can also be important to have information about the processing of goals. Some previous systems (e.g., <ref> [Elliott92] </ref>) have generated emotions based only on static goal information, but my experience has shown that having dynamic processing information can also be useful. <p> In Em, the goal of the team winning the game fails, and the responsibility is assigned to the player taking the bad shot, which also results in anger, distress, and reproach. According to Elliots implementation of the OCC model <ref> [Elliott92] </ref>, when reproach and distress are combined to form anger, the two component emotions are removed from the system. Em keeps the distress, reproach, and anger emotion structures. The emotion expression part of the architecture decides which (possibly more than one) to display. <p> Unfortunately, there is less work in psychology and AI related to computational models of emotional expression as there is work on emotion generation. One such model is Gil-boa and Ortonys model of action responses which is described in <ref> [Elliott92] </ref>, but that is less-well developed than the Ortony, Clore and Collins model of emotion generation [Ortony88]. <p> In an architecture like Tok, though, I believe that these mechanisms provide the primary ways of creating the kinds of emotional expression that artist say are needed. These ideas come from a variety of sources, including the emotion literature (e.g., <ref> [Elliott92] </ref>, [Oatley92], [Frijda86]), AI (e.g., [Carbonell79]), the arts (e.g., [Thomas81]), and personal experience. The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here. <p> The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here. One of the most detailed examinations of emotional expression is given by Gilboa and Ortony in <ref> [Elliott92] </ref>. They postulate a set of action responses for emotional expression.
Reference: [Elliott93] <author> Elliott, C. and Siegle, G. </author> <title> Variables Inuencing the Intensity of Simulated Affectrive States. </title> <publisher> AAAI Technical Report SS-93-05. AAAI Press. </publisher> <address> Menlo Park, CA. </address> <year> 1993. </year>
Reference-contexts: The original model postulated four global intensity variables (arousal, unexpectedness, proximity, and sense of reality) and up to four local variables (e.g., goal importance in goal-related emotions such as joy) for each emotion type. Elliott and Siegle <ref> [Elliott93] </ref> continued to expand and refine the complex emotion intensity model to include 24 emotion intensity variables (such as physical well-being)and that was claimed to be an incomplete list.
Reference: [Fikes72] <author> Fikes, R. and Nilsson, N. </author> <title> STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving. </title> <journal> In Artificial Intelligence. </journal> <volume> Vol. 2. </volume> <year> 1972. </year>
Reference-contexts: Deliberative approaches (e.g., <ref> [Newell76, Fikes72] </ref>) have tended to rely on representation as an important part of being able to solve difficult problems.
Reference: [Firby89] <author> Firby, J. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> Ph.D. Thesis. Technical Report YALEU/CSD/RR #672, </type> <institution> Yale University. </institution> <address> New Haven, CT. </address> <month> January </month> <year> 1989. </year>
Reference: [Forgy82] <author> Forgy, C. </author> <title> Rete: A Fast Algortihm for the Many Pattern/ Many Object Pattern Match Problem. </title> <journal> In Artificial Intelligence. </journal> <volume> Vol. 19. </volume> <month> September </month> <year> 1982. </year>
Reference-contexts: A better implementation would have feature changes that are driven by changes to the intensities of emotion structures or nodes in the emotion type hierarchy. This is closer to what happens in the real-time version of Em; the matching is done by an incremental Rete matcher <ref> [Forgy82] </ref>, which automatically optimizes the matching to limit the needed computations.
Reference: [Frijda86] <author> Frijda, N. </author> <title> The Emotions. </title> <publisher> Cambridge University Press. </publisher> <address> Cambridge, England. </address> <year> 1986. </year>
Reference-contexts: For example, Elliotts Affective Reasoner [Elliott92] is a fairly true model of the Ortony, Clore and Collins model of human emotions [Ortony88]. Similarly, Frijda and Swagermans AI work [Frijda87] is based on Frijdas previous theoretical work <ref> [Frijda86] </ref>. In such systems the goal is to build a working model of a psychological theory and to that end these systems are quite effective. <p> In an architecture like Tok, though, I believe that these mechanisms provide the primary ways of creating the kinds of emotional expression that artist say are needed. These ideas come from a variety of sources, including the emotion literature (e.g., [Elliott92], [Oatley92], <ref> [Frijda86] </ref>), AI (e.g., [Carbonell79]), the arts (e.g., [Thomas81]), and personal experience. The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here.
Reference: [Frijda87] <author> Frijda, N. and Swagerman, J. </author> <title> Can Computers Feel? Theory and Design of an Emotional System. </title> <journal> In Cognition and Emotion. </journal> <volume> Vol. 1. No. 3. </volume> <publisher> Lawrence Erlbaum Associates Limited. </publisher> <year> 1987. </year>
Reference-contexts: For example, Elliotts Affective Reasoner [Elliott92] is a fairly true model of the Ortony, Clore and Collins model of human emotions [Ortony88]. Similarly, Frijda and Swagermans AI work <ref> [Frijda87] </ref> is based on Frijdas previous theoretical work [Frijda86]. In such systems the goal is to build a working model of a psychological theory and to that end these systems are quite effective.
Reference: [Gardner91] <author> Gardner, J. </author> <title> The Art of Fiction. </title> <publisher> Vintage Books, </publisher> <address> New York, NY. </address> <year> 1991. </year>
Reference-contexts: Some of these ideas can be traced as far back as Aristotles Poetics [Aristotle87b] and artists in other media (such as screenplays [Horton94], novels <ref> [Gardner91] </ref>, and even comic books [McCloud91]) make similar claims. To summarize, the four important lessons to draw from the arts are: Emotions are important for creating believable characters. Believable Emotional Agents BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 28 Emotions need to be specific to the character in question.
Reference: [Georgeff87] <author> Georgeff, M. and Lansky, A. and Schoppers, M. </author> <title> Reasoning and Planning in Dynamic Domains: An Experiment with a Mobile Robot. </title> <type> Technical Report 380, </type> <institution> Artificial Intelligence Center, SRI International. </institution> <address> Menlo Park, CA. </address> <note> BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 283 1987. </note>
Reference: [Hayes94] <author> Hayes, P., Ford, K., and Agnew, N. </author> <title> On Babies and Bathwater: A Cautionary Tale. </title> <journal> In AI Magazine. </journal> <volume> Vol. 15. No. 4. </volume> <booktitle> American Association for Artificial Intelligence. </booktitle> <month> Winter </month> <year> 1994. </year>
Reference-contexts: For instance, how much does one character need to know about the goal, beliefs, relationships, internal processing, capabilities, etc. of other agents in order to interact with them? The question of representation (typically related to modeling the physical world) is one that has perplexed the AI community for years <ref> [Hayes94] </ref>. Deliberative approaches (e.g., [Newell76, Fikes72]) have tended to rely on representation as an important part of being able to solve difficult problems.
Reference: [Hatfield94] <author> Hatfield, E. and Cacioppo, J. and Rapson, R. </author> <title> Emotional Contagion. </title> <publisher> Cambridge University Press. </publisher> <address> Cambridge, England. </address> <year> 1994. </year>
Reference-contexts: If everyone around you is happy, you might tend to be happy, even if you dont have any other reason to be happy. Some psychology research has been done in this area by Hatfield et al. <ref> [Hatfield94] </ref>. The default emotion generators do not have any such rules. Summary BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 71 Body-Feedback Emotions The state of the body can lead to emotions, such as feeling happier when smiling.
Reference: [Hickman91] <author> Hickman, S. and Shiels, M. </author> <title> Situated Action as a Basis for Cooperation. </title> <booktitle> In Decentralized AI 2, The Proceedings of the Second European Workshop on Modeling Autonomous Agents in a Multi-Agent World. </booktitle> <editor> Y. Demazeau and J-P Muller, eds. </editor> <publisher> Elsevier Science Publishers B.V., North Hol-land. </publisher> <year> 1991. </year>
Reference-contexts: Methodology for Building Social Behaviors BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 149 Some researchers have created social agents without using any representations, such as Mataric [Mataric92], Wavish [Wavish92], and Hickman and Shiels <ref> [Hickman91] </ref>. In all of these cases, the behaviors are (socially) much simpler than artists will often want and none of them incorporate believability. For instance, Matarics robots can ock and gather food with other agents, but they are really just co-existing instead of directly socially interacting. <p> She is not working on the creation of agents with personality and emotion that engage each other in more complex behaviors like negotiation and making friendsthe complexity in her work comes largely from the fact that she is working with real robots. Hickman and Shiels <ref> [Hickman91] </ref> have also built behavior-based social agents that build television sets in a simulated world. They have taken a reasonably typical DAI task and shown that it can be accomplished by a set of agents without any representation.
Reference: [Holmes86] <author> Holmes, R. </author> <title> The Mystery of Edwin Drood. Vocal Score. Warner Bros. </title> <publisher> Publications. </publisher> <year> 1986. </year>
Reference: [Horswill94] <author> Horswill, I. </author> <title> Playing with Robots. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Believable Agents. </booktitle> <address> Stanford, CA. </address> <month> March </month> <year> 1994. </year>
Reference-contexts: In some cases, the emotion might not be expressed immediately, but the agent is never frozen waiting for the mind to choose the next action, which I have found to be crucial for subjective believability. Horswill also provides some evidence that this is true <ref> [Horswill94] </ref>. <p> To top all this off, the system is written in Lisp and garbage collects a few times during any given twenty-minute interaction, which makes the system even less responsive. Past experience as well as evidence from Horswills robots <ref> [Horswill94] </ref> indicates that speed of response is an important element in creating believability, so increasing the speed of this system should Summary & Future Directions BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 230 make it more effective. (A note: the Woggle system is written in C and is fast enough to produce real-time
Reference: [Horton94] <author> Horton, A. </author> <title> Writing the Character-Centered Screenplay. </title> <publisher> University of California Press. </publisher> <address> Berkeley, CA. </address> <year> 1994. </year>
Reference-contexts: Some of these ideas can be traced as far back as Aristotles Poetics [Aristotle87b] and artists in other media (such as screenplays <ref> [Horton94] </ref>, novels [Gardner91], and even comic books [McCloud91]) make similar claims. To summarize, the four important lessons to draw from the arts are: Emotions are important for creating believable characters. Believable Emotional Agents BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 28 Emotions need to be specific to the character in question.
Reference: [Hovy88] <author> Hovy, E. </author> <title> Generating Natural Language under Pragmatic Constraints. </title> <publisher> Lawrence Erlbaum Associates. </publisher> <address> Hillsdale, NJ. </address> <year> 1988. </year>
Reference-contexts: Mueller [Mueller90] has examined the issue of the relationship between emotions and daydreaming in depth. Natural Language. Although I use some natural language in the characters I have built, I have not examined natural language issues in any depth in this research. This is the subject of [Loyall96], [Kantrowitz96], <ref> [Hovy88] </ref> and other related literature which I will not attempt to cite. Nonetheless, here are a few suggestions about how the natural language architecture should take emotions into account. Natural Language Generation Lexical. Emotions should be able to affect word choice. Syntactic.
Reference: [Huber95] <author> Huber, M. and Durfee, E. </author> <title> Deciding When to Commit to Action During Observation-Based Coordination. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems. </booktitle> <address> San Francisco, CA. </address> <publisher> MIT Press. </publisher> <year> 1995. </year>
Reference-contexts: Although I have not borrowed representations from DAI systems for the agents I have built, there has been a lot of work on knowledge representation for multi-agent domains that some artists might find useful (e.g., <ref> [Huber95] </ref>, [Rao95], [Vidal95], [Mor95]). I expect this will be true especially if the agent needs to be highly competent at some social task. 7.4.2 Behavior-Based AI My work towards creating believable social behaviors has also been inuenced by work in behavior-based AI.
Reference: [Jones89] <author> Jones, C. Chuck Amuck: </author> <title> The Life and Times of an Animated Cartoonist. </title> <address> Farrar, Straus & Giroux, New York. </address> <year> 1989. </year>
Reference-contexts: This list may not be complete, but, according to users, the behaviors created with these elements of personality incorporated have proven to be effective. Methodology for Building Social Behaviors BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 147 Personality quirks. Artists (e.g., <ref> [Jones89] </ref>) often talk about the importance of quirks for bringing a character to life. The fact that Bugs Bunny says, Whats up doc? in his distinctive Brooklyn accent is part of his personality and his success as a character.
Reference: [Kantrowitz96] <author> Kantrowitz, M. </author> <type> Ph.D. Thesis. </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address> <month> Forthcoming </month> <year> 1996. </year>
Reference-contexts: Expressing Emotions BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 110 TABLE 5-2 Default Mapping from Behavioral Features to Body State Effects. 5.2.3 Natural Language Natural language understanding and generation problems are still being studied for believable agents (see [Loyall96] and <ref> [Kantrowitz96] </ref>) and I dont have a lot to say about this topic. However, because the language understanding and generation are being done in Hap, I expect many of the same techniques that are used to affect processing in the action system to apply here. <p> Mueller [Mueller90] has examined the issue of the relationship between emotions and daydreaming in depth. Natural Language. Although I use some natural language in the characters I have built, I have not examined natural language issues in any depth in this research. This is the subject of [Loyall96], <ref> [Kantrowitz96] </ref>, [Hovy88] and other related literature which I will not attempt to cite. Nonetheless, here are a few suggestions about how the natural language architecture should take emotions into account. Natural Language Generation Lexical. Emotions should be able to affect word choice. Syntactic.
Reference: [Kass92] <author> Kass, A., Burke, R., Blevis, E., and Williamson, M. </author> <title> The GuSS Project: Integrating Instruction and Practice through Guided Social Simulation. Technical Report #34, BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 284 The Institute for the Learning Sciences, </title> <institution> Northwestern University. </institution> <address> Evanston, IL. </address> <month> September </month> <year> 1992. </year>
Reference-contexts: For instance, educational and training applications can put a student in a simulation with other agents for some educational (as opposed to dramatic) purpose. Work along these lines is Future Directions BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 237 being done by Kass et al. at Northwestern University <ref> [Kass92] </ref>. These systems have characters with social skills and some emotions, but they could be improved to be even more emotional and have better personalities. Improving the personalities of agents in education and training system has two potential advantages. <p> Because this is a hard problem, current training and education systems often use techniques like those used in games; they use video or animation clips and limit interactions to point-and-click or menu-based approaches (e.g., Kasss GuSS system <ref> [Kass92] </ref>). Using my approach to building social agents, it might be possible to develop interfaces for simulation-based training and education systems where the user has a greater feeling of freedom than is currently available. This feeling of freedom could potentially lead to more immersive and more effective educational experiences.
Reference: [Kelso92] <author> Kelso, M.T., Wehyrauch, P. and Bates, J. </author> <title> Dramatic Presence. In PRESENCE: The Journal of Teleoperators and Virtual Environments. </title> <journal> Vol.2. </journal> <volume> No. 1. </volume> <publisher> MIT Press. </publisher> <year> 1992. </year>
Reference: [Laird87] <author> Laird, J. </author> <title> Soar: An Architecture for General Intelligence. </title> <journal> In Artitifial Intelligence. </journal> <volume> Vol. 33. </volume> <year> 1987. </year>
Reference: [Laurel90] <author> Laurel, B. </author> <title> The Art of Human-Computer Interface Design. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address> <year> 1990. </year>
Reference: [Lebowitz85] <author> Lebowitz, M. </author> <title> Story-Telling as Planning and Learning. </title> <booktitle> In Poetics. </booktitle> <volume> Vol. 14. No. 6. </volume> <month> December </month> <year> 1985. </year>
Reference-contexts: The TALESPIN framework could never tell a story about Yosemite Sam holding up a bank. Lebowitzs work on UNIVERSE <ref> [Lebowitz84, Lebowitz85] </ref>, which generates non-interactive soap-opera plots, is also relevant in some respects. The UNIVERSE characters have 4-dimensional relationships. Those dimensions are the following: positive/negative, intimate/distant, dominant/submissive, and attract-edness. The first three are from Schank and Abelson [Schank77], based on the work of Wish [Wish76].
Reference: [Lebowitz84] <author> Lebowitz, M. </author> <title> Creating Characters. In Poetics. </title> <journal> Vol.13. </journal> <volume> No. 3. </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: The TALESPIN framework could never tell a story about Yosemite Sam holding up a bank. Lebowitzs work on UNIVERSE <ref> [Lebowitz84, Lebowitz85] </ref>, which generates non-interactive soap-opera plots, is also relevant in some respects. The UNIVERSE characters have 4-dimensional relationships. Those dimensions are the following: positive/negative, intimate/distant, dominant/submissive, and attract-edness. The first three are from Schank and Abelson [Schank77], based on the work of Wish [Wish76].
Reference: [Lesser95] <author> Lesser. </author> <title> Distributed AI Report. </title> <type> IJCAI Technical Session. </type> <institution> Montral, Canada. </institution> <year> 1995. </year>
Reference-contexts: and ideas: Distributed AI, behavior-based AI, and story-based AI. 7.4.1 Distributed AI Distributed AI (DAI) is concerned with getting agents to either (1) work together with other agents to solve some common problem, or (2) solve an individual problem in an environment with other agents that are also solving problems <ref> [Lesser95] </ref>. In both cases the main goal of the agent is to solve a problem or attain a goal. The agents are engineered to be rational and optimal in solving their problems and when dealing with the other agents. These are hard problems.
Reference: [Loyall96] <author> Loyal, </author> <title> A.B. Believable Agents that Act and Generate Language. </title> <type> Ph.D. Thesis. </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <address> Pittsburgh, PA. </address> <month> Forthcoming </month> <year> 1996. </year>
Reference-contexts: These priority examples demonstrate how I achieved some types of integration of the various capabilities of a Tok agent (e.g., emotion, action, inference) and also how such integration can be useful. For more on this subject, see <ref> [Loyall96] </ref>. 3.3 The Tools: Ems Default Emotion Generation Rules Em comes with a standard library of emotion generation rules based mostly on the Ortony, Clore and Collins cognitive emotion model [Ortony88]. (For brevity, I will refer to this as the OCC model.) When designing a specific agent, artists can use these <p> Recall that emotion structures have a type (e.g., anger), a cause (e.g., being insulted), a direction (e.g., the agent who made the insult), and an intensity (e.g., 7 out of 10). Readers interested in the details of Hap are referred to <ref> [Loyall93, Loyall96] </ref>. Otherwise, it is not necessary to understand the Hap code and I suggest looking at the pseudo-code instead. Frustration is generated when the agent has an important behavior that fails. <p> For example, aggressive (agent) leads to scowling, but not at any particular agent. Expressing Emotions BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 110 TABLE 5-2 Default Mapping from Behavioral Features to Body State Effects. 5.2.3 Natural Language Natural language understanding and generation problems are still being studied for believable agents (see <ref> [Loyall96] </ref> and [Kantrowitz96]) and I dont have a lot to say about this topic. However, because the language understanding and generation are being done in Hap, I expect many of the same techniques that are used to affect processing in the action system to apply here. <p> Related Work BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 151 I chose the behavior-based Hap language <ref> [Loyall96] </ref> as a substrate not only for physical behaviors, but for social behaviors as well. <p> Bryan Loyall is also using Hap to control real-time, believable text generation <ref> [Loyall96] </ref>. Believable Social Agents BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 152 TALESPIN tells simple, fable-like stories about simple characters with different personalities and goals, where the characters interact with each other [Meehan76]. These stories are created by the simulation of a set of characters in a simulated world over time. <p> Creating a variety of templates for each response leads to less repeated text. Also, I will demonstrate how I built Melvin to express his personality be choosing between templates based on elements of his character, like his emotions and relationships. Loyall <ref> [Loyall96] </ref> is currently developing a more sophisticated language generator, which I expect will make the characters more believable than I was able to achieve using templates. <p> The player does this again with different names in the third turn of the example and Melvin is still confused. The canned response is effective onceit is terrible twice. Some work in this area is already being pursued by Loyall <ref> [Loyall96] </ref>. Social Issues. The only instance where a user mentioned social factors contributing to a break in the users suspension of disbelief happened with user #2. This user started the interaction by hitting Melvin. Melvin immediately responded by running away and sulking. <p> Mueller [Mueller90] has examined the issue of the relationship between emotions and daydreaming in depth. Natural Language. Although I use some natural language in the characters I have built, I have not examined natural language issues in any depth in this research. This is the subject of <ref> [Loyall96] </ref>, [Kantrowitz96], [Hovy88] and other related literature which I will not attempt to cite. Nonetheless, here are a few suggestions about how the natural language architecture should take emotions into account. Natural Language Generation Lexical. Emotions should be able to affect word choice. Syntactic.
Reference: [Loyall93] <author> Loyall, A.B. and Bates, J. </author> <title> Real-time Control of Animated Broad Agents. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society. </booktitle> <address> Boulder, CO. </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Recall that emotion structures have a type (e.g., anger), a cause (e.g., being insulted), a direction (e.g., the agent who made the insult), and an intensity (e.g., 7 out of 10). Readers interested in the details of Hap are referred to <ref> [Loyall93, Loyall96] </ref>. Otherwise, it is not necessary to understand the Hap code and I suggest looking at the pseudo-code instead. Frustration is generated when the agent has an important behavior that fails. <p> All of these are described briey in Chapter 1 and in <ref> [Loyall93] </ref>. Using these mechanisms, the artist needs to organize the behaviors to provide robustness in ways that fit the personalities of the characters.
Reference: [Maes95] <author> Maes, P. , Darrell, T., Blumberg, B., and Pentland, A. </author> <title> The ALIVE System: Wireless, Full-Body Interaction with Autonomous Agents. </title> <journal> In Communcations of the ACM, Special Issue on New Horizons of Commercial and Industrial AI. </journal> <volume> Vol. 38. No. </volume> <year> 1900. </year> <month> November </month> <year> 1995. </year>
Reference: [Mataric92] <author> Mataric, M.. </author> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher> <address> Cambridge, MA. </address> <year> 1992. </year>
Reference-contexts: Methodology for Building Social Behaviors BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 149 Some researchers have created social agents without using any representations, such as Mataric <ref> [Mataric92] </ref>, Wavish [Wavish92], and Hickman and Shiels [Hickman91]. In all of these cases, the behaviors are (socially) much simpler than artists will often want and none of them incorporate believability. <p> I have found that, for speed and robustness, avoiding unnecessary representations is a useful approach in the behaviors I have built. Maja Mataric has looked at building behavior-based social behaviors, such as ocking and group food gathering <ref> [Mataric92] </ref>. The success of her work with simple robots was encouraging, but she is working on a problem that is very different from mine in a number of ways. She is mostly interested in creating working robots that display simple, recognizable group behaviors like following and ocking. <p> Since the worlds these agents inhabit can be fairly complex and unpredictable, the behavior-based approach seems like a natural choice. Unfortunately, the behavior-based approach, which relies on sensing instead of representation, is well-suited for robust, physical action but is less-well suited for social behaviors. Mataric <ref> [Mataric92] </ref> has shown that some social behaviors, like ocking, can be built without representation, but I found it impossible to create some social behaviors, like variations of negotiation, without any representation. The methodology I propose is to use minimal amounts of representation of other agents.
Reference: [McCloud93] <author> McCloud, S. </author> <title> Understanding Comics. </title> <publisher> Tundra Publishers. </publisher> <address> Northampton, MA. </address> <year> 1993. </year>
Reference: [Meehan76] <author> Meehan, J. </author> <title> The Metanovel: Writing Stories by Computer. </title> <type> Research Report #74. </type> <institution> Computer Science Department, Yale University. </institution> <address> New Haven, CT. </address> <year> 1976. </year> <title> BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 285 </title>
Reference-contexts: I have found useful ideas and inspiration in the work of Meehan <ref> [Meehan76] </ref>, Carbonell [Carbonell79], Dyer [Dyer83], and Lebowitz [Lebowitz84,Lebowitz85]. Each has provided some insight into the problem of building believable emotional characters. Dyers BORIS system understands stories about emotional episodes, such as divorces. <p> Bryan Loyall is also using Hap to control real-time, believable text generation [Loyall96]. Believable Social Agents BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 152 TALESPIN tells simple, fable-like stories about simple characters with different personalities and goals, where the characters interact with each other <ref> [Meehan76] </ref>. These stories are created by the simulation of a set of characters in a simulated world over time. These stories, however, are not interactive, so the behaviors do not need to be robust enough to endure interactions with real users, just other author-defined agents.
Reference: [Moore89] <author> Moore, D. and McCabe, G. </author> <title> Introduction to the Practice of Statistics. W.H. </title> <publisher> Freeman and Company, </publisher> <address> New York, NY. </address> <year> 1989. </year>
Reference-contexts: Also, the presence of outliers means that the claims that I am able to make are not as strong as they otherwise could be, not that they are wrong <ref> [Moore89] </ref>. To compare the emotion scores of Melvin and Chuckie, I applied a t-test to the difference of the scores each user gave Melvin and Chuckie. In other words, for each user, I determined the difference in emotion scores given to Melvin and Chuckie. <p> The coefficient of correlation in this case is 0.46 on a scale of 0 to 1, indicating that scores for emotion and quality of character have a rather high correlation <ref> [Moore89] </ref> 3 . 1. It is standard statistical practice to throw out data that is clearly skewed if there is a reason for the skewing unrelated to the test in question. <p> In this case, a coding bug led to problems that made Melvin not respond to the player for a large portion of this interaction. 2. Linear regression does not assume that the underlying population has any specific distribution (e.g., normal) <ref> [Moore89] </ref>. 3. To get an idea for what this value means, a study relating high school and college success of computer science students at a large midwestern university found that the relationship between math and verbal SAT scores for these students were correlated with coefficient 0.46 [Moore89]. <p> specific distribution (e.g., normal) <ref> [Moore89] </ref>. 3. To get an idea for what this value means, a study relating high school and college success of computer science students at a large midwestern university found that the relationship between math and verbal SAT scores for these students were correlated with coefficient 0.46 [Moore89]. Validation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 124 FIGURE 6-2 Melvin vs. Chuckie: How good a character? This histogram shows the differences in how Melvin and Chuckie were scored in terms of quality of character. The scale ranged from 1 (awful character) to 7 (great character). <p> To get an idea for what this value means, a study relating high school and college success of computer science students at a large midwestern university found that the relationship between high school science grades and overall high school grades scores for these students were correlated with coefficient 0.33 <ref> [Moore89] </ref>. Validation of the Em System BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 127 FIGURE 6-4 Melvin vs.
Reference: [Mor95] <author> Mor, Y., Goldman, C., and Rosenschein, J. </author> <title> Learn Your Opponents Strategy (In Polynomial Time)! In Working Notes of the IJCAI-95 Workshop on Adaptation and Learning in Multi-Agent Systems. </title> <address> Montreal, Canada. </address> <year> 1995. </year>
Reference-contexts: Although I have not borrowed representations from DAI systems for the agents I have built, there has been a lot of work on knowledge representation for multi-agent domains that some artists might find useful (e.g., [Huber95], [Rao95], [Vidal95], <ref> [Mor95] </ref>). I expect this will be true especially if the agent needs to be highly competent at some social task. 7.4.2 Behavior-Based AI My work towards creating believable social behaviors has also been inuenced by work in behavior-based AI.
Reference: [Mueller90] <author> Mueller, E. </author> <title> Daydreaming in Humans and Machines. </title> <publisher> Ablex Publishing Corporation. </publisher> <address> Norwood, NJ. </address> <year> 1990. </year>
Reference-contexts: Whether or not new generators would need to be written will depend upon the structure of the daydreaming system. It could be that the daydreams are handled by the already existing cognitive-appraisal rules. Eric Mueller <ref> [Mueller90] </ref> has done a good deal of work on the relationship between emotions and daydreaming and this work would provide a good starting point for extending Tok/Em to handle daydreaming and daydream based emotions. Sympathetic Emotions People often have emotions about the emotions of other agents. <p> Any restrictions on the type of learning system are unclear. For ideas about the relationship between depres sion and learning, see [Webster92]. Daydreaming. Emotions could drive many different kinds of daydreams, such as dreams of revenge or dreams being driven by anger at someone. Mueller <ref> [Mueller90] </ref> has examined the issue of the relationship between emotions and daydreaming in depth. Natural Language. Although I use some natural language in the characters I have built, I have not examined natural language issues in any depth in this research.
Reference: [Newell76] <author> Newell, A. and Simon, H. </author> <title> Human Problem Solving. </title> <publisher> Pren-tice-Hall Book Company. </publisher> <address> Englewood Cliffs, NJ. </address> <year> 1973. </year>
Reference-contexts: Deliberative approaches (e.g., <ref> [Newell76, Fikes72] </ref>) have tended to rely on representation as an important part of being able to solve difficult problems.
Reference: [Oatley92] <author> Oatley, K. </author> <title> The Best Laid Schemes: A Psychology of Emotions. </title> <publisher> Cambridge University Press. </publisher> <address> Cambridge, England. </address> <year> 1992. </year>
Reference-contexts: Gilboa and Ortony never published this theory, though it is described in [Elliott92]. Foundation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 29 I will, however, briey describe two of the models that I did not choose as a basis for my work: the basic emotion model of Oatley <ref> [Oatley92] </ref> and the emergent emotion model of Sloman [Sloman86]. Oatley hypothesizes five basic emotions that are the foundation of all emotional experience: joy, distress, fear, anger, and disgust. All other emotions are hypothesized to be related to these emotions in various (and, it appears to me, unspecified) ways. <p> In an architecture like Tok, though, I believe that these mechanisms provide the primary ways of creating the kinds of emotional expression that artist say are needed. These ideas come from a variety of sources, including the emotion literature (e.g., [Elliott92], <ref> [Oatley92] </ref>, [Frijda86]), AI (e.g., [Carbonell79]), the arts (e.g., [Thomas81]), and personal experience. The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here.
Reference: [Ortony88] <author> Ortony, A., Clore, A, and Collins G. </author> <title> The Cognitive Structure of Emotions. </title> <publisher> Cambridge University Press. </publisher> <address> Cam-bridge, England. </address> <year> 1988. </year>
Reference-contexts: This means that the emotion theories I draw on do not have to be correct to suit my needs. They only need to be able to help artists build believable emotional characters. I chose as a basis for my work the emotion theories of Ortony, Clore and Collins (OCC) <ref> [Ortony88] </ref> and Gilboa and Ortony [Elliott92] 1 . The first describes when people are emotional and the second describes how people express emotions. One reason for choosing these models is that they were designed to be implemented computationally. <p> Both devised a system to model the relationships and emotions that characters can have about each other. The systems are quite different from each other and Lebowitzs model is different from the psychological model that he drew on [Wish76]. Furthermore, the attitude system that the OCC model <ref> [Ortony88] </ref> proposes has only like and dislike attitudes. So, here are four different models, all of which are useful, though limited. And if I were to choose the superset of all of these models, the chances are good that I would still miss attitudes that artists would want to use. <p> Chapters 3 through 5 will describe the default emotion system in great detail, but here is a brief overview of what it contains: I provide a set of emotion generators based on the cognitive emotion model of Ortony et al. <ref> [Ortony88] </ref>. <p> Emotion System Previous work in the area of computational emotions has taken a psychological model of human emotions and put it into code. For example, Elliotts Affective Reasoner [Elliott92] is a fairly true model of the Ortony, Clore and Collins model of human emotions <ref> [Ortony88] </ref>. Similarly, Frijda and Swagermans AI work [Frijda87] is based on Frijdas previous theoretical work [Frijda86]. In such systems the goal is to build a working model of a psychological theory and to that end these systems are quite effective. <p> In the text-based Oz systems, emotion generators have access to previous sensory inputs as well as the current sensory inputs. Goals, Standards, and Attitudes These are the basic sources of emotions in the cognitive emotion model of Ortony et al. <ref> [Ortony88] </ref>. According to this theory, events in the world are appraised relative to the agents goals; actions of self and other agents are appraised according to a set of standards; and objects are appraised according to attitudes. <p> If he believes the officer is very violent, he will be more scared. Some emotional theories, such as that of Ortony et al. <ref> [Ortony88] </ref>, rely on modeling the plans and goals of other agents in order to appraise their emotional states. Ems default generators can use agent models as inputs, though I have found this kind of modeling to be unnecessary for some characters and worlds. <p> For more on this subject, see [Loyall96]. 3.3 The Tools: Ems Default Emotion Generation Rules Em comes with a standard library of emotion generation rules based mostly on the Ortony, Clore and Collins cognitive emotion model <ref> [Ortony88] </ref>. (For brevity, I will refer to this as the OCC model.) When designing a specific agent, artists can use these default rules in their entirety, or they can pick and choose those that fit their agents. Artists can also add to and modify the rules that are provided. <p> I developed a standard set of emotion generators based on the cognitive emotion model of Ortony, Clore, and Collins <ref> [Ortony88] </ref>. This set of generators provides a starting point for artists, who might otherwise find it very difficult Emotion Generation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 72 to create a good emotional agent with only the Hap language and the set of inputs. <p> Expressive Types Ortony et al., who introduced the notion of emotion types <ref> [Ortony88] </ref>, defined them in terms of their antecedents. So, distress emotions are defined as those caused by appraising an external event to be the cause of the failure or increased likelihood of failure of a goal that is important to the agent. <p> One such model is Gil-boa and Ortonys model of action responses which is described in [Elliott92], but that is less-well developed than the Ortony, Clore and Collins model of emotion generation <ref> [Ortony88] </ref>. <p> The default set of generators is based on the cognitive emotion model of Ortony, Clore, and Collins <ref> [Ortony88] </ref>. I have extended, modified, and simplified their model of emotion generation in a number of ways to make it more suitable to the artistic nature of the task. <p> My suggestions are ways to make Em even more general and exible than it currently is, though I dont know which of these ideas will actually prove useful to artists. The emotion generation suggestions are a sampling of ideas from the emotion research of Ortony, Clore, and Collins <ref> [Ortony88] </ref>. The ideas about emotion expression come from a wider range of sources and are cited below. Some extensions to emotion generation The amount of control the agent has over the outcome of the goal can affect the intensity of various emotion structures, like hope, fear, and anger.
Reference: [Pfeifer93] <author> Pfeifer, R. </author> <title> On the Need to Study "Fungu Eaters." </title> <booktitle> In Working Notes of the Workshop on Architectures Underlying Motivation and Emotion. </booktitle> <address> Birmingham, England. </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This feeling of freedom could potentially lead to more immersive and more effective educational experiences. Another possible area to explore is robotics. Some initial work has already been done in the creation of emotional robots (e.g., <ref> [Pfeifer93] </ref>, [Yamamoto94]), though with very simple tasks and emotion models. I also expect work like that of Sloman and Beaudoin [Sloman94, Beaudoin94] to be useful in this area, even though their goal is not explicitly a robotics one.
Reference: [Rao95] <author> Rao, A. and Georgeff, M. </author> <title> BDI Agents: From Theory to Practice. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems. </booktitle> <address> San Francisco, CA. </address> <publisher> MIT Press. </publisher> <year> 1995. </year>
Reference-contexts: Although I have not borrowed representations from DAI systems for the agents I have built, there has been a lot of work on knowledge representation for multi-agent domains that some artists might find useful (e.g., [Huber95], <ref> [Rao95] </ref>, [Vidal95], [Mor95]). I expect this will be true especially if the agent needs to be highly competent at some social task. 7.4.2 Behavior-Based AI My work towards creating believable social behaviors has also been inuenced by work in behavior-based AI.
Reference: [Read93] <author> Read, T. and Sloman, A. </author> <title> The Technological Pitfalls of Studying Emotion. </title> <booktitle> In Working notes of the Workshop on Architectures Underlying Emotion. </booktitle> <address> Birmingham, England. </address> <month> August, </month> <year> 1993. </year>
Reference-contexts: Read and Sloman <ref> [Read93] </ref> have previously discussed some of the terminological perils associated with working in the area of emotion research. To reiterate the note in the two figures, much of the terminology I use is specific to my work.
Reference: [Reilly94] <author> Reilly, W. S. </author> <title> Synergistic Capabilites in Believable Agents. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Believable Agents. </booktitle> <institution> Stanford University. </institution> <month> March </month> <year> 1994. </year>
Reference: [Schacter62] <author> Schacter, S. and Singer, J. </author> <title> Cognitive, Social, and Physiological Determinants of Emotional State. </title> <journal> In Psychological Review. </journal> <volume> Vol. 69. </volume> <year> 1962. </year>
Reference-contexts: Body State I have simple models of the agents facial expression, state of physical arousal, and muscular state that are available to the emotion generation system. The work of Schacter and Singer <ref> [Schacter62] </ref> showed that physical arousal can inuence the emotional state, even if the arousal isnt actually connected with the apparent cause of the emotion. There are also folk psychology claims that facial expressions can generate emotionsfor instance, smiling can make you happier. <p> Similarly, general arousal (e.g., an adrenaline rush from physical exertion) can affect the intensity of emotions. Psychological evidence has pointed to this being true in humans <ref> [Schacter62] </ref>. The default Em system does not currently have any body-feedback rules. I have included this discussion because I feel that explaining the potential breadth of the architecture might be useful for artists.
Reference: [Schank77] <author> Schank, R. and Abelson, R. </author> <title> Scripts, Plans, Goals, and BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 286 Understanding: An Inquiry into Human Knowledge Structures. </title> <editor> L. </editor> <publisher> Erlbaum Associates. </publisher> <address> Hillsdale, NJ. </address> <year> 1977. </year>
Reference-contexts: Lebowitzs work on UNIVERSE [Lebowitz84, Lebowitz85], which generates non-interactive soap-opera plots, is also relevant in some respects. The UNIVERSE characters have 4-dimensional relationships. Those dimensions are the following: positive/negative, intimate/distant, dominant/submissive, and attract-edness. The first three are from Schank and Abelson <ref> [Schank77] </ref>, based on the work of Wish [Wish76]. Lebowitz added the last because he found it necessary for his task. Perhaps the most interesting part of the four-dimensional theory is that Lebowitz found that three isnt enough.
Reference: [Sengers96] <author> Sengers, P. </author> <title> Controlling Behaviors in a Schizophrenic Agent. </title> <type> Thesis Proposal. </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <address> Pittsburgh, PA. </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Segue goals will probably not scale up well as a solution, but they are capable of providing solutions at least some of the time. Sengers is exploring the use of transition behaviors to solve this problem in a more general way <ref> [Sengers96] </ref>. Segue goals are placed between behaviors and are responsible for creating a smooth transition. For instance, a segue goal might be placed between an on-going social behavior and an interrupting behavior. In the example above, Melvins segue goal would be to speak to the player before walking away.
Reference: [Simon66] <author> Simon, N. </author> <title> The Odd Couple. Random House. </title> <address> New York, NY. </address> <year> 1966. </year>
Reference: [Simon67] <author> Simon, H. </author> <title> Motivational and Emotional Controls of Cognition. </title> <journal> In Psychological Review. </journal> <volume> Vol. 74. </volume> <year> 1967. </year>
Reference-contexts: Sloman (like Simon <ref> [Simon67] </ref>) hypothesizes that emotions are emergent properties of complex, resource-limited, motivation-processing systems. In this model, emotions are states of the overall system and there is no separate emotion component.
Reference: [Sloan91] <author> Sloan, S. </author> <title> Interactive Fiction, Virtual Realities, and the Reading-Writing Relationship. </title> <type> Ph.D. Thesis, </type> <institution> The Ohio State University. Columbus, OH. </institution> <year> 1991. </year>
Reference: [Sloman94] <author> Sloman, A. </author> <title> Explorations in Design Space. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence. </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Another possible area to explore is robotics. Some initial work has already been done in the creation of emotional robots (e.g., [Pfeifer93], [Yamamoto94]), though with very simple tasks and emotion models. I also expect work like that of Sloman and Beaudoin <ref> [Sloman94, Beaudoin94] </ref> to be useful in this area, even though their goal is not explicitly a robotics one. Because my architecture has been designed to be exible, it could be useful as a testbed for the development of emotion systems for robots that are not necessarily artistic.
Reference: [Sloman86] <author> Sloman, A. </author> <title> Motives, Mechanisms, and Emotions. Cognitive Studies Research Paper No. </title> <type> CSRP 062. </type> <institution> University of Sussex, </institution> <address> Brighton. </address> <year> 1986. </year>
Reference-contexts: Foundation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 29 I will, however, briey describe two of the models that I did not choose as a basis for my work: the basic emotion model of Oatley [Oatley92] and the emergent emotion model of Sloman <ref> [Sloman86] </ref>. Oatley hypothesizes five basic emotions that are the foundation of all emotional experience: joy, distress, fear, anger, and disgust. All other emotions are hypothesized to be related to these emotions in various (and, it appears to me, unspecified) ways. <p> In particular, I will show how integrating emotion with the perception and motivation systems enables the creation of models of emotions that are often simpler and more complete than the cognitive models of Ortony et al. that I started with. 1. For an exception, see <ref> [Sloman86] </ref> and [Beaudoin94]. Sloman and Beaudoin hypothesize that states of certain kinds of broad mental architectures can be called emotional. Beaudoin has looked, in particular, at the relationship of motive processing and emotional states. <p> to in section 3.3, is that breadth often allows us to create simpler and more complete models of how emotions are generated by using parts of the architecture such as the motivation and perception systems. 3.1.3 An Explicit Emotion Module Some work in emotion modeling, such as that of Sloman <ref> [Sloman86] </ref> and Beau-doin [Beaudoin94], does not use an explicit emotion system to model emotions. In Slomans model, emotions are states of complex, resource-limited, goal-processing systems. That is, there is no explicit emotion systememotional reactions emerge from the complex processing of certain kinds of goal-processing systems in certain environments.
Reference: [Suchman88] <author> Suchman, L. </author> <title> Plans and Situated Actions: The Problem of Human Machine Communication. </title> <publisher> Cambridge University Press. </publisher> <address> Cambridge, England. </address> <year> 1988. </year>
Reference: [Sycara88] <author> Sycara, K. </author> <title> Resolving Goal Conicts via Negotiation. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <address> St. Paul, MN. </address> <month> July </month> <year> 1988. </year>
Reference-contexts: This difference in emphasis, however, doesnt mean that artists have nothing to learn from DAI. When designing a behavior (like negotiation) it is useful to know how a more competent agent would do it. For instance, I used Katia Sy-caras PERSUADER <ref> [Sycara88] </ref> program as a source of inspiration when designing Melvin and Sluggo. Although I wasnt interested in emulating her system, I benefitted from the analysis she had done in order to create her system. <p> Negotiation was chosen, in large part, as a comparison to other AI work on negotiating agents. My hope was that by choosing a behavior that is already studied by the field, the differences in goals will become more obvious. Traditional approaches (e.g., <ref> [Sycara88] </ref>) aim at competent behaviors. My goal is to create believable behavior, so such issues of personality as incorporating personality quirks and deciding how competent to make the agent are new problems. <p> Also, negotiation is often considered to be a problem where modeling the other parties of the negotiation is considered important, if not necessary. For example, Sycaras system <ref> [Sycara88] </ref> uses structured models of the goals and the goal processing of other agents. To create a highly competent negotiation behavior, this modeling is, in fact, useful and important. Such modeling, I claim, is not always necessary or important for believable agents. <p> I didnt try to analyze the behavior too deeply because I knew that I wasnt going to build agents that were supposed to take part in complex labor negotiations (like Sycaras agents <ref> [Sycara88] </ref>). I was more interested in simpler agents that used simple forms of everyday negotiation. I began by trying to understand some of the dimensions of complexity negotiation can take on.
Reference: [Tambe95] <author> Tambe, M., Johnson, W. L., Jones, R. M., Koss, F., Laird, J. E., Rosenbloom, P. S., and Schwamb, K. </author> <title> Intelligent Agents for Interactive Simulation Environments. </title> <journal> In AI Magazine. </journal> <volume> Vol. 16. No. 1. </volume> <year> 1995. </year>
Reference: [Terzopoulus95] <author> Terzopoulus, D. and Waters, K. </author> <title> Analysis and Synthesis of Facial Image Sequences using Physical and Anatomical Models. </title> <journal> In IEEE Transactions on Pattern Analysis and Machine Intelligence. </journal> <volume> Vol. 15. No. 6. </volume> <year> 1993. </year>
Reference: [Thomas81] <author> Thomas, F. and Johnston, O. </author> <title> Disney Animation: The Illusion of Life. </title> <publisher> Abbeville Press, </publisher> <address> New York. </address> <year> 1981. </year>
Reference-contexts: In fact, artists tell us that emotions are critical to the believability of their characters. Frank Thomas and Ollie Johnston, two of Disneys original animators, wrote a book called The Illusion of Life about creating believable animated characters; here are some of the things they have to say <ref> [Thomas81] </ref>: From the very beginning, it was obvious that these feelings of the characters would be the heart and soul of Disney pictures. (p.473) From the earliest days, it has been the portrayal of emotions that has given the Disney characters the illusion of life. (p.505) The overriding goal for this <p> One important idea about how to create effective emotional agents is that the emotions should be specific to the character. In other words, each character needs to be unique and its emotions need to fit its particular personality. Again, here are some excerpts from The Illusion of Life <ref> [Thomas81] </ref>: These characters showed hatred and scorn in their own way, but in a convincing manner. <p> By expressing emotion in only the characters face, for example, artists find it harder to communicate than if the whole character is used to express the emotion. Thomas and Johnston have this to say on the subject <ref> [Thomas81] </ref>: Foundation BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 27 If a scene calls for showing tense emotions such as anguish, scorn, bitterness, or envy with only facial expression, the animator will be quite limited. <p> My experience has been that this is the hardest of the artistic principles to graspeven Walt Disney had trouble expressing it. Again, from <ref> [Thomas81] </ref>: There was some confusion among the animators when Walt first asked for more realism then criticized the result because it was not exaggerated enough... When Walt asked for realism, he wanted a caricature of realism. One artist analyzed it correctly when he said, I dont think he meant realism. <p> I think he meant something that was more convincing, that made a bigger contact with people, and he just said realism because real things do... (p. 66) Thomas and Johnston, however, are not unclear on the issue <ref> [Thomas81] </ref>: It should be believable, but not realistic.... Tell your story through the broad cartoon characters rather than the straight ones. There is no way to animate strong-enough attitudes, feelings, or expressions on realistic characters to get the communication you should have. <p> One such model is Gil-boa and Ortonys model of action responses which is described in [Elliott92], but that is less-well developed than the Ortony, Clore and Collins model of emotion generation [Ortony88]. Recall from Chapter 2 that four important lessons to learn from that arts about emotions are <ref> [Thomas81] </ref>: (1) emotions (including the expression of emotions) are critical to creating believable agents, (2) emotions should permeate behavior (i.e. they should be expressed through facial expression, motion, speech, thought, etc.), (3) emotional expression should reect the individual, quirky personality of the character, and (4) believability is the goal, not realism. <p> the agent acts as if in a bad mood energy the agent acts lethargically/energetically Feature Type Description of General Effect on Agent Expressing Emotions BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 94 Artistic Considerations in Behavioral Feature Maps Artists claim that exaggeration is an important aspect of traditional acting and animation (e.g., <ref> [Thomas81] </ref>). By exaggeration, they mean that certain aspects of the character are emphasized to make them clearer. For instance, an actor on a stage uses exaggerated movements because they are easier to see from the audience. More subtle forms of exaggeration are used in movies and television acting as well. <p> In an artistic sense, this exaggerates the expression of joy to make it clearer. Another thing to be learned from other media, like animation <ref> [Thomas81] </ref>, is that it can be very difficult to express conicting thoughts or emotions at the same time. In animation, a common technique is to express one thing and express it strongly. <p> in the introduction to this chapter, I have tried to take a cue from traditional arts, where the artists talk about the importance of emotion permeating the character: how they move, how they act, how they speak, what their face looks like, what their body stance is, and much more <ref> [Thomas81] </ref>. Based on these suggestions, I provide artists a large set of mechanisms for expressing emotions in their characters. <p> In an architecture like Tok, though, I believe that these mechanisms provide the primary ways of creating the kinds of emotional expression that artist say are needed. These ideas come from a variety of sources, including the emotion literature (e.g., [Elliott92], [Oatley92], [Frijda86]), AI (e.g., [Carbonell79]), the arts (e.g., <ref> [Thomas81] </ref>), and personal experience. The arts suggest that all of these ways of expressing emotions are important, though they dont make such suggestions at the low level of description that I use here. One of the most detailed examinations of emotional expression is given by Gilboa and Ortony in [Elliott92]. <p> Artists develop their techniques through lots of practice and experimentation. For instance, one of the keys to success for the early Disney animators was the ability to use pencil sketches and other techniques to create quick prototypes; this allowed them to test lots of ideas very quickly <ref> [Thomas81] </ref>. Similarly, I expect that many of the artistic techniques for creating believable agents will only develop with time and practice.
Reference: [Vidal95] <author> Vidal, J. and Durfee, E. </author> <title> Recursive Agent Modeling Using Limited Rationality. </title> <booktitle> In Proceedings of the First Interna BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 287 tional Conference on Multi-Agent Systems. </booktitle> <address> San Francisco, CA. </address> <publisher> MIT Press. </publisher> <year> 1995. </year>
Reference-contexts: Although I have not borrowed representations from DAI systems for the agents I have built, there has been a lot of work on knowledge representation for multi-agent domains that some artists might find useful (e.g., [Huber95], [Rao95], <ref> [Vidal95] </ref>, [Mor95]). I expect this will be true especially if the agent needs to be highly competent at some social task. 7.4.2 Behavior-Based AI My work towards creating believable social behaviors has also been inuenced by work in behavior-based AI.
Reference: [Warner91] <author> Warner, </author> <title> R.L. A Computational Model of Human Emotions. M.S. </title> <type> Thesis. </type> <institution> Department of Computer Science, VPI & SU. Blacksburg, VA. </institution> <year> 1991 </year>
Reference-contexts: The first describes when people are emotional and the second describes how people express emotions. One reason for choosing these models is that they were designed to be implemented computationally. Other researchers (e.g., [Elliott92] and <ref> [Warner91] </ref>) have also implemented versions of these models. Another reason for adopting these models is that they are reasonably simple to understand.
Reference: [Wavish92] <author> Wavish, P. </author> <title> Exploiting Emergent Behavior in Multi-Agent Systems. </title> <booktitle> In Decentralized AI 3, The Proceedings of the Third European Workshop on Modeling Autonomous Agents in a Multi-Agent World. </booktitle> <editor> Y. Demazeau and E. Werner, eds. </editor> <publisher> Elsevier Science Publishers B.V., North Hol-land. </publisher> <year> 1992. </year>
Reference-contexts: Methodology for Building Social Behaviors BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 149 Some researchers have created social agents without using any representations, such as Mataric [Mataric92], Wavish <ref> [Wavish92] </ref>, and Hickman and Shiels [Hickman91]. In all of these cases, the behaviors are (socially) much simpler than artists will often want and none of them incorporate believability.
Reference: [Webster92] <author> Webster, C. </author> <title> Why Intelligent Systems should get Depressed Occasionally and Appropriately. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. Bloomington, IN. </booktitle> <year> 1992. </year>
Reference-contexts: To create this kind of effect, the agent clearly needs some sort of learning system. Any restrictions on the type of learning system are unclear. For ideas about the relationship between depres sion and learning, see <ref> [Webster92] </ref>. Daydreaming. Emotions could drive many different kinds of daydreams, such as dreams of revenge or dreams being driven by anger at someone. Mueller [Mueller90] has examined the issue of the relationship between emotions and daydreaming in depth. Natural Language.
Reference: [Wehyrauch96] <author> Wehyrauch, P. </author> <title> Guiding Interactive Drama. </title> <type> Ph.D. Thesis. </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <address> Pittsburgh, PA. </address> <month> Forthcoming </month> <year> 1996. </year>
Reference: [Weizenbaum66] <author> Weizenbaum, J. </author> <title> Eliza. </title> <journal> In Communications of the ACM. </journal> <volume> Vol. 9. </volume> <year> 1966. </year>
Reference-contexts: In this case, Sluggo has no idea what the player has said, but instead of breaking the suspension of disbelief, I am able to turn Sluggos response into an outlet for his personality. This is similar in spirit to Weizenbaums Eliza program <ref> [Weizenbaum66] </ref> except that artists need to invent new, personality-specific tricks. Elizas therapist role made it possible to turn everything the user said into a question. Sluggo uses a different approach suitable to his personality. <p> In fact, this is a much simpler behavior that simply uses the previous offer to generate the counter-offer. Melvin needs no representation at all to accomplish this. This technique is somewhat similar to techniques used in Eliza <ref> [Weizenbaum66] </ref> where pattern matching and templates are used to replace knowledge and representation. What 1. I could also have used a trust attitude. In either case, Melvin knows how he feels about the agent but he doesnt rely on any independent model of the agent to know this.
Reference: [Wish76] <author> Wish, M., Deutsch, M., and Kaplan, S. </author> <title> Perceived Dimensions of Interpersonal Relations. </title> <journal> In Journal of Personality and Social Psychology. </journal> <volume> Vol. 33. No. 6. </volume> <month> April </month> <year> 1976. </year>
Reference-contexts: Both devised a system to model the relationships and emotions that characters can have about each other. The systems are quite different from each other and Lebowitzs model is different from the psychological model that he drew on <ref> [Wish76] </ref>. Furthermore, the attitude system that the OCC model [Ortony88] proposes has only like and dislike attitudes. So, here are four different models, all of which are useful, though limited. <p> Lebowitzs work on UNIVERSE [Lebowitz84, Lebowitz85], which generates non-interactive soap-opera plots, is also relevant in some respects. The UNIVERSE characters have 4-dimensional relationships. Those dimensions are the following: positive/negative, intimate/distant, dominant/submissive, and attract-edness. The first three are from Schank and Abelson [Schank77], based on the work of Wish <ref> [Wish76] </ref>. Lebowitz added the last because he found it necessary for his task. Perhaps the most interesting part of the four-dimensional theory is that Lebowitz found that three isnt enough.
Reference: [Yamamoto94] <author> Yamamoto, M. SOZZY: </author> <title> A Hormone-Driven Autonomous Vacuum Cleaner. </title> <booktitle> In SPIE Proceedings. Vol. 2058. </booktitle> <address> Boston, MA. </address> <year> 1994. </year> <title> BELIEVABLE SOCIAL AND EMOTIONAL AGENTS 288 </title>
Reference-contexts: This feeling of freedom could potentially lead to more immersive and more effective educational experiences. Another possible area to explore is robotics. Some initial work has already been done in the creation of emotional robots (e.g., [Pfeifer93], <ref> [Yamamoto94] </ref>), though with very simple tasks and emotion models. I also expect work like that of Sloman and Beaudoin [Sloman94, Beaudoin94] to be useful in this area, even though their goal is not explicitly a robotics one.
References-found: 77

