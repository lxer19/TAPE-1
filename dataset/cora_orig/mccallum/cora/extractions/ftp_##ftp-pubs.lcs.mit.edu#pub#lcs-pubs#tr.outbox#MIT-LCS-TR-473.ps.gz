URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-473.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/trlow.html
Root-URL: 
Title: Automatic Replication for Highly Available Services  
Author: by Sanjay Ghemawat 
Address: Cambridge, Massachusetts 02139  
Affiliation: Massachusetts Institute of Technology Laboratory for Computer Science  
Note: c Massachusetts Institute of Technology 1990 This research was supported in part by the Advanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research under contract N00014-83-K-0125, and in part by the National Science Foundation under grant DCR-8503662. The author was supported by a National Science Foundation graduate student fellowship.  
Date: January 1990  
Abstract-found: 0
Intro-found: 1
Reference: [AD76] <author> P. A. Alsberg and J. D. Day. </author> <title> A principle for resilient sharing of distributed resources. </title> <booktitle> In Proceedings of Second National Conference on Software Engineering, </booktitle> <pages> pages 562-570. </pages> <publisher> IEEE, </publisher> <year> 1976. </year>
Reference-contexts: The replicas have to be organized so that as a group they provide a service equivalent to the original unreplicated service. Various replication schemes that achieve this organization are described in the literature: weighted voting [Gif79], quorum consensus [Her86] and primary copy replication <ref> [AD76] </ref>. This thesis describe an implementation of viewstamped replication [Oki88], which is an extension of primary copy replication. This implementation demonstrates that viewstamped replication is an efficient technique for building highly available services.
Reference: [Bar78] <author> Joel F. Bartlett. </author> <title> A `nonstop' operating system. </title> <booktitle> In Eleventh Hawaii International Conference on System Sciences, </booktitle> <pages> pages 103-117, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis [Bir85], Circus [Coo85], Tandem's NonStop System <ref> [Bar78, Bar81] </ref>, and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. The communication network may have an arbitrary topology for example, many local area networks connected by a long haul network.
Reference: [Bar81] <author> Joel F. Bartlett. </author> <title> A nonstop kernel. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 22-29, </pages> <month> December 14-16 </month> <year> 1981. </year> <journal> Appeared in a special issue of SIGOPS Operating System Review, </journal> <volume> Vol. 15, No. 5. </volume> <booktitle> Held at Asilomar Conference Grounds, </booktitle> <address> Pacific Grove, California. </address>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis [Bir85], Circus [Coo85], Tandem's NonStop System <ref> [Bar78, Bar81] </ref>, and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. The communication network may have an arbitrary topology for example, many local area networks connected by a long haul network.
Reference: [BBG83] <author> Anita Borg, Jim Baumbach, and Sam Glazer. </author> <title> A message system supporting fault tolerance. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 90-99, </pages> <month> October 10-13 </month> <year> 1983. </year> <note> Appeared in a special issue of SIGOPS Operating System Review, Vol. 17, No. </note> <month> 5. </month> <title> Held at the Mt. </title> <address> Washington Hotel, Bretton Woods, New Hampshire. </address>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis [Bir85], Circus [Coo85], Tandem's NonStop System [Bar78, Bar81], and Auragen <ref> [BBG83] </ref>). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. The communication network may have an arbitrary topology for example, many local area networks connected by a long haul network.
Reference: [Bir85] <author> Kenneth P. Birman. </author> <title> Replication and fault tolerance in the isis system. </title> <booktitle> In Proceedings of the Tenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86. </pages> <publisher> ACM SIGOPS, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis <ref> [Bir85] </ref>, Circus [Coo85], Tandem's NonStop System [Bar78, Bar81], and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. <p> Performance measurements of the replicated system are given, and compared with the corresponding measurements for an unreplicated Argus system. A brief comparison of the replicated system with the replicated transaction facility in Isis <ref> [Bir85] </ref> is also given. Chapter 5 presents my implementation of the algorithm used to handle system failures. This algorithm involves communication between the replicas to select a new primary. The performance of this algorithm is discussed, and some future optimizations are presented. <p> Section 4.5 describes the implementation of handler calls. Section 4.6 describes the two phase commit protocol used for topaction commits. Section 4.7 presents some performance figures for transactions running under viewstamped replication. Finally, Section 4.7.3 compares the performance of the replicated system with the Isis replicated transaction facility <ref> [Bir85] </ref>. 4.1 Overview The organization of the transaction processing run-time system is given in Figure 4.1. The primary executes incoming handler calls and participates in the two phase commit protocol with primaries of other guardian groups. <p> This translates into a difference of 1.6 ms per handler call, which is reasonable, given the observed difference of 2 ms between handler calls in R (3; 3) and O. 4.7.3 Comparison with Isis This section compares the performance of the replicated transaction system with the Isis replicated transaction facility <ref> [Bir85] </ref>. Each service in the Isis system is composed of several replicas. Rather than designating one replica as the primary for the entire service, any replica can act as the coordinator for a particular handler call. Since different handler calls can have 4.7. <p> Operation Isis VS Lookup 100 28 Insert 158 30 These numbers measure the average cost per handler call (in milliseconds) of a transaction containing 25 requests. The first column gives the performance of an Isis system <ref> [Bir85] </ref> running on Sun 2/50 computers. The second column gives the numbers for a system with viewstamped replication running on MicroVAX II computers. Both systems use a 10 megabit/second ethernet for communication. The difference in performance arises mainly from the large costs of the atomic broadcast protocols used in Isis.
Reference: [BJ87] <author> Kenneth P. Birman and Thomas A. Joseph. </author> <title> Reliable communication in the presence of failure. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: PERFORMANCE 49 different coordinators, the handler calls have to be synchronized with each other. Read handler calls are handled locally by acquiring a read lock and returning the result to the caller. Write handler calls are handled by first using an atomic broadcast protocol <ref> [BJ87] </ref> to acquire write locks at all the replicas and then performing the modifications. The following table gives the costs of looking up and inserting new entries into a directory service consisting of three replicas.
Reference: [Coh89] <author> Jeffery I. Cohen. </author> <title> Distributed atomic stable storage. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: However, it is not possible to use this mechanism on disk-less workstations. On disk-less workstations, the service should probably use a network based stable storage service to provide high resiliency to failures. Using stable storage. <ref> [DST87, Coh89] </ref> describe highly available network-based stable storage services. These services achieve high availability by replicating the stable storage servers and using a voting scheme [Gif79] to access the replicas. Such stable storage services can be used to provide highly available and reliable general purpose services.
Reference: [Coo85] <author> Eric C. Cooper. </author> <title> Replicated distributed programs. </title> <type> Technical Report UCB/CSD 85/231, </type> <address> U. C. </address> <institution> Berkeley, EECS Dept., Computer Science division, </institution> <month> May </month> <year> 1985. </year> <type> Ph.D. thesis. </type>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis [Bir85], Circus <ref> [Coo85] </ref>, Tandem's NonStop System [Bar78, Bar81], and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network.
Reference: [DST87] <author> Dean S. Daniels, Alfred Z. Spector, and Dean S. Thompson. </author> <title> Distributed logging for transaction processing. </title> <booktitle> In In ACM Special Intrerest Group on Management of Data 1987 Annual Conference, </booktitle> <pages> pages 82-96. </pages> <booktitle> ACM SIGMOD, </booktitle> <month> May </month> <year> 1987. </year> <note> 77 78 BIBLIOGRAPHY </note>
Reference-contexts: However, it is not possible to use this mechanism on disk-less workstations. On disk-less workstations, the service should probably use a network based stable storage service to provide high resiliency to failures. Using stable storage. <ref> [DST87, Coh89] </ref> describe highly available network-based stable storage services. These services achieve high availability by replicating the stable storage servers and using a voting scheme [Gif79] to access the replicas. Such stable storage services can be used to provide highly available and reliable general purpose services.
Reference: [ESC85] <author> Amr El Abbadi, Dale Skeen, and Flaviu Cristian. </author> <title> An efficient, fault-tolerant protocol for replicated data management. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles of Data Base Systems, </booktitle> <pages> pages 215-229. </pages> <publisher> ACM, </publisher> <year> 1985. </year>
Reference-contexts: When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions <ref> [ESC85] </ref>, Isis [Bir85], Circus [Coo85], Tandem's NonStop System [Bar78, Bar81], and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. <p> When the communication capability of the guardian group changes, due to either failure or recovery of a communication link or a node, a view change algorithm is initiated to form a new view. A modification of the virtual partitions protocol proposed in <ref> [ESC85] </ref> is used for this purpose. The algorithm creates a new view consisting of at least a majority of the cohorts in the configuration. It also assigns a unique viewid to this new view.
Reference: [Gif79] <author> D. K. Gifford. </author> <title> Weighted voting for replicated data. </title> <booktitle> In Proceedings of the Seventh Symposium on Operating Systems Principles, </booktitle> <pages> pages 150-162, </pages> <address> Pacific Grove, CA, </address> <month> December </month> <year> 1979. </year> <note> ACM SIGOPS. </note>
Reference-contexts: This replication, however, does not come for free. The replicas have to be organized so that as a group they provide a service equivalent to the original unreplicated service. Various replication schemes that achieve this organization are described in the literature: weighted voting <ref> [Gif79] </ref>, quorum consensus [Her86] and primary copy replication [AD76]. This thesis describe an implementation of viewstamped replication [Oki88], which is an extension of primary copy replication. This implementation demonstrates that viewstamped replication is an efficient technique for building highly available services. <p> When this reorganization is complete, normal activities are allowed to resume. [Oki88] gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting <ref> [Gif79] </ref>, Virtual Partitions [ESC85], Isis [Bir85], Circus [Coo85], Tandem's NonStop System [Bar78, Bar81], and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of many nodes linked by a communication network. <p> On disk-less workstations, the service should probably use a network based stable storage service to provide high resiliency to failures. Using stable storage. [DST87, Coh89] describe highly available network-based stable storage services. These services achieve high availability by replicating the stable storage servers and using a voting scheme <ref> [Gif79] </ref> to access the replicas. Such stable storage services can be used to provide highly available and reliable general purpose services. Consider an unreplicated Argus guardian that uses a stable storage service for resiliency.
Reference: [GL89] <author> Robert Gruber and Barbara Liskov. </author> <title> Witnesses. Mercury Design Note 36, Programming Methodology Group, </title> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Under most circumstances, the availability of the system is also not significantly reduced when some copies are replaced with witnesses. [MHS89] and <ref> [GL89] </ref> adapt the witness scheme for primary copy replication. In primary copy replication, witnesses participate only in primary elections (view changes); they do not store any service state.
Reference: [GLPT76] <author> J. N. Gray, R. A. Lorie, G. F. Putzolu, and I. L. Traiger. </author> <title> Granularity of locks and degrees of consistency in a shared data base. </title> <editor> In G.M. Nijssen, editor, </editor> <booktitle> Modeling in Data Base Management Systems. </booktitle> <publisher> North Holland, </publisher> <year> 1976. </year>
Reference-contexts: A crash destroys all processes and objects at a guardian. After the crash, the Argus run-time system recovers the guardian's objects from stable storage. The loss of processes is masked by running computations as atomic actions <ref> [GLPT76] </ref>, or actions for short. Actions also solve the problems created by allowing concurrency within one guardian. 2.2 Atomic Actions Two useful properties of atomic actions that help solve the problems created by concurrency and failures are serializability and totality. <p> Operations running on behalf of atomic actions limit their access to atomic objects. Two key properties of atomic objects help control this access to implement both serializability and totality of atomic actions. Two Phase Locking First, every operation carried out on any atomic object uses strict two-phase locking <ref> [GLPT76] </ref> to implement serializability of concurrent actions. Every operation on an atomic object is classified as either a reader or a writer. All operations that modify an object are called writers; other operations are called readers.
Reference: [Gra78] <author> J. Gray. </author> <booktitle> Notes on database operating systems. In Operating Systems An Advanced Course, volume 60 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: In addition, communication link failures might cause the network to partition into isolated subnetworks that cannot communicate with each other. We assume that both node and link failures are repaired. 1.3. OVERVIEW 15 1.3 Overview Chapter 2 presents the Argus model of computation which is based on transactions <ref> [Gra78] </ref>, and describes the portions relevant to this implementation. Chapter 3 gives a general overview of the replication scheme. In particular, the terminology used is presented, and then the replication scheme is described. Chapter 4 covers my implementation of transactions. <p> In this manner, topaction T, which started at G1 manages to spread to G2, G3 and G4. When T commits, all modifications made by T's descendants are written to stable storage using the standard two-phase commit protocol <ref> [Gra78] </ref>. The guardian where the topaction started acts as the coordinator for the commit protocol; guardians visited by descendants of the topaction are the participants. Information about the guardians visited by the descendants of a topaction is collected in handler call reply messages. <p> In the preceding example, G1 will be the coordinator and G1, G2, G3 and G4 will be the participants. 2.3. COMMITTING AND ABORTING ACTIONS 23 2.3.1 Two Phase Commit Subaction commit and aborts are implemented using the rules in Figure 2.3. The two phase commit protocol <ref> [Gra78] </ref> is used to implement topaction commits. In the first phase, the coordinator sends out prepare messages to all the participants. When a participant receives a prepare message for topaction T, it releases all read locks held by T and sends a reply back to the coordinator.
Reference: [Her86] <author> M. Herlihy. </author> <title> A quorum-consensus replication method for abstract data types. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(1) </volume> <pages> 32-53, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: This replication, however, does not come for free. The replicas have to be organized so that as a group they provide a service equivalent to the original unreplicated service. Various replication schemes that achieve this organization are described in the literature: weighted voting [Gif79], quorum consensus <ref> [Her86] </ref> and primary copy replication [AD76]. This thesis describe an implementation of viewstamped replication [Oki88], which is an extension of primary copy replication. This implementation demonstrates that viewstamped replication is an efficient technique for building highly available services.
Reference: [Hwa87] <author> D. J. Hwang. </author> <title> Constructing a highly-available location service for a distributed environment. </title> <type> Technical Report MIT/LCS/TR-410, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <month> November </month> <year> 1987. </year> <type> Master's thesis. </type>
Reference-contexts: Details of how a new guardian takes over from an old one need to be worked out. In 6.2. EXTENSIONS 71 particular, some form of a view change algorithm is required to replace the original guardian after it crashes. In addition, a location service <ref> [Hwa87] </ref> is needed to route requests to the new guardian. It would also be interesting to see if the voting scheme used for implementing the replicated stable storage services should be replaced by a primary copy replication scheme.
Reference: [Lam81] <author> B. Lampson. </author> <title> Atomic transactions. </title> <booktitle> In Distributed Systems: Architecture and Implementation, volume 105 of Lecture Notes in Computer Science, </booktitle> <pages> pages 246-265. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Data stored on disks survives most node crashes, but some node crashes may result in disk media failures that destroy this data. Each node also has a small amount of stable storage. Stable storage is a memory device that preserves information written to it with a very high probability <ref> [Lam81] </ref>. The communication network may drop messages, duplicate them, deliver them out of order, or delay them. In addition, communication link failures might cause the network to partition into isolated subnetworks that cannot communicate with each other. We assume that both node and link failures are repaired. 1.3.
Reference: [LCJS87] <author> B. Liskov, D. Curtis, P. Johnson, and R. Scheifler. </author> <title> Implementation of argus. </title> <booktitle> In Proc. of the 11th Symposium on Operating Systems Principles, </booktitle> <address> Austin, Tx, </address> <month> November </month> <year> 1987. </year> <note> ACM. </note>
Reference-contexts: This technique was motivated by the very large granularity (10 ms) of the system clock. All measurements were obtained on MicroVAX II's running the 4.3 BSD operating system and connected by a 10 megabits/second ethernet. For more detailed performance measurements of the unreplicated system see <ref> [LCJS87] </ref>. 4.7. PERFORMANCE 45 Handler Orig- Replicated Call inal 1,1 1,3 3,1 3,3 Null 19 20 20 20 20 Read 23 24 24 24 25 Write 23 24 25 25 25 Table 4.1: The time (in milliseconds) required to make a handler call.
Reference: [Lis88] <author> B. Liskov. </author> <title> Distributed programming in argus. </title> <journal> Comm. of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Performance measurements that support this claim are given in a later chapter. This implementation is done in context of the Argus programming language and run-time system <ref> [Lis88] </ref>. The Argus system has been designed as a tool for easy construction of long-lived services. The Argus run-time system is modified to automatically make services written using the Argus programming language highly available. 13 14 CHAPTER 1. <p> The performance of this algorithm is discussed, and some future optimizations are presented. Chapter 6 presents a summary of what has been accomplished and discusses some directions for future research. Chapter 2 The Argus System Argus is an integrated programming language and run-time system <ref> [Lis88] </ref> for developing distributed programs. It is intended to be used primarily to write programs that maintain online data for long periods of times, such as banking systems, mail systems and airline reservation systems. This on-line data is required to remain consistent in spite of failures and concurrent access.
Reference: [Lis89] <author> Barbara Liskov. </author> <title> Using time to avoid communication. Mercury Design Note 37, Programming Methodology Group, </title> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: This optimization could not be made because the primary could not be sure whether or not a new primary had been selected without its knowledge. There is a simple solution to this problem <ref> [MHS89, Lis89] </ref>. Each backup periodically makes a promise to the primary saying that it will not enter a new view for the next n seconds. This information allows the primary to place a lower bound on the earliest time at which a new view can be formed.
Reference: [MHS89] <author> T. Mann, A. Hisgen, and G. Swart. </author> <title> An algorithm for data replication. </title> <type> Report 46, </type> <institution> DEC Systems Research Center, </institution> <address> Palo Alto, CA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Under most circumstances, the availability of the system is also not significantly reduced when some copies are replaced with witnesses. <ref> [MHS89] </ref> and [GL89] adapt the witness scheme for primary copy replication. In primary copy replication, witnesses participate only in primary elections (view changes); they do not store any service state. <p> This optimization could not be made because the primary could not be sure whether or not a new primary had been selected without its knowledge. There is a simple solution to this problem <ref> [MHS89, Lis89] </ref>. Each backup periodically makes a promise to the primary saying that it will not enter a new view for the next n seconds. This information allows the primary to place a lower bound on the earliest time at which a new view can be formed.
Reference: [Mos81] <author> J.E.B. Moss. </author> <title> Nested transactions: an approach to reliable distributed computing. </title> <type> Technical Report MIT/LCS/TR-260, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <year> 1981. </year> <type> PhD thesis. BIBLIOGRAPHY 79 </type>
Reference-contexts: If the action aborts, the tentative version is thrown away. 2.2.2 Nested Actions Atomic actions can be generalized to nested atomic actions by using subactions to build higher level actions in a hierarchical fashion, thus forming trees of nested actions <ref> [Mos81] </ref>. An action that is nested inside another is called a subaction. Non-nested actions are called topactions. The standard tree terminology of parent, child, ancestor and descendant applies to action trees. Nested actions have two desirable properties.
Reference: [Nel81] <author> B. Nelson. </author> <title> Remote procedure call. </title> <type> Technical Report CMU-CS-81-119, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, Pa., </address> <year> 1981. </year>
Reference-contexts: Internally, a guardian contains data objects that represent the resources it is controlling. These objects can be accessed only by invoking the guardian's handlers using a remote procedure call mechanism <ref> [Nel81] </ref>. The caller sends the name of the handler to be invoked, and some arguments, to the guardian in a call message. When the handler invocation is finished, the results are passed back to the caller in a return message (see Figure 2.2). The Argus run-time 17 18 CHAPTER 2.
Reference: [Oki88] <author> B. M. </author> <title> Oki. Viewstamped replication for highly available distributed systems. </title> <type> Technical Report MIT/LCS/TR-423, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <address> Cam-bridge, MA, </address> <month> May </month> <year> 1988. </year> <type> PhD thesis. </type>
Reference-contexts: Various replication schemes that achieve this organization are described in the literature: weighted voting [Gif79], quorum consensus [Her86] and primary copy replication [AD76]. This thesis describe an implementation of viewstamped replication <ref> [Oki88] </ref>, which is an extension of primary copy replication. This implementation demonstrates that viewstamped replication is an efficient technique for building highly available services. The replication scheme has little impact on the performance of the service, but allows the service to remain available in the presence of failures. <p> Whenever a failure occurs, normal activity is suspended, the replicas are reorganized, and a new primary is selected if the old one is now inaccessible. When this reorganization is complete, normal activities are allowed to resume. <ref> [Oki88] </ref> gives a comparison of the viewstamped replication scheme with several well-known replication schemes and systems from the literature (Voting [Gif79], Virtual Partitions [ESC85], Isis [Bir85], Circus [Coo85], Tandem's NonStop System [Bar78, Bar81], and Auragen [BBG83]). 1.2 System Model The replication method operates in a distributed computer system that consists of <p> Since the programming model is not changed, the programmer writes programs as before, and the run-time system is configured so that the services provided by these programs are highly available. This chapter gives an overview of the replication scheme as described in <ref> [Oki88] </ref>. The replication scheme is then broken down into several different sections, and the implementation of each is described separately in later chapters. 3.1 Overview Each individual guardian is replicated to obtain a guardian group. Each guardian group consists of several members called cohorts. <p> For example, let V SS (T; P ) for an action be fhv1:3i, hv1:5i, hv1:6i, hv2:1i, hv2:8ig The reduced viewstamp set will be just fhv1:6i, hv2:8ig. This optimization is performed as the viewstamp sets are merged up the action tree. The optimization was first given in <ref> [Oki88] </ref>, but it was used directly, and not presented as a method for reducing the size of the viewstamp sets. 4.3 Event Log When a backup joins a view, the primary may need to update the backup's state so that it corresponds to the primary's state. <p> Second, all previously committed information should be present at the primary of the new view. The view formation conditions in Figure 5.11 ensure that the new view has the two required properties. These conditions are a relaxation of the ones described in <ref> [Oki88] </ref> and were developed during a discussion with Robert E. Gruber. The algorithm in Figure 5.11 checks that the new view has the first property in the obvious way. 62 CHAPTER 5. <p> units to 2 (n 1) because the primary will send n 1 messages (one to each backup), and the n 1 backups will each send a message to the primary. 5.5.2 Preventing Concurrent View Managers The view change algorithm performs correctly in the presence of multiple concurrent view change managers <ref> [Oki88] </ref>. However, it is desirable to minimize the number of concurrent view changes initiated in response to a failure or recovery detection, as concurrent view changes can interfere with each other and delay the formation of a new view. <p> These measurements are analyzed and compared with corresponding measurements of the performance of an unreplicated system. * The conditions required for forming a new view as given in <ref> [Oki88] </ref> were more stringent than required. <p> view formations in cases where the original conditions would not have allowed a view formation. * A clean break-down of the replication scheme into different components is given (see * The original replication scheme would transfer the entire service state when sending information from a new primary to the backups <ref> [Oki88] </ref>. The thesis describes a mechanism that solves this problem by transferring only the differences between the replica states.
Reference: [P 86] <author> Jehan-Francois P aris. </author> <title> Voting with witnesses: A consistency scheme for replicated files. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computer Systems, </booktitle> <pages> pages 606-612. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: Both voting and viewstamped replication schemes require a minimum of three replicas to provide higher availability than an unreplicated system. However, two copies of data are enough to make it resilient to many failures (most stable storage implementations use two disks to store each piece of data). <ref> [P 86] </ref> proposes an extension to voting schemes where some replicas do not store the service state; they just provide votes for operations. The replicas without service state are called witnesses; the replicas with service state are copies.
Reference: [Sch83] <author> F. B. Schneider. </author> <title> Fail-stop processors. </title> <booktitle> In Digest of Papers from Spring Compcon '83, </booktitle> <pages> pages 66-70, </pages> <address> San Francisco, </address> <month> March </month> <year> 1983. </year> <note> IEEE. </note>
Reference-contexts: We assume that in the absence of failures, any node can send a message to any other node in the system. Both the nodes and the communication network may fail. Nodes may crash, but we assume that they are failstop <ref> [Sch83] </ref>; i.e., once a node fails in any manner, it stops all activity. Each node has volatile storage that is lost in a crash. Nodes may also have disks that provide nonvolatile storage.
Reference: [Tan81] <author> A. S. Tanenbaum. </author> <booktitle> Computer Networks, </booktitle> <pages> pages 148-164. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: The process sender [b] waits until some event needs to be sent to be b. It then uses a reliable message delivery system to send the portion of the event log between extent [b] and flood-to [b] to b. The current implementation uses a sliding window protocol <ref> [Tan81] </ref> for fast and efficient message delivery. This protocol updates extent [b] based on the information present in acknowledgment messages received from b. The interface to this transmission mechanism is provided by the procedure force-events shown in Figure 4.5.
References-found: 27

