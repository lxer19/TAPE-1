URL: ftp://ftp.media.mit.edu/pub/donath/IllustratedConversation.ps
Refering-URL: http://judith.www.media.mit.edu/Judith/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Abstract  
Abstract: Collaboration-at-a-Glance is a program which provides a visual interface to an online conversation. Although the participants in the conversation may be at widely separate locations, the interface provides a visible shared electronic space for their interactions. The participants each have a first person view-point from which they can see who else is present and who is communicating with whom. In the first part of this paper, I describe the current implementation. In the second part of the paper, I discuss some of issues involved in expanding it to supplement an ongoing conversation with additional expressive information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agawa, Hiroshi; Xu, Gang; Nagashim, Yoshio; and Kishino, Fumio. </author> <year> 1990. </year> <title> Image analysis for face modeling and facial image reconstruction. </title> <booktitle> Proceedings of SPIE: Visual Communications and Image Processing Vol 1360, </booktitle> <pages> 1184-1197. </pages>
Reference: [2] <author> Brennan, S. </author> <year> 1982. </year> <title> Caricature generator. </title> <type> Masters Thesis, </type> <institution> MIT. </institution>
Reference: [3] <editor> Brilliant, R. </editor> <address> 1991.Portraiture. Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: Fading can show a continuum; it also emphasizes the images of those who are active. While the meaning of stylizations such as changing a photograph to a line drawing [16], or fading it with time are taken directly from real life, neither are they wholly arbitrary <ref> [3] </ref>. Knowing the precise meaning of a face that appears as a drawing may require asking, but recognizing that those shown as full color realistic images are more present is clear. Location.
Reference: [4] <author> Bull, Peter. </author> <year> 1990. </year> <title> What does gesture add to the spoken word. </title> <editor> In (H. Barlow, C. Blakemore and M. Weston-Smith, eds.) </editor> <title> Images and Understanding. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <pages> 108-121. </pages>
Reference: [5] <author> Burke, Peter. </author> <title> 1993.The Art of Conversation. </title> <address> Ithaca: </address> <publisher> Cornell University Press. </publisher>
Reference: [6] <author> Choi, S.C.; Aizawa, K.;Harashima H. and Tsuyoshi, T. </author> <title> 1994Analysis and synthesis of facial image sequences in model-based image coding. </title> <journal> In IEEE Transactions on Circuits and Systmes for Video Technology. </journal> <volume> Vol. 4. </volume> <pages> 257-275. </pages>
Reference: [7] <author> Curtis, P. </author> <year> 1992. </year> <title> Mudding: social phenomena in text-based virtual realities. </title> <booktitle> Proceedings of the 1992 Conference on Directions and Implications of Advanced Computing. </booktitle> <address> Berkeley, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In the second part, I will discuss some directions for future research, including issues such as the visualization of emotion and the balance of control between the viewer and the subject. 1. A MUD (Multi-User Dungeon) is a network-accessible, multi-participant, user-extensible virtual reality whose user interface is entirely textual <ref> [7] </ref>. Some MUDs are used for game-playing. Many, however, are social environments where the primary active is real-time communication. The Illustrated Conversation Judith S. Donath MIT Media Lab To appear in Multimedia Tools and Applications, Vol 1, March 1995. <p> For a limited expression domain a possible method for invoking expressions is verbal: the participant types the words smile or angry or surprised, and the expression on the representing image changes accordingly. Some evidence for the viability of this approach is found in the world of MUDs <ref> [7] </ref>. In this community, which is conversational but entirely text based, there has emerged a practice of indicating expressive actions verbally: if a user named Sal types :frowns, then all the other participants see Sal frowns.
Reference: [8] <author> Donath, J. </author> <year> 1994. </year> <title> Casual collaboration. </title> <booktitle> In Proceedings of the International Conference on Multimedia Computing and Systems. </booktitle> <address> California: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [9] <author> Dourish, P. and Bly, S. Portholes: </author> <title> Supporting Awareness in a Distributed Work Group. </title> <booktitle> Proceedings of ACM Conference on Human Factors in Computer Systems, CHI 92, </booktitle> <address> Monterey, CA. </address>
Reference: [10] <author> Goffman, E. </author> <year> 1959. </year> <title> The Presentation of Self in Everyday Life. </title> <address> New York: </address> <publisher> Doubleday. </publisher>
Reference: [11] <author> Gombrich, E.H. </author> <year> 1972. </year> <title> The mask and the face: </title> <booktitle> the perception of physiognomic likeness in life and in art. In Art, Perception and Reality. </booktitle> <address> Baltimore: </address> <publisher> The Johns Hopkins University Press, </publisher> <pages> 1-46. </pages>
Reference: [12] <author> Hochberg, J. </author> <year> 1978. </year> <title> Perception. 2nd ed. </title> <address> Englewood Cliffs: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: In order to clearly see the subject of ones attention, one must be looking close to straight at it. Furthermore, turning to face the person or item of interest is an instinct that develops quite early. We are quite good at following gaze <ref> [12] </ref>. In general, we assume that someones thoughts are on the item they are looking at. <p> It is not surprising that one is especially attuned to being looked at - we are able to detect with an accuracy of one minute of visual angle whether or not someone is looking at us <ref> [12] </ref>. Yet in many collaborative interfaces, the images of the other participants are always shown facing out at the viewer. In Collaboration-at-a-Glance gaze direction provides information.
Reference: [13] <author> Hochberg, J. </author> <year> 1972. </year> <title> The representation of things and people. In Art, Perception and Reality. </title> <publisher> Baltimore: The Johns Hopkins University Press, </publisher> <pages> 47-94. </pages>
Reference: [14] <author> Koch, Reinhard. </author> <year> 1991. </year> <title> Adaptation of a 3D Facial Mask to Human Faces in Videophone Sequences using Model Based Image Analysis. </title> <booktitle> Proceedings of the Picture Coding Symposium, </booktitle> <address> Tokyo, Japan. </address> <pages> 285-288. </pages>
Reference: [15] <author> Morishima, S. Aizawa, K. and Harashima, H. </author> <year> 1990. </year> <title> A real-time facial action image synthesis sytem driven by speech and text. </title> <booktitle> In SPIE Visual Communications and Image Processing. </booktitle> <volume> Vol 1360, </volume> <pages> 1151-1158. </pages>
Reference: [16] <author> Pearson, D.; Hanna, E.; and Martinez, K. </author> <year> 1990. </year> <title> Computer Generated Cartoons. </title> <editor> In (H.Barlow, C. Blakemore, and M. Weston-Smith, eds.) </editor> <title> Images and Understanding. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <pages> 46-60. </pages>
Reference-contexts: Fading can show a continuum; it also emphasizes the images of those who are active. While the meaning of stylizations such as changing a photograph to a line drawing <ref> [16] </ref>, or fading it with time are taken directly from real life, neither are they wholly arbitrary [3]. Knowing the precise meaning of a face that appears as a drawing may require asking, but recognizing that those shown as full color realistic images are more present is clear. Location.
Reference: [17] <author> Reid, E. </author> <year> 1991. </year> <title> Electropolis: Communication and community on internet relay chat. </title> <type> Thesis, </type> <institution> Dept. of History, University of Melbourne. </institution>
Reference-contexts: These include the non-real time exchanges of postings found in netnews or on bulletin boards, and also the real-time discussions that take place in the social MUDs 1 , on Internet Relay Chat <ref> [17] </ref> and in the chatrooms of America OnLine. These conversations are entirely text-based. Lacking visual cues, the participants in these conversations cannot see how many people are around, nor can they see where the attention of the group is directed.
Reference: [18] <author> Rheingold, H. </author> <year> 1993. </year> <title> The Virtual Community: </title> <booktitle> Homesteading on the Electronic Frontier. </booktitle> <address> MA: </address> <publisher> Addison-Wesley Pub. </publisher> <address> Co. </address> <note> The Illustrated Conversation page 9 of 9 </note>
Reference: [19] <author> Sproull, L. & Kiesler, S. </author> <year> 1991. </year> <title> Connections: New Ways of Working in the Net-worked Organization. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference: [20] <author> Sproull, L. & Faraj, S. </author> <year> 1993. </year> <title> Atheism, sex, and databases: the net as a social technology. </title> <editor> Forthcoming in (B. Kahin and J. Keller, eds.) </editor> <title> Public Access to the Internet. </title> <publisher> Prentice-Hall. </publisher>
Reference: [21] <author> Sproull, L.; Subramani, R.; Walker, J.; and Kiesler, S. </author> <year> 1994. </year> <title> When the interface is a face. </title> <type> Unpublished manuscript. </type>
Reference: [22] <author> Takeuchi, A.and Nagao, K.. </author> <year> 1993. </year> <title> Communicative facial displays as a new conversational modality. </title> <booktitle> In Proceedings of Interchi 93. </booktitle> <publisher> ACM Press. </publisher>
Reference: [23] <author> Tufte, E. R. </author> <year> 1990. </year> <title> Envisioning Information. </title> <address> Cheshire, CT: </address> <publisher> Graphics Press. </publisher>
Reference: [24] <author> Waters, K. and Terzopoulos, D. </author> <year> 1992. </year> <title> The Computer Synthesis of Expressive Faces. </title> <journal> Philosophical Transactions of the Royal Society B. </journal> <volume> 335, </volume> <pages> 87-93. </pages>
References-found: 24

