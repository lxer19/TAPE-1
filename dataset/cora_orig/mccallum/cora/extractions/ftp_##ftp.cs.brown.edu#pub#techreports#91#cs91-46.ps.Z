URL: ftp://ftp.cs.brown.edu/pub/techreports/91/cs91-46.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-91-46.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AbW] <author> N. Abe and M. Warmuth, </author> <title> "On the Computational Complexity of Approximating Distributions by Probabilistic Automata," </title> <type> UCSC, </type> <institution> UCSC-CRL-90-63, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: PPM-based algorithms are practical implementations of mth order coding methods, and Theorem 3 indicates that performance should be good. Empirical work is progressing to compare PPM-based methods with the Ziv-Lempel methods. The framework of Abe and Warmuth <ref> [AbW] </ref>, who investigated a quite different learning problem related to FSAs, has led us to propose a static PAC-learning framework for prefetching, in which the prefetcher is trained on several independently 18 6 CONCLUSIONS generated sequences of a particular length generated by a source, and the prefetcher should converge sufficiently fast.
Reference: [AmM] <author> Y. Amit and M. Miller, </author> <title> "Large Deviations for Coding Markov Chains and Gibbs Random Fields," </title> <institution> Washington University, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: vectors (p 1 ; . . . ; p ff ) and (r 1 ; . . . ; r ff ), we have ff X jp i r i j 2 i=1 p i : Proof : The above lemma is well known; however, we reproduce the proof from <ref> [AmM] </ref> here for the sake of completeness. From the fact that x ln x x + 1 0 and that 3 (x 1) 2 (4x + 2)(x ln x x + 1), we get jx 1j (4x + 2)=3 x ln x x + 1.
Reference: [BCW] <author> T. C. Bell, J. C. Cleary, and I. H. Witten, </author> <title> "Text Compression," </title> <year> 1990. </year>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [BCW, Lana] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in [ZiL] hold without change for the character-based approach. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiL] hold without change for the character-based approach. Example 1 Assume for simplicity that our alphabet is fa; bg. We consider the page access sequence "aaaababaabbbabaa . . .". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa) . . .".
Reference: [BEHa] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <note> "Occam's Razor," Information Processing Letters 24 (1987). </note>
Reference-contexts: Their evaluations of prefetching performance are empirical. In this paper, we give the first provable theoretical bounds on prefetching performance. Our novel approach is to use optimal data compression methods to do optimal prefetching. Recent work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is synonymous with generalization and data compression.
Reference: [BEHb] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> "Learnability and the Vapnik Chervonenkis Dimension," </title> <note> Journal of the ACM (October 1989). </note>
Reference-contexts: Their evaluations of prefetching performance are empirical. In this paper, we give the first provable theoretical bounds on prefetching performance. Our novel approach is to use optimal data compression methods to do optimal prefetching. Recent work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is synonymous with generalization and data compression.
Reference: [BoP] <author> R. Board and L. Pitt, </author> <title> "On the Necessity of Occam Algorithms," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computation (May 1990), </booktitle> <pages> 54-63. </pages>
Reference-contexts: Their evaluations of prefetching performance are empirical. In this paper, we give the first provable theoretical bounds on prefetching performance. Our novel approach is to use optimal data compression methods to do optimal prefetching. Recent work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is synonymous with generalization and data compression.
Reference: [BIR] <author> A. Borodin, S. Irani, P. Raghavan, and B. Schieber, </author> <title> "Competitive Paging with Locality of Reference," </title> <booktitle> Proceedings of the 23rd Annual ACM Symposium on Theory of Computation (May 1991). </booktitle> <pages> 19 </pages>
Reference-contexts: Competitive algorithms for caching are well examined in the literature <ref> [BIR, FKL, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetch-ing. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time.
Reference: [ClW] <author> J. G. Cleary and I. H. Witten, </author> <title> "Data Compression using Adaptive Coding and Partial String Matching," </title> <journal> IEEE Transactions on Communication 32 (April 1984), </journal> <pages> 396-402. </pages>
Reference-contexts: In practice, when such knowledge is available, we could combine our prefetcher with a logical prefetcher based on the semantics of the application, so as to get the best of both worlds. Similar techniques for caching appear in [FKL]. We expect that PPM-based algorithms (prediction by partial match) <ref> [ClW] </ref>, which are among the best text compressors in practice, will also perform well in practice for prefetching. PPM-based algorithms are practical implementations of mth order coding methods, and Theorem 3 indicates that performance should be good. Empirical work is progressing to compare PPM-based methods with the Ziv-Lempel methods.
Reference: [FKL] <author> A. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator, and N. E. Young, </author> <title> "Competitive Paging Algorithms," </title> <institution> Carnegie-Mellon University, CS-88-196, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Competitive algorithms for caching are well examined in the literature <ref> [BIR, FKL, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetch-ing. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> In practice, when such knowledge is available, we could combine our prefetcher with a logical prefetcher based on the semantics of the application, so as to get the best of both worlds. Similar techniques for caching appear in <ref> [FKL] </ref>. We expect that PPM-based algorithms (prediction by partial match) [ClW], which are among the best text compressors in practice, will also perform well in practice for prefetching. PPM-based algorithms are practical implementations of mth order coding methods, and Theorem 3 indicates that performance should be good.
Reference: [Gal] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: We denote the cache size by k and the total number of different pages (or alphabet size) by ff. The logarithm to the base two is denoted by "lg," the natural logarithm by "ln," and the empty string by . Definition 1 <ref> [Gal] </ref> We define a probabilistic finite state automaton (probabilistic FSA) as a quintuple (S; A; g; p; z 0 ), where S is a finite set of states with jSj = s, A is a finite alphabet of size ff, g is a deterministic "next state" function that maps S fi <p> Definition 2 <ref> [Gal] </ref> Let M be a Markov source. <p> Since we throw away our data structures at the end of each block, each of the b random variables Fault P;n , for 1 i b, depends only on the start state for each block, and our result follows by the ergodic theorem <ref> [Gal] </ref>. 2 The proof of Theorem 2 essentially deals with showing that F converges to F M for almost all as n ! 1.
Reference: [HoV] <author> P. G. Howard and J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," Proceedings of the 1991 IEEE Data Compression Conference (April 1991), </title> <type> invited paper. </type>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiL] hold without change for the character-based approach. Example 1 Assume for simplicity that our alphabet is fa; bg. We consider the page access sequence "aaaababaabbbabaa . . .". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa) . . .". <p> To identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 1 being used as a model by an arithmetic coder.
Reference: [KeS] <author> M. J. Kearns and R. E. Schapire, </author> <title> "Efficient Distribution-Free Learning of Probabilistic Concepts," </title> <booktitle> Proceedings of 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 382-391. </pages>
Reference-contexts: A harder model is to assume that the prefetcher is trained on one sufficiently long sequence generated by a source. For certain special cases of sources, like mth order Markov sources, we expect that the optimal prefetcher is PAC-learnable. An interesting related model is that of probabilistic concepts <ref> [KeS] </ref>.
Reference: [Lana] <author> G. G. Langdon, </author> <title> "A note on the Ziv-Lempel model for compressing individual sequences," </title> <journal> IEEE Transactions on Information Theory 29 (March 1983), </journal> <pages> 284-287. </pages>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [BCW, Lana] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in [ZiL] hold without change for the character-based approach. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiL] hold without change for the character-based approach. Example 1 Assume for simplicity that our alphabet is fa; bg. We consider the page access sequence "aaaababaabbbabaa . . .". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa) . . .".
Reference: [Lanb] <author> G. G. Langdon, </author> <title> "An Introduction to Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 28 (March </month> <year> 1984), </year> <pages> 135-149. </pages>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> To identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 1 being used as a model by an arithmetic coder.
Reference: [LeZ] <author> A. Lempel and J. Ziv, </author> <title> "On the Complexity of Finite Sequences," </title> <note> IEEE Transactions on Information Theory 22 (January 1976). </note>
Reference-contexts: It is shown in <ref> [LeZ] </ref> that 0 c ( n n lg ff ; lim * n = 0: (8) Theorem 4 is clearly true when c ( n 1 ) = o (n= lg n) since Compression E;n ( n 1 ) ~ 0 as n ! 1.
Reference: [McS] <author> L. A. McGeoch and D. D. Sleator, </author> <title> "A Strongly Competitive Randomized Paging Algorithm," </title> <institution> Carnegie-Mellon University, CS-89-122, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Competitive algorithms for caching are well examined in the literature <ref> [BIR, FKL, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetch-ing. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time.
Reference: [Nat] <author> S. Natarajan, </author> <title> "Large Deviations, Hypothesis Testing, and Source Coding for Finite Markov Sources ," IEEE Transactions on Information Theory 31 (1985), </title> <type> 360-365. </type>
Reference-contexts: The probability of sequences of length n for which X f z f z;i ln p z;i is exponentially small in n <ref> [Nat, Theorem 2] </ref>, for ffi &gt; 0. By Lemmas 5 and 6, we have E (Fault X;n ) F p with exponentially small probability. By the definition of fault rate and Lemma 4, it follows that E (Fault X;n ) = F M (n) E (Fault M 0 ;n ). <p> The prefetcher estimates the probability of each 17 transition to be the frequency that it is taken. These frequencies converge to the actual probabilities exponentially fast <ref> [Nat] </ref>. Since the state structure of the source is known, M is always in the same state that the source M is in. Let the algorithm M prefetch the pages with the top k estimated probabilities.
Reference: [PaZ] <author> M. Palmer and S. Zdonik, </author> <title> "Fido: A Cache that Learns to Fetch," </title> <booktitle> Proceedings of the 1991 International Conference on Very Large Databases (September 1991). </booktitle>
Reference-contexts: user's page accesses to detect if pages are ever accessed sequentially, in which case it prefetches the next page in sequence. 2 2 PAGE ACCESS MODELS AND MAIN RESULTS Independently to our approach, there has been recent work by Palmer and Zdonik, who use a neural network approach to prediction <ref> [PaZ] </ref>, and by Salem, who computes various first-order statistics for prediction [Sal]. Their evaluations of prefetching performance are empirical. In this paper, we give the first provable theoretical bounds on prefetching performance. Our novel approach is to use optimal data compression methods to do optimal prefetching.
Reference: [Sal] <author> K. Salem, </author> <title> "Adaptive Prefetching for Disk Buffers," </title> <type> CESDIS, </type> <institution> Goddard Space Flight Center, TR-91-64, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: in which case it prefetches the next page in sequence. 2 2 PAGE ACCESS MODELS AND MAIN RESULTS Independently to our approach, there has been recent work by Palmer and Zdonik, who use a neural network approach to prediction [PaZ], and by Salem, who computes various first-order statistics for prediction <ref> [Sal] </ref>. Their evaluations of prefetching performance are empirical. In this paper, we give the first provable theoretical bounds on prefetching performance. Our novel approach is to use optimal data compression methods to do optimal prefetching.
Reference: [SlT] <author> D. D. Sleator and R. E. Tarjan, </author> <title> "Amortized Efficiency of List Update and Paging Rules," </title> <booktitle> Communications of the ACM 28 (February 1985), </booktitle> <pages> 202-208. </pages>
Reference-contexts: An algorithm is online if it must make its decisions based only on the past history. An o*ine algorithm can use the knowledge of the future. Any implementable algorithm for caching or prefetching must clearly be online. The notion of competitiveness introduced by Sleator and Tarjan <ref> [SlT] </ref> determines the goodness of an online algorithm by comparing its performance to that of o*ine algorithms. <p> Competitive algorithms for caching are well examined in the literature <ref> [BIR, FKL, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetch-ing. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time.
Reference: [WNC] <author> I. H. Witten, R. M. Neal, and J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Communications of the ACM 30 (June 1987), </journal> <pages> 520-540. </pages>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> To identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 1 being used as a model by an arithmetic coder.
Reference: [ZiL] <author> J. Ziv and Abraham Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <journal> IEEE Transactions on Information Theory 24 (September 1978), </journal> <pages> 530-536. </pages>
Reference-contexts: Our models and results are summarized in the next section. In Section 3, for our main Markov source model we apply a character-by-character version of the Ziv-Lempel data compression algorithm <ref> [ZiL] </ref>. We assume that there is sufficient time to prefetch as many pages as wanted, limited only by the cache size k. In practice, prefetching requests would be interrupted by the user's actual read requests. <p> This minimum fault probability, weighted by the probability of being in state z, summed over all states z, gives us F M . This is formalized later in Definition 4. We adapt a character-by-character version of the Ziv-Lempel <ref> [ZiL] </ref> data compressor to get our optimal prefetcher P. Theorems 1 and 2 below are our main results. Theorem 1 Let M be a Markov source. The expected page fault rate achieved by P approaches F M , as the page access sequence length n ! 1. <p> The original Ziv-Lempel algorithm <ref> [ZiL] </ref> is a word-based data compression algorithm. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in <ref> [ZiL] </ref> hold without change for the character-based approach. Example 1 Assume for simplicity that our alphabet is fa; bg. We consider the page access sequence "aaaababaabbbabaa . . .". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa) . . .". <p> sequence n 1 and also to encode n 1 (via arithmetic coding), the average compression achieved is equal to the entropy of M ; that is, E (Compression M;n ) = H M (n): (2) The compression definitions in Definition 3 above are similar to those of Ziv and Lempel <ref> [ZiL] </ref>, except that they define M (s) to be a class of "information lossless" non-probabilistic FSA encoders, use in place of Compression, and use n lg ff in place of n in (1) to get a ratio of output length to input length. We generalize Ziv and Lempel's main result [ZiL] <p> <ref> [ZiL] </ref>, except that they define M (s) to be a class of "information lossless" non-probabilistic FSA encoders, use in place of Compression, and use n lg ff in place of n in (1) to get a ratio of output length to input length. We generalize Ziv and Lempel's main result [ZiL] to our model M (s) of probabilis tic FSAs, using an iterative analysis based on arithmetic coding, to get the following theorem: Theorem 4 The compression of E on n 1 is no worse than the best probabilistic FSA in the limit as n ! 1. <p> Proof of Theorem 4: It has been shown in <ref> [ZiL] </ref> that Compression E;n ( n 1 ) 1 ) + 1 lg (2ff (c ( n where c ( n 1 ) is the maximum number of nodes in any parse tree 1 for n 1 . <p> Corollary 2 Let M be a Markov source with s states. Then we have E Compression E;n s (n); lim ffi 0 1 This is not the definition of c ( n 1 ) in <ref> [ZiL] </ref> but it is easy to verify that the proofs in [ZiL] also hold under this definition of c ( n 1 ). 4.2 Bounds on Fault Rate 11 4.2 Bounds on Fault Rate Along the lines of entropy in Definition 2, we introduce the corresponding notion of the minimum expected <p> Corollary 2 Let M be a Markov source with s states. Then we have E Compression E;n s (n); lim ffi 0 1 This is not the definition of c ( n 1 ) in <ref> [ZiL] </ref> but it is easy to verify that the proofs in [ZiL] also hold under this definition of c ( n 1 ). 4.2 Bounds on Fault Rate 11 4.2 Bounds on Fault Rate Along the lines of entropy in Definition 2, we introduce the corresponding notion of the minimum expected fault rate F M of a Markov source M .
References-found: 22

