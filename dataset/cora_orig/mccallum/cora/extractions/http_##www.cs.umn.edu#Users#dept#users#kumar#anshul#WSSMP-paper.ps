URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/WSSMP-paper.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/
Root-URL: http://www.cs.umn.edu
Title: WSSMP: A High-Performance Serial and Parallel Symmetric Sparse Linear Solver  
Author: Anshul Gupta Mahesh Joshi and Vipin Kumar 
Address: P.O. Box 218, Yorktown Heights, NY 10598, U.S.A.  Minneapolis, MN 55455, U.S.A.  
Affiliation: IBM T.J. Watson Research Center  Department of Computer Science University of Minnesota,  
Abstract: The Watson Symmetric Sparse Matrix Package, WSSMP, is a high-performance, robust, and easy to use software package for solving large sparse symmetric systems of linear equations. It can can be used as a serial package, or in a shared-memory multiprocessor environment, or as a scalable parallel solver in a message-passing environment, where each node can either be a uniprocessor or a shared-memory multiprocessor. WSSMP uses scalable parallel multifrontal algorithms for sparse symmetric factorization and triangular solves. Sparse symmetric factorization in WSSMP has been clocked at up to 210 MFLOPS on an RS6000/590, 500 MFLOPS on an RS6000/397 and in excess of 20 GFLOPS on a 64-node SP with RS6000/397 nodes. This paper gives an overview of the algorithms, implementation aspects, performance results, and the user interface of WSSMP. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. Ashcraft and J. W.-H. Liu. </author> <title> Robust ordering of sparse matrices using mul-tisection. </title> <type> Technical Report CS 96-01, </type> <institution> Department of Computer Science, York University, </institution> <address> Ontario, Canada, </address> <year> 1996. </year>
Reference-contexts: In [5], we presented an optimally scalable parallel algorithm for factoring a large class of sparse symmetric matrices. This algorithm works efficiently only with graph-partitioning based ordering. Although, traditionally, local ordering heuristics have been preferred, recent research <ref> [8, 1, 6, 3] </ref> has shown the graph-partitioning based ordering heuristics can match and often exceed the fill-reduction of local heuristics. The ordering heuristics that WSSMP uses have been described in detail in [5].
Reference: 2. <author> E. Chu et al. </author> <title> Users guide for SPARSPAK-A: Waterloo sparse linear equations package. </title> <type> Technical Report CS-84-36, </type> <institution> University of Waterloo, Waterloo, IA, </institution> <year> 1984. </year>
Reference-contexts: Both C-style (indices starting from 0) and Fortran-style (indices starting from 1) numbering is sup ported. WSSMP supports two popular formats for the coefficient matrix: Com--pressed Sparse Row/Column (CSR/CSC) and Modified Sparse Rows/Column (MSR/MSC), which are described in detail in <ref> [2, 4] </ref>. All major functions of the package can be performed by calling a single subroutine.
Reference: 3. <author> A. Gupta. </author> <title> Fast and effective algorithms for graph partitioning and sparse matrix ordering. </title> <journal> IBM Journal of Research and Development, </journal> 41(1/2):171-183, 1997. 
Reference-contexts: In [5], we presented an optimally scalable parallel algorithm for factoring a large class of sparse symmetric matrices. This algorithm works efficiently only with graph-partitioning based ordering. Although, traditionally, local ordering heuristics have been preferred, recent research <ref> [8, 1, 6, 3] </ref> has shown the graph-partitioning based ordering heuristics can match and often exceed the fill-reduction of local heuristics. The ordering heuristics that WSSMP uses have been described in detail in [5].
Reference: 4. <author> A. Gupta, M. Joshi, and V. Kumar. WSSMP: </author> <title> Watson symmetric sparse matrix package: The user interface. </title> <type> RC 20923 (92669), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> July 17, </month> <year> 1997. </year>
Reference-contexts: Thus, depending on the configuration of the SP, the message-passing library PWSSMP is used in conjunction with either WSSMP or TWSSMP. In this section, we give a brief description of the main routines of the package. The details of the user interface can be found in <ref> [4] </ref>. WSSMP accepts as input a triangular portion of the sparse coefficient matrix. Both C-style (indices starting from 0) and Fortran-style (indices starting from 1) numbering is sup ported. <p> Both C-style (indices starting from 0) and Fortran-style (indices starting from 1) numbering is sup ported. WSSMP supports two popular formats for the coefficient matrix: Com--pressed Sparse Row/Column (CSR/CSC) and Modified Sparse Rows/Column (MSR/MSC), which are described in detail in <ref> [2, 4] </ref>. All major functions of the package can be performed by calling a single subroutine. <p> The total size N of system of equations is p1 i=0 N i , where p is the number of nodes being used. The columns of the coefficient matrix can be distributed among the nodes in any fashion (see <ref> [4] </ref> for details). N 0 = N and N i = 0 for i &gt; 0 is also an acceptable distribution.
Reference: 5. <author> A. Gupta, G. Karypis, and V. Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(5) </volume> <pages> 502-520, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Two main classes of successful heuristics have evolved over the years: (1) Local greedy heuristics, and (2) Global graph-partitioning based heuristics. In <ref> [5] </ref>, we presented an optimally scalable parallel algorithm for factoring a large class of sparse symmetric matrices. This algorithm works efficiently only with graph-partitioning based ordering. <p> Although, traditionally, local ordering heuristics have been preferred, recent research [8, 1, 6, 3] has shown the graph-partitioning based ordering heuristics can match and often exceed the fill-reduction of local heuristics. The ordering heuristics that WSSMP uses have been described in detail in <ref> [5] </ref>. Basically, the sparse matrix is regarded as the adjacency matrix of an undirected graph and a divide-and-conquer strategy (also known as nested dissection) is ap-plied recursively to label the vertices of the graph by partitioning it into smaller subgraphs. <p> In this phase, we first compute the elimination tree, then perform a quick symbolic step to predict the amount of potential numerical computation associated with each part of the elimination tree, and then adjust the elimination tree (as described in <ref> [5] </ref>) in order to balance the distribution of numerical factorization work among nodes. The elimination tree is also manipulated to reduce amount of stack memory required for factorization [10]. <p> We use a highly scalable parallel algorithm for this step, the detailed description and analysis of which can be found in <ref> [5] </ref>. The parallel numerical factorization in WSSMP is based on the multifrontal algorithm [10]. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. <p> The subgroup of nodes working on various subtrees are shown in the form of a logical mesh labeled with P. The frontal matrix of each supervertex is distributed among this logical mesh using a bit-mask based block-cyclic scheme <ref> [5] </ref>. Figure 1 (b) shows such a distribution for unit block size. This distribution ensures that the extend-add operations required by the multifrontal algorithm can be performed in parallel with each node exchanging roughly half of its data only with its partner from the other subcube.
Reference: 6. <author> B. Hendrickson and E. Rothberg. </author> <title> Improving the runtime and quality of nested dissection ordering. </title> <institution> SAND96-0868J, Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1996. </year>
Reference-contexts: In [5], we presented an optimally scalable parallel algorithm for factoring a large class of sparse symmetric matrices. This algorithm works efficiently only with graph-partitioning based ordering. Although, traditionally, local ordering heuristics have been preferred, recent research <ref> [8, 1, 6, 3] </ref> has shown the graph-partitioning based ordering heuristics can match and often exceed the fill-reduction of local heuristics. The ordering heuristics that WSSMP uses have been described in detail in [5].
Reference: 7. <author> M. Joshi, A. Gupta, G. Karypis, and V. Kumar. </author> <title> Two dimensional scalable parallel algorithms for solution of triangular systems. </title> <type> TR 97-024, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Because of the bit-mask based block-cyclic distribution, this can be done by using at most two communication steps <ref> [7] </ref>. The details of the two-dimensional pipelined dense forward elimination algorithm are illustrated in Figure 2 (b) for a hypothetical supervertex. The solutions are computed by the nodes owning diagonal elements of L matrix and flow down along a column.
Reference: 8. <author> G. Karypis and V. Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> TR 95-035, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: In [5], we presented an optimally scalable parallel algorithm for factoring a large class of sparse symmetric matrices. This algorithm works efficiently only with graph-partitioning based ordering. Although, traditionally, local ordering heuristics have been preferred, recent research <ref> [8, 1, 6, 3] </ref> has shown the graph-partitioning based ordering heuristics can match and often exceed the fill-reduction of local heuristics. The ordering heuristics that WSSMP uses have been described in detail in [5].
Reference: 9. <author> G. Karypis and V. Kumar. Parmetis: </author> <title> Parallel graph partitioning and sparse matrix ordering library. </title> <type> TR 97-060, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1997. </year>
Reference-contexts: The vertices of the separator are numbered after the vertices in the subgraphs are numbered by following the same strategy recursively. There are two main approaches to parallelizing this algorithm. In the first approach, the process of finding the separator is performed in parallel <ref> [9] </ref>. The advantages of this approach are that a reasonably good speedup on ordering can be obtained and the graphs (both original and the subsequent subgraphs) are stored on multiple nodes, thereby avoiding excessive memory use on any single node in a distributed-memory environment.
Reference: 10. <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice. SIAM Review, </journal> <volume> 34 </volume> <pages> 82-109, </pages> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The elimination tree is also manipulated to reduce amount of stack memory required for factorization <ref> [10] </ref>. If the parallel machine has heterogeneous nodes in terms of processing power or the amount of memory, the symbolic phase takes that into account while assigning tasks to nodes. <p> We use a highly scalable parallel algorithm for this step, the detailed description and analysis of which can be found in [5]. The parallel numerical factorization in WSSMP is based on the multifrontal algorithm <ref> [10] </ref>. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. The algorithm performs a postorder traversal of the elimination tree associated with A.
References-found: 10

