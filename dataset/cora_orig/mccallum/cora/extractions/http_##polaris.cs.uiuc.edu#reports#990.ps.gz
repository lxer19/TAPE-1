URL: http://polaris.cs.uiuc.edu/reports/990.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Improving Memory Utilization in Cache Coherence Directories  
Author: David J. Lilja Pen-Chung Yew 
Keyword: cache coherence; compiler optimization; directory; dynamic pointer allocation; memory overhead; shared memory multiprocessor; version control.  
Note: (to appear in) IEEE Transactions on Parallel and Distributed Systems  
Address: 200 Union Street S.E. Minneapolis, MN 55455  Street Urbana, Illinois 61801  
Affiliation: Department of Electrical Engineering University of Minnesota  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign 305 Talbot Laboratory 104 South Wright  
Email: lilja@ee.umn.edu  
Phone: (612) 625-5007  
Date: August 13, 1991 Revised: December 18, 1991 Revised: March 4, 1992  
Abstract: Efficiently maintaining cache coherence is a major problem in large-scale shared memory multiprocessors. Hardware directory coherence schemes have very high memory requirements, while software-directed schemes must rely on imprecise compile-time memory disambiguation. Recently proposed dynamically tagged directory schemes allocate pointers to blocks only as they are referenced, which significantly reduces their memory requirements, but they still allocate pointers to blocks that do not need them. We present two compiler optimizations that exploit the high-level sharing information available to the compiler to further reduce the size of a tagged directory by allocating pointers only when necessary. Trace-driven simulations are used to show that the performance of this combined hardware-software approach is comparable to other coherence schemes, but with significantly lower memory requirements. In addition, these simulations suggest that this approach is less sensitive to the quality of the memory disambiguation and interprocedural analysis performed by the compiler than software-only coherence schemes. This work was supported by the National Science Foundation under Grant No. NSF MIP-8410110, with additional support from NASA Ames Research Center Grant No. NASA NCC 2-559 (DARPA), National Science Foundation Grant No. NSF MIP-88-07775, and Department of Energy Grant No. DOE DE-FG02-85ER25001. David Lilja also was supported by a DARPA/NASA Assistantship in Parallel Processing. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Sarita V. Adve, Vikram S. Adve, Mark D. Hill, and Mary K. Vernon, </author> <title> ``Comparison of Hardware and Software Cache Coherence Schemes,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 298-308, </pages> <year> 1991. </year>
Reference-contexts: The primary advantage of the hardware coherence schemes is their perfect memory disambiguation. By tracking the actual memory addresses, these schemes can invalidate only those blocks that are actually stale, assuming that they save enough state information. Several studies <ref> [1, 26, 29] </ref> have indicated that, due to its reduced network traffic, an ideal implementation of a compiler-based coherence scheme with perfect memory disambiguation can have performance comparable to, and in some cases better than, that of a full hardware directory.
Reference: 2. <author> Anant Agarwal and Anoop Gupta, </author> <title> ``Memory-Reference Characteristics of Multiprocessor Applications Under MACH,'' </title> <booktitle> ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 215-225, </pages> <year> 1988. </year>
Reference-contexts: References to private blocks can never cause coherence problems since these blocks are referenced by only a single processor. Because shared read-only blocks are never written, they also can never become incoherent. Several studies <ref> [2, 5, 13, 25, 36] </ref> have shown that from 30 to more than 90 percent of all blocks referenced by a program may be private or read-only, and thus would not need to be allocated pointers. Examples of read-only data blocks include all instruction blocks, preinitialized lookup tables, and constants.
Reference: 3. <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz, </author> <title> ``An Evaluation of Directory Schemes for Cache Coherence,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 280-289, </pages> <year> 1988. </year>
Reference-contexts: Replacing the bus with a multistage interconnection network [16, 23, 32] reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium. Hardware coherence schemes that dynamically determine which memory operations need coherence actions <ref> [3, 4, 7, 19] </ref> have access to memory addresses only as the program generates them. Since it is impossible for the hardware to predict how the blocks will be shared, they must track the state and sharing characteristics of every memory block referenced by the program. <p> In addition, unlike the compiler-based schemes, the hardware schemes make coherence enforcement completely transparent to procedure calls and subroutines. These differences between compiler-based and hardware directory coherence schemes are summarized in Table 1. In the conventional hardware directories <ref> [3, 4, 7, 19] </ref>, pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. <p> Section 4 examines the ideal performance improvement possible with these optimizations, as well as the potential degradation due to imprecise memory disambiguation and poor interprocedural analysis. Section 5 compares the performance of this compiler-assisted directory with the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref> and with the software-directed version control scheme [12]. The results and conclusions are summarized in the last section. 2. Compiler Assistance for Coherence Directories The two compiler optimizations presented in this section can be used to reduce the directory size needed to maintain coherence. <p> Table 2 shows how the processor interacts with its data cache and the memory when using this block partitioning compiler optimization. 2.2. Delayed Allocation Marking In the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref>, memory bits needed to point to processors with a cached copy of a block are statically bound to each block in memory when the machine is built so that the total number of pointer bits, and thus the size of the directory, is proportional to the size <p> These two schemes differ primarily in the organization of the pointer cache. Each pointer in the tag cache scheme consists of an address tag plus n processor pointers, as in the n-pointer scheme <ref> [3] </ref>. For blocks that are simultaneously shared by more than n processors, and thus need more than n pointers, there is a second-level tag cache that uses the p+1 bit pointers of the full distributed directory. <p> Using the same trace-driven simulation methodology, the performance and the memory requirements of the pointer cache are compared with several hardware directory schemes <ref> [3, 4, 7, 19] </ref>, and with the software-directed version control scheme [12] in the same shared memory multiprocessor. These different schemes represent a wide range of performance and memory overhead trade-offs and provide a solid basis for comparing the proposed approach to existing coherence schemes. <p> These additional messages also will increase the network congestion, further increasing the invalidation time. The n-pointers plus broadcast scheme <ref> [3] </ref> compromises between the number of pointers and the need to broadcast. In this scheme, n pointers are associated with each block in memory to point to the first n processors that request a copy of the block.
Reference: 4. <author> James Archibald and Jean-Loup Baer, </author> <title> ``An Economical Solution to the Cache Coherence Problem,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 355-362, </pages> <year> 1984. </year>
Reference-contexts: Replacing the bus with a multistage interconnection network [16, 23, 32] reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium. Hardware coherence schemes that dynamically determine which memory operations need coherence actions <ref> [3, 4, 7, 19] </ref> have access to memory addresses only as the program generates them. Since it is impossible for the hardware to predict how the blocks will be shared, they must track the state and sharing characteristics of every memory block referenced by the program. <p> In addition, unlike the compiler-based schemes, the hardware schemes make coherence enforcement completely transparent to procedure calls and subroutines. These differences between compiler-based and hardware directory coherence schemes are summarized in Table 1. In the conventional hardware directories <ref> [3, 4, 7, 19] </ref>, pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. <p> Section 4 examines the ideal performance improvement possible with these optimizations, as well as the potential degradation due to imprecise memory disambiguation and poor interprocedural analysis. Section 5 compares the performance of this compiler-assisted directory with the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref> and with the software-directed version control scheme [12]. The results and conclusions are summarized in the last section. 2. Compiler Assistance for Coherence Directories The two compiler optimizations presented in this section can be used to reduce the directory size needed to maintain coherence. <p> Table 2 shows how the processor interacts with its data cache and the memory when using this block partitioning compiler optimization. 2.2. Delayed Allocation Marking In the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref>, memory bits needed to point to processors with a cached copy of a block are statically bound to each block in memory when the machine is built so that the total number of pointer bits, and thus the size of the directory, is proportional to the size <p> Using the same trace-driven simulation methodology, the performance and the memory requirements of the pointer cache are compared with several hardware directory schemes <ref> [3, 4, 7, 19] </ref>, and with the software-directed version control scheme [12] in the same shared memory multiprocessor. These different schemes represent a wide range of performance and memory overhead trade-offs and provide a solid basis for comparing the proposed approach to existing coherence schemes. <p> Since the directory has exact information about which processors have a copy of which blocks, the number of invalidation messages required with this directory is the same as the number of cached copies of a block. To reduce the memory requirements, the broadcast directory <ref> [4] </ref> maintains only two state bits for each block in the memories and the caches. This approach significantly reduces the number of bits needed for coherence, but it requires the directory to broadcast all of its invalidation messages to all of the processors.
Reference: 5. <author> Sandra Johnson Baylor and Bharat Deep Rathi, </author> <title> ``A Study of the Memory Reference Behavior of Engineering/Scientific Applications in Parallel Processors,'' </title> <booktitle> International Conference on 26 Parallel Processing, </booktitle> <volume> Vol. I: Architecture, </volume> <pages> pp. 78-82, </pages> <year> 1989. </year>
Reference-contexts: References to private blocks can never cause coherence problems since these blocks are referenced by only a single processor. Because shared read-only blocks are never written, they also can never become incoherent. Several studies <ref> [2, 5, 13, 25, 36] </ref> have shown that from 30 to more than 90 percent of all blocks referenced by a program may be private or read-only, and thus would not need to be allocated pointers. Examples of read-only data blocks include all instruction blocks, preinitialized lookup tables, and constants.
Reference: 6. <author> M. Berry, D. Chen, P. Koss, D. Kuck, and S. Lo, </author> <title> ``The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers,'' </title> <institution> University of Illinois, </institution> <type> CSRD Report No. 827, </type> <institution> Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Test Programs The test programs used in this performance evaluation, shown in Table 5, have a total of more than 45 million memory references and have significant differences in block sharing characteristics. The arc3d, flo52, and trfd programs are taken from the Perfect benchmark suite <ref> [6] </ref> with slight changes in the problem size and loop iteration counts. Arc3d analyzes a three-dimensional fluid flow and flo52 analyzes the transonic flow past an airfoil. The trfd program is a quantum mechanical simulation of a two-electron integral transformation that uses a series of matrix multiplications.
Reference: 7. <author> Lucien M. Censier and Paul Feautrier, </author> <title> ``A New Solution to Coherence Problems in Multicache Systems,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-27, No. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Replacing the bus with a multistage interconnection network [16, 23, 32] reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium. Hardware coherence schemes that dynamically determine which memory operations need coherence actions <ref> [3, 4, 7, 19] </ref> have access to memory addresses only as the program generates them. Since it is impossible for the hardware to predict how the blocks will be shared, they must track the state and sharing characteristics of every memory block referenced by the program. <p> In addition, unlike the compiler-based schemes, the hardware schemes make coherence enforcement completely transparent to procedure calls and subroutines. These differences between compiler-based and hardware directory coherence schemes are summarized in Table 1. In the conventional hardware directories <ref> [3, 4, 7, 19] </ref>, pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. <p> Section 4 examines the ideal performance improvement possible with these optimizations, as well as the potential degradation due to imprecise memory disambiguation and poor interprocedural analysis. Section 5 compares the performance of this compiler-assisted directory with the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref> and with the software-directed version control scheme [12]. The results and conclusions are summarized in the last section. 2. Compiler Assistance for Coherence Directories The two compiler optimizations presented in this section can be used to reduce the directory size needed to maintain coherence. <p> Table 2 shows how the processor interacts with its data cache and the memory when using this block partitioning compiler optimization. 2.2. Delayed Allocation Marking In the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref>, memory bits needed to point to processors with a cached copy of a block are statically bound to each block in memory when the machine is built so that the total number of pointer bits, and thus the size of the directory, is proportional to the size <p> Using the same trace-driven simulation methodology, the performance and the memory requirements of the pointer cache are compared with several hardware directory schemes <ref> [3, 4, 7, 19] </ref>, and with the software-directed version control scheme [12] in the same shared memory multiprocessor. These different schemes represent a wide range of performance and memory overhead trade-offs and provide a solid basis for comparing the proposed approach to existing coherence schemes. <p> More Memory Delay Models All of the traditional directories use essentially the same protocol to maintain coherence so that the primary performance difference between the schemes is the time required to perform the invalidations. The p+1-bit full distributed directory <ref> [7] </ref> maintains in each memory module p valid bits and a single exclusive bit for each block in the module, where p is the number of processors.
Reference: 8. <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal, </author> <title> ``Directory-Based Cache Coherence in Large-Scale Multiprocessors,'' </title> <journal> Computer, </journal> <volume> Vol. 23, No. 6, </volume> <pages> pp. 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: To compare the potential range of performance of the linked list schemes, however, these simulations assume that invalidations are propagated from both ends in the doubly-linked directory, and from one end in the singly-linked directory <ref> [8] </ref>. In the version control software-directed scheme, T miss cycles are required to service both read and write misses.
Reference: 9. <author> David Chaiken, John Kubiatowicz, and Anant Agarwal, </author> <title> ``LimitLESS Directories: A Scalable Cache Coherence Scheme,'' </title> <booktitle> International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 224-234, </pages> <year> 1991. </year>
Reference-contexts: In the conventional hardware directories [3, 4, 7, 19], pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. Recently proposed dynamically tagged directories <ref> [9, 17, 26, 30] </ref> take advantage of the observation that only blocks that are actually cached in one or more processors need to be allocated pointers. <p> The second compiler optimization, delayed allocation marking, is a significant extension of this idea of combining hardware and software coherence enforcement. It uses the predictive power of the compiler to delay the allocation of a coherence pointer as long as possible. This optimization requires a dynamically tagged directory <ref> [9, 17, 26, 30] </ref>, but by delaying the allocation of pointers, they are in use for a shorter period of time, and thus can be reused more frequently. <p> In the tagged directories <ref> [9, 17, 26, 30] </ref>, however, pointers are dynamically bound to the blocks as they are referenced using an address tag within the pointer. As a result, the size of the tagged directory is proportional to the data cache size. <p> Again, this bit count ignores the cost of the associative matching logic. The LimitLESS directory implemented for the Alewife machine <ref> [9] </ref> uses a structure similar to the tag cache for the first level of pointers, but it generates a processor interrupt when an entry needs more than n pointers. An interrupt service routine then emulates the function of a p+1-bit full directory to maintain the complete sharing information. <p> The pointer cache coherence scheme [26] is used as an example of a tagged directory, although the compiler optimizations apply equally well to the other tagged directories <ref> [9, 17, 30] </ref>. The Alliant compiler is used to automatically generate parallel assembly code from Fortran source code, which then is converted into equivalent assembly code for a multiprocessor emulator. <p> Block partitioning can be used with these traditional directories, however, and this optimization is used in the following simulations. Since the version control scheme updates version numbers only for blocks that are actually written, the compiler marking algorithms do not apply to this scheme. The other dynamically tagged directories <ref> [9, 17, 30] </ref> also could benefit from the compiler marking strategies. They differ from the pointer cache scheme [26] primarily in the organization of the pointers in the directory.
Reference: 10. <author> Hoichi Cheong and Alexander V. Veidenbaum, </author> <title> ``A Cache Coherence Scheme with Fast Selective Invalidation,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 299-307, </pages> <year> 1988. </year>
Reference-contexts: The number of memory bits needed to store this information can be enormous for these schemes. The compiler-directed schemes <ref> [10, 12, 24, 28, 35] </ref>, on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10]. <p> The compiler-directed schemes [10, 12, 24, 28, 35], on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme <ref> [10] </ref>. While the version control scheme [12] and the time-stamp scheme [28] require more local memory in each processor to hold the version or time-stamp numbers, this storage still can be significantly smaller than that required by the dynamic hardware-only schemes.
Reference: 11. <author> Hoichi Cheong and Alexander V. Veidenbaum, </author> <title> ``Stale Data Detection and Coherence Enforcement Using Flow Analysis,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol. I: Architecture, </volume> <pages> pp. 138-145, </pages> <year> 1988. </year>
Reference-contexts: With minor adjustments, this marking optimization can be readily adapted to other parallelism models and synchronization schemes. With this model, a block B can become stale within processor P i 's cache with the following sequence of events <ref> [11] </ref> E 0 : Processor P i reads block B zero or more times. E 1 : Processor P i reads block B. E 2 : A processor reassignment occurs (synchronization point).
Reference: 12. <author> Hoichi Cheong and Alexander Veidenbaum, </author> <title> ``A Version Control Approach to Cache Coherence,'' </title> <booktitle> ACM International Conference on Supercomputing, </booktitle> <pages> pp. 322-330, </pages> <year> 1989. </year>
Reference-contexts: The number of memory bits needed to store this information can be enormous for these schemes. The compiler-directed schemes <ref> [10, 12, 24, 28, 35] </ref>, on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10]. <p> The compiler-directed schemes [10, 12, 24, 28, 35], on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10]. While the version control scheme <ref> [12] </ref> and the time-stamp scheme [28] require more local memory in each processor to hold the version or time-stamp numbers, this storage still can be significantly smaller than that required by the dynamic hardware-only schemes. <p> Section 5 compares the performance of this compiler-assisted directory with the traditional hardware directory schemes [3, 4, 7, 19] and with the software-directed version control scheme <ref> [12] </ref>. The results and conclusions are summarized in the last section. 2. Compiler Assistance for Coherence Directories The two compiler optimizations presented in this section can be used to reduce the directory size needed to maintain coherence. The block partitioning optimization can be used with any hardware directory. <p> Using the same trace-driven simulation methodology, the performance and the memory requirements of the pointer cache are compared with several hardware directory schemes [3, 4, 7, 19], and with the software-directed version control scheme <ref> [12] </ref> in the same shared memory multiprocessor. These different schemes represent a wide range of performance and memory overhead trade-offs and provide a solid basis for comparing the proposed approach to existing coherence schemes.
Reference: 13. <author> Susan J. Eggers and Randy H. Katz, </author> <title> ``A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 373-382, </pages> <year> 1988. </year>
Reference-contexts: References to private blocks can never cause coherence problems since these blocks are referenced by only a single processor. Because shared read-only blocks are never written, they also can never become incoherent. Several studies <ref> [2, 5, 13, 25, 36] </ref> have shown that from 30 to more than 90 percent of all blocks referenced by a program may be private or read-only, and thus would not need to be allocated pointers. Examples of read-only data blocks include all instruction blocks, preinitialized lookup tables, and constants.
Reference: 14. <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua, </author> <title> ``Cedar Fortran and Its Compiler,'' </title> <institution> University of Illinois, </institution> <type> CSRD Report No. 966, </type> <institution> Urbana, IL, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Using symbolic data dependence analysis, a restructuring compiler such as Cedar/KAP <ref> [14] </ref> can classify the blocks into one of these three types (ie. shared-writable, private, or shared read-only) based on the sharing patterns it detects. All private and shared read-only block types then are marked safe_blk to indicate that the directory can be completely bypassed when these blocks are referenced.
Reference: 15. <author> James R. Goodman, </author> <title> ``Using Cache Memory to Reduce Processor-Memory Traffic,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-131, </pages> <year> 1983. </year>
Reference-contexts: 1. Introduction Private data caches in shared memory multiprocessors can significantly reduce the average time to access global memory, but they also introduce a consistency problem since multiple copies of a memory location can be resident in several different caches simultaneously. Snoopy coherence schemes <ref> [15, 20, 31] </ref> can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck.
Reference: 16. <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry Rudolph, and Marc Snir, </author> <title> ``The NYU Ultracomputer -- Designing a MIMD, Shared-Memory Parallel Machine,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 27-42, </pages> <year> 1982. </year>
Reference-contexts: Snoopy coherence schemes [15, 20, 31] can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck. Replacing the bus with a multistage interconnection network <ref> [16, 23, 32] </ref> reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium.
Reference: 17. <author> Anoop Gupta, Wolf-Dietrich Weber, and Todd Mowry, </author> <title> ``Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol. I: Architecture, </volume> <pages> pp. 312-321, </pages> <year> 1990. </year>
Reference-contexts: In the conventional hardware directories [3, 4, 7, 19], pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. Recently proposed dynamically tagged directories <ref> [9, 17, 26, 30] </ref> take advantage of the observation that only blocks that are actually cached in one or more processors need to be allocated pointers. <p> The second compiler optimization, delayed allocation marking, is a significant extension of this idea of combining hardware and software coherence enforcement. It uses the predictive power of the compiler to delay the allocation of a coherence pointer as long as possible. This optimization requires a dynamically tagged directory <ref> [9, 17, 26, 30] </ref>, but by delaying the allocation of pointers, they are in use for a shorter period of time, and thus can be reused more frequently. <p> In the tagged directories <ref> [9, 17, 26, 30] </ref>, however, pointers are dynamically bound to the blocks as they are referenced using an address tag within the pointer. As a result, the size of the tagged directory is proportional to the data cache size. <p> This approach handles the common case directly in hardware, while using the flexibility of the software interrupt to handle the more infrequent overflow case. The cost of this scheme is similar to the other tagged pointer schemes. The coarse vector tagged directory <ref> [17] </ref> dynamically allocates pointers consisting of log 2 m address tag bits, a valid bit, a dirty bit, v pointer bits, and a mode bit. <p> The pointer cache coherence scheme [26] is used as an example of a tagged directory, although the compiler optimizations apply equally well to the other tagged directories <ref> [9, 17, 30] </ref>. The Alliant compiler is used to automatically generate parallel assembly code from Fortran source code, which then is converted into equivalent assembly code for a multiprocessor emulator. <p> Block partitioning can be used with these traditional directories, however, and this optimization is used in the following simulations. Since the version control scheme updates version numbers only for blocks that are actually written, the compiler marking algorithms do not apply to this scheme. The other dynamically tagged directories <ref> [9, 17, 30] </ref> also could benefit from the compiler marking strategies. They differ from the pointer cache scheme [26] primarily in the organization of the pointers in the directory.
Reference: 18. <author> M. D. Guzzi, D. A. Padua, J. P. Hoeflinger, and D. H. Lawrie, </author> <title> ``Cedar Fortran and Other Vector and Parallel Fortran Dialects,'' </title> <journal> Journal of Supercomputing, </journal> <volume> Vol. 4, No. 1, </volume> <pages> pp. 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In addition, if the compiler preschedules the execution of specific loop iterations onto specific processors, it may be able to identify blocks that are accessed by only a single processor throughout the execution of the program and mark them as private. Compiler optimizations and transformations <ref> [18] </ref> may be able to further reduce the number of shared-writable blocks.
Reference: 19. <author> David V. James, Anthony T. Laundrie, Stein Gjessing, and Gurindar S. Sohi, </author> <title> ``Scalable Coherent Interface,'' </title> <journal> Computer, </journal> <volume> Vol. 23, No. 6, </volume> <pages> pp. 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Replacing the bus with a multistage interconnection network [16, 23, 32] reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium. Hardware coherence schemes that dynamically determine which memory operations need coherence actions <ref> [3, 4, 7, 19] </ref> have access to memory addresses only as the program generates them. Since it is impossible for the hardware to predict how the blocks will be shared, they must track the state and sharing characteristics of every memory block referenced by the program. <p> In addition, unlike the compiler-based schemes, the hardware schemes make coherence enforcement completely transparent to procedure calls and subroutines. These differences between compiler-based and hardware directory coherence schemes are summarized in Table 1. In the conventional hardware directories <ref> [3, 4, 7, 19] </ref>, pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. <p> Section 4 examines the ideal performance improvement possible with these optimizations, as well as the potential degradation due to imprecise memory disambiguation and poor interprocedural analysis. Section 5 compares the performance of this compiler-assisted directory with the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref> and with the software-directed version control scheme [12]. The results and conclusions are summarized in the last section. 2. Compiler Assistance for Coherence Directories The two compiler optimizations presented in this section can be used to reduce the directory size needed to maintain coherence. <p> Table 2 shows how the processor interacts with its data cache and the memory when using this block partitioning compiler optimization. 2.2. Delayed Allocation Marking In the traditional hardware directory schemes <ref> [3, 4, 7, 19] </ref>, memory bits needed to point to processors with a cached copy of a block are statically bound to each block in memory when the machine is built so that the total number of pointer bits, and thus the size of the directory, is proportional to the size <p> Using the same trace-driven simulation methodology, the performance and the memory requirements of the pointer cache are compared with several hardware directory schemes <ref> [3, 4, 7, 19] </ref>, and with the software-directed version control scheme [12] in the same shared memory multiprocessor. These different schemes represent a wide range of performance and memory overhead trade-offs and provide a solid basis for comparing the proposed approach to existing coherence schemes. <p> In this scheme, n pointers are associated with each block in memory to point to the first n processors that request a copy of the block. If more than n processors need a block, the scheme resorts to broadcasting. Another scheme <ref> [19] </ref> reduces the size of the directory without requiring broadcasts by maintaining a linked list from the directory to each of the processors having a copy of a block. When an invalidation message is sent from the memory, every node on the list must be visited sequentially. <p> Invalidations still may be propagated from only one end of the list due to the potential race condition if they are simultaneously propagated from both ends <ref> [19] </ref>. To compare the potential range of performance of the linked list schemes, however, these simulations assume that invalidations are propagated from both ends in the doubly-linked directory, and from one end in the singly-linked directory [8].
Reference: 20. <author> R. Katz, S. Eggers, D. A. Wood, C. Perkins, and R. G. Sheldon, </author> <title> ``Implementing a Cache Consistency Protocol,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-283, 27 </pages>
Reference-contexts: 1. Introduction Private data caches in shared memory multiprocessors can significantly reduce the average time to access global memory, but they also introduce a consistency problem since multiple copies of a memory location can be resident in several different caches simultaneously. Snoopy coherence schemes <ref> [15, 20, 31] </ref> can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck.
Reference: 21. <author> David Kroft, </author> <title> ``Lockup-Free Instruction Fetch/Prefetch Cache Organization,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <year> 1981. </year>
Reference-contexts: The processors used in these simulations can have multiple outstanding memory requests so that some of the memory delay can be overlapped with other processing. This type of processor requires the use of lockup-free data caches <ref> [21, 33, 34] </ref> which could increase the complexity of the processors and potentially increase contention in the interconnection network. However, to directly compare the memory performance of the different coherence schemes, the average memory delay is used as the figure of merit.
Reference: 22. <author> Clyde P. Kruskal and Marc Snir, </author> <title> ``The Performance of Multistage Interconnection Networks for Multiprocessors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-32, No. 12, </volume> <pages> pp. 1091-1098, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: The processor-memory interconnection network consists of log j p switch stages, where j is the number of inputs and outputs in each switch. Since 2-by-2 switches are used in this system, j=2. Network delays are modeled <ref> [22] </ref> as T net =log 2 p (1+f contention ) cycles, where f contention =[m 2 (1-1/j)l] /[2 (1-ml)], l is the probability that a packet arrives at each input to the network during each cycle, and m is the number of packets per message.
Reference: 23. <author> David J. Kuck, Edward S. Davidson, Duncan J. Lawrie, and Ahmed H. Sameh, </author> <title> ``Parallel Supercomputing Today and the Cedar Approach,'' </title> <journal> Science, </journal> <volume> Vol. 231, </volume> <pages> pp. 967-974, </pages> <month> 28 February </month> <year> 1986. </year>
Reference-contexts: Snoopy coherence schemes [15, 20, 31] can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck. Replacing the bus with a multistage interconnection network <ref> [16, 23, 32] </ref> reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium. <p> This section discusses the hardware required to support each compiler optimization. 2.3.1. Hardware support for block partitioning When using block partitioning, the processors' data cache controllers and the directories must be able to distinguish blocks marked safe_blk from those marked coh_blk. As in the Cedar system <ref> [23] </ref>, a high-order address bit can divide the address space into two portions to distinguish blocks that need a pointer from those that do not. Alternatively, different segments can be allocated for the two block types, or page table entries may be individually marked.
Reference: 24. <author> Roland L. Lee, Pen-Chung Yew, and Duncan J. Lawrie, </author> <title> ``Multiprocessor Cache Design Considerations,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 253-262, </pages> <year> 1987. </year>
Reference-contexts: The number of memory bits needed to store this information can be enormous for these schemes. The compiler-directed schemes <ref> [10, 12, 24, 28, 35] </ref>, on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10].
Reference: 25. <author> David J. Lilja, David M. Marcovitz, and Pen-Chung Yew, </author> <title> ``Memory Referencing Behavior and a Cache Performance Metric in a Shared Memory Multiprocessor,'' Center for Supercomputing Research and Development Report No. </title> <type> 836, </type> <institution> University of Illinois, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: References to private blocks can never cause coherence problems since these blocks are referenced by only a single processor. Because shared read-only blocks are never written, they also can never become incoherent. Several studies <ref> [2, 5, 13, 25, 36] </ref> have shown that from 30 to more than 90 percent of all blocks referenced by a program may be private or read-only, and thus would not need to be allocated pointers. Examples of read-only data blocks include all instruction blocks, preinitialized lookup tables, and constants.
Reference: 26. <author> David J. Lilja, </author> <title> ``Processor Parallelism Considerations and Memory Latency Reduction in Shared Memory Multiprocessors,'' Center for Supercomputing Research and Development Report No. </title> <type> 1136, </type> <institution> University of Illinois (Ph.D. </institution> <type> Thesis), </type> <institution> Urbana, </institution> <year> 1991. </year>
Reference-contexts: The primary advantage of the hardware coherence schemes is their perfect memory disambiguation. By tracking the actual memory addresses, these schemes can invalidate only those blocks that are actually stale, assuming that they save enough state information. Several studies <ref> [1, 26, 29] </ref> have indicated that, due to its reduced network traffic, an ideal implementation of a compiler-based coherence scheme with perfect memory disambiguation can have performance comparable to, and in some cases better than, that of a full hardware directory. <p> In the conventional hardware directories [3, 4, 7, 19], pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. Recently proposed dynamically tagged directories <ref> [9, 17, 26, 30] </ref> take advantage of the observation that only blocks that are actually cached in one or more processors need to be allocated pointers. <p> The second compiler optimization, delayed allocation marking, is a significant extension of this idea of combining hardware and software coherence enforcement. It uses the predictive power of the compiler to delay the allocation of a coherence pointer as long as possible. This optimization requires a dynamically tagged directory <ref> [9, 17, 26, 30] </ref>, but by delaying the allocation of pointers, they are in use for a shorter period of time, and thus can be reused more frequently. <p> In the tagged directories <ref> [9, 17, 26, 30] </ref>, however, pointers are dynamically bound to the blocks as they are referenced using an address tag within the pointer. As a result, the size of the tagged directory is proportional to the data cache size. <p> There are several variations of these tagged directories, any one of which can be used with this compiler optimization. Four of these directories are described below to give an idea of their complexity. The pointer cache tagged directory <ref> [26] </ref> allocates to each memory block exactly the number of pointers it needs from a cache of pointers in each memory module. <p> With this pipelining, associative pointer matching can be overlapped with other required operations, such 8 as the sending of invalidation messages. A fully associative memory is expensive to implement, but, due to the sharing characteristics of many programs, a set associative implementation still can offer good performance <ref> [26] </ref>. If there are more requests for pointers than there are entries in the pointer cache, a free pointer entry is created by evicting an active pointer from the pointer cache. <p> In all of these simulations, it is assumed that no context switching or task migration takes place. 3.1. Machine Model These simulations use a shared memory multiprocessor with p=32 processors connected to an equal number of memory modules through a multistage interconnection network. The pointer cache coherence scheme <ref> [26] </ref> is used as an example of a tagged directory, although the compiler optimizations apply equally well to the other tagged directories [9, 17, 30]. <p> Since the version control scheme updates version numbers only for blocks that are actually written, the compiler marking algorithms do not apply to this scheme. The other dynamically tagged directories [9, 17, 30] also could benefit from the compiler marking strategies. They differ from the pointer cache scheme <ref> [26] </ref> primarily in the organization of the pointers in the directory. These other schemes are not included in this study since all of these tagged directories should have approximately the same performance with roughly the same memory requirements when using compiler marking. 5.1. <p> The word size is w =32 bits with p=32 processors, and the data cache block size is b=1 word. We find similar results to those reported below when the block size is four words or fewer, and that the performance seriously degrades for larger block sizes <ref> [26] </ref>. Typical cache memory sizes are in the range of 64K words to 256K words, and a typical memory module may contain from 2M words to 16M words.
Reference: 27. <author> David J. Lilja and Pen-Chung Yew, </author> <title> ``Combining Hardware and Software Cache Coherence Strategies,'' </title> <booktitle> ACM International Conference on Supercomputing, </booktitle> <pages> pp. 274-283, </pages> <year> 1991. </year>
Reference-contexts: Acknowledgements Portions of this work were presented at the ACM International Conference on Supercomputing in Cologne, Germany in June 1991 <ref> [27] </ref>. The authors wish to thank Hoichi Cheong and the anonymous reviewers for their helpful comments and suggestions.
Reference: 28. <author> Sang Lyul Min and Jean-Loup Baer, </author> <title> ``A Timestamp-Based Cache Coherence Scheme,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol. I: Architecture, </volume> <pages> pp. 23-32, </pages> <year> 1989. </year>
Reference-contexts: The number of memory bits needed to store this information can be enormous for these schemes. The compiler-directed schemes <ref> [10, 12, 24, 28, 35] </ref>, on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10]. <p> While the version control scheme [12] and the time-stamp scheme <ref> [28] </ref> require more local memory in each processor to hold the version or time-stamp numbers, this storage still can be significantly smaller than that required by the dynamic hardware-only schemes.
Reference: 29. <author> Sang Lyul Min and Jean-Loup Baer, </author> <title> ``A Performance Comparison of Directory-based and Timestamp-based Cache Coherence Schemes,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol I: Architecture, </volume> <pages> pp. 305-311, </pages> <year> 1990. </year>
Reference-contexts: The primary advantage of the hardware coherence schemes is their perfect memory disambiguation. By tracking the actual memory addresses, these schemes can invalidate only those blocks that are actually stale, assuming that they save enough state information. Several studies <ref> [1, 26, 29] </ref> have indicated that, due to its reduced network traffic, an ideal implementation of a compiler-based coherence scheme with perfect memory disambiguation can have performance comparable to, and in some cases better than, that of a full hardware directory.
Reference: 30. <author> Brian W. O'Krafka and A. Richard Newton, </author> <title> ``An Empirical Evaluation of Two Memory-Efficient Directory Methods,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 138-147, </pages> <year> 1990. </year>
Reference-contexts: In the conventional hardware directories [3, 4, 7, 19], pointer resources are statically associated with each block in the main memory fixing the total number of pointers to the size of the memory. Recently proposed dynamically tagged directories <ref> [9, 17, 26, 30] </ref> take advantage of the observation that only blocks that are actually cached in one or more processors need to be allocated pointers. <p> The second compiler optimization, delayed allocation marking, is a significant extension of this idea of combining hardware and software coherence enforcement. It uses the predictive power of the compiler to delay the allocation of a coherence pointer as long as possible. This optimization requires a dynamically tagged directory <ref> [9, 17, 26, 30] </ref>, but by delaying the allocation of pointers, they are in use for a shorter period of time, and thus can be reused more frequently. <p> In the tagged directories <ref> [9, 17, 26, 30] </ref>, however, pointers are dynamically bound to the blocks as they are referenced using an address tag within the pointer. As a result, the size of the tagged directory is proportional to the data cache size. <p> The actual cost of implementing this scheme is higher than indicated by this bit count since there is additional logic needed for the associative search of the tags, and to automatically perform the pointer overflowing or invalidation. The tag cache directory <ref> [30] </ref> is similar to the pointer cache in that the pointers are stored in each memory module in a special pointer cache. These two schemes differ primarily in the organization of the pointer cache. <p> The pointer cache coherence scheme [26] is used as an example of a tagged directory, although the compiler optimizations apply equally well to the other tagged directories <ref> [9, 17, 30] </ref>. The Alliant compiler is used to automatically generate parallel assembly code from Fortran source code, which then is converted into equivalent assembly code for a multiprocessor emulator. <p> Block partitioning can be used with these traditional directories, however, and this optimization is used in the following simulations. Since the version control scheme updates version numbers only for blocks that are actually written, the compiler marking algorithms do not apply to this scheme. The other dynamically tagged directories <ref> [9, 17, 30] </ref> also could benefit from the compiler marking strategies. They differ from the pointer cache scheme [26] primarily in the organization of the pointers in the directory.
Reference: 31. <author> Mark S. Papamarcos and Janak H. Patel, </author> <title> ``A Low-Overhead Coherence Solution for Multiprocessors with Private Cache Memories,'' </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <pages> pp. 348-354, </pages> <year> 1984. </year>
Reference-contexts: 1. Introduction Private data caches in shared memory multiprocessors can significantly reduce the average time to access global memory, but they also introduce a consistency problem since multiple copies of a memory location can be resident in several different caches simultaneously. Snoopy coherence schemes <ref> [15, 20, 31] </ref> can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck.
Reference: 32. <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss, </author> <title> ``The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <year> 1985. </year>
Reference-contexts: Snoopy coherence schemes [15, 20, 31] can efficiently ensure that all processors use the most up-to-date copy of each shared variable, but the shared bus can become a serious performance bottleneck. Replacing the bus with a multistage interconnection network <ref> [16, 23, 32] </ref> reduces the bottleneck, but it compounds the need for the caches by increasing the delay, and it exacerbates the coherence problem by eliminating the snooping medium.
Reference: 33. <author> C. Scheurich and M. Dubois, </author> <title> ``The Design of a Lockup-Free Cache for High-Performance Multiprocessors,'' </title> <booktitle> Proceedings of Supercomputing '88, </booktitle> <pages> pp. 352-359, </pages> <year> 1988. </year>
Reference-contexts: The processors used in these simulations can have multiple outstanding memory requests so that some of the memory delay can be overlapped with other processing. This type of processor requires the use of lockup-free data caches <ref> [21, 33, 34] </ref> which could increase the complexity of the processors and potentially increase contention in the interconnection network. However, to directly compare the memory performance of the different coherence schemes, the average memory delay is used as the figure of merit.
Reference: 34. <author> Per Stenstrom, Fredrik Dahlgren, and Lars Lundberg, </author> <title> ``A Lockup-free Multiprocessor Cache Design,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The processors used in these simulations can have multiple outstanding memory requests so that some of the memory delay can be overlapped with other processing. This type of processor requires the use of lockup-free data caches <ref> [21, 33, 34] </ref> which could increase the complexity of the processors and potentially increase contention in the interconnection network. However, to directly compare the memory performance of the different coherence schemes, the average memory delay is used as the figure of merit.
Reference: 35. <author> Alexander V. Veidenbaum, </author> <title> ``A Compiler-Assisted Cache Coherence Solution for Multiprocessors,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pp. 1029-1036, </pages> <year> 1986. </year> <month> 28 </month>
Reference-contexts: The number of memory bits needed to store this information can be enormous for these schemes. The compiler-directed schemes <ref> [10, 12, 24, 28, 35] </ref>, on the other hand, have an advantage over the hardware schemes in this area since they may need only a few state bits per cache block, such as in the fast, selective invalidation scheme [10].

References-found: 35

