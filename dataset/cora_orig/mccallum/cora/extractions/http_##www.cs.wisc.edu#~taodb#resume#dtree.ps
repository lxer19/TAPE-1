URL: http://www.cs.wisc.edu/~taodb/resume/dtree.ps
Refering-URL: http://www.cs.wisc.edu/~taodb/
Root-URL: http://www.cs.wisc.edu
Author: Tao, Chun Zhang 
Date: May 11, 1998  
Note: Dongbin  Contents  
Abstract: Classification in the RainForest Framework CS764 & CS784 Joint-Project Report 
Abstract-found: 1
Intro-found: 1
Reference: [GRG98] <author> Johannes Gehrke, Raghu Ramakrishnan, Venkatesh Ganti, </author> <title> RainForest A Framework for Fast Decision Tree Classification of Large Datasets. </title> <type> 23 </type>
Reference-contexts: Algorithms that build decision tree classifiers include C4.5 [Qui93], ID3 [Qui86] and QUEST [LS97]. Most decision-tree algorithms have been designed without taking scalability requirements for very 3 large datasets into account <ref> [GRG98] </ref>, and scalability is a major motivation for the design of the RainForest framework [GRG98]. RainForest is a framework for inducing decision trees with single-dimension multi-interval splitting functions on very large (e.g. 1M-tuple) datasets. One key idea of RainForest is a data structure called the AVC set. <p> Algorithms that build decision tree classifiers include C4.5 [Qui93], ID3 [Qui86] and QUEST [LS97]. Most decision-tree algorithms have been designed without taking scalability requirements for very 3 large datasets into account <ref> [GRG98] </ref>, and scalability is a major motivation for the design of the RainForest framework [GRG98]. RainForest is a framework for inducing decision trees with single-dimension multi-interval splitting functions on very large (e.g. 1M-tuple) datasets. One key idea of RainForest is a data structure called the AVC set. An AVC set describes the class label distribution information for a specific predictor attribute. <p> We then describe the three pruning implementations. 2.1 SPRINT SPRINT is a scalable classifier introduced in [SAM96]. Since then a few papers have been generated describing new algorithms improving over SPRINT. The RainForest algorithms <ref> [GRG98] </ref> are based on the observation that only a "summary" of an attribute list is sufficient for classification. <p> We henceforth call this data generator the IBMDataGenerator. Descriptions of the IBMDataGenerator can also be found in <ref> [GRG98] </ref> and [SAM96]. For our experiments, we selected 12 three of the predicate functions: Predicate Function 0 (F0), Function 1 (F1) and Function 7 (F7). F0 is the simplest. <p> This simple buffer management scheme suffices for the current RainForest algorithms, which estimate the AVC set sizes conservatively <ref> [GRG98] </ref>. If we are to improve the RainForest algorithms 22 and make more optimistic AVC set size estimation, the buffer management needs to be improved.
Reference: [SAM96] <author> John Shafer, Rakesh Agrawal, Manish Mehta, SPRINT: </author> <title> A Scalable Parallel Classifier for Data Mining, </title> <booktitle> Proceedings of the 22nd VLDB Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The extended discussion of these algorithms and the theory behind them can be found in the original papers and books. We then describe the three pruning implementations. 2.1 SPRINT SPRINT is a scalable classifier introduced in <ref> [SAM96] </ref>. Since then a few papers have been generated describing new algorithms improving over SPRINT. The RainForest algorithms [GRG98] are based on the observation that only a "summary" of an attribute list is sufficient for classification. <p> We henceforth call this data generator the IBMDataGenerator. Descriptions of the IBMDataGenerator can also be found in [GRG98] and <ref> [SAM96] </ref>. For our experiments, we selected 12 three of the predicate functions: Predicate Function 0 (F0), Function 1 (F1) and Function 7 (F7). F0 is the simplest. We mainly use it to verify the correctness of the implementations and as a base line for the quality and performance experiments. <p> For instance, SPRINT <ref> [SAM96] </ref> uses gini, and the ID3 family [Qui86, CFI88, Fay91] use entropy and gain.
Reference: [JKK98] <author> Mahesh V. Joshi, George Karypis, Vipin Kumar, ScalParC: </author> <title> A New Scalable and Efficient Parallel Classification Algorithm for Mining Large Datasets, </title> <booktitle> Proceedings of 1998 International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1998. </year>
Reference-contexts: Since then a few papers have been generated describing new algorithms improving over SPRINT. The RainForest algorithms [GRG98] are based on the observation that only a "summary" of an attribute list is sufficient for classification. In addition, a summary is likely to be much smaller than the attribute list. <ref> [JKK98] </ref> shows that the parallel formulation of SPRINT is not scalable, and a new formulation, ScalParC, is devised which outperforms SPRINT and is scalable in both runtime and memory requirements. Sprint builds a classification tree in two phases: tree induction and bottom-up pruning.
Reference: [Fay91] <author> Usama M. Fayyad, </author> <title> On the Induction of Decision Trees for Multiple Concept Learning, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, the University of Michigan, </institution> <address> Ann Arbor, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The simplest discretization is to partition the data into two subsets and represent them as 0 and 1. This is binary discretization. <ref> [Fay91] </ref> shows that the derivation of multiple partitions is more advantageous than the derivation of binary partitions. Therefore, we implemented the multi-partition (multi-interval) discretization. The algorithm used is from [Fay91], and is applied to both ID3 and GID3*. <p> This is binary discretization. <ref> [Fay91] </ref> shows that the derivation of multiple partitions is more advantageous than the derivation of binary partitions. Therefore, we implemented the multi-partition (multi-interval) discretization. The algorithm used is from [Fay91], and is applied to both ID3 and GID3*. The discretization algorithm first sorts the continuous-valued data, then a cut point is selected partitioning the data into two intervals. The two intervals are then recursively partitioned into smaller intervals. <p> The question is how to determine whether a value should be branched or not. A new measure is introduced called tear <ref> [Fay91] </ref> for this purpose. <p> For instance, SPRINT [SAM96] uses gini, and the ID3 family <ref> [Qui86, CFI88, Fay91] </ref> use entropy and gain. There is seldom any explanation why an algorithm chooses a particular index instead of another, therefore we think it would be interesting to see how each index works for different algorithms and whether the choice of index affects the performance or quality.
Reference: [CFI88] <author> J.Chen, U.M.Jayyard, K.B.Irani, and Z.Qian, </author> <title> Improved decision trees: A generalized version of ID3", </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> p.100-108, Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Nonetheless, it has some inherent weaknesses such as overspecialization and missing branches <ref> [CFI88] </ref>. GID3 [CFI88] was introduced in order to avoid these problems. However, this algorithm requires a user-provided parameter T L, in the range [0; 1], to control branching at a node. When T L = 0:0, the behavior exactly matches the ID3 algorithm. <p> Nonetheless, it has some inherent weaknesses such as overspecialization and missing branches <ref> [CFI88] </ref>. GID3 [CFI88] was introduced in order to avoid these problems. However, this algorithm requires a user-provided parameter T L, in the range [0; 1], to control branching at a node. When T L = 0:0, the behavior exactly matches the ID3 algorithm. <p> The splitting attribute A i is chosen such that splitting over it give us the biggest gain comparing to other attributes. It is shown in <ref> [CFI88] </ref> that the splitting over each distinct value can overspecialize the classification tree. The intuition is that, in some cases not every value appeared for an attribute is relevant to the class of a tuple; only some values of an attribute determine which class a tuple belongs. <p> For instance, SPRINT [SAM96] uses gini, and the ID3 family <ref> [Qui86, CFI88, Fay91] </ref> use entropy and gain. There is seldom any explanation why an algorithm chooses a particular index instead of another, therefore we think it would be interesting to see how each index works for different algorithms and whether the choice of index affects the performance or quality.
Reference: [Qui86] <author> J.R.Quinlan, </author> <title> Induction of decision trees, </title> <booktitle> Machine Learning 1, </booktitle> <year> 1986, </year> <pages> 81-106. </pages>
Reference-contexts: Algorithms that build decision tree classifiers include C4.5 [Qui93], ID3 <ref> [Qui86] </ref> and QUEST [LS97]. Most decision-tree algorithms have been designed without taking scalability requirements for very 3 large datasets into account [GRG98], and scalability is a major motivation for the design of the RainForest framework [GRG98]. <p> In the second step, two children are created for the node and portions of the attribute lists are sent to either one of them based on the split decision. 5 2.2 ID3 and GID3* ID3 <ref> [Qui86] </ref> is a very popular classification algorithm in the machine learning community, due to its simplicity and efficiency. Nonetheless, it has some inherent weaknesses such as overspecialization and missing branches [CFI88]. GID3 [CFI88] was introduced in order to avoid these problems. <p> Since Ent (S) is the same for all attributes at a particular tree node, maximizing the gain is equivalent to minimizing Ent (A; S). There is a problem with the above gain formula however. It is shown in <ref> [Qui86] </ref> that this formula is biased in favor of attributes with a large number of values. Therefore Quinlan introduced a new measure to compensate the bias. <p> For instance, SPRINT [SAM96] uses gini, and the ID3 family <ref> [Qui86, CFI88, Fay91] </ref> use entropy and gain. There is seldom any explanation why an algorithm chooses a particular index instead of another, therefore we think it would be interesting to see how each index works for different algorithms and whether the choice of index affects the performance or quality.
Reference: [Qui93] <author> J.R.Quinlan, </author> <title> C4.5 : programs for machine learning, </title> <publisher> Morgan Kaufmann Publishers, c1993. </publisher>
Reference-contexts: In addition, if f returns bn from a set of two branch numbers, it is said to generate binary splits; if f returns bn from a set of possibly more than two branch numbers, it is said to generate multiple splits. Algorithms that build decision tree classifiers include C4.5 <ref> [Qui93] </ref>, ID3 [Qui86] and QUEST [LS97]. Most decision-tree algorithms have been designed without taking scalability requirements for very 3 large datasets into account [GRG98], and scalability is a major motivation for the design of the RainForest framework [GRG98]. <p> Instead of stopping tree growth while inducting decision trees, bottom-up approaches first of all grow the tree to its full size and then various techniques can be applied to prune branches to reduce the influence of randomness from training set. We implemented Quinlan's error-based bottom-up pruning <ref> [Qui93] </ref>. Let us consider a YES/NO classification problem. For a subtree with N tuples, we classify each tuple using the prevailing class label, suppose we have E classification errors.
Reference: [PSF91] <author> G.Piatetsky Shapiro and W.J.Frawley, </author> <title> editors, Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Data Mining <ref> [PSF91] </ref> emerges as one of the fastest growing field within the database research community. As its name indicates, one of the major goals of data mining research is to identify interesting patterns that are normally hidden beneath vast amounts of data.
Reference: [MAR96] <author> M.Mehta, R.Agrawal and J.Rissanen, SLIQ: </author> <title> A Fast Scalable Classifier for Data Mining, </title> <booktitle> Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <month> March </month> <year> 1996. </year>
Reference: [KSD96] <author> Ron Kohavi, Dan Sommerfield, and James Dougherty, </author> <title> Data Mining using MLC++: A Machine Learning Library in C++, </title> <booktitle> Tools With AI 1996, </booktitle> <pages> 234-245. </pages>
Reference: [LS97] <author> Wei-Yin Loh and Yu-Shan Shih. </author> <title> Split selection methods for classification trees. </title> <journal> Statistica Sinica, </journal> <volume> 7(4), </volume> <month> October </month> <year> 1997. </year>
Reference-contexts: Algorithms that build decision tree classifiers include C4.5 [Qui93], ID3 [Qui86] and QUEST <ref> [LS97] </ref>. Most decision-tree algorithms have been designed without taking scalability requirements for very 3 large datasets into account [GRG98], and scalability is a major motivation for the design of the RainForest framework [GRG98]. <p> If it is, a j is branched on, otherwise it is lumped together with other non-relevant values into a "default" branch. 2.3 QUEST QUEST <ref> [LS97] </ref> is a classification algorithm for generating unbiased, efficient decision trees. As shown in [LS97], QUEST's classification accuracy of its trees are typically comparable to those of exhaustive search algorithms, while QUEST is substantially faster. <p> If it is, a j is branched on, otherwise it is lumped together with other non-relevant values into a "default" branch. 2.3 QUEST QUEST <ref> [LS97] </ref> is a classification algorithm for generating unbiased, efficient decision trees. As shown in [LS97], QUEST's classification accuracy of its trees are typically comparable to those of exhaustive search algorithms, while QUEST is substantially faster. QUEST clearly separates two issues in inducting a decision tree: split point selection and variable selection. This separation allow us to easily adapt QUEST to RainForest's framework.
Reference: [HW79] <author> Hartigan, J.A. and Wong, M.A. </author> <title> A k-means clustering algorithm Applied Statistics, </title> <address> 28:100, </address> <year> 1979. </year>
Reference-contexts: The outline of our implementation is given as follows. * QDA-based split point selection For a given node N and a given predictor variable v we consider, (AV CSet) mfin provides all necessary information. In case m &gt; 2, we applied k-mean clustering algorithm <ref> [HW79] </ref> to group all n class labels into two groups G 1 and G 2 , we define a new (AV CSet 0 ) mfin with n = 2: AV CSet 0 i;j = k2G j AV CSet i;k where i = 1::m; j = 1; 2 Assuming that two columns
Reference: [SCH94] <author> C.Schaffer, </author> <title> A conservation law for generalization performance, </title> <booktitle> Machine Learning: proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., pp.259-265. </publisher>
Reference-contexts: Four algorithms are implemented in the RainForest framework. Our intention was not to implement the best algorithms in the literature, as there is no such an algorithm according to the No Free Lunch Theorem or the Conservation Law <ref> [WOL94, SCH94] </ref>, but to implement some algorithms that we considered interesting. One of our main concerns was the accessibility of the original papers, and another was the time constraint. We implemented three algorithms: ID3, GID3*, and QUEST, and the fourth in RainForest is the original SPRINT.
Reference: [WOL94] <author> D.H.Wolpert, </author> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, The Mathematics of Generalization, </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: Four algorithms are implemented in the RainForest framework. Our intention was not to implement the best algorithms in the literature, as there is no such an algorithm according to the No Free Lunch Theorem or the Conservation Law <ref> [WOL94, SCH94] </ref>, but to implement some algorithms that we considered interesting. One of our main concerns was the accessibility of the original papers, and another was the time constraint. We implemented three algorithms: ID3, GID3*, and QUEST, and the fourth in RainForest is the original SPRINT.
Reference: [AIS93] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami, </author> <title> Database mining: A performance perspective, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <year> 1993. </year>
Reference-contexts: We are most interested in two aspects of these inductors, namely the quality and the performance. Variables that influence these two aspects are described as follows: * Predicates Each predicate defines a decision tree, it is used by the synthetic data generator <ref> [AIS93] </ref> to generate a dataset that exhibits the tree pattern. Please refer to the next section for different predicates we experimented on. * Noise Level Instead of synthesizing dataset that accurately reflects a predicate, the generator allow us to perturb each predictor value. <p> While the hardware will not influence the quality studies, it makes a major difference in performance results. 3.2 Datasets and the Predicate Functions Since there is no large real-life datasets, we use the synthetic data generated by the data generator in <ref> [AIS93] </ref>. We henceforth call this data generator the IBMDataGenerator. Descriptions of the IBMDataGenerator can also be found in [GRG98] and [SAM96]. For our experiments, we selected 12 three of the predicate functions: Predicate Function 0 (F0), Function 1 (F1) and Function 7 (F7). F0 is the simplest.
Reference: [UCI94] <author> P.M.Murphy and D.W.Aha, </author> <title> UCI repository of machine learning databases, </title> <address> http://www.ics.uci.edu/~mlearn/MLRepository.html, Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution> <month> 24 </month>
Reference-contexts: The real solution is again a better data generator which can simulate real-life data set. Many researchers based their research on UCI <ref> [UCI94] </ref>, which is the most well-know real-life dataset repository. However, the largest data set Connect-4 Opening Database contains only 67557 tuples, which is insufficient for studies on scalable classification algorithms. all algorithms are substantially reduced. For many data points, more than half of tree nodes are pruned.
References-found: 16

