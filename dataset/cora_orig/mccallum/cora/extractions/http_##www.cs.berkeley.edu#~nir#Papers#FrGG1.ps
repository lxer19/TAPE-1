URL: http://www.cs.berkeley.edu/~nir/Papers/FrGG1.ps
Refering-URL: http://http.cs.berkeley.edu/~nir/publications.html
Root-URL: 
Email: nir@cs.berkeley.edu  dang@cs.technion.ac.il  MOISES GOLDSZMIDT moises@erg.sri.com  
Title: Bayesian Network Classifiers  
Author: NIR FRIEDMAN DAN GEIGER Editor: G. Provan, P. Langley, and P. Smyth 
Address: 387 Soda Hall, University of California, Berkeley, CA 94720  Israel, 32000  333 Ravenswood Ave., Menlo Park, CA 94025  
Affiliation: Computer Science Division,  Computer Science Department, Technion, Haifa,  SRI International,  
Note: 1-37 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Binder, J., D. Koller, S. Russell, & K. </author> <title> Kanazawa (1997). Adaptive probabilistic networks with hidden variables. </title> <journal> Machine Learning, </journal> <note> this issue. </note>
Reference-contexts: This implies that, to maximize the choice of parameters for a fixed network structure, we must resort to search methods such as gradient descent over the space of parameters (e.g., using the techniques of <ref> (Binder et al., 1997) </ref>). When learning the network structure, this search must be repeated for each structure candidate, rendering the method computationally expensive. Whether we can find heuristic approaches that will allow effective learning using the conditional log likelihood remains an open question. <p> That is, the score cannot be written as the sum of local terms (as in Equation 4). Moreover, to evaluate the optimal choice of parameters for a candidate network structure, we must perform nonlinear optimization using either EM (Lauritzen, 1995) or gradient descent <ref> (Binder et al., 1997) </ref>. The problem of selecting the best structure is usually intractable in the presence of missing values. Several recent efforts (Geiger et al., 1996; Chickering & Heck-erman, 1996) have examined approximations to the marginal score that can be evaluated efficiently.
Reference: <author> Bouckaert, R. R. </author> <year> (1994). </year> <title> Properties of Bayesian network learning algorithms. </title> <editor> In R. Lopez de Mantaras & D. Poole (Eds.), </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 102-109). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W. </author> <year> (1991). </year> <title> Theory refinement on Bayesian networks. </title> <editor> In B. D. D'Ambrosio, P. Smets, & P. P. Bonissone (Eds.), </editor> <booktitle> Proceedings of the Seventh Annual Conference on Uncertainty Artificial Intelligence (pp. </booktitle> <pages> 52-60). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W. </author> <year> (1996). </year> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 8, </volume> <pages> 195-210. </pages>
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: a crucial task in machine learning. </title> <editor> In L. C. Aiello (Ed.), </editor> <booktitle> Proceedings of the 9th European Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-149). </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference: <author> Chickering, D. M. & D. </author> <title> Heckerman (1996). Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <editor> In E. Horvits & F. Jensen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 158-168). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chow, C. K. & C. N. </author> <title> Liu (1968). Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> 14, </volume> <pages> 462-467. </pages>
Reference-contexts: Transform the resulting undirected tree to a directed one by choosing a root variable and setting the direction of all edges to be outward from it. CL prove that this procedure finds the tree that maximizes the likelihood given the data D. Theorem 1 <ref> (Chow & Liu, 1968) </ref> Let D be a collection of N instances of X 1 ; : : : ; X n . The Construct-Tree procedure constructs a tree B T that maximizes LL (B T jD) and has time complexity O (n 2 N ). <p> We can use the algorithm in Theorem 1 separately to the attributes that correspond to each value of the class variable. This results in a multinet in which each network is a tree. Corollary 1 <ref> (Chow & Liu, 1968) </ref> Let D be a collection of N instances of C, A 1 ; : : : ; A n . There is a procedure of time complexity O (n 2 N ) which constructs a multinet consisting of trees that maximizes log likelihood.
Reference: <author> Cooper, G. F. & E. </author> <title> Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> Cormen, T. H., C. E. Leiserson, & R. L. </author> <title> Rivest (1990). Introduction to Algorithms. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There are well-known algorithms for solving this problem of time complexity O (n 2 log n), where n is the number of vertices in the graph <ref> (Cormen et al., 1990) </ref>. The Construct-Tree procedure of CL consists of four steps: 1. Compute I ^ P D (X i ; X j ) between each pair of variables, i 6= j, where I P (X; Y) = x;y P (x; y) is the mutual information function.
Reference: <author> Cover, T. M. & J. A. </author> <title> Thomas (1991). Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher> <address> 36 N. </address> <note> FRIEDMAN, </note> <author> D. GEIGER, AND M. GOLDSZMIDT Dawid, A. P. </author> <year> (1976). </year> <title> Properties of diagnostic data distributions. </title> <journal> Biometrics, </journal> <volume> 32, </volume> <pages> 647-658. </pages>
Reference-contexts: Hence, we need to maximize the term X I ^ P D (A i ; A (i) ; C) + i;(i)=0 14 N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT We simplify this term by using the identity known as the chain law for mutual information <ref> (Cover & Thomas, 1991) </ref>: I P (X; Y; Z) = I P (X; Z) + I P (X; YjZ): Hence, we can rewrite expression (8) as X I ^ P D (A i ; C) + i;(i)&gt;0 Note that the first term is not affected by the choice of (i).
Reference: <author> DeGroot, M. H. </author> <year> (1970). </year> <title> Optimal Statistical Decisions. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Domingos, P. & M. </author> <title> Pazzani (1996). Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <editor> In L. Saitta (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 105-112). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dougherty, J., R. Kohavi, & M. </author> <title> Sahami (1995). Supervised and unsupervised discretization of continuous features. </title> <editor> In A. Prieditis & S. Russell (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 194-202). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O. & P. E. </author> <title> Hart (1973). Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Ezawa, K. J. & T. </author> <month> Schuermann </month> <year> (1995). </year> <title> Fraud/uncollectable debt detection using a Bayesian network based learning system: A rare binary outcome with mixed data structures. </title> <editor> In P. Besnard & S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 157-166). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M. & K. B. </author> <title> Irani (1993). Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1022-1027). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, J. </author> <year> (1997a). </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <booktitle> 1, </booktitle> <pages> 55-77. </pages>
Reference: <author> Friedman, N. </author> <year> (1997b). </year> <title> Learning belief networks in the presence of missing values and hidden variables. </title> <editor> In D. Fisher (Ed.), </editor> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 125-133). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, N. & M. </author> <title> Goldszmidt (1996a). Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1277-1284). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Friedman, N. & M. </author> <title> Goldszmidt (1996b). Discretization of continuous attributes while learning Bayesian networks. </title> <editor> In L. Saitta (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 157-165). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, N. & M. </author> <title> Goldszmidt (1996c). Learning Bayesian networks with local structure. </title> <editor> In E. Horvits & F. Jensen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 252-262). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Geiger, D. </author> <year> (1992). </year> <title> An entropy-based learning algorithm of Bayesian conditional trees. </title> <editor> In D. Dubois, M. P. Wellman, B. D. D'Ambrosio, & P. Smets (Eds.), </editor> <booktitle> Proceedings of the Eighth Annual Conference on Uncertainty Artificial Intelligence (pp. </booktitle> <pages> 92-97). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Geiger, D. & D. </author> <title> Heckerman (1996). Knowledge representation and inference in similarity networks and Bayesian multinets. </title> <journal> Artificial Intelligence, </journal> <volume> 82, </volume> <pages> 45-74. </pages>
Reference: <author> Geiger, D., D. Heckerman, & C. </author> <title> Meek (1996). Asymptotic model selection for directed graphs with hidden variables. </title> <editor> In E. Horvits & F. Jensen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 283-290). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Heckerman, D. </author> <year> (1991). </year> <title> Probabilistic Similarity Networks. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research. </institution>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents <ref> (Heckerman, 1995) </ref>.
Reference: <author> Heckerman, D. & D. </author> <title> Geiger (1995). Learning Bayesian networks: a unification for discrete and Gaussian domains. </title> <editor> In P. Besnard & S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 274-284). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents <ref> (Heckerman, 1995) </ref>.
Reference: <author> Heckerman, D., D. Geiger, & D. M. </author> <title> Chickering (1995). Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 197-243. </pages>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents <ref> (Heckerman, 1995) </ref>.
Reference: <author> John, G. & R. </author> <title> Kohavi (1997). Wrappers for feature subset selection. </title> <journal> Artificial Intelligence. </journal> <note> Accepted for publication. A preliminary version appears in Proceedings of the Eleventh International Conference on Machine Learning, </note> <year> 1994, </year> <pages> pp. </pages> <month> 121-129, </month> <title> under the title "Irrelevant features and the subset selection problem". </title>
Reference: <author> John, G. H. & P. </author> <title> Langley (1995). Estimating continuous distributions in Bayesian classifiers. </title>
Reference: <editor> In P. Besnard & S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 338-345). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. BAYESIAN NETWORK CLASSIFIERS 37 Kohavi, R. </publisher> <year> (1995). </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1137-1143). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, another related effort that is somewhere between the categories mentioned above is reported by Singh and Provan (1995, 1996). They combine several feature subset selection strategies with an unsupervised Bayesian network learning routine. This procedure, however, can be computationally intensive (e.g., some of their strategies <ref> (Singh & Provan, 1995) </ref> involve repeated calls to a the Bayesian network learning routine). 6.2. The conditional log likelihood Even though the use of log likelihood is warranted by an asymptotic argument, as we have seen, it may not work well when we have a limited number of samples.
Reference: <author> Kohavi, R., G. John, R. Long, D. Manley, & K. </author> <month> Pfleger </month> <year> (1994). </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Proc. Sixth International Conference on Tools with Artificial Intelligence (pp. </booktitle> <pages> 740-743). </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: These two artificial data sets were designed by John and Kohavi (1997) to evaluate methods for feature subset selection. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each data set. We used the MLC++ system <ref> (Kohavi et al., 1994) </ref> to estimate the prediction accuracy for each classifier, as well as the variance of this accuracy.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <editor> In Y. Kodratoff (Ed.), </editor> <booktitle> Proc. Sixth European Working Session on Learning (pp. </booktitle> <pages> 206-219). </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Work in the second category (Kononenko, 1991; Pazzani, 1995; Ezawa & Schuer-mann, 1995) are closer in spirit to our proposal, since they attempt to improve the predictive accuracy by removing some of the independence assumptions. The semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> is a model of the form P (C; A 1 ; : : : ; A n ) = P (C) P (A 1 jC) P (A k jC) (9) where A 1 ; : : : ; A k are pairwise disjoint groups of attributes.
Reference: <author> Kullback, S. & R. A. </author> <month> Leibler </month> <year> (1951). </year> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <pages> 76-86. </pages>
Reference-contexts: This reading suggests that by maximizing the log likelihood we are minimizing the description of D. Another way of viewing this optimization process is to use cross entropy, which is also known as the Kullback-Leibler divergence <ref> (Kullback & Leibler, 1951) </ref>. Cross entropy is a measure of distance between two probability distributions. Formally, D (P (X)jjQ (X)) = x2Val (X) P (x) : (A.1) One information-theoretic interpretation of cross entropy is the average redundancy incurred in encoding when we use a wrong probability measure.
Reference: <author> Lam, W. & F. </author> <title> Bacchus (1994). Learning Bayesian belief networks. An approach based on the MDL principle. </title> <journal> Computational Intelligence, </journal> <volume> 10, </volume> <pages> 269-293. </pages>
Reference: <author> Langley, P., W. Iba, & K. </author> <title> Thompson (1992). An analysis of Bayesian classifiers. </title> <booktitle> In Proceedings, Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 223-228). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This is in fact the definition of naive Bayes commonly found in the literature <ref> (Langley et al., 1992) </ref>. The problem of learning a Bayesian network can be informally stated as: Given a training set D = fu 1 ; : : : ; u N g of instances of U, find a network B that best matches D.
Reference: <author> Langley, P. & S. </author> <title> Sage (1994). Induction of selective Bayesian classifiers. </title> <editor> In R. Lopez de Mantaras & D. Poole (Eds.), </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 399-406). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lauritzen, S. L. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19, </volume> <pages> 191-201. </pages>
Reference-contexts: That is, the score cannot be written as the sum of local terms (as in Equation 4). Moreover, to evaluate the optimal choice of parameters for a candidate network structure, we must perform nonlinear optimization using either EM <ref> (Lauritzen, 1995) </ref> or gradient descent (Binder et al., 1997). The problem of selecting the best structure is usually intractable in the presence of missing values. Several recent efforts (Geiger et al., 1996; Chickering & Heck-erman, 1996) have examined approximations to the marginal score that can be evaluated efficiently.
Reference: <author> Lewis, P. M. </author> <year> (1959). </year> <title> Approximating probability distributions to reduce storage requirements. </title> <journal> Information and Control, </journal> <volume> 2, </volume> <pages> 214-225. </pages>
Reference: <author> Murphy, P. M. & D. W. </author> <note> Aha (1995). UCI repository of machine learning databases. http:// www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: Moreover, on some data sets they have better accuracy than TAN and CL multinets. 5. Experimental methodology and results We ran our experiments on the 25 data sets listed in Table 1. All of the data sets come from the UCI repository <ref> (Murphy & Aha, 1995) </ref>, with the exception of "mofn-3-7-10" and "corral". These two artificial data sets were designed by John and Kohavi (1997) to evaluate methods for feature subset selection.
Reference: <author> Pazzani, M. J. </author> <year> (1995). </year> <title> Searching for dependencies in Bayesian classifiers. </title> <editor> In D. Fisher & H. Lenz (Eds.), </editor> <booktitle> Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both are provided by Bayesian networks <ref> (Pearl, 1988) </ref>. These networks are directed acyclic graphs that allow efficient and effective representation of the joint probability distribution over a set of random variables. Each vertex in the graph represents a random variable, and edges represent direct correlations between the variables. <p> We base our definition of relevant attributes on the notion of a Markov blanket of a variable X, which consists of X's parents, X's children, and the parents of X's children in a given network structure G <ref> (Pearl, 1988) </ref>. This set has the property that, conditioned on X's Markov blanket, X is independent of all other variables in the network. <p> As we now show, we can take advantage of this restriction to learn a TAN model efficiently. The procedure for learning these edges is based on a well-known method reported by Chow and Liu (CL from now on) (1968), for learning tree-like Bayesian networks (see also <ref> (Pearl, 1988, pp. 387-390) </ref>). We start by reviewing CL's result.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The complete results for the smoothed version of naive Bayes are reported in Table 3. Given that TAN performs better than naive Bayes and that naive Bayes is comparable to C4.5 <ref> (Quinlan, 1993) </ref>, a state-of-the-art decision tree learner, we may infer that TAN should perform rather well in comparison to C4.5. To confirm this prediction, we performed experiments comparing TAN to C4.5, and also to the selective naive Bayesian classifier (Langley & Sage, 1994; John & Kohavi, 1997).
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern recognition and neural networks. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: In general, neither of these approaches dominates the other <ref> (Ripley, 1996) </ref>. The naive Bayesian classifier and the extensions we have evaluated belong to the sampling paradigm. Although the unrestricted Bayesian networks (described in Section 3) do not strictly belong in either paradigm, they are closer in spirit to the sampling paradigm. 6.3.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471. </pages>
Reference-contexts: An in-depth discussion of the pros and cons of each scoring function is beyond the scope of this paper. Henceforth, we concentrate on the MDL scoring function. The MDL principle <ref> (Rissanen, 1978) </ref> casts learning in terms of data compression. Roughly speaking, the goal of the learner is to find a model that facilitates the shortest description of the original data.
Reference: <author> Rubin, D. R. </author> <year> (1976). </year> <title> Inference and missing data. </title> <journal> Biometrica, </journal> <volume> 63, </volume> <pages> 581-592. </pages>
Reference-contexts: We suspect that there exist analogues to Theorem 2 for such hybrid networks but we leave this issue for future work. Regarding the problem of missing values, in theory probabilistic methods provide a principled solution. If we assume that values are missing at random <ref> (Rubin, 1976) </ref>, then we can use the marginal likelihood (the probability assigned to the parts of the instance that were observed) as the basis for scoring models.
Reference: <author> Singh, M. & G. M. </author> <title> Provan (1995). A comparison of induction algorithms for selective and nonselective Bayesian classifiers. </title> <editor> In A. Prieditis & S. Russell (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 497-505). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, another related effort that is somewhere between the categories mentioned above is reported by Singh and Provan (1995, 1996). They combine several feature subset selection strategies with an unsupervised Bayesian network learning routine. This procedure, however, can be computationally intensive (e.g., some of their strategies <ref> (Singh & Provan, 1995) </ref> involve repeated calls to a the Bayesian network learning routine). 6.2. The conditional log likelihood Even though the use of log likelihood is warranted by an asymptotic argument, as we have seen, it may not work well when we have a limited number of samples.
Reference: <author> Singh, M. & G. M. </author> <title> Provan (1996). Efficient learning of selective Bayesian network classifiers. </title> <editor> In L. Saitta (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 453-461). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spiegelhalter, D. J., A. P. Dawid, S. L. Lauritzen, & R. G. </author> <title> Cowell (1993). Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8, </volume> <pages> 219-283. </pages>
Reference: <author> Suzuki, J. </author> <year> (1993). </year> <title> A construction of Bayesian networks from databases based on an MDL scheme. </title>
Reference: <editor> In D. Heckerman & A. Mamdani (Eds.), </editor> <booktitle> Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 266-273). </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 51

