URL: http://www.cs.wustl.edu/cs/techreports/1993/wucs-93-23.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: bshouty@cpsc.ucalgary.ca  sg@cs.wustl.edu  hancock@learning.siemens.com  sleiman@cpsc.ucalgary.ca  
Title: Asking Questions to Minimize Errors  
Author: Nader H. Bshouty Sally A. Goldman Thomas R. Hancock Sleiman Matar 
Note: WUCS-93-23  
Date: September 1993  
Address: 2500 University Drive N.W. Calgary, Alberta, Canada T2N 1N4  St. Louis, MO 63130  755 College Road East Princeton, NJ 08540  2500 University Drive N.W. Calgary, Alberta, Canada T2N 1N4  
Affiliation: Department of Computer Science The University of Calgary  Department of Computer Science Washington University  Siemens Corporate Research, Inc.  Department of Computer Science The University of Calgary  
Abstract: A number of efficient learning algorithms achieve exact identification of an unknown function from some class using membership and equivalence queries. Using a standard transformation such algorithms can easily be converted to on-line learning algorithms that use membership queries. Under such a transformation the number of equivalence queries made by the query algorithm directly corresponds to the number of mistakes made by the on-line algorithm. In this paper we consider several of the natural classes known to be learnable in this setting, and investigate the minimum number of equivalence queries with accompanying counterexamples (or equivalently the minimum number of mistakes in the on-line model) that can be made by a learning algorithm that makes a polynomial number of membership queries and uses polynomial computation time. We are able both to reduce the number of equivalence queries used by the previous algorithms and often to prove matching lower bounds. As an example, consider the class of DNF formulas over n variables with at most k = O(log n) terms. Previously, the algorithm of Blum and Rudich [BR92] provided the best known upper bound of 2 O(k) log n for the minimum number of equivalence queries needed for exact identification. We greatly improve on this upper bound showing that exactly k counterexamples are needed if the learner knows k a priori and exactly k +1 counterexamples are needed if the learner does not know k a priori. This exactly matches known lower bounds [BC92]. For many of our results we obtain a complete characterization of the tradeoff between the number of membership and equivalence queries needed for exact identification. The classes we consider here are monotone DNF formulas, Horn sentences, O(log n)-term DNF formulas, read-k sat-j DNF formulas, read-once formulas over various bases, and deterministic finite automata. 
Abstract-found: 1
Intro-found: 1
Reference: [AFP90] <author> Dana Angluin, Michael Frazier, and Leonard Pitt. </author> <title> Learning conjunctions of horn clauses. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 186-192, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata [Ang87b], Horn sentences <ref> [AFP90] </ref>, read-once formulas [AHK89, BHH92a, BHH92b], k-term DNF formulas [BR92], etc. <p> 1 [Ang88] m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + 1 m + 1 m known m z m x Horn Sentences m not known m (2n + 1) <ref> [AFP90] </ref> m known O log m+log n y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n [BHH92b] O log n n Arithmetic read-once formulas for jF j = o (n= log n) n [BHH92a] O log n n log jFj DFA with <p> Thus this class can be viewed as a generalization of the class of monotone DNF formulas. Angluin, Frazier, and Pitt give a polynomial time algorithm to learn Horn sentences <ref> [AFP90] </ref>. For this class m denotes the number of Horn clauses in the sentence, and a polynomial time algorithm can use time polynomial in n and m.
Reference: [AFP92] <author> Dana Angluin, Michael Frazier, and Leonard Pitt. </author> <title> Learning conjunctions of horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year> <note> Special Issue for COLT 90. </note>
Reference: [AHK89] <author> Dana Angluin, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report UCB/CSD 89/528, </type> <institution> University of California Berkeley Computer Science Division, </institution> <year> 1989. </year>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata [Ang87b], Horn sentences [AFP90], read-once formulas <ref> [AHK89, BHH92a, BHH92b] </ref>, k-term DNF formulas [BR92], etc. <p> nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + 1 m + 1 m known m z m x Horn Sentences m not known m (2n + 1) [AFP90] m known O log m+log n y mn Read-once formulas over _; ^; : n <ref> [AHK89] </ref> O log n n over B k n [BHH92b] O log n n Arithmetic read-once formulas for jF j = o (n= log n) n [BHH92a] O log n n log jFj DFA with n states n not known n [Ang87b, RS89] O log n n known log n fl <p> We improve that transformation, somewhat surprisingly, to show that only n= log n equivalence queries are required. This improvement decreases the number of equivalence queries required to learn read-once formulas over certain bases <ref> [AHK89, BHH92b] </ref>, arithmetic read-once formulas [BHH92a] and non-monotone switch configurations [RS90], giving bounds that are asymptotically tight. Next, in Section 10 we present our results for learning deterministic finite state automaton (DFAs). <p> Angluin, Hellerstein, and Karpinski give a polynomial time exact identification algorithm for the formulas over the operators (or basis) f_; ^; :g <ref> [AHK89] </ref>, and Bshouty, Hancock, and Hellerstein have extended this to more complicated sets of Boolean functions, including the set B k of all functions on k or fewer inputs (for an arbitrary constant k) [BHH92b]. <p> This is achieved as a consequence of a more general result, showing that an algorithm that makes use of equivalence queries only to generate justifying assignments (defined below) needs to make only O (n= log n) queries. This is an improvement from a previous technique that uses n queries <ref> [AHK89, BHHK91] </ref>, and immediately gives us improved upper bounds for various classes of read-once formulas and non-monotone switch configurations. These upper bounds are tight from the work of Bshouty and Cleve [BC92]. In this section we consider the following classes of read-once formulas. <p> If not we can repeat with a different p i that agrees with y 0 on Y 1 [ Y 2 , and so on. The bounds in the statement of the theorem follow from straightforward analysis. 2 Applying the above technique to previous algorithms <ref> [AHK89, BHH92a, BHHK91, RS90] </ref> we obtain the following result. Corollary 18 The quantities E (ROF n (AN D; OR; N OT )), E (ROF n (B k )), and E (Switch Configurations n ) are all O (n= log n).
Reference: [Ang87a] <author> Dana Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Yale University, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Nevertheless, as we have discussed, there are many situations in which it is extremely important to minimize the number of equivalence queries needed to obtain exact identification. 3 Definitions We now formalize the model of learning from membership and equivalence queries <ref> [Ang87a] </ref>. The learner must infer an unknown target concept f chosen from some known representation class C, which is a set of representations of functions mapping some domain X into a range Y. <p> Here is a summary of the representation classes we study in this paper. k-term DNF This is the class of DNF formulas having at most k terms. Angluin gave a polynomial time identification algorithm for the special case when k is constant <ref> [Ang87a] </ref>, and Blum and Rudich have since given a more efficient algorithm that runs in polynomial time for k = O (log n) [BR92].
Reference: [Ang87b] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata <ref> [Ang87b] </ref>, Horn sentences [AFP90], read-once formulas [AHK89, BHH92a, BHH92b], k-term DNF formulas [BR92], etc. <p> y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n [BHH92b] O log n n Arithmetic read-once formulas for jF j = o (n= log n) n [BHH92a] O log n n log jFj DFA with n states n not known n <ref> [Ang87b, RS89] </ref> O log n n known log n fl For k = O ( log n) we can obtain this result using k-term DNF formulas as the hypotheses for the equivalence queries. For the remaining cases, general DNF formulas are used for the equivalence queries. y With unlimited computation. <p> DFAs These are deterministic finite automata representing regular languages over some alphabet . These languages can be viewed as functions from fl to f0; 1g, and an efficient exact identification algorithm is due to Angluin <ref> [Ang87b] </ref> and has since been improved by Rivest and Schapire [RS89]. Here we let n denote the number of states in the target automaton. <p> For the upper bound we use a modification of Schapire's algorithm [Sc91] which is itself a modification of Angluin's original algorithm <ref> [Ang87b] </ref>. For x 2 fl , we define fl (x) = 1 if and only if x 2 U . An observation table [Ang87b], denoted by (S; E; T ), records the value of fl (x) for x 2 (S [ S ) E . <p> For the upper bound we use a modification of Schapire's algorithm [Sc91] which is itself a modification of Angluin's original algorithm <ref> [Ang87b] </ref>. For x 2 fl , we define fl (x) = 1 if and only if x 2 U . An observation table [Ang87b], denoted by (S; E; T ), records the value of fl (x) for x 2 (S [ S ) E . We denote the row of table (S; E; T ) labeled by s 2 S [ S by row (s). <p> Furthermore, observe that S is prefix closed and E is suffix closed. We now restate the following theorem from Angluin <ref> [Ang87b] </ref>. Theorem 22 If (S,E,T) is a closed, consistent observation table, then the acceptor M (S,E,T) is consistent with the finite function T. Any other acceptor consistent with T but inequivalent to M (S,E,T) must have more states.
Reference: [Ang88] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year> <month> 42 </month>
Reference-contexts: 1 Introduction A very well studied formal learning model is the membership and equivalence query model developed by Angluin <ref> [Ang88] </ref>. In this model the learner's goal is to learn exactly how an unknown target function f , taken from some known representation class C, classifies all instances from the domain. This goal is commonly referred to as exact identification. <p> After the prediction is made, the learner is told whether the prediction is correct and is then able to use polynomial time (and here a polynomial number of membership queries) before proceeding to the next trial. Using a standard transformation <ref> [Ang88, Lit88] </ref> algorithms that use membership and equivalence queries can easily be converted to on-line learning algorithms that use membership queries. <p> A previous 2 Representation Previous New Lower Class Upper Bound Upper Bound Bound k-term DNF (k =O (log n) fl ) k not known 2 O (k) log n [BR92] k + 1 k + 1 [BC92] k known k k [BC92] Monotone DNF m not known m + 1 <ref> [Ang88] </ref> m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + 1 m + 1 m known m z m x Horn Sentences m not known m (2n + 1) [AFP90] m <p> Another motivation comes from the goal of minimizing the number of prediction mistakes in an on-line learning model. As we have mentioned, the model of learning with membership and equivalence queries is essentially equivalent to the on-line learning model when the learner is provided with membership queries <ref> [Ang88, Lit88] </ref>. The conversion of an algorithm A that uses membership queries and equivalence queries to an on-line algorithm A 0 works as follows. If algorithm A wants to perform some internal computation or perform a membership query then algorithm A 0 will perform the same task. <p> Their algorithm uses n fi (jk) equivalence queries. Monotone DNF This is the class of monotone DNF formulas. These were proved to be efficiently learnable by Valiant [Val84] and Angluin <ref> [Ang88] </ref>. We use m to denote the number of terms in the target formula. A polynomial time learning algorithm can use time polynomial in n and m. Horn Sentences These are the conjunction of implications each of the form v i 1 v i m ! v j . <p> The result follows since (n=k n ) k n grows superpolynomially. 2 7.2 Upper Bounds In this section we describe an algorithm that matches the above lower bounds. We begin by briefly describing Angluin's algorithm <ref> [Ang88] </ref> for learning a monotone DNF formula using at most m equivalence queries. A prime implicant of a Boolean formula f is a conjunction t (not containing contradictory literals) such that t implies f , but no proper subset of t implies f . <p> When no further changes are possible, add the conjunction of variables still set to 1 in x as a new term for h. O k k + mn 2 membership queries. Given this observation, there is a fairly straightforward exact identification algorithm due to Angluin <ref> [Ang88] </ref> (based on a previous PAC learning algorithm of Valiant [Val84]). We use each equivalence query to find a new prime implicant. Our current hypothesis is the disjunction of all known prime implicants (initially the always false hypothesis).
Reference: [Ang90] <author> Dana Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: It is easily shown that membership queries alone are not sufficient for efficient learning of these classes, and Angluin has developed a technique of "approximate fingerprints" to show that equivalence queries alone are also not enough <ref> [Ang90] </ref>. (In both cases the arguments are information theoretic, and hold even when the computation time is unbounded.) Our research extends Angluin's results to establish tight bounds on how many equivalence queries are required for a number of these classes.
Reference: [AP91] <author> Howard Aizenstein and Leonard Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 170-179, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Other open problems include applying these techniques to find tight bounds for other classes for which polynomial time exact identification algorithms are known, such as decision trees [Bsh93], and read-twice DNF formulas <ref> [AP91, Han91, PR93] </ref>. Another interesting problem is how the use of membership queries can reduce the number of equivalence queries needed to learn classes that can in fact be learned with equivalence queries alone, such as k-DNF.
Reference: [AP92] <author> Howard Aizenstein and Leonard Pitt. </author> <title> Exact learning of read-k disjoint DNF and not-so-disjoint DNF. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 71-76, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: log n [BR92] k + 1 k + 1 [BC92] k known k k [BC92] Monotone DNF m not known m + 1 [Ang88] m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) <ref> [AP92] </ref> m + 1 m + 1 m known m z m x Horn Sentences m not known m (2n + 1) [AFP90] m known O log m+log n y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n [BHH92b] O log n <p> While almost all of our positive results apply for the more stringent definition we have given, the negative results hold even under this more general model. 6 read-k sat-j DNF formulas was proved to be learnable by Aizenstein and Pitt <ref> [AP92] </ref>. Their algorithm uses n fi (jk) equivalence queries. Monotone DNF This is the class of monotone DNF formulas. These were proved to be efficiently learnable by Valiant [Val84] and Angluin [Ang88]. We use m to denote the number of terms in the target formula. <p> The class of read-k sat-j DNF formulas was proven to be learnable by Aizenstein and Pitt <ref> [AP92] </ref>. The running time of their algorithm is O (n 4kj+2j+2 ) and the algorithm uses at most k (n j+2 + n j+1 ) membership queries and at most kn 2kj+j+1 equivalence queries. <p> Let Read-k Sat-j DNF n represent the class of read-k sat-j DNF formulas on n variables, and let Read-k Sat-j DNF n;m be the subclass of those formulas that have at most m terms. 13 6.1 An Upper Bound for Read-k Sat-j DNF Formulas The algorithm of Aizenstein and Pitt <ref> [AP92] </ref> shares the same high level structure as the Blum and Rudich algorithm to learn k-term DNF formulas, shown in Figure 2.
Reference: [BC92] <author> Nader H. Bshouty and Richard Cleve. </author> <title> On the exact learning of formulas in parallel. </title> <booktitle> In 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 1-15, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A previous 2 Representation Previous New Lower Class Upper Bound Upper Bound Bound k-term DNF (k =O (log n) fl ) k not known 2 O (k) log n [BR92] k + 1 k + 1 <ref> [BC92] </ref> k known k k [BC92] Monotone DNF m not known m + 1 [Ang88] m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + 1 m + 1 m known m <p> A previous 2 Representation Previous New Lower Class Upper Bound Upper Bound Bound k-term DNF (k =O (log n) fl ) k not known 2 O (k) log n [BR92] k + 1 k + 1 <ref> [BC92] </ref> k known k k [BC92] Monotone DNF m not known m + 1 [Ang88] m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + 1 m + 1 m known m z m x Horn Sentences <p> C the class of Horn sentences or the class of DNF formulas, E U (C) = O mn In Section 16 we show that both of these results are asymptotically tight, by giving a matching lower bound for the class of Horn Sentences. 5 k-term DNF Formulas Bshouty and Cleve <ref> [BC92] </ref> prove that k equivalence queries are required to learn a k-term DNF formula when the learner knows k a priori, and that k + 1 queries are required when k is not known in advance (the extra query comes because the algorithm does not know when to stop looking for <p> The Adversary The lower bound here holds when jk = o (n=(log n) 2 ) and the number of terms in the target formula is at most p j (k 1)n=2. We establish this bound using the techniques of Bshouty and Cleve <ref> [BC92] </ref>. <p> This is an improvement from a previous technique that uses n queries [AHK89, BHHK91], and immediately gives us improved upper bounds for various classes of read-once formulas and non-monotone switch configurations. These upper bounds are tight from the work of Bshouty and Cleve <ref> [BC92] </ref>. In this section we consider the following classes of read-once formulas. Let ROF n (B) denote the set of read once-formulas whose gates are labeled with functions from B (the "basis"). We let B k denote the basis of all boolean functions over k inputs, for a constant k. <p> It follows from the work of Bshouty and Cleve <ref> [BC92] </ref> that E (ROF n (AN D; OR; N OT )) = n E (ROF n (B k )) = n E (Switch Configurations n ) = log n E AROF (F) n (+; fi; =; ) = n log jF j ; when jF j = o (n=logn). 9.1 Generating <p> When the size of the field F is at least 2n + 5 then the algorithm does not use equivalence queries at all (however, the algorithm is randomized in this case). The lower bound, established by Bshouty and Cleve <ref> [BC92] </ref>, of (n log jF j = log n) on the number of equivalence queries holds when the size of F is o (n= log n).
Reference: [BF72] <author> IAn Barzdin and Rusi~ns Freivald. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: For Horn sentences the relationship is more complex. In this paper log denotes the logarithm base 2, and ln denotes the natural logarithm. 4 A Generalization of the Halving Algorithm In this section we consider a generalization of the halving algorithm <ref> [BF72, Lit88] </ref> in which we can reduce the number of equivalence queries required by allowing the learner to make membership queries. In the section we do not bound the computation time of the learner, and the hypotheses proposed need not come from C.
Reference: [BHH92a] <author> Nader H. Bshouty, Thomas R. Hancock, and Lisa Hellerstein. </author> <title> Learning arithmetic read-once formulas. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 370-381, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata [Ang87b], Horn sentences [AFP90], read-once formulas <ref> [AHK89, BHH92a, BHH92b] </ref>, k-term DNF formulas [BR92], etc. <p> Sentences m not known m (2n + 1) [AFP90] m known O log m+log n y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n [BHH92b] O log n n Arithmetic read-once formulas for jF j = o (n= log n) n <ref> [BHH92a] </ref> O log n n log jFj DFA with n states n not known n [Ang87b, RS89] O log n n known log n fl For k = O ( log n) we can obtain this result using k-term DNF formulas as the hypotheses for the equivalence queries. <p> We improve that transformation, somewhat surprisingly, to show that only n= log n equivalence queries are required. This improvement decreases the number of equivalence queries required to learn read-once formulas over certain bases [AHK89, BHH92b], arithmetic read-once formulas <ref> [BHH92a] </ref> and non-monotone switch configurations [RS90], giving bounds that are asymptotically tight. Next, in Section 10 we present our results for learning deterministic finite state automaton (DFAs). <p> Our results also apply for the class of non-monotone switch configurations [RS90] and arithmetic read-once formulas over the basis of addition, subtraction, multiplication and division over a field F <ref> [BHH92a] </ref>. DFAs These are deterministic finite automata representing regular languages over some alphabet . These languages can be viewed as functions from fl to f0; 1g, and an efficient exact identification algorithm is due to Angluin [Ang87b] and has since been improved by Rivest and Schapire [RS89]. <p> If not we can repeat with a different p i that agrees with y 0 on Y 1 [ Y 2 , and so on. The bounds in the statement of the theorem follow from straightforward analysis. 2 Applying the above technique to previous algorithms <ref> [AHK89, BHH92a, BHHK91, RS90] </ref> we obtain the following result. Corollary 18 The quantities E (ROF n (AN D; OR; N OT )), E (ROF n (B k )), and E (Switch Configurations n ) are all O (n= log n). <p> Update Y and A. 34 9.2 Arithmetic Read-once Formulas We now consider the class of arithmetic read-once formulas AROF (F) n (+; fi; =; ). There is a polynomial time identification algorithm for this class that uses membership queries and n equivalence queries <ref> [BHH92a] </ref>. When the size of the field F is at least 2n + 5 then the algorithm does not use equivalence queries at all (however, the algorithm is randomized in this case).
Reference: [BHH92b] <author> Nader H. Bshouty, Thomas R. Hancock, and Lisa Hellerstein. </author> <title> Learning Boolean read-once formulas with arbitrary symmetric and constant fan-in gates. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 1-15, </pages> <month> August </month> <year> 1992. </year> <note> To appear, Journal of Computer and Systems Sciences. </note>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata [Ang87b], Horn sentences [AFP90], read-once formulas <ref> [AHK89, BHH92a, BHH92b] </ref>, k-term DNF formulas [BR92], etc. <p> n O (kj) [AP92] m + 1 m + 1 m known m z m x Horn Sentences m not known m (2n + 1) [AFP90] m known O log m+log n y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n <ref> [BHH92b] </ref> O log n n Arithmetic read-once formulas for jF j = o (n= log n) n [BHH92a] O log n n log jFj DFA with n states n not known n [Ang87b, RS89] O log n n known log n fl For k = O ( log n) we can <p> We improve that transformation, somewhat surprisingly, to show that only n= log n equivalence queries are required. This improvement decreases the number of equivalence queries required to learn read-once formulas over certain bases <ref> [AHK89, BHH92b] </ref>, arithmetic read-once formulas [BHH92a] and non-monotone switch configurations [RS90], giving bounds that are asymptotically tight. Next, in Section 10 we present our results for learning deterministic finite state automaton (DFAs). <p> polynomial time exact identification algorithm for the formulas over the operators (or basis) f_; ^; :g [AHK89], and Bshouty, Hancock, and Hellerstein have extended this to more complicated sets of Boolean functions, including the set B k of all functions on k or fewer inputs (for an arbitrary constant k) <ref> [BHH92b] </ref>. Our results also apply for the class of non-monotone switch configurations [RS90] and arithmetic read-once formulas over the basis of addition, subtraction, multiplication and division over a field F [BHH92a]. DFAs These are deterministic finite automata representing regular languages over some alphabet .
Reference: [BHHK91] <author> Nader H. Bshouty, Thomas R. Hancock, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning Boolean read-once formulas with arbitrary symmetric and constant fan-in gates. </title> <type> Technical Report TR-92-020, </type> <institution> International Computer Science Institute, </institution> <year> 1991. </year> <note> To appear, Computational Complexity. </note>
Reference-contexts: For k-term DNF, k is the number of terms. Note that all upper bounds for an unknown size parameter can be used when the size parameter is known. Likewise, all lower bounds for a known size parameter apply when the size parameter is unknown. 3 reduction <ref> [BHHK91] </ref> shows that justifying assignments for n variables can be generated with n equivalence queries (as long as the class being learned satisfies the technical condition of being projection closed). We improve that transformation, somewhat surprisingly, to show that only n= log n equivalence queries are required. <p> This is achieved as a consequence of a more general result, showing that an algorithm that makes use of equivalence queries only to generate justifying assignments (defined below) needs to make only O (n= log n) queries. This is an improvement from a previous technique that uses n queries <ref> [AHK89, BHHK91] </ref>, and immediately gives us improved upper bounds for various classes of read-once formulas and non-monotone switch configurations. These upper bounds are tight from the work of Bshouty and Cleve [BC92]. In this section we consider the following classes of read-once formulas. <p> If not we can repeat with a different p i that agrees with y 0 on Y 1 [ Y 2 , and so on. The bounds in the statement of the theorem follow from straightforward analysis. 2 Applying the above technique to previous algorithms <ref> [AHK89, BHH92a, BHHK91, RS90] </ref> we obtain the following result. Corollary 18 The quantities E (ROF n (AN D; OR; N OT )), E (ROF n (B k )), and E (Switch Configurations n ) are all O (n= log n).
Reference: [BM76] <author> John A. Bondy and U. S. R. Murty. </author> <title> Graph theory with applications. </title> <publisher> Macmillan, </publisher> <address> London, </address> <year> 1977, c1976. </year>
Reference-contexts: For a graph G, we use *(G) to denote the number of edges in G. We use the following standard lemmas. (For example see Theorem 7.9 and Exercise 1.2.9 from Bondy and Murty <ref> [BM76] </ref>. Lemma 9 If a graph G is simple and contains no K j+1 , then *(G) *(T j;m ), where m is the number of vertices in G. Lemma 10 *(T j;m ) = mh h+1 where h = bm=jc.
Reference: [BR92] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In the membership and equivalence query model a number of interesting polynomial time algorithms have been presented to learn target classes such as deterministic finite automata [Ang87b], Horn sentences [AFP90], read-once formulas [AHK89, BHH92a, BHH92b], k-term DNF formulas <ref> [BR92] </ref>, etc. <p> As an example of the type of results we have obtained, consider the problem of learning a formula from the class of DNF formulas over n variables with at most k = O (log n) terms. Previously, the algorithm of Blum and Rudich <ref> [BR92] </ref> provided the best known upper bound of 2 O (k) log n for the minimum number of equivalence queries needed for exact identification. <p> A previous 2 Representation Previous New Lower Class Upper Bound Upper Bound Bound k-term DNF (k =O (log n) fl ) k not known 2 O (k) log n <ref> [BR92] </ref> k + 1 k + 1 [BC92] k known k k [BC92] Monotone DNF m not known m + 1 [Ang88] m fi (1) m !(1) m known m fi log nlog log m log nlog log m Read-k Sat-j DNF m not known n O (kj) [AP92] m + <p> Angluin gave a polynomial time identification algorithm for the special case when k is constant [Ang87a], and Blum and Rudich have since given a more efficient algorithm that runs in polynomial time for k = O (log n) <ref> [BR92] </ref>. Read-k Sat-j DNF A DNF formula is a read-k sat-j DNF formula if every variable appears at most k times, and every assignment satisfies at most j terms in the formula. <p> (h) If answer "yes" then return h Else let y be the counterexample If f (y) = 0, remove from h all terms t for which t (y) = 1 Until f (y) = 1 x y Until done Our algorithms are based on the algorithm of Blum and Rudich <ref> [BR92] </ref>, which we now briefly summarize. The key processing of the Blum-Rudich algorithm can be encapsulated as a procedure Produce-terms that when given a positive example x, produces c = 2 O (k) (log n) O (1) terms, one of which is in the target formula f 3 .
Reference: [Bsh93] <author> Nader H. Bshouty. </author> <title> Exact learning. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: Other open problems include applying these techniques to find tight bounds for other classes for which polynomial time exact identification algorithms are known, such as decision trees <ref> [Bsh93] </ref>, and read-twice DNF formulas [AP91, Han91, PR93]. Another interesting problem is how the use of membership queries can reduce the number of equivalence queries needed to learn classes that can in fact be learned with equivalence queries alone, such as k-DNF.
Reference: [Han91] <author> Thomas R. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Other open problems include applying these techniques to find tight bounds for other classes for which polynomial time exact identification algorithms are known, such as decision trees [Bsh93], and read-twice DNF formulas <ref> [AP91, Han91, PR93] </ref>. Another interesting problem is how the use of membership queries can reduce the number of equivalence queries needed to learn classes that can in fact be learned with equivalence queries alone, such as k-DNF.
Reference: [HKLW88] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 42-55. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: For previous work aimed mainly at proving tractability, this is not an important distinction, since a generic transformation generic transformation conversion from an algorithm that knows the size of the target to one that does not <ref> [HKLW88] </ref>. However for our precise bounds this difference can be important, and for some classes we obtain different results depending on whether or not the size of the target is known a priori.
Reference: [Lit88] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year> <month> 43 </month>
Reference-contexts: There is a very close relationship between this learning model and the on-line learning model <ref> [Lit88] </ref>. In the on-line learning model the learning session is divided into a set of trials where in each trial the learner is asked to make a prediction for some unknown instance x from the domain. <p> After the prediction is made, the learner is told whether the prediction is correct and is then able to use polynomial time (and here a polynomial number of membership queries) before proceeding to the next trial. Using a standard transformation <ref> [Ang88, Lit88] </ref> algorithms that use membership and equivalence queries can easily be converted to on-line learning algorithms that use membership queries. <p> Another motivation comes from the goal of minimizing the number of prediction mistakes in an on-line learning model. As we have mentioned, the model of learning with membership and equivalence queries is essentially equivalent to the on-line learning model when the learner is provided with membership queries <ref> [Ang88, Lit88] </ref>. The conversion of an algorithm A that uses membership queries and equivalence queries to an on-line algorithm A 0 works as follows. If algorithm A wants to perform some internal computation or perform a membership query then algorithm A 0 will perform the same task. <p> For Horn sentences the relationship is more complex. In this paper log denotes the logarithm base 2, and ln denotes the natural logarithm. 4 A Generalization of the Halving Algorithm In this section we consider a generalization of the halving algorithm <ref> [BF72, Lit88] </ref> in which we can reduce the number of equivalence queries required by allowing the learner to make membership queries. In the section we do not bound the computation time of the learner, and the hypotheses proposed need not come from C. <p> For k-DNF, Littlestone's algorithm uses O (k` log n) equivalence queries where ` is the number of terms <ref> [Lit88] </ref>. A lower bound is n log n. The gap is still open, even with using membership queries (though if ` is known, our generalized halving algorithm uses O d (log k`+log log n) queries). Acknowledgments We thank Tino Tamon for many useful discussions.
Reference: [MT92] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: Maass and Turan have also studied upper and lower bounds on the number of equivalence queries required for learning, both with and without membership queries <ref> [MT92] </ref>. However they do not restrict the learner to run in polynomial time, and they count only the total number of queries rather than the individual number of queries of each type.
Reference: [NN90] <author> Joseph Naor and Moni Naor. </author> <title> Small-bias probability spaces: Efficient constructions and applications. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 213-223, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Naor and Naor <ref> [NN90] </ref> give an explicit construction of an (n; k)universal set of size t = O (k2 3k log n). Lemma 2 Let S be an (n; k)-universal set, and let f be a k-term DNF formula.
Reference: [PR93] <author> Krishnan Pillapakkamnatt and Vijay Raghavan. </author> <title> Read-Twice DNF Formulas are Properly Learnable (Revised). </title> <type> Technical Report TR-CS-93-59, </type> <institution> Department of Computer Science, Vanderbilt University. </institution>
Reference-contexts: Other open problems include applying these techniques to find tight bounds for other classes for which polynomial time exact identification algorithms are known, such as decision trees [Bsh93], and read-twice DNF formulas <ref> [AP91, Han91, PR93] </ref>. Another interesting problem is how the use of membership queries can reduce the number of equivalence queries needed to learn classes that can in fact be learned with equivalence queries alone, such as k-DNF.
Reference: [RS89] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 411-420, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: y mn Read-once formulas over _; ^; : n [AHK89] O log n n over B k n [BHH92b] O log n n Arithmetic read-once formulas for jF j = o (n= log n) n [BHH92a] O log n n log jFj DFA with n states n not known n <ref> [Ang87b, RS89] </ref> O log n n known log n fl For k = O ( log n) we can obtain this result using k-term DNF formulas as the hypotheses for the equivalence queries. For the remaining cases, general DNF formulas are used for the equivalence queries. y With unlimited computation. <p> We now argue that is also of practical interest. As one example consider the situation in which the target function f measures some observable consequence of the learner's action. For example, Rivest and Schapire <ref> [RS89] </ref> motivate the problem of learning an unknown DFA by the problem of a robot trying to learn to navigate in an environment describable by a finite state machine. Here a membership query represents experimentation by the robot, followed by an observation of its perceived state after executing the experiment. <p> DFAs These are deterministic finite automata representing regular languages over some alphabet . These languages can be viewed as functions from fl to f0; 1g, and an efficient exact identification algorithm is due to Angluin [Ang87b] and has since been improved by Rivest and Schapire <ref> [RS89] </ref>. Here we let n denote the number of states in the target automaton.
Reference: [RS90] <author> Vijay Ragavan and Stephen R. Schach. </author> <title> Learning switch configurations. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 38-51. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: We improve that transformation, somewhat surprisingly, to show that only n= log n equivalence queries are required. This improvement decreases the number of equivalence queries required to learn read-once formulas over certain bases [AHK89, BHH92b], arithmetic read-once formulas [BHH92a] and non-monotone switch configurations <ref> [RS90] </ref>, giving bounds that are asymptotically tight. Next, in Section 10 we present our results for learning deterministic finite state automaton (DFAs). <p> Our results also apply for the class of non-monotone switch configurations <ref> [RS90] </ref> and arithmetic read-once formulas over the basis of addition, subtraction, multiplication and division over a field F [BHH92a]. DFAs These are deterministic finite automata representing regular languages over some alphabet . <p> If not we can repeat with a different p i that agrees with y 0 on Y 1 [ Y 2 , and so on. The bounds in the statement of the theorem follow from straightforward analysis. 2 Applying the above technique to previous algorithms <ref> [AHK89, BHH92a, BHHK91, RS90] </ref> we obtain the following result. Corollary 18 The quantities E (ROF n (AN D; OR; N OT )), E (ROF n (B k )), and E (Switch Configurations n ) are all O (n= log n).
Reference: [Sc91] <author> Robert E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: For the upper bound we use a modification of Schapire's algorithm <ref> [Sc91] </ref> which is itself a modification of Angluin's original algorithm [Ang87b]. For x 2 fl , we define fl (x) = 1 if and only if x 2 U .
Reference: [Val84] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 44 </month>
Reference-contexts: Their algorithm uses n fi (jk) equivalence queries. Monotone DNF This is the class of monotone DNF formulas. These were proved to be efficiently learnable by Valiant <ref> [Val84] </ref> and Angluin [Ang88]. We use m to denote the number of terms in the target formula. A polynomial time learning algorithm can use time polynomial in n and m. <p> O k k + mn 2 membership queries. Given this observation, there is a fairly straightforward exact identification algorithm due to Angluin [Ang88] (based on a previous PAC learning algorithm of Valiant <ref> [Val84] </ref>). We use each equivalence query to find a new prime implicant. Our current hypothesis is the disjunction of all known prime implicants (initially the always false hypothesis).
References-found: 27

