URL: ftp://thales.cs.umd.edu/pub/reports/eivna.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/reports/Contents.html
Root-URL: 
Title: Errors in Variables for Numerical Analysts  
Author: G. W. Stewart 
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-96-64 Department of Computer Science  
Pubnum: TR-3683  
Abstract: Numerical analysts and statisticians are both concerned with errors in their least squares and regression matrices. However, their respective errors have different sources and pose different problems. Consequently, the two camps speak different languages, even when their algorithms are mathematically the same. In this paper we consider some measurement error models and derive the basic formulas in a way that should be accessible to numerical analysts with a minimal background in probability theory. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. This work is supported in part by the National Science Foundation under grant CCR 95503126. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. J. Adcock. </author> <title> A problem in least squares. </title> <journal> The Analyst, </journal> <volume> 5 </volume> <pages> 53-54, </pages> <note> 1878. Cited in [2]. </note>
Reference-contexts: A refreshing exception to this difference in views is a model which Fuller [4, x2.3] calls the model with no error in the equation and whose numerical counterpart is the method of total least squares [6]. The case of a single variable was first considered by Adcock in 1878 <ref> [1] </ref>. In 1901, Pearson gave the general procedure for the case of uncorrelated errors [7]. In 1945 Tinter stated the estimation technique in full generality [12], and in 1951 Anderson gave an asymptotic analysis.
Reference: [2] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problems. </title> <publisher> SIAM, </publisher> <address> Philadel-phia, </address> <year> 1996. </year>
Reference-contexts: However, these extensions seldom depend on the notion of maximum likelihood. Consequently, the techniques developed in this paper may be as useful to statisticians as numerical analysts. A word on computations. It is sometimes recommended that the eigenvalue problem (5.2) be recast as a generalized singular value problem <ref> [2, x4.2] </ref> because the former is the moral equivalent of forming and solving the normal equations. The putative problem is that the eigenvalues of (5.2) are the squares of the generalized singular values. If the latter are small, the former may be lost to rounding error.
Reference: [3] <author> C. Eckart and G. Young. </author> <title> The approximation of one matrix by another of lower rank. </title> <journal> Psychometrika, </journal> <volume> 1 </volume> <pages> 211-218, </pages> <year> 1936. </year>
Reference-contexts: Lacking further knowledge it seems that the nearest such matrix will be most suitable. Now if ^ is the smallest singular value of ( ~ X ~y) , and u and v are the corresponding left and right singular vectors, then by the Schmidt-Eckart-Young theorem <ref> [8, 3] </ref>, the nearest rank-deficient matrix is given by ( ~ X ~y) ^uv T ; and v is its null vector. Now ^ 2 is the smallest eigenvalue of ( ~ X ~y) T ( ~ X ~y) and v is the corresponding eigenvector.
Reference: [4] <author> W. A. Fuller. </author> <title> Measurement Error Models. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: They must estimate the magnitude of the effects of these errors on the quantities of interest. Their tools are probability theory and large sample asymptotics. A refreshing exception to this difference in views is a model which Fuller <ref> [4, x2.3] </ref> calls the model with no error in the equation and whose numerical counterpart is the method of total least squares [6]. The case of a single variable was first considered by Adcock in 1878 [1]. <p> However, under appropriate hypotheses more can be said. The reader is referred to Fuller's book <ref> [4] </ref> for details. 5.
Reference: [5] <author> G. H. Golub, A. Hoffman, and G. W. Stewart. </author> <title> A generalization of the Eckart-Young matrix approximation theorem. </title> <journal> Linear Algebra and Its Applications, </journal> 88/89:317-327, 1987. Errors in the Variables <volume> 11 </volume>
Reference-contexts: We can, however, regard the semidefinite case as a limiting case of the definite case. This leads to project-and-approximate procedures such as are found in <ref> [5] </ref>. 7. Afterthoughts We have shown that the estimation formulas for measurement error models can be derived by looking for biases and eliminating them. It should be stressed that the statistical treatment of these models includes more than simple derivations | e.g., asymptotic distribution properties and hypotheses testing.
Reference: [6] <author> G. H. Golub and C. F. Van Loan. </author> <title> An analysis of the total least squares problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 883-893, </pages> <year> 1980. </year>
Reference-contexts: Their tools are probability theory and large sample asymptotics. A refreshing exception to this difference in views is a model which Fuller [4, x2.3] calls the model with no error in the equation and whose numerical counterpart is the method of total least squares <ref> [6] </ref>. The case of a single variable was first considered by Adcock in 1878 [1]. In 1901, Pearson gave the general procedure for the case of uncorrelated errors [7]. In 1945 Tinter stated the estimation technique in full generality [12], and in 1951 Anderson gave an asymptotic analysis.
Reference: [7] <author> K. Pearson. </author> <title> On lines and planes of closest fit to points in space. Philosophical Magazine, </title> <booktitle> 2 (sixth series):559-572, </booktitle> <year> 1901. </year>
Reference-contexts: The case of a single variable was first considered by Adcock in 1878 [1]. In 1901, Pearson gave the general procedure for the case of uncorrelated errors <ref> [7] </ref>. In 1945 Tinter stated the estimation technique in full generality [12], and in 1951 Anderson gave an asymptotic analysis.
Reference: [8] <author> E. Schmidt. </author> <title> Zur Theorie der linearen und nichtlinearen Integralgleichungen. I Teil. Entwicklung willkurlichen Funktionen nach System vorgeschriebener. </title> <journal> Mathematische Annalen, </journal> <volume> 63 </volume> <pages> 433-476, </pages> <year> 1907. </year>
Reference-contexts: Lacking further knowledge it seems that the nearest such matrix will be most suitable. Now if ^ is the smallest singular value of ( ~ X ~y) , and u and v are the corresponding left and right singular vectors, then by the Schmidt-Eckart-Young theorem <ref> [8, 3] </ref>, the nearest rank-deficient matrix is given by ( ~ X ~y) ^uv T ; and v is its null vector. Now ^ 2 is the smallest eigenvalue of ( ~ X ~y) T ( ~ X ~y) and v is the corresponding eigenvector.
Reference: [9] <author> R. J. Serfling. </author> <title> Approximation Theorems of Mathematical Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: covariance matrix E h i ! i * i ) = HH & He He 2 ! The sample covariance matrix will be written 1 (H e) (H e) = S HH s He He s 2 ! The sample covariance matrix converges with probability one to the covariance matrix <ref> [9] </ref>. 3. Behavior of Least Squares Estimates Before proceeding to specific measurement error models, it is instructive to con sider the effects of the perturbations (2.1) on the ordinary least squares estimators.
Reference: [10] <author> G. W. Stewart. </author> <title> Perturbation theory and least squares with errors in the variables. </title> <editor> In P. J. Brown and W. A. Fuller, editors, </editor> <booktitle> Contemporary Mathematics 112: Statistical Analysis of Measurement Error Models and Applications, </booktitle> <pages> pages 171-181, </pages> <address> Providence, Rhode Island, 1990. </address> <publisher> American Mathematical Society. </publisher>
Reference-contexts: be shown [11] that E (k Gk 2 ) 1 HH X y k 2 : (3.2) Thus if we have a rough estimate of the size of the errors in X we can tell when the ordinary least squares estimate is approximately unbiased. (For more on this model see <ref> [10] </ref>.) Let us now investigate what happens with larger errors. We begin by examining the augmented cross-product matrix.
Reference: [11] <author> G. W. Stewart. </author> <title> Stochastic perturbation theory. </title> <journal> SIAM Review, </journal> <volume> 32 </volume> <pages> 576-610, </pages> <year> 1990. </year>
Reference-contexts: But without further information we cannot do this anyway. We must, however, be able to determine when G T is negligible. If we write G T = G T + O (kHk 2 ); then it can be shown <ref> [11] </ref> that E (k Gk 2 ) 1 HH X y k 2 : (3.2) Thus if we have a rough estimate of the size of the errors in X we can tell when the ordinary least squares estimate is approximately unbiased. (For more on this model see [10].) Let us
Reference: [12] <author> G. Tintner. </author> <title> A note on multicollinearity and multiple regression. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 16 </volume> <pages> 304-308, </pages> <year> 1945. </year>
Reference-contexts: The case of a single variable was first considered by Adcock in 1878 [1]. In 1901, Pearson gave the general procedure for the case of uncorrelated errors [7]. In 1945 Tinter stated the estimation technique in full generality <ref> [12] </ref>, and in 1951 Anderson gave an asymptotic analysis. The connection between the statistical model and total least squares is not immediately obvious, since statisticians approach the problem through maximum likelihood estimation whereas numerical analysts use the approximation properties of the singular value decomposition.
References-found: 12

