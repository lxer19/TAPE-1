URL: http://www.cs.cmu.edu/afs/cs/academic/class/15883-s97/Web/handouts/kaelbling96a.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/academic/class/15883-s97/Web/
Root-URL: 
Email: lpk@cs.brown.edu  mlittman@cs.brown.edu  awm@cs.cmu.edu  
Title: Reinforcement Learning: A Survey  
Author: Leslie Pack Kaelbling Michael L. Littman Andrew W. Moore Smith Hall , 
Date: 1910,  
Address: Box  Providence, RI 02912-1910 USA  5000 Forbes Avenue Pittsburgh, PA 15213 USA  
Affiliation: Computer Science Department,  Brown University  Carnegie Mellon University,  
Note: Journal of Artificial Intelligence Research 4 (1996) 237-285 Submitted 9/95; published 5/96  
Abstract: This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. H., & Littman, M. L. </author> <year> (1990). </year> <title> Generalization and scaling in reinforcement learning. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pp. </pages> <address> 550-557 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods (Sutton, 1984). CRBP The complementary reinforcement backpropagation algorithm <ref> (Ackley & Littman, 1990) </ref> (crbp) consists of a feed-forward network mapping an encoding of the state to an encoding of the action.
Reference: <author> Albus, J. S. </author> <year> (1975). </year> <title> A new approach to manipulator control: Cerebellar model articulation controller (cmac). </title> <journal> Journal of Dynamic Systems, Measurement and Control, </journal> <volume> 97, </volume> <pages> 220-227. </pages>
Reference-contexts: In Sutton's com 262 Reinforcement Learning: A Survey parative experiments with Boyan and Moore's counter-examples, he changes four aspects of the experiments: 1. Small changes to the task specifications. 2. A very different kind of function approximator (CMAC <ref> (Albus, 1975) </ref>) that has weak generalization. 3. A different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of value iteration. 4. A different training regime. Boyan and Moore sampled states uniformly in state space, whereas Sutton's method sampled along empirical trajectories.
Reference: <author> Albus, J. S. </author> <year> (1981). </year> <title> Brains, Behavior, and Robotics. </title> <publisher> BYTE Books, Subsidiary of McGraw-Hill, </publisher> <address> Peterborough, New Hampshire. </address>
Reference-contexts: Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC <ref> (Albus, 1981) </ref>, and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods.
Reference: <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Ashar, R. R. </author> <year> (1994). </year> <title> Hierarchical learning in stochastic domains. </title> <type> Master's thesis, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference-contexts: The HDG algorithm works by analogy with navigation in a harbor. The environment is partitioned (a priori, but more recent work <ref> (Ashar, 1994) </ref> addresses the case of learning the partition) into a set of regions whose centers are known as "landmarks." If the agent is 266 Reinforcement Learning: A Survey currently in the same region as the goal, then it uses low-level actions to move to the goal.
Reference: <author> Baird, L. </author> <year> (1995). </year> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 30-37 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values. Baird's residual gradient technique <ref> (Baird, 1995) </ref> provides guaranteed convergence to locally optimal solutions. Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995) report that their counter-examples can be made to work with problem-specific hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains.
Reference: <author> Baird, L. C., & Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Tech. rep. </type> <institution> WL-TR-93-1147, Wright-Patterson Air Force Base Ohio: Wright Laboratory. </institution> <note> 276 Reinforcement Learning: A Survey Barto, </note> <author> A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 (1), </volume> <pages> 81-138. </pages>
Reference-contexts: Training such a network is not conceptually difficult, but using the network to find the optimal action can be a challenge. One method is to do a local gradient-ascent search on the action in order to find one with high value <ref> (Baird & Klopf, 1993) </ref>. Gullapalli (1990, 1992) has developed a "neural" reinforcement-learning unit for use in continuous action spaces. The unit generates actions with a normal distribution; it adjusts the mean and variance based on previous experience.
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-13 (5), </volume> <pages> 834-846. </pages>
Reference-contexts: This class of algorithms is known as temporal difference methods (Sutton, 1988). We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model. 4.1 Adaptive Heuristic Critic and TD () The adaptive heuristic critic algorithm is an adaptive version of policy iteration <ref> (Barto, Sutton, & Anderson, 1983) </ref> in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called T D (0). A block diagram for this approach is given in Figure 4.
Reference: <author> Bellman, R. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finite-horizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy <ref> (Bellman, 1957) </ref>. We will speak of the optimal value of a state|it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy.
Reference: <author> Berenji, H. R. </author> <year> (1991). </year> <title> Artificial neural networks and approximate reasoning for intelligent control in space. </title> <booktitle> In American Control Conference, </booktitle> <pages> pp. 1075-1080. </pages>
Reference: <author> Berry, D. A., & Fristedt, B. </author> <year> (1985). </year> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK. </address>
Reference-contexts: A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret <ref> (Berry & Fristedt, 1985) </ref>. It penalizes mistakes wherever they occur during the run. <p> The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature <ref> (Berry & Fristedt, 1985) </ref>. The agent is in a room with a collection of k gambling machines (each called a "one-armed bandit" in colloquial English). The agent is permitted a fixed number of pulls, h. Any arm may be pulled on each turn. <p> Although it is instructive, the methods it provides do not scale well to more complex problems. 2.1.1 Dynamic-Programming Approach If the agent is going to be acting for a total of h steps, it can use basic Bayesian reasoning to solve for an optimal strategy <ref> (Berry & Fristedt, 1985) </ref>. This requires an assumed prior joint distribution for the parameters fp i g, the most natural of which is that each p i is independently uniformly distributed between 0 and 1.
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another important result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged <ref> (Bertsekas, 1987) </ref>. And in practice, the greedy policy is often optimal long before the value function has converged. Value iteration is very flexible.
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts. </address> <note> Volumes 1 and 2. </note>
Reference-contexts: in which the agent is supposed to take actions that optimize its long-run average reward: lim E ( h t=0 Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 <ref> (Bertsekas, 1995) </ref>. One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of which does not.
Reference: <author> Bertsekas, D. P., & Casta~non, D. A. </author> <year> (1989). </year> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34 (6), </volume> <pages> 589-598. </pages>
Reference: <author> Bertsekas, D. P., & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Box, G. E. P., & Draper, N. R. </author> <year> (1987). </year> <title> Empirical Model-Building and Response Surfaces. </title> <publisher> Wiley. </publisher>
Reference-contexts: Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods <ref> (Box & Draper, 1987) </ref>, which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible. 2.3 More General Problems When there are multiple states, but reinforcement is still immediate, then any of the
Reference: <author> Boyan, J. A., & Moore, A. W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 Cambridge, </booktitle> <address> MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Burghes, D., & Graham, A. </author> <year> (1980). </year> <title> Introduction to Control Theory including Optimal Control. </title> <publisher> Ellis Horwood. </publisher>
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Chapman, D., & Kaelbling, L. P. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence Sydney, </booktitle> <address> Australia. </address>
Reference-contexts: Decision Trees In environments that are characterized by a set of boolean or discrete-valued variables, it is possible to learn compact decision trees for representing Q values. The G-learning algorithm <ref> (Chapman & Kaelbling, 1991) </ref>, works as follows. It starts by assuming that no partitioning is necessary and tries to learn Q values for the entire environment as if it were one state.
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 183-188 San Jose, CA. </address> <note> AAAI Press. 277 Kaelbling, </note> <author> Littman, & Moore Chrisman, L., & Littman, M. </author> <year> (1993). </year> <title> Hidden state and short-term memory.. Presentation at Reinforcement Learning Workshop, </title> <booktitle> Machine Learning Conference. </booktitle>
Reference: <author> Cichosz, P., & Mulawka, J. J. </author> <year> (1995). </year> <title> Fast and efficient reinforcement learning with truncated temporal differences. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 99-107 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is computationally more expensive to execute the general T D (), though it often converges considerably faster for large (Dayan, 1992; Dayan & Sejnowski, 1994). There has been some recent work on making the updates more efficient <ref> (Cichosz & Mulawka, 1995) </ref> and on changing the definition to make T D () more consistent with the certainty-equivalent method (Singh & Sutton, 1996), which is discussed in Section 5.1. 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm
Reference: <author> Cleveland, W. S., & Delvin, S. J. </author> <year> (1988). </year> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83 (403), </volume> <pages> 596-610. </pages>
Reference: <author> Cliff, D., & Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 (2), </volume> <pages> 101-150. </pages>
Reference: <author> Condon, A. </author> <year> (1992). </year> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96 (2), </volume> <pages> 203-224. </pages>
Reference-contexts: Each iteration can be performed in O (jAjjSj 2 ) steps, or faster if there is sparsity in the transition function. However, the number of iterations required can grow exponentially in the discount factor <ref> (Condon, 1992) </ref>; as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O (jAjjSj 2 + jSj 3 ) can be prohibitive.
Reference: <author> Connell, J., & Mahadevan, S. </author> <year> (1993). </year> <title> Rapid task learning for real robots. In Robot Learning. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Most interesting examples of robotic reinforcement learning employ this technique to some extent <ref> (Connell & Mahadevan, 1993) </ref>. reflexes: One thing that keeps agents that know nothing from learning anything is that they have a hard time even finding the interesting parts of the space; they wander 275 Kaelbling, Littman, & Moore around at random never getting near the goal, or they are always "killed"
Reference: <author> Crites, R. H., & Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), </editor> <booktitle> Neural Information Processing Systems 8. </booktitle>
Reference-contexts: The performance of the Q-learned policies were almost as good as a simple hand-crafted controller for the job. 4. Q-learning has been used in an elevator dispatching task <ref> (Crites & Barto, 1996) </ref>. The problem, which has been implemented in simulation only at this stage, involved four elevators servicing ten floors. The objective was to minimize the average squared wait time for passengers, discounted into future time.
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8 (3), </type> <pages> 341-362. </pages>
Reference-contexts: We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. * Eventual convergence to optimal. Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior <ref> (Watkins & Dayan, 1992) </ref>. This is reassuring, but useless in practical terms.
Reference: <author> Dayan, P., & Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5 San Mateo, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P., & Sejnowski, T. J. </author> <year> (1994). </year> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14 (3). </volume>
Reference-contexts: The robot has three motors indicated by torque vectors t 1 ; t 2 ; t 3 . Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go <ref> (Schraudolph, Dayan, & Sejnowski, 1994) </ref> and Chess (Thrun, 1995). It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning.
Reference: <author> Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. </author> <year> (1993). </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence Washington, </booktitle> <address> DC. </address>
Reference: <author> D'Epenoux, F. </author> <year> (1963). </year> <title> A probabilistic production and inventory problem. </title> <journal> Management Science, </journal> <volume> 10, </volume> <pages> 98-108. </pages>
Reference: <author> Derman, C. </author> <year> (1970). </year> <title> Finite State Markovian Decision Processes. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Dorigo, M., & Bersini, H. </author> <year> (1994). </year> <title> A comparison of q-learning and classifier systems. </title> <booktitle> In From Animals to Animats: Proceedings of the Third International Conference on the Simulation of Adaptive Behavior Brighton, </booktitle> <address> UK. </address>
Reference-contexts: In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems <ref> (Dorigo & Bersini, 1994) </ref>. Cliff and Ross (1994) start with Wilson's zeroth 268 Reinforcement Learning: A Survey level classifier system (Wilson, 1995) and add one and two-bit memory registers.
Reference: <author> Dorigo, M., & Colombetti, M. </author> <year> (1994). </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71 (2), </volume> <pages> 321-370. </pages> <note> 278 Reinforcement Learning: </note> <editor> A Survey Dorigo, M. </editor> <year> (1995). </year> <title> Alecsys and the AutonoMouse: Learning to control a real robot by distributed classifier systems. </title> <journal> Machine Learning, </journal> <volume> 19. </volume>
Reference-contexts: In spite of some early successes, the original design does not appear to handle partially observed environments robustly. Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems <ref> (Dorigo & Bersini, 1994) </ref>. Cliff and Ross (1994) start with Wilson's zeroth 268 Reinforcement Learning: A Survey level classifier system (Wilson, 1995) and add one and two-bit memory registers.
Reference: <author> Fiechter, C.-N. </author> <year> (1994). </year> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pp. 88-97. </pages> <institution> Association of Computing Machinery. </institution>
Reference: <author> Gittins, J. C. </author> <year> (1989). </year> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY. </address>
Reference-contexts: fl values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon. 2.1.2 Gittins Allocation Indices Gittins gives an "allocation index" method for finding the optimal choice of action at each step in k-armed bandit problems <ref> (Gittins, 1989) </ref>. The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, n, versus the number of times it has paid off, w.
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> MA. </address>
Reference: <author> Gordon, G. J. </author> <year> (1995). </year> <title> Stable function approximation in dynamic programming. </title> <editor> In Priedi-tis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 261-268 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gullapalli, V. </author> <year> (1990). </year> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 671-692. </pages>
Reference: <author> Gullapalli, V. </author> <year> (1992). </year> <title> Reinforcement learning and its application to control. </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Hilgard, E. R., & Bower, G. H. </author> <year> (1975). </year> <title> Theories of Learning (fourth edition). </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: The probabilities of taking different actions would be adjusted according to their previous successes and failures. An example, which stands among a set of algorithms independently developed in the mathematical psychology literature <ref> (Hilgard & Bower, 1975) </ref>, is the linear reward-inaction algorithm. Let p i be the agent's probability of taking action i. * When action a i succeeds, p i := p i + ff (1 p i ) * When action a i fails, p j remains unchanged (for all j). <p> The necessary bias can come in a variety of forms, including the following: shaping: The technique of shaping is used in training animals <ref> (Hilgard & Bower, 1975) </ref>; a teacher presents very simple problems to solve first, then gradually exposes the learner to more complex problems.
Reference: <author> Hoffman, A. J., & Karp, R. M. </author> <year> (1966). </year> <title> On nonterminating stochastic games. </title> <journal> Management Science, </journal> <volume> 12, </volume> <pages> 359-370. </pages>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference: <author> Howard, R. A. </author> <year> (1960). </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 (6). </volume>
Reference: <author> Jaakkola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1995). </year> <title> Monte-carlo reinforcement learning in non-Markovian decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 Cambridge, </booktitle> <address> MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Kaelbling, L. P. </author> <year> (1993a). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning Amherst, </booktitle> <address> MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kaelbling, L. P. </author> <year> (1993b). </year> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method <ref> (Kaelbling, 1993b) </ref> (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 Randomized Strategies Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose <p> In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The cascade method <ref> (Kaelbling, 1993b) </ref> allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 Delayed Reward Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning.
Reference: <author> Kaelbling, L. P. </author> <year> (1994a). </year> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning, </journal> <volume> 15 (3). </volume> <pages> 279 Kaelbling, </pages> <note> Littman, </note> & <author> Moore Kaelbling, L. P. </author> <year> (1994b). </year> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning, </journal> <volume> 15 (3). </volume>
Reference-contexts: Taking inspiration from mainstream machine learning work, Kaelbling developed two algorithms for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive 261 Kaelbling, Littman, & Moore the generalization process (Kaelbling, 1994b); the other searches the space of syntactic descriptions of functions using a simple generate-and-test method <ref> (Kaelbling, 1994a) </ref>. The restriction to a single boolean output makes these techniques difficult to apply. In very benign learning situations, it is possible to extend this approach to use a collection of learners to independently learn the individual bits that make up a complex output.
Reference: <author> Kirman, J. </author> <year> (1994). </year> <title> Predicting Real-Time Planner Performance by Domain Characterization. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference: <author> Koenig, S., & Simmons, R. G. </author> <year> (1993). </year> <title> Complexity analysis of real-time reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 99-105 Menlo Park, California. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: learning phase and the acting phase. * How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly <ref> (Koenig & Simmons, 1993) </ref>. See Figure 5 for an example. * The possibility of changes in the environment is also problematic.
Reference: <author> Kumar, P. R., & Varaiya, P. P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference-contexts: This method is known as certainty equivlance <ref> (Kumar & Varaiya, 1986) </ref>.
Reference: <author> Lee, C. C. </author> <year> (1991). </year> <title> A self learning rule-based controller employing approximate reasoning and neural net concepts. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 6 (1), </volume> <pages> 71-93. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Decision Trees In environments that are characterized by a set of boolean or discrete-valued variables, it is possible to learn compact decision trees for representing Q values. The G-learning algorithm <ref> (Chapman & Kaelbling, 1991) </ref>, works as follows. It starts by assuming that no partitioning is necessary and tries to learn Q values for the entire environment as if it were one state. <p> Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up <ref> (Lin, 1991) </ref>, and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995). local reinforcement signals: Whenever possible, agents should be given reinforcement signals that are local. <p> In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly (Mataric, 1994). imitation: An agent can learn by "watching" another agent perform the task <ref> (Lin, 1991) </ref>. For real robots, this requires perceptual abilities that are not yet available.
Reference: <author> Lin, L.-J. </author> <year> (1993a). </year> <title> Hierachical learning of robot skills by reinforcement. </title> <booktitle> In Proceedings of the International Conference on Neural Networks. </booktitle>
Reference: <author> Lin, L.-J. </author> <year> (1993b). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method <ref> (Kaelbling, 1993b) </ref> (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 Randomized Strategies Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose <p> In general, however, that approach suffers from the problem of very unreliable reinforcement: if a single learner generates an inappropriate output bit, all of the learners receive a low reinforcement value. The cascade method <ref> (Kaelbling, 1993b) </ref> allows a collection of learners to be trained collectively to generate appropriate joint outputs; it is considerably more reliable, but can require additional computational effort. 6.1.2 Delayed Reward Another method to allow reinforcement-learning techniques to be applied in large state spaces is modeled on value iteration and Q-learning.
Reference: <author> Lin, L.-J., & Mitchell, T. M. </author> <year> (1992). </year> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Tech. rep. </type> <institution> CMU-CS-92-138, Carnegie Mellon University, School of Computer Science. </institution>
Reference: <author> Littman, M. L. </author> <year> (1994a). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 157-163 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games <ref> (Littman, 1994a) </ref> and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system (Samuel, 1959).
Reference: <author> Littman, M. L. </author> <year> (1994b). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title>
Reference-contexts: This approach may yield plausible results in some cases, but again, there are no guarantees. It is reasonable, though, to ask what the optimal policy (mapping from observations to actions, in this case) is. It is NP-hard <ref> (Littman, 1994b) </ref> to find this mapping, and even the best mapping can have very poor performance.
Reference: <editor> In Cliff, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior Cambridge, </booktitle> <address> MA. </address> <publisher> The MIT Press. </publisher>
Reference: <author> Littman, M. L., Cassandra, A., & Kaelbling, L. P. </author> <year> (1995a). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 362-370 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Littman, M. L., Dean, T. L., & Kaelbling, L. P. </author> <year> (1995b). </year> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI-95) Montreal, </booktitle> <address> Quebec, Canada. </address>
Reference-contexts: However, in the worst case the number of iterations grows polynomially in 1=(1 fl), so the convergence rate slows considerably as the discount factor approaches 1 <ref> (Littman, Dean, & Kaelbling, 1995b) </ref>. 249 Kaelbling, Littman, & Moore 3.2.2 Policy Iteration The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. <p> However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP <ref> (Littman et al., 1995b) </ref>. 3.2.3 Enhancement to Value Iteration and Policy Iteration In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. <p> In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O (jAjjSj 2 + jSj 3 ) can be prohibitive. There is no known tight worst-case bound available for policy iteration <ref> (Littman et al., 1995b) </ref>. Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some practictioners (Rust, 1996).
Reference: <author> Lovejoy, W. S. </author> <year> (1991). </year> <title> A survey of algorithmic methods for partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28, </volume> <month> 47-66. </month> <title> 280 Reinforcement Learning: A Survey Maes, </title> <editor> P., & Brooks, R. A. </editor> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 796-802. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mahadevan, S. </author> <year> (1994). </year> <title> To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 164-172 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning <ref> (Mahadevan, 1994) </ref>. With that in mind, researchers have studied the problem of learning optimal average-reward policies. Mahadevan (1996) surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms.
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <volume> 22 (1). </volume>
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1991a). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence Anaheim, </booktitle> <address> CA. </address>
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1991b). </year> <title> Scaling reinforcement learning to robotics by exploiting the subsumption architecture. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pp. 328-332. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1994). </year> <title> Reward functions for accelerated learning. </title> <editor> In Cohen, W. W., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In applications in which it is possible to compute a gradient, rewarding the agent for taking steps up the gradient, rather than just for achieving the final goal, can speed learning significantly <ref> (Mataric, 1994) </ref>. imitation: An agent can learn by "watching" another agent perform the task (Lin, 1991). For real robots, this requires perceptual abilities that are not yet available.
Reference: <author> McCallum, A. K. </author> <year> (1995). </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Rochester. </institution>
Reference-contexts: McCallum (1995) describes the "utile suffix memory" which learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain <ref> (McCallum, 1995) </ref>. Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 190-196 Amherst, Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference Machine Learning, </booktitle> <pages> pp. </pages> <address> 387-395 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: McCallum (1995) describes the "utile suffix memory" which learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy. This system has had excellent results in a very complex driving-simulation domain <ref> (McCallum, 1995) </ref>. Ring (1994) has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations.
Reference: <author> Meeden, L., McGraw, G., & Blank, D. </author> <year> (1993). </year> <title> Emergent control and planning in an autonomous vehicle. </title> <editor> In Touretsky, D. (Ed.), </editor> <booktitle> Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pp. 735-740. </pages> <publisher> Lawerence Erlbaum Associates, </publisher> <address> Hills-dale, NJ. </address>
Reference: <author> Millan, J. d. R. </author> <year> (1996). </year> <title> Rapid, safe, and incremental learning of navigation strategies. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 26 (3). </volume>
Reference: <author> Monahan, G. E. </author> <year> (1982). </year> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28, </volume> <pages> 1-16. </pages>
Reference: <author> Moore, A. W. </author> <year> (1991). </year> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued spaces. </title> <booktitle> In Proc. Eighth International Machine Learning Workshop. </booktitle> <pages> 281 Kaelbling, </pages> <note> Littman, </note> & <author> Moore Moore, A. W. </author> <year> (1994). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <editor> In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. </pages> <address> 711-718 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It cannot, however, acquire partitions in which attributes are only significant in combination (such as those needed to solve parity problems). Variable Resolution Dynamic Programming The VRDP algorithm <ref> (Moore, 1991) </ref> enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A kd-tree (similar to a decision tree) is used to partition state space into coarse regions.
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1992). </year> <title> An investigation of memory-based function ap-proximators for learning control. </title> <type> Tech. rep., </type> <institution> MIT Artifical Intelligence Laboratory, </institution> <address> Cambridge, MA. </address>
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13. </volume>
Reference-contexts: Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping <ref> (Moore & Atkeson, 1993) </ref>. 2.2.2 Randomized Strategies Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random. <p> It is particularly unhelpful when the goal has just been reached or when the agent is stuck in a dead end; it continues to update random state-action pairs, rather than concentrating on the "interesting" parts of the state space. These problems are addressed by prioritized sweeping <ref> (Moore & Atkeson, 1993) </ref> and Queue-Dyna (Peng & Williams, 1993), which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail. <p> But how well can it do? The resulting problem is not Markovian, and Q-learning cannot be guaranteed to converge. Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate <ref> (Chrisman & 267 Kaelbling, Littman, & Moore Littman, 1993) </ref>. It is possible to use a model-based approach, however; act according to some policy and gather statistics about the transitions between observations, then solve for the optimal policy based on those observations.
Reference: <author> Moore, A. W., Atkeson, C. G., & Schaal, S. </author> <year> (1995). </year> <title> Memory-based learning for control. </title> <type> Tech. rep. </type> <institution> CMU-RI-TR-95-18, CMU Robotics Institute. </institution>
Reference-contexts: Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991). CMAC (Albus, 1981), and local memory-based methods <ref> (Moore, Atkeson, & Schaal, 1995) </ref>, such as generalizations of nearest neighbor methods.
Reference: <author> Narendra, K., & Thathachar, M. A. L. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Narendra, K. S., & Thathachar, M. A. L. </author> <year> (1974). </year> <title> Learning automata|a survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 4 (4), </volume> <pages> 323-334. </pages>
Reference-contexts: Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making ff small <ref> (Narendra & Thathachar, 1974) </ref>. There is no literature on the regret of this algorithm. 245 Kaelbling, Littman, & Moore 2.2 Ad-Hoc Techniques In reinforcement-learning practice, some simple, ad hoc strategies have been popular.
Reference: <author> Peng, J., & Williams, R. J. </author> <year> (1993). </year> <title> Efficient learning and planning within the Dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1 (4), </volume> <pages> 437-454. </pages>
Reference-contexts: These problems are addressed by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna <ref> (Peng & Williams, 1993) </ref>, which are two independently-developed but very similar techniques. We will describe prioritized sweeping in some detail.
Reference: <author> Peng, J., & Williams, R. J. </author> <year> (1994). </year> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 226-232 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Q-learning can also be extended to update states that occurred more than one step previously, as in T D () <ref> (Peng & Williams, 1994) </ref>. When the Q values are nearly converged to their optimal values, it is appropriate for the agent to act greedily, taking, in each situation, the action with the highest Q value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made.
Reference: <author> Pomerleau, D. A. </author> <year> (1993). </year> <title> Neural network perception for mobile robot guidance. </title> <publisher> Kluwer Academic Publishing. </publisher>
Reference-contexts: For real robots, this requires perceptual abilities that are not yet available. But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel <ref> (Pomerleau, 1993) </ref>. problem decomposition: Decomposing a huge learning problem into a collection of smaller ones, and providing useful reinforcement signals for the subproblems is a very powerful technique for biasing learning.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference: <author> Puterman, M. L., & Shin, M. C. </author> <year> (1978). </year> <title> Modified policy iteration algorithms for discounted Markov decision processes. </title> <journal> Management Science, </journal> <volume> 24, </volume> <pages> 1127-1137. </pages>
Reference-contexts: Arguments have been put forth to the effect that each approach is better for large problems. Puterman's modified policy iteration algorithm <ref> (Puterman & Shin, 1978) </ref> provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of V . <p> In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of O (jAjjSj 2 + jSj 3 ) can be prohibitive. There is no known tight worst-case bound available for policy iteration (Littman et al., 1995b). Modified policy iteration <ref> (Puterman & Shin, 1978) </ref> seeks a trade-off between cheap and effective iterations and is preferred by some practictioners (Rust, 1996). Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux, 1963; Hoffman & Karp, 1966).
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> Ph.D. thesis, </type> <institution> University of Texas at Austin, Austin, Texas. </institution>
Reference: <author> Rude, U. </author> <year> (1993). </year> <title> Mathematical and computational techniques for multilevel adaptive methods. </title> <institution> Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania. </institution>
Reference-contexts: Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution <ref> (Rude, 1993) </ref>. State aggre gation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas & Casta~non, 1989). 250 Reinforcement Learning: A Survey 3.2.4 Computational Complexity Value iteration works by producing successive approximations of the optimal value function.
Reference: <editor> Rumelhart, D. E., & McClelland, J. L. (Eds.). </editor> <booktitle> (1986). Parallel Distributed Processing: Explorations in the microstructures of cognition. Volume 1: Foundations. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Some of these mappings, such as transitions and immediate rewards, can be learned using straightforward supervised learning, and can be handled using any of the wide variety of function-approximation techniques for supervised learning that support noisy training examples. Popular techniques include various neural-network methods <ref> (Rumelhart & McClelland, 1986) </ref>, fuzzy logic (Berenji, 1991; Lee, 1991). CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods.
Reference: <author> Rummery, G. A., & Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Tech. rep. </type> <institution> CUED/F-INFENG/TR166, Cambridge University. </institution> <note> 282 Reinforcement Learning: A Survey Rust, J. </note> <year> (1996). </year> <title> Numerical dynamic programming in economics. In Handbook of Computational Economics. </title> <publisher> Elsevier, North Holland. </publisher>
Reference-contexts: Small changes to the task specifications. 2. A very different kind of function approximator (CMAC (Albus, 1975)) that has weak generalization. 3. A different learning algorithm: SARSA <ref> (Rummery & Niranjan, 1994) </ref> instead of value iteration. 4. A different training regime. Boyan and Moore sampled states uniformly in state space, whereas Sutton's method sampled along empirical trajectories. There are intuitive reasons to believe that the fourth factor is particularly important, but more careful research is needed.
Reference: <author> Sage, A. P., & White, C. C. </author> <year> (1977). </year> <title> Optimum Systems Control. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Between each trial, a form of dynamic programming specific to linear control policies and locally linear transitions was used to improve the policy. The form of dynamic programming is known as linear-quadratic-regulator design <ref> (Sage & White, 1977) </ref>. 272 Reinforcement Learning: A Survey 2. Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes large boxes for extended periods of time. Box-pushing is a well-known difficult robotics problem, characterized by immense uncertainty in the results of actions.
Reference: <author> Salganicoff, M., & Ungar, L. H. </author> <year> (1995). </year> <title> Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 480-487 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward <ref> (Salganicoff & Ungar, 1995) </ref>.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> 211-229. </pages> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York 1963. </address>
Reference-contexts: Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement learning in these environments. One application, spectacularly far ahead of its time, was Samuel's checkers playing system <ref> (Samuel, 1959) </ref>. This learned a value function represented by a linear function approximator, and employed a training scheme similar to the updates used in value iteration, temporal differences and Q-learning. More recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm to backgammon.
Reference: <author> Schaal, S., & Atkeson, C. </author> <year> (1994). </year> <title> Robot juggling: An implementation of memory-based learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14. </volume>
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for multi-agent learning and incremental self-improvement in unrestricted environments. </title> <editor> In Yao, X. (Ed.), </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. </publisher>
Reference-contexts: This approach has been taken by work in genetic algorithms and genetic programming, c fl1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Kaelbling, Littman, & Moore as well as some more novel search techniques <ref> (Schmidhuber, 1996) </ref>. The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991a). </year> <title> Curious model-building control systems. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, Singapore, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 1458-1463. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration <ref> (Schmidhuber, 1991a) </ref>, and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 Randomized Strategies Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991b). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pp. </pages> <address> 500-506 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. </author> <year> (1994). </year> <title> Temporal difference learning of position evaluation in the game of Go. </title> <editor> In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. </pages> <address> 817-824 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The robot has three motors indicated by torque vectors t 1 ; t 2 ; t 3 . Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go <ref> (Schraudolph, Dayan, & Sejnowski, 1994) </ref> and Chess (Thrun, 1995). It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning.
Reference: <author> Schrijver, A. </author> <year> (1986). </year> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, NY. </address>
Reference-contexts: There is no known tight worst-case bound available for policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some practictioners (Rust, 1996). Linear programming <ref> (Schrijver, 1986) </ref> is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux, 1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality linear-programming packages are available, although the time and space requirements can still be quite high.
Reference: <author> Schwartz, A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 298-305 Amherst, Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. </author> <year> (1994). </year> <title> Robust reinforcement learning in motion planning. </title> <editor> In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. </pages> <address> 655-662 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S. P., & Sutton, R. S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <volume> 22 (1). </volume> <pages> 283 Kaelbling, </pages> <note> Littman, </note> & <author> Moore Singh, S. P. </author> <year> (1992a). </year> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 202-207 San Jose, CA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D () more consistent with the certainty-equivalent method <ref> (Singh & Sutton, 1996) </ref>, which is discussed in Section 5.1. 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement.
Reference: <author> Singh, S. P. </author> <year> (1992b). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 323-340. </pages>
Reference: <author> Singh, S. P. </author> <year> (1993). </year> <title> Learning to Solve Markovian Decision Processes. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Massachusetts. </institution> <note> Also, CMPSCI Technical Report 93-77. </note>
Reference-contexts: This type of sample backup <ref> (Singh, 1993) </ref> is critical to the operation of the model-free methods discussed in the next section. The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions.
Reference: <author> Stengel, R. F. </author> <year> (1986). </year> <title> Stochastic Optimal Control. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. </title> <editor> In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), </editor> <booktitle> Neural Information Processing Systems 8. </booktitle>
Reference-contexts: There has been some recent work on making the updates more efficient (Cichosz & Mulawka, 1995) and on changing the definition to make T D () more consistent with the certainty-equivalent method <ref> (Singh & Sutton, 1996) </ref>, which is discussed in Section 5.1. 4.2 Q-learning The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is typically easier to implement.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: Such algorithms can and have been used in the context of a delayed reinforcement, for instance, as the RL component in the AHC architecture described in Section 4.1. They can also be generalized to real-valued reward through reward comparison methods <ref> (Sutton, 1984) </ref>. CRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman, 1990) (crbp) consists of a feed-forward network mapping an encoding of the state to an encoding of the action. <p> The hope is that the random distribution will generate an action that works better, and then that action will be reinforced. ARC The associative reinforcement comparison (arc) algorithm <ref> (Sutton, 1984) </ref> is an instance of the ahc architecture for the case of boolean actions, consisting of two feed 260 Reinforcement Learning: A Survey forward networks. One learns the value of situations, the other learns a policy. These can be simple linear networks or can have hidden units.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference-contexts: Instead, we will use insights from value iteration to adjust the estimated value of a state based on 251 Kaelbling, Littman, & Moore the immediate reward and the estimated value of the next state. This class of algorithms is known as temporal difference methods <ref> (Sutton, 1988) </ref>. <p> Here s is the agent's state before the transition, a is its choice of action, r the instantaneous reward it receives, and s 0 its resulting state. The value of a policy is learned using Sutton's T D (0) algorithm <ref> (Sutton, 1988) </ref> which uses the update rule V (s) := V (s) + ff (r + flV (s 0 ) V (s)) : Whenever a state s is visited, its estimated value is updated to be closer to r + flV (s 0 ), since r is the instantaneous reward received
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning Austin, </booktitle> <address> TX. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna <ref> (Sutton, 1990) </ref>, curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993). 2.2.2 Randomized Strategies Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability p, choose an action at random.
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pp. 353-357. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference: <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6 (2), </volume> <pages> 215-219. </pages>
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38 (3), </volume> <pages> 58-67. </pages>
Reference: <author> Tham, C.-K., & Prager, R. W. </author> <year> (1994). </year> <title> A modular q-learning architecture for manipulator task decomposition. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. </author> <year> (1995). </year> <title> Learning to play the game of chess. </title> <editor> In Tesauro, G., Touretzky, D. S., & Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 Cambridge, </booktitle> <address> MA. </address> <note> The MIT Press. 284 Reinforcement Learning: A Survey Thrun, </note> <author> S., & Schwartz, A. </author> <year> (1993). </year> <title> Issues in using function approximation for reinforcement learning. </title> <editor> In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A. (Eds.), </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School Hillsdale, </booktitle> <address> NJ. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Although experiments with other games have in some cases produced interesting learning behavior, no success close to that of TD-Gammon has been repeated. Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess <ref> (Thrun, 1995) </ref>. It is still an open question as to if and how the success of TD-Gammon can be repeated in other domains. 8.2 Robotics and Control In recent years there have been many robotics and control applications that have used reinforcement learning.
Reference: <author> Thrun, S. B. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. A., & Sofge, D. A. (Eds.), </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY. </address>
Reference: <author> Tsitsiklis, J. N. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 (3). </volume>
Reference: <author> Tsitsiklis, J. N., & Van Roy, B. </author> <year> (1996). </year> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <volume> 22 (1). </volume>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 (11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accuracy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework <ref> (Valiant, 1984) </ref>, there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK. </address>
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 279-292. </pages>
Reference-contexts: We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use. * Eventual convergence to optimal. Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior <ref> (Watkins & Dayan, 1992) </ref>. This is reassuring, but useless in practical terms.
Reference: <author> Whitehead, S. D. </author> <year> (1991). </year> <title> Complexity and cooperation in Q-learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning Evanston, </booktitle> <address> IL. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: serious objections to this method: * It makes an arbitrary division between the learning phase and the acting phase. * How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data <ref> (Whitehead, 1991) </ref> than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993). See Figure 5 for an example. * The possibility of changes in the environment is also problematic.
Reference: <author> Williams, R. J. </author> <year> (1987). </year> <title> A class of gradient-estimating algorithms for reinforcement learning in neural networks. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks San Diego, </booktitle> <address> CA. </address>
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 229-256. </pages>
Reference: <author> Williams, R. J., & Baird, III, L. C. </author> <year> (1993a). </year> <title> Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. </title> <type> Tech. rep. </type> <institution> NU-CCS-93-11, Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address>
Reference-contexts: In most implementations, however, both components operate simultaneously. Only the alternating implementation can be guaranteed to converge to the optimal policy, under appropriate conditions. Williams and Baird explored the convergence properties of a class of AHC-related algorithms they call "incremental variants of policy iteration" <ref> (Williams & Baird, 1993a) </ref>. It remains to explain how the critic can learn the value of a policy. We define hs; a; r; s 0 i to be an experience tuple summarizing a single transition in the environment.
Reference: <author> Williams, R. J., & Baird, III, L. C. </author> <year> (1993b). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Tech. rep. </type> <institution> NU-CCS-93-14, Northeastern University, College of Computer Science, </institution> <address> Boston, MA. </address>
Reference-contexts: One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function <ref> (Williams & Baird, 1993b) </ref>.
Reference: <author> Wilson, S. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3 (2), </volume> <pages> 147-173. </pages>
Reference-contexts: Recently, this approach has been reexamined using insights from the reinforcement-learning literature, with some success. Dorigo did a comparative study of Q-learning and classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson's zeroth 268 Reinforcement Learning: A Survey level classifier system <ref> (Wilson, 1995) </ref> and add one and two-bit memory registers. They find that, although their system can learn to use short-term memory registers effectively, the approach is unlikely to scale to more complex environments.
Reference: <author> Zhang, W., & Dietterich, T. G. </author> <year> (1995). </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intel-lience. </booktitle> <pages> 285 </pages>
References-found: 128

