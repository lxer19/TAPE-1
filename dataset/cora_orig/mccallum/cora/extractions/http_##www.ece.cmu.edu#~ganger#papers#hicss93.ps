URL: http://www.ece.cmu.edu/~ganger/papers/hicss93.ps
Refering-URL: http://www.ece.cmu.edu/~ganger/disksim/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ganger@eecs.umich.edu  
Phone: Phone: (313) 763-5396 FAX: (313) 763-4617  
Title: Disk Subsystem Load Balancing: Disk Striping vs. Conventional Data Placement  
Author: Gregory R. Ganger, Bruce L. Worthington, Robert Y. Hou, Yale N. Patt 
Address: Ann Arbor 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan,  
Note: Appeared in the Proceedings of the Hawaii International Conference on System Sciences, January 1993, pp. 40-49.  
Abstract: The I/O subsystem is becoming a major bottleneck in an increasing number of computer systems. To provide improved I/O performance, as well as to accommodate growing storage requirements, disk subsystems are increasing in size. A major hurdle to obtaining the performance available from these large disk subsystems is load imbalance, or disk skew. Dynamic data placement, the conventional load balancing technique, is usually inadequate to deal with load imbalance because it is forced to accept atomic data sets with rapidly changing access patterns. We name this rapid fluctuation floating load imbalance and distinguish it from the conventional view of load imbalance, referred to as fixed load imbalance. Dynamic data placement also becomes increasingly difficult as the number of disks in the subsystem grows. Disk striping at a high granularity is suggested as a solution to floating load imbalance, the atomic data set problem and the complexity of balancing large disk subsystems. Disk striping uniformly spreads data sets across the disks in the subsystem and essentially randomizes the disk accessed by each request. This randomization effectively handles both fixed and floating load imbalance. Unlike dynamic data placement, disk striping does not become more complex as the number of disks increases. While a more optimal load balance may be possible for some very well understood and well-controlled environments, disk striping should provide significantly improved load balance with reduced complexity for many applications. This improvement will result in shorter response times and higher throughput. 
Abstract-found: 1
Intro-found: 1
Reference: [Asch89] <author> J. Aschoff, </author> <title> "Performance Management of Storage Subsystems", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1989, </year> <pages> pp. 730-739. </pages>
Reference-contexts: This final step would need to be scheduled during a period of little or no activity to avoid serious performance degradation. The human effort described in the papers cited above is considerable, and some attempts have been made to reduce it [Wolf89] <ref> [Asch89] </ref>. The second problem is that dynamic data placement rarely succeeds in solving the load imbalance problem. There are two major problems which prevent dynamic data placement from being successful.
Reference: [Bate91] <author> K. Bates, </author> <title> "I/O Subsystem Performance", DEC Professional, </title> <month> November </month> <year> 1991, </year> <pages> pp. 60-70. </pages>
Reference-contexts: Some of the load balancing effects of disk striping have been noted previously. [Livn87] noted that fixed load imbalance is removed by disk striping. In <ref> [Bate91] </ref>, Bates describes how disk striping could reduce load imbalance by spreading hot data sets among disks. In [Bate92], he also describes how disk striping statistically spreads requests among disks.
Reference: [Bate92] <author> K. Bates, </author> <title> "I/O Subsystem Performance", DEC Professional, </title> <month> February </month> <year> 1992, </year> <pages> pp. 42-49. </pages>
Reference-contexts: Some of the load balancing effects of disk striping have been noted previously. [Livn87] noted that fixed load imbalance is removed by disk striping. In [Bate91], Bates describes how disk striping could reduce load imbalance by spreading hot data sets among disks. In <ref> [Bate92] </ref>, he also describes how disk striping statistically spreads requests among disks.
Reference: [Berr91] <author> R. Berry, J. Hellerstain, J. Kolb, P. VanLeer, </author> <title> "Choosing a Service Level Indicator: Why Not Queue Length?", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1991, </year> <pages> pp. 404-413. </pages>
Reference-contexts: Queue length is closely related to queue time and may be easier to measure at given points in time. [Zhou87] and <ref> [Berr91] </ref> both provide experimental evidence showing that queue length is an appropriate load indicator. Therefore, we will use queue lengths as load indicators in this paper. We identify two types of load imbalance that a disk subsystem can experience.
Reference: [Buze90] <author> J. Buzen, A. Shum, </author> <title> "I/O Performance TradeOffs and MVS/ESA Considerations", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1990, </year> <pages> pp. 695-702. </pages>
Reference-contexts: Most descriptions of load imbalance are in terms of fixed load imbalance, often suggesting the existence of an 80/20-like Rule [Wilm89] [Scra83]. There are several exceptions to this, however. For example, <ref> [Buze90] </ref> suggests that load imbalance is caused by very gradual changes in access patterns. Such a situation would appear at any point in time as fixed load imbalance, but would be floating very slowly. <p> For example, in some cases information about the average workload seen by each disk is available. Data from very busy disks are exchanged with data from relatively idle disks [Papy88] [Wolf89] <ref> [Buze90] </ref>. More aggressive schemes will use information about individual data sets and may consider variations in the temperatures when making the placement decisions [McEl88]. There are essentially two problems with these algorithms for load balancing. First, they can be quite expensive. <p> Floating load imbalance is the other major difficulty faced by dynamic data placement schemes. If access patterns changed very slowly, then it would simply be a matter of rebalancing occasionally as suggested in [Wolf89] and <ref> [Buze90] </ref>. We believe, however, that load imbalance floats very quickly (on the order of seconds and/or minutes) in many environments. Floating load imbalance makes it probable that disk temperatures present during the measurement process will not accurately predict future disk temperatures.
Reference: [Chen90] <author> P. Chen, D. Patterson, </author> <title> "Maximizing throughput in a striped disk array", </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 322-331. </pages>
Reference-contexts: In addition, there have been many studies of the performance consequences of disk striping [Kim86] [Redd89] <ref> [Chen90] </ref>. These studies have shown that there is a definite tradeoff between using multiple disk drives to handle one request and using the disk drives to handle separate requests when possible. This trade-off is embodied in the choice of stripe unit. <p> This trade-off is embodied in the choice of stripe unit. It appears that for any given workload an optimal stripe unit can be chosen. In <ref> [Chen90] </ref> algorithms for choosing the stripe unit were developed assuming various types of workload information. Chen and Patterson showed that the most important parameter in the choice of a stripe unit is the amount of access concurrency in the workload.
Reference: [Chen90a] <author> P. Chen, G. Gibson, R. Katz, D. Patterson, </author> <title> "An Evaluation of Redundant Arrays of Disks using an Amdahl 5890", </title> <booktitle> Proceedings of SIGMETRICS, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: Reliability is an important issue in storage subsystems and is of growing concern as the sizes of these subsystems increase. Much work has been done to determine the cost/performance trade-offs of various redundant architectures, such as mirroring (RAID 1) and parity striping (RAID 5) [Patt88] <ref> [Chen90a] </ref> [Meno92]. We believe that the choices of data placement policy and redundancy scheme are logically independent. It has yet to be determined exactly how these choices interact in terms of performance. It may not be appropriate to stripe over all of the disks in a very large subsystem.
Reference: [Gray90] <author> J. Gray, B. Horst, M. Walker, </author> <title> "Parity Striping of Disk Arrays: Low-Cost Reliable Storage with Acceptable Throughput", </title> <booktitle> Proceedings of the 16th VLDB Conference, </booktitle> <month> August </month> <year> 1990, </year> <pages> pp. 148-161. </pages> <editor> [Gray91] ed. J. Gray, </editor> <title> The Benchmark Handbook for Database and Transaction Processing Systems, </title> <year> 1991. </year>
Reference-contexts: on the first disk, the second stripe unit is on the second disk, and the Nth stripe unit is on disk (N-1 mod M)+1, where M is the number of disks involved. (See Figure 2) Disk striping has been used by many companies, including IBM, Digital Equipment Corporation and Amdahl <ref> [Gray90] </ref>, to improve data bandwidth for a given request by transferring data in parallel to or from multiple disks. In addition, there have been many studies of the performance consequences of disk striping [Kim86] [Redd89] [Chen90]. <p> Except where otherwise noted, all further discussion of disk striping will be in terms of stripe units large enough to ensure that most requests use a single disk. It has also been suggested that striping may be undesirable for transaction processing applications [Ng89] <ref> [Gray90] </ref>. Because these applications are characterized by frequent small disk accesses, it would be undesirable to use more than one disk for any single request. Therefore, one would need to stripe at a very coarse granularity.
Reference: [Kim86] <author> M. Kim, </author> <title> "Synchronized Disk Interleaving", </title> <journal> IEEE Transactions on Computers, </journal> <month> November </month> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: While part of the responsibility for achieving this goal lies with the application program, most lies with the storage manager. The largest obstacle faced by the storage manager in reaching the desired utilization is load imbalance, or disk skew <ref> [Kim86] </ref>. Generally speaking, a storage subsystem is said to suffer from load imbalance when some of the disk drives receive higher percentages of the access stream than do others. This skewed access is often referred to as the 80/20 Rule or, in more extreme cases, the 90/10 Rule [Wilm89]. <p> In addition, there have been many studies of the performance consequences of disk striping <ref> [Kim86] </ref> [Redd89] [Chen90]. These studies have shown that there is a definite tradeoff between using multiple disk drives to handle one request and using the disk drives to handle separate requests when possible. This trade-off is embodied in the choice of stripe unit. <p> The arguments made indicate that disk striping is not worth the implementation effort for these applications. 5 Effect of Disk Striping on Load Im balance Disk striping using a small stripe unit, such as bit or byte, can balance the load seen by the disks in the stripe set <ref> [Kim86] </ref>. This is accomplished by simply using all disk drives for all requests, which causes each disk to see exactly the same workload. Disk striping can also help with the load balancing problem in cases where requests are small and each is handled by a single disk.
Reference: [Livn87] <author> M. Livny, S. Khoshafian, H. Boral, </author> <title> "Multi-Disk Management Algorithms", </title> <booktitle> SIGMETRICS, </booktitle> <year> 1987, </year> <pages> pp. 69-77. </pages>
Reference-contexts: We believe that for most workloads, this is not a reasonable assumption and that the effort required for manual load balancing far outweighs the realistic potential improvement in load balance. Some of the load balancing effects of disk striping have been noted previously. <ref> [Livn87] </ref> noted that fixed load imbalance is removed by disk striping. In [Bate91], Bates describes how disk striping could reduce load imbalance by spreading hot data sets among disks. In [Bate92], he also describes how disk striping statistically spreads requests among disks. <p> Adding to the number of disk drives in a conventional disk subsystem makes load balancing via dynamic data placement more difficult. Striping, on the other hand, does not increase in complexity and continues to provide a good average-case load balance. 8.2 Data Placement We have found, as did <ref> [Livn87] </ref>, that the load balance between disks provided by disk striping is independent of the placement of data within the striped disk space.
Reference: [Majo87] <author> J. </author> <title> Major, "Empirical Models of DASD Response Time", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1987, </year> <pages> pp. 390-398. </pages>
Reference: [McEl88] <author> M. McElwee, </author> <title> "The Real World of DASD Management", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1988, </year> <pages> pp. 672-677. </pages>
Reference-contexts: Even if much of this skew may be attributed to fixed load imbalance, the per disk) rapidly changing skew factors indicate that floating load imbalance does exist in these environments. <ref> [McEl88] </ref> describes the existence of sporadically busy data sets which would be very likely to cause serious floating load imbalance. 3 Dynamic Data Placement In a conventional disk subsystem, the data on each disk are considered independent and logically sequential. <p> Data from very busy disks are exchanged with data from relatively idle disks [Papy88] [Wolf89] [Buze90]. More aggressive schemes will use information about individual data sets and may consider variations in the temperatures when making the placement decisions <ref> [McEl88] </ref>. There are essentially two problems with these algorithms for load balancing. First, they can be quite expensive.
Reference: [McNu86] <author> B. McNutt, </author> <title> "An Empirical Study of Variations in DASD Volume Activity", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1986, </year> <pages> pp. 274-283. </pages>
Reference-contexts: For example, [Buze90] suggests that load imbalance is caused by very gradual changes in access patterns. Such a situation would appear at any point in time as fixed load imbalance, but would be floating very slowly. While not discussing the workload seen by specific disks, <ref> [McNu86] </ref> shows that the amount of load imbalance, which he represents by a skew factor, changes rapidly throughout the day in many real storage subsystems.
Reference: [Meno92] <author> J. Menon, J. Kasson, </author> <title> "Methods for Improved Update Performance of Disk Arrays", </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Reliability is an important issue in storage subsystems and is of growing concern as the sizes of these subsystems increase. Much work has been done to determine the cost/performance trade-offs of various redundant architectures, such as mirroring (RAID 1) and parity striping (RAID 5) [Patt88] [Chen90a] <ref> [Meno92] </ref>. We believe that the choices of data placement policy and redundancy scheme are logically independent. It has yet to be determined exactly how these choices interact in terms of performance. It may not be appropriate to stripe over all of the disks in a very large subsystem.
Reference: [Ng89] <author> S. Ng, </author> <title> "Some Design Issues of Disk Arrays", </title> <booktitle> COM-PCON, Spring 1989, </booktitle> <pages> pp. 137-142. </pages>
Reference-contexts: Except where otherwise noted, all further discussion of disk striping will be in terms of stripe units large enough to ensure that most requests use a single disk. It has also been suggested that striping may be undesirable for transaction processing applications <ref> [Ng89] </ref> [Gray90]. Because these applications are characterized by frequent small disk accesses, it would be undesirable to use more than one disk for any single request. Therefore, one would need to stripe at a very coarse granularity.
Reference: [Papy88] <author> W. Papy, </author> <title> "DASD I/O Performance Tuning: A Case Study of Techniques and Results", </title> <booktitle> CMG Proceedings, </booktitle> <year> 1988, </year> <pages> pp. 665-671. </pages>
Reference-contexts: These people use their experience and the available information to make an educated guess at how to rearrange the data. For example, in some cases information about the average workload seen by each disk is available. Data from very busy disks are exchanged with data from relatively idle disks <ref> [Papy88] </ref> [Wolf89] [Buze90]. More aggressive schemes will use information about individual data sets and may consider variations in the temperatures when making the placement decisions [McEl88]. There are essentially two problems with these algorithms for load balancing. First, they can be quite expensive. <p> Placing a logical unit of data in a conventional storage subsystem entails placing it entirely on a single disk. This problem can prevent dynamic data placement schemes from solving even fixed load imbalance in many cases. Partitioning problem data sets can be very helpful when combating load imbalance <ref> [Papy88] </ref>. Unfortunately, this generally requires changes to tables in the software which owns the data set and may require considerable effort. Floating load imbalance is the other major difficulty faced by dynamic data placement schemes.
Reference: [Patt88] <author> D. Patterson, G. Gibson, R. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)", </title> <booktitle> ACM SIGMOD, </booktitle> <month> May </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Reliability is an important issue in storage subsystems and is of growing concern as the sizes of these subsystems increase. Much work has been done to determine the cost/performance trade-offs of various redundant architectures, such as mirroring (RAID 1) and parity striping (RAID 5) <ref> [Patt88] </ref> [Chen90a] [Meno92]. We believe that the choices of data placement policy and redundancy scheme are logically independent. It has yet to be determined exactly how these choices interact in terms of performance. It may not be appropriate to stripe over all of the disks in a very large subsystem.
Reference: [Redd89] <author> A.L.N. Reddy, P. Banerjee, </author> <title> "An Evaluation of Multiple-Disk I/O Systems", </title> <journal> IEEE Transactions on Computers, </journal> <month> December </month> <year> 1989, </year> <pages> pp. 1680-1690. </pages>
Reference-contexts: In addition, there have been many studies of the performance consequences of disk striping [Kim86] <ref> [Redd89] </ref> [Chen90]. These studies have shown that there is a definite tradeoff between using multiple disk drives to handle one request and using the disk drives to handle separate requests when possible. This trade-off is embodied in the choice of stripe unit.
Reference: [Scra83] <author> R. A. Scranton, D. A. Thompson, D. W. Hunter, </author> <title> "The Access Time Myth", </title> <journal> IBM Research Report, </journal> <volume> RC 10197, </volume> <month> September 21, </month> <year> 1983. </year>
Reference-contexts: We distinguish between the two to highlight the importance of floating load imbalance, which generally receives less attention than it merits, and to simplify the discussion of each. Most descriptions of load imbalance are in terms of fixed load imbalance, often suggesting the existence of an 80/20-like Rule [Wilm89] <ref> [Scra83] </ref>. There are several exceptions to this, however. For example, [Buze90] suggests that load imbalance is caused by very gradual changes in access patterns. Such a situation would appear at any point in time as fixed load imbalance, but would be floating very slowly.
Reference: [Wilm89] <author> R. Wilmot, </author> <title> "File Usage Patterns from SMF Data: Highly Skewed Usage", </title> <booktitle> Computer Measurement Group, </booktitle> <year> 1989. </year>
Reference-contexts: Generally speaking, a storage subsystem is said to suffer from load imbalance when some of the disk drives receive higher percentages of the access stream than do others. This skewed access is often referred to as the 80/20 Rule or, in more extreme cases, the 90/10 Rule <ref> [Wilm89] </ref>. The 80/20 rule suggests that twenty percent of the resources (disk drives) receive eighty percent of the accesses, while the remaining eighty percent of the resources receive only twenty percent. Furthermore, the percentages are applied recursively. <p> We distinguish between the two to highlight the importance of floating load imbalance, which generally receives less attention than it merits, and to simplify the discussion of each. Most descriptions of load imbalance are in terms of fixed load imbalance, often suggesting the existence of an 80/20-like Rule <ref> [Wilm89] </ref> [Scra83]. There are several exceptions to this, however. For example, [Buze90] suggests that load imbalance is caused by very gradual changes in access patterns. Such a situation would appear at any point in time as fixed load imbalance, but would be floating very slowly.
Reference: [Wolf89] <author> J. Wolf, </author> <title> "The Placement Optimization Program: </title>
Reference-contexts: For example, in some cases information about the average workload seen by each disk is available. Data from very busy disks are exchanged with data from relatively idle disks [Papy88] <ref> [Wolf89] </ref> [Buze90]. More aggressive schemes will use information about individual data sets and may consider variations in the temperatures when making the placement decisions [McEl88]. There are essentially two problems with these algorithms for load balancing. First, they can be quite expensive. <p> This final step would need to be scheduled during a period of little or no activity to avoid serious performance degradation. The human effort described in the papers cited above is considerable, and some attempts have been made to reduce it <ref> [Wolf89] </ref> [Asch89]. The second problem is that dynamic data placement rarely succeeds in solving the load imbalance problem. There are two major problems which prevent dynamic data placement from being successful. <p> Floating load imbalance is the other major difficulty faced by dynamic data placement schemes. If access patterns changed very slowly, then it would simply be a matter of rebalancing occasionally as suggested in <ref> [Wolf89] </ref> and [Buze90]. We believe, however, that load imbalance floats very quickly (on the order of seconds and/or minutes) in many environments. Floating load imbalance makes it probable that disk temperatures present during the measurement process will not accurately predict future disk temperatures.
References-found: 21

