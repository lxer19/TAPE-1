URL: http://www.cs.indiana.edu/hyplan/bramley/xwang.thesis.ps.gz
Refering-URL: http://www.cs.indiana.edu/scicomp/partial_orthog.html
Root-URL: http://www.cs.indiana.edu
Title: INCOMPLETE FACTORIZATION PRECONDITIONING FOR LINEAR LEAST SQUARES PROBLEMS  
Author: BY XIAOGE WANG 
Degree: B.Sc., Tsinghua University, 1982 M.Sc., Tsinghua University, 1984 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1994 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Arioli, I. Duff, and P. P. M. De Rijk. </author> <title> On the augmented system approach to sparse least-squares problems. </title> <journal> Numerische Mathematik, </journal> <volume> 55 </volume> <pages> 667-684, </pages> <year> 1989. </year>
Reference-contexts: This method was proposed by Bartels et al., and later considered for the sparse case by Hachtel. Bjorck used it in a study of iterative refinement for least square solutions [7]. Numerical experiments on this method and the comparison with other methods have been done <ref> [1] </ref> [20]. If the pivots are chosen from the diagonal when Cholesky factorization is applied to the augmented system, then after m steps, the reduced system is exactly the normal equations. <p> The parameter ff is usually unknown before the computation. If ff is not small enough, the result may not be acceptable. On the other hand, if ff is too small, the amount of fill-in will be too large to be accepted. Experimental results are presented in <ref> [1] </ref>. 8 2.5 Iterative methods As in solving large sparse linear systems, an iterative method is a very attractive alternative to direct methods for solving large sparse linear least squares problems. There are basically two classes of methods.
Reference: [2] <author> O. Axelsson, </author> <title> editor. Proc. of Conf. on Preconditioned Conjugate Gradient Methods, </title> <institution> University of Nijmegen, </institution> <address> The Netherlands, </address> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] <ref> [2] </ref> [39] [59] [21] [4] [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [3] <author> O. Axelsson and P. S. Vassilevski. </author> <title> A survey of multilevel preconditioned iterative methods. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 769-793, </pages> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] [59] [21] [4] [61] <ref> [3] </ref> [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [4] <author> R. Beauwens. </author> <title> Approximate factorizations with S/P consistently ordered M-factors. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 658-681, </pages> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] [59] [21] <ref> [4] </ref> [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [5] <author> M. W. Berry. </author> <title> Multiprocessor sparse SVD algorithms and Applications. </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1990. </year> <note> Also available as Technical Report # 1049, </note> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana - Champaign. </institution>
Reference-contexts: One example of an application of the linear least squares problems is seismic travel tomography <ref> [5] </ref>. In this application, the linear least squares problem arises from the solution of nonlinear inverse problems associated with the approximation of acoustic or elastic wavespeed from travel time.
Reference: [6] <author> M. W. Berry and R. J. Plemmons. </author> <title> Algorithms and experiments for structural mechanics on high-performance architectrues. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 64 </volume> <pages> 487-507, </pages> <year> 1987. </year>
Reference-contexts: Find x 2 &lt; n which minimizes the value of k b Ax k 2 : (1:1) Such problems occur frequently in scientific and engineering applications such as linear programming [17], augmented Lagrangian methods for CFD [51], and the natural factor method in partial differential equations [41] <ref> [6] </ref>. One example of an application of the linear least squares problems is seismic travel tomography [5]. In this application, the linear least squares problem arises from the solution of nonlinear inverse problems associated with the approximation of acoustic or elastic wavespeed from travel time.
Reference: [7] <author> A. Bjorck. </author> <title> Iterative refinement of linear least squares solutions. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 257-278, </pages> <year> 1967. </year>
Reference-contexts: This method was proposed by Bartels et al., and later considered for the sparse case by Hachtel. Bjorck used it in a study of iterative refinement for least square solutions <ref> [7] </ref>. Numerical experiments on this method and the comparison with other methods have been done [1] [20]. If the pivots are chosen from the diagonal when Cholesky factorization is applied to the augmented system, then after m steps, the reduced system is exactly the normal equations.
Reference: [8] <author> A. Bjorck. </author> <title> Solving linear least squares problems by Gram-Schmidt orthogonalization. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 1-21, </pages> <year> 1967. </year>
Reference-contexts: For example, it may cause the algorithm to `break down'. The next set of results investigate the effect of rounding errors on IMGS. This is accomplished by generalizing the standard result of Bjorck's error analysis of MGS <ref> [8] </ref>. Theorem 3 Suppose A 2 R mfin has full rank and P is the drop set which specifies the forced zero positions of the matrix R produced by IMGS. <p> Proof: Since we will use some results due to Bjorck <ref> [8] </ref>, we use his notation. Computed quantities are distinguished by bars. <p> The resulting bound, k E k F 1:54 2 t n k A k F ; is the same as the bound Bjorck derives for MGS <ref> [8] </ref>. Note that this proof has only exploited the sparsity in R implied by the drop set P , i.e., we have modified a theorem that was derived for a dense A.
Reference: [9] <author> A. Bjorck. </author> <title> SSOR preconditioning methods for sparse least squares problems. </title> <editor> In J. F. Gentleman, editor, </editor> <booktitle> Proceedings of the Computer Science and Statistics: 12-th Annual Symposium on the Interface, </booktitle> <pages> pages 21-25, </pages> <institution> University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1979. </year>
Reference-contexts: They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning <ref> [9] </ref>. C = I + !L T , where A has been normalized so that A T A = L + I + L T with L strictly lower triangular, and ! is a scalar parameter. * Incomplete Cholesky factorization.
Reference: [10] <author> A. Bjorck. </author> <title> Stability analysis of the method of seminormal equations for linear least squares problems. </title> <journal> Linear Algebra and its Applications, </journal> 88/89:31-48, 1987. 
Reference-contexts: However, a stability analysis of seminormal equations method by Bjorck shows that the error of the solution of the seminormal equations method is of the 6 same order as the solution of the normal equations <ref> [10] </ref>. Under mild conditions, a correction step can be added to yield a solution as accurate as the QR method. As with the normal equations, if there are a few dense rows in A, severe fill-in occurs in R.
Reference: [11] <author> A. Bjorck. </author> <title> A bidiagonalization algorithm for solving large and sparse ill-posed systems of linear equations. </title> <journal> BIT, </journal> <volume> 28 </volume> <pages> 659-670, </pages> <year> 1988. </year>
Reference-contexts: To avoid the explicit form of A T A, the standard iterative methods use the normal equations in the form of A T (b Ax) = 0, while CG type methods use intermediate vectors for the computation [54] [55] [56] <ref> [11] </ref>. Although these methods do not explicitly form the normal equations, the rate of convergence still relies on the spectrum of A T A. Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature.
Reference: [12] <author> A. Bjorck. </author> <title> Least squares methods. </title> <type> Technical Report LiTH-MAT-R-1991, </type> <institution> Department of Mathematics, Linkoping University, </institution> <year> 1991. </year>
Reference-contexts: Excellent surveys of work in this area are [22], [40], and <ref> [12] </ref>. 2.1 Normal equations The normal equation method is described by the following algorithm: Algorithm: begin Compute the lower triangular portion of C = A T A. d = A T b. Compute the Cholesky factorization C = GG T .
Reference: [13] <author> A. Bjorck. </author> <title> Pivoting and stability in the augmented system method. </title> <type> Technical Report LiTH-MAT-R-1991-30, </type> <institution> Department of Mathematics, Linkoping University, </institution> <year> 1991. </year>
Reference-contexts: Using smaller ff will introduce 2-by-2 pivots of the form 0 ff a ir 1 A . The choice of ff will effect the choice of pivots and the stability. The error analysis by Bjorck <ref> [13] </ref> shows that min 2 (M ff ) = 2 r 2 oe 1 ) 2 2 2 (A) is attained for ~ff = 1 p 2 oe n (A), where oe 1 : : : oe n are the singular values of A.
Reference: [14] <author> A. Bjorck and I. S. Duff. </author> <title> A direct method for the solution of sparse linear least squares problems. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 34 </volume> <pages> 43-67, </pages> <year> 1980. </year>
Reference-contexts: A. Bjorck and I. Duff later proposed a modification on this method <ref> [14] </ref> which is adaptable to large sparse problems. Let L = @ L 2 A ; P 1 b = @ b 2 A ; be a partition such that L 1 2 &lt; nfin and b 1 2 &lt; n .
Reference: [15] <author> J. R. Bunch and L. Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 31 </volume> <pages> 162-179, </pages> <year> 1977. </year>
Reference-contexts: Expressing the least squares problems in augmented system form with scaling allows more flexibility in pivoting and avoids the loss of information in forming A T A. The system is symmetric indefinite, so a symmetric pivoting strategy proposed by Bunch and Kaufman <ref> [15] </ref> could be applied to preserve the symmetry and maintain stability as well. It determines 2 fi 2 block pivots as well as single element pivots.
Reference: [16] <author> T. F. Chan. </author> <title> Fourier analysis of relaxed incomplete factorization preconditioners. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 12(3) </volume> <pages> 668-680, </pages> <month> May </month> <year> 1991. </year> <month> 197 </month>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] [59] [21] [4] [61] [3] <ref> [16] </ref>. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [17] <author> I.C. Chio, C. L. Monma, and D.F. Shanno. </author> <title> Further development of a primal-dual interior point method. </title> <journal> ORSA Journal on Computing, </journal> <volume> 2(4) </volume> <pages> 304-311, </pages> <year> 1990. </year>
Reference-contexts: Find x 2 &lt; n which minimizes the value of k b Ax k 2 : (1:1) Such problems occur frequently in scientific and engineering applications such as linear programming <ref> [17] </ref>, augmented Lagrangian methods for CFD [51], and the natural factor method in partial differential equations [41] [6]. One example of an application of the linear least squares problems is seismic travel tomography [5]. <p> It is natural for some applications to have a few dense rows in A. In such case, C = A T A becomes a dense matrix. One way to handle the dense rows is to separate them from A, then use the Sherman-Morrison-Woodbury formula for a low rank correction <ref> [17] </ref>. Another way is to split dense rows and create a larger sparse matrix by adding properly defined coefficients.
Reference: [18] <author> T. F. Coleman, A. Edenbrandt, and J. R. Gilbert. </author> <title> Predicting fill for sparse orthogonal factorization. </title> <journal> Journal of the ACM, </journal> <volume> 33(3) </volume> <pages> 517-532, </pages> <year> 1986. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] [34] [53] <ref> [18] </ref> [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [19] <author> R. J. Van der Bei. </author> <title> Splitting dense columns in sparse linear systems. </title> <booktitle> Linear Algebra and its Applications, </booktitle> <pages> pages 107-117, </pages> <year> 1991. </year>
Reference-contexts: Another way is to split dense rows and create a larger sparse matrix by adding properly defined coefficients. The solution to the normal equations is determined by solving an enlarged sparse linear system <ref> [19] </ref> [38]. 5 Generally, the normal equations will have difficulty for a matrix A where A T A is too large to use the dense matrix techniques or too dense to apply sparse matrix techniques or most often when A is nearly rank deficient. 2.2 Orthogonal factorization The 2-norm is preserved
Reference: [20] <author> I. S. Duff. </author> <title> The solution of large-scale least squares problems on supercomputers. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 241-252, </pages> <year> 1990. </year>
Reference-contexts: This method was proposed by Bartels et al., and later considered for the sparse case by Hachtel. Bjorck used it in a study of iterative refinement for least square solutions [7]. Numerical experiments on this method and the comparison with other methods have been done [1] <ref> [20] </ref>. If the pivots are chosen from the diagonal when Cholesky factorization is applied to the augmented system, then after m steps, the reduced system is exactly the normal equations.
Reference: [21] <author> I. S. Duff and G. A. Meurand. </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] [59] <ref> [21] </ref> [4] [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9]. <p> This not only reduces the fill-in but also has beneficial consequences for the parallel processing. It is known, however, that this ordering tends to generate an IC preconditioner with less quality than other reorderings that are less concerned with minimizing fill-in <ref> [21] </ref>. The RCM ordering attempts to reduce the width of the profile of the Cholesky factor of the matrix. i.e., to cluster the non-zero elements in a band near the diagonal. This has significance for Gram-Schmidt-type factorizations of the matrix A. <p> We have not implemented a parallel version of this algorithm, but it is an area for future work. The quality of a IC preconditioner is related to the ordering <ref> [21] </ref> [23],[24]. A good preconditioner must have not only potential for parallel processing but must also reduce the number of iterations so that overall computation time is reduced.
Reference: [22] <author> I. S. Duff and J. K. Reid. </author> <title> A comparison of some methods for the solution of sparse overdetermined systems of linear equations. </title> <journal> Journal of the Institute of Mathematics and Its Applications, </journal> <volume> 17(3) </volume> <pages> 267-280, </pages> <year> 1976. </year>
Reference-contexts: Excellent surveys of work in this area are <ref> [22] </ref>, [40], and [12]. 2.1 Normal equations The normal equation method is described by the following algorithm: Algorithm: begin Compute the lower triangular portion of C = A T A. d = A T b. Compute the Cholesky factorization C = GG T .
Reference: [23] <author> V. Eijkhout. </author> <title> Beware of modified incomplete point factorizations. </title> <type> Technical Report CSRD Report No. 1048, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference: [24] <author> V. Eijkhout. </author> <title> Beware of unperturbed modified incomplete factorizations. </title> <type> Technical Report CSRD Report No. 1109, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference: [25] <author> D. H. Evans, </author> <title> editor. Preconditioning Methods: Analysis and Applications. </title> <publisher> Gordon and Breach Science Publishers, </publisher> <year> 1983. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are <ref> [25] </ref> [2] [39] [59] [21] [4] [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [26] <author> T. Fiala. </author> <title> On the monotonicity of incomplete factorizations. </title> <journal> Numerische Mathematik, </journal> <volume> 57 </volume> <pages> 473-479, </pages> <year> 1990. </year>
Reference-contexts: It has been shown that if P ae ~ P are two drop sets used for IC factorizations of a symmetric positive definite M-matrix, then R ~ R where this inequality is taken componentwise, and R and ~ R are the corresponding IC factors <ref> [26] </ref>. Lemma 7 and Theorem 13 show that this monotonicity also holds for CIMGS when the normal equations are an M-matrix. Lemma 7 Let A 2 &lt; nfin and B 2 &lt; nfin be symmetric positive definite M-matrices where A B.
Reference: [27] <author> K. Gallivan, A. Sameh, and Z. Zlatev. </author> <title> A parallel hybrid sparse linear system solver. </title> <booktitle> Computing Systems in Engineering, </booktitle> <address> 1(2-4):183-195, </address> <year> 1990. </year>
Reference-contexts: C = ^ R, where ^ R is from approximation of Cholesky factoriza tion of A T A, and ^ R is more sparse than the normal Cholesky factor. * Approximate orthogonal factorization [58] [43] <ref> [27] </ref> [66] [42]. C = ~ R, where A = ~ Q ~ R + E is an approximation of a QR factorization, ~ Q is an orthogonal matrix and ~ R is upper triangular matrix.
Reference: [28] <author> W. M. Gentleman. </author> <title> Least squares computations by Givens transformations without square roots. </title> <journal> Journal of the Institute of Mathematics and Its Applications, </journal> <pages> pages 329-336, </pages> <year> 1973. </year>
Reference-contexts: Their algorithm first computes the incomplete orthogonal factorization A = QDR + E using the Gentleman version <ref> [28] </ref> of Givens rotation with a numerical dropping technique which has been apply in the Y12M sparse linear system solver [65]. DR is used as a preconditioner for CG applied to the normal equations.
Reference: [29] <author> A. George and M. T. Heath. </author> <title> Solution of sparse linear least squares problems using Givens rotations. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 34 </volume> <pages> 69-83, </pages> <year> 1980. </year>
Reference-contexts: Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] [34] [53] [18] [64] <ref> [29] </ref> [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [30] <author> A. George and J. Liu. </author> <title> Householder reflections versus Givens rotations in sparse orthogonal decomposition. </title> <journal> Linear Algebra and its Applications, </journal> 88/89:223-238, 1987. 
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] <ref> [30] </ref> [32] [34] [53] [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [31] <author> A. George, J. Liu, and E. Ng. </author> <title> Row-ordering schemes for sparse Givens transformations. I. bipartite graph model. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 61 </volume> <pages> 55-81, </pages> <year> 1984. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested <ref> [31] </ref> [46] [30] [32] [34] [53] [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [32] <author> A. George, J. Liu, and E. Ng. </author> <title> A data structure for sparse QR and LU factorizations. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(1) </volume> <pages> 100-121, </pages> <year> 1988. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] <ref> [32] </ref> [34] [53] [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [33] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Keeping the fill-in small in the complete Cholesky factorization is the motivation for three of the orderings we consider in this section: minimum degree ordering (MD), nested dissection (ND) ordering, reverse Cuthill-McKee (RCM) ordering. These three orderings are standards in sparse computations <ref> [33] </ref>. As the motivation above indicated they were originally designed to keep fill-in low when solving 67 large sparse symmetric positive definite systems. So they will be applied to the normal equations A T A and the resulting permutation will be used as a column reordering of A.
Reference: [34] <author> A. George and E. Ng. </author> <title> Orthogonal reduction of sparse matrices to upper triangular form using Householder transformations. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 7(2) </volume> <pages> 460-472, </pages> <year> 1986. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] <ref> [34] </ref> [53] [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [35] <author> P.E. Gill, G. H. Golub, and M.A. Saunders. </author> <title> Methods for modifying matrix factorizations. </title> <journal> Mathematics of Computation, </journal> <volume> 28 </volume> <pages> 505-35, </pages> <year> 1974. </year> <month> 198 </month>
Reference: [36] <author> G. H. Golub and R. J. Plemmons. </author> <title> Large-scale geodetic least-squares adjustment by dissection and orthogonal decomposition. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 34 </volume> <pages> 3-27, </pages> <year> 1980. </year>
Reference-contexts: The system 1.3 is in general large and sparse. Another example is the computation involved in geodetic adjustment and surveys <ref> [36] </ref>. The computations are based on a mathematical model called a geodetic position network. It consists of several mesh 1 points or stations, with unknown positions over a reference surface or 3-dimensional space. The points are connected by lines, each representing the observation involving these two points.
Reference: [37] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Compute the Cholesky factorization C = GG T . Solve Gy = d and G T x LS = y. end The algorithm requires (m + n=3)n 2 flops in the dense case. Details of this method can be found in <ref> [37] </ref> [45]. This is the most widely used method for solving full rank linear least squares problems [37]. It is easy to derive and understand. It relies on standard algorithms: Cholesky factorization, triangular system solver, matrix-matrix multiplication, and matrix-vector multiplication which are well developed in both dense and sparse cases. <p> Solve Gy = d and G T x LS = y. end The algorithm requires (m + n=3)n 2 flops in the dense case. Details of this method can be found in <ref> [37] </ref> [45]. This is the most widely used method for solving full rank linear least squares problems [37]. It is easy to derive and understand. It relies on standard algorithms: Cholesky factorization, triangular system solver, matrix-matrix multiplication, and matrix-vector multiplication which are well developed in both dense and sparse cases. <p> than RCM ordering, the MD ordering is the most effective in reducing nz (N (A)). 112 113 CHAPTER 6 Incomplete Cholesky factorization with pattern modifications 6.1 Overview of problem The incomplete Cholesky factorization is one of the most important preconditioners for iterative methods of solving symmetric positive definite linear systems <ref> [37] </ref>. Its major weakness is a lack of stability; to overcome this difficulty, several modifications have been suggested [47][44][61]. Those modification methods use information about the values of dropped elements and modify the process to assure the existence of the factorization. We refer to these modifications as numerical strategies.
Reference: [38] <author> J. F. Grcar. </author> <title> Matrix streching for linear equations. </title> <type> Technical Report SAND90-8723, </type> <institution> Sandia National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: Another way is to split dense rows and create a larger sparse matrix by adding properly defined coefficients. The solution to the normal equations is determined by solving an enlarged sparse linear system [19] <ref> [38] </ref>. 5 Generally, the normal equations will have difficulty for a matrix A where A T A is too large to use the dense matrix techniques or too dense to apply sparse matrix techniques or most often when A is nearly rank deficient. 2.2 Orthogonal factorization The 2-norm is preserved by
Reference: [39] <author> A. Greenbaum and G. H. Rodrigue. </author> <title> Optimal preconditioners of a given sparsity pattern. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 610-634, </pages> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] <ref> [39] </ref> [59] [21] [4] [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [40] <author> M. T. Heath. </author> <title> Numerical methods for large sparse linear least squares problems. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 497-513, </pages> <year> 1984. </year>
Reference-contexts: Excellent surveys of work in this area are [22], <ref> [40] </ref>, and [12]. 2.1 Normal equations The normal equation method is described by the following algorithm: Algorithm: begin Compute the lower triangular portion of C = A T A. d = A T b. Compute the Cholesky factorization C = GG T .
Reference: [41] <author> M. T. Heath, R. J. Plemmons, and R. C. Ward. </author> <title> Sparse orthogonal schemes for structural optimization using the force method. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 514-532, </pages> <year> 1984. </year>
Reference-contexts: Find x 2 &lt; n which minimizes the value of k b Ax k 2 : (1:1) Such problems occur frequently in scientific and engineering applications such as linear programming [17], augmented Lagrangian methods for CFD [51], and the natural factor method in partial differential equations <ref> [41] </ref> [6]. One example of an application of the linear least squares problems is seismic travel tomography [5]. In this application, the linear least squares problem arises from the solution of nonlinear inverse problems associated with the approximation of acoustic or elastic wavespeed from travel time.
Reference: [42] <author> D. James. </author> <title> Conjugate Gradient Methods for Constrained Least Squares Problems. </title> <type> PhD thesis, </type> <institution> North Carolina State University, </institution> <year> 1990. </year>
Reference-contexts: C = ^ R, where ^ R is from approximation of Cholesky factoriza tion of A T A, and ^ R is more sparse than the normal Cholesky factor. * Approximate orthogonal factorization [58] [43] [27] [66] <ref> [42] </ref>. C = ~ R, where A = ~ Q ~ R + E is an approximation of a QR factorization, ~ Q is an orthogonal matrix and ~ R is upper triangular matrix. <p> The difficulty of finding the optimal value of * also appears in that the number of elements that will be kept in R are not know a priori for a given *. 2.6.4 Method of James James also discusses incomplete QR factorization <ref> [42] </ref>. The method is an incomplete QR factorization using Givens rotation with positional dropping. The set of non-zero positions kept in R is predetermined using the pattern of normal equations. The pivot row is kept sparse by dropping the elements in the zero positions. <p> The set of non-zero positions kept in R is predetermined using the pattern of normal equations. The pivot row is kept sparse by dropping the elements in the zero positions. All the fill-in in target rows are kept. The following statements are made in <ref> [42] </ref> to summarize the reported experiments: * Incomplete QR factorization can breakdown even if A T A is diagonally dominant or an M-matrix. * Row permutation may or may not prevent breakdown. * For the test problems from H/B collection, the quality of the IQR preconditioner was marginal at best.
Reference: [43] <author> A. Jennings and M. A. Ajiz. </author> <title> Incomplete methods for solving A T Ax = b. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5(4) </volume> <pages> 978-987, </pages> <year> 1984. </year>
Reference-contexts: C = ^ R, where ^ R is from approximation of Cholesky factoriza tion of A T A, and ^ R is more sparse than the normal Cholesky factor. * Approximate orthogonal factorization [58] <ref> [43] </ref> [27] [66] [42]. C = ~ R, where A = ~ Q ~ R + E is an approximation of a QR factorization, ~ Q is an orthogonal matrix and ~ R is upper triangular matrix. <p> discussed in [52] [48]. 2.6 Incomplete orthogonal factorization preconditioning Since our study will focus on a class of approximate orthogonal factorization preconditioning, we give a more detailed review of the previous work on this subject. 9 2.6.1 Methods of Jennings and Ajizi Jennings and Ajiz discussed three incomplete factorization methods <ref> [43] </ref>. Two are incomplete orthogonal factorizations. The incomplete Gram-Schmidt method proposed in this paper is the modified version of Gram-Schmidt orthogonalization with a rejection test applied right after the formation of the off-diagonal elements of the factor R. <p> The rapid decay early in the interval tends to be slowed if fi =j r ii j is used in the dropping test. The method proposed by Jennings and Ajiz <ref> [43] </ref> is an example of dynamic selection P on IMGS that has appeared in the literature. The results of solving the test suite using dynamic pattern selection with * = 0:02 are in Tables A.1 and A.2 in the Appendix. The choice of * is motivated by the observation above.
Reference: [44] <author> A. Jennings and G. M. Malik. </author> <title> Partial elimination. </title> <journal> Journal of the Institute of Mathematics and Its Applications, </journal> <volume> 20 </volume> <pages> 307-316, </pages> <year> 1977. </year>
Reference: [45] <editor> C.L. Lawson and R.J. Hanson. </editor> <title> Solving Least Square Problems. </title> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: Compute the Cholesky factorization C = GG T . Solve Gy = d and G T x LS = y. end The algorithm requires (m + n=3)n 2 flops in the dense case. Details of this method can be found in [37] <ref> [45] </ref>. This is the most widely used method for solving full rank linear least squares problems [37]. It is easy to derive and understand. It relies on standard algorithms: Cholesky factorization, triangular system solver, matrix-matrix multiplication, and matrix-vector multiplication which are well developed in both dense and sparse cases.
Reference: [46] <author> J. Liu. </author> <title> On general row merging schemes for sparse Givens transformations. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 7(4) </volume> <pages> 1190-1211, </pages> <year> 1986. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] <ref> [46] </ref> [30] [32] [34] [53] [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [47] <author> T. A. Manteuffel. </author> <title> Shifted incomplete Choleski factorization. </title> <editor> In I. S. Duff and G.W. Stewart, editors, </editor> <title> Sparse Matrix Proceedings 1978, </title> <booktitle> Philadelphia,PA, 1979. </booktitle> <publisher> SIAM Publications. </publisher>
Reference: [48] <author> T.L. Markham, M. Neumann, and R.J. Plemmons. </author> <title> Convergence of a direct-iterative method for large-scale least square problems. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 69 </volume> <pages> 155-167, </pages> <year> 1985. </year>
Reference-contexts: The work presented in this thesis will focus on a class of incomplete QR factorization preconditioners. Another class of methods, such as 2-block SOR and 3-block SOR, are based on splitting the augmented system. The conditions for convergence are discussed in [52] <ref> [48] </ref>. 2.6 Incomplete orthogonal factorization preconditioning Since our study will focus on a class of approximate orthogonal factorization preconditioning, we give a more detailed review of the previous work on this subject. 9 2.6.1 Methods of Jennings and Ajizi Jennings and Ajiz discussed three incomplete factorization methods [43].
Reference: [49] <author> P. Matstoms. </author> <title> The multifrontal solution of sparse linear least squares problems. </title> <type> Technical Report LIU-TEK-LIC-1991:33, </type> <institution> Department of Mathematics, Linkoping University, </institution> <year> 1991. </year>
Reference-contexts: Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] [34] [53] [18] [64] [29] <ref> [49] </ref>. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [50] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Mathematics of Computation, </journal> <volume> 31(137) </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: Theorem 4 states that one step of Gaussian elimination preserves the properties of an M-matrix. Theorem 5 states a condition on a range of perturbations to an M-matrix that yields another M-matrix. Theorem 4 ( due to Fan <ref> [50] </ref>). Let A = (a ij ) be a nonsingular matrix and let A (1) = (a (1) matrix that arises by zeroing out the off-diagonal elements in the first column of A via one step of Gaussian elimination. <p> If A is an M-matrix then A (1) exists and is also an M-matrix. 21 Theorem 5 ( due to Meijerink and van der Vorst <ref> [50] </ref>). Let A = (a ij ) be an n fi n M-matrix. If the matrix B = (b ij ) is such that a ij b ij 0 f or i 6= j and 0 &lt; a ii b ii then B is also an M-matrix. <p> Proof: This results directly from Theorem 4 and Lemma 1. 2 The results above can be generalized to IC and IMGS. The generalization to IC appears in the literature and can be stated as Lemma 3. Lemma 3 ( due to Meijerink and van der Vorst <ref> [50] </ref>). Let A = (a ij ) be a symmetric positive definite matrix and let A (1) = (a (1) ij ) be the matrix that arises by from one step of IC. If A is an M-matrix then A (1) exists and is also an M-matrix. <p> do not change significantly for the worse. (For MGS one of the eigenvalues is made exactly 1 and the remaining ones are in an interval bounded by the minimum and maximum eigenvalues of the normal equations of the matrix before the step is performed.) A similar result is contained in <ref> [50] </ref> and states that A T A = LL T ^ E is a regular splitting where L results from applying IC to A T A which is an M-matrix and ^ E 0 and ae ( ^ E) &lt; 1.
Reference: [51] <author> M.Fortin and R. Glowinski. </author> <title> Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems. </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: Find x 2 &lt; n which minimizes the value of k b Ax k 2 : (1:1) Such problems occur frequently in scientific and engineering applications such as linear programming [17], augmented Lagrangian methods for CFD <ref> [51] </ref>, and the natural factor method in partial differential equations [41] [6]. One example of an application of the linear least squares problems is seismic travel tomography [5].
Reference: [52] <author> R. Morandi and F. Sgallari. </author> <title> Parallel algorithms for the iterative solution of sparse least squares problems. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 271-280, </pages> <year> 1990. </year>
Reference-contexts: The work presented in this thesis will focus on a class of incomplete QR factorization preconditioners. Another class of methods, such as 2-block SOR and 3-block SOR, are based on splitting the augmented system. The conditions for convergence are discussed in <ref> [52] </ref> [48]. 2.6 Incomplete orthogonal factorization preconditioning Since our study will focus on a class of approximate orthogonal factorization preconditioning, we give a more detailed review of the previous work on this subject. 9 2.6.1 Methods of Jennings and Ajizi Jennings and Ajiz discussed three incomplete factorization methods [43].
Reference: [53] <author> G. Ostrouchov. </author> <title> Symbolic Givens reduction and row-ordering in large sparse least squares problems. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 8(3) </volume> <pages> 248-264, </pages> <year> 1987. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] [34] <ref> [53] </ref> [18] [64] [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [54] <author> C. Paige and M. Saunders. </author> <title> LSQR: An algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8(1) </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: To avoid the explicit form of A T A, the standard iterative methods use the normal equations in the form of A T (b Ax) = 0, while CG type methods use intermediate vectors for the computation <ref> [54] </ref> [55] [56] [11]. Although these methods do not explicitly form the normal equations, the rate of convergence still relies on the spectrum of A T A. Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature.
Reference: [55] <author> C. C. Paige. </author> <title> Bidiagonalization of matrices and the solution of linear equations. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 11 </volume> <pages> 197-209, </pages> <year> 1974. </year> <month> 199 </month>
Reference-contexts: To avoid the explicit form of A T A, the standard iterative methods use the normal equations in the form of A T (b Ax) = 0, while CG type methods use intermediate vectors for the computation [54] <ref> [55] </ref> [56] [11]. Although these methods do not explicitly form the normal equations, the rate of convergence still relies on the spectrum of A T A. Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature.
Reference: [56] <author> C.C. Paige and M.A. Saunders. </author> <title> Solution of sparse indefinite systems of equations and least squares problems. </title> <type> Technical Report STAN-CS-73-399, </type> <institution> Stanford University, </institution> <year> 1973. </year>
Reference-contexts: To avoid the explicit form of A T A, the standard iterative methods use the normal equations in the form of A T (b Ax) = 0, while CG type methods use intermediate vectors for the computation [54] [55] <ref> [56] </ref> [11]. Although these methods do not explicitly form the normal equations, the rate of convergence still relies on the spectrum of A T A. Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. <p> It is possible, however, to organize the computations of CG applied to the normal equations in such a way that A T A need not be formed explicitly. The resulting method, CGLS <ref> [56] </ref>, is used in this thesis as the basic iterative method to solve the least squares problems and, for the square matrices, the nonsymmetric linear systems. Of course, even though CGLS does not form the normal equations explicitly its convergence depends on their spectrum.
Reference: [57] <author> G. Peters and J. H. Wilkinson. </author> <title> The least squares problem and pseudo-inverses. </title> <journal> The Computer Journal, </journal> <volume> 13 </volume> <pages> 309-316, </pages> <year> 1970. </year>
Reference-contexts: Solve U P T 2 x LS = y. where P 1 AP 2 = LU is an LU factorization with complete pivoting. P 1 , P 2 are permutation matrices. The method was first proposed by Peters and Wilkinson <ref> [57] </ref>. Usually, by using an appropriate pivoting strategy, it is possible to control the condition of L, so L T L is better conditioned than A T A. The numerical stability is not as satisfactory as with the orthogonal factorization methods.
Reference: [58] <author> Youcef Saad. </author> <title> Preconditioning techniques for nonsymmetric and indefinite linear systems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 </volume> <pages> 89-105, </pages> <year> 1988. </year>
Reference-contexts: C = ^ R, where ^ R is from approximation of Cholesky factoriza tion of A T A, and ^ R is more sparse than the normal Cholesky factor. * Approximate orthogonal factorization <ref> [58] </ref> [43] [27] [66] [42]. C = ~ R, where A = ~ Q ~ R + E is an approximation of a QR factorization, ~ Q is an orthogonal matrix and ~ R is upper triangular matrix. <p> The other drawback is that, as the authors indicated, the splitting may not improve the condition of the matrix. It is therefore not clear if this approach can be used as a basis for an effective preconditioning strategy. 2.6.2 ILQ method of Saad The work in <ref> [58] </ref> was motivated by the need to solve indefinite nonsymmetric linear systems. The ILQ preconditioner is combined with the normal equations to solve a nonsymmetric linear system. Obviously, this approach can be applied to solve linear least squares problems. <p> When implementing ICGS, it is possible to organize the computations so that it performs fewer floating point computations than IMGS. This is done by exploiting an observation of Saad on the implementation of his ILQ pre-conditioner to which ICGS is related <ref> [58] </ref>, (see Section 4.3.4). When computing q j from a j in ICGS, the computation of each of the r ij , for i = 1; : : : ; j 1, involves a dot product of q i and a j . <p> For these problems, the normal equation method would need to use the pattern for higher levels of retained fill-in in IC, i.e., IC (1) or IC (2), when driving IMGS or ICGS. An alternative to this is the semidynamic scheme discussed below. 4.3.3 Semidynamic pattern selection Saad, <ref> [58] </ref>, proposed a dropping strategy that falls between the dynamic and static dropping strategies discussed above. In this method, the incompleteness is determined by an integer, s, or set of integers, s i , i = 1; : : : ; n. <p> Zlatev dropped elements in A as it evolved into R when using plane rotations to perform the reduction [65]. The problem of breakdown was handled by decreasing the drop tolerance and restarting the factorization if a failure occurred. The ILQ algorithm proposed in Saad's work, <ref> [58] </ref>, is equivalent to ICGS with semidynamic pattern selection in both Q and R (ILQ will be used to refer to this form of ICGS for the remainder of the thesis).
Reference: [59] <author> M. A. Saunders. </author> <title> Sparse least squares by conjugate gradients: A comparison of preconditioning methods. </title> <editor> In J. F. Gentleman, editor, </editor> <booktitle> Proceedings of the Computer Science and Statistics: 12th Annual Symposium on the Interface, </booktitle> <pages> pages 15-20, </pages> <institution> University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1979. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] <ref> [59] </ref> [21] [4] [61] [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [60] <author> J. H. Wilkinson. </author> <title> A priori error analysis of algebraic processes. </title> <booktitle> In Proc. International Congress of Mathematicians, </booktitle> <pages> pages 119-129, </pages> <year> 1968. </year>
Reference-contexts: The proof draws on results established by Wilkinson in his classic paper A priori error analysis of algebra processes <ref> [60] </ref>, which we summarize before proceeding with the proof. 1 Recall that in the version of the algorithm the elements of B were maintained separately from the elements of L T = R. 88 Let B = B (1) = @ b T 1 0 t 11 t 1 1 A
Reference: [61] <author> G. Wittum and F. Liebau. </author> <title> On truncated incomplete decompositions. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 719-740, </pages> <year> 1989. </year>
Reference-contexts: Preconditioning techniques that accelerate the convergence of these methods have received extensive attention in the literature. Some examples of the literature of preconditioning symmetric positive definite linear systems and least squares problems are [25] [2] [39] [59] [21] [4] <ref> [61] </ref> [3] [16]. They can be summarized as follows: * Column scaling. C = diag (d i ), where d i are norms of columns of A. * SSOR preconditioning [9].
Reference: [62] <author> Ulrike Meier Yang. </author> <title> Preconditioned conjugate gradient-like methods for nonsymmetric linear systems. </title> <type> Technical Report 1210, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: The value for k b k 2 clearly shows numerical rank deficiency. The matrix fs 760 3 is very ill conditioned with a maximum eigenvalue magnitude of 10 9 and a minimum eigenvalue magnitude of 10 9 <ref> [62] </ref>. The remaining 5 marked matrices are moderately ill conditioned. Of course, the fact that neither SPARSPAK-B nor the incomplete Gram-Schmidt preconditioners perform column pivoting for stability reasons also contributes to the lack of accuracy of the solutions.
Reference: [63] <author> D. M. Young. </author> <title> Iterative soultion of large linear systems. </title> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: From the relations shown above and Theorem 5 it follows that ~ A T ~ A is also an M-matrix. 2 We need one final result from the literature before stating our main result in Theorem 7. Theorem 6 (due to Young <ref> [63] </ref>). Let A = M N be a regular splitting of the nonsingular matrix A. Then A 1 0 if and only if ae (M 1 N ) &lt; 1.
Reference: [64] <author> Z. Zlatev. </author> <title> Comparison of two pivotal strategies in sparse plane rotations. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 8(2) </volume> <pages> 119-135, </pages> <year> 1982. </year>
Reference-contexts: They usually need much more space and time than the normal equations method. Q is usually much denser than A. Although Q can be discarded, large storage may still be needed for the computation. Several sparse orthogonal factorization algorithms have been suggested [31] [46] [30] [32] [34] [53] [18] <ref> [64] </ref> [29] [49]. If multiple solutions have to be computed for the same A and multiple b, where not all b are available at the same time, and if Q is not stored, the seminormal equations method can be used.
Reference: [65] <author> Z. Zlatev. </author> <title> Computational Methods for General Sparse Matrices. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Their algorithm first computes the incomplete orthogonal factorization A = QDR + E using the Gentleman version [28] of Givens rotation with a numerical dropping technique which has been apply in the Y12M sparse linear system solver <ref> [65] </ref>. DR is used as a preconditioner for CG applied to the normal equations. In this incomplete orthogonalization, E is from the contribution of the dropped elements whose magnitude are smaller than a given positive number * called the dropping tolerance. <p> By dynamic we mean that the set P is unknown until the factorization is complete. Certain criteria are imposed on the value of a matrix element and at some point during the factorization a decision is made to keep or drop it based on the criteria (see <ref> [65] </ref> for a detailed discussion of this topic). Such a strategy has the advantage that it can adapt to numerical values encountered during the factorization and not just precondition based on its sparsity pattern. <p> This does not, of course, rule out the effectiveness of such a strategy if the dynamic data structures are handled well. Similar approaches have been used successfully in incomplete LU factorizations <ref> [65] </ref>. <p> A semidynamic restriction allowing only the largest s elements of q i to be kept could also be applied. Zlatev dropped elements in A as it evolved into R when using plane rotations to perform the reduction <ref> [65] </ref>. The problem of breakdown was handled by decreasing the drop tolerance and restarting the factorization if a failure occurred. <p> does have a significant advantage over IC and other preconditioners - CIMGS with dynamic or semidynamic dropping is the only robust preconditioner based on numerical dropping that does not have to rely on elaborate restart mechanisms to achieve robustness when the factorization fails to exist for a given drop tolerance <ref> [65] </ref>. This enables CIMGS to use its adjustability to improve its solution's efficiency by using a smaller and more problem dependent non-zero set to generate its preconditioner to offset the added cost of its factorization compared to IC and other similar methods.

References-found: 65

