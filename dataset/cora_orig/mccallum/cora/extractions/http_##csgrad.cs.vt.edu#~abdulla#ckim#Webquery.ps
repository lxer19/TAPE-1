URL: http://csgrad.cs.vt.edu/~abdulla/ckim/Webquery.ps
Refering-URL: http://memex.cs.vt.edu/~liub/index/proj.html
Root-URL: http://www.cs.vt.edu
Email: email: abdulla,liub,fox@vt.edu  
Phone: Phone: (540)231-3877, Fax (540)231-6075  
Title: Analyzing Accesses to Web Information Retrieval Systems  
Author: Ghaleb Abdulla, Binzhang Liu, Edward A. Fox 
Address: Virginia, 24061-0106  
Affiliation: Computer Science Department, Virginia Polytechnic Institute and State University  
Abstract: We analyze client accesses to Web Information Retrieval Systems (IRS), and in particular their queries, in connection with an ongoing project to develop tools and methods for modeling WWW traffic. We compare accesses from groups of clients to Web IRS instead of just focusing on one server log file. Although the analysis covers a number of client groups, clients behavior does not vary much between different groups. For example, clients tend to use simple queries with a small number of terms and operators regardless of user computer background or knowledge. In addition, i the number of accesses per client follows a Weibull distribution. Among the identified Web IRS, Yahoo was the most popular. By identifying client sessions based on some reasonable heuristics we can quantify the amount of searching and browsing per session. One of the interesting correlations is that increased searching corresponds to reduced network and server traffic. It appears that with more searching steps, there is less need for browsing steps, which leads to a reduction in the number of bytes transferred over the network. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christine L. Borgman, Sandra G. Hirsh, and John Hiller. </author> <title> Rethinking online monitoring methods for information retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 47(7) </volume> <pages> 568-583, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We call these tools Web Information Retrieval Systems (IRS). Web IRS have evolved rapidly over the last five years, and have been used by millions who thereby had their first experience with search engines. Borgman et al. <ref> [1] </ref> observed that "The end users who now dominate searching are using systems with exploratory interfaces, under less time pressure, and have less clear retrieval goals than do skilled search intermediaries." As part of a larger effort to characterize WWW traffic so as to support modeling, planning and prediction, and to
Reference: [2] <author> H. Braun and K. Claffy. </author> <title> Web traffic characterization: An assessment of the impact of caching documents from NCSA's Web server. </title> <booktitle> In Proc. 2nd Int. WWW Conference, </booktitle> <address> Chicago, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: In some other cases multiple instances of the browser might be running on a multiuser machine. In such cases, accesses from this client might be due to several users running multiple instances of the browser. Sessions are very hard to characterize <ref> [2] </ref>. In this paper we define sessions with respect to clients not users.
Reference: [3] <author> Carsten Schlichting and Erik Nilsen. </author> <title> Signal detection analysis of WWW search engines. In Designing for the Web Empirical Studies. </title> <publisher> Microsoft, </publisher> <month> October </month> <year> 1996. </year> <note> &lt;URL: http://www.microsoft.com/usability/webconf.htm &gt;. </note>
Reference-contexts: We can use the author's name and a key word from the article to locate it. While the Web is used in other ways, these three methods cover a high volume of WWW traffic, and so are worth more careful characterization. 3 2.3 Related Work In <ref> [3] </ref> the authors compare several WWW search engines by using five faculty members to search for certain subjects and then judge the relevance of the returned results. Evaluation of the search engines is done using "signal detection analysis." The authors provide two measures to evaluate search engines.
Reference: [4] <author> Annabel Pollock and Andrew Hockley. </author> <title> What's wrong with internet searching. In Designing for the Web Empirical Studies. </title> <publisher> Microsoft, </publisher> <month> October </month> <year> 1996. </year> <note> &lt;URL: http://www.microsoft.com/usability/webconf.htm &gt;. </note>
Reference-contexts: The first one is sensitivity of search engine in finding useful information. The second one is how conservative or liberal the search engine is in determining which sites to include in the search results. The authors conclude that the current search systems exhibit poor performance. Pollock <ref> [4] </ref> demonstrates that user background does not affect the search process or results. The author demonstrates this through an experiment that compares naive and non-naive user searches. The author concludes that there are misconceptions and problems with Web searching due to the design of available search tools. <p> Tables 2 and 3 show that Web users do use Web IRS to locate information. However the degree of computer knowledge does not appear to predict the percentage of accesses that are queries, for our workloads. This is consistent with the findings in <ref> [4] </ref>. However, the task effect is very clear in two cases, the library and the high school (AUB). Table 4 shows the distribution of accesses to different Web IRS. One thing that is common between all workloads except AOL is that Yahoo is the top used IRS.
Reference: [5] <author> Averill M. Law and Stephen Vincent. </author> <title> ExpertFit User's Guide. </title> <editor> Averill M. </editor> <publisher> Law and Associates, </publisher> <address> Tucson, AZ 85717, </address> <month> August </month> <year> 1995. </year> <month> 15 </month>
Reference-contexts: In all workloads we noticed that we have a small percentage of clients which are very active followed by less active clients. The number of accesses decays rapidly and shows an asymptotic behavior. By fitting the data to distributions using ExpertFit <ref> [5] </ref> we found out that they follow a "Weibull" distribution [6] with ff = 0:44 for the Korea workload, ff = 0:51 for the lib workload, ff = 0:37 for the CS workload, and ff = 0:48 for the AUB workload.
Reference: [6] <author> Paul L. Meyer. </author> <title> Introductory Probability and Statistical Applications. </title> <publisher> Addison Wesley, 2725 Sand Hall Road, </publisher> <address> Menlo Park, CA 94025, 2nd edition, </address> <year> 1970. </year>
Reference-contexts: The number of accesses decays rapidly and shows an asymptotic behavior. By fitting the data to distributions using ExpertFit [5] we found out that they follow a "Weibull" distribution <ref> [6] </ref> with ff = 0:44 for the Korea workload, ff = 0:51 for the lib workload, ff = 0:37 for the CS workload, and ff = 0:48 for the AUB workload. This indicates that a small percentage of the clients are responsible for most of the accesses.
Reference: [7] <author> Ghaleb Abdulla, Edward A. Fox, and Marc Abrams. </author> <title> WWW proxy traffic characterization with application to caching. </title> <type> Technical Report TR-97-03, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Currently we are looking into the possibility of training a knowledge base system to extract clients sessions based on the algorithm shown in Figure 3. This effort is part of an ongoing effort to characterize and model Web traffic by looking at Web objects and their methods <ref> [7, 8, 9] </ref>. 8 Acknowledgments Dan Aronson provided the America Online traces. Jurgen Koenemann, Neill Kipp and Members of the VT NRG provided helpful comments on the manuscript. NSF grants CDA-9312611 and NCR-9627922 partially supported this work.
Reference: [8] <author> Ghaleb Abdulla, Binzhang Liu, Rani saad, and Edward A. Fox. </author> <title> Characterizing World Wide Web queries. </title> <type> Technical Report TR-97-04, </type> <institution> Computer Sci. Dept., Virginia Tech, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Currently we are looking into the possibility of training a knowledge base system to extract clients sessions based on the algorithm shown in Figure 3. This effort is part of an ongoing effort to characterize and model Web traffic by looking at Web objects and their methods <ref> [7, 8, 9] </ref>. 8 Acknowledgments Dan Aronson provided the America Online traces. Jurgen Koenemann, Neill Kipp and Members of the VT NRG provided helpful comments on the manuscript. NSF grants CDA-9312611 and NCR-9627922 partially supported this work.
Reference: [9] <author> S. Williams, M. Abrams, C. R. Standridge, G. Abdulla, and E. A. Fox. </author> <title> Removal policies in network caches for World-Wide Web documents. </title> <booktitle> In ACM SIGCOMM'96 Conference, </booktitle> <pages> pages 293-305, </pages> <institution> Stanford University, California, </institution> <month> August </month> <year> 1996. </year> <title> &lt;URL: http://ei.cs.vt.edu/~succeed/96sigcomm/96sigcomm.html&gt;. 16 a- Korea b- Library (VT) c-CS (VT) d-AUB e- AOL 17 If (access is from same client) - if (accessed path is related to previous accessed path) -This is the same session-else - if (Time between accesses&lt; 300 seconds) -This is the same session-else different session - else -different session 18 </title>
Reference-contexts: Currently we are looking into the possibility of training a knowledge base system to extract clients sessions based on the algorithm shown in Figure 3. This effort is part of an ongoing effort to characterize and model Web traffic by looking at Web objects and their methods <ref> [7, 8, 9] </ref>. 8 Acknowledgments Dan Aronson provided the America Online traces. Jurgen Koenemann, Neill Kipp and Members of the VT NRG provided helpful comments on the manuscript. NSF grants CDA-9312611 and NCR-9627922 partially supported this work.
References-found: 9

