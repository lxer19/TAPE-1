URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-98-04.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Title: A Structured Approach to Parallel Programming  
Degree: Thesis by Berna Massingill In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Date: 1998 (Submitted September 25, 1997)  
Address: Pasadena, California  
Affiliation: California Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. C. Adams, W. S. Brainerd, J. T. Martin, B. T. Smith, and J. L. Wagener. </author> <title> Fortran 90 Handbook: Complete ANSI/ISO Reference. </title> <publisher> Intertext Publications : McGraw-Hill Book Company, </publisher> <year> 1992. </year>
Reference-contexts: We then revisit these ideas in the context of two representative programming notations: (1) a theory-oriented notation, Dijkstra's guarded-command language [35, 37], where by "theory-oriented" we mean a notation used primarily as a basis for formal work on program semantics, and (2) a practical programming notation, Fortran 90 <ref> [1, 46] </ref>, where by "practical programming notation" we mean a notation used primarily for the development of applications, particularly large-scale ones. We present our ideas in the context of a theory-oriented notation to demonstrate that they can be made rigorous in a notation for which a formal semantics is defined. <p> following example is not a valid arb composition; the two assignments are not arb-compatible. arb (a := 1; b := a) 2.5 arb composition and Fortran 90 2.5.1 Fortran 90 and our model Giving a formal definition of the semantics of a large practical programming language such as Fortran 90 <ref> [1, 46] </ref> is far from trivial.
Reference: [2] <author> G. R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1991. </year>
Reference-contexts: barrier command n times: (8j :: (iB j = cB j + 1) ^ (iB j = n)) ; (8j :: (cB j = n)) : We observe that this specification simply captures formally the usual meaning of barrier synchronization and is consistent with other formalizations, for example those of <ref> [2] </ref> and [70]. <p> This combined definition implements a common 87 approach to barrier synchronization based on keeping a count of processes waiting at the barrier, as in <ref> [2] </ref> and [70]. In the context of our model, we implement this approach using two protocol variables local to the parallel composition, a count Q of suspended components and a flag Arriving that indicates whether components are arriving at the barrier or leaving. <p> (nS j;k n) ^ (iR j;k = n)) ; (cR j;k = n) : We observe that this specification, like the one for barrier synchronization in Chapter 4, simply captures formally the usual meaning of this type of message passing, and is consistent with other formalizations, for example those of <ref> [2] </ref> and [69]. The terminology ("slack") and overall method (in which initiations and completions of a command are considered separately) are based on [55]. 5.1.2 Definitions Like many other implementations of message-passing, for example those of [2] and [69], our definition represents channels as queues: We define for each ordered pair <p> type of message passing, and is consistent with other formalizations, for example those of <ref> [2] </ref> and [69]. The terminology ("slack") and overall method (in which initiations and completions of a command are considered separately) are based on [55]. 5.1.2 Definitions Like many other implementations of message-passing, for example those of [2] and [69], our definition represents channels as queues: We define for each ordered pair (P j ; P k ) a queue C j;k whose elements represent messages in transit from P j to P k .
Reference: [3] <author> ANSI Technical Committee X3H5. </author> <title> X3H5 parallel extensions for Fortran, document number X3H5/93-SD1-Revision M. h http://www.kai.com/hints/ftn m.ps.Z.uu i, </title> <month> April </month> <year> 1994. </year> <month> Accessed July </month> <year> 1996. </year>
Reference-contexts: Language constructs consistent with this form of composition include the par and parfor constructs of CC++ [21, 19], the INDEPENDENT directive of HPF [43], and the PARALLEL DO and PARALLEL SECTIONS constructs of the Fortran X3H5 proposal <ref> [3] </ref>. 2.6.2.1 Parallel execution using HPF An arb-model program in which all arb compositions are of the arball form can be transformed into an equivalent program in HPF by replacing arball with forall and preceding each such block with an INDEPENDENT directive, as illustrated in the following examples. <p> barrier synchronization in a way consistent with our definitions (which in turn are consistent with the usual meaning of parallel composition with barrier synchronization). 4.4.1 Parallel execution using X3H5 Fortran For example, a par-model program can be transformed into an equivalent program in the notation of the Fortran X3H5 proposal <ref> [3] </ref> by replacing par and end par with PARALLEL SECTIONS, SECTION, and END PARALLEL SECTIONS, replacing parall and end parall with PARALLEL DO and END PARALLEL DO (nested if necessary), and replacing barrier with BARRIER. 4.4.2 Example For example, the last example of Section 4.3.2 is equivalent to the following program
Reference: [4] <author> S. Atlas, S. Banerjee, J. C. Cummings, P. J. Hinker, M. Srikant, J. V. W. Reynders, and M. Tholburn. POOMA: </author> <title> A high performance distributed simulation environment for scientific applications. h http://www.acl.lanl.gov/PoomaFramework/papers/SCPaper-95.html i, </title> <year> 1995. </year>
Reference-contexts: The mesh-spectral archetype is based to some extent on the idea of distributed objects, as discussed for example in work on pC++ [12] and POOMA <ref> [4] </ref>. Our work on archetypes differs from this work in that we focus more on the pattern of computation and on identifying and exploiting patterns of computation and communication. Communication libraries. Many researchers have investigated and developed reusable general libraries of communication routines; MPI [58] is a notable example.
Reference: [5] <author> R. J. R. </author> <title> Back. Refinement calculus, part II: Parallel and reactive programs. In Stepwise Refinement of Distributed Systems: Models, Formalisms, Correctness, </title> <booktitle> volume 430 of Lecture Notes in Computer Science, </booktitle> <pages> pages 67-93. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Program development via stepwise refinement. Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back [6], Gries, and Hoare [44], and for parallel programs in the work of, for example, Back <ref> [5] </ref>, Martin [56], and Van de Velde [74]. Operational models. <p> Our emphasis on program development by stepwise refinement builds on the work of Back [6], Gries [42], and Hoare [44] for sequential programs, and Back <ref> [5] </ref>, Martin [56], and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models.
Reference: [6] <author> R. J. R. Back and J. von Wright. </author> <title> Refinement calculus, part I: Sequential nondeterministic programs. In Stepwise Refinement of Distributed Systems: Models, Formalisms, Correctness, </title> <booktitle> volume 430 of Lecture Notes in Computer Science, </booktitle> <pages> pages 42-66. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: We base our notions of program correctness on the work of Hoare [44] and others. Program development via stepwise refinement. Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back <ref> [6] </ref>, Gries, and Hoare [44], and for parallel programs in the work of, for example, Back [5], Martin [56], and Van de Velde [74]. Operational models. <p> We differ from much work on reasoning about parallel programs, for example Chandy and Misra [24] and Lamport [50], in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties). Our emphasis on program development by stepwise refinement builds on the work of Back <ref> [6] </ref>, Gries [42], and Hoare [44] for sequential programs, and Back [5], Martin [56], and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models.
Reference: [7] <author> R. Bagrodia, K. M. Chandy, and M. Dhagat. </author> <title> UC | a set-based language for data-parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(2) </volume> <pages> 186-201, </pages> <year> 1995. </year>
Reference-contexts: The name (arb) is derived from UC <ref> [7] </ref> and is intended to connote that such groups of program elements may be interleaved in any arbitrary fashion without changing the result. <p> We give two presentations of this material: one relying mostly on natural 1 As mentioned in Chapter 1, the name (arb) is derived from UC <ref> [7] </ref> and is intended to connote that such groups of program elements may be interleaved in any arbitrary fashion without changing the result. 9 language and omitting detailed proofs (Section 2.1 and Section 2.2), and one making more extensive use of symbolic notation and presenting more detailed proofs (Section 2.7 and
Reference: [8] <author> C. F. Baillie, O. Broker, O. A. McBryan, and J. P. Wilson. MPI-RGL: </author> <title> a regular grid library for MPI. </title> <note> h http://www.cs.colorado.edu/~ broker/mpi rgl/mpi rgl.ps i, </note> <year> 1995. </year>
Reference-contexts: Communication libraries. Many researchers have investigated and developed reusable general libraries of communication routines; MPI [58] is a notable example. Others have developed more specialized libraries, for example MPI-RGL <ref> [8] </ref> for regular grids.
Reference: [9] <author> R. A. Ballance, A. J. Giancola, G. F. Luger, and T. J. Ross. </author> <title> A framework-based environment for object-oriented scientific codes. </title> <journal> Scientific Programming, </journal> <volume> 2(4) </volume> <pages> 111-121, </pages> <year> 1993. </year> <month> 167 </month>
Reference-contexts: Program development strategies. Fang [39] describes a programming strategy similar to ours, but with less focus on the identification and exploitation of patterns. The Basel approach [16] is more concerned with developing and exploiting a general approach for classifying and dealing with parallel programs. Ballance et al. <ref> [9] </ref> are more explicitly concerned with the development of tools for application support; while our work can be exploited to create such tools, it is not our primary focus. Kumaran and Quinn [48] focus more on automated conversion of template-based applications into efficient programs for different architectures. Dataflow patterns.
Reference: [10] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: Brinch Hansen's work on parallel structures [15] is similar in motivation to our work, but his model programs are typically more narrowly defined than our archetypes. Other work addresses lower-level patterns, as for example the use of templates to develop algorithms for linear algebra in <ref> [10] </ref>. Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole [27], Botorog and Kuchen [13, 14], and Darlington et al. [32].
Reference: [11] <author> J. H. Beggs, R. J. Luebbers, D. Steich, H. S. Langdon, and K. S. Kunz. </author> <title> User's manual for three-dimensional FDTD version C code for scattering from frequency-independent dielectric and magnetic materials. </title> <type> Technical report, </type> <institution> The Pennsylvania State University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Two versions of this code were available: a public-domain version ("version A", described in [49]) that performs only the near-field calculations, and an export-controlled version ("version C", described in <ref> [11] </ref>) that performs both near-field and far-field calculations. The two versions were sufficiently different that we parallelized them separately, producing two parallelization experiments. 8.3.2 Parallelization strategy In most respects, this application fits the pattern of the mesh archetype of Section 7.2.3.
Reference: [12] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, and S. X. Yang. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3) </volume> <pages> 7-22, </pages> <year> 1993. </year>
Reference-contexts: Programming skeletons, design patterns, and distributed objects. Our work is also in some respects complementary to work exploring the use of programming skeletons and patterns in 4 parallel computing, for example that of Cole [27] and Brinch Hansen [15], and even work exploring distributed objects, pC++ <ref> [12] </ref> for example. We also make use of abstractions that capture exploitable commonalities among programs, but we use these abstractions to guide a program development methodology based on program transformations. Communication libraries. <p> The mesh-spectral archetype is based to some extent on the idea of distributed objects, as discussed for example in work on pC++ <ref> [12] </ref> and POOMA [4]. Our work on archetypes differs from this work in that we focus more on the pattern of computation and on identifying and exploiting patterns of computation and communication. Communication libraries.
Reference: [13] <author> G. H. Botorog and H. Kuchen. </author> <title> Efficient parallel programming with algorithmic skeletons. </title> <editor> In L. Bouge, editor, </editor> <booktitle> Proceedings of EuroPar '96, volume 1123-1124 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Other work addresses lower-level patterns, as for example the use of templates to develop algorithms for linear algebra in [10]. Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole [27], Botorog and Kuchen <ref> [13, 14] </ref>, and Darlington et al. [32]. This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and [14] combines functional skeletons with sequential imperative code.
Reference: [14] <author> G. H. Botorog and H. Kuchen. Skil: </author> <title> An imperative language with algorithmic skeletons for efficient distributed programming. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Other work addresses lower-level patterns, as for example the use of templates to develop algorithms for linear algebra in [10]. Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole [27], Botorog and Kuchen <ref> [13, 14] </ref>, and Darlington et al. [32]. This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and [14] combines functional skeletons with sequential imperative code. <p> This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and <ref> [14] </ref> combines functional skeletons with sequential imperative code. This work, like that of Brinch Hansen, describes a program development strategy that consists of filling in the "blanks" of a parallel structure with sequential code.
Reference: [15] <author> P. Brinch Hansen. </author> <title> Model programs for computational science: A programming methodology for multicomputers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(5) </volume> <pages> 407-423, </pages> <year> 1993. </year>
Reference-contexts: Programming skeletons, design patterns, and distributed objects. Our work is also in some respects complementary to work exploring the use of programming skeletons and patterns in 4 parallel computing, for example that of Cole [27] and Brinch Hansen <ref> [15] </ref>, and even work exploring distributed objects, pC++ [12] for example. We also make use of abstractions that capture exploitable commonalities among programs, but we use these abstractions to guide a program development methodology based on program transformations. Communication libraries. <p> Schmidt [66] focuses more on parallel structure, but in a different context from our work and with less emphasis on code reuse. Shaw [67] examines higher-level patterns in the context of software architectures. Brinch Hansen's work on parallel structures <ref> [15] </ref> is similar in motivation to our work, but his model programs are typically more narrowly defined than our archetypes. Other work addresses lower-level patterns, as for example the use of templates to develop algorithms for linear algebra in [10]. Program skeletons.
Reference: [16] <author> H. Burkhart, R. Frank, and G. Hachler. </author> <title> Structured parallel programming: How informatics can help overcome the software dilemma. </title> <journal> Scientific Programming, </journal> <volume> 5(1) </volume> <pages> 33-45, </pages> <year> 1996. </year>
Reference-contexts: Our approach is similar, but we allow the sequential code to reference the containing parallel structure, as in the mesh-spectral archetype examples. Program development strategies. Fang [39] describes a programming strategy similar to ours, but with less focus on the identification and exploitation of patterns. The Basel approach <ref> [16] </ref> is more concerned with developing and exploiting a general approach for classifying and dealing with parallel programs. Ballance et al. [9] are more explicitly concerned with the development of tools for application support; while our work can be exploited to create such tools, it is not our primary focus.
Reference: [17] <author> R. M. Butler and E. L. Lusk. </author> <title> Monitors, messages, and clusters | the p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <year> 1994. </year>
Reference-contexts: We have developed for this archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines, 3 with versions based on Fortran M [40], Fortran with p4 <ref> [17] </ref>, and Fortran with NX [60].
Reference: [18] <author> C. Canuto. </author> <title> Spectral Methods in Fluid Dynamics. </title> <booktitle> Springer Series in Computational Physics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: It is described further in Chapter 8. 7.3.2.3 Incompressible flow This spectral code provides a numerical solution of the 3-dimensional Euler equations for incompressible flow with axisymmetry. Periodicity is assumed in the axial direction; the numerical scheme <ref> [18] </ref> uses a Fourier spectral method in the periodic direction and a fourth-order finite difference 144 method in the radial direction. It is based on the 2-dimensional spectral archetype and has been implemented in Fortran M for networks of Sun workstations and the IBM SP.
Reference: [19] <author> P. Carlin, K. M. Chandy, and C. Kesselman. </author> <title> The Compositional C++ language definition. </title> <type> Technical Report CS-TR-92-02, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Language constructs consistent with this form of composition include the par and parfor constructs of CC++ <ref> [21, 19] </ref>, the INDEPENDENT directive of HPF [43], and the PARALLEL DO and PARALLEL SECTIONS constructs of the Fortran X3H5 proposal [3]. 2.6.2.1 Parallel execution using HPF An arb-model program in which all arb compositions are of the arball form can be transformed into an equivalent program in HPF by replacing
Reference: [20] <author> K. M. Chandy. </author> <title> Concurrent program archetypes. </title> <booktitle> In Proceedings of the Scalable Parallel Library Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Our theoretical framework could be used to prove not only manually-applied transformations but also those applied by parallelizing compilers. Design patterns. Many researchers have investigated the use of patterns in developing algorithms and applications. Our previous work <ref> [20, 23] </ref> explores a more general notion of archetypes and their role in developing both sequential and parallel programs. Gamma et al. [41] address primarily the issue of patterns of computation, in the context of object-oriented design.
Reference: [21] <author> K. M. Chandy and C. Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming language. </title> <type> Technical Report CS-TR-92-01, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Language constructs consistent with this form of composition include the par and parfor constructs of CC++ <ref> [21, 19] </ref>, the INDEPENDENT directive of HPF [43], and the PARALLEL DO and PARALLEL SECTIONS constructs of the Fortran X3H5 proposal [3]. 2.6.2.1 Parallel execution using HPF An arb-model program in which all arb compositions are of the arball form can be transformed into an equivalent program in HPF by replacing
Reference: [22] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed snapshots | determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3 </volume> <pages> 63-75, </pages> <year> 1985. </year> <month> 168 </month>
Reference-contexts: We will then map this sequence of transitions to a computation of C S . Constructing the sequence of transitions. We construct the sequence of transitions in a manner analogous to a well-known nondeterministic sorting algorithm (as discussed in, e.g., <ref> [22] </ref>) in which an array is sorted by repeatedly choosing one out-of-sequence pair and exchanging its elements. 55 We first consider finite computations. Suppose C P is a finite computation of P .
Reference: [23] <author> K. M. Chandy, R. Manohar, B. L. Massingill, and D. I. Meiron. </author> <title> Integrating task and data parallelism with the group communication archetype. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: Archetypes can also provide frameworks for testing and documentation. Performance models. Archetypes may also be helpful in developing performance models for classes of programs with common structure, as discussed in [64]. Program composition. Archetypes can be useful in structuring programs that combine task and data parallelism, as described in <ref> [23] </ref>. 7.1.2 An archetype-based program development strategy Our general strategy for writing programs using archetypes is as follows: Start with a sequential algorithm or problem description. Identify an appropriate archetype. Develop an initial archetype-based version of the algorithm. <p> Our theoretical framework could be used to prove not only manually-applied transformations but also those applied by parallelizing compilers. Design patterns. Many researchers have investigated the use of patterns in developing algorithms and applications. Our previous work <ref> [20, 23] </ref> explores a more general notion of archetypes and their role in developing both sequential and parallel programs. Gamma et al. [41] address primarily the issue of patterns of computation, in the context of object-oriented design.
Reference: [24] <author> K. M. Chandy and J. Misra. </author> <title> Parallel Program Design: A Foundation. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra <ref> [24] </ref>, Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. <p> notation and only brief sketches of most proofs; Section 2.7 presents the same material formally and in more detail, including a description (Section 2.7.1) of notational conventions. 2.1.1 Overview Treating programs as state-transition systems is not a new approach; it has been used in work such as Chandy and Misra <ref> [24] </ref>, Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli [61] to reason about both parallel and sequential programs. <p> We take our notion of program specifications from some of the standard ways of giving program specifications for sequential program, for example those of Hoare [44] and Dijkstra [36]. We differ from much work on reasoning about parallel programs, for example Chandy and Misra <ref> [24] </ref> and Lamport [50], in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties). <p> Parallel programming models. Since we are more interested in sequential-style specifications than in those involving ongoing behavior, our work differs considerably from much other work on parallel programming, for example that of Chandy and Misra (UNITY) <ref> [24] </ref> and Hoare (CSP) [45]. Other work on parallel programs with sequential equivalents includes that of Valiant (BSP) [73] and Thornley [71]; the former, however, emphasizes performance analysis over analysis of correctness, while the latter focuses mostly on programs for a shared-memory model. Operational models of program semantics. <p> Operational models of program semantics. Our operational model adapts the ideas of state-transitions systems, as described in Chandy and Misra <ref> [24] </ref>, Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli [61]; we give a formulation of these ideas that is aimed at facilitating our proofs. Automatic parallelizing compilers.
Reference: [25] <author> A. Church and J. B. Rosser. </author> <title> Some properties of conversion. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 39 </volume> <pages> 472-482, </pages> <year> 1936. </year>
Reference-contexts: (s 2 or s 0 2 ) by executing first a and then b, then we can reach the same state by first executing b and then a, and vice versa. 2 Remarks about Definition 2.13. * a and b commute exactly when a and b have the diamond property <ref> [25, 53] </ref>. 2 We now define the desired condition. 20 Definition 2.14 (arb-compatible).
Reference: [26] <author> M. J. Clement and M. J. Quinn. </author> <title> Overlapping computations, communications and I/O in parallel sorting. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(2) </volume> <pages> 162-172, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Two variants of the quicksort algorithm are presented: a standard recursive version, and a "one-deep" nonrecursive version more suitable for scalable parallel implementations <ref> [26, 68] </ref>. 6.4.2 Recursive program This program sorts the array of integers in place.
Reference: [27] <author> M. I. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Programming skeletons, design patterns, and distributed objects. Our work is also in some respects complementary to work exploring the use of programming skeletons and patterns in 4 parallel computing, for example that of Cole <ref> [27] </ref> and Brinch Hansen [15], and even work exploring distributed objects, pC++ [12] for example. We also make use of abstractions that capture exploitable commonalities among programs, but we use these abstractions to guide a program development methodology based on program transformations. Communication libraries. <p> Other work addresses lower-level patterns, as for example the use of templates to develop algorithms for linear algebra in [10]. Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole <ref> [27] </ref>, Botorog and Kuchen [13, 14], and Darlington et al. [32]. This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and [14] combines functional skeletons with sequential imperative code. <p> Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole <ref> [27] </ref>, Botorog and Kuchen [13, 14], and Darlington et al. [32]. This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and [14] combines functional skeletons with sequential imperative code. This work, like that of Brinch Hansen, describes a program development strategy that consists of filling in the "blanks" of a parallel structure with sequential code.
Reference: [28] <author> K. D. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The Parascope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 82(2) </volume> <pages> 244-263, </pages> <year> 1993. </year>
Reference-contexts: Automatic parallelization of sequential programs. Our work is in many respects complementary to efforts to develop parallelizing compilers, for example Fortran D <ref> [28] </ref> and HPF [43]. The focus of such work is on the automatic detection of exploitable parallelism, while our work addresses how to exploit parallelism once it is known to exist. Our theoretical framework could be used to prove not only manually-applied transformations but also those applied by parallelizing compilers. <p> Automatic parallelizing compilers. Much effort has gone into development of compilers that automatically recognize potential concurrency and emit parallel code, for example Fortran D <ref> [28] </ref> and 161 HPF [43]. The focus of such work is on the automatic detection of exploitable parallelism, while our work addresses how to exploit parallelism once it is known to exist.
Reference: [29] <author> D. Dabdub and R. Manohar. </author> <title> Performance and portability of an air quality model. </title> <booktitle> Parallel Computing, </booktitle> <year> 1997. </year> <note> To appear in special issue on regional weather models. </note>
Reference-contexts: Inefficiencies in executing the code on the base number of processors (e.g., paging) probably explain the better-than-ideal speedup for small numbers of processors. Fortran M on the IBM SP. Data supplied by Greg Davis. 7.3.2.4 Airshed model This code, known as the CIT airshed model <ref> [29, 30, 31] </ref> models smog in the Los Angeles basin. <p> It is conceptually based on the mesh-spectral archetype, although it does not use the mesh-spectral implementation, and has been implemented on a number of platforms, including the Intel Delta, the Intel Paragon, the Cray T3D, and the IBM SP2, as described in <ref> [29] </ref>. 145 Chapter 8 Stepwise parallelization methodology This chapter presents experimental work in support of the transformational aspects of our model and methodology.
Reference: [30] <author> D. Dabdub and J. H. Seinfeld. </author> <title> Air quality modeling on massively parallel computers. </title> <journal> Atmospheric Environment, </journal> <volume> 28(9) </volume> <pages> 1679-1687, </pages> <year> 1994. </year>
Reference-contexts: Inefficiencies in executing the code on the base number of processors (e.g., paging) probably explain the better-than-ideal speedup for small numbers of processors. Fortran M on the IBM SP. Data supplied by Greg Davis. 7.3.2.4 Airshed model This code, known as the CIT airshed model <ref> [29, 30, 31] </ref> models smog in the Los Angeles basin.
Reference: [31] <author> D. Dabdub and J. H. Seinfeld. </author> <title> Parallel computation in atmospheric chemical modeling. </title> <journal> Parallel Computing, </journal> <volume> 22 </volume> <pages> 111-130, </pages> <year> 1996. </year>
Reference-contexts: Inefficiencies in executing the code on the base number of processors (e.g., paging) probably explain the better-than-ideal speedup for small numbers of processors. Fortran M on the IBM SP. Data supplied by Greg Davis. 7.3.2.4 Airshed model This code, known as the CIT airshed model <ref> [29, 30, 31] </ref> models smog in the Los Angeles basin.
Reference: [32] <author> J. Darlington, A. J. Field, P. O. Harrison, P. H. J. Kelly, D. W. N. Sharp, Q. Wu, and R. L. White. </author> <title> Parallel programming using skeleton functions. </title> <editor> In A. Bode, editor, </editor> <booktitle> Proceedings of PARLE 1993, volume 694 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Program skeletons. Much work has also been done on structuring programs by means of program skeletons, including that of Cole [27], Botorog and Kuchen [13, 14], and Darlington et al. <ref> [32] </ref>. This work is more oriented toward functional programming than ours, although [27] mentions the possibility of expressing the idea of program skeletons in imperative languages, and [14] combines functional skeletons with sequential imperative code.
Reference: [33] <author> G. Davis. </author> <title> Spectral methods for scientific computing. h http://www.etext.caltech.edu/ Implementations/Spectral/spectral.html i, </title> <year> 1994. </year>
Reference-contexts: A key element of this archetype is support for ensuring that these requirements are met. This support can take the form of guidelines for manually transforming programs, as in our archetype-implementation user guides <ref> [34, 57, 33] </ref>, or it could be expressed in terms of more formal transformations with arguments for their correctness, as in the transformations of Chapter 3. 7.2.1.3 Communication patterns This data-distribution scheme thus gives rise to the need (in distributed memory) for a small set of communication operations: Grid redistribution. <p> We have developed for this archetype an implementation (based on Fortran M [40]) consisting of a code skeleton and an archetype-specific library of communication routines. The implementation is described in detail in <ref> [33] </ref>; it has been used to run an application on the IBM SP and on a network of Sun workstations. 7.2.3 The mesh archetype The mesh archetype is a strict subset of the mesh-spectral archetype (Section 7.2.1), with one minor change.
Reference: [34] <author> G. Davis and B. Massingill. </author> <title> The mesh-spectral archetype. </title> <type> Technical Report CS-TR-96-26, </type> <institution> California Institute of Technology, </institution> <year> 1997. </year> <note> Also available via h http://www.etext.caltech.edu/ Implementations/ i. </note>
Reference-contexts: A key element of this archetype is support for ensuring that these requirements are met. This support can take the form of guidelines for manually transforming programs, as in our archetype-implementation user guides <ref> [34, 57, 33] </ref>, or it could be expressed in terms of more formal transformations with arguments for their correctness, as in the transformations of Chapter 3. 7.2.1.3 Communication patterns This data-distribution scheme thus gives rise to the need (in distributed memory) for a small set of communication operations: Grid redistribution. <p> The implementation is described in detail in <ref> [34] </ref>.
Reference: [35] <author> E. W. Dijkstra. </author> <title> Guarded commands, nondeterminacy, and formal derivations of programs. </title> <journal> Communications of the ACM, </journal> <volume> 18(8) </volume> <pages> 453-457, </pages> <year> 1975. </year>
Reference-contexts: We then revisit these ideas in the context of two representative programming notations: (1) a theory-oriented notation, Dijkstra's guarded-command language <ref> [35, 37] </ref>, where by "theory-oriented" we mean a notation used primarily as a basis for formal work on program semantics, and (2) a practical programming notation, Fortran 90 [1, 46], where by "practical programming notation" we mean a notation used primarily for the development of applications, particularly large-scale ones. <p> in HPF [43]: one that contains no subobjects | e.g., a scalar data object or a scalar element of an array. 28 2.4 arb composition and Dijkstra's guarded-command lan guage 2.4.1 Dijkstra's guarded-command language and our model It is straightforward to define the commands and constructors of Dijkstra's guarded-command language <ref> [35, 37] </ref> in terms of our model. <p> = (s 1 # fvg)) By symmetry, a similar construction applies to computations in which a k is performed first. 2 2.9 Appendix: Dijkstra's guarded-command language and our model, details In this section we sketch definitions in our model for some of the commands and constructors of Dijkstra's guarded-command language <ref> [35, 37] </ref>. 2.9.1 Simple commands Definition 2.29 (Skip). <p> Observe that this definition is given in terms of restricted forms of the alternative (IF ) and repetition (DO ) constructs of Dijkstra's guarded-command language <ref> [35, 37] </ref>, but it applies to any programming notation with equivalent constructs. Definition 4.5 (par-compatible).
Reference: [36] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1976. </year> <month> 169 </month>
Reference-contexts: We define our programming models as simple extensions to the standard sequential model of Dijkstra <ref> [36, 37] </ref>, Gries [42], and others. We base our notions of program correctness on the work of Hoare [44] and others. Program development via stepwise refinement. <p> We take our notion of program specifications from some of the standard ways of giving program specifications for sequential program, for example those of Hoare [44] and Dijkstra <ref> [36] </ref>. We differ from much work on reasoning about parallel programs, for example Chandy and Misra [24] and Lamport [50], in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties).
Reference: [37] <author> E. W. Dijkstra and C. S. Scholten. </author> <title> Predicate Calculus and Program Semantics. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: We define our programming models as simple extensions to the standard sequential model of Dijkstra <ref> [36, 37] </ref>, Gries [42], and others. We base our notions of program correctness on the work of Hoare [44] and others. Program development via stepwise refinement. <p> We then revisit these ideas in the context of two representative programming notations: (1) a theory-oriented notation, Dijkstra's guarded-command language <ref> [35, 37] </ref>, where by "theory-oriented" we mean a notation used primarily as a basis for formal work on program semantics, and (2) a practical programming notation, Fortran 90 [1, 46], where by "practical programming notation" we mean a notation used primarily for the development of applications, particularly large-scale ones. <p> in HPF [43]: one that contains no subobjects | e.g., a scalar data object or a scalar element of an array. 28 2.4 arb composition and Dijkstra's guarded-command lan guage 2.4.1 Dijkstra's guarded-command language and our model It is straightforward to define the commands and constructors of Dijkstra's guarded-command language <ref> [35, 37] </ref> in terms of our model. <p> We sometimes use the proof format of Dijkstra and Scholten <ref> [37] </ref>, which is perhaps most concisely described via an example: Suppose we want to show that a formula A is equal to another formula C by showing that A = B and B = C for some intermediate formula B. <p> = (s 1 # fvg)) By symmetry, a similar construction applies to computations in which a k is performed first. 2 2.9 Appendix: Dijkstra's guarded-command language and our model, details In this section we sketch definitions in our model for some of the commands and constructors of Dijkstra's guarded-command language <ref> [35, 37] </ref>. 2.9.1 Simple commands Definition 2.29 (Skip). <p> Observe that this definition is given in terms of restricted forms of the alternative (IF ) and repetition (DO ) constructs of Dijkstra's guarded-command language <ref> [35, 37] </ref>, but it applies to any programming notation with equivalent constructs. Definition 4.5 (par-compatible). <p> The proof makes use of the restrictions on when variables that affect b can be written. For terminating computations, the proof can be constructed using the standard unrolling of the repetition command (as in [42] or <ref> [37] </ref>) together with Theorem 4.8 and Theorem 4.11. For nonterminating computations, the proof must consider two classes of computations: those that fail to terminate because an iteration of one of the loops fails to terminate, and those that fail to terminate because one of the loops iterates forever.
Reference: [38] <author> D. C. Dinucci and R. G. Babb II. </author> <title> Development of portable parallel programs with Large-Grain Data Flow 2. </title> <editor> In G. Goos and J. Hartmanis, editors, </editor> <booktitle> CONPAR 90 | VAPP IV, volume 457 of Lecture Notes in Computer Science, </booktitle> <pages> pages 253-264. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Kumaran and Quinn [48] focus more on automated conversion of template-based applications into efficient programs for different architectures. Dataflow patterns. Other work, e.g., Dinucci and Babb <ref> [38] </ref>, has addressed the question of structuring parallel programs in terms of dataflow; our work differs in that it addresses patterns of both dataflow and computation. 162 Distributed objects.
Reference: [39] <author> N. Fang. </author> <title> Engineering parallel algorithms. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Our approach is similar, but we allow the sequential code to reference the containing parallel structure, as in the mesh-spectral archetype examples. Program development strategies. Fang <ref> [39] </ref> describes a programming strategy similar to ours, but with less focus on the identification and exploitation of patterns. The Basel approach [16] is more concerned with developing and exploiting a general approach for classifying and dealing with parallel programs.
Reference: [40] <author> I. T. Foster and K. M. Chandy. </author> <title> FORTRAN M: A language for modular parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1) </volume> <pages> 24-35, </pages> <year> 1995. </year>
Reference-contexts: Examples include Fortran M <ref> [40] </ref> (which supports multiple-address-space parallel composition via process blocks and single-sender-single-receiver message-passing via channels) and MPI [58] (which assumes execution in an environment of multiple-address-space parallel composition and supports single-sender-single-receiver message-passing via tagged point-to-point sends and receives). 5.4.2 Example Program P 00 from Section 5.3.2 can be implemented by the following <p> be implemented in any desired language or library as part of an archetype implementation. 7.2.1.4 Implementation We have developed for the mesh-spectral archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines for the operations described in Section 7.2.1.3, with versions based on Fortran M <ref> [40] </ref> and Fortran with MPI [58]. The implementation is described in detail in [34]. <p> The communication operations consist of a restricted form of redistribution | row to column and vice versa | and support for reduction operations and file input/output. We have developed for this archetype an implementation (based on Fortran M <ref> [40] </ref>) consisting of a code skeleton and an archetype-specific library of communication routines. <p> We have developed for this archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines, 3 with versions based on Fortran M <ref> [40] </ref>, Fortran with p4 [17], and Fortran with NX [60].
Reference: [41] <author> E. Gamma, R. Helm, R. Johnson, and J. Vlissides. </author> <title> Design Patterns: Elements of Reusable Object-Oriented Software. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Design patterns. Many researchers have investigated the use of patterns in developing algorithms and applications. Our previous work [20, 23] explores a more general notion of archetypes and their role in developing both sequential and parallel programs. Gamma et al. <ref> [41] </ref> address primarily the issue of patterns of computation, in the context of object-oriented design. Our notion of a parallel program archetype, in contrast, includes patterns of dataflow and communication.
Reference: [42] <editor> D. Gries. </editor> <booktitle> The Science of Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: We define our programming models as simple extensions to the standard sequential model of Dijkstra [36, 37], Gries <ref> [42] </ref>, and others. We base our notions of program correctness on the work of Hoare [44] and others. Program development via stepwise refinement. <p> The proof makes use of the restrictions on when variables that affect b can be written. For terminating computations, the proof can be constructed using the standard unrolling of the repetition command (as in <ref> [42] </ref> or [37]) together with Theorem 4.8 and Theorem 4.11. For nonterminating computations, the proof must consider two classes of computations: those that fail to terminate because an iteration of one of the loops fails to terminate, and those that fail to terminate because one of the loops iterates forever. <p> We differ from much work on reasoning about parallel programs, for example Chandy and Misra [24] and Lamport [50], in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties). Our emphasis on program development by stepwise refinement builds on the work of Back [6], Gries <ref> [42] </ref>, and Hoare [44] for sequential programs, and Back [5], Martin [56], and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models. <p> by stepwise refinement builds on the work of Back [6], Gries <ref> [42] </ref>, and Hoare [44] for sequential programs, and Back [5], Martin [56], and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models. Since we are more interested in sequential-style specifications than in those involving ongoing behavior, our work differs considerably from much other work on parallel programming, for example that of Chandy and Misra (UNITY) [24] and Hoare (CSP) [45].
Reference: [43] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Automatic parallelization of sequential programs. Our work is in many respects complementary to efforts to develop parallelizing compilers, for example Fortran D [28] and HPF <ref> [43] </ref>. The focus of such work is on the automatic detection of exploitable parallelism, while our work addresses how to exploit parallelism once it is known to exist. Our theoretical framework could be used to prove not only manually-applied transformations but also those applied by parallelizing compilers. <p> file sequentially, mod:P should include a variable representing the file, since concurrent attempts by two programs to read the same file result in program actions that do not meet the commutativity test for arb-compatibility.) 2 4 An atomic data object is as defined in our semantics or, equivalently, in HPF <ref> [43] </ref>: one that contains no subobjects | e.g., a scalar data object or a scalar element of an array. 28 2.4 arb composition and Dijkstra's guarded-command lan guage 2.4.1 Dijkstra's guarded-command language and our model It is straightforward to define the commands and constructors of Dijkstra's guarded-command language [35, 37] in <p> forms, arb and par. (par composition is defined in Chapter 4.) 2.5.3.3 arball To allow us to express the arb composition of, for example, the iterations of a loop, we define an indexed form of arb composition, with syntax modeled after that of the FORALL construct of High Performance Fortran <ref> [43] </ref>, as follows. This notation is syntactic sugar only, and all theorems that apply to arb composition apply to arball as well. Definition 2.27 (arball). <p> Language constructs consistent with this form of composition include the par and parfor constructs of CC++ [21, 19], the INDEPENDENT directive of HPF <ref> [43] </ref>, and the PARALLEL DO and PARALLEL SECTIONS constructs of the Fortran X3H5 proposal [3]. 2.6.2.1 Parallel execution using HPF An arb-model program in which all arb compositions are of the arball form can be transformed into an equivalent program in HPF by replacing arball with forall and preceding each such <p> Such blocks correspond to sections of the parallel program in which processes execute independently and without interaction. * Each data-exchange operation consists of an arb-compatible set of assignment statements, with the following restrictions: 1 An atomic data object is as defined in HPF <ref> [43] </ref>: one that contains no subobjects | e.g., a scalar data object or a scalar element of an array. 147 No left-hand or right-hand side may reference atomic data objects belonging to more than one of the N simulated-local-data partitions. <p> Automatic parallelizing compilers. Much effort has gone into development of compilers that automatically recognize potential concurrency and emit parallel code, for example Fortran D [28] and 161 HPF <ref> [43] </ref>. The focus of such work is on the automatic detection of exploitable parallelism, while our work addresses how to exploit parallelism once it is known to exist. Our theoretical framework could be used to prove not only manually-applied transformations but also those applied by parallelizing compilers. Design patterns.
Reference: [44] <author> C. A. R. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 576-583, </pages> <year> 1969. </year>
Reference-contexts: We define our programming models as simple extensions to the standard sequential model of Dijkstra [36, 37], Gries [42], and others. We base our notions of program correctness on the work of Hoare <ref> [44] </ref> and others. Program development via stepwise refinement. Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back [6], Gries, and Hoare [44], and for parallel programs in the work of, for example, Back [5], Martin [56], and <p> We base our notions of program correctness on the work of Hoare <ref> [44] </ref> and others. Program development via stepwise refinement. Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back [6], Gries, and Hoare [44], and for parallel programs in the work of, for example, Back [5], Martin [56], and Van de Velde [74]. Operational models. <p> We take our notion of program specifications from some of the standard ways of giving program specifications for sequential program, for example those of Hoare <ref> [44] </ref> and Dijkstra [36]. We differ from much work on reasoning about parallel programs, for example Chandy and Misra [24] and Lamport [50], in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties). <p> Our emphasis on program development by stepwise refinement builds on the work of Back [6], Gries [42], and Hoare <ref> [44] </ref> for sequential programs, and Back [5], Martin [56], and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models.
Reference: [45] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: Such a program refines the original program: Each send-receive pair of operations produces the same result as the assignment statement from which it was derived (as discussed in <ref> [45] </ref> and [56]), and the arb-compatibility of the assignments ensures that these pairs can be executed in any order without changing the result. <p> Parallel programming models. Since we are more interested in sequential-style specifications than in those involving ongoing behavior, our work differs considerably from much other work on parallel programming, for example that of Chandy and Misra (UNITY) [24] and Hoare (CSP) <ref> [45] </ref>. Other work on parallel programs with sequential equivalents includes that of Valiant (BSP) [73] and Thornley [71]; the former, however, emphasizes performance analysis over analysis of correctness, while the latter focuses mostly on programs for a shared-memory model. Operational models of program semantics.
Reference: [46] <author> International Standards Organization. ISO/IEC 1539:1991 (E), </author> <title> Fortran 90, </title> <year> 1991. </year>
Reference-contexts: We then revisit these ideas in the context of two representative programming notations: (1) a theory-oriented notation, Dijkstra's guarded-command language [35, 37], where by "theory-oriented" we mean a notation used primarily as a basis for formal work on program semantics, and (2) a practical programming notation, Fortran 90 <ref> [1, 46] </ref>, where by "practical programming notation" we mean a notation used primarily for the development of applications, particularly large-scale ones. We present our ideas in the context of a theory-oriented notation to demonstrate that they can be made rigorous in a notation for which a formal semantics is defined. <p> following example is not a valid arb composition; the two assignments are not arb-compatible. arb (a := 1; b := a) 2.5 arb composition and Fortran 90 2.5.1 Fortran 90 and our model Giving a formal definition of the semantics of a large practical programming language such as Fortran 90 <ref> [1, 46] </ref> is far from trivial.
Reference: [47] <author> E. Isaacson and H. B. Keller. </author> <title> Analysis of Numerical Methods. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1966. </year>
Reference-contexts: (((N/2)+1):N, :, 2) our models and methodology. 118 6.2 1-dimensional heat equation solver 6.2.1 Problem description In this example, the goal is to solve the 1-dimensional heat diffusion equation @U = @x 2 with boundary condition U (x; t) = 1 for boundary x : Following the method described in <ref> [47] </ref>, we discretize the problem domain (representing the x di mension as an array of N points) and use the following approximation: U (x i ; t k+1 ) U (x i ; t k ) = x 2 We assume an initial value of 0 for all non-boundary points.
Reference: [48] <author> S. Kumaran and M. J. Quinn. </author> <title> An architecture-adaptable problem solving environment for scientific computing, </title> <note> 1996. Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: Ballance et al. [9] are more explicitly concerned with the development of tools for application support; while our work can be exploited to create such tools, it is not our primary focus. Kumaran and Quinn <ref> [48] </ref> focus more on automated conversion of template-based applications into efficient programs for different architectures. Dataflow patterns.
Reference: [49] <author> K. S. Kunz and R. J. Luebbers. </author> <title> The Finite Difference Time Domain Method for Electromag-netics. </title> <publisher> CRC Press, </publisher> <year> 1993. </year>
Reference-contexts: Two versions of this code were available: a public-domain version ("version A", described in <ref> [49] </ref>) that performs only the near-field calculations, and an export-controlled version ("version C", described in [11]) that performs both near-field and far-field calculations.
Reference: [50] <author> L. Lamport. </author> <title> Proving the correctness of multiprocess programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 2 </volume> <pages> 125-143, </pages> <year> 1977. </year>
Reference-contexts: We take our notion of program specifications from some of the standard ways of giving program specifications for sequential program, for example those of Hoare [44] and Dijkstra [36]. We differ from much work on reasoning about parallel programs, for example Chandy and Misra [24] and Lamport <ref> [50] </ref>, in emphasizing sequential-style specifications over specifications describing ongoing behavior (e.g., safety and progress properties).
Reference: [51] <author> L. Lamport. </author> <title> A temporal logic of actions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3) </volume> <pages> 872-923, </pages> <year> 1994. </year>
Reference-contexts: Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle [52], Lamport <ref> [51] </ref>, Manna and Pnueli [54], and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. <p> most proofs; Section 2.7 presents the same material formally and in more detail, including a description (Section 2.7.1) of notational conventions. 2.1.1 Overview Treating programs as state-transition systems is not a new approach; it has been used in work such as Chandy and Misra [24], Lynch and Tuttle [52], Lamport <ref> [51] </ref>, Manna and Pnueli [54], and Pnueli [61] to reason about both parallel and sequential programs. <p> Operational models of program semantics. Our operational model adapts the ideas of state-transitions systems, as described in Chandy and Misra [24], Lynch and Tuttle [52], Lamport <ref> [51] </ref>, Manna and Pnueli [54], and Pnueli [61]; we give a formulation of these ideas that is aimed at facilitating our proofs. Automatic parallelizing compilers.
Reference: [52] <author> N. A. Lynch and M. R. Tuttle. </author> <title> Hierarchical correctness proofs for distributed algorithms. </title> <booktitle> In Proceedings of the 6th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1987. </year> <month> 170 </month>
Reference-contexts: Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle <ref> [52] </ref>, Lamport [51], Manna and Pnueli [54], and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. <p> sketches of most proofs; Section 2.7 presents the same material formally and in more detail, including a description (Section 2.7.1) of notational conventions. 2.1.1 Overview Treating programs as state-transition systems is not a new approach; it has been used in work such as Chandy and Misra [24], Lynch and Tuttle <ref> [52] </ref>, Lamport [51], Manna and Pnueli [54], and Pnueli [61] to reason about both parallel and sequential programs. <p> Operational models of program semantics. Our operational model adapts the ideas of state-transitions systems, as described in Chandy and Misra [24], Lynch and Tuttle <ref> [52] </ref>, Lamport [51], Manna and Pnueli [54], and Pnueli [61]; we give a formulation of these ideas that is aimed at facilitating our proofs. Automatic parallelizing compilers.
Reference: [53] <author> B. J. MacLennan. </author> <title> Functional Programming: Practice and Theory. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: (s 2 or s 0 2 ) by executing first a and then b, then we can reach the same state by first executing b and then a, and vice versa. 2 Remarks about Definition 2.13. * a and b commute exactly when a and b have the diamond property <ref> [25, 53] </ref>. 2 We now define the desired condition. 20 Definition 2.14 (arb-compatible).
Reference: [54] <author> Z. Manna and A. Pnueli. </author> <title> Completing the temporal picture. </title> <journal> Theoretical Computer Science, </journal> <volume> 83(1) </volume> <pages> 97-130, </pages> <year> 1991. </year>
Reference-contexts: Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli <ref> [54] </ref>, and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. Programming models similar in spirit to ours have been proposed by Valiant [73] and Thornley [71]; our model differs in that we provide a more explicit supporting theoretical framework and in the use we make of archetypes. <p> presents the same material formally and in more detail, including a description (Section 2.7.1) of notational conventions. 2.1.1 Overview Treating programs as state-transition systems is not a new approach; it has been used in work such as Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli <ref> [54] </ref>, and Pnueli [61] to reason about both parallel and sequential programs. <p> Operational models of program semantics. Our operational model adapts the ideas of state-transitions systems, as described in Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli <ref> [54] </ref>, and Pnueli [61]; we give a formulation of these ideas that is aimed at facilitating our proofs. Automatic parallelizing compilers. Much effort has gone into development of compilers that automatically recognize potential concurrency and emit parallel code, for example Fortran D [28] and 161 HPF [43].
Reference: [55] <author> A. J. Martin. </author> <title> An axiomatic definition of synchronization primitives. </title> <journal> Acta Informatica, </journal> <volume> 16 </volume> <pages> 219-235, </pages> <year> 1981. </year>
Reference-contexts: Most details of the specification were obtained from [72]; the overall method (in which initiations and completions of a command are considered separately) owes much to <ref> [55] </ref>. 4.1.2 Definitions We define barrier synchronization by extending the definition of parallel composition given in Definition 2.12 and defining a new command, barrier. <p> The terminology ("slack") and overall method (in which initiations and completions of a command are considered separately) are based on <ref> [55] </ref>. 5.1.2 Definitions Like many other implementations of message-passing, for example those of [2] and [69], our definition represents channels as queues: We define for each ordered pair (P j ; P k ) a queue C j;k whose elements represent messages in transit from P j to P k .
Reference: [56] <author> A. J. Martin. </author> <title> Compiling communicating processes into delay-insensitive VLSI circuits. </title> <journal> Distributed Computing, </journal> <volume> 1(4) </volume> <pages> 226-234, </pages> <year> 1986. </year>
Reference-contexts: Program development via stepwise refinement. Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back [6], Gries, and Hoare [44], and for parallel programs in the work of, for example, Back [5], Martin <ref> [56] </ref>, and Van de Velde [74]. Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. <p> Such a program refines the original program: Each send-receive pair of operations produces the same result as the assignment statement from which it was derived (as discussed in [45] and <ref> [56] </ref>), and the arb-compatibility of the assignments ensures that these pairs can be executed in any order without changing the result. <p> Our emphasis on program development by stepwise refinement builds on the work of Back [6], Gries [42], and Hoare [44] for sequential programs, and Back [5], Martin <ref> [56] </ref>, and Van de Velde [74] for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models.
Reference: [57] <author> B. Massingill. </author> <title> The mesh archetype. </title> <type> Technical Report CS-TR-96-25, </type> <institution> California Institute of Technology, </institution> <year> 1997. </year> <note> Also available via h http://www.etext.caltech.edu/Implementations/ i. </note>
Reference-contexts: A key element of this archetype is support for ensuring that these requirements are met. This support can take the form of guidelines for manually transforming programs, as in our archetype-implementation user guides <ref> [34, 57, 33] </ref>, or it could be expressed in terms of more formal transformations with arguments for their correctness, as in the transformations of Chapter 3. 7.2.1.3 Communication patterns This data-distribution scheme thus gives rise to the need (in distributed memory) for a small set of communication operations: Grid redistribution. <p> We have developed for this archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines, 3 with versions based on Fortran M [40], Fortran with p4 [17], and Fortran with NX [60]. The implementation is described in detail in <ref> [57] </ref>; it has been used to run applications on the IBM SP, the Intel Delta, the Intel Paragon, and a network of Sun workstations. 7.3 Applications This section discusses the second phase of the archetypes-related experimental work, in which we used the example archetypes presented in Section 7.2 to develop applications. <p> Nonetheless, because of its simplicity, we chose this method for an initial parallelization. 8.3.3 Applying our methodology Determining how to apply the strategy. First, we determined how to apply the parallelization strategy, guided by documentation <ref> [57] </ref> for the mesh archetype, as follows: * Identify which variables should be distributed (among grid processes) and which duplicated (across all processes). For those variables that are to be distributed, determine which ones should be surrounded by a ghost boundary.
Reference: [58] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <pages> 8(3-4), </pages> <year> 1994. </year>
Reference-contexts: Communication libraries. Much work has been done in developing program libraries intended to insulate application developers from the details of the parallel architecture on which their programs are to execute, for example MPI <ref> [58] </ref>. <p> Examples include Fortran M [40] (which supports multiple-address-space parallel composition via process blocks and single-sender-single-receiver message-passing via channels) and MPI <ref> [58] </ref> (which assumes execution in an environment of multiple-address-space parallel composition and supports single-sender-single-receiver message-passing via tagged point-to-point sends and receives). 5.4.2 Example Program P 00 from Section 5.3.2 can be implemented by the following Fortran M program: program main integer a (4) 114 inport (integer) inp (2) outport (integer) outp <p> language or library as part of an archetype implementation. 7.2.1.4 Implementation We have developed for the mesh-spectral archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines for the operations described in Section 7.2.1.3, with versions based on Fortran M [40] and Fortran with MPI <ref> [58] </ref>. The implementation is described in detail in [34]. <p> Our work on archetypes differs from this work in that we focus more on the pattern of computation and on identifying and exploiting patterns of computation and communication. Communication libraries. Many researchers have investigated and developed reusable general libraries of communication routines; MPI <ref> [58] </ref> is a notable example. Others have developed more specialized libraries, for example MPI-RGL [8] for regular grids.
Reference: [59] <author> J. M. Morris. </author> <title> Piecewise data refinement. </title> <editor> In E. W. Dijkstra, editor, </editor> <title> Formal Development of Programs and Proofs. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1990. </year>
Reference-contexts: holds, a read reference to the original variable can be transformed into a read reference to any one of the copies without changing the meaning of the program. 74 3.3.4.1 Phase 1: duplicating the variable We can accomplish such a transformation using the techniques of data refinement, as described in <ref> [59] </ref>. <p> Let P 0 be the result of applying these refinement rules to P . Then P v P 0 . We do not give a detailed proof, but such a proof could be produced using the rules of data refinement (as given in <ref> [59] </ref>) and structural induction on P . 3.3.4.2 Phase 2: further refinements For our purposes, however, P 0 as just defined may not be quite what we want, since in some situations it would be advantageous to postpone re-establishing copy consistency (e.g., it might make it possible 75 to apply Theorem
Reference: [60] <author> P. Pierce. </author> <title> The NX message-passing interface. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 463-480, </pages> <year> 1994. </year>
Reference-contexts: We have developed for this archetype an implementation consisting of a code skeleton and an archetype-specific library of communication routines, 3 with versions based on Fortran M [40], Fortran with p4 [17], and Fortran with NX <ref> [60] </ref>.
Reference: [61] <author> A. Pnueli. </author> <title> The temporal semantics of concurrent programs. </title> <journal> Theoretical Computer Science, </journal> <volume> 13 </volume> <pages> 45-60, </pages> <year> 1981. </year>
Reference-contexts: Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli <ref> [61] </ref>. 1.2.2 Related and complementary work Parallel programming models. Programming models similar in spirit to ours have been proposed by Valiant [73] and Thornley [71]; our model differs in that we provide a more explicit supporting theoretical framework and in the use we make of archetypes. <p> material formally and in more detail, including a description (Section 2.7.1) of notational conventions. 2.1.1 Overview Treating programs as state-transition systems is not a new approach; it has been used in work such as Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli <ref> [61] </ref> to reason about both parallel and sequential programs. <p> Operational models of program semantics. Our operational model adapts the ideas of state-transitions systems, as described in Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli <ref> [61] </ref>; we give a formulation of these ideas that is aimed at facilitating our proofs. Automatic parallelizing compilers. Much effort has gone into development of compilers that automatically recognize potential concurrency and emit parallel code, for example Fortran D [28] and 161 HPF [43].
Reference: [62] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: over efficiency; the programs we derive for shared-memory and distributed-memory architectures are more efficient than the arb-model programs from which they are produced, but additional transformations could be applied to further improve efficiency. 6.1 2-dimensional FFT 6.1.1 Problem description This program performs a 2-dimensional FFT in place, as described in <ref> [62] </ref>.
Reference: [63] <author> D. I. Pullin. </author> <title> Direct simulation methods for compressible ideal gas flow. </title> <journal> Journal of Computational Physics, </journal> <volume> 34:231, </volume> <year> 1980. </year>
Reference-contexts: These two codes simulate high Mach number compressible flow using a conservative and monotonicity-preserving finite difference scheme <ref> [63] </ref>. Both are based on the 2-dimensional mesh archetype and have been implemented in Fortran with NX for the Intel Delta and the Intel Paragon. Figure 7.10 shows execution times and speedups for the first code, executing on the Intel Delta.
Reference: [64] <author> A. Rifkin and B. L. Massingill. </author> <title> Performance analysis for mesh and mesh-spectral archetype applications. </title> <type> Technical Report CS-TR-96-27, </type> <institution> California Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: Archetypes can also provide frameworks for testing and documentation. Performance models. Archetypes may also be helpful in developing performance models for classes of programs with common structure, as discussed in <ref> [64] </ref>. Program composition. Archetypes can be useful in structuring programs that combine task and data parallelism, as described in [23]. 7.1.2 An archetype-based program development strategy Our general strategy for writing programs using archetypes is as follows: Start with a sequential algorithm or problem description. Identify an appropriate archetype. <p> Within these constraints, programmers may choose any data distribution; choosing the data distribution that gives the best performance is important but orthogonal to the concerns of this chapter. However, an archetype-based performance model, such as that described in <ref> [64] </ref>, may help with this choice. 132 to compute a local reduction result and then combining them, for example via recursive doubling. Reduction operations, like grid operations, may be performed on data distributed in any convenient fashion. After completion of a reduction operation, all processes have access to its result.
Reference: [65] <author> R. Samtaney and D. I. Meiron. </author> <title> Hypervelocity Richtmyer-Meshkov instability. </title> <journal> Physics of Fluids, </journal> <volume> 9(6) </volume> <pages> 1783-1803, </pages> <year> 1997. </year>
Reference-contexts: Figure 7.10 shows execution times and speedups for the first code, executing on the Intel Delta. Speedups are relative to single-processor execution of the parallel code. The second version of the code <ref> [65] </ref> is notable for the fact that it was developed by an "end user" (applied mathematician) using the mesh archetype implementation and documentation, with minimal assistance from the archetype developers. steps, using Fortran with NX on the Intel Delta.
Reference: [66] <author> D. C. Schmidt. </author> <title> Using design patterns to develop reusable object-oriented communication software. </title> <journal> Communications of the ACM, </journal> <volume> 38(10) </volume> <pages> 65-74, </pages> <year> 1995. </year>
Reference-contexts: Gamma et al. [41] address primarily the issue of patterns of computation, in the context of object-oriented design. Our notion of a parallel program archetype, in contrast, includes patterns of dataflow and communication. Schmidt <ref> [66] </ref> focuses more on parallel structure, but in a different context from our work and with less emphasis on code reuse. Shaw [67] examines higher-level patterns in the context of software architectures.
Reference: [67] <author> M. Shaw. </author> <title> Patterns for software architectures. </title> <editor> In J. O. Coplien and D. C. Schmidt, editors, </editor> <booktitle> Pattern Languages of Program Design, </booktitle> <pages> pages 453-462. </pages> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Our notion of a parallel program archetype, in contrast, includes patterns of dataflow and communication. Schmidt [66] focuses more on parallel structure, but in a different context from our work and with less emphasis on code reuse. Shaw <ref> [67] </ref> examines higher-level patterns in the context of software architectures. Brinch Hansen's work on parallel structures [15] is similar in motivation to our work, but his model programs are typically more narrowly defined than our archetypes.
Reference: [68] <author> H. Shi and J. Schaeffer. </author> <title> Parallel sorting by regular sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(4) </volume> <pages> 361-372, </pages> <month> April </month> <year> 1992. </year> <month> 171 </month>
Reference-contexts: Two variants of the quicksort algorithm are presented: a standard recursive version, and a "one-deep" nonrecursive version more suitable for scalable parallel implementations <ref> [26, 68] </ref>. 6.4.2 Recursive program This program sorts the array of integers in place.
Reference: [69] <author> P. A. G. Sivilotti. </author> <title> A verified integration of imperative parallel programming paradigms in an object-oriented language. </title> <type> Technical Report CS-TR-93-21, </type> <institution> California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: n) ^ (iR j;k = n)) ; (cR j;k = n) : We observe that this specification, like the one for barrier synchronization in Chapter 4, simply captures formally the usual meaning of this type of message passing, and is consistent with other formalizations, for example those of [2] and <ref> [69] </ref>. The terminology ("slack") and overall method (in which initiations and completions of a command are considered separately) are based on [55]. 5.1.2 Definitions Like many other implementations of message-passing, for example those of [2] and [69], our definition represents channels as queues: We define for each ordered pair (P j <p> message passing, and is consistent with other formalizations, for example those of [2] and <ref> [69] </ref>. The terminology ("slack") and overall method (in which initiations and completions of a command are considered separately) are based on [55]. 5.1.2 Definitions Like many other implementations of message-passing, for example those of [2] and [69], our definition represents channels as queues: We define for each ordered pair (P j ; P k ) a queue C j;k whose elements represent messages in transit from P j to P k .
Reference: [70] <author> P. A. G. Sivilotti. </author> <title> Reliable synchronization primitives for Java threads. </title> <type> Technical Report CS-TR-96-11, </type> <institution> California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: n times: (8j :: (iB j = cB j + 1) ^ (iB j = n)) ; (8j :: (cB j = n)) : We observe that this specification simply captures formally the usual meaning of barrier synchronization and is consistent with other formalizations, for example those of [2] and <ref> [70] </ref>. <p> This combined definition implements a common 87 approach to barrier synchronization based on keeping a count of processes waiting at the barrier, as in [2] and <ref> [70] </ref>. In the context of our model, we implement this approach using two protocol variables local to the parallel composition, a count Q of suspended components and a flag Arriving that indicates whether components are arriving at the barrier or leaving.
Reference: [71] <author> J. Thornley. </author> <title> A parallel programming model with sequential semantics. </title> <type> Technical Report CS-TR-96-12, </type> <institution> California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Programming models similar in spirit to ours have been proposed by Valiant [73] and Thornley <ref> [71] </ref>; our model differs in that we provide a more explicit supporting theoretical framework and in the use we make of archetypes. Automatic parallelization of sequential programs. Our work is in many respects complementary to efforts to develop parallelizing compilers, for example Fortran D [28] and HPF [43]. <p> Other work on parallel programs with sequential equivalents includes that of Valiant (BSP) [73] and Thornley <ref> [71] </ref>; the former, however, emphasizes performance analysis over analysis of correctness, while the latter focuses mostly on programs for a shared-memory model. Operational models of program semantics.
Reference: [72] <author> J. Thornley and K. M. Chandy. </author> <title> Barriers: Specification. Unpublished document. </title>
Reference-contexts: Most details of the specification were obtained from <ref> [72] </ref>; the overall method (in which initiations and completions of a command are considered separately) owes much to [55]. 4.1.2 Definitions We define barrier synchronization by extending the definition of parallel composition given in Definition 2.12 and defining a new command, barrier.
Reference: [73] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: Programming models similar in spirit to ours have been proposed by Valiant <ref> [73] </ref> and Thornley [71]; our model differs in that we provide a more explicit supporting theoretical framework and in the use we make of archetypes. Automatic parallelization of sequential programs. <p> Other work on parallel programs with sequential equivalents includes that of Valiant (BSP) <ref> [73] </ref> and Thornley [71]; the former, however, emphasizes performance analysis over analysis of correctness, while the latter focuses mostly on programs for a shared-memory model. Operational models of program semantics.
Reference: [74] <author> E. F. Van de Velde. </author> <title> Concurrent Scientific Computing. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Our approach to program development is based on stepwise refinement and program transformations, as described for sequential programs in the work of Back [6], Gries, and Hoare [44], and for parallel programs in the work of, for example, Back [5], Martin [56], and Van de Velde <ref> [74] </ref>. Operational models. Our operational model is based on defining programs as state-transition systems, as in the work of Chandy and Misra [24], Lynch and Tuttle [52], Lamport [51], Manna and Pnueli [54], and Pnueli [61]. 1.2.2 Related and complementary work Parallel programming models. <p> (k, uk) end do 122 6.3 2-dimensional iterative Poisson solver This problem is similar in many respects to the heat-equation problem; it is included to show how iteration until convergence differs from fixed iteration. 6.3.1 Problem description This example is largely based on the discussion of the Poisson problem in <ref> [74] </ref>. 1 The program finds a numerical solution to the Poisson problem @x 2 @y 2 = f (x; y) with Dirichlet boundary condition u (x; y) = g (x; y) where f and g are given. <p> Note the nesting of arb and arball in the initialization. 1 We derive a slightly different program because of our focus on readability over efficiency. Note however that nothing in our methodology precludes developing the same program presented in <ref> [74] </ref>. 2 Actually, we can reduce the storage requirements of the program by reducing the number of points for which we maintain both "current" and "next" values, as in [74]. <p> Note however that nothing in our methodology precludes developing the same program presented in <ref> [74] </ref>. 2 Actually, we can reduce the storage requirements of the program by reducing the number of points for which we maintain both "current" and "next" values, as in [74]. <p> Our emphasis on program development by stepwise refinement builds on the work of Back [6], Gries [42], and Hoare [44] for sequential programs, and Back [5], Martin [56], and Van de Velde <ref> [74] </ref> for parallel programs. Sequential programming models. We base our programming model on the standard sequential model as defined for example by Gries [42]. Parallel programming models.
References-found: 74

