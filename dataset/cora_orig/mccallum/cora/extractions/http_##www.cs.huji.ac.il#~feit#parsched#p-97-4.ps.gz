URL: http://www.cs.huji.ac.il/~feit/parsched/p-97-4.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched97.html
Root-URL: http://www.cs.huji.ac.il
Email: feit@cs.huji.ac.il  
Title: Memory Usage in the LANL CM-5 Workload  
Author: Dror G. Feitelson 
Web: or http://www.cs.huji.ac.il/~feit  
Address: 91904 Jerusalem, Israel  
Affiliation: Institute of Computer Science The Hebrew University,  
Abstract: It is generally agreed that memory requirements should be taken into account in the scheduling of parallel jobs. However, so far the work on combined processor and memory scheduling has not been based on detailed information and measurements. To rectify this problem, we present an analysis of memory usage by a production workload on a large parallel machine, the 1024-node CM-5 installed at Los Alamos National Lab. Our main observations are The distribution of memory requests has strong discrete components, i.e. some sizes are much more popular than others. Many jobs use a relatively small fraction of the memory available on each node, so there is some room for time slicing among several memory-resident jobs. Larger jobs (using more nodes) tend to use more memory, but it is difficult to characterize the scaling of per-processor memory usage.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Alverson, S. Kahan, R. Korry, C. McCann, and B. Smith, </author> <title> "Scheduling on the Tera MTA". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitel-son and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 19-44, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 2. <author> G. M. </author> <title> Amdahl, "Validity of the single processor approach to achieving large scale computer capabilities". </title> <booktitle> In AFIPS Spring Joint Comput. Conf., </booktitle> <volume> vol. 30, </volume> <pages> pp. 483-485, </pages> <month> Apr </month> <year> 1967. </year>
Reference-contexts: This assumes that the work done by a job is fixed, and parallelism is used to solve the same problems faster. Therefore the runtime and per-processor memory usage are assumed to be inversely proportional to the degree of parallelism. This model is the basis for Amdahl's law <ref> [2] </ref>. Fixed time [11,12,22]. Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed.
Reference: 3. <author> D. C. Burger, R. S. Hyder, B. P. Miller, and D. A. Wood, </author> <title> "Paging tradeoffs in distributed-shared-memory multiprocessors". </title> <journal> J. Supercomput. </journal> <volume> 10(1), </volume> <pages> pp. 87-104, </pages> <year> 1996. </year>
Reference: 4. <author> J. J. Dongarra, H. W. Meuer, and E. Strohmaier, </author> <title> "Top500 supercomputer sites". </title> <note> http://www.netlib.org/benchmark/top500.html. (updated every 6 months). </note>
Reference-contexts: While such machines are no longer manufactured, this one is still in active use, and considered quite powerful | it ranked 21st in the world in the November '96 Top500 list, and came in first among Connection Machines <ref> [4] </ref>. The CM-5 is a distributed memory machine based on SPARC processors. 1024 of the 1056 nodes are used for parallel computation, with a total of 32 GB of memory (i.e. 32 MB per node).
Reference: 5. <author> D. G. Feitelson, </author> <title> "Packing schemes for gang scheduling". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 89-110, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: We can get some speculative evidence concerning this question by comparing the resource requirements of jobs that actually ran on different size partitions. Our preliminary results concerning memory usage, combined with our previous results regarding the correlation between runtime and parallelism <ref> [5] </ref>, indicate that the truth probably lies between the fixed-time model and the memory bound model. In a nutshell, all three resources tend to scale up together: larger jobs use more processors, use more memory, and run longer.
Reference: 6. <author> D. G. Feitelson, </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Research Report RC 19790 (87657), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Oct </month> <year> 1994. </year>
Reference: 7. <author> D. G. Feitelson and M. A. Jette, </author> <title> "Improved utilization and responsiveness with gang scheduling". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. Rudolph (eds.), </editor> <publisher> Springer Verlag, </publisher> <year> 1997. </year> <note> Lecture Notes in Computer Science (this volume). </note>
Reference: 8. <author> D. G. Feitelson and B. Nitzberg, </author> <title> "Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 337-360, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 9. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Parallel job scheduling: issues and approaches". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 1-18, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 10. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Toward convergence in job schedulers for parallel supercomputers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Fei-telson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 1-26, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> Lecture Notes in Computer Science Vol. 1162. </note>
Reference-contexts: Two of the most important resources are computing cycles and memory locations. The allocation of computing cycles allows for some tradeoff between the degree of parallelism and time | moldable and malleable jobs may use less processors for more time to accumulate the same overall number of cycles <ref> [10] </ref>. With memory, such a tradeoff is only possible if paging is used. As paging is typically considered to be too expensive due to its overhead and adverse effect on communication and synchronization, parallel jobs typically have to be memory resident throughout their execution.
Reference: 11. <author> J. L. Gustafson, </author> <title> "Reevaluating Amdahl's law". </title> <journal> Comm. ACM 31(5), </journal> <pages> pp. 532-533, </pages> <month> May </month> <year> 1988. </year> <note> See also Comm. ACM 32(2), pp. 262-264, </note> <month> Feb </month> <year> 1989, </year> <journal> and Comm. ACM 32(8), </journal> <pages> pp. 1014-1016, </pages> <month> Aug </month> <year> 1989. </year>
Reference: 12. <author> J. L. Gustafson, G. R. Montry, and R. E. Benner, </author> <title> "Development of parallel methods for a 1024-processor hypercube". </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 9(4), </volume> <pages> pp. 609-638, </pages> <month> Jul </month> <year> 1988. </year>
Reference: 13. <author> C. McCann and J. Zahorjan, </author> <title> "Scheduling memory constrained jobs on distributed memory parallel computers". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 208-219, </pages> <month> May </month> <year> 1995. </year>
Reference: 14. <author> Minnesota Supercomputer Center, Inc., </author> <title> The Distributed Job Manager Administration Guide. </title> <booktitle> 1993. </booktitle> <address> ftp://ec.msc.edu/pub/LIGHTNING/djm 1.0.0 src.tar.Z. </address>
Reference-contexts: Runtimes are expressed in seconds (s), and memory usage in kilobytes (KB). The data was collected by DJM <ref> [14] </ref>, the Distributed Job Manager used on CM-5 machines. Most jobs were indeed run using DJM, but 1492 of them were "foreign", i.e. launched directly by users. The log contains less information about foreign jobs, e.g. they do not have predefined resource requests.
Reference: 15. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Coordinated allocation of memory and processors in multiprocessors". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 57-67, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: When the partition size is adjustable, do not reduce it too much, because small partitions cause jobs to run longer and thus increase the memory pressure <ref> [15] </ref>. In systems that use swapping, make the residence time proportional to the memory footprint size in order to amortize the cost of loading the memory image [1,7].
Reference: 16. <author> V. G. J. Peris, M. S. Squillante, and V. K. Naik, </author> <title> "Analysis of the impact of memory in distributed parallel processing systems". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 5-18, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Again, concern has been expressed that it would not be possible to reduce the partition sizes and exploit this feature, because of memory requirements <ref> [16] </ref>. Our results indicate that rather small partition sizes may suffice in many cases. 4.2 Modeling Memory Usage A separate issue is the modeling of memory usage for use in simulations and analysis.
Reference: 17. <author> S. K. Setia, </author> <title> "The interaction between memory allocation and adaptive partitioning in message-passing multicomputers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp. 146-165, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference: 18. <author> J. P. Singh, J. L. Hennessy, and A. Gupta, </author> <title> "Scaling parallel programs for multiprocessors: methodology and examples". </title> <booktitle> Computer 26(7), </booktitle> <pages> pp. 42-50, </pages> <month> Jul </month> <year> 1993. </year>
Reference-contexts: However, it seems that all these models are over-simplified to the point where it is hard to correlate them with measured results. In particular, users configure their applications according to their needs rather than according to the way resources happen to be packaged in the machine <ref> [18] </ref>. Thus users rarely use all the memory available, on any size partition. It is true, however, that they tend to use more on larger partitions.
Reference: 19. <author> X-H. Sun and L. M. Ni, </author> <title> "Scalable problems and memory-bounded speedup". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 19(1), </volume> <pages> pp. 27-37, </pages> <month> Sep </month> <year> 1993. </year>
Reference-contexts: Here it is assumed that parallelism is used to solve increasingly larger problems, under the constraint that the total runtime stays fixed. In this case, the runtime distribution is independent of the degree of parallelism, but the total memory usage is expected to increase with increased parallelism. Memory bound <ref> [19] </ref>. This model assumes that the problem size is increased to fill the available memory on the larger machine, so that the per-processor memory usage is maintained.
Reference: 20. <institution> Thinking Machines Corp., </institution> <type> Connection Machine CM-5 Technical Summary. </type> <month> Nov </month> <year> 1992. </year>
Reference-contexts: The machine is statically partitioned into partitions with power-of-two numbers of processors from 32 up to 512. Within each partition, jobs may be gang-scheduled, or they may request dedicated use of the partition <ref> [20] </ref>. While the fact that only 5 sizes are available is restrictive, other work on parallel workload characterization has shown conclusively that users prefer powers of two even if there are no architectural constraints [8,5].
Reference: 21. <author> K. Y. Wang and D. C. Marinescu, </author> <title> "Correlation of the paging activity of individual node programs in the SPMD execution model". </title> <booktitle> In 28th Hawaii Intl. Conf. System Sciences, </booktitle> <volume> vol. I, </volume> <pages> pp. 61-71, </pages> <month> Jan </month> <year> 1995. </year>
Reference: 22. <author> P. H. Worley, </author> <title> "The effect of time constraints on scaled speedup". </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 11(5), </volume> <pages> pp. 838-858, </pages> <month> Sep </month> <year> 1990. </year>
References-found: 22

