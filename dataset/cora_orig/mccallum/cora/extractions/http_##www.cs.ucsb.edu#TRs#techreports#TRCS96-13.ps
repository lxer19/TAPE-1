URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-13.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Design, Implementation, and Analysis of a Split-C Profiler  
Author: Bjorn Haake Advisor: Prof. Klaus Erik Schauser, Ph. D. Chris Scheiman, Ph. D. 
Note: Date: May 2nd, 1996  
Address: Santa Barbara, USA  
Affiliation: University of California at  
Abstract-found: 0
Intro-found: 1
Reference: [AG89] <author> G. Almasi and A. Gottlieb. </author> <title> Highly Parallel Programming. </title> <address> Benjamin/Cummings, Redwood City, CA and others, </address> <year> 1989. </year>
Reference: [Bra89] <author> S. Brawer. </author> <title> Introduction to Parallel Programming. </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference: [BSM95] <author> R. Block, S. Sarukkai, and P. Mehra. </author> <title> Automated Performance Prediction of Message-Passing Parallel Programs. </title> <booktitle> In Proceedings SUPERCOMPUTING '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference: [C + 95] <author> D. Culler et al. </author> <title> Introduction to Split-C. </title> <type> Technical report, </type> <institution> UC Berkeley, </institution> <year> 1995. </year>
Reference-contexts: The Meiko CS-2 is an example of a message passing machine. It consists of 64 Sparc 10's which are connected via a fast fat-tree network. Each processor has its own Elan co-processor to handle communication. Split-C <ref> [C + 95] </ref> is a new parallel language, available on the Meiko CS-2 as well as many other platforms. It is an extension to C that supports communication between processors. It is based on active messages [vECGS92] which provide fast low-latency communication.
Reference: [CDG + 93] <author> D. Culler, A. Dusseau, S. Goldstein, et al. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings SUPERCOMPUTING '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: C is a fast, general purpose language and therefore widely used for sequential programs in scientific computing. Since it was written for uniprocessors, it lacks the ability of interprocessor communication. Therefore, at the University of California, Berkeley the parallel programming language Split-C <ref> [CDG + 93] </ref> was developed. The underlying language is C extended with constructs satisfying two fundamental concepts of parallel programming: * one active thread of control on each processor, and * a new level of storage hierarchy involving access to remote memory modules via an interconnection network. <p> The synchronization is limited to the points where processors need to exchange values. In this model, it is possible to run parts of the code on just a subset of processors. This concept is called multiple persistent thread model <ref> [CDG + 93] </ref>. Each processor can be abstracted as a thread of control. 3.1.3 Global Objects A global address space holds data accessible from each processor. Each processor has its own local address range where it can store local variables.
Reference: [Fos95] <author> I. Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1995. </year>
Reference-contexts: A tool used for analyzing where a program spends its execution time is called a 7 8 CHAPTER 1. INTRODUCTION profiler <ref> [Fos95] </ref>. It shows which functions use most of the time so that these can be examined further. These limiting parts of a program are called bottlenecks. The goal in both, sequential and parallel programs, is to minimize bottlenecks in order to increase performance.
Reference: [GKM82] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> A Call Graph Execution Profiler. </title> <journal> Software-Practice and Experience, </journal> <volume> 17(6) </volume> <pages> 120-126, </pages> <year> 1982. </year>
Reference-contexts: Unfortunately it is the fundamental concept of C to have these small, fast subroutines, so using the internal hardware clock would not give adequate results. A solution, that solves the previously mentioned problems, uses a statistical method [GKM83], <ref> [GKM82] </ref>. At a certain frequency the program counter's location is checked. At the end the total time gets distributed according to the distribution of the program counter's locations. This timing is not real-time, but a statistical measurement.
Reference: [GKM83] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> An Execution Profiler for Modular Programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 13(8) </volume> <pages> 671-685, </pages> <year> 1983. </year>
Reference-contexts: Unfortunately it is the fundamental concept of C to have these small, fast subroutines, so using the internal hardware clock would not give adequate results. A solution, that solves the previously mentioned problems, uses a statistical method <ref> [GKM83] </ref>, [GKM82]. At a certain frequency the program counter's location is checked. At the end the total time gets distributed according to the distribution of the program counter's locations. This timing is not real-time, but a statistical measurement. <p> It is written for UNIX machines and its intention is to determine where a sequential C program spends its time. This is done by measuring the execution time for each function and counting how often they are called. <ref> [GKM83] </ref> provides a detailed description. Here, we just give a short overview. The Instrumentation Server The instrumentation is done during compile time. The compiler gets a special option, -pg, which leads to a modified program after the compilation.
Reference: [HE91] <author> M. Heath and J. Etheridge. </author> <title> Visualizing the Performance of Parallel Programs. </title> <journal> IEEE Software, </journal> <volume> 8(5) </volume> <pages> 29-39, </pages> <year> 1991. </year>
Reference-contexts: The Portable Instru-mentable Communication Library (PICL), described earlier, generates the tracefile. It works the same way as upshot, generating a tracefile for each node. These tracefiles are later merged into one single file which is used for ParaGraph. <ref> [HE91] </ref> gives a good overview of ParaGraph. In [HM95], [RJ93] and [KS93] discuss in detail the importance of the convenient graphical output of a profiler. The main aspects ParaGraph keeps track of are communication, computation and tasks. 24 CHAPTER 2. PROFILING TECHNIQUES Typical ParaGraph output is shown in Figure 2.2.
Reference: [HM95] <author> S. Hackstadt and A. Malony. </author> <title> Visualizing Parallel Programs and Performance. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 12-14, </pages> <year> 1995. </year>
Reference-contexts: The Portable Instru-mentable Communication Library (PICL), described earlier, generates the tracefile. It works the same way as upshot, generating a tracefile for each node. These tracefiles are later merged into one single file which is used for ParaGraph. [HE91] gives a good overview of ParaGraph. In <ref> [HM95] </ref>, [RJ93] and [KS93] discuss in detail the importance of the convenient graphical output of a profiler. The main aspects ParaGraph keeps track of are communication, computation and tasks. 24 CHAPTER 2. PROFILING TECHNIQUES Typical ParaGraph output is shown in Figure 2.2.
Reference: [JD94] <author> Jr. J. DelSignore. </author> <title> Meiko CS-2 TotalView. </title> <type> Technical report, </type> <institution> BBN, </institution> <year> 1994. </year>
Reference-contexts: An online monitor is much harder to write on a parallel machine than on a sequential one. The main difficulty is the communication between the host program and the different nodes. The difficulties that were encountered using the parallel debugger totalview <ref> [JD94] </ref> on the Meiko CS-2 and trying to port Mantis, a parallel version of the GNU gdb debugger on the same machine, back this last statement. 2.4.4 The Split-C Profiler an Overview with user-level tracecalls for those parts of the program, which he wants to trace more thoroughly.
Reference: [JL93] <author> J. Jones and X. Li. </author> <title> A Performance Analysis Tool for Split-C Programs. </title> <type> Technical report, </type> <year> 1993. </year>
Reference-contexts: PROFILING TECHNIQUES 2.5 Existing Profiling Tools There are already some profilers available. We discuss each of them, giving a little bit of information on how they work, what their outputs look like, and describing their advantages and disadvantages. Some technical reports ([Pik93], <ref> [JL93] </ref>) already realized the problems for a parallel Split-C profiler, without giving detailed solutions, though. 2.5.1 Gprof This first program described is not a parallel profiler. It is written for UNIX machines and its intention is to determine where a sequential C program spends its time.
Reference: [K + 94] <editor> V. Kumar et al. </editor> <title> Introduction to Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Parallel programming is very powerful because it is much faster than a sequential approach and it suits scientific computation with the large data sets that need to be processed. Most parallel architectures can be classified as either SIMD or MIMD <ref> [K + 94] </ref>. SIMD or 'single instruction stream, multiple data stream' performs well for large regular applications that contain a considerable amount of data parallelism. It is not efficient on problems with an irregular data layout. Another model developed is MIMD or 'multiple instruction stream, multiple data stream'. <p> Another model developed is MIMD or 'multiple instruction stream, multiple data stream'. Each processor executes different instructions, so that a wider range of problems can be parallelized efficiently. Parallel architectures can also be classified as shared memory or message passing <ref> [K + 94] </ref>, depending on how they access data. In a shared memory model the processors write and read data from a common piece of memory. In order to avoid data inconsistencies (read/write and write/write problems) some locking mechanism like a semaphore has to be provided. <p> A parallel program is more difficult to write than a sequential one because distributing the work equally among the processors for efficient use is very hard. We call this problem load imbalance <ref> [K + 94] </ref>. Each processor should have more or less the same percentage of utilization. It is also important that the data is distributed evenly across the processors. <p> It is difficult just by looking at the code to see why the performance is bad, and this task is almost impossible with a parallel program. We can measure the performance of a parallel program by describing its speedup. The speed-up-theorem <ref> [K + 94] </ref> says: S = T p where S is the speedup, T s the total sequential execution time with the known best performing algorithm, T p describes the running time on a parallel machine using p processors. <p> Split-C has a global address space, where global variables are stored, to solve this problem. 3.1.2 The Programming Model There are different programming models that exist for parallel machines. One that is suited for problems with data subject to identical processing is called data parallelism <ref> [K + 94] </ref>. The language supporting this model is called a data-parallel programming language, where every processor is exactly performing the same steps of execution. This can lead to inefficient code because of the global synchronization involved.
Reference: [KR88] <author> B. Kernighan and D. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year> <note> 83 84 BIBLIOGRAPHY </note>
Reference: [KS93] <author> E. Kraemer and J. Stasko. </author> <title> The Visualization of Parallel Systems: An Overview. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 105-117, </pages> <year> 1993. </year>
Reference-contexts: The Portable Instru-mentable Communication Library (PICL), described earlier, generates the tracefile. It works the same way as upshot, generating a tracefile for each node. These tracefiles are later merged into one single file which is used for ParaGraph. [HE91] gives a good overview of ParaGraph. In [HM95], [RJ93] and <ref> [KS93] </ref> discuss in detail the importance of the convenient graphical output of a profiler. The main aspects ParaGraph keeps track of are communication, computation and tasks. 24 CHAPTER 2. PROFILING TECHNIQUES Typical ParaGraph output is shown in Figure 2.2. The panel in the upper left corner is the control panel.
Reference: [LH91] <author> E. Lusk and V. Herrarte. </author> <title> Studying Parallel Program Behaviour with Upshot. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: The steps to produce a profiling image are linking the user's program with a special library, running it to produce a tracefile and postprocessing this tracefile with a graphical interface. Upshot was developed at Argonne National Laboratories. A further introduction is found in <ref> [LH91] </ref>. The Instrumentation Server The instrumentation system for upshot is the alog library. The user extends his program by certain tracecalls, then links the library against it. These tracecalls are macros defined in the library, and, during run time, are responsible for collecting the data. Upshot only traces so-called events.
Reference: [M + 95] <author> B. Miller et al. </author> <title> The Paradyn Parallel Performance Measurement Tool. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 37-46, </pages> <year> 1995. </year>
Reference-contexts: are three components a software monitor is always composed of: * Local Instrumentation Server (LIS) The Local Instrumentation Server, or LIS, is responsible for inserting the tracecalls into the user's program, which can be done statically before compiling like in PICL [Wor92], or dynamically at run time like in Paradyn <ref> [M + 95] </ref>. The tracecalls, along with the modified communication library, generate entries that go into the tracefile. <p> The graphs are not scaled and it is difficult to understand what units are used. Another disadvantage is, that VT works only on color workstations. 2.5.4 Paradyn Paradyn <ref> [M + 95] </ref> is one of the few examples of online monitoring. Its goal is to significantly cut the overhead of collecting trace data by dynamically inserting tracecalls during run time and, once the interesting part of the program is passed, to remove it again.
Reference: [Mei93] <institution> Meiko, </institution> <address> Waltham, Ma. </address> <booktitle> The Elan Communications Processor, </booktitle> <year> 1993. </year>
Reference-contexts: Using DMA leads to a very high bandwidth (up to 42 MB/s). This makes bulk transfers very efficient. Bulk transfers send a huge amount of data, like an array, with a single command. Each of the Sparc stations has a co-processor <ref> [Mei93] </ref> which is responsible for communication. This co-processor performs DMA operations, receives messages, and performs other commands issued by the Sparc 10 processors. 3.3.2 The Network The individual Sparc stations are connected via a fast, fat-tree communication network.
Reference: [Pik93] <author> G. Pike. </author> <title> The Design and Implementation of a Split-C Profiler. Term project, </title> <year> 1993. </year>
Reference: [RJ93] <author> D. Rover and C. Wright Jr. </author> <title> Visualizing the Performance of SPMD and Data-Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 129-146, </pages> <year> 1993. </year>
Reference-contexts: The Portable Instru-mentable Communication Library (PICL), described earlier, generates the tracefile. It works the same way as upshot, generating a tracefile for each node. These tracefiles are later merged into one single file which is used for ParaGraph. [HE91] gives a good overview of ParaGraph. In [HM95], <ref> [RJ93] </ref> and [KS93] discuss in detail the importance of the convenient graphical output of a profiler. The main aspects ParaGraph keeps track of are communication, computation and tasks. 24 CHAPTER 2. PROFILING TECHNIQUES Typical ParaGraph output is shown in Figure 2.2.
Reference: [SM93] <author> S. Sarukkai and A. Malony. </author> <title> Perturbation Analysis of High Level Instrumentation for SPMD Programs. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(7) </volume> <pages> 44-53, </pages> <year> 1993. </year>
Reference-contexts: For collecting data, the user's program is modified to recognize certain events and to record them for display. Note that collecting and displaying data will perturb the user's program <ref> [SM93] </ref>. 2.2.1 Data Collection and Data Transformation To collect the data we describe an instrumentation system or IS [WR95]. An IS deals with the collection and management of performance data. A monitor is a tool used to observe the activities of a system.
Reference: [SS95] <author> K. Schauser and C. Scheiman. </author> <title> Active Messages Implementations for the Meiko cs-2. </title> <booktitle> In 9th International Parallel Processing Symposium (IPPS'95), </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The fundamental concepts in Split-C are a global address space for variables shared among all processors, split-phase assignments to access these global variables, and barriers to synchronize the different processors. Most implementations of Split-C are based on active messages a method for fast communication <ref> [SS95] </ref>, [vECGS92]. The parallel architecture used for our experimental results is the Meiko CS-2, which consists of 64 Sparc 10s connected on a high speed network.
Reference: [SSFK96] <author> K. E. Schauser, C. J. Scheiman, J. M. Ferguson, and P. Z. Kolano. </author> <title> Exploiting the Capabilities of Communications Co-processors. </title> <booktitle> In 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: To reduce the times, polls must be added. One way to do this on the Meiko CS-2 is to have the communications co-processor do the polling and service read requests. We have designed another Split-C implementation using this approach <ref> [SSFK96] </ref>, and the optimized numbers are shown in the right most column of the table. They substantially reduce the wait times to 23-33 s. By adding some polls on the main processor, we obtain the communication pattern shown in Figure 5.22.
Reference: [Uni94] <institution> University of Illinois, Urbana, </institution> <month> Ill. </month> <title> Pablo Instrumentation Environment User's Guide, </title> <year> 1994. </year>
Reference-contexts: The usual solution is based on the Internet Protocol (IP), but some IS's have sockets (Pablo) <ref> [Uni94] </ref> or pipes (Paradyn) as a faster means of communication. 2.2.2 Displaying Data Collecting all the trace data is useless unless we can conveniently display it.
Reference: [Var93] <author> D.A. Varley. </author> <title> Practical experience of the limitations of gprof. </title> <journal> Software-Practice and Experience, </journal> <volume> 23(4) </volume> <pages> 461-463, </pages> <year> 1993. </year>
Reference-contexts: Due to the fact that the method is based on a statistical method, gprof has some limitations on tracing events <ref> [Var93] </ref>. The second important feature for sequential profiling, which is done rather easily, is counting how often the function was called. A simple counter added to the subroutine entry will do. The only thing to worry about is if it was invoked recursively.
Reference: [vECGS92] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Each processor has its own Elan co-processor to handle communication. Split-C [C + 95] is a new parallel language, available on the Meiko CS-2 as well as many other platforms. It is an extension to C that supports communication between processors. It is based on active messages <ref> [vECGS92] </ref> which provide fast low-latency communication. A global address space makes objects accessible by any processor. A global object is located on a local processor's memory, but can be accessed via a global pointer from any node. <p> The fundamental concepts in Split-C are a global address space for variables shared among all processors, split-phase assignments to access these global variables, and barriers to synchronize the different processors. Most implementations of Split-C are based on active messages a method for fast communication [SS95], <ref> [vECGS92] </ref>. The parallel architecture used for our experimental results is the Meiko CS-2, which consists of 64 Sparc 10s connected on a high speed network. <p> Active messages <ref> [vECGS92] </ref> are a means of very fast communication in large-scale multiprocessors. They were developed because existing message passing libraries have a very high communication overhead. Active messages are based on a lean communication protocol to speed-up the run time of a program.
Reference: [Wor92] <author> P. Worley. </author> <title> A New PICL Trace File Format. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: There are three components a software monitor is always composed of: * Local Instrumentation Server (LIS) The Local Instrumentation Server, or LIS, is responsible for inserting the tracecalls into the user's program, which can be done statically before compiling like in PICL <ref> [Wor92] </ref>, or dynamically at run time like in Paradyn [M + 95]. The tracecalls, along with the modified communication library, generate entries that go into the tracefile. <p> A traceline contains fields with the type of the action (e.g. entry or exit), the type of event (send, receive or idle), the timestamp, the processors that send and receive the message, the message length and some PICL internal fields. A detailed description can be found in <ref> [Wor92] </ref>. The Transfer Protocol The transfer protocol used is parallel I/O. The buffers get transferred to the tracefiles concurrently.
Reference: [WR95] <author> A. Waheed and D. </author> <title> Rover. A Structured Approach to Instrumentation System Development and Evaluation. </title> <booktitle> In Proceedings SUPERCOMPUTING '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: For collecting data, the user's program is modified to recognize certain events and to record them for display. Note that collecting and displaying data will perturb the user's program [SM93]. 2.2.1 Data Collection and Data Transformation To collect the data we describe an instrumentation system or IS <ref> [WR95] </ref>. An IS deals with the collection and management of performance data. A monitor is a tool used to observe the activities of a system. In general, monitors observe the performance of systems, collect performance statistics, analyze data, and display results. <p> Mainly there are two ways to do so: 2.4. PROFILING METHODOLOGIES 15 * FOF: Flushing whenever a buffer on the node is full, * FAOF: Flush all the buffers when one of the node buffers is full. Detailed studies <ref> [WR95] </ref> have shown that the second strategy is the more efficient one. Two things were investigated: * Trace stopping time: the time to fill a buffer for flushing, * Flushing frequency: ratio of flushes to data collected on one node.
References-found: 28

