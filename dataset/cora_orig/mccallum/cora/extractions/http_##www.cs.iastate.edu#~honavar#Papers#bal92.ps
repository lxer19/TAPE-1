URL: http://www.cs.iastate.edu/~honavar/Papers/bal92.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: Faster Learning in Multi-Layer Networks by Handling  
Author: Output-Layer Flat-Spots Karthik Balakrishnan and Vasant Honavar 
Address: Ames 50011 USA  
Affiliation: Department of Computer Science Iowa State University  
Abstract: Generalized delta rule, popularly known as back-propagation (BP) [9, 5] is probably one of the most widely used procedures for training multi-layer feed-forward networks of sigmoid units. Despite reports of success on a number of interesting problems, BP can be excruciatingly slow in converging on a set of weights that meet the desired error criterion. Several modifications for improving the learning speed have been proposed in the literature [2, 4, 8, 1, 6]. BP is known to suffer from the phenomenon of flat spots [2]. The slowness of BP is a direct consequence of these flat-spots together with the formulation of the BP Learning rule. This paper proposes a new approach to minimizing the error that is suggested by the mathematical properties of the conventional error function and that effectively handles flat-spots occurring in the output layer. The robustness of the proposed technique is demonstrated on a number of data-sets widely studied in the machine learning community. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Becker, S. and LeCun, Y. </author> <title> The Feasibility of Applying Numerical Optimization Techniques to BackPropagation. </title> <booktitle> Proceedings, 1988 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Several authors have explored modifications to BP in an attempt to reduce the number of training epochs needed to learn a task <ref> [2, 4, 8, 1, 6] </ref>. Such modifications are of significant practical interest if BP is to be applied to real-world problems. However, most of these modifications are either extremely heuristic or else make use of sophisticated optimization and approximation techniques.
Reference: [2] <author> Fahlman, S. </author> <title> E . Faster-learning variations of backpropagation : An empirical study. </title> <editor> In D. Touretzky, G. E. Hinton, and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 38-51, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Several authors have explored modifications to BP in an attempt to reduce the number of training epochs needed to learn a task <ref> [2, 4, 8, 1, 6] </ref>. Such modifications are of significant practical interest if BP is to be applied to real-world problems. However, most of these modifications are either extremely heuristic or else make use of sophisticated optimization and approximation techniques. <p> We define a unit to be in the active range if its output is greater than a pre-specified value of (1-MARGIN) and less than MARGIN, a technique that was suggested by Fahlman <ref> [2] </ref> . <p> The solution is to scale these weight-updates into the conventional range, and this is done by a parameter introduced in the rules, appropriately named squash. 4 Simulation Results XOR and Encoder/Decoder are problems which have been extensively studied in the Machine Learn ing community <ref> [2] </ref>. We report results for the XOR problem and a 10-5-10 Encoder/Decoder. Some real-world data sets like : Iris 2 , Audiology and Soybean 3 have been investigated by a number of researchers [7, 10]. <p> UT-Austin for making the data set available Table 1: The XOR problem Method ff % Converged Epochs (Iterations) 2.0 0.2 80 405 0.8 0.5 80 579 Modified BP 1.0 0.5 60 278 0.9 0.5 80 389 4.2 The 10-5-10 Encoder/Decoder Problem This problem has been studied in detail by Fahlman <ref> [2] </ref>. The simulations used variable convergence criteria and the best results are reported for each such criterion. A trial was said to have converged if the mean squared error fell below the corresponding error threshold. Results are averaged over ten trials.
Reference: [3] <author> Honavar, V., and Uhr, L. </author> <title> Generative Learning Structures for Generalized Connectionist Networks. </title> <journal> In Information Sciences,( Special Issue on Neural Networks and Artificial Intelligence). </journal> <note> In press. </note>
Reference: [4] <author> Parker, D. B. </author> <title> Optimal Algorithms for Adaptive Networks: Second Order Back Propagation, Second Order Direct Propagation, and Second Order Hebbian Learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 593-600. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Several authors have explored modifications to BP in an attempt to reduce the number of training epochs needed to learn a task <ref> [2, 4, 8, 1, 6] </ref>. Such modifications are of significant practical interest if BP is to be applied to real-world problems. However, most of these modifications are either extremely heuristic or else make use of sophisticated optimization and approximation techniques.
Reference: [5] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <title> Learning Internal Representations by Error-Propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L. (eds), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, chapter 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, and Lon-don, England, </address> <year> 1986. </year>
Reference-contexts: The weight-update rule that minimizes E can be easily derived (refer <ref> [5] </ref> for details of the derivation), and can be shown to be : w ij = ffi j o i (2) ffi j being popularly referred to as the instantaneous gradient.
Reference: [6] <author> Samad, T. </author> <title> Back Propagation With Expected Source Values. </title> <booktitle> In Neural Networks, </booktitle> <address> Vol.4,pp.615-618, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Several authors have explored modifications to BP in an attempt to reduce the number of training epochs needed to learn a task <ref> [2, 4, 8, 1, 6] </ref>. Such modifications are of significant practical interest if BP is to be applied to real-world problems. However, most of these modifications are either extremely heuristic or else make use of sophisticated optimization and approximation techniques.
Reference: [7] <author> Shavlik, J. W., Mooney, R. J. and Towell, G. G. </author> <title> Symbolic and Neural Learning Algorithms : An Empirical Comparison. </title> <journal> Machine Learning, </journal> <volume> Vol 6, </volume> <year> 1991. </year>
Reference-contexts: We report results for the XOR problem and a 10-5-10 Encoder/Decoder. Some real-world data sets like : Iris 2 , Audiology and Soybean 3 have been investigated by a number of researchers <ref> [7, 10] </ref>. The availability of results in the literature prompted us to study the performance of our learning rule on these data-sets. 4.1 The XOR Problem Simulations were run with BP and the Modified method, and the best results for each method are reported. <p> The 10-5-10 Encoder/Decoder problem Method ff Error Threshold % Converged Epochs (Iterations) 0.9 0.5 0.3 100 121 0.8 0.5 0.12 100 203 Modified BP 0.9 0.5 0.2 100 40 0.8 0.5 0.12 100 57 4.3 Iris, Audiology and Soybean These real-world data sets have been investigated by Shavlik et al. <ref> [7] </ref>, Yang and Honavar [10] and others. In addition to the number of epochs required for convergence, these data sets can also be used to test the generalization ability of the network on unseen data-inputs.
Reference: [8] <author> Watrous, R. L. </author> <title> Learning Algorithms for Connectionist Networks : Applied Gradient Methods for Non-Linear Optimization. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 619-627. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Several authors have explored modifications to BP in an attempt to reduce the number of training epochs needed to learn a task <ref> [2, 4, 8, 1, 6] </ref>. Such modifications are of significant practical interest if BP is to be applied to real-world problems. However, most of these modifications are either extremely heuristic or else make use of sophisticated optimization and approximation techniques.
Reference: [9] <author> Werbos, P. J. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis , Harvard University, </type> <year> 1974. </year>
Reference: [10] <author> Yang, J and Honavar, V. </author> <title> Experiments with the Cascade-Correlation Algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: We report results for the XOR problem and a 10-5-10 Encoder/Decoder. Some real-world data sets like : Iris 2 , Audiology and Soybean 3 have been investigated by a number of researchers <ref> [7, 10] </ref>. The availability of results in the literature prompted us to study the performance of our learning rule on these data-sets. 4.1 The XOR Problem Simulations were run with BP and the Modified method, and the best results for each method are reported. <p> Method ff Error Threshold % Converged Epochs (Iterations) 0.9 0.5 0.3 100 121 0.8 0.5 0.12 100 203 Modified BP 0.9 0.5 0.2 100 40 0.8 0.5 0.12 100 57 4.3 Iris, Audiology and Soybean These real-world data sets have been investigated by Shavlik et al. [7], Yang and Honavar <ref> [10] </ref> and others. In addition to the number of epochs required for convergence, these data sets can also be used to test the generalization ability of the network on unseen data-inputs.
References-found: 10

