URL: http://www.ai.mit.edu/people/viola/nips-95-final.ps.gz
Refering-URL: http://www.ai.mit.edu/~viola/pub.html
Root-URL: 
Email: viola@salk.edu  
Title: Empirical Entropy Manipulation for Real-World Problems  
Author: Paul Viola Nicol N. Schraudolph, Terrence J. Sejnowski 
Date: 1996.  
Note: To appear in Advances in Neural Information Processing Systems 8, MIT Press,  
Address: 10010 North Torrey Pines Road La Jolla, CA 92037-1099  
Affiliation: Computational Neurobiology Laboratory The Salk Institute for Biological Studies  
Abstract: No finite sample is sufficient to determine the density, and therefore the entropy, of a signal directly. Some assumption about either the functional form of the density or about its smoothness is necessary. Both amount to a prior over the space of possible density functions. By far the most common approach is to assume that the density has a parametric form. By contrast we derive a differential learning rule called EMMA that optimizes entropy by way of kernel density estimation. Entropy and its derivative can then be calculated by sampling from this density estimate. The resulting parameter update rule is surprisingly simple and efficient. We will show how EMMA can be used to detect and correct corruption in magnetic resonance images (MRI). This application is beyond the scope of existing parametric entropy models.
Abstract-found: 1
Intro-found: 1
Reference: <author> Becker, S. and Hinton, G. E. </author> <year> (1992). </year> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355 </volume> <pages> 161-163. </pages>
Reference-contexts: 1 Introduction Information theory is playing an increasing role in unsupervised learning and visual processing. For example, Linsker has used the concept of information maximization to produce theories of development in the visual cortex (Linsker, 1988). Becker and Hinton have used information theory to motivate algorithms for visual processing <ref> (Becker and Hinton, 1992) </ref>. Bell and Sejnowski have used information maximization fl Author to whom correspondence should be addressed. Current address: M.I.T., 545 Technology Square, Cambridge, MA 02139. to solve the "cocktail party" or signal separation problem (Bell and Sejnowski, 1995).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximisation approach to blind separation. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing, volume 7, </booktitle> <address> Denver 1994. </address> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Bell and Sejnowski have used information maximization fl Author to whom correspondence should be addressed. Current address: M.I.T., 545 Technology Square, Cambridge, MA 02139. to solve the "cocktail party" or signal separation problem <ref> (Bell and Sejnowski, 1995) </ref>. In order to simplify analysis and implementation, each of these techniques makes specific assumptions about the nature of the signals used, typically that the signals are drawn from some parametric density. In practice, such assumptions are very inflexible.
Reference: <author> Bienenstock, E., Cooper, L., and Munro, P. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal of Neuroscience, </journal> <volume> 2. </volume>
Reference-contexts: To provide some intuition regarding the behavior of ECA we have run ECA-MAX, ECA-MIN, Oja's rule, and two related procedures, BCM and BINGO, on the same density. BCM is a learning rule that was originally proposed to explain development of receptive fields patterns in visual cortex <ref> (Bienenstock, Cooper and Munro, 1982) </ref>. More recently it has been argued that the rule finds projections that are far from Gaussian (Intrator and Cooper, 1992). Under a limited set of conditions this is equivalent to finding the minimum entropy projection.
Reference: <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Rather than assume that the density has a parametric form, whose parameters are selected using maximum likelihood estimation, we will instead use Parzen window density estimation <ref> (Duda and Hart, 1973) </ref>.
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the bcm theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17. </pages>
Reference-contexts: BCM is a learning rule that was originally proposed to explain development of receptive fields patterns in visual cortex (Bienenstock, Cooper and Munro, 1982). More recently it has been argued that the rule finds projections that are far from Gaussian <ref> (Intrator and Cooper, 1992) </ref>. Under a limited set of conditions this is equivalent to finding the minimum entropy projection. BINGO was proposed to find axes along which there is a bimodal distribution (Schraudolph and Sejnowski, 1993). density is a mixture of two clusters.
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 105-117. </pages>
Reference-contexts: 1 Introduction Information theory is playing an increasing role in unsupervised learning and visual processing. For example, Linsker has used the concept of information maximization to produce theories of development in the visual cortex <ref> (Linsker, 1988) </ref>. Becker and Hinton have used information theory to motivate algorithms for visual processing (Becker and Hinton, 1992). Bell and Sejnowski have used information maximization fl Author to whom correspondence should be addressed.
Reference: <author> Oja, E. </author> <year> (1982). </year> <title> A simplified neuron model as a principal component analyzer. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 15 </volume> <pages> 267-273. </pages>
Reference-contexts: Oja has derived an elegant on-line rule for learning ^v when presented with a sample of X <ref> (Oja, 1982) </ref>. Under the assumption that X is Gaussian is is easily proven that Y ^v has maximum entropy. Moreover, in the absence of noise, Y ^v , contains maximal information about X.
Reference: <author> Schraudolph, N. N. and Sejnowski, T. J. </author> <year> (1993). </year> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing, </booktitle> <volume> volume 5, </volume> <pages> pages 499-506, </pages> <address> Denver 1992. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: More recently it has been argued that the rule finds projections that are far from Gaussian (Intrator and Cooper, 1992). Under a limited set of conditions this is equivalent to finding the minimum entropy projection. BINGO was proposed to find axes along which there is a bimodal distribution <ref> (Schraudolph and Sejnowski, 1993) </ref>. density is a mixture of two clusters. Each cluster has high kurtosis in the horizontal direction. The oblique axis projects the data so that it is most uniform and hence has the highest entropy; ECA-MAX finds this axis.
Reference: <author> Viola, P. A. </author> <year> (1995). </year> <title> Alignment by Maximization of Mutual Information. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology. MIT AI Laboratory TR 1548. </institution>
Reference-contexts: This is especially critical in entropy manipulation problems, where the derivative of entropy is evaluated many hundreds or thousands of times. Without the quadratic savings that arise from using smaller samples entropy manipulation would be impossible (see <ref> (Viola, 1995) </ref> for a discussion of these issues). 2.2 Estimating the Covariance In addition to the learning rate , the covariance matrices of the Parzen window functions, g , are important parameters of EMMA. These parameters may be chosen so that they are optimal in the maximum likelihood sense. <p> Right: The density of pixel values in the MRI scan before and after correction. 4 Applications EMMA has proven useful in a number of applications. In object recognition EMMA has been used align 3D shape models with video images <ref> (Viola and Wells III, 1995) </ref>. <p> In the area of medical imaging EMMA has been used to register data that arises from differing medical modalities such as magnetic resonance images, computed tomography images, and positron emission tomography <ref> (Wells, Viola and Kikinis, 1995) </ref>. 4.1 MRI Processing In addition, EMMA can be used to process magnetic resonance images (MRI). An MRI is a 2 or 3 dimensional image that records the density of tissues inside the body.
Reference: <author> Viola, P. A. and Wells III, W. M. </author> <year> (1995). </year> <title> Alignment by maximization of mutual information. </title> <booktitle> In Fifth Intl. Conf. on Computer Vision, </booktitle> <pages> pages 16-23, </pages> <address> Cambridge, MA. </address> <publisher> IEEE. </publisher>
Reference-contexts: This is especially critical in entropy manipulation problems, where the derivative of entropy is evaluated many hundreds or thousands of times. Without the quadratic savings that arise from using smaller samples entropy manipulation would be impossible (see <ref> (Viola, 1995) </ref> for a discussion of these issues). 2.2 Estimating the Covariance In addition to the learning rate , the covariance matrices of the Parzen window functions, g , are important parameters of EMMA. These parameters may be chosen so that they are optimal in the maximum likelihood sense. <p> Right: The density of pixel values in the MRI scan before and after correction. 4 Applications EMMA has proven useful in a number of applications. In object recognition EMMA has been used align 3D shape models with video images <ref> (Viola and Wells III, 1995) </ref>. <p> In the area of medical imaging EMMA has been used to register data that arises from differing medical modalities such as magnetic resonance images, computed tomography images, and positron emission tomography <ref> (Wells, Viola and Kikinis, 1995) </ref>. 4.1 MRI Processing In addition, EMMA can be used to process magnetic resonance images (MRI). An MRI is a 2 or 3 dimensional image that records the density of tissues inside the body.
Reference: <author> Wells, W., Viola, P., and Kikinis, R. </author> <year> (1995). </year> <title> Multi-modal volume registration by maximization of mutual information. </title> <booktitle> In Proceedings of the Second International Symposium on Medical Robotics and Computer Assisted Surgery, </booktitle> <pages> pages 55 - 62. </pages> <publisher> Wiley. </publisher>
Reference-contexts: Right: The density of pixel values in the MRI scan before and after correction. 4 Applications EMMA has proven useful in a number of applications. In object recognition EMMA has been used align 3D shape models with video images <ref> (Viola and Wells III, 1995) </ref>. <p> In the area of medical imaging EMMA has been used to register data that arises from differing medical modalities such as magnetic resonance images, computed tomography images, and positron emission tomography <ref> (Wells, Viola and Kikinis, 1995) </ref>. 4.1 MRI Processing In addition, EMMA can be used to process magnetic resonance images (MRI). An MRI is a 2 or 3 dimensional image that records the density of tissues inside the body.
Reference: <author> Wells III, W., Grimson, W., Kikinis, R., and Jolesz, F. </author> <year> (1994). </year> <title> Statistical Gain Correction and Segmentation of MRI Data. </title> <booktitle> In Proceedings of the Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Seattle, Wash. </address> <note> IEEE , Submitted. </note>
Reference-contexts: In principle the density of pixel values in an MRI should be clustered, with one cluster for each tissue class. In reality MRI signals are corrupted by a bias field, a multiplicative offset that varies slowly in space. The bias field results from unavoidable variations in magnetic field (see <ref> (Wells III et al., 1994) </ref> for an overview of this problem). Because the densities of each tissue type cluster together tightly, an uncorrupted MRI should have relatively low entropy. Corruption from the bias field perturbs the MRI image, increasing the values of some pixels and decreasing others.
References-found: 12

