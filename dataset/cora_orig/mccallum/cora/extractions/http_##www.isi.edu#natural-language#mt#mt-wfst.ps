URL: http://www.isi.edu/natural-language/mt/mt-wfst.ps
Refering-URL: http://www.isi.edu/natural-language/people/knight.html
Root-URL: http://www.isi.edu
Email: knight@isi.edu, yaser@isi.edu  
Title: Translation with Finite-State Devices  
Author: Kevin Knight and Yaser Al-Onaizan 
Address: Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: Statistical models have recently been applied to machine translation with interesting results. Algorithms for processing these models have not received wide circulation, however. By contrast, general finite-state transduction algorithms have been applied in a variety of tasks. This paper gives a finite-state reconstruction of statistical translation and demonstrates the use of standard tools to compute statistically likely translations. Ours is the first translation algorithm for "fertil ity/permutation" statistical models to be described in replicable detail.
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Alshawi, A. Buchsbaum, and F. Xia. </author> <year> 1997. </year> <title> A comparison of head transducers and transfer for a limited domain translation applications. </title> <booktitle> In Proc. ACL. </booktitle>
Reference: <author> P. F. Brown, S. A. Della-Pietra, V. J. Della-Pietra, and R. L. Mercer. </author> <year> 1993. </year> <title> The mathematics of statistical machine translation: Parameter estimation. </title> <journal> Computational Linguistics, </journal> <volume> 19(2). </volume>
Reference-contexts: These rules are usually coarse and probabilistic|for example, in a certain corpus, bat translates as palo 71% of the time, and as murcielago 29%. The most-established SMT system is based on word-for-word substitution <ref> (Brown et al., 1993) </ref>, although some experimental SMT systems employ syntactic and semantic processing (Wu, 1997; Alshawi et al., 1997; Su et al., 1995). An advantage of the SMT approach is that designers can improve translation accuracy by modestly changing the underlying model rather than overhauling large handcrafted resources. <p> Unfortunately, SMT has had little impact on machine translation in practice. One reason is that SMT accuracy is not significantly better than that obtained by handcrafting. Another reason is that few people understand SMT techniques like those presented in <ref> (Brown et al., 1993) </ref>, because of their highly mathematical rather than linguistic nature. These mathematical difficulties have slowed duplication and improvement of SMT results. By contrast, finite-state methods are having a broad effect on various aspects of human language processing (Roche and Schabes, 1997). <p> A common model for P (e) is the word-ngram model, which assigns values to strings based on their sub-sequence frequencies. We turn to models for P (f je) in the next section. 1 We follow the <ref> (Brown et al., 1993) </ref> terminology of denoting natural language strings with letters e and f , even if the languages are not English and French. <p> We will discuss decoding in our finite-state reconstruction. 3 Translation Models This section summarizes translation Models 1-3 as presented in <ref> (Brown et al., 1993) </ref>. Each model specifies how to compute a value for any P (f je) by consulting tables of probabilities. In all three models, a source sentence may produce the same target sentence in several different ways|these ways are covered by different alignments. <p> seguridad seems to require t 11 = cinturon; t 12 = de; t 13 = seguridad 11 = 1; 12 = 2; 13 = 3 t 11 = de; t 12 = cinturon; t 13 = seguridad 11 = 2; 12 = 1; 13 = 3 produces the same thing. <ref> (Brown et al., 1993) </ref> note that the permutation process can generate non-strings|if every ik is chosen to be 5, then all target words will be piled up in position 5. Model 3 is therefore said to be deficient. <p> There we achieve the same values for P (f; aje) without carrying out any combinations or factorials, and we compute P (f je) without explicit reference to alignments at all. 4 Alternative Models We will look at two alternatives to Model 3 not documented in <ref> (Brown et al., 1993) </ref>. One replaces the d ( ik ji; l; m) table by a simpler d ( ik ji) table. The other disposes of distortion probabilities altogether. <p> This gives us a k-best list of possible translations. One of our aims in giving this Model 3 translation algorithm is to fill a gap in the translation algorithm ("decoding") literature. Unfortunately, <ref> (Brown et al., 1993) </ref> never published a replicable fertility/distortion translation algorithm, and subsequent papers have focussed on simpler Model 2 variants (Wang and Waibel, 1997), syntactic models (Wu, 1996), and cases without word-order variation (Tillmann et al., 1997). We now translate some novel sentences not seen in training. <p> This is because we search for the best paths rather than the best sequences. Our smoothed language model 2 Implementation note. We follow <ref> (Brown et al., 1993) </ref> in Viterbi-style training, as the EM algorithm is too expensive even for this small corpus. We bootstrap initial parameters using Models 1 and 2. <p> We must add information to the permutation acceptor, e.g., by adding arcs for known phrases: Tokens like post office and daba una bofetada would then incur single rather than multiple distortion costs. This last consideration motivates Models 4 and 5 of <ref> (Brown et al., 1993) </ref>. 9 Discussion We have presented a reconstruction of statistical machine translation in terms of cascaded finite-state automata, and we have shown how these automata can be used in translating sentences and calculating conditional probabilities. <p> We use general composition and decoding algorithms that are also useful in a wide range of other applications. There are several interesting questions. First, will the finite-state composition scale up to large vocabularies and sentence lengths? <ref> (Brown et al., 1993) </ref> employ an undocumented suboptimal search and report search errors, so scale is clearly a problem.
Reference: <author> D. Eppstein. </author> <year> 1994. </year> <title> Finding the k shortest paths. </title> <booktitle> In Proc. 35th FOCS. </booktitle>
Reference-contexts: It includes forward-backward learning, which we use here to train and smooth our trigram language model. It also includes a k-best path extractor that follows the recently discovered O (m + n log n + kn) algorithm of <ref> (Eppstein, 1994) </ref>. This toolkit has been used for several MT-related tasks and is available for research purposes. offers several paths that produce the same e, and we have already seen the multiplicity of paths relating a particular pair e and f .
Reference: <author> F. Jelinek and R. L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition in Practice. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference-contexts: Although many acceptors are possible, we will assume a word-trigram model with deleted interpolation smoothing <ref> (Jelinek and Mercer, 1980) </ref>. Here is a piece of this acceptor: 6.2 Transducer One This transducer determines fertilities for words on its input. If the fertility of an input word is zero, the word is dropped in the output (e.g., did). If the fertility is one, it is copied.
Reference: <author> K. Knight. </author> <year> 1997. </year> <title> Automating knowledge acquisition for machine translation. </title> <journal> AI Magazine, </journal> <volume> 18(4). </volume>
Reference-contexts: Garcia tambien tiene una empresa . the groups do not sell zanzanine . los grupos no venden zanzanina . the small groups are not modern . los grupos pequenos no son modernos . Fig. 1. Sample bilingual text corpus used in training SMT systems <ref> (adapted from Knight, 1997) </ref> The aim of this paper is to give a finite-state reconstruction of SMT, with three goals in mind. One is to present a new implementation for SMT in which standard algorithms are used to compute probabilities. <p> Fig. 2. Sample monolingual text corpus used in training SMT systems <ref> (adapted from Knight, 1997) </ref>. 2 Word-for-Word SMT SMT views any string of words e as a potential translation of any string f . 1 We would like to set up a probability distribution P (ejf) over all pairs of strings, so that given f , we can output the e which
Reference: <author> F. Pereira and M. Riley. </author> <year> 1997. </year> <title> Speech recognition by composition of weighted finite automata. </title> <editor> In E. Roche and Y. Schabes, editors, </editor> <title> Finite-State Language Processing. </title> <publisher> MIT Press. </publisher>
Reference-contexts: To ensure compliance with the laws of probability, we require that all arcs leaving state s with input word x have w's that sum to one. Transducers can be composed algorithmically <ref> (Pereira and Riley, 1997) </ref>. If one transducer implements P (xjy) and another implements P (yjz), then we can compute a third transducer implementing P (xjz). This allows a cascade of small transducers to simulate a big one. <p> On the other hand, our finite-state tools are reasonably complete. Our C++ implementation includes composition of weighted transducers and acceptors <ref> (Pereira and Riley, 1997) </ref> with full handling of *-transitions. It includes forward-backward learning, which we use here to train and smooth our trigram language model. It also includes a k-best path extractor that follows the recently discovered O (m + n log n + kn) algorithm of (Eppstein, 1994).
Reference: <author> E. Roche and Y. Schabes, editors. </author> <year> 1997. </year> <title> Finite-State Language Processing. </title> <publisher> MIT Press. </publisher>
Reference-contexts: These mathematical difficulties have slowed duplication and improvement of SMT results. By contrast, finite-state methods are having a broad effect on various aspects of human language processing <ref> (Roche and Schabes, 1997) </ref>. Techniques for building and composing automata are widely understood, and software toolkits are becoming available to researchers. Finite-state machines offer advantages such as speed, precision, and smooth integration with speech and character recognition. Garcia and associates .
Reference: <author> K.-Y. Su, J.-S. Chang, and Y.-L. U. Hsu. </author> <year> 1995. </year> <title> A corpus-based two-way design for parameterized MT systems: Rationale, architecture and training issues. </title> <booktitle> In Proc. </booktitle> <address> TMI. </address>
Reference: <author> C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. </author> <year> 1997. </year> <title> A DP-based search using monotone alignments in statistical translation. </title> <booktitle> In Proc. ACL. </booktitle>
Reference-contexts: Unfortunately, (Brown et al., 1993) never published a replicable fertility/distortion translation algorithm, and subsequent papers have focussed on simpler Model 2 variants (Wang and Waibel, 1997), syntactic models (Wu, 1996), and cases without word-order variation <ref> (Tillmann et al., 1997) </ref>. We now translate some novel sentences not seen in training. ORIGINAL: la empresa tiene enemigos fuertes en Europa .
Reference: <author> Y. Wang and A. Waibel. </author> <year> 1997. </year> <title> Decoding algorithm in statistical machine translation. </title> <booktitle> In Proc. ACL. </booktitle>
Reference-contexts: One of our aims in giving this Model 3 translation algorithm is to fill a gap in the translation algorithm ("decoding") literature. Unfortunately, (Brown et al., 1993) never published a replicable fertility/distortion translation algorithm, and subsequent papers have focussed on simpler Model 2 variants <ref> (Wang and Waibel, 1997) </ref>, syntactic models (Wu, 1996), and cases without word-order variation (Tillmann et al., 1997). We now translate some novel sentences not seen in training. ORIGINAL: la empresa tiene enemigos fuertes en Europa .
Reference: <author> D. Wu. </author> <year> 1996. </year> <title> A polynomial-time algorithm for statistical machine translation. </title> <booktitle> In Proc. ACL. </booktitle>
Reference-contexts: Unfortunately, (Brown et al., 1993) never published a replicable fertility/distortion translation algorithm, and subsequent papers have focussed on simpler Model 2 variants (Wang and Waibel, 1997), syntactic models <ref> (Wu, 1996) </ref>, and cases without word-order variation (Tillmann et al., 1997). We now translate some novel sentences not seen in training. ORIGINAL: la empresa tiene enemigos fuertes en Europa .
Reference: <author> D. Wu. </author> <year> 1997. </year> <title> Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. </title> <journal> Computational Linguistics, </journal> <volume> 23(3). </volume>
References-found: 12

