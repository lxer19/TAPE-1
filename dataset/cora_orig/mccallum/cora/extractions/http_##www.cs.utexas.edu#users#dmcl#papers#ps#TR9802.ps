URL: http://www.cs.utexas.edu/users/dmcl/papers/ps/TR9802.ps
Refering-URL: http://www.cs.utexas.edu/users/dmcl/allpapers.html
Root-URL: 
Email: E-mail: fsriram,lorenzo,ving@cs.utexas.edu,  
Phone: Telephone: (512) 471-9792, Fax: (512) 471-8885  
Title: The Cost of Recovery in Message Logging Protocols  
Author: Sriram Rao, Lorenzo Alvisi, and Harrick M. Vin 
Note: Technical Report TR-98-02,  
Web: URL: http://www.cs.utexas.edu/users/fsriram,lorenzo,ving  
Address: Taylor Hall 2.124, Austin, Texas 78712-1188, USA  Austin, TX.  
Affiliation: Department of Computer Sciences The University of Texas at Austin  Department of Computer Sciences, University of Texas,  
Abstract: Message logging is a popular technique for building low-overhead protocols that tolerate process crash failures. Past research in message logging has focused on studying the relative overhead imposed by pessimistic, optimistic, and causal protocols during failure-free executions. In this paper, we give the first experimental evaluation of the performance of these protocols during recovery. We discover that, if a single failure is to be tolerated, pessimistic and causal protocols perform best, because they avoid rollbacks of correct processes. For multiple failures, however, the dominant factor in determining performance becomes where the recovery information is logged (i.e. at the sender, at the receiver, or replicated at a subset of the processes in the system) rather than when this information is logged (i.e. if logging is synchronous or asynchronous). From our results, we distil a few lessons that can guide the design of message-logging protocols that combine low-overhead during failure-free executions with fast recovery. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Alvisi, B. Hoppe, and K. Marzullo. </author> <title> Nonblocking and Orphan-Free Message Logging Protocols. </title> <booktitle> In Proceedings of the 23rd Fault-Tolerant Computing Symposium, </booktitle> <pages> pages 145154, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Optimistic protocols can perform very efficiently in failure-free executions. However, if any of the determinants are lost when a process crashes, then orphans may be created. To reach a consistent global state, these processes must be identified and rolled back. * Causal protocols <ref> [1, 9] </ref> combine some of the positive aspects of pessimistic and optimistic protocols: They never create orphans, yet they do not write determinants to stable storage synchronously. In causal protocols, determinants are logged in volatile memory.
Reference: [2] <author> L. Alvisi and K. Marzullo. </author> <title> Tradeoffs in Implementing Optimal Message Logging Protocols. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 5867. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1996. </year>
Reference-contexts: As we will see in Section 4, this optimization dramatically improves the performance of the protocol during recovery. Causal logging: We have implemented the det family-based message-logging protocol <ref> [2] </ref>. This protocol is based on the following observation: in a system where processes fail independently and no more than f processes fail concurrently, one can ensure the availability of determinants during recovery by replicating them in the volatile memory of f + 1 processes.
Reference: [3] <author> L. Alvisi and K. Marzullo. </author> <title> Message Logging: Pessimistic, Optimistic, Causal, and Optimal. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 24(2):149159, </volume> <month> February </month> <year> 1998. </year>
Reference-contexts: In our implementation, this is accomplished by piggybacking determinants on existing application messages until they are logged by at least f + 1 processes <ref> [3, 9] </ref>. Recovery of a failed process proceeds in two phases. In the first phase, the process obtains its determinants from the volatile logs of the operational processes. In addition, the process also obtains the content of messages it delivered before crashing. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [3, 9, 10, 12, 21] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [4] <author> A. Borg, J. Baumbach, and S. Glazer. </author> <title> A message system supporting fault tolerance. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Principles, pages 9099. ACM SIGOPS, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: An orphan process is an operational process whose state is inconsistent with the recovered state of a crashed process. All message-logging protocols guarantee that upon recovery no process is an orphan, but differ in the way they enforce this consistency condition: * Pessimistic protocols <ref> [4, 12, 18] </ref> require that a process, before sending a message, synchronously log on stable storage the determinants and the content of all messages delivered so far. Thus, pessimistic protocols never create orphan processes.
Reference: [5] <author> O. P. Damani and V. K. Garg. </author> <title> How to Recover Efficiently and Asynchronously when Optimism Fails. </title> <booktitle> In Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 108115, </pages> <year> 1996. </year>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. <p> However, with causal protocols a process can start its recovery only after collecting the necessary determinants from the volatile logs of the operational processes. It has been qualitatively argued <ref> [5] </ref> that optimistic protocols that start recovery without waiting for data from other processes may have a shorter recovery time than causal protocols. <p> Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance [10, 12, 13, 14, 20, 21, 23]), we have implemented the protocol described in <ref> [5] </ref>. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks [16]. <p> An orphan process first synchronously flushes its logs to stable storage. Then, it rolls back to a checkpoint consistent with the recovered state of the failed process and uses its logs to roll-forward to the latest possible consistent state. In our implementation, we have modified the pseudo-code presented in <ref> [5] </ref> so that the recovering process sends the failure announcements before replaying any message from the log, rather than after all messages in the log have been replayed. This optimization allows the roll-forward of recovering processes to proceed in parallel with the identification, roll-back and eventual roll-forward of orphan processes.
Reference: [6] <author> E. N. Elnozahy. </author> <title> On the relevance of communication costs of rollback-recovery protocols. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 7479, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Also, in the first phase, a recovering process obtains lost messages from the remaining processes. In the second phase, the collected data is replayed, restoring the process to its pre-crash state. To handle multiple concurrent failures, we implemented a protocol that recovers crashed processes without blocking operational processes <ref> [6] </ref>. In this protocol, the recovering processes elect a leader, which is responsible for collecting determinants and messages on behalf of all recovering processes.
Reference: [7] <author> E. N. Elnozahy, D. B. Johnson, and Y. M. Wang. </author> <title> A Survey of Rollback-Recovery Protocols in Message-Passing Systems. </title> <type> Technical Report CMU-CS-96-181, </type> <institution> Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: This property is sufficient to restore a crashed process in a state consistent with the state of all operational processes. Although several studies have measured the overhead imposed by each of these approaches during failure-free executions <ref> [7, 10] </ref>, their merits during recovery have been so far argued mostly qualitatively. For instance, there is consensus that pessimistic protocols are well-suited for supporting fast recovery, since they guarantee that all determinants can be readily retrieved from stable storage. The opinions about optimistic protocols are less unanimous. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated.
Reference: [8] <author> E. N. Elnozahy, D. B. Johnson, and W. Zwaenepoel. </author> <title> The Performance of Consistent Checkpoint-ing. </title> <booktitle> In Proceedings of the Eleventh Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3947, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In our current implementation, checkpoints are synchronous (i.e., applications block during check-pointing). The checkpointing mechanisms used are similar to those described in [17]. We are currently enhancing our implementation to utilize optimizations such as incremental checkpointing and copy-on-write <ref> [8] </ref>.
Reference: [9] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback and fast output commit. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5):526531, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Optimistic protocols can perform very efficiently in failure-free executions. However, if any of the determinants are lost when a process crashes, then orphans may be created. To reach a consistent global state, these processes must be identified and rolled back. * Causal protocols <ref> [1, 9] </ref> combine some of the positive aspects of pessimistic and optimistic protocols: They never create orphans, yet they do not write determinants to stable storage synchronously. In causal protocols, determinants are logged in volatile memory. <p> In our implementation, this is accomplished by piggybacking determinants on existing application messages until they are logged by at least f + 1 processes <ref> [3, 9] </ref>. Recovery of a failed process proceeds in two phases. In the first phase, the process obtains its determinants from the volatile logs of the operational processes. In addition, the process also obtains the content of messages it delivered before crashing. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [3, 9, 10, 12, 21] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [10] <author> E. N. Elnozahy and W. Zwaenepoel. </author> <title> On the use and implementation of message logging. </title> <booktitle> In Digest of Papers: 24 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 298 307. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: This property is sufficient to restore a crashed process in a state consistent with the state of all operational processes. Although several studies have measured the overhead imposed by each of these approaches during failure-free executions <ref> [7, 10] </ref>, their merits during recovery have been so far argued mostly qualitatively. For instance, there is consensus that pessimistic protocols are well-suited for supporting fast recovery, since they guarantee that all determinants can be readily retrieved from stable storage. The opinions about optimistic protocols are less unanimous. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [3, 9, 10, 12, 21] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks. <p> Such receiver-based logging protocols, however, are known to impose a high overhead on application performance during failure-free runs <ref> [10] </ref>. We are currently developing new protocols to overcome this tradeoff and simultaneously provide both low-overhead during failure-free executions and fast crash recovery.
Reference: [11] <author> D. B. Johnson. </author> <title> Efficient transparent optimistic rollback recovery for distributed application programs. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated.
Reference: [12] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> In Digest of Papers: 17 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 1419. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: An orphan process is an operational process whose state is inconsistent with the recovered state of a crashed process. All message-logging protocols guarantee that upon recovery no process is an orphan, but differ in the way they enforce this consistency condition: * Pessimistic protocols <ref> [4, 12, 18] </ref> require that a process, before sending a message, synchronously log on stable storage the determinants and the content of all messages delivered so far. Thus, pessimistic protocols never create orphan processes. <p> The first protocol is receiver-based: a process, before sending a message, logs to stable storage both the determinants and the contents of the messages delivered so far. The second protocol is instead sender-based <ref> [12] </ref>: the receiver logs synchronously to stable storage only the determinant of every message it delivers, while the contents of the message are stored in a volatile log kept by the message's sender 2 This protocol is similar to the one described in [22]. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. <p> The determinants and the content of the messages delivered are kept in volatile memory 2 Some sender-based pessimistic protocols keep both determinants and message contents at the senders <ref> [12, 14] </ref>. We have not implemented these protocols because they can only tolerate at most two concurrent failures. 4 logs at the receiver and periodically flushed to stable storage. Since in a crash these logs in volatile memory are lost, orphans may be created. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [3, 9, 10, 12, 21] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [13] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Recovery in Distributed Systems Using Optimistic Message Logging and Checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11:462491, </volume> <year> 1990. </year>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes.
Reference: [14] <author> T. Y. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454461. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. <p> The determinants and the content of the messages delivered are kept in volatile memory 2 Some sender-based pessimistic protocols keep both determinants and message contents at the senders <ref> [12, 14] </ref>. We have not implemented these protocols because they can only tolerate at most two concurrent failures. 4 logs at the receiver and periodically flushed to stable storage. Since in a crash these logs in volatile memory are lost, orphans may be created.
Reference: [15] <author> L. Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7):558565, </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: In causal protocols, determinants are logged in volatile memory. To prevent orphans, processes piggyback their volatile log of determinants on every message they send 1 . This guarantees that if the state of an operational process p causally depends <ref> [15] </ref> on the delivery of a message m, then p has a copy of m's determinant in its volatile memory. This property is sufficient to restore a crashed process in a state consistent with the state of all operational processes.
Reference: [16] <author> F. Mattern. </author> <title> Virtual Time and Global States of Distributed Systems. </title> <editor> In M. Cosnard et. al., editor, </editor> <booktitle> Parallel and Distributed Algorithms, </booktitle> <pages> pages 215226. </pages> <publisher> Elsevir Science Publishers B. V., </publisher> <year> 1989. </year>
Reference-contexts: This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks <ref> [16] </ref>. On a message send, the sender piggybacks its vector clock on the message; on a message deliver, the receiver updates its vector clock by computing a component-wise maximum with the piggybacked vector clock.
Reference: [17] <author> J. S. Plank, M. Beck, G. Kingsley, and K. Li. </author> <title> Libckpt:Transparent checkpointing under Unix. </title> <booktitle> In Proceedings of the USENIX Technical Conference, </booktitle> <pages> pages 213224, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In case of a failure, the failed process is re-started and its state is restored to that recorded in the latest checkpoint. In our current implementation, checkpoints are synchronous (i.e., applications block during check-pointing). The checkpointing mechanisms used are similar to those described in <ref> [17] </ref>. We are currently enhancing our implementation to utilize optimizations such as incremental checkpointing and copy-on-write [8].
Reference: [18] <author> M. L. Powell and D. L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechanism. </title> <booktitle> In Proceedings of the Ninth Symposium on Operating System Principles, pages 100109. ACM SIGOPS, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: An orphan process is an operational process whose state is inconsistent with the recovered state of a crashed process. All message-logging protocols guarantee that upon recovery no process is an orphan, but differ in the way they enforce this consistency condition: * Pessimistic protocols <ref> [4, 12, 18] </ref> require that a process, before sending a message, synchronously log on stable storage the determinants and the content of all messages delivered so far. Thus, pessimistic protocols never create orphan processes.
Reference: [19] <author> F. B. Schneider. </author> <title> Implementing faulttolerant services using the state machine approach: A Tutorial. </title> <journal> Computing Surveys, </journal> <volume> 22(3):299319, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: Distributed applications requiring both fault-tolerance and high availability were few and highly sophisticated, and its users could typically afford to invest the resources necessary to mask failures through explicit replication in space <ref> [19] </ref> instead of recovering from failures through replication in time. As distributed computing becomes commonplace and many more applications are faced with the current costs of high availability, there is a fresh need for recovery-based techniques that combine high performance during failure-free executions with fast recovery.
Reference: [20] <author> A. P. Sistla and J. L. Welch. </author> <title> Efficient Distributed Recovery Using Message Logging. </title> <booktitle> In Proceedings of the Eighth Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223238. </pages> <publisher> ACM SIGACT/SIGOPS, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes.
Reference: [21] <author> R. B. Strom and S. Yemeni. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3):204226, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes. <p> This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect <ref> [21] </ref>, implements a singularly efficient method for detecting orphans processes. In this protocol, causal dependencies are tracked using vector clocks [16]. <p> Interestingly, this principle is in stark contrast with recent trends in the design of message logging protocols, which, in order to improve performance during failure free executions, have messages logged by the senders <ref> [3, 9, 10, 12, 21] </ref>. Indeed, a conclusion of our experiments is that there exists a tradeoff between performance during failure-free executions and recovery. Second, it is a good idea to avoid rollbacks.
Reference: [22] <author> R. E. Strom, D. F. Bacon, and S. A. Yemini. </author> <title> Volatile Logging in n-Fault-Tolerant Distributed Systems. </title> <booktitle> In Proceedings of the Eighteenth Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 4449, </pages> <year> 1988. </year>
Reference-contexts: The second protocol is instead sender-based [12]: the receiver logs synchronously to stable storage only the determinant of every message it delivers, while the contents of the message are stored in a volatile log kept by the message's sender 2 This protocol is similar to the one described in <ref> [22] </ref>. In both of these protocols, the first step of recovering a process p consists in restoring it to its latest checkpoint. Then, in the receiver-based protocol, the messages logged on stable storage are replayed to p in the appropriate order.
Reference: [23] <author> S. Venkatesan and T. Y. Juang. </author> <title> Efficient Algorithms for Optimistic Crash Recovery. </title> <booktitle> Distributed Computing, </booktitle> <address> 8(2):105114, </address> <month> June </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: Thus, pessimistic protocols never create orphan processes. However, synchronously logging determinants on stable storage imposes a significant overhead during failure-free executions. * Optimistic protocols <ref> [5, 13, 14, 20, 21, 23] </ref> allow processes to communicate even if the determinants they depend upon are not yet logged on stable storage. These protocols only require that determinants reach stable storage eventually. Optimistic protocols can perform very efficiently in failure-free executions. <p> Although the literature contains careful analyses of the cost of recovery for different optimistic protocols in terms of the number of messages and the rounds of communication needed to identify and roll back orphan processes <ref> [5, 7, 11, 13, 20, 21, 23] </ref>, in general no experimental evaluations of their performance during recovery are offered. The performance of causal protocols during recovery has also been debated. <p> These messages are matched by p with the corresponding determinants logged on stable storage and then replayed in the appropriate order. Optimistic logging: Among the numerous optimistic protocols that have been proposed in the the literature (for instance <ref> [10, 12, 13, 14, 20, 21, 23] </ref>), we have implemented the protocol described in [5]. This protocol, in addition to tolerating an arbitrary number of failures and preventing the uncontrolled cascading of rollbacks known as the domino effect [21], implements a singularly efficient method for detecting orphans processes.
References-found: 23

