URL: http://www.cs.umd.edu/~wak/dissertation.ps.Z
Refering-URL: http://www.cs.umd.edu/users/wak/
Root-URL: 
Title: UNIFIED TRANSFORMATION FRAMEWORK  
Degree: Wayne Anthony Kelly, Doctor of Philosophy, 1996 Dissertation directed by: Professor William W. Pugh  
Affiliation: Department of Computer Science  
Note: ABSTRACT Title of Dissertation: OPTIMIZATION WITHIN A  
Abstract: Programmers typically want to write scientific programs in a high level language with semantics based on a sequential execution model. To execute efficiently on a parallel machine, however, a program typically needs to contain explicit parallelism and possibly explicit communication and synchronization. So, we need compilers to convert programs from the first of these forms to the second. There are two basic choices to be made when parallelizing a program. First, the computations of the program need to be distributed amongst the set of available processors. Second, the computations on each processor need to be ordered. My contribution has been the development of simple mathematical abstractions for representing these choices and the development of new algorithms for making these choices. I have developed a new framework that achieves good performance by minimizing communication between processors, minimizing the time processors spend waiting for messages from other 
Abstract-found: 1
Intro-found: 1
Reference: [AAL95] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proc. of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: I would like to extend the algorithm to produce time mappings that correspond to additional transformations such as loop skewing, loop tiling and index set splitting. Other, non iteration reordering transformations, such as the data transformations proposed by Anderson et.al. <ref> [AAL95] </ref>, should also be incorporated into the overall framework. 6.6 My Ultimate Goal My ultimate goal is to strengthen the system, both in terms of efficiency and effectiveness, to a point where it could be directly incorporated into a production-quality parallelizing compiler. 148 Chapter 7 Conclusions The main contribution of my
Reference: [ACK87] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the Fourteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Similarly, if I want iteration x of statement p to occur in the same iteration of the level d loop as iteration x + ffi of statement q, then I will select constants such that d d p = d d (these constants correspond to loop alignment transformations <ref> [ACK87] </ref>). My current implementation separates statements at the minimum loop depth possible. This allows the maximal degrees of parallelism to be exploited for all statements. In some cases it may be possible and in fact preferable to separate statements at a depth greater that the minimal allowable.
Reference: [AG94] <author> George S. Almasi and Allan Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjanin/Cummings, </address> <year> 1994. </year>
Reference-contexts: In the case of Uniform Memory Access (UMA) <ref> [AG94] </ref> machines, the fact that doall and doacross loops cannot control the mapping of iterations onto processors is of no consequence since the choice of mapping can not affect performance (provided some care is taken to ensure load balancing). However, in the case of Non-Uniform Memory Access (NUMA) machines [AG94], the <p> (UMA) <ref> [AG94] </ref> machines, the fact that doall and doacross loops cannot control the mapping of iterations onto processors is of no consequence since the choice of mapping can not affect performance (provided some care is taken to ensure load balancing). However, in the case of Non-Uniform Memory Access (NUMA) machines [AG94], the choice of mapping can have a substantial affect on performance. For NUMA machines, the choice of mapping will determine which data accesses will be satisfied locally and which data accesses will require communication with other processors. Consider for example the program shown in Figure 2.1.
Reference: [AI91] <author> Corinne Ancourt and Franccois Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The problem of generating perfectly nested loops to iterate over all and only those points in such a convex region has been studied by a number of researchers starting with the seminal work of Ancourt and Irigoin <ref> [AI91] </ref>. If the original iteration space is non-convex (as a consequence of non-unit loop steps), or if the time mapping applied is not onto, then the transformed 90 iteration space may be non-convex. <p> Working with actual loop permutations, rather than with formulas describing schedules, makes it much easier to analyze complex performance issues such as data locality. 5.3 Generating Code The problem of generating code for a convex region was first addressed by Ancourt and Irigoin <ref> [AI91] </ref>. They use Fourier pairwise elimination at each level to provide bounds on each of the index variables. They then form the union of all of these projections to produce a single set of constraints which explicitly contains all of the information necessary to generate code.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89c, Wol90, CK92] </ref>. Figures 2.5 through 2.8 show an example of such a sequence of transformations (black dots correspond to iterations of the first statement and white dots correspond to iterations of the second statement). Each of these transformations has its own special legality checks and transformation rules.
Reference: [AL93a] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In ACM '93 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program.
Reference: [AL93b] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In ACM '93 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: select data distributions and restructure the program, then deciding how to divide a program into phases can be particularly difficult as it also involves making decisions about the best order in which to perform the computations. 2.2.2 My New Abstraction I use a relatively new abstraction called a space mapping <ref> [Fea94, AL93b] </ref> to directly map the computations of a program to a virtual processor array. I associate a tuple relation S p , with each statement p. It specifies that each iteration i of statement p will be executed on virtual processor S p (i). <p> Most related works estimate parallelism or partition the program into phases based on the original loop structure. Anderson and Lam <ref> [AL93b] </ref> have developed methods to automatically find computation and data decompositions that optimize both parallelism and locality. They first perform loop distribution and unimodular transformations to produce a canonical form consisting of fully permutable loop nests.
Reference: [AT93] <author> Eduard Ayguade and Jordi Torres. </author> <title> Partitioning the statement per iteration space using non-singular matrices. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 407-415, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Li and Pingali [LP92] consider the non-convex case resulting from mappings that are not necessarily onto. They use a linear algebra framework and compute loop bounds and steps using Hermite normal form. They do not consider the multiple mapping case. Ayguade and Torres <ref> [AT93] </ref> consider a limited case of the multiple mapping case 142 where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski [Cha93a, Cha93b] generates Nested Loop Structures.
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91] </ref> go some way towards solving this problem. Unimodular transformations are able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. Such a transformation is described by a unimodular linear mapping from the original iteration space to a new iteration space. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91] </ref>. <p> They do not consider transformations such as loop distribution or interchange and do not consider pipelined parallelism nor the differences in synchronization costs between different candidate loops. 5.2 Ordering Computation The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as my work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [BKK93] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93349-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year> <month> 152 </month>
Reference-contexts: Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program. <p> The problem with this approach is that it is not possible to sacrifice some parallelism in a particular statement (by instead pipelining it or not distributing it), in order to reduce overall communication costs. Although some other systems <ref> [BKK93, GAL95] </ref> also use exact rather than greedy heuristic algorithms, the size of the problems and the methods used are very different. I consider a list of candidate distributions for each statement, whereas these systems consider a list of candidate distributions for each array in each phase. <p> These systems use 0-1 integer programming formulations, whereas I have developed my own graph search algorithm. The performance numbers given in [GAL95] (which uses a commercial 0-1 integer programming system called LINGO) tend 133 to suggest that the search algorithms are comparable in speed. In Kremer's system <ref> [BKK93] </ref>, an admittedly arbitrary scheme is used to identify a sequential loop nest that contains a series of phases, which are executed atomically (i.e., without overlap). Parallelism is exploited within each phase but not between them. Using techniques not described in their 1993 paper [BKK93], a set of candidate distributions are <p> In Kremer's system <ref> [BKK93] </ref>, an admittedly arbitrary scheme is used to identify a sequential loop nest that contains a series of phases, which are executed atomically (i.e., without overlap). Parallelism is exploited within each phase but not between them. Using techniques not described in their 1993 paper [BKK93], a set of candidate distributions are generated for each phase, and the system determines the cost of executing each phase in each distribution and the cost of re-mapping variables between each transition.
Reference: [Blu92] <author> William Joseph Blume. </author> <title> Success and limitations in automatic parallelization of the Perfect benchmarks T M programs. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Of course, the problem with this approach is that converting sequential programs into efficient parallel programs is a very difficult task. So, not surprisingly, existing parallelizing compilers for massively parallel machines often fail to produce satisfactory results <ref> [Blu92, SH91] </ref>. There are two basic choices to be made when parallelizing a program. First, the computations of the program need to be distributed amongst the set of available processors. Second, the computations on each processor need to be ordered.
Reference: [CF93] <author> J.-F. Collard and P. Feautrier. </author> <title> Automatic generation of data parallel code. </title> <editor> In H.J. Sips, editor, </editor> <booktitle> Proceedings of the Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 321-332, </pages> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Collard, Feautrier and Risset [CFR93] show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the single mapping case. Collard and Feautrier <ref> [CF93] </ref> address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated. They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd).
Reference: [CFR93] <author> Jean-Franccois Collard, Paul Feautrier, and Tanguy Risset. </author> <title> Construction of DO loops from systems of affine constraints. </title> <type> Technical Report N o 93-15, </type> <institution> Laboratoire de l'Informatique du Parallelisme, Ecolo Normal Superieure de Lyon, Instiut IMAG, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: However, the constraints I have seen in both dependence analysis and code generation are quite sparse, and Fourier elimination is quite efficient for sparse constraints [PW94a]. Collard, Feautrier and Risset <ref> [CFR93] </ref> show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the single mapping case. Collard and Feautrier [CF93] address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated.
Reference: [Cha93a] <author> Zbigniew Chamski. </author> <title> Fast and efficient generation of loop bounds. </title> <note> Publication interne 771, </note> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: They do not consider the multiple mapping case. Ayguade and Torres [AT93] consider a limited case of the multiple mapping case 142 where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski <ref> [Cha93a, Cha93b] </ref> generates Nested Loop Structures. He discusses generating code only for the single mapping convex case. He reduces control overhead by generating sequences of loops to remove all min and max expressions in loop bounds. The cost of code duplication may be large when all such overheads are removed. <p> He discusses generating code only for the single mapping convex case. He reduces control overhead by generating sequences of loops to remove all min and max expressions in loop bounds. The cost of code duplication may be large when all such overheads are removed. Chamski claims <ref> [Cha93a] </ref> that Fourier variable elimination is prohibitively expensive for code generation. I have found it to be a very efficient method, and suspect he used unrealistic examples and/or a poor implementation of Fourier variable elimination.
Reference: [Cha93b] <author> Zbigniew Chamski. </author> <title> Nested loop sequences: Towards efficient loop structures in automatic parallelization. </title> <note> Publication interne 772, </note> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: They do not consider the multiple mapping case. Ayguade and Torres [AT93] consider a limited case of the multiple mapping case 142 where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski <ref> [Cha93a, Cha93b] </ref> generates Nested Loop Structures. He discusses generating code only for the single mapping convex case. He reduces control overhead by generating sequences of loops to remove all min and max expressions in loop bounds. The cost of code duplication may be large when all such overheads are removed.
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In this model, parallelism is achieved by performing the same operation on different data elements at the same time. The distribution of computation to processors is specified implicitly in such systems via data distribution annotations [HKT91, For92] and the owner-computes rule <ref> [CK88] </ref>. A data distribution is an affine mapping from the elements of an array to a virtual processor array. The owner-computes rule states that each iteration should be performed on the virtual processor that "owns" the array element being written.
Reference: [CK92] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings Supercomputing'92, </booktitle> <pages> pages 114-125, </pages> <address> Minneapolis, Minnesota, </address> <month> Nov </month> <year> 1992. </year>
Reference-contexts: Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89c, Wol90, CK92] </ref>. Figures 2.5 through 2.8 show an example of such a sequence of transformations (black dots correspond to iterations of the first statement and white dots correspond to iterations of the second statement). Each of these transformations has its own special legality checks and transformation rules.
Reference: [Fea92a] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, One-dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct </month> <year> 1992. </year>
Reference-contexts: Because it uses only single level affine schedules and requires that all dependences be carried by the outer loop, it can only be applied to programs that can be executed in linear time on a parallel machine. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> independently developed a framework which is very similar to my own. It is similar in the following respects: * He represents reordering transformations using multi-dimensional schedules which are similar in form to my time mappings. * He generates a separate schedule for each statement. <p> They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Such stride constraints arise frequently in Feautrier's parallelization framework 143 <ref> [Fea92a, Fea92b] </ref>; I try to avoid them in my framework, since generating good code for them is difficult. In earlier work Pugh, Rosser and I [KPR95] developed an effective code generation algorithm for the general multiple time mapping problem.
Reference: [Fea92b] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part II, Multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Because it uses only single level affine schedules and requires that all dependences be carried by the outer loop, it can only be applied to programs that can be executed in linear time on a parallel machine. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> independently developed a framework which is very similar to my own. It is similar in the following respects: * He represents reordering transformations using multi-dimensional schedules which are similar in form to my time mappings. * He generates a separate schedule for each statement. <p> They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Such stride constraints arise frequently in Feautrier's parallelization framework 143 <ref> [Fea92a, Fea92b] </ref>; I try to avoid them in my framework, since generating good code for them is difficult. In earlier work Pugh, Rosser and I [KPR95] developed an effective code generation algorithm for the general multiple time mapping problem.
Reference: [Fea94] <author> Paul Feautrier. </author> <title> Toward automatic distribution. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(3) </volume> <pages> 233-244, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: select data distributions and restructure the program, then deciding how to divide a program into phases can be particularly difficult as it also involves making decisions about the best order in which to perform the computations. 2.2.2 My New Abstraction I use a relatively new abstraction called a space mapping <ref> [Fea94, AL93b] </ref> to directly map the computations of a program to a virtual processor array. I associate a tuple relation S p , with each statement p. It specifies that each iteration i of statement p will be executed on virtual processor S p (i). <p> Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program. <p> Feautrier's approach <ref> [Fea94] </ref> is to find a schedule for executing the program with maximum parallelism, ignoring locality and latency. Then, he uses a greedy algorithm, based on the dimensionality of value-based flow dependences, to select a computation distribution.
Reference: [For92] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <year> 1992. </year> <note> Revised May, 1993. 153 </note>
Reference-contexts: A common approach is the so called data parallel paradigm. In this model, parallelism is achieved by performing the same operation on different data elements at the same time. The distribution of computation to processors is specified implicitly in such systems via data distribution annotations <ref> [HKT91, For92] </ref> and the owner-computes rule [CK88]. A data distribution is an affine mapping from the elements of an array to a virtual processor array. The owner-computes rule states that each iteration should be performed on the virtual processor that "owns" the array element being written.
Reference: [GAL95] <author> Jordi Garcia, Eduard Ayguade, and Jesus Labarta. </author> <title> A novel approach towards automatic data distribution. In Workshop on Automatic Data Layout and Performance Predition, </title> <month> April </month> <year> 1995. </year>
Reference-contexts: Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program. <p> The problem with this approach is that it is not possible to sacrifice some parallelism in a particular statement (by instead pipelining it or not distributing it), in order to reduce overall communication costs. Although some other systems <ref> [BKK93, GAL95] </ref> also use exact rather than greedy heuristic algorithms, the size of the problems and the methods used are very different. I consider a list of candidate distributions for each statement, whereas these systems consider a list of candidate distributions for each array in each phase. <p> My search spaces will therefore tend to be much larger. These systems use 0-1 integer programming formulations, whereas I have developed my own graph search algorithm. The performance numbers given in <ref> [GAL95] </ref> (which uses a commercial 0-1 integer programming system called LINGO) tend 133 to suggest that the search algorithms are comparable in speed. <p> The system is very dependent on obtaining a good partitioning of the program into phases and on having a good method to generate and evaluate distributions for each phase. The system described by Garcia et.al. <ref> [GAL95] </ref> uses a static data decomposition for the entire program. They minimize communication volume and try to insure that the program can be executed in parallel simply by making some of the loops in the original program doall loops.
Reference: [Gav72] <author> Fanica Gavril. </author> <title> Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. </title> <journal> SIAM J. Computing, </journal> <volume> 1(2) </volume> <pages> 180-187, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: Intuitively, placing this first barrier splits a "ring" of statements into a "chain" of statements. It is then easy to place the remaining barriers optimally. This problem reduces to finding a minimal clique cover for an interval graph and is known to be solvable in polynomial time <ref> [Gav72] </ref>. All ordering of statements and positions will now implicitly be with respect to the above total order. Let b i be the position of the i th barrier after b 0 .
Reference: [GS96] <author> M. Gupta and E. Schonberg. </author> <title> Static analysis to reduce synchronization costs in data-parallel programs. </title> <booktitle> In Conference Record of POPL '96: The Twenty-third ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: The basic idea used in this algorithm can also be used for applications other than inserting barriers, for example it could be used to determine the optimal placement of blocking communication statements for distributed memory machines. See <ref> [GS96] </ref> for a discussion of this problem. 3.5.4.2 Inserting Post-Wait Pairs Post-wait pairs are used to enforce any forward dependences that haven't already been enforced by barriers. Any dependences that are already enforced 107 by post-wait pairs at deeper levels are removed from consideration.
Reference: [Gup92] <author> M. Gupta. </author> <title> Automatic Data Partioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program.
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed memory machines. </title> <booktitle> In Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: A common approach is the so called data parallel paradigm. In this model, parallelism is achieved by performing the same operation on different data elements at the same time. The distribution of computation to processors is specified implicitly in such systems via data distribution annotations <ref> [HKT91, For92] </ref> and the owner-computes rule [CK88]. A data distribution is an affine mapping from the elements of an array to a virtual processor array. The owner-computes rule states that each iteration should be performed on the virtual processor that "owns" the array element being written. <p> a dependence is carried in the transformed iteration space is the shallowest and therefore most efficient level at which to insert synchronization to enforce that dependence (the same basic idea has been used 103 in the past to decide the outermost level at which communication statements can be legally placed <ref> [HKT91] </ref>). Synchronization inserted at deeper levels can make synchronization inserted at shallower levels redundant, so I first insert synchronization for dependences carried at deeper levels. <p> The only work that directly addresses the problem of reordering iterations given a computation distribution is the work on cross-processor loops in Fortran D <ref> [HKT91] </ref>. Fortran D normally uses an owner-computes rule and user-supplied data decompositions. An algorithm is given in [HKT91] to identify "cross-processor loops". Cross-processor loops are informally defined as: Sequential space-bound loops causing computation wavefronts that cross processor boundaries (i.e., sweeps over the distributed dimensions). <p> The only work that directly addresses the problem of reordering iterations given a computation distribution is the work on cross-processor loops in Fortran D <ref> [HKT91] </ref>. Fortran D normally uses an owner-computes rule and user-supplied data decompositions. An algorithm is given in [HKT91] to identify "cross-processor loops". Cross-processor loops are informally defined as: Sequential space-bound loops causing computation wavefronts that cross processor boundaries (i.e., sweeps over the distributed dimensions). For a block decomposition, cross-processor loops are interchanged inward; for a cyclic decomposition cross-processor loops are interchanged outward.
Reference: [HLL92] <author> Tien Huynh, Catherine Lassez, and Jean-Louis Lassez. </author> <title> Practical issues on the projection of polyhedral sets. </title> <journal> Annals of mathematics and artificial intelligence, </journal> <month> November </month> <year> 1992. </year>
Reference-contexts: They propose that fast inexact techniques be used to remove redundancies from this set before it is used to generate code. They consider only the single mapping convex case. Lassez et.al. <ref> [HLL92] </ref> provide a good perspective on different methods used to project integer convex polyhedra. Li and Pingali [LP92] consider the non-convex case resulting from mappings that are not necessarily onto. They use a linear algebra framework and compute loop bounds and steps using Hermite normal form.
Reference: [Ken92] <author> Ken Kennedy. </author> <title> Guest editorial: Software for supercomputers of the future. </title> <journal> The Journal of Supercomputing, </journal> <pages> pages 251-262, </pages> <year> 1992. </year>
Reference-contexts: The other method used to increase computing power is to use multiple processors that can work in parallel. Over the last five years, the trend has been towards massively parallel computers, where hundreds or even thousands of processors are used <ref> [Ken92] </ref>. The problem with using multiple processors is that it is very difficult to write programs that can attain the theoretical peak performance of these machines. The reasons for these difficulties include load balancing problems, memory 1 contention, interprocessor communication bottlenecks, and lack of explicit parallelism.
Reference: [KKB92] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hieracical parallel machines in polynomial time. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 82-92, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: They do not consider transformations such as loop distribution or interchange and do not consider pipelined parallelism nor the differences in synchronization costs between different candidate loops. 5.2 Ordering Computation The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as my work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [KMP + 95] <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The Omega Library interface guide. </title> <type> Technical Report CS-TR-3445, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: tuple [i; i] if i is even and j is between 1 and the symbolic constant n, or i is less than j: f [i; j] ! [i; i] j (9ff s:t: i = 2ff ^ 1 j n) _ i &lt; jg I have helped develop the Omega Library <ref> [KMP + 95, KPRS95] </ref> which is a set of C++ classes for representing and manipulating tuple relations and sets. The Omega Library can represent any tuple relation or set that can be described using Presburger arithmetic. <p> In addition to the transformation system described here, PETIT has been extended to include array expansion, induction variable recognition and advanced dependence analysis using the Omega Test [Pug92, Won95]. All of these extensions have been implemented using the Omega Library <ref> [KMP + 95] </ref> which is set of C++ classes for representing and manipulating tuple relations and sets. PETIT and the Omega Library are freely available via anonymous ftp from ftp://ftp.cs.umd.edu/pub/omega/omega system.
Reference: [KMW67] <author> R.M. Karp, R.E. Miller, and S. Winograd. </author> <title> The organization of computations for uniform recurrence equations. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 563-590, </pages> <year> 1967. </year>
Reference-contexts: But for non-stencil computations, there are some real codes on which the problems are manifested. For Cholesky decomposition (Figure 3.15) with the first dimension distributed, it identifies all 3 loops as being "cross-processor", providing no guidance. The basis for systolic techniques was laid by Karp, Miller and Winograd's paper <ref> [KMW67] </ref> on uniform recurrences. The basic idea is that given a set of function values to be computed, and a set of recurrence equations constraining the 139 function values, the computation can be organized by specifying a schedule that defines the "time" at which each function value should be computed.
Reference: [KP93] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-2995.1, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: If I changed this policy, then alignment would become a much bigger problem. To solve the problem completely, it would be necessary to implement an algorithm to select constant offsets that properly align the statements, such as the algorithm I developed in <ref> [KP93] </ref>. The problem with that approach is that it may be too slow to use for large programs. Another approach is to backtrack in the search procedure to find a different set of space mappings or time mappings that don't need to be specially aligned.
Reference: [KPR95] <author> Wayne Kelly, William Pugh, and Evan Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 332-341, </pages> <address> McLean, Virginia, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The corresponding transformed iteration space can be "very" non-convex; that is, there is no set of perfectly nested loops without conditionals, even with non-unit steps, that can scan the space. In earlier work Pugh, Rosser and I <ref> [KPR95] </ref> developed an effective code generation algorithm for this most general class of time mappings. That work included techniques to generate non-unit steps to iterate over non-convex regions and to split iteration spaces to handle the overlap of the transformed iteration spaces caused by multiple time mappings. <p> Such stride constraints arise frequently in Feautrier's parallelization framework 143 [Fea92a, Fea92b]; I try to avoid them in my framework, since generating good code for them is difficult. In earlier work Pugh, Rosser and I <ref> [KPR95] </ref> developed an effective code generation algorithm for the general multiple time mapping problem. That earlier work included techniques to generate non-unit steps to iterate over non-convex regions and a framework within which to trade-off control overhead for code duplication.
Reference: [KPRS95] <author> Wayne Kelly, William Pugh, Evan Rosser, and Tatiana Shpeisman. </author> <title> Transitive closure of infinite graphs and its applications. </title> <booktitle> In Eighth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: tuple [i; i] if i is even and j is between 1 and the symbolic constant n, or i is less than j: f [i; j] ! [i; i] j (9ff s:t: i = 2ff ^ 1 j n) _ i &lt; jg I have helped develop the Omega Library <ref> [KMP + 95, KPRS95] </ref> which is a set of C++ classes for representing and manipulating tuple relations and sets. The Omega Library can represent any tuple relation or set that can be described using Presburger arithmetic.
Reference: [Lam74] <author> Leslie Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: The basic idea is that given a set of function values to be computed, and a set of recurrence equations constraining the 139 function values, the computation can be organized by specifying a schedule that defines the "time" at which each function value should be computed. Lamport's paper <ref> [Lam74] </ref> on Hyper-planes, was the first to apply these ideas to parallelizing compilers. These ideas have continued to evolve following the development of systolic arrays. Lengauer [Len93] provides a good summary of traditional systolic techniques.
Reference: [Len93] <author> Christian Lengauer. </author> <title> Loop parallelization in the polytope model. </title> <booktitle> In CONCUR, </booktitle> <year> 1993. </year>
Reference-contexts: Lamport's paper [Lam74] on Hyper-planes, was the first to apply these ideas to parallelizing compilers. These ideas have continued to evolve following the development of systolic arrays. Lengauer <ref> [Len93] </ref> provides a good summary of traditional systolic techniques. Pugh [Pug91] gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by unimodular transformations.
Reference: [Li95] <author> Wei Li. </author> <title> Compiler cache optimizations for banded matrix problems. </title> <booktitle> In 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The importance of exploiting locality between different iterations of the sequential loops that surround the parallel loops is discussed but not solutions are proposed. The data locality models used by McKinley [McK92], Wolf [Wol92] and Li <ref> [Li95] </ref> differ from mine in two major respects. I assume that a data item will only remain in the cache during some period of time if it is reused every iteration during that time period.
Reference: [LP92] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: In these cases it is still possible to generate suitable perfectly nested loops; however, some of the loop steps will be non-unit. Techniques for handling this case are described by Li and Pingali <ref> [LP92] </ref>. The algorithm I describe in this section addresses the more general case, where a potentially different time mapping is used for each statement. <p> It therefore cannot represent some important transformations such as loop fusion, loop distribution and statement reordering. Unimodular transformations are generalized by Li and Pingali in <ref> [LP92] </ref>. to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. <p> I felt that the high cost of this analysis would not be justified by the small potential benefits of exploiting group reuse. The locality model used by Li and Pingali <ref> [LP92] </ref> is different in that it is used to derive both a loop transformation and a computation distribution given a data distribution. The first goal is to minimize off-processor data accesses by as much as possible performing computations on the same processor that owns the data being referenced. <p> They propose that fast inexact techniques be used to remove redundancies from this set before it is used to generate code. They consider only the single mapping convex case. Lassez et.al. [HLL92] provide a good perspective on different methods used to project integer convex polyhedra. Li and Pingali <ref> [LP92] </ref> consider the non-convex case resulting from mappings that are not necessarily onto. They use a linear algebra framework and compute loop bounds and steps using Hermite normal form. They do not consider the multiple mapping case.
Reference: [McK92] <author> Kathryn S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice U., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. Unimodular transformations are combined with blocking in [WL91, ST92]. A similar approach, although not using a unimodular framework, is described in [Wol89b]. McKinley <ref> [McK92] </ref> and Wolf [Wol92] have both developed models to estimate data locality and parallelism. Both papers highlight the importance of the tradeoff between parallelism and locality. Wolf's system is based on unimodular transformations while McKinley's system uses traditional transformations. <p> The importance of exploiting locality between different iterations of the sequential loops that surround the parallel loops is discussed but not solutions are proposed. The data locality models used by McKinley <ref> [McK92] </ref>, Wolf [Wol92] and Li [Li95] differ from mine in two major respects. I assume that a data item will only remain in the cache during some period of time if it is reused every iteration during that time period.
Reference: [Pug91] <author> William Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In 1991 International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Lamport's paper [Lam74] on Hyper-planes, was the first to apply these ideas to parallelizing compilers. These ideas have continued to evolve following the development of systolic arrays. Lengauer [Len93] provides a good summary of traditional systolic techniques. Pugh <ref> [Pug91] </ref> gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by unimodular transformations.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Tiny programs consist of a single procedure containing for loops and assignment statements involving array expressions. In addition to the transformation system described here, PETIT has been extended to include array expansion, induction variable recognition and advanced dependence analysis using the Omega Test <ref> [Pug92, Won95] </ref>. All of these extensions have been implemented using the Omega Library [KMP + 95] which is set of C++ classes for representing and manipulating tuple relations and sets. PETIT and the Omega Library are freely available via anonymous ftp from ftp://ftp.cs.umd.edu/pub/omega/omega system.
Reference: [Pug94] <author> William Pugh. </author> <title> Counting solutions to presburger formulas: How and why. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Using dimensionality allows me to obtain accurate indications of the relative volumes of different inter-processor communications without having to resort to complex and expensive symbolic volume estimation algorithms <ref> [Pug94] </ref>. Dimensionality, however, only provides an asymptotic indication of the amount of communication that will result. It may, for example, indicate that O (n 2 ) or O (n 3 ) communication is required. It can not distinguish between, for example, 2n 2 and 3n 2 communication.
Reference: [PW92] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year> <note> An earlier version of this paper appeared at the SIGPLAN PLDI'92 conference. 155 </note>
Reference-contexts: The input tuple sets must be convex (as my iteration spaces always are) and the result (by definition of the hull operation) will also be convex. A tuple set is convex if it can be represented as a conjunction of affine equality and inequality constraints. The gist operator <ref> [PW92, Won95] </ref> is used to simplify a set of constraints given that some other set of constraints is known to be true.
Reference: [PW93] <author> William Pugh and David Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Lecture Notes in Computer Science 768: Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference-contexts: I obtain accurate indications of the relative volumes of different inter-processor communications by computing the dimensionality of 41 value-based flow dependence relations <ref> [PW93] </ref> (an abstraction that precisely describes which iterations actually read values written by which other iterations). This allows me to analyze communication costs without knowing the order of the computations. 5. <p> Value-based flow dependence relations <ref> [PW93] </ref> are used to obtain accurate indications of the relative volumes of different inter-processor communications. I always use value based flow dependence relations in this section regardless of the abstraction chosen in the previous section.
Reference: [PW94a] <author> William Pugh and David Wonnacott. </author> <title> Experiences with constraint-based array dependence analysis. </title> <booktitle> In Principles and Practice of Constraint Programming Workshop, </booktitle> <address> Orcas Island, Washington, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: However, the constraints I have seen in both dependence analysis and code generation are quite sparse, and Fourier elimination is quite efficient for sparse constraints <ref> [PW94a] </ref>. Collard, Feautrier and Risset [CFR93] show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the single mapping case. Collard and Feautrier [CF93] address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated.
Reference: [PW94b] <author> William Pugh and David Wonnacott. </author> <title> Static analysis of upper and lower bounds on dependences and parallelism. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(3) </volume> <pages> 1248-1278, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Rather than creating flow, output and anti dependences between these references to var, I instead create a new type of data dependence called a reduction dependence between these references <ref> [Wol82, PW94b] </ref>. Reduction dependences have a different semantics from other types of data dependences. Rather than specifying that one operation must be performed after some other operation, they specify that a set of operations can be performed in any order. Reduction operations are ignored when inserting synchronization.
Reference: [SH91] <author> Jaswinder Pal Singh and John L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Of course, the problem with this approach is that converting sequential programs into efficient parallel programs is a very difficult task. So, not surprisingly, existing parallelizing compilers for massively parallel machines often fail to produce satisfactory results <ref> [Blu92, SH91] </ref>. There are two basic choices to be made when parallelizing a program. First, the computations of the program need to be distributed amongst the set of available processors. Second, the computations on each processor need to be ordered.
Reference: [SSP + 95] <author> T. J. She*er, R. Schreiber, W. Pugh, J. R. Gilbert, and S. Chatterjee. </author> <title> Efficient distribution analysis via graph contraction. </title> <booktitle> In Eighth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Generate Code: Generate new loop structures using time mappings Generate SPMD code using space mappings Insert synchronization for inter-processor dependences 40 3.2 Selecting Space Mappings 3.2.1 Introduction The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93a, BKK93, GAL95, SSP + 95] </ref>. My work improves on most previous work in the following ways: 1. I am not influenced by the order of the computation of the original program.
Reference: [ST92] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: They do not consider transformations such as loop distribution or interchange and do not consider pipelined parallelism nor the differences in synchronization costs between different candidate loops. 5.2 Ordering Computation The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as my work, in that it attempts to provide a unified framework for describing loop transformations. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. Unimodular transformations are combined with blocking in <ref> [WL91, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89b]. McKinley [McK92] and Wolf [Wol92] have both developed models to estimate data locality and parallelism. Both papers highlight the importance of the tradeoff between parallelism and locality.
Reference: [WB87] <author> Michael Wolfe and Utpal Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> Internation J. Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: I have found that this method is infeasible for analyzing large programs. 3.2.3.2 A faster but less accurate method My other approach is to use extended direction vectors <ref> [WB87, Wol91a] </ref> to represent dependences.
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91] </ref> go some way towards solving this problem. Unimodular transformations are able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. Such a transformation is described by a unimodular linear mapping from the original iteration space to a new iteration space. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91] </ref>. <p> This simplification is justifiable because self reuse can reduce the number of cache misses by a factor of (n), whereas group reuse can only reduce the number of cache misses by a factor of O (1). I do, however, take into account both temporal and spatial locality <ref> [WL91] </ref>. <p> They do not consider transformations such as loop distribution or interchange and do not consider pipelined parallelism nor the differences in synchronization costs between different candidate loops. 5.2 Ordering Computation The framework of Unimodular transformations <ref> [Ban90, WL91, ST92, KKB92] </ref> has the same goal as my work, in that it attempts to provide a unified framework for describing loop transformations. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. Unimodular transformations are combined with blocking in <ref> [WL91, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89b]. McKinley [McK92] and Wolf [Wol92] have both developed models to estimate data locality and parallelism. Both papers highlight the importance of the tradeoff between parallelism and locality.
Reference: [Wol82] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Rather than creating flow, output and anti dependences between these references to var, I instead create a new type of data dependence called a reduction dependence between these references <ref> [Wol82, PW94b] </ref>. Reduction dependences have a different semantics from other types of data dependences. Rather than specifying that one operation must be performed after some other operation, they specify that a set of operations can be performed in any order. Reduction operations are ignored when inserting synchronization.
Reference: [Wol89a] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Approaches taken to the second of these tasks have traditionally been 2 very ad-hoc. Specifically, parallelizing compilers have attempted to parallelize programs and improve their performance by applying a sequence of source to source transformations, such as loop interchange, loop skewing and loop distribution <ref> [Wol89a] </ref>. Each of these transformations has its own legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without actually performing the transformations and analyzing the resulting code.
Reference: [Wol89b] <author> Michael Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year> <month> 156 </month>
Reference-contexts: Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. Unimodular transformations are combined with blocking in [WL91, ST92]. A similar approach, although not using a unimodular framework, is described in <ref> [Wol89b] </ref>. McKinley [McK92] and Wolf [Wol92] have both developed models to estimate data locality and parallelism. Both papers highlight the importance of the tradeoff between parallelism and locality. Wolf's system is based on unimodular transformations while McKinley's system uses traditional transformations.
Reference: [Wol89c] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89c, Wol90, CK92] </ref>. Figures 2.5 through 2.8 show an example of such a sequence of transformations (black dots correspond to iterations of the first statement and white dots correspond to iterations of the second statement). Each of these transformations has its own special legality checks and transformation rules. <p> j = 1 to i-1 3 b (i) = b (i) - s (i) Iteration space: I 1 : f [ i ] j 1 i n g I 3 : f [ i ] j 1 i n g transformations such as loop fusion, loop distribution and statement reordering <ref> [Wol89c] </ref>. 2.3.2 My New Abstraction Before describing the abstraction I use to represent the ordering of computations, I first need to define some terms. 2.3.2.1 Iteration spaces Each statement p has associated with it an iteration space I p , which is a subspace of Z n p (where n p
Reference: [Wol90] <author> Michael Wolfe. </author> <title> Massive parallelism through program restructuring. </title> <booktitle> In Symposium on Frontiers on Massively Parallel Computation, </booktitle> <pages> pages 407-415, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89c, Wol90, CK92] </ref>. Figures 2.5 through 2.8 show an example of such a sequence of transformations (black dots correspond to iterations of the first statement and white dots correspond to iterations of the second statement). Each of these transformations has its own special legality checks and transformation rules.
Reference: [Wol91a] <author> Michael Wolfe. </author> <title> Experiences with data dependence abstractions. </title> <booktitle> In Proc. of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 321-329, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: I have found that this method is infeasible for analyzing large programs. 3.2.3.2 A faster but less accurate method My other approach is to use extended direction vectors <ref> [WB87, Wol91a] </ref> to represent dependences.
Reference: [Wol91b] <author> Michael Wolfe. </author> <title> The tiny loop restructuring research tool. </title> <booktitle> In Proc of 1991 International Conference on Parallel Processing, </booktitle> <address> pages II-46 - II-53, </address> <year> 1991. </year>
Reference-contexts: PETIT is based on Michael Wolfe's Tiny tool <ref> [Wol91b] </ref> for loop restructuring research and has been extended over the last five years by myself and other members of the Omega Project (see http://www.cs.umd.edu/projects/omega for more information). PETIT accepts as input, programs written in a toy language called Tiny.
Reference: [Wol92] <author> Michael Edward Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford U., </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Li and Pingali use a technique called access normalization to improve data locality and reduce communication on non-uniform memory access computers. Unimodular transformations are combined with blocking in [WL91, ST92]. A similar approach, although not using a unimodular framework, is described in [Wol89b]. McKinley [McK92] and Wolf <ref> [Wol92] </ref> have both developed models to estimate data locality and parallelism. Both papers highlight the importance of the tradeoff between parallelism and locality. Wolf's system is based on unimodular transformations while McKinley's system uses traditional transformations. McKinley determines the preferred loop order for each loop nest based on data locality. <p> The importance of exploiting locality between different iterations of the sequential loops that surround the parallel loops is discussed but not solutions are proposed. The data locality models used by McKinley [McK92], Wolf <ref> [Wol92] </ref> and Li [Li95] differ from mine in two major respects. I assume that a data item will only remain in the cache during some period of time if it is reused every iteration during that time period.
Reference: [Won95] <author> David G. Wonnacott. </author> <title> Constraint-Based Array Data Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, The University of Maryland, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: The input tuple sets must be convex (as my iteration spaces always are) and the result (by definition of the hull operation) will also be convex. A tuple set is convex if it can be represented as a conjunction of affine equality and inequality constraints. The gist operator <ref> [PW92, Won95] </ref> is used to simplify a set of constraints given that some other set of constraints is known to be true. <p> Tiny programs consist of a single procedure containing for loops and assignment statements involving array expressions. In addition to the transformation system described here, PETIT has been extended to include array expansion, induction variable recognition and advanced dependence analysis using the Omega Test <ref> [Pug92, Won95] </ref>. All of these extensions have been implemented using the Omega Library [KMP + 95] which is set of C++ classes for representing and manipulating tuple relations and sets. PETIT and the Omega Library are freely available via anonymous ftp from ftp://ftp.cs.umd.edu/pub/omega/omega system.
Reference: [ZC91] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1991. </year> <month> 157 </month>
Reference-contexts: via either doall and doacross loops or via a data distribution for each array. 2.2.1.1 Doall and Doacross Loops One way to specify the parallelism available in a program written in a sequential language is to annotate the do loops in that program as being either doall, doacross or dosequential <ref> [ZC91] </ref>. A doall loop indicates that all of the iterations of that loop are independent and therefore can be executed in any order (including in parallel). A doall loop does not indicate which iterations should be performed on which processors. <p> Such computation distributions are often desirable, especially for statements that are performing a reduction. A reduction is a set of operations that compute a scalar value from an array <ref> [ZC91] </ref>. For example, in the program shown in Figure 2.3, it would be best to assign iteration [i; j] to virtual processor j in order to perform the reduction in parallel.
References-found: 61

