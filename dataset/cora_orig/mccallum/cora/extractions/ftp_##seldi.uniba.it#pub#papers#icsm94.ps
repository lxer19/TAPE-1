URL: ftp://seldi.uniba.it/pub/papers/icsm94.ps
Refering-URL: http://www.cs.umd.edu/~lanubile/public.html
Root-URL: 
Title: An Experiment on the Effect of Design Recording on Impact Analysis differences in design recording
Author: F. Abbattista, F. Lanubile, G. Mastelloni, and G. Visaggio 
Note: 1: Introduction The objective of this paper is to investigate how  2:  work products of  
Address: Italy  
Affiliation: Dipartimento di Informatica University of Bari,  
Abstract: An experimental study is presented in which participants perform impact analysis on alternate forms of design record information. The primary objective of the research is to assess the maintainer performance with respect to various approaches of design recording. Among the approaches there is the model dependency descriptor which includes decision capturing and explicit traceability links between software objects and decisions. Results indicate that design recording approaches slightly differ in work completeness and time to finish but the model dependency descriptor leads to an impact analysis which is the most accurate. These results suggest that design records have the potential to be effective for software maintenance but training and process discipline is needed to make design recording worthwhile. The need to find better ways to collect information on development and maintenance activities is essential for the software evolution. The concept of design record is the answer to the information gap that maintainers suffer in doing their work. In this sense, we adopt the working definition of design recording, given in [1], where a design record is defined as a collection of information with the purpose to support activities following the development phase. Everybody advocate to collect information about software products and software processes as they evolve. However, as in the past for the structured programming and structured design, little empirical research has been done to show that the availability of design records makes easier the maintenance work and produces more reliable results. performance, and more specifically if decision capture and traceability support result in measurable improvements in maintenance performance. To isolate only those maintenance aspects which are important with respect to the evaluation goal of this study, we focused on the impact analysis as one phase of the maintenance process. Impact analysis is the activity of identifying what to modify to accomplish a change, or of identifying the potential consequences of a change [2]. In our software maintenance model, impact analysis starts when a programmer is given a change request and finish with identifying which modules change. Next section briefly describes the approaches to design recording which have been compared. Section 3 presents the experiment and section 4 shows the results. Last section suggests some conclusions and indicates issues for future research. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> R.S.Arnold, M.Slovin, and N.Wilde, </author> <title> "Do design records really benefit software maintenance?", </title> <booktitle> Proceedings of Conference on Software Maintenance 93, </booktitle> <address> Montreal, Canada, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993, </year> <month> pp.234-243. </month>
Reference-contexts: The concept of design record is the answer to the information gap that maintainers suffer in doing their work. In this sense, we adopt the working definition of design recording, given in <ref> [1] </ref>, where a design record is defined as a collection of information with the purpose to support activities following the development phase. Everybody advocate to collect information about software products and software processes as they evolve.
Reference: [2] <author> R.S.Arnold, S.A.Bohner, </author> <title> "Impact analysis - towards a framework for comparison", </title> <booktitle> Proceedings of Conference on Software Maintenance 93 , Montreal, </booktitle> <address> Canada, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993, </year> <month> pp.292-301. </month>
Reference-contexts: Impact analysis is the activity of identifying what to modify to accomplish a change, or of identifying the potential consequences of a change <ref> [2] </ref>. In our software maintenance model, impact analysis starts when a programmer is given a change request and finish with identifying which modules change. Next section briefly describes the approaches to design recording which have been compared. Section 3 presents the experiment and section 4 shows the results.
Reference: [3] <author> V.R.Basili, </author> <title> "Viewing maintenance as reuse-oriented software development", </title> <note> IEEE Software , January 1990, pp.19-25. </note>
Reference-contexts: As most maintainers in the real world, our students have become skilled in taking the source code and make the necessary changes first to the code and later, but not always, to the accompanying documentation. For this approach, called quick-fix model in <ref> [3] </ref>, the added design record information is not a help but a weight to suffer. Intensive training and a careful process discipline are needed to change the old habits.
Reference: [4] <author> R.Brooks, </author> <title> "Studying programmer behavior experimentally: the problems of proper methodology, </title> <journal> Communications of the ACM, vol.23, </journal> <volume> n.4, </volume> <year> 1980, </year> <month> pp.207-213. </month>
Reference-contexts: We did not use software programming professionals because we could not involve enough subjects for handling the high heterogeneity which characterize the programming behavior. Since using few subjects there is a risk of obscuring the experiment with subject differences <ref> [4] </ref>, we preferred to use students as subjects of this experiment. All students had gained experience with Structured Analysis/Structured Design and Pascal programming from a practical software engineering project in a previous university course.
Reference: [5] <author> A.Cimitile, F.Lanubile, G.Visaggio, </author> <title> "Traceability based on design decisions", </title> <booktitle> Proceedings of Conference on Software Maintenance 92 , Orlando, </booktitle> <address> Florida, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1992, </year> <month> pp.309-317. </month>
Reference-contexts: The latter is the lack of rationales behind the choices of the original developers. An alternative approach may be a system which stores decisions and traceability links to navigate into the documentation and rationales. In <ref> [5] </ref>, a design recording model, called model dependency descriptor , was presented to support software maintenance. The model dependency descriptor describes a software system as a web which integrates the different work products of software life cycle and their mutual interrelationships. Rationales are distinguished between design and implementation decisions.
Reference: [6] <author> V.R.Gibson, and J.A.Senn, </author> <title> "System structure and software maintenance performance", </title> <journal> Communications of the ACM , vol.32, </journal> <volume> no.3, </volume> <month> March </month> <year> 1989, </year> <month> pp.347-358. </month>
Reference-contexts: Subjective scores on system and task comprehension are generally optimistic and exhibit a uniform distribution over the design recording factor and the other independent variables. Although their interpretation is doubtful, we want to stress the analogy with results in <ref> [6] </ref> where structural differences between programs were not discernible to programmers. Code Std Doc MDD Task1 58 47 49 Task2 71 63 82 Task3 57 63 78 Overall 62 58 70 Table 6.
Reference: [7] <author> M.Shepperd, and D.Ince, </author> <title> "Design metrics and software maintainability: an experimental investigation", Software Maintenance: </title> <journal> Research and Practice , vol.3, </journal> <year> 1991, </year> <month> pp.215-232. </month>
Reference-contexts: Programmers were also encouraged to write comments about the experience. 4: Analysis of results Before proceeding to a statistical analysis, collected data were analyzed looking for doubtful data points. We used the criteria, already applied in <ref> [7] </ref>, of discarding data when subjects showed a misunderstanding of experiment instructions or an unawareness of basic programming concepts. The consequence was that 13 out of a total of 69 impact analyses were discarded as being too incomplete or inaccurate to produce reliable results.
Reference: [8] <author> B.J.Winer, </author> <title> Statistical principles in experimental design , Mc-Graw Hill Book Company, </title> <publisher> Inc., </publisher> <year> 1962. </year>
Reference-contexts: The primary purpose of this plan was to obtain complete within-subject information on all main effects. However, only partial information can be obtained on all the interaction effects because they are partially confounded with differences between groups. A detailed description of this type of design can be found in <ref> [8, p.566] </ref>. Programmer performance was measured by three variables: the completeness of the estimated impact the accuracy of the estimated impact the time taken to analyze the change impact Completeness and accuracy were measured by comparing the actual impact set and the estimated impact set.
References-found: 8

