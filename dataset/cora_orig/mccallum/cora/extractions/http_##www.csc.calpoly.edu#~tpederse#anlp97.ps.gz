URL: http://www.csc.calpoly.edu/~tpederse/anlp97.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: pedersen@seas.smu.edu, rbruce@seas.smu.edu, wiebe@cs.nmsu.edu  
Title: Sequential Model Selection for Word Sense Disambiguation  
Author: Ted Pederseny and Rebecca Brucey and Janyce Wiebez 
Address: Dallas, TX 75275  Las Cruces, NM 88003  
Affiliation: yDepartment of Computer Science and Engineering Southern Methodist University,  zDepartment of Computer Science New Mexico State University,  
Note: Appears in the Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP-97), April 1997, Washington, DC  
Abstract: Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Akaike. </author> <year> 1974. </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control, AC-19(6):716-723. </journal>
Reference-contexts: This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log-likelihood ratio statistic G 2 (Bishop et al., 1975), and two information criteria, Akaike's Information Criterion (AIC) <ref> (Akaike, 1974) </ref> and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G 2 is defined as: G 2 = 2 fi i=1 f i (2) where f i and e i are the observed and expected counts of the i th feature vector, respectively.
Reference: <author> J. Badsberg. </author> <year> 1995. </year> <title> An Environment for Graphical Models. </title> <type> Ph.D. thesis, </type> <institution> Aalborg University. </institution>
Reference-contexts: However, if some features are of questionable value the Naive Bayes model will continue to utilize them while sequential model selection will disregard them. All of the search strategies and evaluation criteria discussed are implemented in the public domain program CoCo <ref> (Badsberg, 1995) </ref>.
Reference: <author> A. Berger, S. Della Pietra, and V. Della Pietra. </author> <year> 1996. </year> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71. </pages>
Reference-contexts: Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; <ref> (Berger et al., 1996) </ref>). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990). <p> Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., <ref> (Berger et al., 1996) </ref>), but within this framework no systematic study of interactions has been proposed.
Reference: <author> Y. Bishop, S. Fienberg, and P. Holland. </author> <year> 1975. </year> <title> Discrete Multivariate Analysis. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word-sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models <ref> (Bishop et al., 1975) </ref>. Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. <p> This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log-likelihood ratio statistic G 2 <ref> (Bishop et al., 1975) </ref>, and two information criteria, Akaike's Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G 2 is defined as: G 2 = 2 fi i=1 f i (2) where f i and e i are the
Reference: <author> E. Black. </author> <year> 1988. </year> <title> An experiment in computational discrimination of English word senses. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 32(2) </volume> <pages> 185-194. </pages>
Reference-contexts: Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. <ref> (Black, 1988) </ref> and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. 5 They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power.
Reference: <author> P. Brown, S. Della Pietra, and R. Mercer. </author> <year> 1991. </year> <title> Word sense disambiguation using statistical methods. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 264-304. </pages>
Reference-contexts: In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. 5 Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., <ref> (Brown et al., 1991) </ref>, (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation).
Reference: <author> R. Bruce and J. Wiebe. </author> <year> 1994a. </year> <title> A new approach to word sense disambiguation. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> pages 244-249. </pages>
Reference-contexts: However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., <ref> (Bruce and Wiebe, 1994a) </ref>, (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)).
Reference: <author> R. Bruce and J. Wiebe. </author> <year> 1994b. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 139-146. </pages>
Reference-contexts: Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties <ref> (Bruce and Wiebe, 1994b) </ref>: 1. <p> In order to utilize models with more complicated interactions among feature variables, <ref> (Bruce and Wiebe, 1994b) </ref> introduce the use of sequential model selection and decomposable models for word-sense disambiguation. 5 Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to
Reference: <author> R. Bruce, J. Wiebe, and T. Pedersen. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 101-112. </pages>
Reference-contexts: The search stops when the IC values for all hypothesized models are greater than zero in the case of BSS, or less than zero in the case of FSS. 5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in <ref> (Bruce et al., 1996) </ref>. The text consists of every sentence from the ACL/DCI Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common.
Reference: <author> I. Dagan, A. Itai, and U. Schwall. </author> <year> 1991. </year> <title> Two languages are more informative than one. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 130-137. </pages>
Reference-contexts: In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. 5 Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), <ref> (Dagan et al., 1991) </ref>, and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed.
Reference: <author> J. Darroch, S. Lauritzen, and T. Speed. </author> <year> 1980. </year> <title> Markov fields and log-linear interaction models for contingency tables. </title> <journal> The Annals of Statistics, </journal> <volume> 8(3) </volume> <pages> 522-539. </pages>
Reference-contexts: This paper presents the results of a comparative study of search strategies and evaluation criteria for measuring model fit. We restrict the selection process to the class of decomposable models <ref> (Darroch et al., 1980) </ref>, since restricting model search to this class has many computational advantages. We begin with a short description of decomposable models (in section 2).
Reference: <author> W. Gale, K. Church, and D. Yarowsky. </author> <year> 1992. </year> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439. </pages>
Reference-contexts: However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), <ref> (Gale et al., 1992) </ref>, (Leacock et al., 1993), and (Mooney, 1996)).
Reference: <author> S. Kreiner. </author> <year> 1987. </year> <title> Analysis of multidimensional contingency tables by exact conditional tests: Techniques and strategies. </title> <journal> Scandinavian Journal of Statistics, </journal> <volume> 14 </volume> <pages> 97-112. </pages>
Reference-contexts: This paper considers two significance tests, the exact conditional test <ref> (Kreiner, 1987) </ref> and the Log-likelihood ratio statistic G 2 (Bishop et al., 1975), and two information criteria, Akaike's Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G 2 is defined as: G 2 = 2 fi i=1 f i
Reference: <author> C. Leacock, G. Towell, and E. Voorhees. </author> <year> 1993. </year> <title> Corpus-based statistical sense resolution. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology. </booktitle>
Reference-contexts: However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), <ref> (Leacock et al., 1993) </ref>, and (Mooney, 1996)).
Reference: <author> R. Mooney. </author> <year> 1996. </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing. </booktitle>
Reference-contexts: However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and <ref> (Mooney, 1996) </ref>). <p> Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and <ref> (Mooney, 1996) </ref>) but, while it is a type of model selection, the models are not parametric. 5 They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power.
Reference: <author> H.T. Ng and H.B. Lee. </author> <year> 1996. </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, </booktitle> <pages> pages 40-47. </pages>
Reference-contexts: R 2 ) the ambiguous word. (3) The three binary collocation-specific features (C 1 ; C 2 ; C 3 ) indicate if a particular word occurs in a sentence with an ambiguous word. 2 An alternative feature set for this data is utilized with an exemplar-based learning algorithm in <ref> (Ng and Lee, 1996) </ref>. The sparse nature of our data can be illustrated by interest. There are 6 possible values for the sense variable. Combined with the other feature variables this results in 37,500,000 possible feature vectors (or joint parameters).
Reference: <author> T. Pedersen, M. Kayaalp, and R. Bruce. </author> <year> 1996. </year> <title> Significant lexical relationships. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <pages> pages 455-460. </pages>
Reference-contexts: For these estimates to be reliable, each of the q possible combinations of feature values must occur in the training sample. This is unlikely for NLP data samples, which are often sparse and highly skewed (c.f., e.g. <ref> (Pedersen et al., 1996) </ref> and (Zipf, 1935)). <p> The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples <ref> (Pedersen et al., 1996) </ref>. 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: IC = G 2 fi dof (3) where G 2 and dof are defined above. Members of this family are distinguished by their different values of .
Reference: <author> G. Schwarz. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464. </pages>
Reference-contexts: This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log-likelihood ratio statistic G 2 (Bishop et al., 1975), and two information criteria, Akaike's Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) <ref> (Schwarz, 1978) </ref>. 4.1 Significance tests The Log-likelihood ratio statistic G 2 is defined as: G 2 = 2 fi i=1 f i (2) where f i and e i are the observed and expected counts of the i th feature vector, respectively.
Reference: <author> J. Whittaker. </author> <year> 1990. </year> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference-contexts: We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word-sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models <ref> (Whittaker, 1990) </ref> which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. <p> Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power <ref> (Whittaker, 1990) </ref>.
Reference: <author> D. Yarowsky. </author> <year> 1993. </year> <title> One sense per collocation. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> pages 266-271. </pages>
Reference-contexts: utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. 5 Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and <ref> (Yarowsky, 1993) </ref> present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed.

References-found: 20

