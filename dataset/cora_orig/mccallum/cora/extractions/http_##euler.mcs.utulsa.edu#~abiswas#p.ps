URL: http://euler.mcs.utulsa.edu/~abiswas/p.ps
Refering-URL: http://euler.mcs.utulsa.edu/~abiswas/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Modeling agent decisions with a family of orthogonal polynomials  
Keyword: Content Areas: multiagent systems, coordination  
Abstract: Agents in a multiagent system (MAS) can benefit by modeling other agents in their environment. In particular, being able to predict decisions to be taken by other agents enables an agent to improve its own utility. In this paper, we present a learning mechanism by which an agent can approximately model the decision function used by another agent given a collection of decisions made by that agent. We consider the case of binary decisions based on a single decision variable. A polynomial time, incremental algorithm to adjust the coefficients of a family of orthogonal functions, Chebychev polynomials, is presented which can be used to develop a model of other agent's decision function. We prove that, in the limit, this algorithm is guaranteed to produce an accurate model. Approximations of several interesting decision functions are presented to show the effects of limited sampling. An interesting property of the algorithm observed from the experiments is that increasing the resolution of sampling can provide better approximations without increasing the number of samples. This property clearly differentiate our approach from commonly used regression techniques. We also present an application of this modeling tech nique in a multiagent load balancing problem. Abstract ID: A493 Statement of sole submission: "The authors certify that this paper has not been accepted by and is not currently under review for another conference or journal. Nor will it be submitted for such during AAAI-98's review period." 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boddy, M., and Dean, T. </author> <year> 1994. </year> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <booktitle> Artificial Intelligence 67 </booktitle> <pages> 245-285. </pages>
Reference: <author> Bui, H. H.; Kieronska, D.; and Venkatesh, S. </author> <year> 1996. </year> <title> Learning other agents' preferences in multiagent negotiation. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 114-119. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: They then proceed to derive an optimal interaction strategy with that opponent. The work presented in this paper can develop models of more complex decision policies. Bui et al. <ref> (Bui, Kieron-ska, & Venkatesh 1996) </ref> uses past responses about meeting scheduling requests to develop a probability distribution of open time slots in the calendar of other agents.
Reference: <author> Carmel, D., and Markovitch, S. </author> <year> 1996a. </year> <title> Incorporating opponent models into adversary search. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 120-125. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: The latter approach is, for example, adopted by game playing agents that can learn the strategy being used by its opponent <ref> (Carmel & Markovitch 1996a) </ref>. Observation-based coordination is a very attractive and plausible approach to developing coherent behavior in an MAS which alleviates the need for explicit communication between agents (Huber & Durfee 1996).
Reference: <author> Carmel, D., and Markovitch, S. </author> <year> 1996b. </year> <title> Learning models of intelligent agents. </title> <booktitle> In Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 62-67. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Related Work Carmel and Markovitch have used restrictive assumptions about the behavioral strategies being used by other agents to derive a simple finite automata model of another agent from its input/output behavior <ref> (Carmel & Markovitch 1996b) </ref>. They then proceed to derive an optimal interaction strategy with that opponent. The work presented in this paper can develop models of more complex decision policies.
Reference: <author> Curtis F. Gerald, P. O. W. </author> <year> 1992. </year> <title> Applied Numerical Analysis. </title> <address> Menlo Park, CA: </address> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference-contexts: We can, however, truncate the above series and still obtain an approximation, ^ f (x), of the function <ref> (Curtis F. Gerald 1992) </ref>. The Chebyshev polynomials converges faster than the Taylor series for the same function. Let us assume we are using only the first n terms of the series.
Reference: <author> Geronimus, L. Y. </author> <year> 1961. </year> <title> Orthogonal Polynomials. </title> <address> New York, NY: </address> <publisher> Consultants Bureau. </publisher>
Reference-contexts: Chebychev polynomials Chebychev polynomials are a family of orthogonal polynomials <ref> (Geronimus 1961) </ref>.
Reference: <author> Gmytrasiewicz, P. J., and Durfee, E. H. </author> <year> 1995. </year> <title> A rigorous, operational formalization of recursive modeling. </title> <booktitle> In First International Conference on Multiagent Systems, </booktitle> <pages> 125-132. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: An important distinction here is that they are modeling primarily the internal state of the other agent than the decision policy being used by that agent. Gmytrasiewicz and Durfee presents a decision-theoretic approach to updating recursive models of other agents <ref> (Gmytrasiewicz & Dur-fee 1995) </ref>. Their model updating procedure, however, is based more on assumptions of rationality of the other agent and is not dependent on observed behavior of other agents.
Reference: <author> Hansen, E. A., and Zilberstein, S. </author> <year> 1996. </year> <title> Monitoring the progress of anytime problem-solving. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 1229-1234. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Huber, M., and Durfee, E. H. </author> <year> 1996. </year> <title> An initial assessment of plan-recognition-based coordination for multi-agent teams. </title> <booktitle> In Proceedings of the Second International Conference on Multiagent Systems, </booktitle> <pages> 126-133. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Observation-based coordination is a very attractive and plausible approach to developing coherent behavior in an MAS which alleviates the need for explicit communication between agents <ref> (Huber & Durfee 1996) </ref>. An observation-based plan-recognition approach uses general domain information and observed actions of another agent to predict future plans of that agent. This may not necessarily lead to a modification of the model of the other agent.
Reference: <author> Schaerf, A.; Shoham, Y.; and Tennenholtz, M. </author> <year> 1995. </year> <title> Adaptive load balancing: A study in multiagent learning. </title> <journal> Journal of Artificial Intelligence Research 2 </journal> <pages> 475-500. </pages>
Reference-contexts: The knowledge acquired by these agents if more of a threshold of acceptance rather than a general decision procedure over the range of input values. A slightly different load balancing prob lem has been investigated by Schaerf et al. <ref> (Schaerf, Shoham, & Tennenholtz 1995) </ref>, who use reinforcement learning agents that learn resource responses and not models of how others load resources. Conclusion In this paper, we have developed an incremental modeling approach that can use past decisions about an agent to approximate its probabilistic decision function.
Reference: <author> Zeng, D., and Sycara, K. </author> <year> 1997. </year> <title> Benefits of learning in negotiation. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 36-41. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Their model updating procedure, however, is based more on assumptions of rationality of the other agent and is not dependent on observed behavior of other agents. Zeng and Sycara present a learning mechanism by which agents can learn about payoff structures of other agents in a sequential negotiation framework <ref> (Zeng & Sycara 1997) </ref>. The knowledge acquired by these agents if more of a threshold of acceptance rather than a general decision procedure over the range of input values.
References-found: 11

