URL: http://L2R.cs.uiuc.edu/~danr/Papers/spellJ.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: golding@merl.com  danr@cs.uiuc.edu  
Title: A Winnow-Based Approach to Context-Sensitive Spelling Correction  
Author: ANDREW R. GOLDING DAN ROTH Editor: Raymond J. Mooney and Claire Cardie 
Keyword: Winnow, multiplicative weight-update algorithms, context-sensitive spelling correction, Bayesian classifiers  
Address: 201 Broadway, Cambridge, MA 02139  Urbana/Champaign, 1304 W. Spring-field Avenue, Urbana, IL 61801  
Affiliation: MERL A Mitsubishi Electric Research Laboratory,  Department of Computer Science, University of Illinois  
Note: 1-25 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts depend on only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. In the work reported here, we present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting to for too, casual for causal, and so on. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) While several aspects of WinSpell's architecture contribute to its superiority over BaySpell, the primary factor is that it is able to learn a better linear separator than BaySpell learns; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blum, A. </author> <year> (1992). </year> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386. </pages>
Reference-contexts: The use of a sparse architecture, as described above, coupled with the representation of each example as a list of active features is reminiscent of the infinite attribute models of Winnow <ref> (Blum, 1992) </ref>. 4.2. Weighted Majority Rather than evaluating the evidence for a given word W i using a single classifier, WinSpell combines evidence from multiple classifiers; the motivation for doing so is discussed below. Weighted Majority (Littlestone and Warmuth, 1994) is used to do the combination. <p> the variables from the beginning, but rather add variables as necessary, the number of mistakes made on disjunctions and conjunctions is logarithmic in the size of the largest example seen and linear in the number of relevant attributes; it is independent of the total number of attributes in the domain <ref> (Blum, 1992) </ref>. Winnow was analyzed in the presence of various kinds of noise, and in cases where no linear threshold function can make perfect classifications (Littlestone, 1991).
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> University of California, Berkeley. </institution>
Reference: <author> Cesa-Bianchi, N., Freund, Y., Helmbold, D. P., and Warmuth, M. </author> <year> (1994). </year> <title> On-line prediction and conversion strategies. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> Eurocolt '93, </volume> <pages> pages 205-216. </pages> <publisher> Oxford University Press. </publisher>
Reference: <author> Chen, S. F. and Goodman, J. </author> <year> (1996). </year> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: Instead, BaySpell performs smoothing by interpolating between the MLE of P (f jW i ) and the MLE of the unigram probability, P (f ). Some means of incorporating a lower-order model in this way is generally regarded as essential for good performance <ref> (Chen and Goodman, 1996) </ref>.
Reference: <author> Cortes, C. and Vapnik, V. </author> <year> (1995). </year> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297. </pages>
Reference-contexts: A similar philosophy, albeit very different technically, is followed by the work on Support Vector Machines <ref> (Cortes and Vapnik, 1995) </ref>. Theoretical analysis has shown Winnow to be able to adapt quickly to a changing target concept (Herbster and Warmuth, 1995). We investigate this issue experimentally in Section 5.5. <p> The use of this strategy in Winnow shares much the same philosophy | if none of the technical underpinnings | as Support Vector Machines <ref> (Cortes and Vapnik, 1995) </ref>. Second, the two-layer architecture used here is related to various voting and boosting techniques studied in recent years in the learning community (Freund and Schapire, 1995; Breiman, 1994; Littlestone and Warmuth, 1994).
Reference: <author> Dagan, I., Karov, Y., and Roth, D. </author> <year> (1997). </year> <title> Mistake-driven learning in text categorization. </title> <booktitle> In EMNLP-97, The Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 55-63. </pages>
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation. </title> <note> To appear. </note>
Reference-contexts: Results are reported as a percentage of correct classifications on each confusion set, as well as an overall score, which gives the percentage correct for all confusion sets pooled together. When comparing scores, we tested for significance using a McNemar test <ref> (Dietterich, 1998) </ref> when possible; when data on individual trials was not available (the system comparison), or the comparison was across different test sets (the within/across study), we instead used a test for the difference of two proportions (Fleiss, 1981). All tests are reported for the 0.05 significance level. 5.2.
Reference: <author> Domingos, P. and Pazzani, M. </author> <year> (1997). </year> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130. </pages>
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference: <author> Fleiss, J. L. </author> <year> (1981). </year> <title> Statistical Methods for Rates and Proportions. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: When comparing scores, we tested for significance using a McNemar test (Dietterich, 1998) when possible; when data on individual trials was not available (the system comparison), or the comparison was across different test sets (the within/across study), we instead used a test for the difference of two proportions <ref> (Fleiss, 1981) </ref>. All tests are reported for the 0.05 significance level. 5.2. Pruned versus unpruned The first step of the evaluation was to test WinSpell under the same conditions that BaySpell normally runs under | i.e., using the pruned set of features from the feature extractor.
Reference: <author> Flexner, S. B., </author> <title> editor (1983). Random House Unabridged Dictionary. Random House, </title> <address> New York. </address> <note> Second edition. </note>
Reference-contexts: Acquiring confusion sets is an interesting problem in its own right; in the work reported here, however, we take our confusion sets largely from the list of "Words Commonly Confused" in the back of the Random House dictionary <ref> (Flexner, 1983) </ref>, which includes mainly homophone errors. A few confusion sets not in Random House were added, representing grammatical and typographic errors. <p> The algorithms were run on 21 confusion sets, which were taken largely from the list of "Words Commonly Confused" in the back of the Random House dictionary <ref> (Flexner, 1983) </ref>. The confusion sets were selected on the basis of being frequently-occurring in Brown and WSJ, and include mainly homophone confusions (e.g., fpeace; pieceg). Several confusion sets not in Random House were added, representing grammatical errors (e.g., famong ; betweeng) and typographic errors (e.g., fmaybe; may beg).
Reference: <author> Freund, Y. and Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> Eurocolt '95, </volume> <pages> pages 23-37. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Gale, W. A., Church, K. W., and Yarowsky, D. </author> <year> (1993). </year> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439. </pages>
Reference-contexts: The problem has started receiving attention in the literature only within about the last half-dozen years. A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers <ref> (Gale et al., 1993) </ref>, decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Golding, A. R. </author> <year> (1995). </year> <title> A Bayesian hybrid method for context-sensitive spelling correction. </title> <booktitle> In Proc. 3rd Workshop on Very Large Corpora, </booktitle> <address> Boston, MA. </address>
Reference-contexts: Context-sensitive spelling correction therefore fits the characterization presented above, and provides an excellent testbed for studying the performance of multiplicative weight-update algorithms on a real-world task. To evaluate the proposed Winnow-based algorithm, WinSpell, we compare it against BaySpell <ref> (Golding, 1995) </ref>, a statistics-based method that is among the most successful tried for the problem. We first compare WinSpell and BaySpell using the heavily-pruned feature set that BaySpell normally uses (typically 10-1000 features). WinSpell is found to perform comparably to BaySpell under this condition. <p> A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids <ref> (Golding, 1995) </ref>, a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> Bayesian approach Of the various approaches that have been tried for context-sensitive spelling correction, the Bayesian hybrid method, which we call BaySpell, has been among the most successful, and is thus the method we adopt here as the benchmark for comparison with WinSpell. BaySpell has been described elsewhere <ref> (Golding, 1995) </ref>, and so will only be briefly reviewed here; however, the version here uses an improved smoothing technique, which is described below.
Reference: <author> Golding, A. R. and Schabes, Y. </author> <year> (1996). </year> <title> Combining trigram-based and feature-based methods for context-sensitive spelling correction. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids <ref> (Golding and Schabes, 1996) </ref>, and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Herbster, M. and Warmuth, M. </author> <year> (1995). </year> <title> Tracking the best expert. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A similar philosophy, albeit very different technically, is followed by the work on Support Vector Machines (Cortes and Vapnik, 1995). Theoretical analysis has shown Winnow to be able to adapt quickly to a changing target concept <ref> (Herbster and Warmuth, 1995) </ref>. We investigate this issue experimentally in Section 5.5. A further feature of WinSpell is that it can prune poorly 12 performing attributes, whose weight falls too low relative to the highest weight of an attribute used by the classifier.
Reference: <author> Holte, R. C., Acker, L. E., and Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proc. International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit. </address> <note> 25 Jones, </note> <author> M. P. and Martin, J. H. </author> <year> (1997). </year> <title> Contextual spelling correction using latent semantic analysis. </title> <booktitle> In Proc. 5th Conference on Applied Natural Language Processing, </booktitle> <address> Washington, DC. </address>
Reference: <author> Katz, S. M. </author> <year> (1987). </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, ASSP-35(3):400-401. </journal>
Reference: <author> Kivinen, J. and Warmuth, M. K. </author> <year> (1995). </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <booktitle> In ACM Symp. on the Theory of Computing. </booktitle>
Reference: <author> Kneser, R. and Ney, H. </author> <year> (1995). </year> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proc. International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 181-184. </pages> <note> Vol. 1. </note>
Reference: <author> Kohavi, R., Becker, B., and Sommerfield, D. </author> <year> (1997). </year> <title> Improving simple Bayes. </title> <booktitle> In Proc. European Conference on Machine Learning. </booktitle>
Reference: <author> Kukich, K. </author> <year> (1992). </year> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439. </pages>
Reference-contexts: Context-sensitive spelling correction is the task of fixing spelling errors that result in valid words, such as I'd like a peace of cake, where peace was typed when piece was intended. These errors account for anywhere from 25% to over 50% of observed spelling errors <ref> (Kukich, 1992) </ref>; yet they go undetected by conventional spell checkers, such as Unix spell, which only flag words that are not found in a word list. Context-sensitive spelling correction involves learning to characterize the linguistic contexts in which different words, such as piece and peace, tend to occur.
Reference: <author> Kucera, H. and Francis, W. N. </author> <year> (1967). </year> <title> Computational Analysis of Present-Day American English. </title> <publisher> Brown University Press, </publisher> <address> Providence, RI. </address>
Reference-contexts: The sections below describe the experimental methodology, and present the experiments, interleaved with discussion. 5.1. Methodology In the experiments that follow, the training and test sets were drawn from two corpora: the 1-million-word Brown corpus <ref> (Kucera and Francis, 1967) </ref> and a 3/4 13 million-word corpus of articles from The Wall Street Journal (WSJ) (Marcus et al., 1993).
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: In this paper, we present a learning algorithm and an architecture with properties suitable for this class of problems. The algorithm builds on recently introduced theories of multiplicative weight-update algorithms. It combines variants of Winnow <ref> (Littlestone, 1988) </ref> and Weighted Majority (Littlestone and Warmuth, 1994). <p> The enhancement of smoothing, and to a minor extent, dependency resolution, greatly improve the performance of BaySpell over the naive Bayesian approach. (The effect of these enhancements can be seen empirically in Section 5.4.) 4. Winnow-based approach There are various ways to use a learning algorithm, such as Winnow <ref> (Littlestone, 1988) </ref>, to do the task of context-sensitive spelling correction. A straightforward approach would be to learn, for each confusion set, a discriminator that distinguishes specifically among the words in that set. <p> The outputs of the classifiers are combined into an output for the cloud using a variant of the Weighted Majority algorithm (Littlestone and Warmuth, 1994). Within each classifier, a variant of the Winnow algorithm <ref> (Littlestone, 1988) </ref> is used. Training occurs whenever the architecture interacts with the world, for example, by reading a sentence of text; the architecture thereby receives new values for its lower-level predicates, which in turn serve as an example for training the high-level ensembles of classifiers.
Reference: <author> Littlestone, N. </author> <year> (1991). </year> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-156. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Winnow was analyzed in the presence of various kinds of noise, and in cases where no linear threshold function can make perfect classifications <ref> (Littlestone, 1991) </ref>.
Reference: <author> Littlestone, N. </author> <year> (1995). </year> <title> Comparing several linear-threshold learning algorithms on tasks involving superfluous attributes. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 353-361. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This condition was recently investigated experimentally (on simulated data) <ref> (Littlestone, 1995) </ref>. It was shown that redundant attributes dramatically affect a Bayesian predictor, while superfluous independent attributes have a less dramatic effect, and only when the number of attributes is very large (on the order of 10,000). <p> This is crucial in the analysis of the algorithm and has been shown to be crucial empirically as well <ref> (Littlestone, 1995) </ref>. One of the advantages of the multiplicative update algorithms is their logarithmic dependence on the number of domain features. This property allows one to learn higher-than-linear discrimination functions by increasing the dimensionality of the feature space.
Reference: <author> Littlestone, N. and Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261. </pages>
Reference-contexts: In this paper, we present a learning algorithm and an architecture with properties suitable for this class of problems. The algorithm builds on recently introduced theories of multiplicative weight-update algorithms. It combines variants of Winnow (Littlestone, 1988) and Weighted Majority <ref> (Littlestone and Warmuth, 1994) </ref>. Extensive analysis of these algorithms in the COLT literature has shown them to have exceptionally good behavior in the presence of irrelevant attributes, noise, and even a target function changing in time (Littlestone, 1988; Littlestone and Warmuth, 1994; Herbster and Warmuth, 1995). <p> All classifiers within the cloud learn the cloud's 7 high-level concept autonomously, as a function of the same lower-level predicates, but with different values of the learning parameters. The outputs of the classifiers are combined into an output for the cloud using a variant of the Weighted Majority algorithm <ref> (Littlestone and Warmuth, 1994) </ref>. Within each classifier, a variant of the Winnow algorithm (Littlestone, 1988) is used. <p> Weighted Majority Rather than evaluating the evidence for a given word W i using a single classifier, WinSpell combines evidence from multiple classifiers; the motivation for doing so is discussed below. Weighted Majority <ref> (Littlestone and Warmuth, 1994) </ref> is used to do the combination. The basic approach is to run several classifiers in parallel within each cloud to try to predict whether W i belongs in the sentence. Each classifier uses different values of the learning parameters, and therefore makes slightly different predictions.
Reference: <author> Mangu, L. and Brill, E. </author> <year> (1997). </year> <title> Automatic rule acquisition for spelling correction. </title> <booktitle> In Proc. 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning <ref> (Mangu and Brill, 1997) </ref>, latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> Two recent methods use some of the same test sets as we do, and thus can readily be compared: RuleS, a transformation-based learner <ref> (Mangu and Brill, 1997) </ref>; and a method based on latent semantic analysis (LSA) (Jones and Martin, 1997). We also compare to Baseline, the canonical straw man for this task, which simply identifies the most common member of the confusion set during training, and guesses it every time during testing.
Reference: <author> Marcus, M. P., Santorini, B., and Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: Methodology In the experiments that follow, the training and test sets were drawn from two corpora: the 1-million-word Brown corpus (Kucera and Francis, 1967) and a 3/4 13 million-word corpus of articles from The Wall Street Journal (WSJ) <ref> (Marcus et al., 1993) </ref>. Note that no particular annotations are needed on these corpora for the task of context-sensitive spelling correction; we simply assume that the texts contain no context-sensitive spelling errors, and thus the observed spellings may be taken as a gold standard.
Reference: <author> Mays, E., Damerau, F. J., and Mercer, R. L. </author> <year> (1991). </year> <title> Context based spelling correction. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 27(5) </volume> <pages> 517-522. </pages>
Reference-contexts: The problem has started receiving attention in the literature only within about the last half-dozen years. A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams <ref> (Mays et al., 1991) </ref>, Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997).
Reference: <author> Ney, H., Essen, U., and Kneser, R. </author> <year> (1994). </year> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38. </pages>
Reference-contexts: Training was on 80% of Brown and testing on the other 20%. When using MLE likelihoods, we broke ties by choosing the word with the largest prior (ties arose when all words had probability 0.0). For Katz smoothing, we used absolute discounting <ref> (Ney et al., 1994) </ref>, as Good-Turing discounting resulted in invalid discounts for our task. For Kneser-Ney smoothing, we used absolute discounting and the backoff distribution based on the "marginal constraint".
Reference: <author> Ng, H. T. and Lee, H. B. </author> <year> (1996). </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proc. 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Santa Cruz, CA. </address>
Reference-contexts: The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids <ref> (Golding and Schabes, 1996) </ref>, and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> Collocations, in contrast, capture the local syntax around the target word. Similar combinations of features have been used in related tasks, such as accent restoration (Yarowsky, 1994) and word sense disambiguation <ref> (Ng and Lee, 1996) </ref>. We use a feature extractor to convert from the initial text representation of a sentence to a list of active features. The feature extractor has a preprocessing phase in which it learns a set of features for the task.
Reference: <author> Powers, D. </author> <year> (1997). </year> <title> Learning and application of differential grammars. </title> <booktitle> In Proc. Meeting of the ACL Special Interest Group in Natural Language Learning, </booktitle> <address> Madrid. </address>
Reference-contexts: trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars <ref> (Powers, 1997) </ref>.
Reference: <author> Reddy, L. and Tadepalli, P. </author> <year> (1997). </year> <title> Active learning with committees for text categorization. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 602-608. </pages>
Reference: <author> Roth, D. </author> <year> (1998). </year> <title> Learning to resolve natural language ambiguities: A unified approach. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 806-813. </pages>
Reference-contexts: We pursue an alternative approach: that of learning the contextual characteristics of each word W i individually. This learning can then be used to distinguish word W i from any other word, as well as to perform a broad spectrum of other natural language tasks <ref> (Roth, 1998) </ref>. In the following, we briefly present the general approach, and then concentrate on the task at hand, context-sensitive spelling correction. The approach developed is influenced by the Neuroidal system suggested by Valiant (1994). <p> Ablation Study The previous sections demonstrate the superiority of WinSpell over BaySpell for the task at hand, but they do not explain why the Winnow-based algorithm does better. At their core, WinSpell and BaySpell are both linear separators <ref> (Roth, 1998) </ref>; is it that Winnow, with its multiplicative update rule, is able to learn a better linear separator than the one given by Bayesian probability theory? Or is it that the non-Winnow enhancements of WinSpell, particularly weighted-majority 16 voting, provide most of the leverage? To address these questions, we ran
Reference: <author> Roth, D. and Zelenko, D. </author> <year> (1998). </year> <title> Part of speech tagging using a network of linear separators. </title> <booktitle> In COLING-ACL 98, The 17th International Conference on Computational Linguistics, </booktitle> <pages> pages 1136-1142. </pages>
Reference-contexts: We pursue an alternative approach: that of learning the contextual characteristics of each word W i individually. This learning can then be used to distinguish word W i from any other word, as well as to perform a broad spectrum of other natural language tasks <ref> (Roth, 1998) </ref>. In the following, we briefly present the general approach, and then concentrate on the task at hand, context-sensitive spelling correction. The approach developed is influenced by the Neuroidal system suggested by Valiant (1994). <p> Ablation Study The previous sections demonstrate the superiority of WinSpell over BaySpell for the task at hand, but they do not explain why the Winnow-based algorithm does better. At their core, WinSpell and BaySpell are both linear separators <ref> (Roth, 1998) </ref>; is it that Winnow, with its multiplicative update rule, is able to learn a better linear separator than the one given by Bayesian probability theory? Or is it that the non-Winnow enhancements of WinSpell, particularly weighted-majority 16 voting, provide most of the leverage? To address these questions, we ran
Reference: <author> Valiant, L. G. </author> <year> (1994). </year> <title> Circuits of the Mind. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Valiant, L. G. </author> <year> (1995). </year> <title> Rationality. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-14. </pages>
Reference-contexts: Learning is thus an on-line process that is done on a continuous basis 4 <ref> (Valiant, 1995) </ref>. correction, and in particular for correcting the words fdesert; dessert g. The bottom tier of the network consists of nodes for lower-level predicates, which in this application correspond to features of the domain. For clarity, only five nodes are shown; thousands typically occur in practice.
Reference: <author> Yarowsky, D. </author> <year> (1994). </year> <title> Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Las Cruces, NM. </address>
Reference-contexts: A number of methods have been proposed, either for context-sensitive spelling correction directly, or for related lexical disambiguation tasks. The methods include word trigrams (Mays et al., 1991), Bayesian classifiers (Gale et al., 1993), decision lists <ref> (Yarowsky, 1994) </ref>, Bayesian hybrids (Golding, 1995), a combination of part-of-speech trigrams and Bayesian hybrids (Golding and Schabes, 1996), and, more recently, transformation-based learning (Mangu and Brill, 1997), latent semantic analysis (Jones and Martin, 1997), and differential grammars (Powers, 1997). <p> They therefore capture aspects of the context with a wide-scope, semantic 5 flavor, such as discourse topic and tense. Collocations, in contrast, capture the local syntax around the target word. Similar combinations of features have been used in related tasks, such as accent restoration <ref> (Yarowsky, 1994) </ref> and word sense disambiguation (Ng and Lee, 1996). We use a feature extractor to convert from the initial text representation of a sentence to a list of active features. The feature extractor has a preprocessing phase in which it learns a set of features for the task.
References-found: 39

