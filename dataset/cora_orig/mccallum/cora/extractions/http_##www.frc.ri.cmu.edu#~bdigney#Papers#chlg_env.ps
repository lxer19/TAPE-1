URL: http://www.frc.ri.cmu.edu/~bdigney/Papers/chlg_env.ps
Refering-URL: http://www.frc.ri.cmu.edu/~bdigney/pubs.html
Root-URL: 
Title: Learning and Shaping in Emergent Hierarchical Control Systems  
Author: Bruce L. Digney 
Address: Box 4000, Medicine Hat, Alberta, CANADA, T1A 8K6  
Affiliation: Defence Research Establishment Suffield  
Abstract: The use of externally imposed hierarchical structures to reduce the complexity of learning control is common. However it is clear that the learning of the hierarchical structure by the machine itself is an important step towards more general and less bounded learning. Presented in this paper is a nested Q-learning technique that generates a hierarchical control structure as the robot interacts with its world. Furthermore, given the frailties of real machines and the long learning times required, it is becoming clear that fully unassisted learning for robots is unrealistic and when one considers the tremendous amount of information that novice humans/animals receive it is also unreasonable. Also, presented in this paper are methods for pre-training and supplying initial guidance to prepare robots for future situations. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> Digney B.L. </author> <year> (1994), </year> <title> A Distributed Adaptive Control System for a Quadruped Mobile Robot, Simulation of Adaptive Behavior SAB 94, </title> <address> Brighton UK, </address> <month> August </month> <year> 1994, </year> <pages> pp 344-354, </pages> <publisher> MIT Press-Bradford Books, Massachussets. </publisher>
Reference-contexts: 1 Introduction Much research is currently being pursued to allow autonomous agents or robots to learn from their environments [1] <ref> [2] </ref>. Researchers have realized that for robots to advance, beyond their current role of unintelligent drones working in highly structured and controlled environments, to intelligent self-directed and adaptive robots operating in unstructured and changing environments, they must be capable of learning.
Reference: [3] <author> Digney, B.L. </author> <year> (1995), </year> <title> Emergent Control Structures: Bottom up/Top down Driven Generation of Control Structures, </title> <booktitle> The Department of National Defence and Canadian Space Agency Conference on Robotics, </booktitle> <month> October </month> <year> 1995, </year> <institution> St. Hubert, </institution> <address> PQ, CANADA. </address>
Reference-contexts: In addition, external guidance in the form of scaf <p>- folding actions and auxiliary reinforcement signals is being supplied from a more experienced operator, instructor or another robot. 2 Nested Q-learning In ongoing research a nested Q-learning algorithm <ref> [3] </ref> is being developed that allows for the generation of hierarchical control systems. Due to the nature of this paper nested Q-learning will only be described in general terms, a more detailed derivation is available elsewhere [3]. In nested Q-learning any distinct recognizable sensory state becomes a feature. <p> instructor or another robot. 2 Nested Q-learning In ongoing research a nested Q-learning algorithm <ref> [3] </ref> is being developed that allows for the generation of hierarchical control systems. Due to the nature of this paper nested Q-learning will only be described in general terms, a more detailed derivation is available elsewhere [3]. In nested Q-learning any distinct recognizable sensory state becomes a feature. These features are discovered by the robot as it moves through its world. The control strategies that move the robot between features become known as skills. <p> cannot reach into the head of a student or animal in training and directly shape the control systems within, one can only influence what develops in these emergent controls systems indirectly by 3 controlling the situations and rewards that are experienced. 3.1 Scaffolding Actions From the development of nested Q-learning <ref> [3] </ref>, the selection of a primitive action or a complex skill is a combination of an exploitive and an exploratory component. Once an action has been taken, be it exploratory or exploitive, state changes and reinforcement signals result from which the control system learns.
Reference: [4] <author> Digney, B.L. </author> <year> (1995), </year> <title> The Operator's Apprentice: Shaping Control Structures, </title> <booktitle> The Department of National Defence and Canadian Space Agency Conference on Robotics, </booktitle> <month> October </month> <year> 1995, </year> <institution> St. Hubert, </institution> <address> PQ., CANADA. </address> <month> 9 </month>
Reference-contexts: In these simulations situations were presented to the robot involving changes to the training environment, the tasks and external influences. In related work it has been shown that in an impoverished training environment the robot will not learn or grasp the complete structure of the task <ref> [4] </ref>. The robot was pre-trained with two tasks using auxiliary reinforcement signals. These two tasks proved useful in the robot's next task and demonstrated staged learning. <p> These two tasks proved useful in the robot's next task and demonstrated staged learning. Transfer of information between tasks has also been demonstrated by first training the robot on one task and then requesting a new but related task <ref> [4] </ref>. Figure 3 summarized the features found to be relevant by the robot.
References-found: 3

