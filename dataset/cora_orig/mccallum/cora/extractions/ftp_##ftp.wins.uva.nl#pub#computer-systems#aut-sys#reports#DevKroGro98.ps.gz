URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/DevKroGro98.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fanuj,krose,groeng@wins.uva.nl  
Title: "Where are you driving to?" Heading Direction for a Mobile Robot from Optical Flow  
Author: A. Dev B.J.A Krose F.C.A. Groen 
Keyword: Real World Computing Partnership, Novel Functions Laboratory SNN  
Address: Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands  
Affiliation: Department of Computer Science, University Of Amsterdam  
Note: Proc. of the IEEE Int. Conf. on Robotics and Automation, Leuven, May 1997, pp 558-563.  
Abstract: If a camera moves on a straight line, the optic flow field is a diverging vector field, of which the singularity is called "focus of expansion" [2][15]. An object which is seen in this FOE is located on the future path of the camera. If the camera is also rotating, the future path is no longer a point in the image domain, but a line. All objects which are on the future path (and thus will cause collisions) are projected on this line. However, not necessary the reverse is true: not all points on the line are collision points. In this paper we derive how the optic flow can be used to compute which points in the image are projections of collision points. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: ~r (x; y) = B T x + xT z + xy x 1 + x 2 T y + yT z + 1 + y 2 1 A with _ ~r (x; y) = _r x (x; y) X y = Z It is well known (see for example <ref> [1] </ref>) that the rotational motion of the camera ~ can fully be observed from the image motion. The translational motion ~ T however can only be observed relative to the depth to the environment and vice versa.
Reference: [2] <author> J.J. Gibson. </author> <title> The perception of the Visual World. </title> <address> Riverside, </address> <year> 1950. </year>
Reference: [3] <author> E. D. Dickmanns, A. Zapp. </author> <title> A Curvature-based Scheme for Improving Road Vehicle Guidance by Computer Vision. </title> <booktitle> Proceedings SPIE, </booktitle> <volume> Vol. </volume> <pages> 727, </pages> <address> Cam-bridge Mass., </address> <year> 1986. </year>
Reference-contexts: The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow [8], stereo [10], range finder [9]), or using a strict model of what is expected to be seen in the image sequence <ref> [3] </ref> [4] [5]. Once the objects are represented in the world domain, we can classify the near future location of the vehicle in terms of the task: collision avoidance, free space following, or goal directed behavior.
Reference: [4] <author> R. Behringer. </author> <title> Road Recognition from Multifocal Vision. </title> <booktitle> Proceedings Intelligent Vehicles, </booktitle> <address> Paris, </address> <year> 1994. </year>
Reference-contexts: The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow [8], stereo [10], range finder [9]), or using a strict model of what is expected to be seen in the image sequence [3] <ref> [4] </ref> [5]. Once the objects are represented in the world domain, we can classify the near future location of the vehicle in terms of the task: collision avoidance, free space following, or goal directed behavior.
Reference: [5] <author> M. Ekenci, </author> <title> B.T. Thomas Road Junction Recognition and Turn-offs for Autonomous Road Vehicle Navigation. </title> <booktitle> Proceedings of the ICPR, </booktitle> <address> Vienna, </address> <year> 1996. </year>
Reference-contexts: The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow [8], stereo [10], range finder [9]), or using a strict model of what is expected to be seen in the image sequence [3] [4] <ref> [5] </ref>. Once the objects are represented in the world domain, we can classify the near future location of the vehicle in terms of the task: collision avoidance, free space following, or goal directed behavior.
Reference: [6] <author> A. Dev, B.J.A. Krose, F.C.A. Groen. </author> <title> Confidence Measures for Image Motion Estimation. Proceedings 1997 RWC Symposium, </title> <type> RWC Technical Report TR - 96001, </type> <pages> pp. 199-206, </pages> <address> Japan. </address>
Reference-contexts: Finally the optical flow, the observable part of the image motion, was estimated using the technique described in [7]. The measure of confidence in the estimated image motion was computed using the technique described in <ref> [6] </ref> (figures 4b, 4d, 4e). The optical flow was compared with the threshold function (x; y).
Reference: [7] <author> B. Lucas, T. Kanade", </author> <title> An iterative image restoration technique with an application to stereo vision. </title> <booktitle> Proceedings of the DARPA Image Understanding workshop, </booktitle> <year> 1981. </year>
Reference-contexts: The ground truth of the collision path of the camera was measured using the model of the scene and the camera parameters (figure 4g). Finally the optical flow, the observable part of the image motion, was estimated using the technique described in <ref> [7] </ref>. The measure of confidence in the estimated image motion was computed using the technique described in [6] (figures 4b, 4d, 4e). The optical flow was compared with the threshold function (x; y).
Reference: [8] <author> A. Dev, B.J.A. Krose, F.C.A. Groen. </author> <title> Navigation of a mobile robot on the temporal development of the optical flow. </title> <booktitle> Proc. IROS97, IEEE/RSJ, </booktitle> <pages> pp. 558-564, </pages> <year> 1997. </year>
Reference-contexts: An obvious choice is to represent labeled image regions in the world domain. The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow <ref> [8] </ref>, stereo [10], range finder [9]), or using a strict model of what is expected to be seen in the image sequence [3] [4] [5].
Reference: [9] <author> P. Weckesser, R. </author> <title> Dillmann Navigating a mobile surface robot in a natural environment using sensor fusion techniques. </title> <booktitle> Proc. IROS97, IEEE/RSJ, </booktitle> <pages> pp. 1423-1429, </pages> <year> 1997. </year>
Reference-contexts: An obvious choice is to represent labeled image regions in the world domain. The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow [8], stereo [10], range finder <ref> [9] </ref>), or using a strict model of what is expected to be seen in the image sequence [3] [4] [5].
Reference: [10] <author> J.L. Crowley et al. </author> <title> Mobile robot perception using vertical line stereo. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 7, </volume> <pages> pp. 125-138, </pages> <year> 1991. </year>
Reference-contexts: An obvious choice is to represent labeled image regions in the world domain. The location of objects in 3D space is measured with ad ditional (virtual) sensors (optical flow [8], stereo <ref> [10] </ref>, range finder [9]), or using a strict model of what is expected to be seen in the image sequence [3] [4] [5].
Reference: [11] <author> H.C. Longuet-Higgings and K. Prazdny. </author> <title> The interpretation of a moving retinal image. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B208, </volume> <year> 1980. </year>
Reference-contexts: For this we need the image motion. For a unit focal length pinhole camera, moving with translational velocity ~ T and rotational velocity ~ , it can be shown <ref> [11] </ref> that the image motion _ ~r on a planar surface of a point located at (X; Y; Z), which projects at x; y on the image surface, is given by: _ ~r (x; y) = B T x + xT z + xy x 1 + x 2 T y
Reference: [12] <author> L.M. Lorigo, R.A. Brooks, W.E.L. </author> <title> Grimson. Visually-Guided Obstacle Avoidance in Unstructured Environments. </title> <booktitle> Proc. IROS97, IEEE/RSJ,, </booktitle> <pages> pp. 373-379, </pages> <year> 1997. </year>
Reference-contexts: For navigation on vision, roads and obstacles are objects which are specified in the image domain. Detection of objects leads to a segmentation of the image into disjoint regions where a region can be labeled as: obstacle, free-space, goal, road etc. (see for example <ref> [12] </ref> [13]). In short, objects can only be detected in the 2 dimensional image do we heading for?". Below the dashed line: via the 3D world domain. Above the dashed line: directly the 2D image domain. main.
Reference: [13] <author> J. Fernandez and A. Casals. </author> <booktitle> Autonomous Navigation in ill-Structured Outdoor Environments. Proc. IROS97, IEEE/RSJ, </booktitle> <pages> pp. 395-401, </pages> <year> 1997. </year>
Reference-contexts: For navigation on vision, roads and obstacles are objects which are specified in the image domain. Detection of objects leads to a segmentation of the image into disjoint regions where a region can be labeled as: obstacle, free-space, goal, road etc. (see for example [12] <ref> [13] </ref>). In short, objects can only be detected in the 2 dimensional image do we heading for?". Below the dashed line: via the 3D world domain. Above the dashed line: directly the 2D image domain. main.
Reference: [14] <author> D.N. Lee. </author> <title> The optic flow field: the foundation of vision. </title> <journal> Philosophical Transactions of the Royal Society of London, </journal> <volume> vol. B, 290, pp169-179, </volume> <year> 1980. </year>
Reference-contexts: It equals the time along the camera curve to reach the point: t (x; y) = y which can be calculated by a parameterization of (5) to time. Notice that this time to collision does not equal the usual definition <ref> [14] </ref> of the depth divided by the translational velocity; we have considered circular motion, not rectlinear motion! 5 Experiments We carried out an experiment to test the accuracy of the prediction of collision points.

References-found: 14

