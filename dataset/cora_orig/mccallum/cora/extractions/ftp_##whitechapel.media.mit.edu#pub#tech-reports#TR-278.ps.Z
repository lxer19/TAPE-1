URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-278.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: steve@media.mit.edu, picard@media.mit.edu  
Title: `Video orbits': characterizing the coordinate transformation between two images using the projective group accurate, robust
Author: S. Mann and R. W. Picard 
Note: Many applications in computer vision benefit from  Contents  
Address: 20 Ames Street; Cambridge, MA 02139  
Affiliation: MIT Media Lab;  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 278 Submitted to ICCV95 Abstract Perhaps the most frequently used coordinate transformation is based on the 6-parameter affine model; it is simple to implement and captures camera translation, zoom, and rotation. Higher order models, such as the 8-parameter bilinear, 8-parameter pseudo-perspective, or 12-parameter `biquadratic', have also been proposed to approximately capture the two extra degrees of freedom that a camera has (pan, tilt) that are not captured by the affine model. However, none of these models exactly captures the eight parameters of camera motion. The desired parameters are those of elements in the projective group, which map the values at location x to those at location x 0 = (Ax + b)=(c T x + 1), where the numerator contains the six affine parameters, and the denominator contains the two additional pan-tilt or "chirp" parameters, c. This paper presents a new method to estimate these eight parameters from two images. The method works without feature correspondences, and without the huge computation demanded by direct nonlinear optimization algorithms. The method yields the "exact" eight parameters for the two no-parallax cases: 1) a rigid planar patch, with arbitrary 3D camera translation, rotation, pan, tilt, and zoom; and 2) an arbitrary 3D scene, with arbitrary camera rotation, pan, tilt, and zoom about a fixed center of projection. We demonstrate the proposed method on real image pairs and discuss new applications for facilitating log ging and browsing of video databases.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Wyckoff. </author> <title> An experimental extended response film. </title> <booktitle> S.P.I.E. NEWSLETTER, </booktitle> <month> JUNE-JULY </month> <year> 1962. </year>
Reference-contexts: c y y + 1 0 a y 0 x x + a y 0 y y + b y 0 or, in matrix form, x = x 0 A [x; y] T + b = c T x + 1 The desired eight parameters are denoted by p = <ref> [A; b; c; 1] </ref>, A 2 IR 2fi2 , b 2 IR 2fi1 , and c 2 IR 2fi1 .
Reference: [2] <author> S. Mann and R.W. </author> <title> Picard. Extending dymanic range by combining differently exposed pictures. </title> <type> Technical Report 289, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, Boston, Massachusetts, </institution> <month> August </month> <year> 1994. </year>
Reference: [3] <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Shape recovery from multiple views: a parallax based approach. </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> Nov </month> <year> 1984. </year>
Reference-contexts: However, when imaging an arbitrary 3D scene, translation of the camera causes parallax 5 . The cases which are exactly solved in this paper do not include parallax, but their accurate solution is an important first step in compensating for parallax <ref> [3] </ref>. 1.2 Coordinate transformations: terminology Terms like affine, perspective, and projective have become confused as they are used differently by people from different backgrounds. <p> Faugeras and Lustman [9], Shashua and Navab [10], and Sawhney [11] have considered the problem of computing the motion of a rigid planar patch, as part of a larger problem of finding 3D motion and structure using parallax relative to an arbitrary plane in the scene. Kumar et al. <ref> [3] </ref> have also suggested registering frames of video by computing the flow along the epipolar lines, for which there is also an initial step of calculating the gross camera movement assuming zero parallax. Others have taken different approaches to the problem of image registration as well [12].
Reference: [4] <author> Semple and Kneebone. </author> <title> Algebraic Projective Geometry. </title> <publisher> Oxford Science Publications, </publisher> <year> 1979. </year>
Reference-contexts: For example, in the field of projective geometry <ref> [4] </ref>, perspective is often used to imply a mapping to a lower dimension (e.g. 3D to 2D), while projective implies a mapping to the same dimension (e.g. 2D to 2D). Many others, however, use the term perspective to denote a projective mapping from 2D to 2D (e.g. <p> Although commu-tativity is not required by the group definition, its presence greatly simplifies parameter estimation see Sec. 3.4. 9 also known as a group action or G-set [13]. 10 Two operands of a group operation are also said to be con gruent if they lie in the same orbit <ref> [4] </ref>. (depicted here as 1D subspaces of the 2D page). In this example, frames 1 to 6 lie in orbits that are approximately the same (the orbit of frame 1 is depicted). Frames 7 and higher lie in a different orbit, corresponding to the second scene.
Reference: [5] <author> George Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, </publisher> <address> 10662 Los Vaqueros Circle, Los Alamitos, CA, 1990. </address> <publisher> IEEE Computer Society Press Monograph. </publisher>
Reference-contexts: Many others, however, use the term perspective to denote a projective mapping from 2D to 2D (e.g. Wolberg's <ref> [5] </ref> perspective mapping, which is so commonly used that it is incorporated into many software applications packages, such as Adobe Photoshop.) The projective coordinate transformation, for example, could also be described as affine or Euclidean in 3D, or linear in 4D (using homogeneous coordinates, writing x 0 = Ax; x; y <p> Let's briefly show how the bilinear and pseudo-perspective models can be obtained from the biquadratic model 6 The bilinear model is perhaps the most widely-used <ref> [5] </ref> in the fields of image processing, medical imaging, remote sensing, and computer graphics. This model is easily obtained from the biquadratic by removing the four x 2 and y 2 terms. <p> Then set h k = p 0;k ffi h. (This should have nearly the same effect as applying p k to h k1 , except that it will avoid additional interpolation and anti-aliasing errors you would get by resampling an already resampled image <ref> [5] </ref>). 19 . Repeat until either the error between h k and g falls below a threshold, or until some maximum number of iterations is achieved. We find that two or three iterations are usually sufficient for frames from nearly the same orbit.
Reference: [6] <author> G. Adiv. </author> <title> Determining 3D Motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <pages> pages 304-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: This model is easily obtained from the biquadratic by removing the four x 2 and y 2 terms. Although the resulting bilinear model captures the effect of converging lines, it completely fails to capture the effect of "chirping". The 8-parameter pseudo-perspective model <ref> [6] </ref>) does, in fact, capture both the converging lines and the "chirping" of a projective coordinate transformation.
Reference: [7] <author> Steve Mann and Nassir Navab. </author> <title> Recovery of relative affine structure using the motion flow field of a rigid planar patch. </title> <booktitle> Mustererkennung 1994, </booktitle> <address> Tagungsband., </address> <year> 1994. </year>
Reference-contexts: This model may be thought of as first, removal of two of the quadratic terms (q x 0 y 2 = q y 0 x 2 = 0), which results in a ten parameter model (the `q-chirp' of Mann <ref> [7] </ref>) and then constraining the four remaining quadratic parameters to have two degrees of freedom.
Reference: [8] <author> R. Y. Tsai and T. S. Huang. </author> <title> Estimating three-dimensional motion parameters of a rigid planar patch. </title> <journal> tassp, </journal> <volume> ASSP-29(9):1147-1152, </volume> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: in camera motion for two interesting "no-parallax" cases outlined in Table 1.3 The second case is sometimes referred to as "computing the motion of a rigid planar patch." This paper is not the first to provide a means of calculating the motion of a rigid planar patch; Tsai and Huang <ref> [8] </ref> solved this problem when feature correspondences were available and stressed the significance of group theory. <p> If the motion is to be tracked automatically (e.g. using features that are found computationally), then most of the computation involves feature selection. Once the features are found, and are known to lie on a rigid planar patch, the computation of the motion of that patch is straightforward <ref> [8] </ref>. A second problem with feature-based methods is their sensitivity to noise and occlusion.
Reference: [9] <author> O. D. Faugeras and F. Lustman. </author> <title> Motion and structure from motion in a piecewise planar environment. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(3) </volume> <pages> 485-508, </pages> <year> 1988. </year>
Reference-contexts: Faugeras and Lustman <ref> [9] </ref>, Shashua and Navab [10], and Sawhney [11] have considered the problem of computing the motion of a rigid planar patch, as part of a larger problem of finding 3D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [10] <author> Amnon Shashua and Nassir Navab. </author> <title> Relative Affine: Theory and Application to 3D Reconstruction From Perspective Views. </title> <booktitle> Proc. IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <month> June </month> <year> 1994. 1994. </year>
Reference-contexts: Faugeras and Lustman [9], Shashua and Navab <ref> [10] </ref>, and Sawhney [11] have considered the problem of computing the motion of a rigid planar patch, as part of a larger problem of finding 3D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [11] <author> H.S. Sawhney. </author> <title> Simplifying motion and structure analysis using planar parallax and image warping. </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: Faugeras and Lustman [9], Shashua and Navab [10], and Sawhney <ref> [11] </ref> have considered the problem of computing the motion of a rigid planar patch, as part of a larger problem of finding 3D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [12] <author> Qinfen Zheng and Rama Chellappa. </author> <title> A Computational Vision Approach to Image Registration . IEEE Image Processing, </title> <month> July </month> <year> 1993. </year> <pages> pages 311-325. </pages>
Reference-contexts: Kumar et al. [3] have also suggested registering frames of video by computing the flow along the epipolar lines, for which there is also an initial step of calculating the gross camera movement assuming zero parallax. Others have taken different approaches to the problem of image registration as well <ref> [12] </ref>.
Reference: [13] <author> M. </author> <title> Artin. Algebra. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Review: Groups, Operators, and Orbits Three of the models presented so far, translation, affine, and projective, each form a group. 7 Thinking about the projective model as a group leads to some useful concepts which we exploit in the rest of this paper. 2.1.1 Definitions: group, group operation A group <ref> [13] </ref> is a collection of objects upon which there is defined the structure: 1. Closure: a law of composition allowing us to combine two members of the group into a single object of the same kind. <p> regarded as real-valued functions of real variables (e.g. discretization will be ignored). 2.1.2 Definition: orbits Two operands of a group operation are said to be in the same orbit if and only if one can be made to take the place of the other by an operation of the group <ref> [13] </ref>. <p> Although commu-tativity is not required by the group definition, its presence greatly simplifies parameter estimation see Sec. 3.4. 9 also known as a group action or G-set <ref> [13] </ref>. 10 Two operands of a group operation are also said to be con gruent if they lie in the same orbit [4]. (depicted here as 1D subspaces of the 2D page).
Reference: [14] <author> Edwin A. Abbott. Flatland. Signet Classic, </author> <month> June </month> <year> 1984. </year>
Reference-contexts: The extension from flatland images to 2D images is conceptually identical; for the affine and projective models, the 12 after the title of Abbott's classic book <ref> [14] </ref>, which is a story about an alien culture living in a 2D world. 5 number of correspondence points needed are at least six and eight respectively. The major problem with feature-based methods is, of course, finding the features.
Reference: [15] <author> R. Y. Tsai and T. S. Huang. </author> <title> Multiframe image restoration and registration. </title> <publisher> ACM, </publisher> <year> 1984. </year>
Reference: [16] <author> T. S. Huang and A.N. Netravali. </author> <title> Motion and structure from feature correspondences: a review. </title> <booktitle> Proc. IEEE, </booktitle> <month> Feb </month> <year> 1984. </year>
Reference: [17] <author> Nassir Navab and Amnon Shashua. </author> <title> Algebraic Description of Relative Affine Structure: Connections to Euclidean, Affine and Projective Structure. MIT Media Lab Memo No. </title> <type> 270, </type> <year> 1994. </year>
Reference-contexts: The major problem with feature-based methods is, of course, finding the features. These features are often hand-selected, or computed, possibly with some degree of human intervention <ref> [17] </ref>. If the motion is to be tracked automatically (e.g. using features that are found computationally), then most of the computation involves feature selection. Once the features are found, and are known to lie on a rigid planar patch, the computation of the motion of that patch is straightforward [8].
Reference: [18] <author> Harry L. </author> <title> Van Trees. Detection, Estimation, and Modulation Theory (Part I). </title> <publisher> John Wiley and Sons, </publisher> <year> 1968. </year>
Reference-contexts: whole family of templates to have the same energy, by expressing the family as: h a;b (x) = p h (ax + b) (3) 13 In the presence of additive white Gaussian noise, this method, also known as "matched filtering", leads to a maxi mum likelihood estimate of the parameters <ref> [18] </ref>. then the maximum likelihood estimate corresponds to selecting the member of the family that gives the largest inner product: hg (x); h a;b (x)i = x This result is known as a cross-wavelet transform. A com-putationally efficient algorithm for the cross-wavelet transform has recently been presented [19].
Reference: [19] <author> R.K. Young. </author> <title> Wavelet theory and its applications. </title> <year> 1993. </year>
Reference-contexts: A com-putationally efficient algorithm for the cross-wavelet transform has recently been presented <ref> [19] </ref>. A good general review article dealing with the use of the wavelet transform for the estimation of affine coordinate transformation is presented in [20].
Reference: [20] <author> Lora G. Weiss. </author> <title> Wavelets and wideband correlation processing. </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pages 13-32, </pages> <year> 1993. </year>
Reference-contexts: A com-putationally efficient algorithm for the cross-wavelet transform has recently been presented [19]. A good general review article dealing with the use of the wavelet transform for the estimation of affine coordinate transformation is presented in <ref> [20] </ref>. The cross-wavelet transform provides a mapping from a pair of 1D images to a parameter space that is a function of two variables stretch and translation. This parameter space may be sampled on any desired 2D lattice of parameters.
Reference: [21] <author> Steve Mann and Simon Haykin. </author> <title> The chirplet transform | a generalization of Gabor's logon transform. </title> <booktitle> Vision Interface '91, </booktitle> <month> June 3-7 </month> <year> 1991. </year>
Reference-contexts: Just like the cross-correlation for the translation group, and the cross-wavelet for the affine group, the "cross-chirplet" can be used to find the parameters of a projective coordinate transformation in flatland, searching over a 3-parameter space. The cross-chirplet is based on a generalization of the wavelet <ref> [21] </ref> known as the "p-chirplet." A p-chirplet has the form: h a;b;c = h ( cx + 1 where h is the `mother chirplet', analogous to the mother wavelet of wavelet theory. Members of this family of functions are related to one another by projective coordinate transformations.
Reference: [22] <author> Steve Mann and Simon Haykin. </author> <title> Adaptive "Chirplet" Transform: an adaptive generalization of the wavelet transform. </title> <journal> Optical Engineering, </journal> <volume> 31(6) </volume> <pages> 1243-1256, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: With 2D images, the search is over an 8-parameter space. A dense sampling of this volume is computationally prohibitive. Consequently, combinations of coarse-to-fine and iterative gradient-based search procedures are required. Adaptive variants of the chirplet transform have been previously reported in the literature <ref> [22] </ref>. 3.3 Featureless methods based on spatiotemporal derivatives This section addresses featureless methods which are motivated by the original optical flow equations of Horn and Schunk for the case of translational motion.
Reference: [23] <author> B. Horn and B. Schunk. </author> <title> Determining Optical Flow. </title> <journal> Artificial Intelligence, </journal> <year> 1981. </year>
Reference-contexts: A detailed new algorithm for applying the last case is given in Sec. 4. 3.3.1 Optical flow: translation (review) When the change from one image to another is small, optical flow <ref> [23] </ref> may be used. In flatland, the traditional optical flow formulation assumes each point x in frame t is a translated version of the corresponding point in frame t + t, and that x and t are chosen in the ratio x=t = u f . <p> We describe an algorithm which exploits this commutativity for estimating the parameters of the non-commutative 2D projective group in Sec. 4.3 and propose it as an optional pre-processing step. 4 Estimating the parameters of the projective group in 2D The brightness constancy constraint equation for 2D images <ref> [23] </ref> gives us the flow velocity components in each of the x and y directions (analogous to (7)): u f E x + v f E y + E t 0 (24) As with the 1D case, we may derive two variants of the generalized optical flow: the `projective fit' to <p> However, as is well-known <ref> [23] </ref>, the optical flow field in 2D is underconstrained 16 . The model of pure translation at every point has two parameters, but there is only one equation (24) to solve.
Reference: [24] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The M.I.T. Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Typically, the higher order terms are neglected, giving the expression for the optical flow at each point in one of the two images: u f E x + E t 0 (7) The derivation for translational optical flow in 2D follows directly <ref> [24] </ref>, although its solution won't be addressed until Sec. 4. 3.3.2 "Affine fit" and "affine flow": a new relationship We now address the following problem: given the optical flow between two images, g and h, we wish to find the coordinate transform to apply to h to make it look most
Reference: [25] <author> John Y.A. Wang and Edward H. Adelson. </author> <booktitle> Spatio-Temopral Segmentation of Video Data . In SPIE Image and Video Processing II, </booktitle> <pages> pages 120-128, </pages> <address> San Jose, Califor-nia, </address> <month> February 7-9 </month> <year> 1994. </year>
Reference-contexts: Wang and Adelson have proposed fitting an affine model to an optical flow field <ref> [25] </ref> of 2D images. We briefly examine their approach with flatland images; the analysis in 2D is the same but with more notation. Coordinates in the original image, g, are denoted by x, and those in the new image, h, are denoted by x 0 . <p> This gives us a means of segmenting the image into regions that have similar affine motion <ref> [25] </ref>. Another approach to the `affine fit' of Wang and Adelson involves computation of the optical flow field using the hierarchical and iterative method of Lucas and Kanade, and then a fit to the affine model.
Reference: [26] <author> J. Bergen, P. Burt, R. Hingorini, and S. Peleg. </author> <title> Computing two motions from three frames. </title> <booktitle> In Proc. Third Int'l Conf. Comput. Vision, </booktitle> <pages> pages 27-32, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: This results in equations for the model we call "affine fit:" P P x x; x 1 a 1 P x E t =E x (9) Alternatively, the affine coordinate transformation may be directly incorporated into the brightness change constraint equation (6). Bergen et al. <ref> [26] </ref> have proposed this method, which we will call `affine flow', to distinguish it from the `affine fit' model of Wang and Adelson above. Let us show how `affine flow' and `affine fit' are related. <p> An analogous variant of the `affine flow' method involves iteration and hierarchy as well, but in this case the iteration and hierarchy are incorporated directly into the affine estimator <ref> [26] </ref>. When they are implemented hierarchically, the two methods differ in additional respects. In situations where the boundaries of the patch are explicitly known, our intuition and experience indicates that the direct hierarchical `affine flow' method of Bergen et al. performs better than the `affine fit' to the hierarchical flow. <p> This equation looks similar to the 6 fi 6 matrix equation presented in Bergen et al. <ref> [26] </ref>. In order to see how well the model describes the coordinate transformation between 2 images, say, g and h, one might warp 18 h to g, using the estimated motion model, and then compute some quantity that indicates how different the resampled version of h is from g.
Reference: [27] <author> Lucas and Kanade. </author> <title> An iterative image-registration technique with an application to stereo vision . In Image Understanding Workshop, </title> <address> pages 121-130, </address> <year> 1981. </year>
Reference-contexts: Both our intuition and our practical experience tends to favor the `affine flow' weighting, but, more generally, perhaps we should ask "What is the best weighting?" (e.g. maybe there is an even better answer than the choice among these two). Lucas and Kanade <ref> [27] </ref>, among others, have considered weighting issues. Given a choice of weightings that make the two methods equivalent, there is another important difference.
Reference: [28] <author> S. Mann and R.W. </author> <title> Picard. Virtual bellows: constructing high-quality images from video. </title> <booktitle> In Proceedings of the IEEE first international conference on image processing, </booktitle> <address> Austin, Texas, </address> <month> Nov. 13-16 </month> <year> 1994. </year>
Reference-contexts: 3.3.3 "Projective fit" and "projective flow" For the affine coordinate transformation, the graph of the range coordinate as a function of the domain coordinate is a straight line; for the projective coordinate transformation, the graph of the range coordinate as a function of the domain coordinate is a rectangular hyperbola <ref> [28] </ref>.
Reference: [29] <author> R. Szeliski and J. Coughlan. </author> <title> Hierarchical spline-based image registration. </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: Szeliski and Coughlan <ref> [29] </ref> suggest a good framework for solving this nonlinear optimization problem, which they applied to image mosaic-ing [30]. (Their method could also be applied to the "Projective fit" equations in (18)). The approach presented here, however, is different.
Reference: [30] <author> R. Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <month> May </month> <year> 1994. </year>
Reference-contexts: Szeliski and Coughlan [29] suggest a good framework for solving this nonlinear optimization problem, which they applied to image mosaic-ing <ref> [30] </ref>. (Their method could also be applied to the "Projective fit" equations in (18)). The approach presented here, however, is different.
Reference: [31] <author> Roland Wilson and Goesta H. </author> <title> Granlund. </title> <booktitle> The Uncertainty Principle in Image Processing . IEEE Transactions on Pattern Analysis and Machine Intelligence, </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: moving patch to be easily found, although we will see that in a multiscale implementation, it is better to use the projective flow, if we know a-priori the boundary of the patch (e.g. when it is the whole image). 3.4 Exploiting commutativity for parameter estimation There is a fundamental uncertainty <ref> [31] </ref> involved in the simultaneous estimation of parameters of a noncommutative group of coordinate transformations, which is akin to the Heisen-berg uncertainty relation of quantum mechanics.
Reference: [32] <author> Bernd Girod and David Kuo. </author> <title> Direct Estimation of Displacement Histograms . OSA Meeting on IMAGE UNDERSTANDING AND MACHINE VISION, </title> <month> June </month> <year> 1989. </year>
Reference-contexts: Estimating the parameters of a commutative group of coordinate transformations is computationally efficient, through the use of Fourier cross-spectra <ref> [32] </ref>. <p> In practice, we have found that it is computationally beneficial to run through the following `commutative initialization' before estimating the parameters of the projective group of coordinate transformations: 1. Assume that h is merely a translated version of g. (a) Estimate this translation using the method of Girod <ref> [32] </ref>. (b) Shift h by the amount indicated by this estimate. (c) Compute the MSE between the shifted h and g, and compare to the original MSE before shifting. (d) If an improvement has resulted, use the shifted h from now on. 2.
Reference: [33] <author> Yunlong Sheng, Claude Lejeune, and Henri H. Arsenault. </author> <title> Frequency-domain Fourier-Mellin descriptors for invariant pattern recognition. Optical Engineering, </title> <month> May </month> <year> 1988. </year>
Reference-contexts: Thus we can simultaneously estimate isotropic-zoom and rotation about the optical axis by applying a log-polar coordinate transformation followed by a translation estimator. Alternatively, this process may be achieved by a direct application of the Fourier-Mellin transform <ref> [33] </ref>. Similarly, if the only difference between g and h is a camera pan, then the pan may be estimated through a coordinate transformation to cylindrical coordinates, followed by a translation estimator.
Reference: [34] <author> A. Nagasaka and Y. Tanaka. </author> <title> Automatic video indexing and full-video search for object appearances. Visual Database Systems, II, 1992. </title> <editor> editor E. Knuth and L. </editor> <publisher> Weg-ner. </publisher>
Reference-contexts: Scene change detection can be thought of as a "temporal edge detection" problem, with early approaches based on statistical differences and thresholded filter outputs <ref> [34] </ref> [35]. Although the current statistics and filter-based methods tend to be low in computation and work 80-90% of the time, they tend to fail when there is lots of camera motion such as pan.
Reference: [35] <author> K. Otsuji and K. Tonomura. </author> <title> Projection-detectiong filter for video cut detection. </title> <journal> Multimedia Systems, </journal> <volume> 1 </volume> <pages> 205-210, </pages> <year> 1994. </year>
Reference-contexts: Scene change detection can be thought of as a "temporal edge detection" problem, with early approaches based on statistical differences and thresholded filter outputs [34] <ref> [35] </ref>. Although the current statistics and filter-based methods tend to be low in computation and work 80-90% of the time, they tend to fail when there is lots of camera motion such as pan.
Reference: [36] <author> L. Teodosio and W. Bender. </author> <title> Salient video stills: Content and context preserved. </title> <booktitle> Proc. ACM Multimedia Conf., </booktitle> <year> 1993. </year>
Reference-contexts: enhances the resolution in the composited frames (in contrast, continuous addition of many affine-transformed frames will begin to blur the mosaic, as the affine model can not undo all of the physical camera motion.) This idea of mosaicing from sequences of images has been done before with the affine model <ref> [36] </ref> for making "salient stills" and [37] for resolution enhancement, and with the projective model [7][29][30] for mosaicing and enhancement. The use of the new projective flow algorithm brings new accuracy and speed to this application.
Reference: [37] <author> M. Irani and S. Peleg. </author> <title> Improving Resolution by Image Registration. Graphical Models and Img. </title> <booktitle> Proc., </booktitle> <month> May </month> <year> 1991. </year> <pages> 13 14 </pages>
Reference-contexts: frames (in contrast, continuous addition of many affine-transformed frames will begin to blur the mosaic, as the affine model can not undo all of the physical camera motion.) This idea of mosaicing from sequences of images has been done before with the affine model [36] for making "salient stills" and <ref> [37] </ref> for resolution enhancement, and with the projective model [7][29][30] for mosaicing and enhancement. The use of the new projective flow algorithm brings new accuracy and speed to this application.
References-found: 37

