URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/sfi.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: almualh@cs.orst.edu tgd@cs.orst.edu  
Title: A Study of Maximal-Coverage Learning Algorithms  
Author: Hussein Almuallim Thomas G. Dietterich 
Date: March 15, 1994  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: The coverage of a learning algorithm is the number of concepts that can be learned by that algorithm from samples of a given size. This paper asks whether good learning algorithms can be designed by maximizing their coverage. The paper extends a previous upper bound on the coverage of any Boolean concept learning algorithm and describes two algorithms|Multi-Balls and Large-Ball|whose coverage approaches this upper bound. Experimental measurement of the coverage of the ID3 and FRINGE algorithms shows that their coverage is far below this bound. Further analysis of Large-Ball shows that although it learns many concepts, these do not seem to be very interesting concepts. Hence, coverage maximization alone does not appear to yield practically-useful learning algorithms. The paper concludes with a definition of coverage within a bias, which suggests a way that coverage maximization could be applied to strengthen weak preference biases.
Abstract-found: 1
Intro-found: 1
Reference: [Almuallim and Dietterich 91] <author> Almuallim, H. and Dietterich, T. G. </author> <title> Learning With Many Irrelevant Features. </title> <booktitle> Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 547-552, </pages> <year> 1991. </year>
Reference-contexts: In short, the bias of Large-Ball is unlikely to be appropriate in real-world learning situations. This argument shows that coverage analysis alone is not sufficient to find a practically-useful inductive bias. This suggests that we combine coverage analysis with other methods for choosing inductive bias. For example, in <ref> [Almuallim and Dietterich 91] </ref>, we described learning situations in which the MIN-FEATURES bias|the bias that prefers consistent concepts definable over fewer features|is appropriate.
Reference: [Almuallim 91] <author> Almuallim, H. </author> <title> Exploiting Symmetry Properties in the Evaluation of Inductive Learning Algorithms: An Empirical Domain-Independent Comparative Study. </title> <type> Technical Report, </type> <institution> 1991-30-09, Dept. of Computer Science, Oregon State University, Corvallis, </institution> <address> OR 97331-3202. </address>
Reference: [Almuallim 92] <author> Almuallim, H. </author> <title> Concept Coverage and Its Application to Two Learning Tasks. </title> <type> Ph.D. Thesis. </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Ore-gon. Forthcoming, </institution> <year> 1992. </year>
Reference-contexts: The details of the above cost-reduction techniques are not central to this study. For a thorough discussion of these, we refer the interested reader to <ref> [Almuallim 92] </ref>. 6.1 Experimental Work Using the above cost-reduction techniques, we experimentally measured the coverage figures for three algorithms: ID3 [Quinlan 86], FRINGE [Pagallo and Haussler 90] and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the 21 Table 1: Coverage Figures For <p> The number of samples tested per concept and the margin 0:007 were chosen so that the probability of a wrong decision (considering an unlearned concept as learned and vice versa) becomes within 0.01 <ref> [Almuallim 92] </ref>, that is, to achieve 99% level of significance. The final results of our experiments are summarized in Table 1. 6.2 Coverage of the "Balls" Approach For comparison, let us compute the coverage obtained by the balls approach under the same conditions of our experiments.
Reference: [Blumer et.al. 87] <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and Warmuth, M. </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <type> Technical Report UCSC-CRL-87-20, </type> <institution> Department of Computer and Information Sciences, University of California, Santa Cruz, </institution> <month> Nov. </month> <year> 1987. </year> <note> Also in Journal of ACM, </note> <month> 36(4) </month> <pages> 929-965, </pages> <year> 1990. </year>
Reference-contexts: Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples <ref> [Blumer et.al. 87] </ref>. Such trade-off between the size of the class of concepts being learned and the required number of training examples dictates how far one can go in attempting to learn larger and larger classes of concepts. <p> A learning algorithm is a mapping from the space of samples to the space of concepts. The output of the algorithm is called an hypothesis. An hypothesis is consistent if it has no disagreement with the training sample. We adopt PAC learning <ref> [Blumer et.al. 87] </ref> as the criterion for successful learning, but we restrict this to learning under the uniform distribution only.
Reference: [Chernoff 52] <author> Chernoff, H. </author> <title> A Measure of Asymptotic Efficiency for Tests of Hypothesis Based on the Sums of Observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <pages> pp. 493-509, </pages> <year> 1952. </year>
Reference: [COLT 88] <institution> Proceedings of the 1988 Workshop on Computational Learning Theory. </institution> <note> Haussler, </note> <editor> D. and Pitt, L., Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year> <month> 25 </month>
Reference-contexts: 1 Introduction Research in computational learning theory (e.g., [Valiant 84], [Natarajan 87], <ref> [COLT 88] </ref>- [COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae). <p> for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of concepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84], [Natarajan 87], <ref> [COLT 88] </ref>-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
Reference: [COLT 89] <institution> Proceedings of the Second Annual Workshop on Computational Learning Theory. </institution> <note> Rivest, </note> <editor> R., Haussler, D. and Warmuth, M.K., Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference: [COLT 90] <institution> Proceedings of the Third Annual Workshop on Computational Learning Theory. Fulk, M.A. and Case, </institution> <note> J., </note> <editor> Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [COLT 91] <institution> Proceedings of the Fourth Annual Workshop on Computational Learning Theory. </institution> <note> Valiant, </note> <editor> L.G. and Warmuth, M., Editors. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Research in computational learning theory (e.g., [Valiant 84], [Natarajan 87], [COLT 88]- <ref> [COLT 91] </ref>) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae).
Reference: [Dietterich 89] <author> Dietterich, T. G. </author> <title> Limitations on inductive learning. </title> <booktitle> In Proceedings of the Sixth International Conference on Machine Learning, </booktitle> <pages> 124-128. </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1989. </year>
Reference-contexts: In fact, many of the concept classes studied in computational learning theory have never been supported by any practical justification. Due to these difficulties, the learning algorithms and sample complexity bounds developed in computational learning theory have rarely been of practical value. Recently, an alternative theoretical framework was introduced <ref> [Dietterich 89] </ref>. <p> Can we design a learning algorithm that attains this optimal coverage? 3. What is the coverage of existing learning algorithms? This paper contributes to answering each of these questions. First, we generalize the upper bound on coverage given in <ref> [Dietterich 89] </ref>. Next, we present two learning algorithms and determine their coverage analytically. The coverage of the first algorithm, Multi-Balls, is shown to be quite close to the upper bound. The coverage of the second algorithm, Large-Ball, turns out to be even better than Multi-Balls in many situations. <p> An upper bound of this type has been proven for the case where the training sample is drawn randomly without replacement <ref> [Dietterich 89] </ref>. In the following, we generalize Dietterich's 4 result and show that the same upper bound also holds for the case where sampling is done with replacement. In addition, we provide a closed-form expression for this bound. <p> Due to the difficulty of performing coverage analysis for empirical algorithms (e.g, ID3 [Quinlan 86]), one may consider measuring the coverage experimentally by running learning algorithms on every possible training sample (as done in <ref> [Dietterich 89] </ref>). However, this involves an immense amount of computation. Specifically, if the number of features is n and the sample size is m, then we have to run an algorithm on as many as 2 m m samaples and test the learnability of 2 2 n concepts. <p> Specifically, if the number of features is n and the sample size is m, then we have to run an algorithm on as many as 2 m m samaples and test the learnability of 2 2 n concepts. This is doable when n = 3 <ref> [Dietterich 89] </ref> or 4, but soon becomes unaffordable when n = 5. To reduce these computational costs, we can employ the following two techniques: First, we can exploit the fact that most of the learning algorithms are symmetric with respect to permutations and/or negations of input features.
Reference: [Hoeffding 63] <author> Hoeffding, W. </author> <title> Probability Inequalities for Sums of Bounded Random Variables. </title> <journal> Journal of The American Statistical Association, </journal> <volume> 58, 13-3-, </volume> <year> 1963. </year>
Reference: [Holte 91] <author> Holte, R. C. </author> <note> Machine Learning as Error-Correction. Unpublished note, </note> <year> 1991. </year>
Reference-contexts: The authors gratefully acknowledge the support of the NSF under grant number IRI-86-57316. Thanks to Bella Bose for clarifying some issues on the Gilbert-Varshamov bound, to Rob Holte for useful discussions and for providing <ref> [Holte 91] </ref>, and to Prasad Tadepalli for comments on an earlier draft of the paper.
Reference: [Natarajan 87] <author> Natarajan, B.K. </author> <title> On learning Boolean Functions. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing 296-304. </booktitle> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Research in computational learning theory (e.g., [Valiant 84], <ref> [Natarajan 87] </ref>, [COLT 88]- [COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. However, an important shortcoming of most work in this area is that it focuses on learning concepts drawn from prespecified classes of concepts (e.g., linearly separable functions, k-DNF formulae). <p> [Rivest 87], for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of concepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84], <ref> [Natarajan 87] </ref>, [COLT 88]-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
Reference: [Pagallo and Haussler 90] <author> Pagallo, G.; and Haussler, D. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: Third, we considerably improve upon Dietterich's limited experiments for estimating the coverage of existing learning algorithms. We find that the coverage of Large-Ball exceeds the coverage of ID3 [Quinlan 86] and FRINGE <ref> [Pagallo and Haussler 90] </ref> by more than an order of magnitude in most cases. These results are very thought-provoking, because, upon careful analysis, it becomes clear that the Large-Ball algorithm is rather trivial and uninteresting. <p> The details of the above cost-reduction techniques are not central to this study. For a thorough discussion of these, we refer the interested reader to [Almuallim 92]. 6.1 Experimental Work Using the above cost-reduction techniques, we experimentally measured the coverage figures for three algorithms: ID3 [Quinlan 86], FRINGE <ref> [Pagallo and Haussler 90] </ref> and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the 21 Table 1: Coverage Figures For Various Algorithms.
Reference: [Peterson and Weldon 72] <author> W. W. Peterson and E. J. Weldon. </author> <title> Error Correcting Codes, </title> <publisher> The MIT Press. </publisher> <address> p.86, </address> <year> 1972. </year>
Reference-contexts: where r is any integer satisfying 0 + l 1 ! 2 + + l 1 ! A proof of this lemma, in addition to a method of actually constructing the l-bit vectors as given above, can be found in many references in the literature of Error-Correcting Coding Theory (e.g., <ref> [Peterson and Weldon 72] </ref>). For our purposes here, it is convenient to draw the following corollary from the Gilbert-Varshamov bound given above.
Reference: [Quinlan 86] <author> Quinlan, J. R. </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The coverage of the second algorithm, Large-Ball, turns out to be even better than Multi-Balls in many situations. Third, we considerably improve upon Dietterich's limited experiments for estimating the coverage of existing learning algorithms. We find that the coverage of Large-Ball exceeds the coverage of ID3 <ref> [Quinlan 86] </ref> and FRINGE [Pagallo and Haussler 90] by more than an order of magnitude in most cases. These results are very thought-provoking, because, upon careful analysis, it becomes clear that the Large-Ball algorithm is rather trivial and uninteresting. <p> Due to the difficulty of performing coverage analysis for empirical algorithms (e.g, ID3 <ref> [Quinlan 86] </ref>), one may consider measuring the coverage experimentally by running learning algorithms on every possible training sample (as done in [Dietterich 89]). However, this involves an immense amount of computation. <p> The details of the above cost-reduction techniques are not central to this study. For a thorough discussion of these, we refer the interested reader to [Almuallim 92]. 6.1 Experimental Work Using the above cost-reduction techniques, we experimentally measured the coverage figures for three algorithms: ID3 <ref> [Quinlan 86] </ref>, FRINGE [Pagallo and Haussler 90] and MDT, which is an exhaustive algorithm that finds a decision tree with fewest nodes consistent with the 21 Table 1: Coverage Figures For Various Algorithms.
Reference: [Rivest 87] <author> Rivest, R. </author> <title> Learning Decision Lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Of course, no such guarantees are given if the target concept is not in C. This naturally means that one should seek algorithms that learn concept classes that are as large (i.e., less restricted) as possible. Rivest <ref> [Rivest 87] </ref>, for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of concepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., [Valiant 84], [Natarajan
Reference: [Ross 88] <author> Ross, S. </author> <title> A First Course in Probability. </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York. </address> <publisher> 3rd edition, </publisher> <pages> pp 111, </pages> <year> 1988. </year>
Reference-contexts: Then P r [R2jR1] = 1 P r i=1 # By the principle of inclusion and exclusion (e.g. <ref> [Ross 88] </ref>), we get P r [ i=1 X P r [A i ] i6=j Now, 2 n *2 n fi + k 1 ) m ; 1 i k 2 n *2 n fi + k 2 ) m ; 1 i; j k and i 6= j P r
Reference: [Valiant 84] <author> L. G. Valiant. </author> <title> A Theory of the Learnable. </title> <journal> Communications of ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Research in computational learning theory (e.g., <ref> [Valiant 84] </ref>, [Natarajan 87], [COLT 88]- [COLT 91]) has provided many insights into the capabilities and limitations of inductive learning from examples. <p> Rivest [Rivest 87], for instance, mentions this goal most explicitly by saying: "One goal of research in machine learning is to identify the largest possible class of concepts that are learnable from examples." This goal is also declared (although less explicitly) in many papers in the related literature (e.g., <ref> [Valiant 84] </ref>, [Natarajan 87], [COLT 88]-[COLT 91]). Nevertheless, it is a well-known fact that learning larger classes of concepts necessarily requires a larger number of training examples [Blumer et.al. 87].
References-found: 19

