URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.ijcai93.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/craven.ijcai93.ps.abstract.html
Root-URL: 
Email: fcraven, shavlikg@cs.wisc.edu  
Title: Learning to Represent Codons: A Challenge Problem for Constructive Induction  
Author: Mark W. Craven and Jude W. Shavlik S. A. 
Address: 1210 West Dayton St. Madison, WI 53706,  
Affiliation: Computer Sciences Department University of Wisconsin  
Note: Appears in the Proceedings of the 13th International Joint Conference on Artificial Intelligence (8/93)  U.  
Abstract: The ability of an inductive learning system to find a good solution to a given problem is dependent upon the representation used for the features of the problem. Systems that perform constructive induction are able to change their representation by constructing new features. We describe an important, real-world problem finding genes in DNA that we believe offers an interesting challenge to constructive-induction researchers. We report experiments that demonstrate that: (1) two different input representations for this task result in significantly different generalization performance for both neural networks and decision trees; and (2) both neural and symbolic methods for constructive induction fail to bridge the gap between these two representations. We believe that this real-world domain provides an interesting challenge problem for constructive induction because the relationship between the two representations is well known, and because the representational shift involved in construct ing the better representation is not imposing.
Abstract-found: 1
Intro-found: 1
Reference: [ Baum and Haussler, 1989 ] <author> E. B. Baum and D. Haus-sler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: The networks that we test have 5, 10, 20 and 40 hidden units. Each hidden unit is fully-connected to the set of input units. The number of free parameters (weights + biases) in a neural network gives a rough indication of the capacity of the network <ref> [ Baum and Haussler, 1989 ] </ref> . The 40-hidden-unit networks used in this experiment have 2481 parameters.
Reference: [ Craven and Shavlik, 1993 ] <author> M. W. Craven and J. W. Shavlik. </author> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proc. of the 26th Hawaii International Conf. on System Sciences, </booktitle> <pages> pages 773-782, </pages> <address> Wailea, HI, 1993. </address> <publisher> IEEE Press. </publisher>
Reference: [ Farber et al., 1992 ] <author> R. Farber, A. Lapedes, and K. Sirotkin. </author> <title> Determination of eucaryotic protein coding regions using neural networks and information theory. </title> <journal> Journal of Molecular Biology, </journal> <volume> 226 </volume> <pages> 471-479, </pages> <year> 1992. </year>
Reference-contexts: The window is considered to be in-frame with a gene when the leftmost nucleotide in the window is the first nucleotide in a codon of the translated gene. Thus the problem involves classifying input sequences into two classes: coding and noncoding. Other researchers have also investigated neural network approaches <ref> [ Farber et al., 1992; Uberbacher and Mural, 1991 ] </ref> to the problem of finding genes in DNA sequences. 3 The Effect of Input Representation In this section we describe two different representations that can be used for DNA sequences and compare the resultant performance for these representations using both perceptrons
Reference: [ Hinton, 1989 ] <author> G. E. Hinton. </author> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 185-234, </pages> <year> 1989. </year>
Reference-contexts: For example, a network with 320 fully-connected hidden units has 19,521 parameters. Using a rule-of-thumb that recommends two training examples per parameter <ref> [ Hinton, 1989 ] </ref> , such a network would require training with more examples than is practicable. with 5, 10, 20 and 40 hidden units, as well as the nucleotides perceptrons used in the first experiment.
Reference: [ Matheus and Rendell, 1989 ] <author> C. J. Matheus and L. A. Rendell. </author> <title> Constructive induction on decision trees. </title> <booktitle> In Proc. of the Eleventh International Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 645-650, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The first approach to constructive induction involves simply adding hidden units to the networks used in the first experiment. The second approach that we investigate is citre <ref> [ Matheus and Rendell, 1989 ] </ref> , which performs constructive induction on decision trees. 2 The Problem Domain This section provides a brief description of the problem that serves as a testbed for our experiments. <p> hidden units should have sufficient capacity to form a representation of codons, it is possible that networks with more hidden units would learn such a representation. 6 Constructing Features in Trees A number of algorithms have been developed to perform feature construction using decision trees as the concept description language <ref> [ Matheus and Rendell, 1989; Pa-gallo and Haussler, 1990 ] </ref> . The citre system [ Matheus and Rendell, 1989 ] provides a general approach for constructive induction on decision trees. <p> The citre system <ref> [ Matheus and Rendell, 1989 ] </ref> provides a general approach for constructive induction on decision trees. In this section we describe an experiment in which we train decision trees using the nucleotides representation, and then use citre to construct features on these decision trees.
Reference: [ Matheus, 1990 ] <author> C. J. Matheus. </author> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: The selection process involves forming conjunctions of pairs of Boolean features. Matheus describes a number of ways in which pairs of features can be selected <ref> [ Matheus, 1990 ] </ref> . In this experiment we use the adjacent method which selects all adjacent pairs of tests that occur on decision-tree branches that lead to positively-labelled leaves. The selection process can also exploit domain knowledge to narrow the set of candidate constructed features. <p> We have also conducted this experiment using the fringe feature selection method [ Pa-gallo and Haussler, 1990 ] , and competitive evaluation of features <ref> [ Matheus, 1990 ] </ref> . However, we found that the adjacent selection method and information-based evaluation provided the best results. Further experimentation needs to be conducted to explore the effects of using domain knowledge, feature-generalization operators, and more iterations of feature construction.
Reference: [ Michalski, 1983 ] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction The ability of an inductive learning system to find a good solution to a given problem is dependent upon the representation used for the features of the problem. Work in constructive induction <ref> [ Michalski, 1983 ] </ref> has focused on methods for constructing new features, thereby changing the problem representation [ Matheus and Rendell, 1989; Matheus, 1990; Pagallo and Haussler, 1990 ] .
Reference: [ Pagallo and Haussler, 1990 ] <author> G. Pagallo and D. Haus-sler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference: [ Quinlan, 1993 ] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Figure 2 and Figure 3 depict artificial neural nets using the nucleotides and codons representations respectively. In order to evaluate the effect of input representation for the problem of recognizing genes in DNA, we construct generalization curves (described below) for both representations using the C4.5 decision tree algorithm <ref> [ Quinlan, 1993 ] </ref> , and perceptrons [ Rosenblatt, 1958 ] . A line shows the observed generalization curve for the nucleotides representation. The dashed line shows the generalization curve for the codons representation. <p> Members of the tuning set are not presented to the network as ordinary training examples, but instead are used during learning to estimate the accuracy of the network on unseen examples. Pessimistic pruning <ref> [ Quinlan, 1993 ] </ref> is used on the trained decision trees to help avoid overfitting. perceptrons using the nucleotides and codons input representations. Figure 5 shows the observed generalization curves for decision trees using both representations.
Reference: [ Rosenblatt, 1958 ] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65(6), </volume> <year> 1958. </year>
Reference-contexts: In order to evaluate the effect of input representation for the problem of recognizing genes in DNA, we construct generalization curves (described below) for both representations using the C4.5 decision tree algorithm [ Quinlan, 1993 ] , and perceptrons <ref> [ Rosenblatt, 1958 ] </ref> . A line shows the observed generalization curve for the nucleotides representation. The dashed line shows the generalization curve for the codons representation. Generalization error is the percentage of misclassified test-set examples. perceptron is a neural network with no hidden units.
Reference: [ Uberbacher and Mural, 1991 ] <author> E. C. Uberbacher and R. J. </author> <title> Mural. Locating protein coding regions in human DNA sequences by a multiple sensor neural network approach. </title> <booktitle> Proc. of the National Academy of Sciences, </booktitle> <volume> 88 </volume> <pages> 11261-11265, </pages> <year> 1991. </year> <title> 3 We plan to make this data set publicly available through the UC-Irvine Repository of Machine Learning Databases and Domain Theories. </title> <note> This database may be accessed by doing an anonymous ftp to ftp.ics.uci.edu. </note>
Reference-contexts: The window is considered to be in-frame with a gene when the leftmost nucleotide in the window is the first nucleotide in a codon of the translated gene. Thus the problem involves classifying input sequences into two classes: coding and noncoding. Other researchers have also investigated neural network approaches <ref> [ Farber et al., 1992; Uberbacher and Mural, 1991 ] </ref> to the problem of finding genes in DNA sequences. 3 The Effect of Input Representation In this section we describe two different representations that can be used for DNA sequences and compare the resultant performance for these representations using both perceptrons
References-found: 11

