URL: http://www.cs.toronto.edu/~frey/stsc.ps
Refering-URL: http://www.cs.toronto.edu/~frey/index.html
Root-URL: 
Title: Efficient Stochastic Source Coding and an Application to a Bayesian Network Source Model  
Author: Brendan J. Frey Geoffrey E. Hinton 
Date: December 15, 1997  
Address: 6 King's College Road Toronto, Canada, M5S 3H5  
Affiliation: Department of Computer Science University of Toronto  
Abstract: Brendan J. Frey and Geoffrey E. Hinton 1997. Efficient stochastic source coding and an application to a Bayesian network source model. The Computer Journal 40, 157-165. In this paper, we introduce a new algorithm called "bits-back coding" that makes stochastic source codes efficient. For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword. Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters | maximum likelihood estimation | actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum likelihood estimation | the generalized expectation maximization algorithm | minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding. We illustrate the performance of bits-back coding using using nonsynthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L. E., and Petrie, T. </author> <year> (1966). </year> <title> Statistical inference for probabilistic functions of finite state Markov chains. </title> <journal> Annals of Mathematical Statistics 37, </journal> <pages> 1559-1563. </pages>
Reference-contexts: Details of HMM operation and parameter estimation are not given here; see Rabiner 1989 for a tutorial exposition. It is sufficient to note that the model parameters can be estimated from a data set using a maximum likelihood estimation technique called the Baum-Welch algorithm <ref> (Baum and Petrie 1966) </ref> and also that for a given symbol sequence the model can be used to produce a codeword that represents both the state sequence y of the HMM and the symbol sequence x.
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <year> (1995). </year> <title> The Helmholtz machine. </title> <booktitle> Neural Computation 7, </booktitle> <pages> 889-904. </pages>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incom-plete data via the EM algorithm (with discussion). </title> <journal> Journal of the Royal Statistical Society B 39, </journal> <pages> 1-38. </pages>
Reference: <author> Frey, B. J., Dayan, P., and Hinton, G. E. </author> <year> (1997). </year> <title> A simple algorithm that discovers effi-cient perceptual codes. </title> <editor> In M. Jenkin and L. R. Harris (eds), </editor> <title> Computational and biological mechanisms of visual coding, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science 268, </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E., and Zemel, R. S. </author> <year> (1994). </year> <title> Autoencoders, minimum description length, and 17 Helmholtz free energy. </title> <editor> In J. K. Cowan, G. Tesauro and J. Alspector (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address>
Reference: <author> Huffman, D. A. </author> <year> (1952). </year> <title> A method for the construction of minimum redundancy codes. </title> <booktitle> Proceedings of the Institute of Radio Engineers 40, </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: If the codeword selection distribution is dyadic 1 , it turns out that Huffman decoding <ref> (Huffman 1952) </ref> can be used to pick codewords. Here, we consider the case of an arbitrary codeword selection distribution.
Reference: <author> Meng, X. L., and Rubin, D. B. </author> <year> (1992). </year> <title> Recent extensions of the EM algorithm (with discussion). </title> <editor> In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (eds), </editor> <booktitle> Bayesian Statistics 4. </booktitle> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Moffat, A., Neal, R. M., and Witten I. H. </author> <year> (1995). </year> <title> Arithmetic coding revisited. </title> <editor> In J. </editor> <publisher> A. </publisher>
Reference: <editor> Storer and M. Cohn (eds), </editor> <booktitle> Proceedings of the Data Compression Conference 1995, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos. </publisher>
Reference: <author> Neal, R. M., and Hinton, G. E. </author> <year> (1993). </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript. </type> <note> Available over the internet by ftp at ftp://ftp.cs.utoronto.ca/pub/radford/em.ps.Z. </note>
Reference-contexts: Often, maximum likelihood estimation is not tractable because the computation of Q fl (yjx) is intractable. In these cases, it is possible to use an approximation to maximum likelihood estimation which can be thought of as a generalization of the EM algorithm <ref> (Neal and Hinton 1993) </ref>. This method approximates Q fl (yjx) with a more tractable distribution ^ Q (yjx), which is continuously updated so as to be close (in the sense of relative entropy) to Q fl (yjx).
Reference: <author> Rabiner, L. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE 77, </booktitle> <pages> 257-286. </pages>
Reference: <author> Rissanen, J., and Langdon, G. G. </author> <year> (1976). </year> <title> Arithmetic coding. </title> <journal> IBM Journal of Research and Development 23, </journal> <pages> 149-162. </pages>
Reference: <author> Thompson, C. J. </author> <year> (1988). </year> <title> Classical Equilibrium Statistical Mechanics. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: the distribution used to select codewords: H (x) y The difference between equations 2 and 3 gives the effective average codeword length F (x) that bits-back coding can achieve: F (x) E (x) H (x): (4) Since this quantity is analogous to the variational Helmholtz free energy from statistical physics <ref> (see textbook reference Thompson 1988) </ref>, we refer to the effective average codeword length as the free energy. Bits-back coding makes gains over shortest codeword selection by taking into account the existence of multiple codewords of similar length.
Reference: <author> Viterbi, A. J., and Omura, J. K. </author> <year> (1979). </year> <title> Principles of Digital Communication and Coding. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: An implementational note: the bits-back coding software yields rates that are indistinguishable from the theoretical free energies for the corresponding models, to at least one decimal place. 2 To find the shortest codeword for a given string, we use the Viterbi algorithm <ref> (Viterbi and Omura 1979) </ref>. 10 Table 1: Rate comparisons for software-implemented source codes on the HMM data.
Reference: <author> Wallace, C. S. </author> <year> (1992). </year> <title> Classification by minimum-message-length inference. </title> <editor> In S. G. Akl et. al. (eds), </editor> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <volume> 468. </volume> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Witten, I. H., Neal R. M., and Cleary J. G. </author> <year> (1987). </year> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM 30, </journal> <pages> 520-540. 18 </pages>
References-found: 17

