URL: http://www.isi.edu/~mhall/sc95.ps
Refering-URL: http://www.isi.edu/~mhall/mypapers.html
Root-URL: http://www.isi.edu
Title: Detecting Coarse-Grain Parallelism Using an Interprocedural Parallelizing Compiler detecting coarser granularity of parallelism than previously
Author: Mary W. Hall Saman P. Amarasinghe Brian R. Murphy Shih-Wei Liao Monica S. Lam 
Note: Experimentation with this system shows that it is capable of  
Address: Stanford, CA 94305  Pasadena, CA 91125  
Affiliation: Stanford University, Computer Systems Laboratory  yCalifornia Institute of Technology Computer Science Dept. 256-80  
Abstract: This paper presents an extensive empirical evaluation of an interprocedural parallelizing compiler, developed as part of the Stanford SUIF compiler system. The system incorporates a comprehensive and integrated collection of analyses, including priva-tization and reduction recognition for both array and scalar variables, and symbolic analysis of array subscripts. The interproce-dural analysis framework is designed to provide analysis results nearly as precise as full inlining but without its associated costs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Specifically, the full SUIF system incorporates data and loop transformations to increase the granularity of parallelism and to improve the memory behavior of the programs <ref> [1, 27] </ref> and optimizations to eliminate unnecessary synchronization [25]. In this paper, however, we adopt a very simple parallel code generation strategy that does not include these optimizations in order to focus on the effects of the parallelization analysis. <p> Another important factor that affects speedups is data locality. Two of these programs, tomcatv and nasa7, have poor memory behavior. The performance of these programs can be improved significantly via data and loop transformations to improve cache locality <ref> [1] </ref> and techniques to minimize synchronization [25]. Nas Benchmarks. The advanced array analyses in SUIF are important to the successful parallelization of the Nas benchmarks, as can be seen in Figure 5 (B2-D2). Comparing SUIF with the baseline system, we observe that the array analyses have two important effects.
Reference: [2] <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, Jay Hoeflinger, D. Padua, P. Pe-tersen, B. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The Polaris system at University of Illinois is also currently being developed to advance the state of the art in parallelization technology <ref> [2] </ref>. The most fundamental difference between our system and Polaris is that Polaris performs no interprocedural analysis, instead relying on full inlining of the programs to obtain interprocedural information.
Reference: [3] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: A parallelizing compiler that automatically locates parallel computations in sequential programs is a particularly attractive programming tool, as it frees programmers from the difficult task of explicitly managing parallelism in their programs. Unfortunately, today's commercially available parallelizing compilers are not effective at getting good performance on multiprocessors <ref> [3, 23] </ref>. As these parallelizers were developed from vectorizing compiler technology, they tend to be successful in parallelizing only innermost loops. Parallelizing just inner loops is not adequate for multiprocessors for two reasons. <p> For example, it is very common for each iteration of a loop to define and use the same variable. The compiler must give each processor a private copy of the variable for the loop to be parallelizable <ref> [3, 23] </ref>. As another example, a compiler can parallelize a reduction (e.g., computation of a sum, product, or maximum over data elements) by having each processor compute a partial reduction locally and update the global result at the end. Compilers traditionally only perform privatization or reduction tranformations to scalar variables. <p> Interprocedural Analysis. If programs are written in a modular style, it is natural that coarse-grain parallel loops will span multiple procedures. For this reason, procedure boundaries must not pose a barrier to analysis <ref> [3] </ref>. One way to eliminate procedure boundaries is to perform inline substitution | replacing each procedure call by a copy of the called procedure | and perform program analysis in the usual way.
Reference: [4] <author> W. Blume and R. Eigenmann. </author> <title> The range test: A dependence test for symbolic, non-linear expressions. </title> <booktitle> In Proceedings of Supercomputing '94. </booktitle> <publisher> IEEE Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: This simple approach captures what is required to analyze many non-linear subscript expressions without the need to adopt sophisticated non-linear dependence tests <ref> [4, 19] </ref>.
Reference: [5] <author> K. Cooper, M.W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2), </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: To avoid this loss of precision, we incorporate into the calling context analysis a technique called selective procedure cloning, where the compiler replicates analysis results for a procedure to determine its data-flow information along paths in the program that contribute significantly different calling contexts <ref> [5] </ref>. Because the replication is done selectively according to the unique data-flow information it exposes, we manage the analysis costs and can usually obtain the same precision as full inlining. Interprocedural Framework.
Reference: [6] <author> B. Creusillet and F. Irigoin. </author> <title> Interprocedural array region analyses. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: This strategy for transforming summaries across procedure boundaries provides a general mechanism for analyzing array reshapes, where the number or size of array dimensions are altered at a call. A similar approach to array reshapes has also recently be adopted by Creussilet <ref> [6] </ref>. Array Data-Flow Analysis. A single array data-flow analysis is used to determine arrays involved in data dependences, to locate pri-vatizable arrays and to recognize reductions. Array data-flow analysis is a bottom-up interprocedural analysis on the loops and procedures of the program, using the region-based analysis framework described above. <p> Irigoin et al. have developed the PIPS system, an interprocedural analysis system that is part of an environment for parallel programming [16]. More recently, PIPS has been extended to incorporate inter-procedural array privatization <ref> [15, 6] </ref>. PIPS is the most similar to our work, but lacks three important features: (1) path-specific interproce-dural information such as obtained through selective procedure cloning, (2) interprocedural reductions, and (3) extensive interprocedural scalar data-flow analysis such as scalar privatization.
Reference: [7] <author> G. Goff, K. Kennedy, and C.-W. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Within this domain, data dependence analysis has been shown to be equivalent to integer programming. While integer programming is potentially expensive, the data dependence problems found in practice are simple, and efficient algorithms have been developed that usually solve these problems exactly <ref> [7, 20] </ref>. For this reason, parallelizing compilers incorporate a host of scalar symbolic analyses to put array indices in affine form, including constant propagation, value numbering, and induction variable recognition. These analyses provide integer coefficients for subscript variables and derive affine equality relationships among variables.
Reference: [8] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic analysis: A basis for paral-lelization, optimization, and scheduling of programs. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Although much research has been devoted to interprocedural analysis for paral-lelization <ref> [8, 12, 13, 15, 18] </ref>, it has not been adopted in practice. The primary obstacle to progress in this area has been that effective interprocedural compilers are substantially harder to build than their intraprocedural counterparts. Moreover, there is an inherent tradeoff between performing analysis efficiently and obtaining precise results.
Reference: [9] <author> M. Hall, B. Murphy, S. Amarasinghe, S. Liao, and M. Lam. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: For the interprocedural paralleliza-tion system in SUIF, we have extended Fiat significantly to support array data-flow analysis and flow-sensitive analysis. 4 Parallelization Analysis Algorithms This section overviews the parallelization analysis algorithms and describes how the different phases of the analysis fit together. Further description can be found elsewhere <ref> [9, 10] </ref>. 4.1 Scalar Analysis Our system has interprocedural scalar analysis that encompasses both scalar parallelization analysis and scalar symbolic analysis. For the scalar parallelization analysis, simple flow-insensitive analysis|interprocedural analysis that does not consider control flow within a procedure|provides the information to locate scalar dependences and scalar reductions. <p> When considering only those loops containing calls for this set of 16 programs, the SUIF system is able to parallelize greater than five times more of these loops <ref> [9] </ref>. The key difference between the two systems is that SUIF contains full interprocedural array analysis, including array privatization and reduction recognition (see Section 5). Static loop counts, however, are not good indicators of whether par-allelization will be successful.
Reference: [10] <author> M. W. Hall, S. Amarasinghe, and B. Murphy. </author> <title> Interprocedural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 650-655, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: For the interprocedural paralleliza-tion system in SUIF, we have extended Fiat significantly to support array data-flow analysis and flow-sensitive analysis. 4 Parallelization Analysis Algorithms This section overviews the parallelization analysis algorithms and describes how the different phases of the analysis fit together. Further description can be found elsewhere <ref> [9, 10] </ref>. 4.1 Scalar Analysis Our system has interprocedural scalar analysis that encompasses both scalar parallelization analysis and scalar symbolic analysis. For the scalar parallelization analysis, simple flow-insensitive analysis|interprocedural analysis that does not consider control flow within a procedure|provides the information to locate scalar dependences and scalar reductions.
Reference: [11] <author> M. W. Hall, J. Mellor-Crummey, A. Carle, and R. Rodriguez. FIAT: </author> <title> A framework for interprocedural analysis and transformation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Port-land, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The region-based analysis and selective procedure cloning techniques are encapsulated in a common interprocedural framework as part of Fiat, a tool for developing interprocedural analysis systems <ref> [11] </ref>. Fiat facilitates adding 8 new interprocedural analyses by providing parameterized templates to drive flow-sensitive analysis and cloning; each analysis problem is implemented by instantiating the templates with functions to compute solutions to data-flow equations.
Reference: [12] <author> P. Havlak. </author> <title> Interprocedural symbolic analysis. </title> <type> PhD thesis, </type> <institution> Rice University, Dept. of Computer Science, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Although much research has been devoted to interprocedural analysis for paral-lelization <ref> [8, 12, 13, 15, 18] </ref>, it has not been adopted in practice. The primary obstacle to progress in this area has been that effective interprocedural compilers are substantially harder to build than their intraprocedural counterparts. Moreover, there is an inherent tradeoff between performing analysis efficiently and obtaining precise results. <p> These analyses provide integer coefficients for subscript variables and derive affine equality relationships among variables. Some systems also propagate inequality relations and other relational constraints on integer variables imposed by surrounding code constructs (IFs and loops) to their uses in array subscripts <ref> [12, 15] </ref>. Relations on Non-Linear Variables. Propagating only affine relations among scalars is not sufficient to parallelize some loops. For example, scientific codes often linearize accesses to (conceptually) multidimensional arrays, resulting in subscript expressions that cannot be expressed as an affine function of the enclosing loop indices.
Reference: [13] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Although much research has been devoted to interprocedural analysis for paral-lelization <ref> [8, 12, 13, 15, 18] </ref>, it has not been adopted in practice. The primary obstacle to progress in this area has been that effective interprocedural compilers are substantially harder to build than their intraprocedural counterparts. Moreover, there is an inherent tradeoff between performing analysis efficiently and obtaining precise results. <p> Scalar modifications, references and reductions are performed in an initial flow-insensitive pass; these analyses could fold into the next pass, but a flow-insensitive implementation can be performed more efficiently. 5 Related Work In the late 1980s, a series of papers presented results on interprocedu-ral parallelization analysis <ref> [13, 18, 24] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations. <p> This section provides an empirical evaluation of the results of the paralleliza-tion analysis on a large collection of benchmark programs. Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops paral-lelized as a result of interprocedural analysis <ref> [13, 14, 18, 24] </ref>. We have compared our results with the most recent of these empirical studies, which examines the Spec89 and Perfect benchmark suites [14].
Reference: [14] <author> M. Hind, M. Burke, P. Carini, and S. Midkiff. </author> <title> An empirical study of precise interprocedural array analysis. </title> <journal> Scientific Programming, </journal> <volume> 3(3) </volume> <pages> 255-271, </pages> <year> 1994. </year>
Reference-contexts: These techniques were shown to be effective in parallelizing linear algebra libraries. More recently, the Fida system was developed at IBM to obtain more precise array sections through partial inlining of array accesses <ref> [14] </ref> (see 11 1. Flow-insensitive pass: * Find modified and referenced variables * Find scalar reductions 2. Bottom-up pass: scalar analysis * Find privatizable scalars * Summarize symbolic behaviors (side-effects) 3. <p> This section provides an empirical evaluation of the results of the paralleliza-tion analysis on a large collection of benchmark programs. Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops paral-lelized as a result of interprocedural analysis <ref> [13, 14, 18, 24] </ref>. We have compared our results with the most recent of these empirical studies, which examines the Spec89 and Perfect benchmark suites [14]. <p> Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops paral-lelized as a result of interprocedural analysis [13, 14, 18, 24]. We have compared our results with the most recent of these empirical studies, which examines the Spec89 and Perfect benchmark suites <ref> [14] </ref>. When considering only those loops containing calls for this set of 16 programs, the SUIF system is able to parallelize greater than five times more of these loops [9].
Reference: [15] <author> F. Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In NSF-CNRS Workshop on Evironments and Tools for Parallel Scientific Programming, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Although much research has been devoted to interprocedural analysis for paral-lelization <ref> [8, 12, 13, 15, 18] </ref>, it has not been adopted in practice. The primary obstacle to progress in this area has been that effective interprocedural compilers are substantially harder to build than their intraprocedural counterparts. Moreover, there is an inherent tradeoff between performing analysis efficiently and obtaining precise results. <p> These analyses provide integer coefficients for subscript variables and derive affine equality relationships among variables. Some systems also propagate inequality relations and other relational constraints on integer variables imposed by surrounding code constructs (IFs and loops) to their uses in array subscripts <ref> [12, 15] </ref>. Relations on Non-Linear Variables. Propagating only affine relations among scalars is not sufficient to parallelize some loops. For example, scientific codes often linearize accesses to (conceptually) multidimensional arrays, resulting in subscript expressions that cannot be expressed as an affine function of the enclosing loop indices. <p> Irigoin et al. have developed the PIPS system, an interprocedural analysis system that is part of an environment for parallel programming [16]. More recently, PIPS has been extended to incorporate inter-procedural array privatization <ref> [15, 6] </ref>. PIPS is the most similar to our work, but lacks three important features: (1) path-specific interproce-dural information such as obtained through selective procedure cloning, (2) interprocedural reductions, and (3) extensive interprocedural scalar data-flow analysis such as scalar privatization.
Reference: [16] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural paralleliza-tion: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Irigoin et al. have developed the PIPS system, an interprocedural analysis system that is part of an environment for parallel programming <ref> [16] </ref>. More recently, PIPS has been extended to incorporate inter-procedural array privatization [15, 6].
Reference: [17] <author> W. Landi and B.G. Ryder. </author> <title> A safe approximate algorithm for interprocedural pointer aliasing. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, SIGPLAN Notices 27(7), </booktitle> <pages> pages 235-248, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Iterative analysis over this structure will be slow because the number of control paths through which information flows can be very large. Such analysis also loses precision by propagating information along unrealizable paths <ref> [17] </ref>; the analysis may propagate calling context information from one caller through a procedure and return the side-effect information to a different caller. Region-Based Flow-Sensitive Analysis. In our system, we use a region-based analysis that solves the problems of unrealizable paths and slow convergence.
Reference: [18] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year> <month> 25 </month>
Reference-contexts: Although much research has been devoted to interprocedural analysis for paral-lelization <ref> [8, 12, 13, 15, 18] </ref>, it has not been adopted in practice. The primary obstacle to progress in this area has been that effective interprocedural compilers are substantially harder to build than their intraprocedural counterparts. Moreover, there is an inherent tradeoff between performing analysis efficiently and obtaining precise results. <p> Scalar modifications, references and reductions are performed in an initial flow-insensitive pass; these analyses could fold into the next pass, but a flow-insensitive implementation can be performed more efficiently. 5 Related Work In the late 1980s, a series of papers presented results on interprocedu-ral parallelization analysis <ref> [13, 18, 24] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations. <p> This section provides an empirical evaluation of the results of the paralleliza-tion analysis on a large collection of benchmark programs. Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops paral-lelized as a result of interprocedural analysis <ref> [13, 14, 18, 24] </ref>. We have compared our results with the most recent of these empirical studies, which examines the Spec89 and Perfect benchmark suites [14].
Reference: [19] <author> V. Maslov. Delinearization: </author> <title> An efficient way to break multiloop dependence equations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: This simple approach captures what is required to analyze many non-linear subscript expressions without the need to adopt sophisticated non-linear dependence tests <ref> [4, 19] </ref>.
Reference: [20] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Within this domain, data dependence analysis has been shown to be equivalent to integer programming. While integer programming is potentially expensive, the data dependence problems found in practice are simple, and efficient algorithms have been developed that usually solve these problems exactly <ref> [7, 20] </ref>. For this reason, parallelizing compilers incorporate a host of scalar symbolic analyses to put array indices in affine form, including constant propagation, value numbering, and induction variable recognition. These analyses provide integer coefficients for subscript variables and derive affine equality relationships among variables.
Reference: [21] <author> R. Metzger and P. Smith. </author> <title> The CONVEX application compiler. </title> <journal> Fortran Journal, </journal> <volume> 3(1) </volume> <pages> 8-10, </pages> <year> 1991. </year>
Reference-contexts: A few commercial parallelizing compilers have initial interprocedu-ral analysis systems. Most notably, the Convex Applications Compiler performs flow-insensitive array analysis and interprocedural constant 12 propagation and obtains some path-specific information through inlin--ing and procedure cloning <ref> [21] </ref>. Applied Parallel Research has demonstrated good speedup results on some of the programs presented here; these programs were parallelized with programmer directives that instruct the compiler to ignore dependences and to privatize certain variables.
Reference: [22] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1981. </year>
Reference-contexts: For example, in a straightforward interprocedural adaptation of traditional iterative analysis, analysis might be carried out over a program representation called the supergraph <ref> [22] </ref>, where individual control flow graphs for the procedures in the program are linked together at pro 7 cedure call and return points. Iterative analysis over this structure will be slow because the number of control paths through which information flows can be very large.
Reference: [23] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A parallelizing compiler that automatically locates parallel computations in sequential programs is a particularly attractive programming tool, as it frees programmers from the difficult task of explicitly managing parallelism in their programs. Unfortunately, today's commercially available parallelizing compilers are not effective at getting good performance on multiprocessors <ref> [3, 23] </ref>. As these parallelizers were developed from vectorizing compiler technology, they tend to be successful in parallelizing only innermost loops. Parallelizing just inner loops is not adequate for multiprocessors for two reasons. <p> For example, it is very common for each iteration of a loop to define and use the same variable. The compiler must give each processor a private copy of the variable for the loop to be parallelizable <ref> [3, 23] </ref>. As another example, a compiler can parallelize a reduction (e.g., computation of a sum, product, or maximum over data elements) by having each processor compute a partial reduction locally and update the global result at the end. Compilers traditionally only perform privatization or reduction tranformations to scalar variables.
Reference: [24] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 176-185. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: Scalar modifications, references and reductions are performed in an initial flow-insensitive pass; these analyses could fold into the next pass, but a flow-insensitive implementation can be performed more efficiently. 5 Related Work In the late 1980s, a series of papers presented results on interprocedu-ral parallelization analysis <ref> [13, 18, 24] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations. <p> This section provides an empirical evaluation of the results of the paralleliza-tion analysis on a large collection of benchmark programs. Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops paral-lelized as a result of interprocedural analysis <ref> [13, 14, 18, 24] </ref>. We have compared our results with the most recent of these empirical studies, which examines the Spec89 and Perfect benchmark suites [14].
Reference: [25] <author> C-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Specifically, the full SUIF system incorporates data and loop transformations to increase the granularity of parallelism and to improve the memory behavior of the programs [1, 27] and optimizations to eliminate unnecessary synchronization <ref> [25] </ref>. In this paper, however, we adopt a very simple parallel code generation strategy that does not include these optimizations in order to focus on the effects of the parallelization analysis. The compiler parallelizes the outermost loop that the analysis has proven to be parallelizable. <p> Another important factor that affects speedups is data locality. Two of these programs, tomcatv and nasa7, have poor memory behavior. The performance of these programs can be improved significantly via data and loop transformations to improve cache locality [1] and techniques to minimize synchronization <ref> [25] </ref>. Nas Benchmarks. The advanced array analyses in SUIF are important to the successful parallelization of the Nas benchmarks, as can be seen in Figure 5 (B2-D2). Comparing SUIF with the baseline system, we observe that the array analyses have two important effects.
Reference: [26] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Some arrays involved in data dependences may yield to privatiza--tion. An array can be privatized if the Write set and the ExposedRead set are disjoint. Our array privatization analysis is an extension of Tu and Padua's approach <ref> [26] </ref>. Their algorithm requires that a privatiz-able array have no read locations upwards-exposed to the beginning of a loop iteration. Our approach is more general, capturing cases such as the one in Figure 2 (c). Array Reductions.
Reference: [27] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> August </month> <year> 1992. </year> <month> 26 </month>
Reference-contexts: Specifically, the full SUIF system incorporates data and loop transformations to increase the granularity of parallelism and to improve the memory behavior of the programs <ref> [1, 27] </ref> and optimizations to eliminate unnecessary synchronization [25]. In this paper, however, we adopt a very simple parallel code generation strategy that does not include these optimizations in order to focus on the effects of the parallelization analysis.
References-found: 27

