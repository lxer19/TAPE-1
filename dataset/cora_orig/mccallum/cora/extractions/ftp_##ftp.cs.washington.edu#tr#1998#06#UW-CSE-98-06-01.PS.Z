URL: ftp://ftp.cs.washington.edu/tr/1998/06/UW-CSE-98-06-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: fselberg, etzionig@cs.washington.edu  http://huskysearch.cs.washington.edu  
Title: Experiments with Collaborative Index Enhancement  
Author: Erik Selberg Oren Etzioni 
Date: January 28, 1997  
Web: http://www.cs.washington.edu/homes/fselberg, etzionig  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report UW-CSE-98-06-01  
Abstract: In this paper we present a method for improving the quality of a searchable index: Collaborative Index Enhancement, or CIE. CIE improves the index quality by feeding the results of searches over an index back into that index, so that the current user can harness the effort and insight of previous users that searched for the same or similar information. We present four prototype enhanced indices implemented as part of the HuskySearch system, a World Wide Web (WWW) search service available publicly. We describe the implementation and tradeoffs of each system, and present experiments based on user studies and log analyses detailing their effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mark S. Ackerman, Brian Starr, and Michael Pazzani. </author> <title> The Do-I-Care Agent: Effective Social Discovery and Filtering on the Web. </title> <booktitle> In RIAO '97: Computer-Assisted Information Searching on the Internet, </booktitle> <pages> pages 17-31, </pages> <address> Montreal, CA, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: These systems generally work by having a user with some information need match his or her user profile to other users' profiles. Once matched, the system then extrapolates relevant information from the experiences or preferences of those users. Recent collaboration systems, such as the Do-I-Care system <ref> [1] </ref> or Ringo [28], showcase the usefulness of this approach. However, these systems are typically geared towards a small group or a narrow domain. This paper explores a combined approach called Collaborative Index Enhancement, or CIE.
Reference: [2] <author> N. J. Belkin, P. Kantor, C. Cool, and R. Quatrain. </author> <title> Combining Evidence for Information Retrieval. </title> <editor> In Donna Harman, editor, TREC-2, </editor> <booktitle> Proc. of the Second Text REtrieval Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The ColRes and ColResSelect indices are used in two fashions. The first is to give potentially relevant documents a second evaluation on the given query using a different retrieval method, a technique that has been shown to be productive using traditional IR benchmarks <ref> [2] </ref>. It is possible that after a million or more queries that ColRes and ColResSelect could become additional global WWW search indices like the other services HuskySearch queries, but the intention is to keep ColRes and ColResSelect small with docu 3 ments that are likely to be relevant.
Reference: [3] <author> T. L. Brauen. </author> <title> Document Vector Modifications in the SMART Retrieval System. In The SMART Retrieval System: Experiments in Automatic Document Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ:, </address> <year> 1971. </year>
Reference-contexts: We will release this corpus publicly and make it initially available via the HuskySearch web site. 5 Related Work Brauen reported his experiments on using user interaction to modify a searchable index <ref> [3] </ref>. He used the SMART [25] system as his testbed, and enhanced his index by directly modifying the document vectors in the index. He evaluated enhancing documents that appear relevant as well as documents that appeared irrelevant, in order to better separate the relevant documents in subsequent queries.
Reference: [4] <author> James Callan, Zhihong Lu, and Bruce Croft. </author> <title> Searching Distributed Collections with Inference Networks. </title> <booktitle> In Proc. of the 1995 SIGIR Conference, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: If we believed that each ranking algorithm was accurate or we knew that all services used the same ranking algorithm, the distribution step would not be needed. In fact, other fusion algorithms, such as those used to merge TREC results <ref> [4] </ref>, would likely perform much better than the NDS algorithm were this the case. We do not yet have either a formal or experimental justification for this algorithm, but it does work well in practice.
Reference: [5] <institution> Digital Equipment Corporation. Alta Vista Home Page. </institution> <note> URL: http://www.altavista.digital.com. </note>
Reference-contexts: 1 Introduction Although WWW information retrieval (IR) systems have proven to be useful to a worldwide audience, there are still several issues that need to be addressed. Current "universal" WWW IR systems that attempt to index every available WWW page, such as Excite [9] or AltaVista <ref> [5] </ref>, use spiders to retrieve pages for their index [20]. Unfortunately, network resources restrict the rate spiders can retrieve pages. <p> The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista <ref> [5] </ref>, Excite [9], HotBot [17], Northern Light [21], PlanetSearch [23], WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of
Reference: [6] <author> Daniel Dreilinger. </author> <title> Integrating Heterogeneous WWW Search Engines. </title> <address> URL: ftp://132.239.54.5/savvy /report.ps.gz, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We present experiments measuring the improvements in Section 3. Future and related work are discussed in Sections 4 and 5, and Section 6 concludes. 2 CIE Implementation The CIE prototype is implemented as part of Husky-Search, a WWW metasearch service similar to MetaCrawler [27] and SavvySearch <ref> [6] </ref>. HuskySearch takes a user query and forwards it to a number of WWW search services in parallel, reformatting the query as appropriate.
Reference: [7] <author> Efthimis Efthimiadis. </author> <title> Interactive Query Expansion and Relevance Feedback for Document Retrieval Systems. </title> <type> PhD thesis, </type> <institution> City University, </institution> <address> London, </address> <year> 1992. </year>
Reference-contexts: Query modifiers, such as searching for "All these words" or "the Person" are not counted as separate terms, but are used to determine uniqueness. 3.2.2 Confidence Previous user studies have noticed discrepancies between the recall users thought they achieved versus the recall they had actually achieved <ref> [7, 30] </ref>. We wanted to see whether users had a realistic picture of the correctness of the answer they found, as well as whether or not the CIE system improved or degraded that judgment.
Reference: [8] <author> Efthimis N. Efthimiadis. </author> <title> Query Expansion. </title> <editor> In Martha E. Williams, editor, ARIST, </editor> <volume> volume 31, </volume> <booktitle> chapter 4. Information Today, </booktitle> <publisher> Inc., </publisher> <address> Medford, NJ, </address> <year> 1996. </year>
Reference-contexts: The ColList and ColListSelect indices are used to implement a form of convenient query expansion <ref> [8] </ref>. The snippets contained within a results document tend to highlight the relevant key terms for the referenced document. By matching a query to those key terms, users can be directed to a previous user's query with different results that might be more appropriate to the question at hand.
Reference: [9] <institution> Excite, Inc. Excite Home Page. </institution> <note> URL: http://www.excite.com. </note>
Reference-contexts: 1 Introduction Although WWW information retrieval (IR) systems have proven to be useful to a worldwide audience, there are still several issues that need to be addressed. Current "universal" WWW IR systems that attempt to index every available WWW page, such as Excite <ref> [9] </ref> or AltaVista [5], use spiders to retrieve pages for their index [20]. Unfortunately, network resources restrict the rate spiders can retrieve pages. <p> The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista [5], Excite <ref> [9] </ref>, HotBot [17], Northern Light [21], PlanetSearch [23], WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments.
Reference: [10] <author> David Filo and Jerry Yang. </author> <note> Yahoo Home Page. URL: http://www.yahoo.com. </note>
Reference: [11] <institution> FireFly Networks Inc. The FireFly Home Page. </institution> <note> URL: http://www.firefly.net. </note>
Reference-contexts: The FireFly system, derived from Ringo, demonstrates that collaborative filtering could be used in the context of selecting music based on comparing two people's music profiles and suggesting the differences in similar profiles <ref> [11] </ref>. Our work differs from these in that users do not need to enter an explicit profile prior to using the system.
Reference: [12] <author> Larry Fitzpatrick and Mei Dent. </author> <title> Automatic Feedback Using Past Queries: Social Searching? In Proc. </title> <booktitle> of the 1997 SIGIR Conference, </booktitle> <pages> pages 306-312, </pages> <address> Philadelphia, PA, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Unlike Ragha-van, we do not attempt to retrieve only optimized queries, and we take advantage of having document summaries available in the results list. Fitzpatrick and Dent explored using prior queries to expand a query automatically, and demonstrated performance improvements on TREC benchmarks <ref> [12] </ref>. Fitzpatrick and Dent's approach is complementary to ours: they modify the query to better match relevant documents, whereas we modify the document representation to better match the query. An obvious next step would be to combine both approaches.
Reference: [13] <author> Susan Gauch, Guijun Wang, and Mario Gomez. ProFusion: </author> <title> Intelligent Fusion from Multiple, Distributed Search Engines. </title> <journal> Journal of Universal Computing, </journal> <volume> 2(9), </volume> <month> Sept </month> <year> 1996. </year> <note> URL: http://www.ittc.ukans.edu/~sgauch /papers/JUCS96.ps. </note>
Reference-contexts: We do not yet have either a formal or experimental justification for this algorithm, but it does work well in practice. Some comparison with other WWW fusion algorithms <ref> [13, 15] </ref> as well as ones used in the TREC environment is definitely warranted. 2 of documents into HuskySearch, which collates these documents into a single results document containing a list of references.
Reference: [14] <author> Donna K. Harman and Ellen M. Voorhees, </author> <title> editors. </title> <booktitle> Information Technology: The Fifth Text REtrieval Conference, </booktitle> <address> Gathersburg, MD, </address> <month> nov </month> <year> 1996. </year> <institution> Dept. of Commerce, National Institute of Standards and Technology. </institution> <note> URL: http://trec.nist.gov. </note>
Reference-contexts: We have designed our system for finding relevant documents on the WWW. Since there is not yet an available standard test corpus of WWW documents, we chose to conduct experiments using the WWW rather than existing static collections of non-WWW documents, such as the TREC collections <ref> [14] </ref>. 3.1 Log Analysis It was demonstrated with MetaCrawler that meta-search services are excellent tools to evaluate how well each underlying search service compares on a number of different metrics [26].
Reference: [15] <author> Adele Howe and Daniel Dreilinger. Savvy-Search: </author> <title> A Meta-Search Engine that Learns Which Search Engines to Query. </title> <journal> AI Magazine, </journal> <volume> 18(2), </volume> <month> summer </month> <year> 1997. </year> <note> URL: http://daniel.www.media.mit.edu/people/daniel/papers/ss-aimag.ps.gz. </note>
Reference-contexts: We do not yet have either a formal or experimental justification for this algorithm, but it does work well in practice. Some comparison with other WWW fusion algorithms <ref> [13, 15] </ref> as well as ones used in the TREC environment is definitely warranted. 2 of documents into HuskySearch, which collates these documents into a single results document containing a list of references.
Reference: [16] <institution> Go2Net Inc. MetaCrawler Home Page. </institution> <note> URL: http://www.metacrawler.com. </note>
Reference-contexts: Thus universal WWW IR systems only have a subset of avail able pages, and need to make tradeoffs between the number of pages in their index and how up to date those pages are. Meta-search services that collect results from multiple services, such as MetaCrawler <ref> [16] </ref>, are able to help alleviate these problems. However there are still limits as to how much improvement meta-search can provide. Another approach being explored is to use forms of collaboration to identify and disseminate useful information on the WWW.
Reference: [17] <institution> Inktomi, Inc. HotBot Home Page. </institution> <note> URL: http://www.hotbot.com. </note>
Reference-contexts: The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista [5], Excite [9], HotBot <ref> [17] </ref>, Northern Light [21], PlanetSearch [23], WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments.
Reference: [18] <author> Karen Sparck Jones. </author> <title> Reflections on TREC. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 31(3), </volume> <year> 1995. </year>
Reference-contexts: Because we do not have any data on the number of available relevant documents on the Web, we are not able to calculate recall. Instead, we use precision at 20 documents, which is a reasonable metric for a system designed for the average user <ref> [18] </ref>. While our approximation of precision at 20 documents is not a perfect metric, we observe from the logs that users do not frequently click on many results.
Reference: [19] <author> J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl. GroupLens: </author> <title> Applying Collaborative Filtering to Usenet News. </title> <journal> Communications of the ACM, </journal> <volume> 40(3) </volume> <pages> 77-87, </pages> <year> 1997. </year> <note> URL: http://www.acm.org/pubs/citations /journals/cacm/1997-40-3/p77-konstan/. </note>
Reference-contexts: An obvious next step would be to combine both approaches. There has also been substantial work on collaborative filtering to help with resource discovery. GroupLens <ref> [19] </ref> uses collaborative filtering to recommend USENET news articles based on whether or not other users with a similar profile read a particular article.
Reference: [20] <author> Michael L. Mauldin and John R. R. Leavitt. </author> <title> Web Agent Related Research at the Center for Machine Translation. </title> <booktitle> In Proceedings of SIGNIDR V, </booktitle> <address> McLean, Virginia, </address> <month> August </month> <year> 1994. </year> <month> 11 </month>
Reference-contexts: Current "universal" WWW IR systems that attempt to index every available WWW page, such as Excite [9] or AltaVista [5], use spiders to retrieve pages for their index <ref> [20] </ref>. Unfortunately, network resources restrict the rate spiders can retrieve pages. Thus universal WWW IR systems only have a subset of avail able pages, and need to make tradeoffs between the number of pages in their index and how up to date those pages are.
Reference: [21] <institution> Northern Light Technology LLC . Northern Light Home Page. </institution> <note> URL: http://www.northernlight.com/. </note>
Reference-contexts: The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista [5], Excite [9], HotBot [17], Northern Light <ref> [21] </ref>, PlanetSearch [23], WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments.
Reference: [22] <author> Brian Pinkerton. </author> <note> WebCrawler Home Page. URL: http://webcrawler.com. </note>
Reference-contexts: The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista [5], Excite [9], HotBot [17], Northern Light [21], PlanetSearch [23], WebCrawler <ref> [22] </ref>, and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments.
Reference: [23] <institution> PlanetSearch Network Inc. PlanetSearch Home Page. </institution> <note> URL: http://www.planetsearch.com. </note>
Reference-contexts: The CIE indices used by HuskySearch use the Verity Search '97 search engine v2.0, running on a DEC AlphaStation under DEC UNIX 3.2. HuskySearch also uses Alta Vista [5], Excite [9], HotBot [17], Northern Light [21], PlanetSearch <ref> [23] </ref>, WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily [31], the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments.
Reference: [24] <author> Vijay V. Raghavan and Hayri Sever. </author> <title> On the Reuse of Past Optimal Queries. </title> <booktitle> In Proc. of the 1995 SIGIR Conference, </booktitle> <pages> pages 344-350, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: He too found that this type of enhancement leads to higher precision and recall. The CIE architecture we have presented generalizes Brauen's system, and his index enhancement algorithms would still be appropriate for an index that used document vectors in a way similar to SMART. Recently, Raghavan <ref> [24] </ref> described an elegant method of locating stored optimized queries by comparing results from a current query to the results from the optimized query. The ColList and ColListSelect indices are in part inspired by this work.
Reference: [25] <author> Gerard Salton, </author> <title> editor. The SMART Retrieval System: Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: We will release this corpus publicly and make it initially available via the HuskySearch web site. 5 Related Work Brauen reported his experiments on using user interaction to modify a searchable index [3]. He used the SMART <ref> [25] </ref> system as his testbed, and enhanced his index by directly modifying the document vectors in the index. He evaluated enhancing documents that appear relevant as well as documents that appeared irrelevant, in order to better separate the relevant documents in subsequent queries.
Reference: [26] <author> Erik Selberg and Oren Etzioni. </author> <title> Multi-Service Search and Comparison Using the MetaCrawler. </title> <booktitle> In Proc. 4th World Wide Web Conference, </booktitle> <pages> pages 195-208, </pages> <address> Boston, MA USA, </address> <month> December </month> <year> 1995. </year> <note> URL: http://huskysearch.cs.washington.edu /papers/www4/html/Overview.html. </note>
Reference-contexts: chose to conduct experiments using the WWW rather than existing static collections of non-WWW documents, such as the TREC collections [14]. 3.1 Log Analysis It was demonstrated with MetaCrawler that meta-search services are excellent tools to evaluate how well each underlying search service compares on a number of different metrics <ref> [26] </ref>. In a similar vein to the MetaCrawler analysis, we evaluate the four CIE indices in comparison to one another and the other WWW search services. The metric we use is to equate relevance with following a reference; this provides us with an approximation to true relevance.
Reference: [27] <author> Erik Selberg and Oren Etzioni. </author> <title> The Meta-Crawler Architecture for Resource Aggregation on the Web. </title> <journal> IEEE Expert, </journal> <volume> 12(1) </volume> <pages> 8-14, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: We present experiments measuring the improvements in Section 3. Future and related work are discussed in Sections 4 and 5, and Section 6 concludes. 2 CIE Implementation The CIE prototype is implemented as part of Husky-Search, a WWW metasearch service similar to MetaCrawler <ref> [27] </ref> and SavvySearch [6]. HuskySearch takes a user query and forwards it to a number of WWW search services in parallel, reformatting the query as appropriate.
Reference: [28] <editor> U. Shardanand and Pattie Maes. </editor> <title> Social Information Filtering: Algorithms for Automating `Word of Mouth'. </title> <booktitle> In Proceedings of the CHI-95 Conference, </booktitle> <address> Denver, CO, </address> <month> may </month> <year> 1995. </year>
Reference-contexts: These systems generally work by having a user with some information need match his or her user profile to other users' profiles. Once matched, the system then extrapolates relevant information from the experiences or preferences of those users. Recent collaboration systems, such as the Do-I-Care system [1] or Ringo <ref> [28] </ref>, showcase the usefulness of this approach. However, these systems are typically geared towards a small group or a narrow domain. This paper explores a combined approach called Collaborative Index Enhancement, or CIE.
Reference: [29] <author> Amanda Spink. </author> <title> Term Relevance Feedback and Query Expansion: Relation to Design. </title> <booktitle> In Proc. of the 1994 SIGIR Conference, </booktitle> <pages> pages 81-90, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: By matching a query to those key terms, users can be directed to a previous user's query with different results that might be more appropriate to the question at hand. Spink showed that the majority of terms users selected for refinement came from document title and descriptor fields <ref> [29] </ref>; we conjecture that users will also be able to make efficient use of terms other users used for their queries.
Reference: [30] <author> Louise T. Su. </author> <title> The relevance of recall and precision in user evaluation. </title> <journal> J. American Society of Information Science, </journal> <volume> 45(3) </volume> <pages> 207-217, </pages> <year> 1994. </year>
Reference-contexts: Query modifiers, such as searching for "All these words" or "the Person" are not counted as separate terms, but are used to determine uniqueness. 3.2.2 Confidence Previous user studies have noticed discrepancies between the recall users thought they achieved versus the recall they had actually achieved <ref> [7, 30] </ref>. We wanted to see whether users had a realistic picture of the correctness of the answer they found, as well as whether or not the CIE system improved or degraded that judgment.
Reference: [31] <institution> The Daily of the University of Washington. The Online Daily of the University of Washington. </institution> <note> URL: http://www.thedaily.washington.edu. </note>
Reference-contexts: HuskySearch also uses Alta Vista [5], Excite [9], HotBot [17], Northern Light [21], PlanetSearch [23], WebCrawler [22], and Yahoo![10], as well as two local intranet search services, The Daily <ref> [31] </ref>, the local student newspaper, and UWSearch, an index specific to HuskySearch using the Verity engine. 3 Experimental Validation In this section, we present two sets of experiments. The first is based upon analysis of log entries from public use of HuskySearch.
Reference: [32] <author> Howard Turtle. </author> <title> Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance. </title> <booktitle> In Proc. of the 1994 SIGIR Conference, </booktitle> <pages> pages 212-220, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: It has been shown that unstructured queries tend to be more effective for average users <ref> [32] </ref>, thus we use a rudimentary search syntax of space-separated keywords, using quotes or parentheses to denote adjacency.
Reference: [33] <author> O. Zamir, O. Etzioni, O. Madani, and R. Karp. </author> <title> Fast and Intuitive Clusturing of Web Documents. </title> <booktitle> In Proc. 3rd Int. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year> <month> 12 </month>
Reference-contexts: HuskySearch takes a user query and forwards it to a number of WWW search services in parallel, reformatting the query as appropriate. It then collates the results into a single ranked relevancy list, with an option to switch to either a sort by URL listing or a clustered listing <ref> [33] </ref>. 2.1 HuskySearch Overview One of the primary constraints we have placed on HuskySearch is that its interface needs to be simple and kept similar to other search services, so that the learning curve is minimal for the average WWW user.
References-found: 33

