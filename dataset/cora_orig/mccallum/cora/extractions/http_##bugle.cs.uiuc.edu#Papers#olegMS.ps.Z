URL: http://bugle.cs.uiuc.edu/Papers/olegMS.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Papers/olegMS.html
Root-URL: http://www.cs.uiuc.edu
Title: PERFORMANCE DATA REDUCTION USING DYNAMIC STATISTICAL CLUSTERING  
Author: BY OLEG NICKOLAYEV 
Degree: 1991 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Date: 1994  
Address: College,  1996 Urbana, Illinois  
Affiliation: M.S., Dartmouth  B.S., Moscow Institute of Physics and Technology,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Paragon System Commands Reference Manual. Intel Scalable Systems Division Technical Publications, Beaverton, </institution> <address> OR, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: A detailed picture of the application's behavior can be obtained from the analysis of performance data. Clearly, the availability of accurate and comprehensive performance data is essential for the success of this task. Several tools for performance visualization are already available. ParaGraph <ref> [1] </ref> is an example of a message passing visualization tool for the Intel Paragon XP/S supercomputer.
Reference: [2] <author> Reed, D. A. </author> <title> Performance Instrumentation Techniques for Parallel Systems. In Models and Techniques for Performance Evaluation of Computer and Communications Systems, </title> <editor> L. Donatiello and R. Nelson, Eds. </editor> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <year> 1993, </year> <pages> pp. 463-490. </pages>
Reference-contexts: Clearly, the availability of accurate and comprehensive performance data is essential for the success of this task. Several tools for performance visualization are already available. ParaGraph [1] is an example of a message passing visualization tool for the Intel Paragon XP/S supercomputer. The Pablo performance analysis environment <ref> [2] </ref> is an example of software package for capture, analysis and display of dynamic performance data across a range of parallel platforms. 1.1 Performance Characterization Techniques In this section we discuss current performance instrumentation techniques. Different techniques usually focus on specific aspects of the application's behavior. <p> In addition, more complex instrumentation techniques usually cause a larger perturbation of the underlying application and generate a larger amount of performance data. Four basic approaches to performance data capture are: timing, counting, profiling, and event tracing <ref> [2] </ref>. Below we briefly describe each instrumentation technique in order of increasing instrumentation complexity. Timing and counting are the simplest and minimally intrusive instrumentation techniques. Timing is the measurement of a system component's execution time. Implementation of this technique only requires access to the system clock.
Reference: [3] <author> Graham, S., Kessler, P., and McKusick, M. </author> <title> gprof: A Call Graph Execution Profiler. </title> <booktitle> In Proceedings of the '82 Symposium on Compiler Construction (Boston, </booktitle> <month> June </month> <year> 1982), </year> <journal> Association for Computing Machinery, </journal> <pages> pp. 120-126. </pages>
Reference-contexts: The volume of the performance data generated by a profiler is approximately the same as in the case of timing or counting. A common example of a profiler is gprof <ref> [3] </ref>. Although profiling provides a much more detailed picture of the execution behavior than timing and counting, it does not answer the questions regarding when and why the system spent its time in each component (i.e. profiling does not capture the dynamics of the application's execution).
Reference: [4] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Furthermore, it is more flexible than the other approaches each of the techniques listed above may be implemented using the event tracing mechanism. The Pablo performance analysis environment provides means for performance data capture, analysis and visual representation using event tracing <ref> [4] </ref>. Event tracing generates a record containing all relevant information for each event of interest. <p> The Pablo trace library allows the user to incorporate functions that would perform real-time statistical analysis of the performance data to reduce the volume of the data recorded <ref> [4] </ref>. This technique, however, suffers from a loss of dynamic detail of the application execution behavior. Another mechanism supported by the Pablo trace library is throttling, which allows the substitution of event tracing by less invasive technique (i.e. counting), if the event rate becomes too high [4]. <p> of the data recorded <ref> [4] </ref>. This technique, however, suffers from a loss of dynamic detail of the application execution behavior. Another mechanism supported by the Pablo trace library is throttling, which allows the substitution of event tracing by less invasive technique (i.e. counting), if the event rate becomes too high [4]. Although throttling prevents generation of extremely large data volumes, it may result in a loss of a consistent view of the application behavior (since event tracing periodically may be substituted by much less informative counting mechanism).
Reference: [5] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Defining Data Format. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> March </month> <year> 1992. </year> <month> 55 </month>
Reference-contexts: Trace records usually include a tag to indicate which event the record represents, a timestamp to indicate when the event took place, and a location (processor number) to indicate where the event took place. in ASCII form <ref> [5] </ref>. This record describes a blocking message send begin event.
Reference: [6] <author> Hollingsworth, J. K., Miller, B. P., and Cargille, J. </author> <title> Dynamic Program In--strumentation for Scalable Performance Tools. </title> <booktitle> In Scalable High Performance Computing Conference (Knoxville, </booktitle> <address> TN, </address> <month> May </month> <year> 1994). </year>
Reference-contexts: Aside from the above disadvantages, these methods may not be sufficient for systems with a large number of processors. Yet another approach is dynamic instrumentation, which dynamically enables or disables data capture during the execution of an instrumented application <ref> [6] </ref>. Although this technique allows significant performance data reduction, it fails to provide the user with a global view of the application execution behavior. 1.2.2 Statistical Data Clustering An alternative approach presented in this thesis is dynamic statistical data clustering.
Reference: [7] <author> Roth, P. C. ETRUSCA: </author> <title> Event Trace Reduction Using Statistical Data Clustering Analysis. M.S. </title> <type> Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1996. </year>
Reference-contexts: Chapter 2 describes the clustering algorithm and introduces the concepts of metric space and sliding windows used in the clustering process. In chapter 3, we describe the prototype implementation of the statistical clustering application <ref> [7] </ref>. Event trace files are processed off-line after execution of the application is complete. The results of the off-line clustering analysis are used to determine optimal values of clustering parameters, such as the size of the sliding window. The process of dynamic (on-line) statistical clustering is described in chapter 4. <p> A sliding window is a certain period of the execution time of the application. Following the terminology introduced by Roth <ref> [7] </ref>, a sliding window is defined in terms of a leading edge timestamp T l , a constant window length L, and a constant window increment I, see Figure 2.1. For each of the dimensions in the metric space, ETRUSCA maintains a corresponding sliding window. <p> We begin by giving a general description of the off-line data reduction process. 3.1 Prototype Implementation The prototype implementation of ETRUSCA is a post-mortem process involving several C++ programs originally developed by Roth <ref> [7] </ref>. The process is schematically illustrated in Figure 3.1. During the execution of a parallel application, each node produces its own trace file. As was mentioned previously, the trace files are recorded in the Pablo Self-Defining Data Format 19 (SDDF) format; see chapter 1.
Reference: [8] <author> Jain, A. K., and Dubes, R. C. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: There are two general categories of the clustering algorithms: hierarchical and partitional <ref> [8] </ref>. Hierarchical clustering algorithms determine a hierarchical relationship between clusters in a data set. The algorithm starts by assigning each data point to an individual cluster. At each step, two clusters that have the highest degree of similarity are merged together, producing a parent cluster. <p> The algorithm starts with a certain partition of data points into clusters and progresses by modifying the current working partition. At each step the data points are migrated between different clusters to obtain an optimal clustering. ETRUSCA uses the square error algorithm to search for clusters <ref> [8] </ref>. A square error algorithm is a partitional clustering algorithm that considers the Euclidean distance between two n-dimensional points as the measure of their similarity.
Reference: [9] <author> Miller, C., Payne, D. G., Phung, T., Siegel, H., and Williams, R. D. </author> <title> Parallel Processing of Spaceborn Imaging Radar Data on the Paragon. </title> <booktitle> Newsletter of the Concurrent Supercomputing Consortium, </booktitle> <pages> pp. 7-9, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: There are up to eight data channels to be processed simultaneously, which results in a very large amount of data. In the SAR code parallelism is achieved both through data distribution for processing a given channel by multiple processors and by processing up to four channels simultaneously <ref> [9] </ref>. Accordingly, nodes (0 - 31) are assigned to process channel 1, nodes (32 - 63) process channel 2 and so forth, each channel being processed by 32 processors. Computation is characterized by three phases.
Reference: [10] <author> Crandall, P. E., Chien, A. A., and Reed, D. A. </author> <title> A Trace of the Input/Output Patterns of Hartree-Fock Calculations. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: The first and the second parts calculate and write to a disk a large amount of integral data that are needed subsequently by the third part. The third part, pscf, solves a set of self-consistent (Hartree-Fock) equations using the 27 integral data calculated by the second part <ref> [10] </ref>. We have applied clustering analysis to the pscf code. Computation is characterized by two phases. During the first initial phase all processors open input files (1 - 100 seconds). These files contain the pre-computed integral data required for solution of the Hartree-Fock equations.
Reference: [11] <author> Nason, G. </author> <title> Three-Dimentional Project Pursuit. </title> <type> Tech. rep., </type> <institution> University of Bristol, Bristol, BS8 1TW, UK, </institution> <year> 1994. </year>
Reference-contexts: A proposal for future enhancement of ETRUSCA would be to implement an automatic selection of the clustering metrics. A possible approach would be to use the projection pursuit technique to produce better clusters <ref> [11] </ref>. Another direction for future work would certainly be the improvement of the clustering algorithm. The currently used square error clustering algorithm is sufficiently fast and accurate for systems of a few hundred nodes.
References-found: 11

