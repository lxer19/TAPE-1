URL: ftp://ftp.idiap.ch/pub/reports/1997/com97-05.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Email: e-mail secretariat@idiap.ch  e-mail: Perry.Moerland@idiap.ch  
Phone: phone +41 27 721 77 11 fax +41 27 721 77 12  97  
Title: C  IDIAP Martigny Valais Suisse Some Methods for Training Mixtures of Experts  
Author: M U C T O P Perry Moerland 
Date: IDIAP-Com 97-05  
Web: http://www.idiap.ch  
Note: internet  November  
Address: I  P.O.Box 592 Martigny Valais Switzerland  
Affiliation: I  for Perceptive Artificial Intelligence  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Christopher M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: This soft-max function makes that the gating network outputs sum to unity and are non-negative; thus implementing the (soft) competition between the experts. A probabilistic interpretation of a ME can be given in the context of mixture models for conditional probability distributions (see section 6.4 in <ref> [1] </ref>; again the dependence on the weights has been left implicit): p (tjx) = j=1 where the OE j represent the conditional densities of target vector t for expert j. <p> A standard way to motivate error functions is from the principle of maximum likelihood of the (independently distributed) training data with input vectors x n and target vectors t n : fx n ; t n g (see section 6.1 in <ref> [1] </ref>): L = n Y p (t n jx n )p (x n ); where dependence of p (x n ; t n ) and p (t n jx n ) on the network parameters has been left implicit. <p> For feed-forward neural networks this involves in specific the partial derivatives of the error function with respect to the network outputs (before thresholding); these derivatives (commonly denoted as ffi j ) form the basis for the back-propagation algorithm [17]. In section 6.4 of <ref> [1] </ref>, the partial derivative for the gating network with respect to its outputs have been calculated in the context of a gradient descent algorithm for the mixture model (3). <p> IDIAP-Com 97-05 4 3.1.1 Gaussian Conditional Density In section 6.4 of <ref> [1] </ref> mixture models are considered with multi-dimensional Gaussian conditional densities (where the covariance matrix is the identity matrix) as mixture components: OE j (t n jx n ) = (2) (d=2) exp 2 ; (13) where d is the dimensionality of t and w j is the set of weight parameters <p> n ; t n )lnff j X m X h j (x n ; t n )lnP j (x n ): IDIAP-Com 97-05 9 This is exactly the error function that is minimized when applying the EM algorithm to a simple Gaussian mixture model (see, for example, section 2.6 in <ref> [1] </ref>). <p> In order to avoid biased estimates and overfitting the Bayesian approach has also been applied to MEs [18] using ensemble learning. The estimation of local error bars from the expert variances is straightforward (section 6.4 of <ref> [1] </ref>) using the definition of MEs (1) and of OE j including the expert variances (30): oe (x) = j j + jjy j (x) y (x)jj 2 In fact, Bishop [1] follows a more general approach where the expert variances are input-dependent and which allows modeling of conditional probability distributions. <p> The estimation of local error bars from the expert variances is straightforward (section 6.4 of <ref> [1] </ref>) using the definition of MEs (1) and of OE j including the expert variances (30): oe (x) = j j + jjy j (x) y (x)jj 2 In fact, Bishop [1] follows a more general approach where the expert variances are input-dependent and which allows modeling of conditional probability distributions. Acknowledgments The author gratefully acknowledges the Swiss National Science Foundation (FN:21-45621.95) for their support of this research. IDIAP-Com 97-05 11
Reference: [2] <author> C. M. Bishop and C. S. Qazaz. </author> <title> Regression with input-dependent noise: A Bayesian treatment. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 9, </booktitle> <address> Cambridge MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A well-known disadvantage of maximum-likelihood estimations, however, is that it gives biased estimates and leads to under-estimation of the noise variance and overfitting on the training data. Therefore, Bayesian techniques have also been applied <ref> [2] </ref> which typically avoid these kind of problems.
Reference: [3] <author> N. P. Bradshaw, A. Duch^ateau, and H. Bersini. </author> <title> Global least-squares vs. EM training for the Gaussian mixture of experts. </title> <editor> In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors, </editor> <booktitle> Artificial Neural Networks - ICANN'97, number 1327 in Lecture Notes in Computer Science, </booktitle> <pages> pages 295-300. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: Of course, a global least-squares IDIAP-Com 97-05 2 x x x Gating network x g g 2 y Expert 1 Expert 3Expert 2 approach could also be used and might be more appropriate when a division in subproblems is not feasible <ref> [3] </ref>.
Reference: [4] <author> J. S. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman Soulie and J. Herault, editors, Neurocomputing: </editor> <booktitle> Algorithms, Architectures, and Applications, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In order to ensure this probabilistic interpretation, the activation function for the outputs of the gating network is chosen to be the soft-max function <ref> [4] </ref>: g j = P m ; (2) where the z i are the gating network outputs before thresholding. This soft-max function makes that the gating network outputs sum to unity and are non-negative; thus implementing the (soft) competition between the experts.
Reference: [5] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The first approach consists of standard gradient-based learning and has been applied with IDIAP-Com 97-05 3 some success in the training of (H)MEs [8][9]. The second approach is an instance of the Expectation-Maximization (EM) algorithm <ref> [5] </ref>, which is often applied to unconditional mixture models [12] and has also been formulated for and applied to conditional mixtures of experts [9][10]. <p> In <ref> [5] </ref> it has been shown that the decrease of the expected complete error function implies the decrease of the original error function E which guarantees convergence to a local minimum. A more detailed treatment of the convergence of the EM algorithm for mixture of experts can be found in [10]. <p> The EM algorithm comes in several variants, the basic one being the one described above with a M-step that minimizes the various error functions. A variant that is often used is called Generalized EM (GEM) <ref> [5] </ref> that is based on the weaker assumption of decreasing (not necessarily minimizing) the error functions. A partial implementation of the E-step has been proposed in [14] which is basically an on-line EM algorithm.
Reference: [6] <author> J. Fritsch, M. Finke, and A. Waibel. </author> <title> Context-dependent hybrid HME/HMM speech recognition using polyphone clustering decision trees. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <year> 1997. </year>
Reference: [7] <author> J. Fritsch. </author> <title> Modular neural networks for speech recognition. </title> <type> Master's thesis, </type> <institution> Carnegie Mellon University & University of Karlsruhe, </institution> <year> 1996. ftp://reports.adm.cs.cmu.edu/usr/anon/1996/CMU-CS96-203.ps.gz. </year>
Reference-contexts: It is important to note that, although we are actually optimizing the joint probability during training, recall is still based on the original conditional mixture model (28). The idea of Gaussian kernels has been extended to the expert networks by Fritsch <ref> [7] </ref>, resulting in a model that can be trained by a generalized EM algorithm.
Reference: [8] <author> Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: [9] <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: The standard choices for gating and expert networks are generalized linear models <ref> [9] </ref> and multilayer perceptrons [20]. <p> The use of a soft-max function in the gating network and the fact that the OE j are densities guarantee that the distribution is normalized: R This distribution forms the basis for the ME error function which can be optimized using gradient descent or the Expectation-Maximization (EM) algorithm <ref> [9] </ref>. Of course, a global least-squares IDIAP-Com 97-05 2 x x x Gating network x g g 2 y Expert 1 Expert 3Expert 2 approach could also be used and might be more appropriate when a division in subproblems is not feasible [3]. <p> Also for these variants the convergence towards a local minimum is still guaranteed. 3.2.1 Weighted Least-Squares Algorithms for M-Step In this section, a simple heuristic to reduce the M-step for MEs with perceptrons as gating and expert networks to a one-pass calculation <ref> [9] </ref> is described. For this purpose we investigate the maximization problems to be solved: eq. (23) for the gating network and eq. (24) for the expert networks. <p> However, from eqs. (25) and (26) it is clear that ordinary weighted least-squares can be used for the transformed equations where the inverse of the soft-max is applied to the network outputs, the posteriors j;n , and the targets t n <ref> [9] </ref>. Inverting the soft-max: y i = P gives x i = lny i + ln X exp (x j ); where the second term is constant for all x i and disappears when the soft-max is applied.
Reference: [10] <author> M. I. Jordan and L. Xu. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(9) </volume> <pages> 1409-1431, </pages> <year> 1995. </year>
Reference-contexts: A more detailed treatment of the convergence of the EM algorithm for mixture of experts can be found in <ref> [10] </ref>.
Reference: [11] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <address> London, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: For example, if the expert and gating networks are perceptrons with a Gaussian conditional density and linear activation function for the experts (thus, expert and gating networks are generalized linear models <ref> [11] </ref>) the updates for the expert network weights w j : w j = j j (y j t)x T ; and for the gating network weights v j : v j = j (g j j )x; where j denotes the learning rate.
Reference: [12] <author> Geoffrey J. McLachlan and Kaye E. Basford. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The divide-and-conquer approach has shown particularly useful in attributing experts to different regimes in piece-wise stationary time series [20], modeling discontinuities in the input-output mapping, and classification problems [6][13][19]. The ME error function is based on the interpretation of MEs as a mixture model <ref> [12] </ref> with conditional densities as mixture components (for the experts) and gating network outputs as mixing coefficients. The purpose of this note is to describe various existing methods for minimizing this ME error function and to do so in a unified notation. <p> The first approach consists of standard gradient-based learning and has been applied with IDIAP-Com 97-05 3 some success in the training of (H)MEs [8][9]. The second approach is an instance of the Expectation-Maximization (EM) algorithm [5], which is often applied to unconditional mixture models <ref> [12] </ref> and has also been formulated for and applied to conditional mixtures of experts [9][10]. <p> Thus the gating network outputs g j sum to one and are non-negative. It is interesting to note that the numerators in eq. (27) can be interpreted as the components of a simple mixture model <ref> [12] </ref>; a fact which can be used to find a good initialization of the kernel parameters.
Reference: [13] <author> Perry Moerland. </author> <title> Mixtures of experts estimate a posteriori probabilities. </title> <address> IDIAP-RR 97-07, IDIAP, Mar-tigny, Switzerland, ftp://ftp.idiap.ch/pub/reports/1997/rr97-07.ps.gz, </address> <year> 1997. </year>
Reference: [14] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. Unpublished manuscript from ftp://ftp.cs.utoronto.ca/pub/radford/em.ps.Z, </title> <year> 1993. </year>
Reference-contexts: A variant that is often used is called Generalized EM (GEM) [5] that is based on the weaker assumption of decreasing (not necessarily minimizing) the error functions. A partial implementation of the E-step has been proposed in <ref> [14] </ref> which is basically an on-line EM algorithm.
Reference: [15] <author> A. D. Nix and A. S. Weigend. </author> <title> Learning local error bars for nonlinear regression. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 489-496, </pages> <address> Cambridge MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This gives an estimate of the confidence one can have in a prediction and the possibility to take into account input-dependent noise. IDIAP-Com 97-05 10 In a maximum-likelihood framework this has been proposed for a single isotropic Gaussian conditional probability density function <ref> [15] </ref> and has been generalized to an arbitrary covariance matrix [21]. A well-known disadvantage of maximum-likelihood estimations, however, is that it gives biased estimates and leads to under-estimation of the noise variance and overfitting on the training data.
Reference: [16] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The one-step solution of this equation is then: W T In order to avoid problems with singularity (of the matrix X T j X) and round-off errors, other possibilities are the use of QR decomposition or the even more suitable singular value decomposition (SVD) (x15.4 in <ref> [16] </ref>), that directly find the best solution in the least-squares sense of the system of linear equations associated with eq. (26): p j XW T p For the gating network and in the case of a multinomial conditional density with a soft-max activation function for the expert networks, this least-squares approach
Reference: [17] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1 : Foundations, chapter 8, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: For feed-forward neural networks this involves in specific the partial derivatives of the error function with respect to the network outputs (before thresholding); these derivatives (commonly denoted as ffi j ) form the basis for the back-propagation algorithm <ref> [17] </ref>. In section 6.4 of [1], the partial derivative for the gating network with respect to its outputs have been calculated in the context of a gradient descent algorithm for the mixture model (3).
Reference: [18] <author> S. R. Waterhouse, D. J. C. MacKay, and A. J. Robinson. </author> <title> Bayesian methods for mixtures of experts. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 351-357, </pages> <address> Cambridge MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Weigend et al. [20] also describe the incorporation of prior belief about the expert variances in a maximum likelihood framework. In order to avoid biased estimates and overfitting the Bayesian approach has also been applied to MEs <ref> [18] </ref> using ensemble learning.
Reference: [19] <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Classification using hierarchical mixtures of experts. </title> <booktitle> In Proceedings 1994 IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 177-186, </pages> <address> Long Beach CA, 1994. </address> <publisher> IEEE Press. </publisher>
Reference: [20] <author> Andreas S. Weigend, Morgan Mangeas, and Ashok N. Srivastava. </author> <title> Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 373-399, </pages> <year> 1995. </year> <pages> IDIAP-Com 97-05 12 </pages>
Reference-contexts: In particular, the gating network of a ME learns to partition the input space (in a soft way, so overlaps are possible) and attributes expert networks to these different regions. The divide-and-conquer approach has shown particularly useful in attributing experts to different regimes in piece-wise stationary time series <ref> [20] </ref>, modeling discontinuities in the input-output mapping, and classification problems [6][13][19]. The ME error function is based on the interpretation of MEs as a mixture model [12] with conditional densities as mixture components (for the experts) and gating network outputs as mixing coefficients. <p> Learning algorithms treated are gradient descent, quasi-Newton methods, Expectation Maximization (EM) [5]<ref> [20] </ref>, and various one-pass solutions of the maximization step of the EM algorithm. The last section gives a short summary of how mixtures of experts can estimate local error bars [20]. 2 Mixtures of Experts In this section the basic definitions of the mixture of experts model are given which will be used in the rest of this note. network both having access to the input vector x; the gating network has one output g i per expert. <p> The standard choices for gating and expert networks are generalized linear models [9] and multilayer perceptrons <ref> [20] </ref>. <p> An interpretation of the first (cross-entropy) term is as the entropy of distributing a pattern x amongst the expert networks. This cost is minimal if experts are mutually exclusive and increases when experts share a pattern <ref> [20] </ref>. The second term has the general form of a weighted maximum likelihood problem; the weighting with j implies that the important experts are the ones with a large value for j . <p> When the expert and gating networks are chosen to be multilayer perceptrons (MLPs) gradient-based optimization is most appropriate. The use of MLPs has the advantage that it adds non-linearity to the ME model; on the other hand convergence to a local minimum is no longer guaranteed <ref> [20] </ref>. A gradient approach (with multinomial or Gaussian conditional densities) leads to the same output error terms as before: (6) for the gating network and (18) for the expert networks. <p> Therefore, Bayesian techniques have also been applied [2] which typically avoid these kind of problems. For MEs it is usual to introduce a local variance for each expert <ref> [20] </ref>, changing (13) to: OE j (t n jx n ) = (2oe 2 jjt n y n 2oe 2 ! The effect of these expert variances is that the model can handle different noise levels which is for instance very useful when dealing with piece-wise stationary time series that switch <p> It has been noted that it reduces overfitting and eases the subdivision of the problem among the experts <ref> [20] </ref>. The introduction of the expert variances necessitates some small changes in various equations of section 3.1. <p> Weigend et al. <ref> [20] </ref> also describe the incorporation of prior belief about the expert variances in a maximum likelihood framework. In order to avoid biased estimates and overfitting the Bayesian approach has also been applied to MEs [18] using ensemble learning.
Reference: [21] <author> P. M. Williams. </author> <title> Using neural networks to model conditional multivariate densities. </title> <journal> Neural Computation, </journal> <volume> 8(4) </volume> <pages> 843-854, </pages> <year> 1996. </year>
Reference-contexts: IDIAP-Com 97-05 10 In a maximum-likelihood framework this has been proposed for a single isotropic Gaussian conditional probability density function [15] and has been generalized to an arbitrary covariance matrix <ref> [21] </ref>. A well-known disadvantage of maximum-likelihood estimations, however, is that it gives biased estimates and leads to under-estimation of the noise variance and overfitting on the training data. Therefore, Bayesian techniques have also been applied [2] which typically avoid these kind of problems.
Reference: [22] <author> L. Xu, M. I. Jordan, and G. E. Hinton. </author> <title> An alternative model for mixtures of experts. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 633-640, </pages> <address> Cambridge MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Of course, also in this case the problem can be solved with SVD instead of the more sensitive pseudo-inverses. 3.2.2 Gating Network with Gaussian Kernels Another way to reduce the M-step for the gating network to a one-pass calculation has been proposed by Xu and Jordan <ref> [22] </ref>. Their idea is to use a modified gating network consisting of normalized kernels (by applying Bayes' rule): g j (x) = P (jjx) = P ; (27) IDIAP-Com 97-05 8 where P i ff i = 1, ff i 0, and the P i 's are probability density functions. <p> The idea of Gaussian kernels has been extended to the expert networks by Fritsch [7], resulting in a model that can be trained by a generalized EM algorithm. Other possible extensions are the use of exponential kernels and modeling the complete covariance matrix <ref> [22] </ref>. 4 Adaptive Variances in Mixtures of Experts The previous sections focused on MEs as single point estimators that predict the conditional average of the target data. This approach is quite suitable for target data that can be described with an input-dependent mean and one global variance parameter.
References-found: 22

