URL: http://www.cs.umd.edu/projects/dimsum/papers/cinvest.ps.gz
Refering-URL: http://www.cs.umd.edu/projects/dimsum/papers/
Root-URL: 
Title: Cache Investment Strategies  
Author: Michael J. Franklin Donald Kossmann 
Keyword: Index terms Distributed Databases, Client-Server Databases, Query Processing, Query Optimization, Caching, Database System Performance.  
Address: Passau  
Affiliation: University of Maryland  University of  Univ. of MD  
Pubnum: Technical Report CS-TR-3803 and UMIACS-TR-97-50  
Email: franklin@cs.umd.edu  kossmann@db.fmi.uni-passau.de  
Date: May 1997  
Abstract: Emerging client-server and peer-to-peer distributed information systems employ data caching to improve performance and reduce the need for remote access to data. In distributed database systems, caching is a by-product of query operator placement | data that are brought to a site by a query operator can be retained at that site for future use. Operator placement, however, must take the location of cached data into account in order to avoid excessive data movement. Thus, there exists a fundamental circular dependency between caching and query optimization. In this paper, we identify this circularity and show that in order to break it, query optimization must be extended to look beyond the performance of a single query. To do so, we propose the notion of Cache Investment, in which a sub-optimal plan may be generated for a particular query in order to effect a data placement that is beneficial for subsequent queries. We develop a framework for integrating Cache Investment decisions into a distributed database system without changing basic components such as the query optimizer's search strategy, the query engine, or the buffer manager. We then describe several cache investment policies, and analyze them using a detailed simulation model. Our results show that cache investment can significantly improve the overall performance of a system compared to the static operator placement strategies that are used by today's database systems. 
Abstract-found: 1
Intro-found: 1
Reference: [AZ93] <author> S. Acharya and S. Zdonik. </author> <title> An efficient scheme for dynamic data replication. </title> <institution> Cs-93-43, Dept. of Computer Science, Brown University, </institution> <address> Providence, Rhode Island 02912, USA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Because caching establishes copies of data at a site, it can be seen as a form of replication. Replication has been thoroughly investigated in previous work; e.g., in <ref> [WJ92b, WJ92a, AZ93, Bes95, SAB + 96] </ref>. Such algorithms however, cannot be directly applied to support caching | they are based on the global popularity of data in order to load balance the entire system and move data closer to a group of sites that frequently use the data. <p> Nevertheless, there are some similarities in the design of some replication algorithms and the policies studied in this work. The design of the Reference-Counting policy is similar to some of the ideas of the dynamic replication algorithms presented in <ref> [WJ92b, WJ92a, AZ93] </ref>.
Reference: [Bes95] <author> A. Bestavros. </author> <title> Demand-based document dissemination to reduce traffic and balance load in distributed information systems. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Processing, </booktitle> <address> San Antonio, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Because caching establishes copies of data at a site, it can be seen as a form of replication. Replication has been thoroughly investigated in previous work; e.g., in <ref> [WJ92b, WJ92a, AZ93, Bes95, SAB + 96] </ref>. Such algorithms however, cannot be directly applied to support caching | they are based on the global popularity of data in order to load balance the entire system and move data closer to a group of sites that frequently use the data.
Reference: [CABK88] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in bubba. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 99-108, </pages> <address> Chicago, IL, USA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: size of the client's cache. fragment value size in pages value/size A 200 50 4 C 200 100 2 Table 1: Example of Cache Value Computation The Reference-Counting policy tries to maximize the value of the fragments stored in a client's cache using an approach similar to that of Bubba <ref> [CABK88] </ref> and [SJGP90], in which the value/size ratio is taken 8 into account. A fragment is considered to be a candidate only if its value is greater than 0 and it would be fully or partially kept in a cache with a total maximum value.
Reference: [CDF + 94] <author> M. Carey, D. DeWitt, M. Franklin, N. Hall, M. McAuliffe, J. Naughton, D. Schuh, M. Solomon, C. Tan, O. Tsatalos, S. White, and M. Zwilling. </author> <title> Shoring up persistent applications. </title> <booktitle> In SIGMOD [SIG94], </booktitle> <pages> pages 383-394. </pages>
Reference-contexts: The techniques we present, however, can naturally be applied to other distributed database architectures, such as a symmetric peer-to-peer system like SHORE <ref> [CDF + 94] </ref>, in which every site acts as a client and/or as a server. We assume the use of a hybrid-shipping query execution model, which as shown in our earlier work, allows query processing to best exploit the resources of such a system [FJK96]. <p> A flexible approach, however, has been used in several recent experimental systems such as ORION-2 [JWKL90] and Mariposa [SAL + 96], and is being integrated into an extended version of the SHORE storage manager <ref> [CDF + 94] </ref> as part of the DIMSUM 1 project. <p> Invalidation has been shown to be more robust than propagation across a wide range of workload and system scenarios [FCL97]. Callback locking is a prominent example of an invalidation-based policy, and it is used in several client-server database systems; e.g., ObjectStore [LLOW91] and SHORE <ref> [CDF + 94] </ref>. 3 Policies for Cache Investment In this section we present policies for determining when and for which fragments the investment required to initiate caching should be made. <p> Since no queries are submitted at servers, the server model does not have a Query Source or an Optimizer. Data owned by other servers can be cached at a server (following <ref> [CDF + 94] </ref>); but to concentrate on the effects of client-side caching, most experiments are carried out with a single server that owns the whole database.
Reference: [CFZ94] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-grained sharing in a page server OODBMS. </title> <booktitle> In SIGMOD [SIG94], </booktitle> <pages> pages 359-370. </pages>
Reference-contexts: We say that the relations at the beginning of the permutation are hot and the relations at the end are cold. 4 The database and relation sizes are kept small in order to achieve acceptable simulation times. It is important to note (as demonstrated in <ref> [CFZ94] </ref>) that rather than the absolute sizes of the cache or data, it is their ratio that is important to measure the effectiveness of caching.
Reference: [CKV93] <author> K. M. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 43-53, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Nevertheless, it might be possible to find new, better cache investment policies by adapting some of the techniques that were designed for prefetching to predict the future behavior of a client (e.g., <ref> [PZ91, CKV93] </ref>). 23 There has, of course, been a great deal of related work in the area of caching. In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache.
Reference: [CL86] <author> M. Carey and H. Lu. </author> <title> Load balancing in a locally distributed database system. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 108-119, </pages> <address> Washington, USA, </address> <year> 1986. </year>
Reference-contexts: Obviously, run-time site selection is vital for making use of the client's cache; for example, to carry out a join at the client if copies of both relations are already cached. Run-time site selection is also needed to allow load balancing <ref> [CL86] </ref>. For interactive, ad-hoc queries, query optimization and site selection are both carried out at execution time. <p> For pre-compiled queries that are part of, say, an application program, a two-step approach can be used, in which most optimization decisions (e.g., join ordering) are made at compile time, but site selection is carried out at execution time. Similar approaches have been proposed in <ref> [CL86, SAL + 96, FJK96] </ref>. 2.2 Cache Management We study an architecture in which data can be cached in a client's main memory or on a client's local disk [FCL93]. We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations.
Reference: [CR94] <author> C. Chen and N. Roussopoulos. </author> <title> The implementation and performance evaluation of the ADMS query optimizer: Integrating query result caching and matching. </title> <booktitle> In Advances in Database Technology - EDBT '94, Proceedings, </booktitle> <address> Cambridge, United Kingdom, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>). More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture [DFMV90]. <p> In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account. In <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>, caching was integrated into a query processing environment by caching the results of queries. Caching query results is still an open research topic by itself and, therefore, is not directly addressed here.
Reference: [DFJ + 96] <author> S. Dar, M. Franklin, B. Jonsson, D. Srivastava, and M. Tan. </author> <title> Semantic data caching and replacement. </title> <booktitle> In VLDB [VLD96], </booktitle> <pages> pages 330-341. </pages>
Reference-contexts: We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>). More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture [DFMV90]. <p> In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account. In <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>, caching was integrated into a query processing environment by caching the results of queries. Caching query results is still an open research topic by itself and, therefore, is not directly addressed here. <p> If the results of sub-queries can be cached at a client, all operators (not only scans) could initiate caching. In separate work, we have proposed Semantic Caching <ref> [DFJ + 96] </ref> as a compromise between physical caching and query result caching. Integrating Cache Investment with Semantic Caching is another promising direction. Finally, we intend to study how the policies studied in this work can be combined with techniques for global memory management such as those proposed in [FCL92].
Reference: [DFMV90] <author> D. J. DeWitt, P. Futtersack, D. Maier, and F. Velez. </author> <title> A study of three alternative workstation server architectures for object-oriented database systems. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <pages> pages 107-121, </pages> <address> Brisbane, Australia, </address> <year> 1990. </year>
Reference-contexts: More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture <ref> [DFMV90] </ref>. A fragment refers to any collection of pages that is stored permanently in a single file at a server; e.g., a relation or horizontal partition of a relation. Caching at a client is initiated by placing a scan operator of a query on the client.
Reference: [EH84] <author> W. Effelsberg and T. </author> <title> Harder. Principles of database buffer management. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 9(4) </volume> <pages> 560-595, </pages> <year> 1984. </year>
Reference-contexts: The way that values are assigned differs according to the particular policy being used. For both policies, the values of fragments at a client are adjusted after the execution of each query at that client. This adjustment is performed using periodic aging by division, as proposed in <ref> [EH84] </ref>. The value of every fragment is initially set to 0. <p> When is a fragment considered to be a candidate? We now describe the Reference-Counting and Profitable policies, focusing on the way that they address these two questions. 3.3.2 The Reference-Counting Policy Reference-Counting is an extension of reference-based replacement policies used in database buffer management <ref> [EH84] </ref>. For Reference-Counting, the component C t (j) of equation 1 is set to 1 if any part of fragment j is used in query t (that is, if during the execution of query t at least one page of fragment j was accessed) and is set to 0 otherwise.
Reference: [FCL92] <author> M. J. Franklin, M. J. Carey, and M. Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <pages> pages 596-609, </pages> <address> Vancouver, Canada, </address> <year> 1992. </year> <month> 25 </month>
Reference-contexts: In particular, the cheap communication makes the investment required to initiate caching for the HiSel and NoSel workloads nearly identical in terms of response time. 8 The performance of the Optimistic policy in this case could be significantly improved using global memory management techniques devised in <ref> [FCL92] </ref> to reduce the overlap between the client's and the server's main memory. 18 5.4 Experiment 3: Throughput, Heterogeneous Servers In all of the experiments so far, the two history-based policies showed roughly the same performance. <p> Integrating Cache Investment with Semantic Caching is another promising direction. Finally, we intend to study how the policies studied in this work can be combined with techniques for global memory management such as those proposed in <ref> [FCL92] </ref>. Acknowledgments We would like to thank Bjorn Thor Jonsson who designed and implemented the simulator used in the performance experiments. We would also like to thank Laurent Amsaleg for helping to improve the presentation of this paper.
Reference: [FCL93] <author> M. J. Franklin, M. J. Carey, and M. Livny. </author> <title> Local disk caching for client-server database systems. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <pages> pages 543-554, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year>
Reference-contexts: Similar approaches have been proposed in [CL86, SAL + 96, FJK96]. 2.2 Cache Management We study an architecture in which data can be cached in a client's main memory or on a client's local disk <ref> [FCL93] </ref>. We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., [RK86, CR94, SJGP90, KB94, DFJ + 96]). <p> When a page is replaced from the client's main memory, it is demoted to the client's disk cache which is managed by the Disk Manager using a FIFO replacement policy as devised in <ref> [FCL93] </ref>. In some experiments, pages are invalidated in the client's cache to model cache consistency maintenance for updates. These invalidations are effected by the Replica Manager.
Reference: [FCL97] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Transactional client-server cache consistency: Alternatives and performance. </title> <journal> ACM Trans. on Database Systems, </journal> <note> 1997. Accepted for Publication. </note>
Reference-contexts: Invalidation has been shown to be more robust than propagation across a wide range of workload and system scenarios <ref> [FCL97] </ref>.
Reference: [FJK96] <author> M. Franklin, B. Jonsson, and D. Kossmann. </author> <title> Performance tradeoffs for client-server query processing. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 149-160, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: This paper focuses on one key aspect of this integration, namely, the circular dependency that exists between caching and query optimization | more specifically, between caching and query operator site selection. In <ref> [FJK96] </ref>, we introduced a query execution model called hybrid-shipping, which is able to exploit the presence of cached data through the use of flexible operator site selection. <p> We assume the use of a hybrid-shipping query execution model, which as shown in our earlier work, allows query processing to best exploit the resources of such a system <ref> [FJK96] </ref>. Hybrid-shipping is a flexible policy in which query processing can be performed at clients, servers, or various combinations of them according to the query plan produced by the optimizer. <p> In the following, we describe the architecture 3 of a hybrid-shipping system, focusing on the features that are relevant to cache investment. 2.1 Query Processing As described in <ref> [FJK96] </ref>, two key aspects of hybrid-shipping query processing are flexible site selection for query operators and the binding of such site selections at query execution time. With hybrid-shipping, queries are executed in an architecture that allows query operators to run on clients and/or on servers. <p> The importance of operator placement flexibility was demonstrated in the two examples of the introduction: in Example 1, the operators of the queries should be executed at the client whereas the query of Example 2 should be executed at the server. Furthermore, as shown in <ref> [FJK96] </ref>, there are cases where the operators of a single query should be split among clients and servers. At present, most client-server database systems do not provide the flexibility to choose among these options. <p> For pre-compiled queries that are part of, say, an application program, a two-step approach can be used, in which most optimization decisions (e.g., join ordering) are made at compile time, but site selection is carried out at execution time. Similar approaches have been proposed in <ref> [CL86, SAL + 96, FJK96] </ref>. 2.2 Cache Management We study an architecture in which data can be cached in a client's main memory or on a client's local disk [FCL93]. We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. <p> As a result, we have developed an alternative way to integrate cache investment policies with the query optimizer. The approach we have adopted effects the firing of a policy by fooling the optimizer to believe that scans of candidate fragments are very cheap at the client. As described in <ref> [FJK96] </ref> before performing operator site selection, our query optimizer obtains information about the contents of the client's cache from the buffer manager. This information is used by the optimizer's cost model to determine how expensive it is to place a scan at the client. <p> integrated without changing the internals of the optimizer search strategy and allows caching decisions to take advantage of the optimizer's cost model. 4 Experimental Environment To investigate the relative performance of the four cache investment policies, we extended the simulation environment used in a previous study of client-server query processing <ref> [FJK96] </ref>. Figure 1 shows the overall structure of the simulator; the server model, the network model, and most components of the client model (including the query optimizer) are identical with those described in [FJK96]. In the following, we describe these models and specify the query workloads used in this study. <p> cache investment policies, we extended the simulation environment used in a previous study of client-server query processing <ref> [FJK96] </ref>. Figure 1 shows the overall structure of the simulator; the server model, the network model, and most components of the client model (including the query optimizer) are identical with those described in [FJK96]. In the following, we describe these models and specify the query workloads used in this study. Furthermore, some details of the query optimizer and its cost model are given. <p> In <ref> [FJK96] </ref>, we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account.
Reference: [GHK92] <author> S. Ganguly, W. Hasan, and R. Krishnamurthy. </author> <title> Query optimization for parallel execution. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <pages> pages 9-18, </pages> <address> San Diego, USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The optimizer can be configured in two different ways: (1) to minimize the cost of a query based on estimates a la [ML86], or (2) to minimize the response time according to the model of <ref> [GHK92] </ref>. In both modes, the cost-model parameters are set depending on the client-server configuration; for example, the cost model assumes that operations at a server are more expensive in a system with 10 clients than in a system with one client due to the expected higher load on the server.
Reference: [Gra93] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The query is then executed according to the optimized query plan: some of the operators are executed on the client and others on servers. The execution of an operator is simulated by the Query Engine which is based on an iterator model (similar to that of Volcano <ref> [Gra93] </ref>) and which provides implementations for scan, select, project, join, and network operators. Although the query engine includes several join methods, only hash joins are used in this study. Network operators effect communication between operators that run on different sites.
Reference: [IK90] <author> Y. E. Ioannidis and Y. C. Kang. </author> <title> Randomized algorithms for optimizing large join queries. </title> <booktitle> In SIGMOD [SIG90], </booktitle> <pages> pages 312-321. </pages>
Reference-contexts: Query plans are produced by a randomized query optimizer. Our randomized query optimizer is based on the approach described in <ref> [IK90] </ref> extended to carry 13 out site selection in addition to other decisions such as join ordering.
Reference: [JWKL90] <author> B. P. Jenq, D. Woelk, W. Kim, and W. L. Lee. </author> <title> Query processing in distributed ORION. </title> <booktitle> In Proc. of the Intl. Conf. on Extending Database Technology (EDBT), </booktitle> <pages> pages 169-187, </pages> <address> Venice, Italy, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: At present, most client-server database systems do not provide the flexibility to choose among these options. A flexible approach, however, has been used in several recent experimental systems such as ORION-2 <ref> [JWKL90] </ref> and Mariposa [SAL + 96], and is being integrated into an extended version of the SHORE storage manager [CDF + 94] as part of the DIMSUM 1 project.
Reference: [KB94] <author> A. Keller and J. Basu. </author> <title> A predicate-based caching scheme for client-server database architectures. </title> <booktitle> In Proc. of the IEEE Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 229-238, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>). More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture [DFMV90]. <p> In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account. In <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>, caching was integrated into a query processing environment by caching the results of queries. Caching query results is still an open research topic by itself and, therefore, is not directly addressed here.
Reference: [LC79] <author> A. Law and J. Carson. </author> <title> A sequential procedure for determining the length of a steady-state simulation. </title> <journal> Operations Research, </journal> <volume> 27(5) </volume> <pages> 1011-1025, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: A stream of queries (e.g., Uniform-NoSel) is executed at every client. For every data point, at least 800 queries are executed to make sure that the 90% confidence intervals for all results are within 5%; the confidence intervals are computed using batch means <ref> [LC79] </ref>.
Reference: [LLOW91] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb. </author> <title> The ObjectStore database system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <year> 1991. </year>
Reference-contexts: Invalidation has been shown to be more robust than propagation across a wide range of workload and system scenarios [FCL97]. Callback locking is a prominent example of an invalidation-based policy, and it is used in several client-server database systems; e.g., ObjectStore <ref> [LLOW91] </ref> and SHORE [CDF + 94]. 3 Policies for Cache Investment In this section we present policies for determining when and for which fragments the investment required to initiate caching should be made.
Reference: [ML86] <author> L. Mackert and G. Lohman. </author> <title> R fl optimizer validation and performance evaluation for distributed queries. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <pages> pages 149-159, </pages> <address> Kyoto, Japan, </address> <year> 1986. </year>
Reference-contexts: The optimizer can be configured in two different ways: (1) to minimize the cost of a query based on estimates a la <ref> [ML86] </ref>, or (2) to minimize the response time according to the model of [GHK92].
Reference: [PZ91] <author> M. L. Palmer and S. B. Zdonik. </author> <title> FIDO: A cache that learns to fetch. </title> <booktitle> In Proc. of the VLDB Conf., </booktitle> <pages> pages 255-264, </pages> <address> Barcelona, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Nevertheless, it might be possible to find new, better cache investment policies by adapting some of the techniques that were designed for prefetching to predict the future behavior of a client (e.g., <ref> [PZ91, CKV93] </ref>). 23 There has, of course, been a great deal of related work in the area of caching. In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache.
Reference: [RK86] <author> N. Roussopoulos and H. Kang. </author> <booktitle> Principles and techniques in the design of ADMS. IEEE Computer, </booktitle> <volume> 19 </volume> <pages> 19-25, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>). More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture [DFMV90]. <p> In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account. In <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>, caching was integrated into a query processing environment by caching the results of queries. Caching query results is still an open research topic by itself and, therefore, is not directly addressed here.
Reference: [SAB + 96] <author> J. Sidell, P. Aoki, S. Barr, A. Sah, C. Staelin, M. Stonebraker, and A. Yu. </author> <title> Data replication in Mariposa. </title> <booktitle> In Proc. IEEE Conf. on Data Engineering, </booktitle> <address> New Orleans, LA, USA, </address> <year> 1996. </year>
Reference-contexts: Because caching establishes copies of data at a site, it can be seen as a form of replication. Replication has been thoroughly investigated in previous work; e.g., in <ref> [WJ92b, WJ92a, AZ93, Bes95, SAB + 96] </ref>. Such algorithms however, cannot be directly applied to support caching | they are based on the global popularity of data in order to load balance the entire system and move data closer to a group of sites that frequently use the data. <p> Like the Profitable policy, replication in Mariposa <ref> [SAL + 96, SAB + 96] </ref> tries to estimate the costs and benefits of establishing a copy at a site; Mariposa's model, however, is intended to load balance a system, and it is not integrated into query optimization so that it does not address the circular dependency between caching and site
Reference: [SAL + 96] <author> M. Stonebraker, P. Aoki, W. Litwin, A. Pfeffer, A. Sah, J. Sidell, C. Staelin, and A. Yu. Mariposa: </author> <title> A wide-area distribured database system. </title> <journal> The VLDB Journal, </journal> <volume> 5(1) </volume> <pages> 48-63, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: At present, most client-server database systems do not provide the flexibility to choose among these options. A flexible approach, however, has been used in several recent experimental systems such as ORION-2 [JWKL90] and Mariposa <ref> [SAL + 96] </ref>, and is being integrated into an extended version of the SHORE storage manager [CDF + 94] as part of the DIMSUM 1 project. <p> For pre-compiled queries that are part of, say, an application program, a two-step approach can be used, in which most optimization decisions (e.g., join ordering) are made at compile time, but site selection is carried out at execution time. Similar approaches have been proposed in <ref> [CL86, SAL + 96, FJK96] </ref>. 2.2 Cache Management We study an architecture in which data can be cached in a client's main memory or on a client's local disk [FCL93]. We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. <p> Like the Profitable policy, replication in Mariposa <ref> [SAL + 96, SAB + 96] </ref> tries to estimate the costs and benefits of establishing a copy at a site; Mariposa's model, however, is intended to load balance a system, and it is not integrated into query optimization so that it does not address the circular dependency between caching and site
Reference: [SIG90] <editor> Proc. </editor> <booktitle> ACM SIGMOD Conf., </booktitle> <address> Atlantic City, USA, </address> <month> April </month> <year> 1990. </year>
Reference: [SIG94] <editor> Proc. </editor> <booktitle> ACM SIGMOD Conf., </booktitle> <address> Minneapolis, MI, USA, </address> <month> May </month> <year> 1994. </year>
Reference: [SJGP90] <author> M. Stonebraker, A. Jhingran, J. Goh, and S. Potamianos. </author> <title> On rules, procedures, caching and views in data base systems. </title> <booktitle> In SIGMOD [SIG90], </booktitle> <pages> pages 281-290. </pages>
Reference-contexts: We focus on the case where, as in many data-shipping systems, cached data consist of pages of base relations. Such physical caching is in contrast to the logical caching of data such as query or sub-query result caching (e.g., <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>). More specifically, we assume that the database is partitioned into fragments and that individual pages of a fragment can be cached using a page-server architecture [DFMV90]. <p> the client's cache. fragment value size in pages value/size A 200 50 4 C 200 100 2 Table 1: Example of Cache Value Computation The Reference-Counting policy tries to maximize the value of the fragments stored in a client's cache using an approach similar to that of Bubba [CABK88] and <ref> [SJGP90] </ref>, in which the value/size ratio is taken 8 into account. A fragment is considered to be a candidate only if its value is greater than 0 and it would be fully or partially kept in a cache with a total maximum value. <p> In [FJK96], we studied how a query could be executed most efficiently given the contents of a client's cache. In that study, however, every query was optimized individually, and the impact of operator placement on caching was not taken into account. In <ref> [RK86, CR94, SJGP90, KB94, DFJ + 96] </ref>, caching was integrated into a query processing environment by caching the results of queries. Caching query results is still an open research topic by itself and, therefore, is not directly addressed here.
Reference: [SSV96] <author> P. Scheuermann, J. Shim, and R. Vingralek. Watchman: </author> <title> A data warehouse intelligent cache manager. </title> <booktitle> In VLDB [VLD96]. </booktitle>
Reference-contexts: Recently, dynamic, cost-based algorithms have been proposed for cache admittance and replacement in environments such as networks of workstations [SW97] and data warehouses <ref> [SSV96] </ref>. These latter approaches use cost and benefit estimates to determine the value of retaining cached copies of data, but they do so by moving data directly, independent of query optimization and processing. <p> More recently, several researchers have investigated the use of cost and benefit calculations for determining when to keep or replace cached data. WATCHMAN <ref> [SSV96] </ref> is a cache manager for data warehousing. It retains the answers to queries in order to avoid having to re-evaluate them. A profit metric is used to determine which results should be cache-resident. A set of related algorithms for distributed caching are proposed and studied in [SW97].
Reference: [SW97] <author> M. Sinnwell and G. Weikum. </author> <title> A cost-model-based online method for distribued caching. </title> <booktitle> In Proc. Intl. Conf. on Data Engineering, </booktitle> <address> Birmingham, U.K., 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: Recently, dynamic, cost-based algorithms have been proposed for cache admittance and replacement in environments such as networks of workstations <ref> [SW97] </ref> and data warehouses [SSV96]. These latter approaches use cost and benefit estimates to determine the value of retaining cached copies of data, but they do so by moving data directly, independent of query optimization and processing. <p> WATCHMAN [SSV96] is a cache manager for data warehousing. It retains the answers to queries in order to avoid having to re-evaluate them. A profit metric is used to determine which results should be cache-resident. A set of related algorithms for distributed caching are proposed and studied in <ref> [SW97] </ref>. These algorithms use estimates of cost and benefit to determine a good placement of (possibly) replicated items in a distributed systems. Both of these papers propose history-based methods for determining the value of caching data.
Reference: [VLD96] <editor> Proc. </editor> <booktitle> of the VLDB Conf., </booktitle> <address> Mumbai, India, </address> <month> September </month> <year> 1996. </year>
Reference: [WJ92a] <author> O. Wolfson and S. Jajodia. </author> <title> An algorithm for dynamic data distribution. </title> <booktitle> In IEEE Workshop on Management of Replicated Data, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Because caching establishes copies of data at a site, it can be seen as a form of replication. Replication has been thoroughly investigated in previous work; e.g., in <ref> [WJ92b, WJ92a, AZ93, Bes95, SAB + 96] </ref>. Such algorithms however, cannot be directly applied to support caching | they are based on the global popularity of data in order to load balance the entire system and move data closer to a group of sites that frequently use the data. <p> Nevertheless, there are some similarities in the design of some replication algorithms and the policies studied in this work. The design of the Reference-Counting policy is similar to some of the ideas of the dynamic replication algorithms presented in <ref> [WJ92b, WJ92a, AZ93] </ref>.
Reference: [WJ92b] <author> O. Wolfson and S. Jajodia. </author> <title> Distributed algorithms for dynamic replication of data. </title> <booktitle> In Proc. ACM SIGMOD/SIGACT Conf. on Princ. of Database Syst. (PODS), </booktitle> <pages> pages 149-163, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Because caching establishes copies of data at a site, it can be seen as a form of replication. Replication has been thoroughly investigated in previous work; e.g., in <ref> [WJ92b, WJ92a, AZ93, Bes95, SAB + 96] </ref>. Such algorithms however, cannot be directly applied to support caching | they are based on the global popularity of data in order to load balance the entire system and move data closer to a group of sites that frequently use the data. <p> Nevertheless, there are some similarities in the design of some replication algorithms and the policies studied in this work. The design of the Reference-Counting policy is similar to some of the ideas of the dynamic replication algorithms presented in <ref> [WJ92b, WJ92a, AZ93] </ref>.
Reference: [ZC97] <author> M. Zaharioudakis and M. Carey. </author> <title> Highly concurrent cache consistency for indices in client-server database systems. </title> <booktitle> In Proc. ACM SIGMOD Conf., </booktitle> <address> Tucson, AZ, </address> <month> May </month> <year> 1997. </year> <month> 26 </month>
Reference-contexts: There are many directions in which this work can be extended. One important area for future research 24 is the caching of indexes. Recently, several algorithms for using and maintaining indexes in a caching environment have been proposed and studied in the context of the SHORE system <ref> [ZC97] </ref>. We plan to investigate the interaction of Cache Investment with these algorithms. Also, as mentioned previously, this study was carried out in an architecture in which only base data and no results of queries or sub-queries could be cached.
References-found: 36

