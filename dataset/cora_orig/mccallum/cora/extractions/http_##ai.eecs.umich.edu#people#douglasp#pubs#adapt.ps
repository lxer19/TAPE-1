URL: http://ai.eecs.umich.edu/people/douglasp/pubs/adapt.ps
Refering-URL: http://ai.eecs.umich.edu/people/douglasp/pubs/adapt.html
Root-URL: http://www.cs.umich.edu
Email: laird@umich.edu, dpearson@umich.edu  huffman@tc.pw.com  
Phone: FAX: (313) 747-1761  
Title: Knowledge-directed Adaptation in Multi-level Agents  
Author: John E. Laird and Douglas J. Pearson Scott B. Huffman Price Waterhouse 
Address: 1101 Beal Ave. Ann Arbor, MI 48109  68 Willow Road Menlo Park, CA 94025  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  Technology Center  
Abstract: Most work on adaptive agents have a simple, single layer architecture. However, most agent architectures support three levels of knowledge and control: a reflex level for reactive responses, a deliberate level for goal-driven behavior, and a reflective layer for deliberate planning and problem decomposition. In this paper we explore agents implemented in Soar that behave and learn at the deliberate and reflective levels. These levels enhance not only behavior, but also adaptation. The agents use a combination of analytic and empirical learning, drawing from a variety of sources of knowledge to adapt to their environment. 
Abstract-found: 1
Intro-found: 1
Reference: <author> DeJong, G. </author> <year> 1995. </year> <title> A case study of explanation-based control. </title> <booktitle> In Proceedings of the Twelth International Workshop on Machine Learning, </booktitle> <pages> 167-175. </pages>
Reference-contexts: We have focused on systems that learn at the two higher levels, deliberate and reflective, within agents that employ all three levels of control. This research complements the work that has been done to combine empirical and analytic learning methods (such as EBNN (Mitchell & Thrun 1993), EBC <ref> (DeJong 1995) </ref> and EBRL (Dietterich & Flann 1995)) that typically focus on agents that use only a single level of control.
Reference: <author> Dietterich, T. G., and Flann, N. S. </author> <year> 1995. </year> <title> Explanation based learning and reinforcement learning: A unified view. </title> <booktitle> In Proceedings of the Twelth International Workshop on Machine Learning, </booktitle> <pages> 176-184. </pages>
Reference-contexts: This research complements the work that has been done to combine empirical and analytic learning methods (such as EBNN (Mitchell & Thrun 1993), EBC (DeJong 1995) and EBRL <ref> (Dietterich & Flann 1995) </ref>) that typically focus on agents that use only a single level of control. Although our approach is deliberate in many ways, it is distinguished from deliberate approaches that treat the knowledge of the agent as a declarative structure that can be examined and modified at will.
Reference: <author> Gil, Y. </author> <year> 1991. </year> <title> A domain-independent framework for effective experimentation in planning. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> 13-17. </pages>
Reference: <author> Huffman, S. B., and Laird, J. E. </author> <year> 1994. </year> <title> Learning from highly flexible tutorial instruction. </title> <booktitle> In Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94). </booktitle>
Reference: <author> Huffman, S. B. </author> <year> 1994. </year> <title> Instructable autonomous agents. </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, Dept. of Electrical Engineering and Computer Science. </institution>
Reference: <author> Laird, J. E. </author> <year> 1988. </year> <title> Recovery from incorrect knowledge in Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 618-623. </pages>
Reference-contexts: The advantages in terms of adaptation of this additional level of deliberations: (a) Selection knowledge (when and why to do something) can be learned and corrected independently of implementation knowledge (what to do). (b) Learning can be incremental, so that existing knowledge does not have to be modified <ref> (Laird 1988) </ref>.
Reference: <author> Miller, C. M. </author> <year> 1991. </year> <title> A constraint-motivated model of concept formation. </title> <booktitle> In The Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 827-831. </pages>
Reference: <author> Miller, C. M. </author> <year> 1993. </year> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> Ph.D. Dissertation, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering. </institution>
Reference: <author> Mitchell, T. M., and Thrun, S. B. </author> <year> 1993. </year> <title> Explanation based learning: A comparison of symbolic and neural network approaches. </title> <booktitle> In Proceedings of the Tenth International Workshop on Machine Learning, </booktitle> <pages> 197-204. </pages>
Reference-contexts: We have focused on systems that learn at the two higher levels, deliberate and reflective, within agents that employ all three levels of control. This research complements the work that has been done to combine empirical and analytic learning methods (such as EBNN <ref> (Mitchell & Thrun 1993) </ref>, EBC (DeJong 1995) and EBRL (Dietterich & Flann 1995)) that typically focus on agents that use only a single level of control.
Reference: <author> Mitchell, T. M.; Keller, R. M.; and Kedar-Cabelli, S. T. </author> <year> 1986. </year> <title> Explanation-based generalization: A unifying view. </title> <booktitle> Machine Learning 1. </booktitle>
Reference-contexts: Then, the agent attempts to explain at the reflective level, using its existing domain knowledge and forward projection, why the instruction will lead to success in the situation. If this explanation succeeds, the agent can learn general knowledge from the instruction (as in standard EBL <ref> (Mitchell, Keller, & Kedar-Cabelli 1986) </ref>). If the explanation fails, it means the agent is missing some knowledge required to complete the explanation. The missing knowledge can be acquired either through further instruction, or in some cases through simple induction over the "gap" in the incomplete explanation.
Reference: <author> Ourston, D., and Mooney, R. J. </author> <year> 1990. </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 815-820. </pages>
Reference: <author> Pearson, D. J., and Laird, J. E. </author> <year> 1996. </year> <title> Toward incremental knowledge correction for agents in complex environments. </title> <editor> In Muggleton, S.; Michie, D.; and Furukawa, K., eds., </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 15. </volume> <publisher> Oxford University Press. </publisher>
Reference-contexts: In order to extend adaptivity to more complex agents | agents with multiple levels of knowledge and control | we have developed a more deliberate approach which has been instantiated within two systems built within the Soar architecture: Instructo-Soar (Huffman & Laird 1994; Huffman 1994) and IMPROV <ref> (Pearson & Laird 1996) </ref>. We find that agents with multiple levels of knowledge and control are not only more able to achieve complex goals, they also can use these layers to adapt to their environment.
References-found: 12

