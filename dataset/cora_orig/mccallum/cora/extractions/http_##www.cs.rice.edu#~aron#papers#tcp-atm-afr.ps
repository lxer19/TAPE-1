URL: http://www.cs.rice.edu/~aron/papers/tcp-atm-afr.ps
Refering-URL: http://www.cs.rice.edu:80/~aron/research.html
Root-URL: 
Title: End-to-End TCP Congestion Control over ATM Networks  
Author: Mohit Aron Peter Druschel 
Affiliation: Department of Computer Science Rice University  
Pubnum: TR97-273  
Abstract: It is well documented that the effective throughput of TCP can suffer on plain ATM networks. Several research efforts have aimed at developing additions to ATM networks like Early Packet Discard that avoid TCP throughput degradation. This paper instead investigates improvements to TCP that allow it to perform well on ATM networks without switch-level enhancements, thus avoiding additional complexity and loss of layer transparency in ATM switches. We present an enhanced version of TCP Vegas and show that it performs nearly as well on plain ATM networks as on packet-oriented networks. Moreover, like the original TCP Vegas, our version achieves a significant increase in throughput over the BSD 4.4-Lite TCP. We also present an analysis of TCP dynamics over ATM networks, and a simulation based study of the various enhancements in our TCP implementation and their impact on TCP performance over ATM and packet-oriented networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Ahn, P. Danzig, Z. Liu, and E. Yan. </author> <title> Evaluation of TCP Vegas: Emulation and Experiment. </title> <booktitle> In Proceedings of the SIGCOMM '95 Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: TCP-Vegas is an implementation of TCP that is described in [6, 8]. TCP-Vegas tries to eliminate the self-induced losses in TCP-Reno. It modifies the slow-start mechanism of TCP-Reno and provides new retransmission and congestion avoidance mechanisms. The performance of TCP-Vegas against TCP-Reno was also evaluated in <ref> [1] </ref>. The rest of this section discusses key differences between ATM and packet networks, how these differences interact with TCP dynamics, and the manner in which they 2 Our version of TCP-Lite corresponds to Lite.4 proposed in [7].
Reference: [2] <author> M. F. Arlitt and C. L. Williamson. </author> <title> Web Server Workload Characterization: The Search for Invariants. </title> <booktitle> In Proceedings of the ACM SIGMETRICS '96 Conference, </booktitle> <address> Philadelphia, PA, </address> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: The work in <ref> [2] </ref> shows that the mean size of HTTP transfers is currently about 13K. With the widespread use of the Web, the importance of these relatively short transfers cannot be underestimated.
Reference: [3] <author> R. Atkinson. </author> <title> Default IP MTU for use over ATM AAL5. </title> <type> IETF Internet Draft, </type> <month> Feb. </month> <year> 1994. </year> <note> Available by anonymous ftp from ds.internic.net:internet-drafts/internet-drafts/draft-ietf-atm-mtu-07.txt. </note>
Reference-contexts: Host and router computation is assumed to have zero overhead. The simulator clock 5 has a granularity of 1s. All the simulations use a TCP window size of 512KB. The default MTU for IP over ATM is 9180 bytes <ref> [3] </ref>. However, the TCP segment size in all the simulations presented in this paper is 9168 bytes. This size was chosen to be an exact multiple of the ATM payload (48 bytes).
Reference: [4] <author> A. Bestavros and G. Kim. </author> <title> TCP Boston: A Fragmentation-tolerant TCP Protocol for ATM Networks. </title> <booktitle> In Proceedings of INFOCOM, </booktitle> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: So in effect, they cause TCP to experience single segment losses, and TCP is well specialized in recovering from those. A disadvantage of this approach is additional switch complexity and loss of layer transparency. Bestavros and Kim <ref> [4] </ref> suggest making changes to TCP so that it uses the partial 17 packets that it receives on an ATM network. They provide a few controlling parameters in their TCP implementation so that it can be used differently on an ATM network than on a packet switched network.
Reference: [5] <author> F. Bonomi, K. Fendick, and N. Giroux. </author> <title> The Available Bit Rate Service. </title> <address> http://palgong.kyungpook.ac.kr/~bckim/abr.html. </address>
Reference: [6] <author> L. Brakmo, S. W. O' Malley, and L. Peterson. </author> <title> TCP Vegas: New Techniques for Congestion Detection and Avoidance. </title> <booktitle> In Proceedings of the SIGCOMM '94 Symposium, </booktitle> <pages> pages 24-35, </pages> <year> 1994. </year>
Reference-contexts: TCP-Lite 2 is an extension of TCP-Reno and provides support for long fat pipes (high bandwidth-delay paths) amongst other improvements. However, the congestion control algorithms used in TCP Lite are essentially the same as those in TCP Reno. TCP-Vegas is an implementation of TCP that is described in <ref> [6, 8] </ref>. TCP-Vegas tries to eliminate the self-induced losses in TCP-Reno. It modifies the slow-start mechanism of TCP-Reno and provides new retransmission and congestion avoidance mechanisms. The performance of TCP-Vegas against TCP-Reno was also evaluated in [1]. <p> in the addition of its congestion avoidance mechanisms (Vegas-SS and Vegas-CA). 6 Further experimentation may be required to tune this scheme for effective operation over heterogeneous networks. 14 The Vegas congestion avoidance mechanisms try to avoid losses by monitoring the difference between the expected sending rate and actual sending rate <ref> [6] </ref>. This difference is maintained so as to occupy only a fixed amount of buffer space in the network. These mechanisms do not perform well with very small buffer sizes.
Reference: [7] <author> L. Brakmo and L. Peterson. </author> <title> Performance Problems in 4.4BSD TCP. </title> <journal> ACM Computer Communication Review, </journal> <volume> 25(5) </volume> <pages> 69-86, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: The performance of TCP-Vegas against TCP-Reno was also evaluated in [1]. The rest of this section discusses key differences between ATM and packet networks, how these differences interact with TCP dynamics, and the manner in which they 2 Our version of TCP-Lite corresponds to Lite.4 proposed in <ref> [7] </ref>. This version fixes many bugs in the original code. 3 affect TCP performance. Many of these observation have been made elsewhere in the literature; we recount them here for completeness. <p> The MTU determines TCP's maximal segment size, which in turn determines the length of traffic bursts generated by a TCP sender (TCP can send up to 2 maximal sized segments per receiver acknowledgment during slow-start; under certain conditions it can even send more <ref> [7] </ref>). A larger MTU therefore requires more buffering at the switches to absorb traffic bursts. In addition, a larger MTU may aggravate the problem of ACK compression [27]. 3 Network Configuration This section describes the simulation environment that was used to produce the results presented in the following sections. <p> These facts will be analyzed in more detail in the following sections. 3 A description of the traces shown in these figures is given in Appendix A. <ref> [7] </ref> gives a more detailed description of the same. 8 5 Analysis of TCP Enhancements In this section, we present and discuss the results of a detailed simulation study to assess the performance of various TCP enhancements over ATM. <p> Note also that Vegas-RM alone accounts for a large fraction of the throughput gains that Vegas obtains over Lite on packet networks. 5 This Vegas implementation was derived from TCP-Reno but it does support the big-windows extension. Brakmo has shown in <ref> [7] </ref> that the code for TCP-Reno remains 5% behind TCP-Lite after his suggested fixes have been applied. We applied the RTO enhancements suggested in [7] to the Vegas code so as to make the codes compatible in performance when similar algorithms are being used. 12 5.1.1 Adaptive Fast Retransmit We have <p> Brakmo has shown in <ref> [7] </ref> that the code for TCP-Reno remains 5% behind TCP-Lite after his suggested fixes have been applied. We applied the RTO enhancements suggested in [7] to the Vegas code so as to make the codes compatible in performance when similar algorithms are being used. 12 5.1.1 Adaptive Fast Retransmit We have shown that the Vegas-RM retransmission mechanism can more effectively handle multiple segment losses in one RTT. <p> TCP-Lite performs round-trip time (RTT) estimation and schedules retransmission timeouts (RTOs) at a granularity of 500 msecs. It is widely recognized that these coarse-grained timers pose problems for TCP on high-bandwidth networks due to the large discrepancy between the actual RTT and the scheduled RTO <ref> [7, 14] </ref>. 16 We have performed simulations with TCP implementations that schedule timeouts at a granularity of 10ms. The performance of Vegas and Vegas-AFR on both ATM and packet networks are not significantly affected by the addition of fine-grained timeouts.
Reference: [8] <author> L. Brakmo and L. Peterson. </author> <title> TCP Vegas: End to End Congestion Avoidance on a Global Internet. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 13(8) </volume> <pages> 1465-1480, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: TCP-Lite 2 is an extension of TCP-Reno and provides support for long fat pipes (high bandwidth-delay paths) amongst other improvements. However, the congestion control algorithms used in TCP Lite are essentially the same as those in TCP Reno. TCP-Vegas is an implementation of TCP that is described in <ref> [6, 8] </ref>. TCP-Vegas tries to eliminate the self-induced losses in TCP-Reno. It modifies the slow-start mechanism of TCP-Reno and provides new retransmission and congestion avoidance mechanisms. The performance of TCP-Vegas against TCP-Reno was also evaluated in [1]. <p> The round-trip delay is measured by timing a distinguished segment. The bandwidth is estimated using the least-squares estimation on the time of receipt of three closely-spaced ACK's received at the sender. A similar attempt was also made in the context of TCP-Vegas <ref> [8] </ref>. There are problems associated with the measurement of both the bandwidth as well as the round-trip delay. ACK compression [27] and ACK clustering [23] can affect the estimation of the available bandwidth. Furthermore, the measurement of the round-trip delay by timing a segment would include the queuing delays. <p> In essence, SACK-TCP would also solve the problem that AFR tries to solve. While AFR requires changes only at the sender side, SACK-TCP requires making changes to existing implementations 18 both at the sender as well as the receiver side. The work in <ref> [8] </ref> attempts to control the "instantaneous sending rate" during slow-start. In slow-start, TCP-Lite sends segments at twice the rate at which the ACKs arrive (as two segments are sent for every ACK). <p> Even if the value of ssthresh reflects the correct bandwidth-delay product segments might still be dropped if the available switch buffer space is not sufficient. An experimental version of Vegas called Vegas* was proposed in <ref> [8] </ref>. Instead of sending two segments back to back for each ACK during slow-start, an event is scheduled in the future to send the second segment. This scheduling of the event is such that the "instantaneous sending rate" remains the same as the bandwidth of the bottleneck link.
Reference: [9] <author> L. S. Brakmo and L. L. Peterson. </author> <title> Experiences with Network Simulation. </title> <booktitle> In SIG-METRICS, </booktitle> <year> 1996. </year>
Reference-contexts: In addition, a larger MTU may aggravate the problem of ACK compression [27]. 3 Network Configuration This section describes the simulation environment that was used to produce the results presented in the following sections. The simulations were done using the x-sim network simulator <ref> [9] </ref>, which is based on the x-kernel [15]. x-sim is an execution-driven network simulator, where the actions of network protocols are simulated by executing its actual protocol implementation code, rather than an abstract behavioral model of the protocol. Host and router computation is assumed to have zero overhead.
Reference: [10] <author> B. J. Ewy, J. B. Evans, V. S. Frost, and G. J. Minden. </author> <title> TCP/ATM Experiences in the MAGIC Testbed. </title> <publisher> ftp://ftp.tisl.ukans.edu/pub/papers/TCP-Perform.ps. </publisher>
Reference: [11] <author> K. Fall and S. Floyd. </author> <title> Simulation-based Comparisons of Tahoe, Reno, and SACK TCP. </title> <address> ftp://ftp.ee.lbl.gov/papers/sacks.ps.Z, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: A second reason is that Lite (and Reno) halve the congestion window for each lost segment detected by duplicate ACKs. As a result, after the fast retransmission of the 2nd lost segment, no further segments can be sent until an ACK for the lost segments arrives (also noted in <ref> [11] </ref>). This causes a stall in TCP's transmission pipeline, temporarily underutilizing the available network capacity. Finally, consider the case where a timeout occurs after some of the losses were detected by fast retransmit, causing the congestion window to be halved multiple times. <p> The work in [14] attempts to make TCP deal effectively with multiple segment losses in the same congestion window. Although it does not incorporate Vegas-styled congestion avoidance, we expect the proposed TCP implementation to perform considerably better over ATM than TCP-Lite. Fall and Floyd <ref> [11] </ref> also investigates the effect of multiple packet losses on the congestion control algorithms of TCP-Reno. They point out that the absence of selective acknowledgments imposes limits on TCP's performance. Their work also shows that TCP with selective acknowledgments can effectively recover from multiple packet losses.
Reference: [12] <author> S. Floyd. </author> <title> TCP and Successive Fast Retransmits. </title> <address> ftp://ftp.ee.lbl.gov/papers/fastretrans.ps, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: While TCP-Lite can effectively recover from the loss of a single segment in one RTT using its fast retransmit mechanism [24], it is not well equipped to handle the loss of two or more segments in a single RTT <ref> [14, 12] </ref>. First, the fast retransmit mechanism can only detect lost segments if the congestion window is large enough to supply duplicate ACKs for each lost segment. If the congestion window is too small, a costly timeout is necessary to recover from the losses. <p> This allows Vegas to detect when its congestion window has exceeded the available capacity of the network, potentially before losses have occurred. As a result, Vegas is not completely dependent on the initial ssthresh value for controlling congestion. Floyd <ref> [12] </ref> discusses the problem of invoking fast retransmit mechanism multiple times for the same window of data. The work in [14] attempts to make TCP deal effectively with multiple segment losses in the same congestion window.
Reference: [13] <author> S. Floyd and V. Jacobson. </author> <title> On Traffic Phase Effects in Packet-Switched Gateways. Internetworking: </title> <journal> Research and Experience, </journal> <volume> 3(3) </volume> <pages> 115-156, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: It more closely resembles a real internet where different hosts on a LAN (local area network) transfer data to different destinations across a WAN (wide area network). When all senders start simultaneously sending data to the respective destinations, we noticed segregation effects <ref> [13, 22] </ref> for low switch buffer sizes. This caused some of the connections to get most of the bandwidth causing the other connections to starve. We staggered the startup times of the senders by 10ms to allow their congestion windows to grow for some time before experiencing congestion.
Reference: [14] <author> J. C. Hoe. </author> <title> Improving the Start-up Behaviour of a Congestion Control Scheme for TCP. </title> <booktitle> In Proceedings of the SIGCOMM '96 Symposium, </booktitle> <year> 1996. </year>
Reference-contexts: While TCP-Lite can effectively recover from the loss of a single segment in one RTT using its fast retransmit mechanism [24], it is not well equipped to handle the loss of two or more segments in a single RTT <ref> [14, 12] </ref>. First, the fast retransmit mechanism can only detect lost segments if the congestion window is large enough to supply duplicate ACKs for each lost segment. If the congestion window is too small, a costly timeout is necessary to recover from the losses. <p> TCP-Lite performs round-trip time (RTT) estimation and schedules retransmission timeouts (RTOs) at a granularity of 500 msecs. It is widely recognized that these coarse-grained timers pose problems for TCP on high-bandwidth networks due to the large discrepancy between the actual RTT and the scheduled RTO <ref> [7, 14] </ref>. 16 We have performed simulations with TCP implementations that schedule timeouts at a granularity of 10ms. The performance of Vegas and Vegas-AFR on both ATM and packet networks are not significantly affected by the addition of fine-grained timeouts. <p> The difficulty with this approach is that it requires an application to know a priori what type of network its traffic will traverse. This approach also causes loss of layer transparency. Hoe <ref> [14] </ref> addresses the problem of improving the startup behavior of the TCP congestion control algorithms. During slow-start, TCP-Lite doubles its congestion window every RTT until either losses occur or the congestion window's size reaches a value specified by the slow-start threshold (ssthresh). <p> During initial connection startup, the ssthresh is normally set to the maximum possible advertised window size as nothing is known about the underlying network. The uninhibited growth of congestion window can cause severe congestion and is confirmed by our simulation results with TCP-Lite. The work in <ref> [14] </ref> proposes to set the initial value of ssthresh by measuring the bandwidth-delay product. The round-trip delay is measured by timing a distinguished segment. The bandwidth is estimated using the least-squares estimation on the time of receipt of three closely-spaced ACK's received at the sender. <p> As a result, Vegas is not completely dependent on the initial ssthresh value for controlling congestion. Floyd [12] discusses the problem of invoking fast retransmit mechanism multiple times for the same window of data. The work in <ref> [14] </ref> attempts to make TCP deal effectively with multiple segment losses in the same congestion window. Although it does not incorporate Vegas-styled congestion avoidance, we expect the proposed TCP implementation to perform considerably better over ATM than TCP-Lite.
Reference: [15] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The simulations were done using the x-sim network simulator [9], which is based on the x-kernel <ref> [15] </ref>. x-sim is an execution-driven network simulator, where the actions of network protocols are simulated by executing its actual protocol implementation code, rather than an abstract behavioral model of the protocol. Host and router computation is assumed to have zero overhead. The simulator clock 5 has a granularity of 1s.
Reference: [16] <author> V. Jacobson. </author> <title> Congestion Avoidance and Control. </title> <booktitle> In Proceedings of the SIGCOMM '88 Symposium, </booktitle> <pages> pages 314-32, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: In Section 6 we discuss other related-work and we conclude by summarizing our results in Section 7. 2 Dynamics of TCP over ATM Since the seminal work of Van Jacobson in <ref> [16] </ref> the BSD releases of TCP have undergone several important changes from 4.3BSD Tahoe in 1988, to 4.3BSD Reno (TCP Reno) in 1990 and finally 4.4BSD-Lite (TCP Lite) [25] in 1994.
Reference: [17] <author> V. Jacobson. </author> <title> Berkeley TCP evolution from 4.3-tahoe to 4.3-reno. </title> <booktitle> In Proceedings of the Eighteenth Internet Engineering Task Force, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: The congestion control techniques incorporated in these implementations include the slow-start mechanism and the fast retransmit and the fast recovery algorithms <ref> [17, 24] </ref>. TCP-Lite 2 is an extension of TCP-Reno and provides support for long fat pipes (high bandwidth-delay paths) amongst other improvements. However, the congestion control algorithms used in TCP Lite are essentially the same as those in TCP Reno.
Reference: [18] <author> R. Jain. </author> <title> The Art of Computer Systems Performance Analysis: Techniques for ExperimentalDesign, Measurement, </title> <publisher> Simulation and Modelling . John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This eliminates most of the segregation effects and all results reported in this paper have a fairness of at least 0.79 for topology 1 and 0.70 for topology 2, as measured by Jain's Fairness index <ref> [18] </ref>. Most of the results reported have a fairness of above 0.95. 6 4 TCP-Lite over ATM In this section we compare the performance of TCP-Lite over ATM and packet networks.
Reference: [19] <author> J. C. </author> <title> Mogul. </title> <booktitle> The Case for Persistent-Connection HTTP. In Proceedings of the SIGCOMM '95 Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: With the widespread use of the Web, the importance of these relatively short transfers cannot be underestimated. Slow-start is known to slow down the time for short transfers because of the many RTTs taken for filling up the pipe <ref> [19] </ref>. TCP-Vegas starts slow-start with an initial congestion window of 2 segments instead of 1, as in Lite.
Reference: [20] <author> R. Morris and H. T. Kung. </author> <title> Impact of ATM Switching and Flow Control on TCP Performance: Measurements on an Experimental Switch. </title> <booktitle> GLOBECOM, </booktitle> <year> 1995. </year> <month> ftp://virtual.harvard.edu:/pub/htk/atm/globecom995.ps.gz. </month>
Reference-contexts: Hence in general, dropping a segments's worth of data at an ATM switch might lead to the loss of cells from several TCP segments. In particular, two consecutive segments from a TCP connection are often lost when a switch drops only one segment's worth of data. <ref> [20] </ref>. Interleaving of Cells: The cells from different TCP connections (each using a different virtual circuit) may be interleaved in an ATM network. Hence dropping one segment's worth of data at an ATM switch can lead to the loss of segments from several TCP connections. <p> As mentioned earlier, cell losses in plain ATM switches may span segment boundaries, resulting in the loss of two TCP segments as opposed to one in a packet network <ref> [20] </ref>. While TCP-Lite can effectively recover from the loss of a single segment in one RTT using its fast retransmit mechanism [24], it is not well equipped to handle the loss of two or more segments in a single RTT [14, 12].
Reference: [21] <author> M. Perloff and K. Reiss. </author> <title> Improvements to TCP Performance in High-Speed ATM Networks. </title> <journal> Communications of the ACM, </journal> <volume> 38(2) </volume> <pages> 90-100, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The link bandwidth was chosen to be 141.33 Mbps so as to make the transmission of an ATM cell (53 bytes) an integral number of microseconds (3s). The round-trip propagation delay is 60ms. This topology resembles the ones used in other studies on TCP dynamics <ref> [22, 21] </ref>. The second topology is shown in Figure 2. This has two more switches (R3 and R4) than the first topology. Four senders each are connected to switches R1 and R2. Four destinations each are connected to R3 and R4.
Reference: [22] <author> A. Romanow and S. Floyd. </author> <title> The Dynamics of TCP Traffic over ATM Networks. </title> <booktitle> In Proceedings of the SIGCOMM '94 Symposium, </booktitle> <pages> pages 79-88, </pages> <year> 1994. </year>
Reference-contexts: This mismatch between ATM and TCP/IP can lead to several undesirable effects on TCP performance: Bandwidth and resource wastage: The remaining cells from partially dropped segments occupy network resources and waste link bandwidth, only to be discarded at the destination <ref> [22] </ref>. This is likely to cause more congestion, since dropping n cells leads to the eventual retransmission of m cells, for n m 2 fl n fl M T U=48 (in the worst case, each cell drop could corrupt two segments and each segment has MTU/48 cells). <p> The link bandwidth was chosen to be 141.33 Mbps so as to make the transmission of an ATM cell (53 bytes) an integral number of microseconds (3s). The round-trip propagation delay is 60ms. This topology resembles the ones used in other studies on TCP dynamics <ref> [22, 21] </ref>. The second topology is shown in Figure 2. This has two more switches (R3 and R4) than the first topology. Four senders each are connected to switches R1 and R2. Four destinations each are connected to R3 and R4. <p> It more closely resembles a real internet where different hosts on a LAN (local area network) transfer data to different destinations across a WAN (wide area network). When all senders start simultaneously sending data to the respective destinations, we noticed segregation effects <ref> [13, 22] </ref> for low switch buffer sizes. This caused some of the connections to get most of the bandwidth causing the other connections to starve. We staggered the startup times of the senders by 10ms to allow their congestion windows to grow for some time before experiencing congestion. <p> This shows that improving timer granularity alone is not sufficient to make TCP-Lite perform as well over ATM as on packet networks. With AFR, the difference in performance narrows down to below 10%. 6 Related Work Romanow and Floyd <ref> [22] </ref> have proposed switch level enhancements to improve the performance of TCP over ATM. Partial Packet Discard involves dropping all subsequent cells from a TCP segment once one cell is dropped. Early Packet discard involves dropping entire TCP segments prior to switch buffer overflow.
Reference: [23] <author> S. Shenker, L. Zhang, and D. Clark. </author> <title> Some Observations on the Dynamics of a Congestion Control Algorithm. </title> <journal> ACM Computer Communication Review, </journal> <volume> 20(4) </volume> <pages> 30-39, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: A similar attempt was also made in the context of TCP-Vegas [8]. There are problems associated with the measurement of both the bandwidth as well as the round-trip delay. ACK compression [27] and ACK clustering <ref> [23] </ref> can affect the estimation of the available bandwidth. Furthermore, the measurement of the round-trip delay by timing a segment would include the queuing delays. Even if the bandwidth was estimated correctly, the value calculated using this value of round-trip delay would be larger than the actual bandwidth-delay product.
Reference: [24] <author> W. Stevens. </author> <title> TCP/IP Illustrated Volume 1 : The Protocols. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year>
Reference-contexts: The congestion control techniques incorporated in these implementations include the slow-start mechanism and the fast retransmit and the fast recovery algorithms <ref> [17, 24] </ref>. TCP-Lite 2 is an extension of TCP-Reno and provides support for long fat pipes (high bandwidth-delay paths) amongst other improvements. However, the congestion control algorithms used in TCP Lite are essentially the same as those in TCP Reno. <p> While TCP-Lite can effectively recover from the loss of a single segment in one RTT using its fast retransmit mechanism <ref> [24] </ref>, it is not well equipped to handle the loss of two or more segments in a single RTT [14, 12]. First, the fast retransmit mechanism can only detect lost segments if the congestion window is large enough to supply duplicate ACKs for each lost segment.
Reference: [25] <author> G. Wright and W. Stevens. </author> <title> TCP/IP Illustrated Volume 2 : The Implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: conclude by summarizing our results in Section 7. 2 Dynamics of TCP over ATM Since the seminal work of Van Jacobson in [16] the BSD releases of TCP have undergone several important changes from 4.3BSD Tahoe in 1988, to 4.3BSD Reno (TCP Reno) in 1990 and finally 4.4BSD-Lite (TCP Lite) <ref> [25] </ref> in 1994. The congestion control techniques incorporated in these implementations include the slow-start mechanism and the fast retransmit and the fast recovery algorithms [17, 24]. TCP-Lite 2 is an extension of TCP-Reno and provides support for long fat pipes (high bandwidth-delay paths) amongst other improvements.
Reference: [26] <author> L. Zhang. </author> <title> Why TCP Timers Don't Work Well. </title> <booktitle> In Proceedings of the SIGCOMM '86 Symposium, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Also this method can also fail in the presence of ACK compression [27] when the ACK spacing does not give a reliable estimate of the available bandwidth. The work in <ref> [26] </ref> discusses the intrinsic limitations of timers in improving performance. 7 Conclusions This paper makes four contributions. First, we show that the inefficiency of TCP-Lite's retransmission algorithms in recovering from multiple-packet losses is a major factor responsible for throughput degradation over ATM.
Reference: [27] <author> L. Zhang, S. Shenker, and D. D. Clark. </author> <title> Observations on the Dynamics of a Congestion Control Algorithm: The Effects of Two-Way Traffic. </title> <booktitle> In Proceedings of the SIGCOMM '91 Symposium, </booktitle> <pages> pages 133-148, </pages> <year> 1991. </year>
Reference-contexts: A larger MTU therefore requires more buffering at the switches to absorb traffic bursts. In addition, a larger MTU may aggravate the problem of ACK compression <ref> [27] </ref>. 3 Network Configuration This section describes the simulation environment that was used to produce the results presented in the following sections. <p> A similar attempt was also made in the context of TCP-Vegas [8]. There are problems associated with the measurement of both the bandwidth as well as the round-trip delay. ACK compression <ref> [27] </ref> and ACK clustering [23] can affect the estimation of the available bandwidth. Furthermore, the measurement of the round-trip delay by timing a segment would include the queuing delays. <p> The problem with this approach is that very fine-grained timers are required which can schedule events at the granularity of the inter-arrival times of ACKS. Also this method can also fail in the presence of ACK compression <ref> [27] </ref> when the ACK spacing does not give a reliable estimate of the available bandwidth. The work in [26] discusses the intrinsic limitations of timers in improving performance. 7 Conclusions This paper makes four contributions.
References-found: 27

