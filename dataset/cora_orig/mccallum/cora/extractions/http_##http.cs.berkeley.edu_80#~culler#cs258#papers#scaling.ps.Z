URL: http://http.cs.berkeley.edu:80/~culler/cs258/papers/scaling.ps.Z
Refering-URL: http://http.cs.berkeley.edu:80/~culler/cs258/
Root-URL: http://www.cs.berkeley.edu
Title: Scaling Parallel Programs for Multiprocessors: Methodology and Examples  
Author: Jaswinder Pal Singh, John L. Hennessy and Anoop Gupta 
Date: 7, July 1993.  
Note: In IEEE COMPUTER, vol. 27, no.  
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: To design effective large-scale multiprocessors, computer architects need to understand how parallel applications will be scaled to use these machines. The methods used to study scaling so far have been based on the computation and communication complexities of individual algorithms, with the data set size typically being the only application parameter that is scaled. The scaling of real applications, however, is considerably more complicated. Other parameters often need to be scaled along with the data set size to reflect how an application scientist would really use additional computing power. We show that system designers must understand these application considerations and use appropriate scaling methods for their benchmark applications, to reach the correct conclusions about the effectiveness and design of larger machines. To demonstrate this, we propose a realistic scaling methodology, apply it to some important scientific applications, and compare the resulting architectural implications with those that would be obtained by scaling only the data set size.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: The problem size is specified by the input parameters that the application is run with; for a particular machine, it may be defined as the number of instructions executed in a uniprocessor run, for example. As pointed out by Amdahl <ref> [1] </ref>, CPS scaling leads to a rapid reduction in parallel efficiency as more processors are used to solve a fixed-size, deterministic problem. Amdahl argued that most parallel programs have some portion of their execution that is inherently serial and must be executed by a single processor while others remain idle.
Reference: [2] <author> Joshua E. Barnes. </author> <type> Personal communication. </type> <month> May </month> <year> 1991. </year>
Reference-contexts: However, the notion of different identifiable sources of simulation error gives rise to a principle that has both intuitive appeal and widespread practical applicability in many physical domains <ref> [2] </ref>: * All sources of error should be scaled so that their error contributions are about equal. This is the scaling principle we use in our study. Clearly, this scaling principle does not apply to all classes of applications.
Reference: [3] <author> Joshua E. Barnes and Piet Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324(4) </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: Our primary example application is a simulation of galaxies using the Barnes-Hut hierarchical N-body method <ref> [3] </ref>, though we use some other applications to support our arguments. Section 2 of this paper discusses the question of how much to scale an application, and the development of scaling models (constraints) in the literature.
Reference: [4] <author> Joshua E. Barnes and Piet Hut. </author> <title> Error analysis of a tree code. </title> <journal> Astrophysics Journal Supplement, </journal> <volume> 70 </volume> <pages> 389-417, </pages> <year> 1989. </year>
Reference-contexts: The last source is a static machine characteristic, and we ignore it. Studies have been done in the astrophysics community to investigate the impacts of the other four parameters on simulation accuracy <ref> [4, 11] </ref>. <p> | | | | | | | | | | | | | | | | | | Number of Processors Speedup fi fi fi fi fi fl fl fl fl Figure 3: Speedups on the Simulator and the DASH multiprocessor (16K particles, = 0.7). * : The results in <ref> [4, 11] </ref> demonstrate a scaling of the force-calculation error proportional to 2 in the range of practical interest (this is for the original Barnes-Hut algorithm: If higher order moments are incorporated in the force calculation, the error scales more quickly with ).
Reference: [5] <author> Graham Carey, Joe Schmidt, Vineet Singh, and Den-nis Yelton. </author> <title> A scalable, object-oriented finite element solver for partial differential equations on multicom-puters. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms. <p> Since computer scientists think about the complexity of an algorithm in terms of its input data set size, this size is typically the only application parameter whose impact on scaling is considered <ref> [10, 8, 5, 6, 13] </ref>. The scaling of real applications is more complicated in practice.
Reference: [6] <author> Ian Foster, William Gropp, and Rick Stevens. </author> <title> The parallel scalability of the spectral transform method. </title> <address> (MCS-P215-0291), </address> <month> September </month> <year> 1991. </year>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms. <p> Since computer scientists think about the complexity of an algorithm in terms of its input data set size, this size is typically the only application parameter whose impact on scaling is considered <ref> [10, 8, 5, 6, 13] </ref>. The scaling of real applications is more complicated in practice.
Reference: [7] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> A fast algorithm for particle simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 73(325), </volume> <year> 1987. </year>
Reference: [8] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms. <p> Since computer scientists think about the complexity of an algorithm in terms of its input data set size, this size is typically the only application parameter whose impact on scaling is considered <ref> [10, 8, 5, 6, 13] </ref>. The scaling of real applications is more complicated in practice.
Reference: [9] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year> <month> 11 </month>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms.
Reference: [10] <author> John L. Gustafson, Gary R. Montry, and Robert E. Brenner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 532-533, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Since computer scientists think about the complexity of an algorithm in terms of its input data set size, this size is typically the only application parameter whose impact on scaling is considered <ref> [10, 8, 5, 6, 13] </ref>. The scaling of real applications is more complicated in practice. <p> Researchers at Sandia National Laboratories were the first to propose scaling models other than CPS scaling. They obtained very good "scaled speedups" on a 1024 processor machine by scaling problems so that their memory requirements grew linearly with the number of processors <ref> [10] </ref>. This is known as memory-constrained (M-C) scaling. It assumes that a user always wants to run the largest data set possible without overflowing the machine's memory. Memory-constrained scaling can lead to unacceptable increases in execution time, which can make it unrealistic from a user's point of view. <p> The ocean application [14] simulates eddy currents in an o cean basin, using a multigrid algorithm to solve elliptic partial differential equations at every time-step. And the Wave Propagation application, used in the Sandia experiments <ref> [10] </ref>, simulates the passage of a two-dimensional wave through a set of acoustic deflectors. <p> The communication-to-computation ratio usually in creases quickly with the number of processors under CPS scaling. It is, however, tempting to believe that the ratio can be kept constant on larger machines by simply running "proportionally scaled" problems. For example, in the programs used in the Sandia experiments <ref> [10] </ref>, for example, the primary dependence of both communication-and-computation on the data set size (n) and the number of processors (p) could be expressed as a simple function of n p .
Reference: [11] <author> Lars Hernquist. </author> <title> Performance characteristics of tree codes. </title> <journal> Astrophysics Journal Supplement, </journal> <volume> 64 </volume> <pages> 715-734, </pages> <year> 1987. </year>
Reference-contexts: The last source is a static machine characteristic, and we ignore it. Studies have been done in the astrophysics community to investigate the impacts of the other four parameters on simulation accuracy <ref> [4, 11] </ref>. <p> | | | | | | | | | | | | | | | | | | Number of Processors Speedup fi fi fi fi fi fl fl fl fl Figure 3: Speedups on the Simulator and the DASH multiprocessor (16K particles, = 0.7). * : The results in <ref> [4, 11] </ref> demonstrate a scaling of the force-calculation error proportional to 2 in the range of practical interest (this is for the original Barnes-Hut algorithm: If higher order moments are incorporated in the force calculation, the error scales more quickly with ). <p> The main sources of memory requirement are the data for particles and tree cells. The former require memory proportional to n. The latter depend on the spatial distribution as well as on n, but are also found to be proportional to n for distributions of interest <ref> [11] </ref>. <p> Computational Complexity The serial time complexity of the application depends on n in an O (n log n) fashion for realistic ranges of . The dependence on is found to be roughly proportional to 1 2 for fixed n <ref> [11] </ref>. Finally, the complexity is roughly proportional to the number of time-steps; that is, inversely proportional to t when a fixed amount of physical time is being simulated, which is usually the case.
Reference: [12] <author> Dan Lenoski, James Laudon, Kourosh Gharachor-loo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference: [13] <author> Daniel Nussbaum and Anant Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 57-61, </pages> <year> 1991. </year>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms. <p> Since computer scientists think about the complexity of an algorithm in terms of its input data set size, this size is typically the only application parameter whose impact on scaling is considered <ref> [10, 8, 5, 6, 13] </ref>. The scaling of real applications is more complicated in practice.
Reference: [14] <author> Jaswinder Pal Singh and John L. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experiences, results, implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1) </volume> <pages> 27-48, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Not surprisingly, therefore, similar results hold for many scientific applications that we have examined. Some examples are summarized in Table 1. The Fast Multipole application uses the Fast Multipole Method [7]|a recent and very promising algorithm for solving N-body problems|in a galactic simulation. The ocean application <ref> [14] </ref> simulates eddy currents in an o cean basin, using a multigrid algorithm to solve elliptic partial differential equations at every time-step. And the Wave Propagation application, used in the Sandia experiments [10], simulates the passage of a two-dimensional wave through a set of acoustic deflectors.
Reference: [15] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Implications of hierarchical N-body techniques for multiprocessor architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: is no difference between realistic and naive scaling for memory requirements in a single address space (multiprocessors that don't provide a shared address space may require substantial replication of memory in private address spaces, the amount of this memory being likely to depend on other parameters than n as well <ref> [15] </ref>). Computational Complexity The serial time complexity of the application depends on n in an O (n log n) fashion for realistic ranges of . The dependence on is found to be roughly proportional to 1 2 for fixed n [11]. <p> memory-constrained scaling, an increase in the number of processors and hence memory by a factor of k allows us to run a problem with k times as many bodies (assuming a shared address space and no need for data replication in main memory, which is the case in this application <ref> [15] </ref>). Increasing the number of processors does not affect the shared data requirements, and the amount of per-processor private memory used is negligible in comparison.
Reference: [16] <author> Jaswinder Pal Singh, Chris Holt, Takashi Totsuka, Anoop Gupta, and John L. Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <note> To appear. Preliminary version available as Stanford U-niveristy Tech. Report no. CSL-TR-92-505, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: The partitioning technique we use yields a balanced workload across processors and provides data locality by ensuring that particles assigned to a processor are close together in space <ref> [16] </ref>. To demonstrate that the program achieves successful parallel performance, Figure 3 shows the speedups obtained under constant problem size scaling on a state-of-the-art experimental multiprocessor, the Stanford DASH multiprocessor, as well as on a larger simulated multiprocessor described in Section 7.1.
Reference: [17] <author> Patrick H. Worley. </author> <title> The effects of time constraints on scaled speedup. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11(5) </volume> <pages> 838-858, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: However, the latter, which we believe is critical for the proper design and evaluation of parallel systems, has not. Although several researchers have studied the scaling of parallel algorithms and architectures <ref> [9, 17, 8, 5, 6, 13] </ref>, the work so far has mostly been based on the computation and communication complexity of individual algorithms, without incorporating considerations imposed by the applications that use these algorithms. <p> fi fi 1024 particles fl fl 128 particles | | | | | | | | | | | | | | | | | | Number of Processors Speedup fi fi fi fl fl fl that constrains the execution time of the parallel program rather than its memory utilization <ref> [17] </ref>. This model is called time-constrained (TC) scaling: The problem is scaled so that the machine takes the same amount of absolute (wall-clock) time to solve it as processors are added. <p> This need to scale other parameters is implicitly addressed by Worley in one of the differential equation solvers he examines (the hyperbolic equation example in <ref> [17] </ref>). However, his reason for scaling the other parameter that he scales is the stability of the algorithm itself, and even his study ignores considerations imposed by real applications that might use the algorithm. Scientific applications often simulate physical phenomena that occur in nature.
References-found: 17

