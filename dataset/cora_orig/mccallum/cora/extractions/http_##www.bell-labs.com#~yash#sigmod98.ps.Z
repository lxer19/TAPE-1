URL: http://www.bell-labs.com/~yash/sigmod98.ps.Z
Refering-URL: http://www.bell-labs.com/~yash/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: anderson@dcs.uky.edu  fyuri,hfk,yashg@research.bell-labs.com  
Title: Replication, Consistency, and Practicality: Are These Mutually Exclusive?  
Author: Todd Anderson Yuri Breitbart Henry F. Korth Avishai Wool 
Address: Lexington, KY 40506  600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: Department of Computer Science University of Kentucky  Bell Laboratories, Lucent Technologies Inc.  
Abstract: Previous papers have postulated that traditional schemes for the management of replicated data are doomed to failure in practice due to a quartic (or worse) explosion in the probability of deadlocks. In this paper, we present results of a simulation study for three recently introduced protocols that guarantee global serializability and transaction atom-icity without resorting to the two-phase commit protocol. The protocols analyzed in this paper include a global locking protocol [10], a "pessimistic" protocol based on a replication graph [5], and an "optimistic" protocol based on a replication graph [7]. The results of the study show a wide range of practical applicability for the lazy replica-update approach employed in these protocols. We show that under reasonable contention conditions and sufficiently high transaction rate, both replication-graph-based protocols outperform the global locking protocol. The distinctions among the protocols in terms of performance are significant. For example, an offered load where 70% - 80% of transactions under the global locking protocol were aborted, only 10% of transactions were aborted under the protocols based on the replication graph. The results of the study suggest that protocols based on a replication graph offer practical techniques for replica management. However, it also shows that performance deteriorates rapidly and dramatically when transaction throughput reaches a saturation point. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Agrawal, A. ElAbbadi, and R. Steinke. </author> <title> Epidemic algorithms in replicated databases. </title> <booktitle> In Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. <p> In [17] another approach to the management of replicated data was described. It ensures eventual replica convergence but does not guarantee global serializability. An alternative approach to replication appears in <ref> [1] </ref>. In [5], we proposed a new approach for guaranteeing global serializability. The protocol reduces the probability of distributed deadlock and lowers the communication overhead as compared with prior work, including [10].
Reference: [2] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concur-rency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: The first protocol we tested was first outlined in [10] and a more precise version (with a correctness proof) appears in [6]. The protocol uses a version of two-phase locking to synchronize read/write (rw and wr) conflicts, and the Thomas Write Rule <ref> [2] </ref> to synchronize write/write (ww) conflicts. The second and third protocols 1 we tested are based on variations of the replication graph technique described in [5]. The difference between these last two protocols is that one of them uses a pessimistic approach while the other takes an optimistic approach. <p> However, the version of the locking protocol in [5] was unnecessarily restrictive. Here we introduce a less restrictive version whose performance is investigated in our experiments. We assume that each local site has a local concurrency control mechanism that guarantees the ACID properties <ref> [2] </ref>. In addition, we assume that there is a global lock manager whose functions we describe below. In this protocol, a transaction must request a read lock from the primary site of each data item that it reads. <p> This is achieved by granting the transaction that updates the primary copy of d a lock which is not released until all data replicas have been updated. Write operations are synchronized using the Thomas Write Rule <ref> [2] </ref>. Thus, in the terminology of [2] the concurrency control mechanism uses the Thomas Write Rule to synchronize ww conflicts and the two-phase locking to synchronize rw and wr conflicts. <p> This is achieved by granting the transaction that updates the primary copy of d a lock which is not released until all data replicas have been updated. Write operations are synchronized using the Thomas Write Rule <ref> [2] </ref>. Thus, in the terminology of [2] the concurrency control mechanism uses the Thomas Write Rule to synchronize ww conflicts and the two-phase locking to synchronize rw and wr conflicts. <p> the Thomas 4 A transaction is said to access a data item d at site s if it has executed a read of d at s or has executed a write on any replica of d regardless of site. 5 We assume the usual notion of read and write conflict <ref> [2] </ref>. A con flict is indirect if it results from a series of direct conflicts. 3 Write Rule [2] (as we shall see in the protocol definitions). Therefore, there is no need to merge virtual sites due to a ww conflict. <p> has executed a read of d at s or has executed a write on any replica of d regardless of site. 5 We assume the usual notion of read and write conflict <ref> [2] </ref>. A con flict is indirect if it results from a series of direct conflicts. 3 Write Rule [2] (as we shall see in the protocol definitions). Therefore, there is no need to merge virtual sites due to a ww conflict. This makes it possible to keep virtual sites smaller and reduce the amount of contention during replica propagation.
Reference: [3] <author> Y. Breitbart, H. Garcia-Molina, and A. Silberschatz. </author> <title> Overview of multidatabase transaction management. </title> <journal> VLDB Journal, </journal> <volume> 1(2), </volume> <year> 1992. </year>
Reference-contexts: Instead of aborting t, t continues as if the write succeeded, though, in fact, the write is ignored. transaction management information from the local transaction managers, unlike the case for multi-database systems <ref> [3] </ref>. Each transaction has a virtual site associated with it at each physical site at which it executes. This virtual site exists from the time the transaction begins until the protocol explicitly removes it from consideration.
Reference: [4] <author> Y. Breitbart, D. Georgakopoulos, M. Rusinkiewicz, and A. Silberschatz. </author> <title> On rigorous transaction scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <year> 1991. </year>
Reference-contexts: In [8] the authors introduced the data-placement graph, and proved that replica consistency can be guaranteed by ensuring the acyclicity of that graph. However, their approach does not guarantee global serializability in the general case. It requires that each local DBMS use rigorous two-phase locking <ref> [4] </ref>. Furthermore, since a site normally contains a large database and the number of sites is usually much smaller than the number of data items, the data-placement graph becomes cyclic quite rapidly, which adversely affects transaction throughput. In [17] another approach to the management of replicated data was described.
Reference: [5] <author> Y. Breitbart and H. F. Korth. </author> <title> Replication and consistency: Being lazy helps sometimes. </title> <booktitle> In Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. <p> In [17] another approach to the management of replicated data was described. It ensures eventual replica convergence but does not guarantee global serializability. An alternative approach to replication appears in [1]. In <ref> [5] </ref>, we proposed a new approach for guaranteeing global serializability. The protocol reduces the probability of distributed deadlock and lowers the communication overhead as compared with prior work, including [10]. <p> The protocol uses a version of two-phase locking to synchronize read/write (rw and wr) conflicts, and the Thomas Write Rule [2] to synchronize write/write (ww) conflicts. The second and third protocols 1 we tested are based on variations of the replication graph technique described in <ref> [5] </ref>. The difference between these last two protocols is that one of them uses a pessimistic approach while the other takes an optimistic approach. Our results show that the replication graph protocols outperform the locking protocol under reasonable contention assumptions, and that replication graph protocols do have practical applicability. <p> After defining the system model, we present the details of each protocol. Much of this section is an extension of the model described in <ref> [5] </ref>. The definition of virtual sites has an important change from [5] that allows the pessimistic protocol in the current paper to be less restrictive than protocol GS of [5]. <p> After defining the system model, we present the details of each protocol. Much of this section is an extension of the model described in <ref> [5] </ref>. The definition of virtual sites has an important change from [5] that allows the pessimistic protocol in the current paper to be less restrictive than protocol GS of [5]. This improved definition of virtual sites is incorporated also into our optimistic protocol. 2.1 System Model We begin with a review of our transaction and data models. <p> Much of this section is an extension of the model described in <ref> [5] </ref>. The definition of virtual sites has an important change from [5] that allows the pessimistic protocol in the current paper to be less restrictive than protocol GS of [5]. This improved definition of virtual sites is incorporated also into our optimistic protocol. 2.1 System Model We begin with a review of our transaction and data models. The database consists of data distributed over a set of sites. <p> However, it would be restarted and re-executed there. Generally, a committed transaction does not have to progress to the completed state, even if it has committed at all sites <ref> [5] </ref>. Our protocols, however, guarantee global serializability by ensuring that every transaction in the committed state eventually reaches the completed state. It was shown by example in [5] that it is possible for consistency to be lost unless committed transactions are transaction can update any data item at its origination site, <p> Generally, a committed transaction does not have to progress to the completed state, even if it has committed at all sites <ref> [5] </ref>. Our protocols, however, guarantee global serializability by ensuring that every transaction in the committed state eventually reaches the completed state. It was shown by example in [5] that it is possible for consistency to be lost unless committed transactions are transaction can update any data item at its origination site, and propagation is done only after t has committed at its origination site. <p> This timestamp is defined to be the timestamp of the transaction that wrote the current value of the data item. 2.2 The Locking Protocol The locking protocol whose performance we study in this paper was first introduced in [10]. A more precise version of this protocol appears in <ref> [5] </ref>. However, the version of the locking protocol in [5] was unnecessarily restrictive. Here we introduce a less restrictive version whose performance is investigated in our experiments. We assume that each local site has a local concurrency control mechanism that guarantees the ACID properties [2]. <p> A more precise version of this protocol appears in <ref> [5] </ref>. However, the version of the locking protocol in [5] was unnecessarily restrictive. Here we introduce a less restrictive version whose performance is investigated in our experiments. We assume that each local site has a local concurrency control mechanism that guarantees the ACID properties [2]. <p> For a given schedule S over the set T [T 0 we require that a replication graph be constructed in compliance with the locality and union rules; thus the replication graph evolves with time according to the schedule. In <ref> [5, 6] </ref>, it was shown that for a schedule S, if there exists an evolution of the replication graph in which the graph is acyclic at every point in time, then S is globally serializable. <p> The specific details for each protocol appear in the next two sections. 2.4 The Pessimistic Protocol In this section, we describe the pessimistic protocol, which is based upon protocol GS of <ref> [5] </ref>. The main distinctions between the protocols are the definition of virtual sites (see Section 2.3.1) and the use of the Thomas Write Rule to handle ww conflicts. These changes lead to smaller virtual sites and thus to less contention in the pessimistic protocol as compared with protocol GS of [5]. <p> <ref> [5] </ref>. The main distinctions between the protocols are the definition of virtual sites (see Section 2.3.1) and the use of the Thomas Write Rule to handle ww conflicts. These changes lead to smaller virtual sites and thus to less contention in the pessimistic protocol as compared with protocol GS of [5]. 1. If T i submits its first operation, assign T i a timestamp. 2. <p> The results clearly suggest that the system model we propose here (and in earlier work, such as <ref> [5] </ref>) is a promising one for practical applications of replicated data. Within the domain of replication-graph based protocols, it appears that the optimistic approach is generally, though not always, better than the pessimistic approach.
Reference: [6] <author> Y. Breitbart and H. F. Korth. </author> <title> Replication and consistency in a distributed environment. </title> <type> Technical report, </type> <institution> Bell Labs, </institution> <year> 1997. </year> <note> Extended version of [5]. </note>
Reference-contexts: The first protocol we tested was first outlined in [10] and a more precise version (with a correctness proof) appears in <ref> [6] </ref>. The protocol uses a version of two-phase locking to synchronize read/write (rw and wr) conflicts, and the Thomas Write Rule [2] to synchronize write/write (ww) conflicts. The second and third protocols 1 we tested are based on variations of the replication graph technique described in [5]. <p> Write operations are synchronized using the Thomas Write Rule [2]. Thus, in the terminology of [2] the concurrency control mechanism uses the Thomas Write Rule to synchronize ww conflicts and the two-phase locking to synchronize rw and wr conflicts. Note that as shown in <ref> [6] </ref>, the locking protocol must ensure that read locks are retained until the transaction completes in order to guarantee global serializability. 2.3 Replication Graph Protocols We begin our description of the two replication graph protocols by presenting two of their key components: the concept of a virtual site and the definition <p> For a given schedule S over the set T [T 0 we require that a replication graph be constructed in compliance with the locality and union rules; thus the replication graph evolves with time according to the schedule. In <ref> [5, 6] </ref>, it was shown that for a schedule S, if there exists an evolution of the replication graph in which the graph is acyclic at every point in time, then S is globally serializable.
Reference: [7] <author> Y. Breitbart, H. F. Korth, and A. Silberschatz. </author> <title> Optimistic protocols for replicated databases. </title> <type> Technical Report BL0112370-970227-07TM, </type> <institution> Bell Laboratories, Lucent Technologies, </institution> <year> 1997. </year>
Reference: [8] <author> P. Chundi, D. J. Rosenkrantz, and S. S. Ravi. </author> <title> Deferred updates and data placement in distributed databases. </title> <booktitle> In Proceedings of the Twelveth International Conference on Data Engineering, </booktitle> <address> New Orleans, Louisiana, </address> <year> 1996. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. <p> Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. In <ref> [8] </ref> the authors introduced the data-placement graph, and proved that replica consistency can be guaranteed by ensuring the acyclicity of that graph. However, their approach does not guarantee global serializability in the general case. It requires that each local DBMS use rigorous two-phase locking [4].
Reference: [9] <institution> CSIM18 simulation engine (C++ version). Mesquite Software, Inc., 3925 W. </institution> <address> Braker Lane, Austin, TX 78759-5321. </address>
Reference-contexts: requires less access to the centrally maintained replication graph, and thus promises reduced load on a part of the system that appears likely to be a bottleneck. 3 The Simulator To evaluate the performance of the three protocols, we developed a simulation model in C++ using the simulation package CSIM <ref> [9] </ref>. The simulated system consists of a set of sites connected by an ATM network. One of these sites serves as the replication graph manager, and is called the graph site. The other sites contain parts of the database and run a simulated local DBMS.
Reference: [10] <author> J. Gray, P. Helland, P. O'Neil, and D. Shasha. </author> <title> The dangers of replication and a solution. </title> <booktitle> In Proceedings of ACM-SIGMOD 1996 International Conference on Management of Data, </booktitle> <address> Montreal, Quebec, </address> <pages> pages 173-182, </pages> <year> 1996. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. <p> It ensures eventual replica convergence but does not guarantee global serializability. An alternative approach to replication appears in [1]. In [5], we proposed a new approach for guaranteeing global serializability. The protocol reduces the probability of distributed deadlock and lowers the communication overhead as compared with prior work, including <ref> [10] </ref>. We are not aware, however, of any performance studies either comparing the lazy replica update protocols or analyzing the performance of any specific lazy replica update protocol. <p> The purpose of this paper is to fill the gap by reporting the results of a simulation study comparing the performance of three replication management protocols that are based on lazy replica update. The first protocol we tested was first outlined in <ref> [10] </ref> and a more precise version (with a correctness proof) appears in [6]. The protocol uses a version of two-phase locking to synchronize read/write (rw and wr) conflicts, and the Thomas Write Rule [2] to synchronize write/write (ww) conflicts. <p> Our results show that the replication graph protocols outperform the locking protocol under reasonable contention assumptions, and that replication graph protocols do have practical applicability. The results validate the conjecture of <ref> [10] </ref> that global locking generates a significant number of deadlocks. However for all three protocols, performance deteriorates rapidly when the transaction rate leads to queuing for data or for physical resources. In Section 2 we discuss the three protocols whose performance we are evaluating here. <p> This timestamp is defined to be the timestamp of the transaction that wrote the current value of the data item. 2.2 The Locking Protocol The locking protocol whose performance we study in this paper was first introduced in <ref> [10] </ref>. A more precise version of this protocol appears in [5]. However, the version of the locking protocol in [5] was unnecessarily restrictive. Here we introduce a less restrictive version whose performance is investigated in our experiments. <p> Although the locking protocol performed poorly relative 11 to the competition in this paper, it, too, showed a consid-erable range of applicability. The promising results in this paper contrast with the conjecture of <ref> [10] </ref> pertaining to unrestricted update regulation: This is a bleak picture, but probably accurate. Simple replication (transactional update-anywhere anytime-anyway) cannot be made to work with global serializability.
Reference: [11] <author> J. Gray, P. Homan, H. F. Korth, and R. Obermarck. </author> <title> A strawman analysis of the probability of wait and deadlock. </title> <type> Technical Report RJ2131, </type> <institution> IBM San Jose Research Laboratory, </institution> <year> 1981. </year>
Reference-contexts: This implies that the transaction size thenumber of database operations in the transaction grows with the degree of replication. Since the deadlock probability grows as the fourth power of the transaction size <ref> [11] </ref>, the eager approach does not scale up well to large databases with a high degree of replication. Furthermore, it does not appear that straightforward modifications of eager replica update propagation can eliminate these deficiencies. Recently, much attention has been directed towards the lazy approach to replica update propagation.
Reference: [12] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The copy of a data item at the primary site is referred to as the primary copy, and all other copies are referred to as secondary copies. The local DBMSs at each site ensure the usual ACID properties <ref> [12] </ref>, and generate a serializable schedule for transactions executing at the local site. The local DBMSs are responsible for managing local deadlocks. The site at which transaction T i is submitted is called the origination site of T i .
Reference: [13] <author> A. A. Helal, A. A. Heddaya, and B. B. Bhargava. </author> <title> Replication Techniques in Distributed Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity.
Reference: [14] <author> E. Holler. </author> <title> Multiple copy update. </title> <booktitle> In Lecture Notes in Computer Science, Distributed Systems | Architecture and Implementation: An Advanced Course. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Depending upon the application, the number of sites may range from only a few sites to several hundred sites. Telecommunication applications require rapid distribution of updates to all replicas with strong guarantees of consistency and availability. Early work on replication, (see, e.g. <ref> [14] </ref> for a survey) focused on eager replica update propagation. Under eager ACM SIGMOD'98, Seattle, June 1998 propagation, the writing of updates to all replicas is part of a single transaction. This implies that the transaction size thenumber of database operations in the transaction grows with the degree of replication.
Reference: [15] <author> R. Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: Had we bounded the degree of replication at a low value (e.g., 5), the number of transactions per second per site would be much higher, especially in our 100-site experiments. Each point in our graphs represents the output of run of 100,000 transactions. Transients <ref> [15] </ref> were eliminated by discarding the first 5 transactions generated by every site. The final measurements were taken when the 100,000'th transaction was submitted, so as to avoid "system wind-down" effects. <p> See <ref> [15] </ref> for a precise definition of confidence interval. 6 than the locking protocol. The differences among the protocols increase significantly with submitted load.
Reference: [16] <author> C. Pu and A. Leff. </author> <title> Replica control in distributed systems: An asynchronous approach. </title> <booktitle> In Proceedings of ACM-SIGMOD 1991 International Conference on Management of Data, </booktitle> <address> Denver, Colorado, </address> <pages> pages 377-386, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity.
Reference: [17] <author> J. Sidell, P. M. Aoki, S. Barr, A. Sah, C. Staelin, M. Stone-braker, and A. Yu. </author> <title> Data replication in Mariposa. </title> <booktitle> In Proceedings of the Twelveth International Conference on Data Engineering, </booktitle> <address> New Orleans, Louisiana, </address> <year> 1996. </year>
Reference-contexts: Recently, much attention has been directed towards the lazy approach to replica update propagation. The lazy approach requires that installation of updates to replicas occur only after the update transaction has committed at the origination site <ref> [8, 10, 13, 16, 17, 5, 1] </ref>. Propagation is performed by independent subtransactions spawned only after the transaction at the origination site has committed. Clearly, the activities of these subtransactions must be managed carefully to ensure global consistency and transaction atomicity. <p> It requires that each local DBMS use rigorous two-phase locking [4]. Furthermore, since a site normally contains a large database and the number of sites is usually much smaller than the number of data items, the data-placement graph becomes cyclic quite rapidly, which adversely affects transaction throughput. In <ref> [17] </ref> another approach to the management of replicated data was described. It ensures eventual replica convergence but does not guarantee global serializability. An alternative approach to replication appears in [1]. In [5], we proposed a new approach for guaranteeing global serializability.
References-found: 17

