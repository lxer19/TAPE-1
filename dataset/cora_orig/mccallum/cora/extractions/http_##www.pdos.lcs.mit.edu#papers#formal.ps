URL: http://www.pdos.lcs.mit.edu/papers/formal.ps
Refering-URL: http://www.pdos.lcs.mit.edu/PDOS-papers.html
Root-URL: 
Title: Implementing Sequentially Consistent Shared Objects using Broadcast and Point-To-Point Communication  
Author: Alan Feketey M. Frans Kaashoekz Nancy Lynchz 
Address: Sydney 2006, Australia. Cambridge MA 02139, U.S.A.  
Affiliation: yDepartment of Computer Science F09 MIT Laboratory for Computer Science University of  
Abstract: A distributed algorithm that implements a sequentially consistent collection of shared read/update objects using a combination of broadcast and point-to-point communication is presented and proved correct. This algorithm is a generalization of one used in the Orca shared object system. The algorithm caches objects in the local memory of processors according to application needs; each read operation accesses a single copy of the object, while each update accesses all copies. Copies of all the objects are kept consistent using a strategy based on sequence numbers for broadcasts. The algorithm is presented in two layers. The lower layer uses the given broadcast and point-to-point communication services, plus sequence numbers, to provide a new communication service called a context multicast channel. The higher layer uses a context multicast channel to manage the object replication in a consistent fashion. Both layers and their combination are described and verified formally, using the I/O automaton model for asynchronous concurrent systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.V. Adve and M.D. Hill. </author> <title> Weak ordering anew definition and some implications. </title> <type> Technical Report TR-902, </type> <institution> University of Wisonsin, Madison, WI, </institution> <month> Dec. </month> <year> 1989. </year>
Reference: [2] <author> Y. Afek, G. Brown, and M. Merritt. </author> <title> Lazy caching. </title> <journal> ACM. Trans. on Programming Languages and Systems, </journal> <volume> 15(1) </volume> <pages> 182-205, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: There are several formalizations of the notion of sequentially consistent memory, differing in subtle ways; we use the state machine definition of Afek, Brown and Merritt <ref> [2] </ref>. Orca runs over the Amoeba operating system [26], which provides two communication services: broadcast and point-to-point communication. Both services provide reliable communication, even in the presence of communication failures. No guarantees are made by Orca if processors fail; therefore, we do not consider processor failures either. <p> Sequential consistency was first defined by Lamport [20]; in this paper, we use an alternative formulation proposed by Afek et al. <ref> [2] </ref>, based on I/O automata. Other papers exploring correctness conditions for shared memory and algorithms that implement them include [1, 3, 5, 8, 9, 11, 12, 13, 15, 16, 17, 21, 24]. <p> In an atomic object, the operations appear to the clients as if they happened in some sequential order, and furthermore, that order must be consistent with the totally-precedes order. Specifically, we let AM, the atomic memory automaton, be the serial object automaton defined by Afek, Brown and Merritt <ref> [2] </ref> for the given collection of objects, except that we generalize it to allow updates that apply functions rather than just blind writes. Note that every client-well-formed fair trace of AM is complete.
Reference: [3] <author> M. Ahamad, P.W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proc. Eleventh International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <address> Arling-ton, TX, </address> <month> May </month> <year> 1991. </year>
Reference: [4] <author> H. Attiya and R. Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 679-691, </pages> <year> 1992. </year>
Reference-contexts: The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing [18]. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch [5] and Attiya and Friedman <ref> [4] </ref>. The rest of the paper is organized as follows. Section 2 introduces basic terminology that is used in the rest of the paper. Section 3 contains the definition of a sequentially consistent shared memory and introduces our new method for proving sequential consistency.
Reference: [5] <author> H. Attiya and J. Welch. </author> <title> Sequential consistency versus linearizability. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(2) </volume> <pages> 91-122, </pages> <year> 1994. </year>
Reference-contexts: The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing [18]. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch <ref> [5] </ref> and Attiya and Friedman [4]. The rest of the paper is organized as follows. Section 2 introduces basic terminology that is used in the rest of the paper. Section 3 contains the definition of a sequentially consistent shared memory and introduces our new method for proving sequential consistency.
Reference: [6] <author> H.E. Bal and M.F. Kaashoek. </author> <title> Object distribution in Orca using compile-time and run-time techniques. </title> <booktitle> In Proc. Eight Annual Conf. on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 162-177, </pages> <address> Washington, DC, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The decision about whether or not to replicate an object is made at run time using information generated by the Orca compiler. The details of this decision process, and also performance measurements to show the benefits of not replicating all objects, can be found in <ref> [6] </ref>. The naive strategy of allowing each read operation to access any copy of the object and each update operation to access all copies is not by itself sufficient to implement a sequentially consistent shared memory. To see why, consider the execution depicted in Figure 1.
Reference: [7] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Trans. on Soft. Eng., </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language <ref> [7] </ref> over the Amoeba distributed operating system [26]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers [7, 25]. <p> This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [7] over the Amoeba distributed operating system [26]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers <ref> [7, 25] </ref>. It provides a simple shared object model in which each object has a state and a set of operations, classified as either read operations or update operations. Read operations do not modify the object state, while update operations may do so.
Reference: [8] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proc. Second Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference: [9] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <address> CMU, Pittsburgh, PA, </address> <month> Sept. </month> <year> 1991. </year>
Reference: [10] <author> K.P. Birman and T.A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The guarantees provided by a context multicast channel are weaker than those that are provided by totally ordered causal multicast channels, as provided by systems such as Isis <ref> [10] </ref>. However, the properties of a context multicast channel are sufficiently strong to support the replica management of the Orca algorithm.
Reference: [11] <author> L.M. Censier and P. Feautrier. </author> <title> A new solution to cache coherence problems in multicache systems. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 1112-1118, </pages> <month> Dec. </month> <year> 1978. </year>
Reference: [12] <author> W.W. Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice Hall Publishers, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference: [13] <author> M. Dubois, C. Scheurich, and F.A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> Feb. </month> <year> 1988. </year> <month> 10 </month>
Reference: [14] <author> A. Fekete, M.F. Kaashoek, and N. Lynch. </author> <title> Imple--menting sequentially consistent shared objects using broadcast and point-to-point communication. </title> <type> Technical Report TM 518, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Section 7 contains a discussion of dynamic reconfiguration, and ideas for future work. Finally, in Section 8 we draw our conclusions. Because of space limitations, most details of the proofs are omitted here. Full details appear in <ref> [14] </ref>. 2 Some Basics 2.1 Partial Orders We use many partial (and total) orders, on events in executions, and on operations. Throughout the paper, we assume that partial and total orders are irreflexive, that is, they do not relate any element to itself.
Reference: [15] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference: [16] <author> P. Gibbons and M. Merritt. </author> <title> Specifying non-blocking shared memories. </title> <booktitle> In Proc. Fourth ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 306-315, </pages> <year> 1992. </year>
Reference: [17] <author> P. Gibbons, M. Merritt, and K. Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories. </title> <booktitle> In Proc. Third ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303, </pages> <year> 1991. </year>
Reference: [18] <author> M.P. Herlihy and J.M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: In most of this work, memory is modeled as a collection of items that are accessed through read and write operations. The study of correctness for shared memory with more general data types was initiated by Herlihy and Wing <ref> [18] </ref>. Sequential consistency and other consistency conditions for general data types has been studied by Attiya and Welch [5] and Attiya and Friedman [4]. The rest of the paper is organized as follows. Section 2 introduces basic terminology that is used in the rest of the paper. <p> Notice that for each client c, totally-precedes fijc totally orders the operations that occur in fijc. 3.2 Definition Our definition of sequential consistency is based on an atomic object [22], also known as a linearizable object <ref> [18] </ref>, whose underlying data type is the entire collection of data objects to be shared. In an atomic object, the operations appear to the clients as if they happened in some sequential order, and furthermore, that order must be consistent with the totally-precedes order.
Reference: [19] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Commun. ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: When a point-to-point message reaches its destination, the recipient delays its delivery until the indicated number of broadcasts have been received. (The idea is similar to the one in Lamport's clock synchronization algorithm <ref> [19] </ref>, but we only apply it to a restricted set of events.) We prove that this algorithm correctly implements a context multicast channel. The higher layer uses an arbitrary context multicast 2 channel to manage the object replication in a consistent fashion.
Reference: [20] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: Read operations do not modify the object state, while update operations may do so. Each operation involves only a single object and appears to be indivisible. More precisely, Orca provides a sequentially consistent memory model <ref> [20] </ref>. Informally speaking, a sequentially consistent memory appears to its users as if it were centralized (even though it may be implemented in a distributed This research was supported in part by ARPA N00014-92-J-4033, NSF 922124-CCR, and ONR-AFOSR F49620-94-1-0199; by ONR contract N00014-94-1-0985 and a NSF Young Investigator Award. fashion). <p> Sequential consistency is widely used because it appears to be closest to what programmers expect from a shared memory system; non-sequentially consistent shared memory systems typically trade programmability for performance. Sequential consistency was first defined by Lamport <ref> [20] </ref>; in this paper, we use an alternative formulation proposed by Afek et al. [2], based on I/O automata. Other papers exploring correctness conditions for shared memory and algorithms that implement them include [1, 3, 5, 8, 9, 11, 12, 13, 15, 16, 17, 21, 24].
Reference: [21] <author> R.J. Lipton and J.S. Sandberg. </author> <title> Pram: a scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Princeton, NJ, </institution> <month> Sept. </month> <year> 1988. </year>
Reference: [22] <author> N. Lynch. </author> <title> Distributed algorithms. </title> <publisher> Morgan Kaufmann publishers, Scheduled for 1995. </publisher>
Reference-contexts: Notice that for each client c, totally-precedes fijc totally orders the operations that occur in fijc. 3.2 Definition Our definition of sequential consistency is based on an atomic object <ref> [22] </ref>, also known as a linearizable object [18], whose underlying data type is the entire collection of data objects to be shared.
Reference: [23] <author> N. Lynch and M. Tuttle. </author> <title> An introduction to input/output automata. </title> <journal> CWI Quarterly, </journal> <volume> 2(3) </volume> <pages> 219-246, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: We prove that this algorithm, combined with any context multicast system, provides a sequentially consistent memory. Our proof uses a new method based on partial orders. All our specifications and proofs are presented in terms of the I/O automaton model for asynchronous concurrent systems <ref> [23] </ref>. General results about the composition of I/O automata allow us to infer the correctness of the complete system from our correctness results for the two separate layers. Many different correctness conditions have been proposed for shared memory, including strong conditions like memory coherence and weaker ones like release consistency. <p> I/O automata can be composed, by identifying actions with the same name. The fair trace semantics is compositional. Output actions of an I/O automaton can also be hidden, which means that they are reclassified as internal actions. See <ref> [23] </ref> for more details. 3 Sequentially Consistent Shared Object Systems In this section, we define a sequentially consistent shared object system and give a new method for proving that a system is sequentially consistent.
Reference: [24] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proc. Fourteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <address> Pittsburg, PA, </address> <month> June </month> <year> 1987. </year>
Reference: [25] <author> A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. </author> <title> Parallel programming using shared objects and broadcasting. </title> <journal> IEEE Computer, </journal> <volume> 25(8) </volume> <pages> 10-19, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [7] over the Amoeba distributed operating system [26]. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers <ref> [7, 25] </ref>. It provides a simple shared object model in which each object has a state and a set of operations, classified as either read operations or update operations. Read operations do not modify the object state, while update operations may do so.
Reference: [26] <author> A.S. Tanenbaum, M.F. Kaashoek, R. van Renesse, and H.E. Bal. </author> <title> The Amoeba distributed operating system a status report. </title> <journal> Computer Communications, </journal> <volume> 14(6) </volume> <pages> 324-335, </pages> <month> Aug. </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: This algorithm is a generalization of one used in the implementation of the Orca distributed programming language [7] over the Amoeba distributed operating system <ref> [26] </ref>. Orca is a language for writing parallel and distributed application programs to run on clusters of workstations, processor pools and massively parallel computers [7, 25]. <p> There are several formalizations of the notion of sequentially consistent memory, differing in subtle ways; we use the state machine definition of Afek, Brown and Merritt [2]. Orca runs over the Amoeba operating system <ref> [26] </ref>, which provides two communication services: broadcast and point-to-point communication. Both services provide reliable communication, even in the presence of communication failures. No guarantees are made by Orca if processors fail; therefore, we do not consider processor failures either.
References-found: 26

