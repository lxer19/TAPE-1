URL: ftp://ftpipr.ira.uka.de/pub/papers/1994/irs94ag.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Phone: 2  
Title: On the reduction of costs for robot controller synthesis  Division of Production Engineering, Machine Design Automation  
Author: Attilio Giordana ?? Michael Kaiser Marnix Nuttin 
Address: C.so Svizzera 185, 10149 Torino, Italy  
Affiliation: 1 Dipartimento di Informatica, Universita di Torino  University of Karlsruhe Institute for Real-Time Computer Systems Robotics 3 Katholieke Universiteit Leuven, Department of Mechanical Engineering  
Abstract: The demand for smarter and more adaptive robots, mainly due to the increasing availability of sophisticated sensor systems for both manipulation and mobile robots and the recently emerging market for intelligent service systems, has a direct impact on the costs of robot programming. Especially the development of control software for industrial robots is becoming more and more expensive. This paper investigates the possibility of automatizing the cycle of designing and refining the control programs. In particular, two function approximators, Time Delay Neural Networks and Fuzzy Controllers, are evaluated showing that both can be induced from examples of control actions supplied by a good teacher. However, due to the difficulty of finding good training sets, pure approximation is not sufficient to solve the task in its generality. Since a human teacher performs not at all optimally in executing control actions and usually uses sensorial inputs that are not directly compatible with the sensor input available to the robot, the off-line generated controller must be refined on-line. Therefore, Reinforcement Learning is discussed and proposed as a methodology to perform the on-line refinement. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.S. Albus. </author> <title> A new approach to manipulator control: The cerebellar model articulation controller. Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <year> 1975. </year>
Reference-contexts: Each node d i in the third layer is associated to a real value r i , representing a point in the range of the function to be approximated, and maps the output (usually 2 <ref> [0; 1] </ref>) of the corresponding node in the second layer to the number r i . Finally, the fourth layer nodes compute the output values by interpolating the output of the second and/or third layer nodes.
Reference: 2. <author> C.W. Anderson. </author> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1986. </year>
Reference: 3. <author> C.W. Anderson. </author> <title> Strategy learning with multilayer connectionist representation. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Laboratory Inc., </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: In particular, Berenji's work on fuzzy controllers, can potentially be very useful for the purpose set in this paper: the GARIC architecture ([9]) is an evolution of the ARIC architecture proposed by Anderson ([2], <ref> [3] </ref>) and is based on the learning rule proposed by Sutton, Barto and Anderson ([6]). Berenji proved to be able to learn effective heuristics in difficult control tasks. The GARIC architecture is conceptually stricly related to Dyna and other architectures of the main stream.
Reference: 4. <author> C. Archibald and E. Petriu. </author> <title> Computational paradigm for creating and executing sensorbased robot skills. </title> <booktitle> In Proceedings of the 24th International Symposium on Industrial Robots (ISIR `93), </booktitle> <year> 1993. </year>
Reference-contexts: In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. These individual capabilities are referred to as skills, the concept itself is also known as shared autonomy ([12], [24], [27], [59], [70]). In robot programming, SKills-Oriented Robot Programming (SKORP, see <ref> [4] </ref>) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], [55]). The approaches to skill learning are mainly aiming at identifying a control function for a given task.
Reference: 5. <author> H. Asada and B.-H. Yang. </author> <title> Skill acquisition from human experts through pattern processing of teaching data. </title> <booktitle> In Proceedings of the 1989 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1302 - 1307, </pages> <year> 1989. </year>
Reference: 6. <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <pages> pages 835-846, </pages> <year> 1983. </year>
Reference: 7. <author> A.G. Barto and R.S. Sutton. </author> <title> Landmark learning: an illustration of associative search. </title> <journal> Biological Cybernetics, </journal> <volume> 42 </volume> <pages> 1-8, </pages> <year> 1981. </year>
Reference: 8. <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: Q-learning is strictly related to TD (0) algorithms, since it is based on an implicit Markovian model of the world, too. Other strict relations exist between Q-learning and TD on one hand, and classical Dynamic Programming (DP, <ref> [8] </ref>) on the other. The fundamental difference is that Q-learning and TD do not build an explicit Markovian model of the prediction process, whereas DP does.
Reference: 9. <author> H. R. Berenji. </author> <title> A reinforcement learning based architecture for fuzzy logic control. </title> <journal> Int. Journal of Approximate Reasoning, </journal> <volume> 6:267 - 292, </volume> <year> 1992. </year>
Reference: 10. <editor> L.B. Booker. </editor> <title> Intelligent Behavior as an Adaptation to the Task Environment. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1982. </year>
Reference: 11. <author> M. Botta and A. Giordana. </author> <title> SMART+: A multi-strategy learning tool. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI '93), </booktitle> <address> Chamberry, France, </address> <year> 1993. </year>
Reference-contexts: The results are reported in table 1. 5.2 Learning a Fuzzy Controller using SMART+ The synthesis of a Fuzzy Controller from examples has been posed as a problem of learning concept descriptions from examples, which can be solved using a symbolic inductive algorithm. In the specific case, SMART+ <ref> [11] </ref> has been used. Coming back to fig. 2, it can be found that the problem is to learn the fuzzy sets of the first layer, the fuzzy sets of the third layer and the set of rules representing the mapping from the first layer to the third one.
Reference: 12. <author> B. Brunner, G. Hirzinger, K. Landzettel, and J. Heindl. </author> <title> Multi-sensory shared autonomy and telesensor-programming key issues in the space robot technology experiment ROTEX. </title> <booktitle> In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '92), </booktitle> <address> Yokohama, Japan, </address> <year> 1992. </year>
Reference: 13. <author> S. J. Buckley. </author> <title> Teaching compliant motion strategies. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5(1), </volume> <year> 1989. </year>
Reference-contexts: This approach is in fact an extension of a new programming paradigm, namely Programming by Human Demonstration ([14]) respectively Robot Programming by Demonstration (RPD, [26], [46]), to the acquisition of elementary skills ([5], <ref> [13] </ref>, [18], [31]). In the robotics community, the concept of skills can be found throughout a number of different applications. In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. <p> The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], <ref> [13] </ref>, [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 14. <author> A. I. Cypher. </author> <title> Watch what I do Programming by Demonstration. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference: 15. <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference: 16. <author> J. del R. Millan. </author> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <booktitle> In Proceedings of the third International Conference on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: Berenji proved to be able to learn effective heuristics in difficult control tasks. The GARIC architecture is conceptually stricly related to Dyna and other architectures of the main stream. Finally, in robot navigation, an important contribution comes from Millan ([17], <ref> [16] </ref>), who proposes an original architecture for combining reactive planning with high level symbolic planning. In conclusion, the Reinforcement Learning community proposes an interesting methodology which could potentially solve the problem we are facing.
Reference: 17. <author> J. del R. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> <volume> 8:363 - 395, </volume> <year> 1992. </year>
Reference: 18. <author> N. Delson and H. West. </author> <title> Robot programming by human demonstration: subtask compliance controller identification. </title> <booktitle> In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems, </booktitle> <address> Yokohama, Japan, </address> <year> 1993. </year>
Reference-contexts: This approach is in fact an extension of a new programming paradigm, namely Programming by Human Demonstration ([14]) respectively Robot Programming by Demonstration (RPD, [26], [46]), to the acquisition of elementary skills ([5], [13], <ref> [18] </ref>, [31]). In the robotics community, the concept of skills can be found throughout a number of different applications. In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. <p> The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], <ref> [18] </ref>, [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 19. <author> B. Dufay and J.-C. Latombe. </author> <title> An approach to automatic robot programming based on inductive learning. </title> <booktitle> In Proceedings of the 1st International Symposium on Robotics Research, </booktitle> <year> 1984. </year>
Reference-contexts: In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also <ref> [19] </ref>, [26], [34], [46], [55]). The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 20. <author> A. Gelperin, J.J. Hopfield, and D.W. Tank. </author> <title> The logic of Limax learning. In Model neural networks and behavior. </title> <editor> A. Selverson, </editor> <address> New York: </address> <publisher> Plenum Press, </publisher> <year> 1985. </year>
Reference-contexts: Moreover, TD methods have also been proposed as a model of classical conditioning ([7], [62], <ref> [20] </ref>, [45], [35]). Nevertheless, temporal differences have been little exploited until 1988 when the work by Sutton ([60]) established a milestone in the machine learning community. Further theoretical developments in the framework proposed by Sutton are due to Barto, Sutton and Watkins ([6]), Dayan ([15]), and Moore and Atkeson ([44]).
Reference: 21. <author> V. Gullapalli. </author> <title> Reinforcement Learning and its application to control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer and Information Science, </institution> <year> 1992. </year>
Reference: 22. <author> V. Gullapalli, R.A. Grupen, and A.G. Barto. </author> <title> Learning reactive admittance control. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Nice, France, </address> <year> 1992. </year>
Reference: 23. <author> S.E. Hampson. </author> <title> A neural model of adaptative behavior. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1983. </year>
Reference: 24. <author> T. Hasegawa, T. Suehiro, and K. Takase. </author> <title> A model-based manipulation system with skill-based execution. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(5), </volume> <year> 1992. </year>
Reference-contexts: In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. These individual capabilities are referred to as skills, the concept itself is also known as shared autonomy ([12], <ref> [24] </ref>, [27], [59], [70]). In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], [55]).
Reference: 25. <author> R. Hecht-Nielsen. </author> <title> Neurocomputing. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: The most popular methods are the ones based on Neural Networks ([50], <ref> [25] </ref>), which recently received a great attention both from machine learning community and from applicative fields such as pattern recognition. Many network topologies have been investigated and many learning algorithms, both one-step and incremental, have been proposed. <p> The TDNN is one of the several variants of multi-layer perceptron and backpropagation training algorithms ([49], see also <ref> [25] </ref>) that have been developed in order to deal with time variant signals. All of them aim at capturing the information intrinsic in a sequence of data which is lost by the classical supervised learning approach.
Reference: 26. <author> R. Heise. </author> <title> Demonstration instead of programming: Focussing attention in robot task acquisition. Research report no. </title> <type> 89/360/22, </type> <institution> Department of Computer Science, University of Calgary, </institution> <year> 1989. </year>
Reference-contexts: A particular appealing idea is to let a human operator perform the control task in order to generate examples. This approach is in fact an extension of a new programming paradigm, namely Programming by Human Demonstration ([14]) respectively Robot Programming by Demonstration (RPD, <ref> [26] </ref>, [46]), to the acquisition of elementary skills ([5], [13], [18], [31]). In the robotics community, the concept of skills can be found throughout a number of different applications. In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. <p> In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], <ref> [26] </ref>, [34], [46], [55]). The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 27. <author> G. Hirzinger. </author> <title> ROTEX the first robot in space. </title> <booktitle> In Proceedings of the International Conference on Advanced Robotics (ICAR '93), </booktitle> <address> Tokyo, Japan, </address> <year> 1993. </year>
Reference-contexts: In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. These individual capabilities are referred to as skills, the concept itself is also known as shared autonomy ([12], [24], <ref> [27] </ref>, [59], [70]). In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], [55]).
Reference: 28. <author> J.H. Holland. </author> <title> Escaping brittleness: The possibilities of general purpose learning algorithms applied to parallel rule based systems. In R.S. </title> <editor> Michalski, J.C. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> 1986. Los Altos, Ca. </address>
Reference: 29. <author> T. C. Hsia. </author> <title> Adaptive control for robot manipulators a review. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1986. </year>
Reference-contexts: In conclusion, the Reinforcement Learning community proposes an interesting methodology which could potentially solve the problem we are facing. Obviously (see, for instance, [63]), Reinforcement Learning tackles problems that are also closely related to those of adaptive control (e.g., <ref> [29] </ref>). Formalisms such as Albus' CMAC ([1]) have already been adopted by the RL community ([37]).
Reference: 30. <author> L.P. Kaelbling. </author> <title> Learning in embedded systems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1990. </year> <note> Technical report TR-90-04. </note>
Reference: 31. <author> M. Kaiser. </author> <title> A framework for the generation of robot controllers from examples. </title> <booktitle> In Proceedings of the 10th ISPE/IFAC Symposium on CAD/CAM, Robotics, and Factories of the Future, </booktitle> <address> Ottawa, Canada, </address> <year> 1994. </year>
Reference-contexts: In the following, we will briefly analyze the process of developing a control program in order to individuate the possible methods which can be exploited in order to automatize it (see also <ref> [31] </ref>, [33]). <p> This approach is in fact an extension of a new programming paradigm, namely Programming by Human Demonstration ([14]) respectively Robot Programming by Demonstration (RPD, [26], [46]), to the acquisition of elementary skills ([5], [13], [18], <ref> [31] </ref>). In the robotics community, the concept of skills can be found throughout a number of different applications. In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks.
Reference: 32. <author> M. Kaiser. </author> <title> Time-delay neural networks for robot control. </title> <note> In Submitted to: Symposium on Robot Control '94 (SYROCO '94), Capri, </note> <institution> Italy, </institution> <year> 1994. </year>
Reference-contexts: This restriction has been weakened for the experiments described here. Further details on Fuzzy Controllers and the TDNN can be found in [47], <ref> [32] </ref>. 5 Synthesis from Examples: an Experiment In order to verify the possibility of synthesizing a controller from examples of correct behaviour, an experiment has been performed using both fuzzy controllers and TDNNs.
Reference: 33. <author> M. Kaiser, A. Giordana, and M. Nuttin. </author> <title> Integrated acquisition, execution, evaluation and tuning of elementary skills for intelligent robots. </title> <booktitle> In Proceedings of the IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC '94), </booktitle> <address> Valencia, Spain, </address> <year> 1994. </year>
Reference-contexts: In the following, we will briefly analyze the process of developing a control program in order to individuate the possible methods which can be exploited in order to automatize it (see also [31], <ref> [33] </ref>).
Reference: 34. <author> M. Kaiser and J. Kreuziger. </author> <title> Integration of symbolic and connectionist processing to ease robot programming and control. </title> <booktitle> In Workshop on Combining Symbolic and Connectionist Processing at the ECAI '94, </booktitle> <year> 1994. </year>
Reference-contexts: In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], <ref> [34] </ref>, [46], [55]). The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 35. <author> A.H. Klopf. </author> <title> A neuronal model of classical conditioning. </title> <type> Technical Report 87-1139, </type> <institution> Wright Aeronautical Laboratories, OH: Wright-Patterson Air Force Base, </institution> <year> 1987. </year>
Reference-contexts: Moreover, TD methods have also been proposed as a model of classical conditioning ([7], [62], [20], [45], <ref> [35] </ref>). Nevertheless, temporal differences have been little exploited until 1988 when the work by Sutton ([60]) established a milestone in the machine learning community. Further theoretical developments in the framework proposed by Sutton are due to Barto, Sutton and Watkins ([6]), Dayan ([15]), and Moore and Atkeson ([44]).
Reference: 36. <author> R.A. Olsen L. Breiman, J.H. Friedman and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <year> 1984. </year>
Reference: 37. <author> C.-S. Lin and H. Kim. </author> <title> Use of CMAC neural networks in reinforcement self-learning control. </title> <booktitle> In Artificial Neural Networks (ANN '91), </booktitle> <year> 1991. </year>
Reference: 38. <author> L. J. Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the AAAI '91, </booktitle> <year> 1991. </year>
Reference-contexts: The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], <ref> [38] </ref>, [42], [57]).
Reference: 39. <author> L. J. Lin. </author> <title> Reinforcement learning for robots using neural networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Significant development in the framework of Q-learning are due to Kaelbling ([30]), and to Mahadevan and Connell ([43]), who applied Q-learning to a real robot in order to learn to react in a box pushing task. Other practical developments are due to Lin ([38], <ref> [39] </ref>, [40]), who showed the importance of tutoring in structuring the learning problem into sub-problems in order to speed up a task which is inherently very complex, and to Singh ([58]), who tackles the problem of complexity by combining solutions of simple subtasks, too.
Reference: 40. <author> L. J. Lin. </author> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 182-189, </pages> <year> 1993. </year>
Reference-contexts: Significant development in the framework of Q-learning are due to Kaelbling ([30]), and to Mahadevan and Connell ([43]), who applied Q-learning to a real robot in order to learn to react in a box pushing task. Other practical developments are due to Lin ([38], [39], <ref> [40] </ref>), who showed the importance of tutoring in structuring the learning problem into sub-problems in order to speed up a task which is inherently very complex, and to Singh ([58]), who tackles the problem of complexity by combining solutions of simple subtasks, too.
Reference: 41. <author> S. Liu and H. Asada. </author> <title> Teaching and learning of deburring robots using neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], <ref> [41] </ref>, [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 42. <author> S. Mahadevan. </author> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], <ref> [42] </ref>, [57]).
Reference: 43. <author> S. Mahadevan and J. Connel. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <year> 1992. </year>
Reference: 44. <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <booktitle> Machine Learning, </booktitle> <year> 1993. </year>
Reference: 45. <author> J.W. Moore, J.E. Desmond, N.E. Berthier, D.E. Blazis, R.S. Sutton, and A.G. Barto. </author> <title> Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: Response topography, neuronal firing and interstimulus intervals. </title> <booktitle> Behavioral Brain Research, </booktitle> <pages> pages 143-154, </pages> <year> 1986. </year>
Reference-contexts: Moreover, TD methods have also been proposed as a model of classical conditioning ([7], [62], [20], <ref> [45] </ref>, [35]). Nevertheless, temporal differences have been little exploited until 1988 when the work by Sutton ([60]) established a milestone in the machine learning community. Further theoretical developments in the framework proposed by Sutton are due to Barto, Sutton and Watkins ([6]), Dayan ([15]), and Moore and Atkeson ([44]).
Reference: 46. <author> S. Munch, J. Kreuziger, M. Kaiser, and R. Dillmann. </author> <title> Robot programming by demonstration using machine learning and user interaction methods for the development of easy and comfortable robot programming systems. </title> <booktitle> In Proceedings of the International Symposium on Industrial Robots (ISIR '94), Hannover, </booktitle> <address> Germany, </address> <year> 1994. </year>
Reference-contexts: A particular appealing idea is to let a human operator perform the control task in order to generate examples. This approach is in fact an extension of a new programming paradigm, namely Programming by Human Demonstration ([14]) respectively Robot Programming by Demonstration (RPD, [26], <ref> [46] </ref>), to the acquisition of elementary skills ([5], [13], [18], [31]). In the robotics community, the concept of skills can be found throughout a number of different applications. In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. <p> In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], <ref> [46] </ref>, [55]). The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 47. <author> M. Nuttin and C. Baroglio. </author> <title> Fuzzy controller synthesis in robotic assembly: Procedure and experiments. </title> <booktitle> In Proceedings of the Second European Workshop on Learning Robots, </booktitle> <address> Torino, Italy, </address> <year> 1993. </year>
Reference-contexts: This restriction has been weakened for the experiments described here. Further details on Fuzzy Controllers and the TDNN can be found in <ref> [47] </ref>, [32]. 5 Synthesis from Examples: an Experiment In order to verify the possibility of synthesizing a controller from examples of correct behaviour, an experiment has been performed using both fuzzy controllers and TDNNs.
Reference: 48. <author> J. Peng and R. J. Williams. </author> <title> Efficient learning and planning within the DYNA framework. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behaviour, </booktitle> <year> 1992. </year>
Reference: 49. <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representation by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel distributed processing explorations in the microstructure of cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: 50. <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing : Explorations in the Microstructure of Coginition, Parts I & II. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: 51. <author> A.L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <year> 1959. </year>
Reference: 52. <author> J. G. Schneider and C. M. Brown. </author> <title> Robot skill learning, basis functions, and control regimes. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], <ref> [52] </ref>) and the machine learning community ([22], [38], [42], [57]).
Reference: 53. <author> J. De Schutter and H. Van Brussel. </author> <title> Compliant robot motion, a control approach based on external control loops. </title> <journal> International Journal on Robotics Research, </journal> <volume> 7(4), </volume> <month> August </month> <year> 1988. </year>
Reference: 54. <author> J. De Schutter, S. Demey, H. Van Brussel, S. Dutre, W. Persoons, W. Witvrouw, and P. Van De Poel. </author> <title> Model based and sensor based programming of compliant motion tasks. </title> <booktitle> In Proceedings of the 24th International Symposium on Industrial Robots (ISIR '93), </booktitle> <year> 1993. </year>
Reference-contexts: The test case has been taken from a real robotic application where a KUKA-IR 361 was requested to perform the classical peg-into-hole task. The robot was already operating controlled by a PD-controller using force and torque as inputs and producing cartesian velocities as output (([53], <ref> [54] </ref>). Hence, each example constisted of six inputs (F x , F y , F z , T x , T y and T z ) and six outputs (V x , V y , V z , ! x , ! y and ! z ).
Reference: 55. <author> A. M. Segre. </author> <title> Machine Learning of Robot Assembly Plans. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], <ref> [55] </ref>). The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], [57]).
Reference: 56. <author> S. P. Singh. </author> <title> Reinforcement learning within a hierarchy of abstract models. </title> <booktitle> In Proceedings of the AAAI '92, </booktitle> <year> 1992. </year>
Reference: 57. <author> S. P. Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: The approaches to skill learning are mainly aiming at identifying a control function for a given task. They can be found both in the robotics ([5], [13], [18], [41], [52]) and the machine learning community ([22], [38], [42], <ref> [57] </ref>).
Reference: 58. <author> S. P. Singh. </author> <title> Learning to solve markovian decision processes. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer Sciene, </institution> <year> 1994. </year>
Reference: 59. <author> M. R. Stein and R. P. Paul. </author> <title> Behavior based control in time delayed teleoperation. </title> <booktitle> In Proceedings of the International Conference on Advanced Robotics (ICAR '93), </booktitle> <address> Tokyo, Japan, </address> <year> 1993. </year>
Reference-contexts: In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. These individual capabilities are referred to as skills, the concept itself is also known as shared autonomy ([12], [24], [27], <ref> [59] </ref>, [70]). In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], [55]).
Reference: 60. <author> R. S. Sutton. </author> <title> Learning to predict by methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3:9 - 44, </volume> <year> 1988. </year>
Reference-contexts: by Sutton (<ref> [60] </ref>) established a milestone in the machine learning community. Further theoretical developments in the framework proposed by Sutton are due to Barto, Sutton and Watkins ([6]), Dayan ([15]), and Moore and Atkeson ([44]). The basic idea of TD can be well explained by the following example taken from [60]: suppose a meteorologist is asked every day along the week to predict Saturday's weather. Using a supervised learning approach, he will collect data pairs associating the measurements taken on each day of the week to Saturday's observation; then he will induce a function from the collected data.
Reference: 61. <author> R. S. Sutton. </author> <title> Integrated modeling and control based on reinforcement learning and dynamic programming. </title> <booktitle> In Advances in Neural Information Processing Systems 3 (NIPS-3), </booktitle> <address> Denver, Colorado, </address> <year> 1990. </year>
Reference: 62. <author> R.S. Sutton and A.G. Barto. </author> <title> A temporal-difference method of classical conditioning. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 355-378, </pages> <address> Seattle, WA, 1987. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Moreover, TD methods have also been proposed as a model of classical conditioning ([7], <ref> [62] </ref>, [20], [45], [35]). Nevertheless, temporal differences have been little exploited until 1988 when the work by Sutton ([60]) established a milestone in the machine learning community.
Reference: 63. <author> R.S. Sutton, A.G. Barto, and R.J. Williams. </author> <title> Reinforcement learning is direct adaptive control. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 19-22, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Finally, in robot navigation, an important contribution comes from Millan ([17], [16]), who proposes an original architecture for combining reactive planning with high level symbolic planning. In conclusion, the Reinforcement Learning community proposes an interesting methodology which could potentially solve the problem we are facing. Obviously (see, for instance, <ref> [63] </ref>), Reinforcement Learning tackles problems that are also closely related to those of adaptive control (e.g., [29]). Formalisms such as Albus' CMAC ([1]) have already been adopted by the RL community ([37]).
Reference: 64. <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on acoustics, speech and signal processing, </journal> <month> March </month> <year> 1989. </year>
Reference-contexts: This is the reason why fuzzy controllers are considered "symbolic" objects. In this paper, the results obtained by applying a fuzzy controller and a variant of classical multilayer perceptron called Time Delay Neural Network (TDNN, see [65], <ref> [64] </ref>). on a control task taken from a robotic application will be given. The TDNN is one of the several variants of multi-layer perceptron and backpropagation training algorithms ([49], see also [25]) that have been developed in order to deal with time variant signals.
Reference: 65. <author> A. Waibel, H. Sawai, and K. Shikano. </author> <title> Modularity and scaling in large phonemic neural networks. </title> <type> Technical report, ATR Report TR-I-0034, </type> <month> August </month> <year> 1988. </year>
Reference-contexts: This is the reason why fuzzy controllers are considered "symbolic" objects. In this paper, the results obtained by applying a fuzzy controller and a variant of classical multilayer perceptron called Time Delay Neural Network (TDNN, see <ref> [65] </ref>, [64]). on a control task taken from a robotic application will be given. The TDNN is one of the several variants of multi-layer perceptron and backpropagation training algorithms ([49], see also [25]) that have been developed in order to deal with time variant signals.
Reference: 66. <author> C. J. C. H. Watkins. </author> <title> Learning with delayed rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference: 67. <author> C. J. C. H. Watkins. </author> <title> A technical note on Q-Learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: In other words, the robot must discover by itself better behaviours by trying. The feedback from a teacher can be just an overall evaluation of the achieved performance. Learning in a similar framework is usually approached using reinforcement learning methods ([6], <ref> [67] </ref>, [68]) and we will follow this line as it will be discussed in Section 5. Reinforcement learning methods can be used as well for on-line tuning in the third phase when the robot is running in a production line.
Reference: 68. <author> R. J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 229 -256, </pages> <year> 1992. </year>
Reference-contexts: In other words, the robot must discover by itself better behaviours by trying. The feedback from a teacher can be just an overall evaluation of the achieved performance. Learning in a similar framework is usually approached using reinforcement learning methods ([6], [67], <ref> [68] </ref>) and we will follow this line as it will be discussed in Section 5. Reinforcement learning methods can be used as well for on-line tuning in the third phase when the robot is running in a production line.
Reference: 69. <author> I.H. Witten. </author> <title> An adaptive optimal controller for discrete-time markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference: 70. <author> J. Yang, Y. Xu, and C.S. Chen. </author> <title> Hidden markov model approach to skill learning and its application in tele-robotics. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: In telerobotics, robots provide the capability to autonomously execute certain operations and relieve the operator from difficult control tasks. These individual capabilities are referred to as skills, the concept itself is also known as shared autonomy ([12], [24], [27], [59], <ref> [70] </ref>). In robot programming, SKills-Oriented Robot Programming (SKORP, see [4]) relies on the existence of a set of skills as the building blocks of a robot program (see also [19], [26], [34], [46], [55]).
Reference: 71. <author> L. A. Zadeh. </author> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8:338 - 353, </volume> <year> 1965. </year> <title> This article was processed using the T E X macro package with IRS94 style </title>
References-found: 71

