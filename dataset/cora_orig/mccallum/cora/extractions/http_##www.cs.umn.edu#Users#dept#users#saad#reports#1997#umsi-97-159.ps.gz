URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/umsi-97-159.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/
Root-URL: http://www.cs.umn.edu
Title: Distributed Schur Complement Techniques for General Sparse Linear Systems  
Author: Yousef Saad Masha Sosonkina 
Date: October 16, 1997  
Abstract: This paper presents a few preconditioning techniques for solving general sparse linear systems on distributed memory environments. These techniques utilize the Schur complement system for deriving the preconditioning matrix in a number of ways. Two of these pre-conditioners consist of an approximate solution process for the global system, which exploit approximate LU factorizations for diagonal blocks of the Schur complement. Another precon-ditioner uses a sparse approximate-inverse technique to obtain certain local approximations of the Schur complement. Comparisons are reported for systems of varying difficulty.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Balay, L. Curfman McInnes, W. D. Gropp, and B. F. Smith. </author> <title> PETSc 2.0 users manual. </title> <type> Technical Report ANL-95/11, </type> <institution> Argonne National Lab, Argonne, IL, </institution> <year> 1995. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. <p> Define V m := [v 1 ; ::::; v m ]. 18. Form the approximate solution for interface variables: 19. Compute y i := y i + V m z m , where 20. z m = argmin z kfie 1 H m zk 2 and e 1 = <ref> [1; 0; : : : ; 0] </ref> T . 21. Find other local unknowns: 22. Exchange interface vector components y i 23. t := X i y i;ext t 25. u i A few explanations are in order.
Reference: [2] <author> T. Barth, T. F. Chan, and W.-P. Tang. </author> <title> A parallel algebraic non-overlapping domain decomposition method for flow problems. </title> <type> Technical report, </type> <institution> NASA Ames Research Center, </institution> <year> 1997. </year> <note> In preparation. </note>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. <p> The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. Recently, a number of methods have been developed which exploit the Schur complement system related to interface variables, see for example, <ref> [12, 2, 8] </ref>. In particular, several distributed preconditioners included in the ParPre package [8] employ variants of Schur complement techniques. One difference between our work and [2] is that our approach does not construct a matrix to approximate the global Schur complement. Instead, the preconditioners constructed are entirely local. <p> Recently, a number of methods have been developed which exploit the Schur complement system related to interface variables, see for example, [12, 2, 8]. In particular, several distributed preconditioners included in the ParPre package [8] employ variants of Schur complement techniques. One difference between our work and <ref> [2] </ref> is that our approach does not construct a matrix to approximate the global Schur complement. Instead, the preconditioners constructed are entirely local. However, they also have a global nature in that they do attempt to solve the global Schur complement system approximately by an iterative technique.
Reference: [3] <author> M. W. Benson and P. O. Frederickson. </author> <title> Iterative solution of large sparse linear systems arising in certain multidimensional approximation problems. </title> <journal> Utilitas Math., </journal> <volume> 22 </volume> <pages> 127-140, </pages> <year> 1982. </year>
Reference-contexts: A particular factorization stems from approximating the Schur complement matrix S using one of several approximate-inverse techniques described next. Given an arbitrary matrix A, approximate-inverse preconditioners consist of finding an approximation Q to its inverse, by solving approximately the optimization problem <ref> [3] </ref>: min kI AQk 2 in which S is a certain set of nfin sparse matrices.
Reference: [4] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse preconditioners for general sparse matrices. </title> <journal> SIAM Journal on Scientific Computing, </journal> -, <note> 1997. To appear. </note>
Reference-contexts: Note that each of the n columns can be computed independently. Different strategies for selecting a nonzero structure of the approximate-inverse are proposed in <ref> [4] </ref> and [9]. In [9] the initial sparsity pattern is taken to be diagonal with further fill-in allowed depending on the improvement in the minimization. The work [4] suggests controlling the sparsity of the approximate inverse by dropping certain nonzero entries in the solution or search directions of a suitable iterative <p> Different strategies for selecting a nonzero structure of the approximate-inverse are proposed in <ref> [4] </ref> and [9]. In [9] the initial sparsity pattern is taken to be diagonal with further fill-in allowed depending on the improvement in the minimization. The work [4] suggests controlling the sparsity of the approximate inverse by dropping certain nonzero entries in the solution or search directions of a suitable iterative method (e.g., GMRES). <p> This iterative method solves the system Am j = e j such that min m j ke j Am j k 2 2 ; for j = 1; 2; :::; n: In this paper, the approximate-inverse technique proposed in <ref> [4] </ref> and [5] is used. Consider the local matrix A i blocked as A i = B i F i and its block LU factorization similar to the one given by (16). <p> For convenience, the following abbreviations will denote preconditioners and solution techniques used in the numerical experiments: SAPINV Distributed approximate block LU factorization: M S i and B 1 i F i are approximated using the matrix Y i , constructed using the approximate-inverse technique described in <ref> [4] </ref>; SAPINVS Distributed approximate block LU factorization: M S i is approximated using the approximate-inverse technique in [4], but B 1 i F i is applied using one matrix-vector multiplication followed by a solve with B i ; SLU Distributed global system preconditioning defined via an approximate solve with M S <p> the numerical experiments: SAPINV Distributed approximate block LU factorization: M S i and B 1 i F i are approximated using the matrix Y i , constructed using the approximate-inverse technique described in <ref> [4] </ref>; SAPINVS Distributed approximate block LU factorization: M S i is approximated using the approximate-inverse technique in [4], but B 1 i F i is applied using one matrix-vector multiplication followed by a solve with B i ; SLU Distributed global system preconditioning defined via an approximate solve with M S , in which S i L S i U S i ; BJ Approximate Additive Schwarz, where
Reference: [5] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse techniques for block-partitioned matrices. </title> <journal> SIAM Journal on Scientific Computing, </journal> <note> 1997. To appear. </note>
Reference-contexts: This iterative method solves the system Am j = e j such that min m j ke j Am j k 2 2 ; for j = 1; 2; :::; n: In this paper, the approximate-inverse technique proposed in [4] and <ref> [5] </ref> is used. Consider the local matrix A i blocked as A i = B i F i and its block LU factorization similar to the one given by (16). <p> The sparse approximate-inverse technique can be applied to approximate B 1 i F i with a certain matrix Y i (as it is done in <ref> [5] </ref>). The resulting matrix Y i is sparse and therefore M S i = C i E i Y i (21) is a sparse approximation to S i . A further approximation can be constructed using an ILU factorization for the matrix M S i . <p> Note that the effectiveness of BJ degrades with increasing number of processors (cf. Subsection 4.1). Comparison of SAPINV and SAPINVS (for RAEFSKY1 and SHERMAN3) confirms the conclusions of <ref> [5] </ref> that using Y i to approximate B 1 i F i (Line 5 in Algorithm 3.2) is more efficient than applying B 1 i F i directly, which is also computationally expensive.
Reference: [6] <author> J. D. F. Cosgrove, J. C. Daz, and A. Griewank. </author> <title> Approximate inverse preconditioning for sparse linear systems. </title> <journal> Intl. J. Comp. Math., </journal> <volume> 44 </volume> <pages> 91-110, </pages> <year> 1992. </year>
Reference: [7] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: i in construction of M S i (equation (21)). 4 Numerical experiments In the experiments, we compared the performance of the described preconditioners and the distributed Additive Schwarz preconditioning (see, e.g., [19]) on 2-D elliptic PDE problems and on several problems with the matrices from the Harwell-Boeing and Davis collections <ref> [7] </ref>. A flexible variant of restarted GMRES (FGMRES) [16] has been used to solve the original system since this accelerator permits a change in the preconditioning operation at each step. This is useful when, for example, an iterative process is used to precondition the input system.
Reference: [8] <author> V. Eijkhout and T. Chan. </author> <title> ParPre a parallel preconditioners package, reference manual for version 2.0.17. </title> <type> Technical Report CAM Report 97-24, </type> <institution> UCLA, </institution> <year> 1997. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. <p> The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. Recently, a number of methods have been developed which exploit the Schur complement system related to interface variables, see for example, <ref> [12, 2, 8] </ref>. In particular, several distributed preconditioners included in the ParPre package [8] employ variants of Schur complement techniques. One difference between our work and [2] is that our approach does not construct a matrix to approximate the global Schur complement. Instead, the preconditioners constructed are entirely local. <p> Recently, a number of methods have been developed which exploit the Schur complement system related to interface variables, see for example, [12, 2, 8]. In particular, several distributed preconditioners included in the ParPre package <ref> [8] </ref> employ variants of Schur complement techniques. One difference between our work and [2] is that our approach does not construct a matrix to approximate the global Schur complement. Instead, the preconditioners constructed are entirely local.
Reference: [9] <author> T. Huckle and M. Grote. </author> <title> A new approach to parallel preconditioning with sparse approximate inverses. </title> <type> Technical Report SCCM-94-03, </type> <institution> Stanford University, Scientific Computing and Computational Mathematics Program, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: Note that each of the n columns can be computed independently. Different strategies for selecting a nonzero structure of the approximate-inverse are proposed in [4] and <ref> [9] </ref>. In [9] the initial sparsity pattern is taken to be diagonal with further fill-in allowed depending on the improvement in the minimization. <p> Note that each of the n columns can be computed independently. Different strategies for selecting a nonzero structure of the approximate-inverse are proposed in [4] and <ref> [9] </ref>. In [9] the initial sparsity pattern is taken to be diagonal with further fill-in allowed depending on the improvement in the minimization. The work [4] suggests controlling the sparsity of the approximate inverse by dropping certain nonzero entries in the solution or search directions of a suitable iterative method (e.g., GMRES).
Reference: [10] <author> Scott A. Hutchinson, John N. Shadid, and R. S. Tuminaro. </author> <title> Aztec user's guide. version 1.0. </title> <type> Technical Report SAND95-1559, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1995. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure.
Reference: [11] <author> D. E. Keyes and W. D. Gropp. </author> <title> A comparison of domain decomposition techniques for elliptic partial differ ential equation and their parallel implementation. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 </volume> <pages> 856-869, </pages> <year> 1987. </year>
Reference-contexts: As is known, with a consistent choice of the initial guess, a block-Jacobi (or SOR) iteration with the reduced system is equivalent to a block-Jacobi iteration (resp. SOR) on the global system (see, e.g., <ref> [11] </ref>, [19]).
Reference: [12] <author> S. Kuznetsov, G. C. Lo, and Y. Saad. </author> <title> Parallel solution of general sparse linear systems. </title> <type> Technical Report UMSI 97/98, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. Recently, a number of methods have been developed which exploit the Schur complement system related to interface variables, see for example, <ref> [12, 2, 8] </ref>. In particular, several distributed preconditioners included in the ParPre package [8] employ variants of Schur complement techniques. One difference between our work and [2] is that our approach does not construct a matrix to approximate the global Schur complement. Instead, the preconditioners constructed are entirely local. <p> This viewpoint was taken in <ref> [13, 12] </ref>. The sequence y (k) can be accelerated with a Krylov subspace algorithm, such as GMRES [21].
Reference: [13] <author> G.-C. Lo and Y. Saad. </author> <title> Iterative solution of general sparse linear systems on clusters of workstations. </title> <type> Technical Report UMSI 96/117 & UM-IBM 96/24, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year> <month> 19 </month>
Reference-contexts: This viewpoint was taken in <ref> [13, 12] </ref>. The sequence y (k) can be accelerated with a Krylov subspace algorithm, such as GMRES [21]. <p> The column Pattern specifies whether a given problem has a structurally symmetric matrix. In all three test problems, the matrix rows followed by the columns were scaled by 2-norm. Also, in the partitioning of a problem one level of overlapping with data exchanging was used following <ref> [13] </ref>. Tables 2-4 show iteration numbers required by FGMRES (20) with SAPINV, SAPINVS, SLU, 14 3 different preconditioners using flexible GMRES (10).
Reference: [14] <author> S. Ma and Y. Saad. </author> <title> Distributed ILU(0) and SOR preconditioners for unstructured sparse linear systems. </title> <type> Technical Report 94-027, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. <p> A preprocessing phase is thus required to determine these dependencies and any other information needed during the iteration phase. The approach described here follows that used in the PSPARSLIB package, see <ref> [20, 22, 14] </ref> for additional details. borrows from the domain decomposition literature so the term "subdomain" is often used instead of the more proper term "subgraph". Each point (node) belonging to a subdomain is actually a pair representing an equation and an associated unknown. <p> In particular, each processor needs to know (1) the processors with which it must communicate, (2) the list of interface points, and (3) a break-up of this list into sublists that must be communicated among neighboring processors. For further details see <ref> [20, 22, 14] </ref>. 3 Derivation of Schur complement techniques Schur complement techniques refer to methods which iterate on the interface unknowns only, implicitly using internal unknowns as intermediate variables. A few strategies for deriving Schur complement techniques will now be described.
Reference: [15] <author> Y. Saad. </author> <title> Krylov subspace methods in distributed computing environments. </title> <type> Technical Report 92-126, </type> <institution> Army High Performance Computing Research Center, Minneapolis, MN, </institution> <year> 1992. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure.
Reference: [16] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: A flexible variant of restarted GMRES (FGMRES) <ref> [16] </ref> has been used to solve the original system since this accelerator permits a change in the preconditioning operation at each step. This is useful when, for example, an iterative process is used to precondition the input system. Thus, it is possible to use ILUT-preconditioned GMRES with lfil fill-in elements. <p> Distributed preconditioners constructed and applied in this manner allow much flexibility in specifying approximations to the local Schur complements and local system solves and in defining the global induced Block-LU preconditioner to the original matrix. With an increasing number of processors, a Krylov subspace method, such as FGMRES <ref> [16] </ref>, preconditioned by the proposed techniques exhibits a very moderate growth in the execution time for scaled problem sizes. Experiments show that the proposed distributed preconditioners based on Schur complement techniques are superior to the commonly used Additive Schwarz preconditioning.
Reference: [17] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 387-402, </pages> <year> 1994. </year>
Reference-contexts: This is useful when, for example, an iterative process is used to precondition the input system. Thus, it is possible to use ILUT-preconditioned GMRES with lfil fill-in elements. Recall that ILUT <ref> [17] </ref> is a form of incomplete LU factorization with a dual threshold strategy for dropping fill-in elements.
Reference: [18] <author> Y. Saad. </author> <title> Krylov subspace methods in distributed computing environments. </title> <editor> In M. Hafez, editor, </editor> <booktitle> State of the Art in CFD, </booktitle> <pages> pages 741-755, </pages> <year> 1995. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure.
Reference: [19] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: As is known, with a consistent choice of the initial guess, a block-Jacobi (or SOR) iteration with the reduced system is equivalent to a block-Jacobi iteration (resp. SOR) on the global system (see, e.g., [11], <ref> [19] </ref>). <p> Numerical experiments will confirm this. 3.3 Induced Preconditioners A key idea in domain decomposition methods is to develop preconditioners for the global system (1) by exploiting methods that approximately solve the reduced system (7). These techniques, termed "induced preconditioners" (see, e.g., <ref> [19] </ref>), can be best explained by considering a reordered version of the global system (1) in which all the internal vector components u = (u 1 ; : : : u p ) T are labeled first followed by all the interface vector components y. <p> Consider the factorization E C = B 0 0 I ; (16) where S is the global Schur complement S = C EB 1 F: This Schur complement matrix is identical to the coefficient matrix of system (7) (see, e.g., <ref> [19] </ref>). The global system (15) can be preconditioned by an approximate LU factorization con structed such that L = B 0 0 I (17) with M S being some approximation to S. Two techniques of this type are discussed in the rest of this section. <p> Obtain an approximation to S i using approximate-inverse techniques (see the next sub section) and then factor it using an ILU technique. The methods in options (1) and (2) are based on the following observation (see <ref> [19] </ref>). <p> An alternative is to exploit the matrix Y i that approximates B 1 i F i in construction of M S i (equation (21)). 4 Numerical experiments In the experiments, we compared the performance of the described preconditioners and the distributed Additive Schwarz preconditioning (see, e.g., <ref> [19] </ref>) on 2-D elliptic PDE problems and on several problems with the matrices from the Harwell-Boeing and Davis collections [7]. A flexible variant of restarted GMRES (FGMRES) [16] has been used to solve the original system since this accelerator permits a change in the preconditioning operation at each step.
Reference: [20] <author> Y. Saad and A. Malevsky. PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers. </title> <editor> In V. E. Malyshkin et al., editor, </editor> <booktitle> Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, Russia, </address> <month> Sept. </month> <year> 1995, 1995. </year>
Reference-contexts: This paper addresses mainly the issue of developing preconditioners for distributed sparse linear systems by regarding these systems as distributed objects. This viewpoint is common in the framework of parallel iterative solution techniques <ref> [15, 14, 18, 20, 10, 1, 2, 8] </ref> and borrows ideas from domain decomposition methods that are prevalent in the PDE literature. The key issue is to develop preconditioners for the global linear system by exploiting its distributed data structure. <p> A preprocessing phase is thus required to determine these dependencies and any other information needed during the iteration phase. The approach described here follows that used in the PSPARSLIB package, see <ref> [20, 22, 14] </ref> for additional details. borrows from the domain decomposition literature so the term "subdomain" is often used instead of the more proper term "subgraph". Each point (node) belonging to a subdomain is actually a pair representing an equation and an associated unknown. <p> In particular, each processor needs to know (1) the processors with which it must communicate, (2) the list of interface points, and (3) a break-up of this list into sublists that must be communicated among neighboring processors. For further details see <ref> [20, 22, 14] </ref>. 3 Derivation of Schur complement techniques Schur complement techniques refer to methods which iterate on the interface unknowns only, implicitly using internal unknowns as intermediate variables. A few strategies for deriving Schur complement techniques will now be described.
Reference: [21] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: This viewpoint was taken in [13, 12]. The sequence y (k) can be accelerated with a Krylov subspace algorithm, such as GMRES <ref> [21] </ref>.
Reference: [22] <author> Y. Saad and K. Wu. </author> <title> Design of an iterative solution module for a parallel sparse matrix library (P SPARSLIB). </title> <editor> In W. Schonauer, editor, </editor> <booktitle> Proceedings of IMACS conference, </booktitle> <address> Georgia, </address> <year> 1994, 1995. </year>
Reference-contexts: A preprocessing phase is thus required to determine these dependencies and any other information needed during the iteration phase. The approach described here follows that used in the PSPARSLIB package, see <ref> [20, 22, 14] </ref> for additional details. borrows from the domain decomposition literature so the term "subdomain" is often used instead of the more proper term "subgraph". Each point (node) belonging to a subdomain is actually a pair representing an equation and an associated unknown. <p> In particular, each processor needs to know (1) the processors with which it must communicate, (2) the list of interface points, and (3) a break-up of this list into sublists that must be communicated among neighboring processors. For further details see <ref> [20, 22, 14] </ref>. 3 Derivation of Schur complement techniques Schur complement techniques refer to methods which iterate on the interface unknowns only, implicitly using internal unknowns as intermediate variables. A few strategies for deriving Schur complement techniques will now be described.
Reference: [23] <author> B. Smith, P. Bjtrstad, and W. Gropp. </author> <title> Domain decomposition: Parallel multilevel methods for elliptic partial differential equations. </title> <publisher> Cambridge University Press, </publisher> <address> New-York, NY, </address> <year> 1996. </year> <month> 20 </month>
Reference-contexts: We can compute the dense matrix S i explicitly or solve system (5) by using a computation of the matrix-vector product S i y, which can be carried out with three sparse matrix-vector multiplies and one accurate linear system solve. As is known (see <ref> [23] </ref>), because of the large computational expense of these accurate solves, the resulting decrease in iteration counts is not sufficient to make the Schur complement iteration competitive.
References-found: 23

