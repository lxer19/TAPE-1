URL: http://www.cs.iastate.edu/tech-reports/TR95-12.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: 
Title: Analysis of Decision Boundaries Generated by Constructive Neural Network Learning Algorithms  
Author: C-H Chen, R. G. Parekh, J. Yang, K. Balakrishnan, V. Honavar 
Address: 226 Atanasoff Hall,  Ames, IA 50011. U.S.A.  
Affiliation: Department of Computer Science  Iowa State University,  
Abstract: Constructive learning algorithms offer an approach to incremental construction of near-minimal artificial neural networks for pattern classification. Examples of such algorithms include Tower, Pyramid, Upstart, and Tiling algorithms which construct multilayer networks of threshold logic units (or, multilayer perceptrons). These algorithms differ in terms of the topology of the networks that they construct which in turn biases the search for a decision boundary that correctly classifies the training set. This paper presents an analysis of such algorithms from a geometrical perspective. This analysis helps in a better characterization of the search bias employed by the different algorithms in relation to the geometrical distribution of examples in the training set. Simple experiments with non linearly separable training sets support the results of mathematical analysis of such algorithms. This suggests the possibility of designing more efficient constructive algorithms that dynamically choose among different biases to build near-minimal networks for pattern classification.
Abstract-found: 1
Intro-found: 1
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimizing neural computation. </title> <type> Ph.D. Thesis. </type> <institution> Center for Cognitive Science. University of Edinburgh, UK. </institution>
Reference-contexts: However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm (Gallant, 93), thermal perceptron <ref> (Frean, 1990) </ref>, loss minimization algorithm (Hrycej, 1992) are designed to find a reasonably good weight vector that correctly classifies a large fraction of the fl This research was partially supported by the National Science Foundation grant IRI-909580 to Vasant Honavar. training set S when S is not linearly separable and converge <p> This paper focuses on constructive algorithms that build multi-layer perceptrons. These include tower, pyramid (Gallant, 93), upstart <ref> (Frean, 1990) </ref>, and tiling (Mezard & Nadal, 89) algorithms. The rest of this paper is organized as follows: Section 2 provides an analysis of different constructive learning algorithms from a geometrical perspective. <p> Since our primary concern in this paper is with understanding the detailed working of the various algorithms that were considered, the experiments were limited to the exclusive OR (XOR) problem (a non 1 The upstart algorithm <ref> (Frean, 1990) </ref> works with binary (as opposed to bipolar input and output values).
Reference: <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm <ref> (Gallant, 93) </ref>, thermal perceptron (Frean, 1990), loss minimization algorithm (Hrycej, 1992) are designed to find a reasonably good weight vector that correctly classifies a large fraction of the fl This research was partially supported by the National Science Foundation grant IRI-909580 to Vasant Honavar. training set S when S is not <p> This paper focuses on constructive algorithms that build multi-layer perceptrons. These include tower, pyramid <ref> (Gallant, 93) </ref>, upstart (Frean, 1990), and tiling (Mezard & Nadal, 89) algorithms. The rest of this paper is organized as follows: Section 2 provides an analysis of different constructive learning algorithms from a geometrical perspective. <p> To facilitate direct comparison, we assume that the weights and thresholds are transformed so as to implement an equivalent bipolar mapping <ref> (Gallant, 93) </ref> after training the network with binary inputs and outputs. linearly separable data set with 4 data points) so that the resulting decision boundaries could be easily represented visually. Each TLU was trained using the fixed correction perceptron algorithm with ratchet modification (Gallant, 93). <p> as to implement an equivalent bipolar mapping <ref> (Gallant, 93) </ref> after training the network with binary inputs and outputs. linearly separable data set with 4 data points) so that the resulting decision boundaries could be easily represented visually. Each TLU was trained using the fixed correction perceptron algorithm with ratchet modification (Gallant, 93). Fig. 5 shows the evolution of the decision boundaries for a tower network when each new TLU was trained for 100 samples randomly chosen (with replacement) from the data set.
Reference: <author> Honavar, V. </author> <year> (1990). </year> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <type> Ph.D. Thesis. </type> <institution> University of Wisconsin, Madison, U.S.A. </institution>
Reference: <author> Honavar, V., and Uhr, L. </author> <year> (1993). </year> <title> Generative Learning Structures and Processes for Connectionist Networks. </title> <booktitle> Information Sciences 70, </booktitle> <pages> 75-108. </pages>
Reference-contexts: When S is not linearly separable, a multi-layer network of TLUs is needed to learn a complex decision boundary that correctly classifies all the training examples. Constructive learning algorithms <ref> (Honavar & Uhr, 1993) </ref> are designed to incrementally construct near-minimal networks of neurons (whose complexity as measured by the number of nodes, links etc.) is commensurate with the complexity of the classification problem implicitly specified by the training set. This paper focuses on constructive algorithms that build multi-layer perceptrons.
Reference: <author> Hrycej, T. </author> <year> (1992). </year> <title> Modular Neural Networks. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: However when S is not linearly separable, such algorithms behave poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule e.g., pocket algorithm (Gallant, 93), thermal perceptron (Frean, 1990), loss minimization algorithm <ref> (Hrycej, 1992) </ref> are designed to find a reasonably good weight vector that correctly classifies a large fraction of the fl This research was partially supported by the National Science Foundation grant IRI-909580 to Vasant Honavar. training set S when S is not linearly separable and converge to zero classification error when
Reference: <author> Mezard, M., and Nadal, J. </author> <year> (1989). </year> <title> Learning in feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. and Gen. </journal> <volume> 22. </volume> <pages> 2191-2203. </pages>
Reference-contexts: This paper focuses on constructive algorithms that build multi-layer perceptrons. These include tower, pyramid (Gallant, 93), upstart (Frean, 1990), and tiling <ref> (Mezard & Nadal, 89) </ref> algorithms. The rest of this paper is organized as follows: Section 2 provides an analysis of different constructive learning algorithms from a geometrical perspective. Section 3 presents experimental results on a simple 2-dimensional data set that confirm the results of analysis presented in section 2.
Reference: <author> Nilsson, N. </author> <year> (1965). </year> <title> Learning Machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: A number of iterative algorithms are available for finding such a ^ W j (if one exists | i.e., when S is linearly separable) <ref> (Nilsson, 1965) </ref>. Most of them use some variant of the perceptron weight update rule: W j W j + (d jp y jp )X p (where is the learning rate).
References-found: 7

