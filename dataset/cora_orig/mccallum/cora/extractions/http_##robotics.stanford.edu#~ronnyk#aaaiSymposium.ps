URL: http://robotics.stanford.edu/~ronnyk/aaaiSymposium.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: ronnyk@CS.Stanford.EDU  
Title: Feature Subset Selection as Search with Probabilistic Estimates  
Author: Ron Kohavi 
Web: http://robotics.stanford.edu/~ronnyk  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Date: 1994  
Note: Appears in the AAAI Fall Symposium on Relevance,  
Abstract: Irrelevant features and weakly relevant features may reduce the comprehensibility and accuracy of concepts induced by supervised learning algorithms. We formulate the search for a feature subset as an abstract search problem with probabilistic estimates. Searching a space using an evaluation function that is a random variable requires trading off accuracy of estimates for increased state exploration. We show how recent feature subset selection algorithms in the machine learning literature fit into this search problem as simple hill climbing approaches, and conduct a small experiment using a best-first search technique. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6(1) </booktitle> <pages> 37-66. </pages>
Reference-contexts: For these functions branch and bound techniques can be used to prune the search space (Narendra & Fukunaga 1977). Common machine learning algorithms, including top-down of decision tree algorithm, such as ID3 and C4.5 (Quinlan 1993), and instance based algorithms, such as IB3 <ref> (Aha, Kibler, & Albert 1991) </ref>, are known to suffer from irrelevant features. For example, running C4.5 without special flags on the Monk 1 problem (Thrun etal. 1991), which has three irrelevant features, generates a tree with 15 interior nodes, five of which test irrelevant features.
Reference: <author> Aha, D. W. </author> <year> 1992. </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies 36(1) </journal> <pages> 267-287. </pages>
Reference: <author> Almuallim, H., and Dietterich, T. G. </author> <year> 1991. </year> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the evaluation function. In order to evaluate the prediction accuracy of the induced structure, k-fold cross validation <ref> (Breiman et al. 1984) </ref> can be used. The training data is split into k approximately equally sized partitions. The induction algorithm is then run k times, each time using k 1 partitions as the training set and the other partition as the test set.
Reference: <author> Devijver, P. A., and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recognition: A Statistical Approach. </title> <booktitle> Prentice-Hall International. </booktitle>
Reference-contexts: A feature is irrelevant if it is not relevant. 3 The Wrapper Model A good subset of features for an inductive learning algorithm should include a subset of the relevant features that optimizes some performance function, usually prediction accuracy. The pattern recognition literature <ref> (Devijver & Kit-tler 1982) </ref>, statistics literature (Miller 1990; Neter, Wasserman, & Kutner 1990), and recent machine learning papers (Almuallim & Dietterich 1991; Kira & Rendell 1992; Kononenko 1994) consist of many such measures that are all based on the data alone.
Reference: <author> Greiner, R. </author> <year> 1992. </year> <title> Probabilistic hill climbing : Theory and applications. In Glasgow, </title> <editor> J., and Hadley, R., eds., </editor> <booktitle> Proceedings of the Ninth Canadian Conference on Artificial Intelligence, </booktitle> <pages> 60-67. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hoeffding, W. </author> <year> 1963. </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association 58 </journal> <pages> 13-30. </pages>
Reference-contexts: The race ends when there is a winner, or when all n steps in the leave-one-out cross validation have been executed. The confidence interval is defined according to Hoeffding's formula <ref> (Hoeffding 1963) </ref>: fi fi f fl (s) b f (s) fi where b f (s) is the average of m evaluations and B bounds the possible spread of point values.
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference-contexts: The search stops when five node expansions do not yield improved performance of more than 0.1%. The datasets shown in Table 1 are the same ones used in Holte's paper <ref> (Holte 1993) </ref> from the UC Irvine repository (Murphy & Aha 1994).
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 121-129. </pages> <note> Morgan Kaufmann. Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: On some artificial datasets, we have seen more dramatic examples of the improvement of a good search strategy. For example, on the monk1 dataset (Thrun etal. 1991), C4.5's accuracy is 75.7%, C4.5-HCS's accuracy is 75.0%, and C4.5-BFS's accuracy is 88.9%. On the Corral dataset <ref> (John, Kohavi, & Pfleger 1994) </ref>, C4.5's accuracy is 81.2%, C4.5-HCS's accuracy is 75%, and C4.5-BFS's accuracy is 100.0%. 7 Summary and Future Work We have abstracted the feature subset selection using cross validation into a search problem with a probabilistic evaluation function.
Reference: <author> Kira, K., and Rendell, L. A. </author> <year> 1992. </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 129-134. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Kononenko, I. </author> <year> 1994. </year> <title> Estimating attributes: Analysis and extensions of Relief. </title> <booktitle> In Proceedings of the European Conference on Machine Learning. </booktitle>
Reference: <author> Maron, O., and Moore, A. W. </author> <year> 1994. </year> <title> Hoeffding races: Accelerating model selection search for classification and function approximation. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Miller, A. J. </author> <year> 1990. </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Moore, A. W., and Lee, M. S. </author> <year> 1994. </year> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In Co-hen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The search stops when five node expansions do not yield improved performance of more than 0.1%. The datasets shown in Table 1 are the same ones used in Holte's paper (Holte 1993) from the UC Irvine repository <ref> (Murphy & Aha 1994) </ref>.
Reference: <author> Narendra, M. P., and Fukunaga, K. </author> <year> 1977. </year> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers C-26(9):917-922. </journal>
Reference-contexts: Practical algorithms, however, are not ideal, and the monotonicity assumption rarely holds. Notable exceptions that do satisfy monotonicity assumption are discriminant functions and distance measures such as the Bhattacharyya distance and divergence. For these functions branch and bound techniques can be used to prune the search space <ref> (Narendra & Fukunaga 1977) </ref>. Common machine learning algorithms, including top-down of decision tree algorithm, such as ID3 and C4.5 (Quinlan 1993), and instance based algorithms, such as IB3 (Aha, Kibler, & Albert 1991), are known to suffer from irrelevant features.
Reference: <author> Neter, J.; Wasserman, W.; and Kutner, M. H. </author> <year> 1990. </year> <title> Applied Linear Statistical Models. </title> <type> Irwin: Homewood, </type> <institution> IL, </institution> <note> 3rd edition. </note>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For these functions branch and bound techniques can be used to prune the search space (Narendra & Fukunaga 1977). Common machine learning algorithms, including top-down of decision tree algorithm, such as ID3 and C4.5 <ref> (Quinlan 1993) </ref>, and instance based algorithms, such as IB3 (Aha, Kibler, & Albert 1991), are known to suffer from irrelevant features.
Reference: <author> Siedlecki, W., and Sklansky, J. </author> <year> 1988. </year> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 2(2) </journal> <pages> 197-220. </pages>
Reference-contexts: For model selection, this pessimism is of minor importance. r." Branch and bound algorithms were introduced by Narendra & Fukunaga (1977). Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search <ref> (Siedlecki & Sklansky 1988) </ref>, best first search (Xu, Yan, & Chang 1989), and genetic algorithms (Vafai & De Jong 1992). All the algorithms described above assume that the evaluation function is deterministic. When the evaluation function is a random variable, the search becomes more complicated.
Reference: <author> Thrun etal. </author> <year> 1991. </year> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Common machine learning algorithms, including top-down of decision tree algorithm, such as ID3 and C4.5 (Quinlan 1993), and instance based algorithms, such as IB3 (Aha, Kibler, & Albert 1991), are known to suffer from irrelevant features. For example, running C4.5 without special flags on the Monk 1 problem <ref> (Thrun etal. 1991) </ref>, which has three irrelevant features, generates a tree with 15 interior nodes, five of which test irrelevant features. The generated tree has an error rate of 24.3%, which is reduced to 11.1% if only the three relevant features are given. <p> On some artificial datasets, we have seen more dramatic examples of the improvement of a good search strategy. For example, on the monk1 dataset <ref> (Thrun etal. 1991) </ref>, C4.5's accuracy is 75.7%, C4.5-HCS's accuracy is 75.0%, and C4.5-BFS's accuracy is 88.9%.
Reference: <author> Vafai, H., and De Jong, K. </author> <year> 1992. </year> <title> Genetic algorithms as a tool for feature selection in machine learning. </title> <booktitle> In Fourth International Conference on Tools with Artificial Intelligence, </booktitle> <pages> 200-203. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best first search (Xu, Yan, & Chang 1989), and genetic algorithms <ref> (Vafai & De Jong 1992) </ref>. All the algorithms described above assume that the evaluation function is deterministic. When the evaluation function is a random variable, the search becomes more complicated. Greiner (1992) describes how to conduct a hill-climbing search when the evaluation function is probabilistic.
Reference: <author> Xu, L.; Yan, P.; and Chang, T. </author> <year> 1989. </year> <title> Best first strategy for feature selection. </title> <booktitle> In Ninth International Conference on Pattern Recognition, </booktitle> <pages> 706-708. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: For model selection, this pessimism is of minor importance. r." Branch and bound algorithms were introduced by Narendra & Fukunaga (1977). Finally, more recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best first search <ref> (Xu, Yan, & Chang 1989) </ref>, and genetic algorithms (Vafai & De Jong 1992). All the algorithms described above assume that the evaluation function is deterministic. When the evaluation function is a random variable, the search becomes more complicated.
Reference: <author> Yan, D., and Mukai, H. </author> <year> 1992. </year> <title> Stochastic discrete optimization. </title> <journal> Siam J. Control and Optimization 30(3) </journal> <pages> 594-612. </pages>
References-found: 23

