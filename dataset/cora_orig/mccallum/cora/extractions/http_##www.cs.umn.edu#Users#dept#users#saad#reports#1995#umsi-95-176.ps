URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/umsi-95-176.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/
Root-URL: http://www.cs.umn.edu
Title: Analysis of Augmented Krylov Subspace Methods  
Author: Yousef Saad 
Date: September 1, 1995  
Abstract: Residual norm estimates are derived for a general class of methods based on projection techniques on subspaces of the form K m + W, where K m is the standard Krylov subspace associated with the original linear system, and W is some other subspace. These `augmented Krylov subspace methods' include eigenvalue deflation techniques as well as block-Krylov methods. Residual bounds are established which suggest a convergence rate similar to one obtained by removing the components of the initial residual vector associated with the eigenvalues closest to zero. Both the symmetric and nonsymmetric case are analyzed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Chan and W. L. Wan. </author> <title> Analysis of projection methods for solving linear systems with multiple right-hand sides. </title> <type> Technical Report CAM-94-26, </type> <institution> University of California at Los-Angeles, department of mathematics, </institution> <address> Los Angless, CA 90024, </address> <year> 1994. </year>
Reference-contexts: An older technique is to augment the original subspace with other Krylov subspaces, typically with the same matrix and randomly generated right-hand sides. This gives rise to the class of block-Krylov and successive right-hand side methods which have recently seen a regain of interest. <ref> [14, 11, 1, 6, 5] </ref>. Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial.
Reference: [2] <author> A. Chapman and Y. Saad. </author> <title> Deflated and augmented Krylov subspace techniques. </title> <type> Technical Report xxx, </type> <institution> Minnesota Supercomputer Institute, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction It has been recently observed that significant improvements in convergence rates can be achieved from Krylov subspace methods by enriching these subspaces in a number of different ways, see, e.g., <ref> [2, 4, 8, 9] </ref>. One of the simplest ideas employed is to add to the Krylov subspace some approximation to an invariant subspace associated with a few of the lowest eigenvalues. A projection process on this augmented subspace is then carried out. <p> Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial. This is especially true when the convergence of the original scheme is hampered by a small number of eigenvalues near zero, see e.g., <ref> [2, 9] </ref>. fl University of Minnesota, department of Computer Science, Minneapolis, MN 55455; saad@cs.umn.edu. This work was supported in part by NASA from grant NAG2-904 and in part by NSF from grant CCR-9214116. 1 In this paper we take a theoretical look at this general class of `augmented Krylov methods'. <p> There are many possible ways in which to choose the subspace W following this intuitive idea. In deflation techniques <ref> [9, 2] </ref>, W is an approximate invariant subspace typically associated with the smallest eigenvalues and obtained as a by-product of earlier projection steps. <p> We can also generate another Krylov sequence starting with an arbitrary vector w 1 and append the resulting vectors w 2 ; : : : ; w s to the subspace. Some of these variations are explored in <ref> [2] </ref>. The above algorithm is a trivial extension of the modified Arnoldi process used in the Flexible GMRES (FGMRES) algorithm [12]. Its result is that the vectors v 1 ; : : : ; v m+p+1 forms an orthonormal set of vectors. A number of immediate properties can be established. <p> The following runs were made. 1. Standard GMRES without restarts and restarted GMRES, with a Krylov dimen sion of 40. 2. Block GMRES (BGMRES) without restarts. The block size chosen is four, which is the size of the cluster. 3. A deflated GMRES algorithm as described in [9] and <ref> [2] </ref>. This consists of adding approximate eigenvectors obtained from the previous Arnoldi step, to the Krylov subspace. The test uses a subspace dimension of 40, the last four of which are approximate eigenvectors (except in the first outer iteration). This is denoted by DGMRES (40,4). 4. <p> If we had only one linear system to solve, the results of the plot indicate that a plain or a deflated GMRES run may achieve far better performance. This is confirmed by experiments elsewhere, see e.g., <ref> [2] </ref>. Acknowledgement. This research was conducted while I was on leave at the University of California at Los Angeles. I wish to thank the members of Applied Mathematics group at UCLA for their hospitality during this visit. The Minnesota Supercomputer Institute provided computer facilities and other resources to conduct this research.
Reference: [3] <author> F. Chatelin. </author> <title> Spectral Approximation of Linear Operators. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1984. </year> <title> 16 cluster around the origin 17 </title>
Reference-contexts: Our goal is to show a residual bound indicating faster convergence when the invariant subspace is very close to W. 3.1 Basic Results We recall the following definition of the `gap' between subspaces. For details on this definition and some properties, see Kato [7] and Chatelin <ref> [3] </ref>. <p> P Y ) is an orthogonal projector onto X (resp. Y ). In fact when the two subspaces X and Y are of the same dimension then, <ref> [3, 7] </ref> fi (X; Y ) = ffi (X; Y ) = ffi (Y; X) = kP X P Y k 2 : In this case, fi (X; Y ) can be viewed as the sine of the angle between the two subspaces X and Y .
Reference: [4] <author> J. Erhel, K. Burrage, and B. Pohl. </author> <title> Restarted GMRES preconditioned by defla-tion. </title> <type> Technical Report -, IRISA, </type> <institution> Rennes, France, </institution> <year> 1994. </year> <note> to appear, Journal of Computational and Applied Mathematics, </note> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction It has been recently observed that significant improvements in convergence rates can be achieved from Krylov subspace methods by enriching these subspaces in a number of different ways, see, e.g., <ref> [2, 4, 8, 9] </ref>. One of the simplest ideas employed is to add to the Krylov subspace some approximation to an invariant subspace associated with a few of the lowest eigenvalues. A projection process on this augmented subspace is then carried out.
Reference: [5] <author> C. Farhat, L. Crivelli, and F. X. Roux. </author> <title> Extending substructure based iterative solvers to multiple load and repeated analyses. </title> <type> Technical report, </type> <institution> Center for Space Structures and Controls, Boulder, CO, </institution> <year> 1993. </year>
Reference-contexts: An older technique is to augment the original subspace with other Krylov subspaces, typically with the same matrix and randomly generated right-hand sides. This gives rise to the class of block-Krylov and successive right-hand side methods which have recently seen a regain of interest. <ref> [14, 11, 1, 6, 5] </ref>. Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial.
Reference: [6] <author> P. F. Fischer. </author> <title> Projection techniques for iterative solution of ax = b with successive right-hand sides. </title> <type> Technical Report TR-93-90, </type> <institution> ICASE, Hampton, VA, </institution> <year> 1993. </year>
Reference-contexts: An older technique is to augment the original subspace with other Krylov subspaces, typically with the same matrix and randomly generated right-hand sides. This gives rise to the class of block-Krylov and successive right-hand side methods which have recently seen a regain of interest. <ref> [14, 11, 1, 6, 5] </ref>. Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial.
Reference: [7] <author> T. Kato. </author> <title> Perturbation Theory for Linear Operators. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: Our goal is to show a residual bound indicating faster convergence when the invariant subspace is very close to W. 3.1 Basic Results We recall the following definition of the `gap' between subspaces. For details on this definition and some properties, see Kato <ref> [7] </ref> and Chatelin [3]. <p> P Y ) is an orthogonal projector onto X (resp. Y ). In fact when the two subspaces X and Y are of the same dimension then, <ref> [3, 7] </ref> fi (X; Y ) = ffi (X; Y ) = ffi (Y; X) = kP X P Y k 2 : In this case, fi (X; Y ) can be viewed as the sine of the angle between the two subspaces X and Y .
Reference: [8] <author> S. A. Kharchenko and A. Yu. Yeremin. </author> <title> Eigenvalue translation based preconditioners for the GMRES(k) method. </title> <type> Technical Report EM-RR-2/92, </type> <institution> Elegant Mathematics, Inc., </institution> -, <year> 1992. </year>
Reference-contexts: 1 Introduction It has been recently observed that significant improvements in convergence rates can be achieved from Krylov subspace methods by enriching these subspaces in a number of different ways, see, e.g., <ref> [2, 4, 8, 9] </ref>. One of the simplest ideas employed is to add to the Krylov subspace some approximation to an invariant subspace associated with a few of the lowest eigenvalues. A projection process on this augmented subspace is then carried out.
Reference: [9] <author> R. B. Morgan. </author> <title> A restarted GMRES method augmented with eigenvectors. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 16:-, </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction It has been recently observed that significant improvements in convergence rates can be achieved from Krylov subspace methods by enriching these subspaces in a number of different ways, see, e.g., <ref> [2, 4, 8, 9] </ref>. One of the simplest ideas employed is to add to the Krylov subspace some approximation to an invariant subspace associated with a few of the lowest eigenvalues. A projection process on this augmented subspace is then carried out. <p> Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial. This is especially true when the convergence of the original scheme is hampered by a small number of eigenvalues near zero, see e.g., <ref> [2, 9] </ref>. fl University of Minnesota, department of Computer Science, Minneapolis, MN 55455; saad@cs.umn.edu. This work was supported in part by NASA from grant NAG2-904 and in part by NSF from grant CCR-9214116. 1 In this paper we take a theoretical look at this general class of `augmented Krylov methods'. <p> There are many possible ways in which to choose the subspace W following this intuitive idea. In deflation techniques <ref> [9, 2] </ref>, W is an approximate invariant subspace typically associated with the smallest eigenvalues and obtained as a by-product of earlier projection steps. <p> The following runs were made. 1. Standard GMRES without restarts and restarted GMRES, with a Krylov dimen sion of 40. 2. Block GMRES (BGMRES) without restarts. The block size chosen is four, which is the size of the cluster. 3. A deflated GMRES algorithm as described in <ref> [9] </ref> and [2]. This consists of adding approximate eigenvectors obtained from the previous Arnoldi step, to the Krylov subspace. The test uses a subspace dimension of 40, the last four of which are approximate eigenvectors (except in the first outer iteration). This is denoted by DGMRES (40,4). 4.
Reference: [10] <author> D. O'Leary. </author> <title> The block conjugate gradient algorithm and related methods. </title> <journal> Linear Algebra Appl., </journal> <volume> 29 </volume> <pages> 243-322, </pages> <year> 1980. </year>
Reference-contexts: The starting vector v (1) 1 of the the first Krylov subspace is the normalized residual r 0 =kr 0 k 2 . A number of results for analyzing block methods have already been established in the literature <ref> [10, 14] </ref>. The approach presented here shows similar results which are somewhat simpler, by introducing sys tematically a subsidiary approximate solution obtained by a projection step onto the subspace spanned by the initial block.
Reference: [11] <author> K. Guru Prasad, D. E. Keyes, and J. H. Kane. </author> <title> GMRES for sequentially multiple nearby systems. </title> <institution> Technical Report -, Old Dominium University, </institution> <year> 1995. </year>
Reference-contexts: An older technique is to augment the original subspace with other Krylov subspaces, typically with the same matrix and randomly generated right-hand sides. This gives rise to the class of block-Krylov and successive right-hand side methods which have recently seen a regain of interest. <ref> [14, 11, 1, 6, 5] </ref>. Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial.
Reference: [12] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: Some of these variations are explored in [2]. The above algorithm is a trivial extension of the modified Arnoldi process used in the Flexible GMRES (FGMRES) algorithm <ref> [12] </ref>. Its result is that the vectors v 1 ; : : : ; v m+p+1 forms an orthonormal set of vectors. A number of immediate properties can be established.
Reference: [13] <author> Y. Saad. </author> <title> Iterative methods for sparse linear systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: Proof. Assume that w is a vector in W such that Aw = v i+1 . Recall the standard relation <ref> [13] </ref>, AV i = V i H i + h i+1;i v i+1 e T A solution among vectors of the form x = x 0 + V i y + ffw will be constructed.
Reference: [14] <author> V. Simoncini and E. Gallopoulos. </author> <title> Convergence properties of block GMRES for solving systems with multiple right-hand sides. </title> <type> Technical Report 1316, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> Oct. </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: An older technique is to augment the original subspace with other Krylov subspaces, typically with the same matrix and randomly generated right-hand sides. This gives rise to the class of block-Krylov and successive right-hand side methods which have recently seen a regain of interest. <ref> [14, 11, 1, 6, 5] </ref>. Results of experiments obtained from these alternatives indicate that the improvement in convergence over standard Krylov subspaces of the same dimension can sometimes be substantial. <p> The starting vector v (1) 1 of the the first Krylov subspace is the normalized residual r 0 =kr 0 k 2 . A number of results for analyzing block methods have already been established in the literature <ref> [10, 14] </ref>. The approach presented here shows similar results which are somewhat simpler, by introducing sys tematically a subsidiary approximate solution obtained by a projection step onto the subspace spanned by the initial block.
References-found: 14

