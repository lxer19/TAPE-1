URL: ftp://flint.cs.yale.edu/pub/flint/publications/closure.ps.gz
Refering-URL: http://daffy.cs.yale.edu/users/shao-zhong/papers.html
Root-URL: http://www.cs.yale.edu
Email: zsh@cs.princeton.edu appel@cs.princeton.edu  
Title: Space-Efficient Closure Representations  
Author: Zhong Shao and Andrew W. Appel 
Address: Princeton, NJ 08544-2087  
Affiliation: Department of Computer Science, Princeton University  
Abstract: Many modern compilers implement function calls (or returns) in two steps: first, a closure environment is properly installed to provide access for free variables in the target program fragment; second, the control is transferred to the target by a "jump with arguments (or results)." Closure conversion, which decides where and how to represent closures at runtime, is a crucial step in compilation of functional languages. We have a new algorithm that exploits the use of compile-time control and data flow information to optimize closure representations. By extensive closure sharing and allocating as many closures in registers as possible, our new closure conversion algorithm reduces heap allocation by 36% and memory fetches for local/global variables by 43%; and improves the already-efficient code generated by the Standard ML of New Jersey compiler by about 17% on a DECstation 5000. Moreover, unlike most other approaches, our new closure allocation scheme satisfies the strong "safe for space complexity" rule, thus achieving good asymptotic space usage. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew W. Appel. </author> <title> Garbage collection can be faster than stack allocation. </title> <journal> Information Processing Letter, </journal> <volume> 25(4) </volume> <pages> 275-79, </pages> <year> 1987. </year>
Reference-contexts: Stacks do have a much better write miss ratio, but not a much better read miss ratio. But on many modern machines, the write miss penalty is approximately zero [23, 16, 7]. 3. The amortized cost of collection can be very low <ref> [1, 7] </ref>, especially with modern generational garbage collection techniques [36].
Reference: [2] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In 1988, Chase [11] observed that certain storage allocation optimization may convert a program that runs robustly into one that does not, due to the requirement of larger fraction of memory than the program actually needs. Appel <ref> [2] </ref> also noticed that programs using linked closures 1 , 1 A linked closure [27] is a record that contains the bound variables of the enclosing function, together with a pointer to the or stack-allocated activation records, may cause a compiled program to use much more memory. fun f (v,w,x,y,z) = <p> The SSC rule is stated as follows:any local variable binding must be unreachable after its last use within its scope (see Appel <ref> [2] </ref> for a more formal definition). Traditional stack allocation schemes and linked closures obviously violate this rule because local variable bindings will stay on the stack until they exit their scope, so may remain live even after their last use. <p> Unlike linked closures, the nesting level of safely linked closures never exceeds more than two, so they still enjoy very fast variable access time. 3 Continuations and Closures We will illustrate CPS-conversion (which is not new <ref> [33, 26, 2] </ref>), and our new closure analysis algorithm, on the example 3 In practice, both this and SSC can be relaxed a little because the asymptotic space complexity will not change if we retain some variables that can be proven of constant size at compile time. in Figure 4. <p> If y's last use is much earlier than w's or x's, then the record (w; x; y) might not obey the SSC rule. Most closure conversion algorithms <ref> [2, 26, 33] </ref> start with a phase to gather the set of raw free variables for each function definition in E. <p> We compare the performance of two compilers, using our Old algorithm <ref> [2, 6] </ref> and the New algorithm described in this paper. The Old algorithm uses a hybrid scheme: it uses linked closure representation if it is space safe, otherwise it uses flat closure representation. Both the Old and New compilers satisfy the "safe for space complexity" rule. <p> Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19]. We had shown how to represent callee-save registers in continuation-passing style <ref> [6, 2] </ref> but our new algorithm does a much better job of it. Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated [9, 11, 2]. <p> We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it. Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated <ref> [9, 11, 2] </ref>. The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore [18, 12].
Reference: [3] <author> Andrew W. Appel and Trevor Jim. </author> <title> Optimizing closure environment representations. </title> <type> Technical Report 168, </type> <institution> Dept. of Computer Science, Princeton University, Princeton, NJ, </institution> <year> 1988. </year>
Reference-contexts: Flat closures do satisfy the SSC rule, but they require that variables be copied many times from one closure to another. Many of the closure strategies described by Appel and Jim <ref> [3] </ref> violate the rule. Most stack-frame implementations also violate SSC, since dead variables remain in the frame until a function returns. This can be avoided by associating a descriptor with each return address, showing which variables are live; but this complicates the garbage collector [7, 8]. <p> Closure analysis: Steele [33] used continuation closures instead of "stack frames;" Rozas [30] and Kranz [25, 26] used closure analysis to choose specialized representations for different kinds of closures; Appel and Jim investigated closure-sharing strategies <ref> [3] </ref>. We combine all of these analyses (except stack allocation) and more. Tail calls: Hanson [20] showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack.
Reference: [4] <author> Andrew W. Appel and Trevor Jim. </author> <title> Continuation-passing, closure-passing style. </title> <booktitle> In Sixteenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 293-302, </pages> <address> New York, 1989. </address> <publisher> ACM Press. </publisher>
Reference-contexts: stack allocation, this is impossible since stack frames normally have shorter lifetime than heap-allocated closures. * Tail recursive calls|which are often quite troublesome to implement correctly on a stack [20]|can be imple mented very easily. * All of our closure optimizations can be cleanly represented using continuation-passing and closure-passing style <ref> [4] </ref> as the intermediate language. * Once a closure is created, no later writes are made to it; this makes generational garbage collection and call/cc efficient, and also reduces the need for alias analysis in the compiler. * Because all closures are either allocated in the heap or in registers, first <p> The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques [26, 6, 33, 30, 20, 22], using a simple and general framework expressed in continuation-passing and closure-passing style <ref> [4, 6] </ref>. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> In our previous work [6], we outlined this framework and demonstrated that it could reduce allocation and memory traffic. However, we did not have a really good algorithm to exploit the flexibility that callee-save registers provide. Closure creation and use can also be represented using the CPS language itself <ref> [4, 24] </ref>. We call this closure-passing style (CLO). The main difference between CLO and CPS is that functions in CLO do not contain free variables, so they can be translated directly into machine code. <p> Where there are more than three free variables, some of the callee-save arguments must be heap-allocated records containing several variables each; thus, the CR closure-record appears as J1 in the call on line 19. Previous closure conversion algorithms <ref> [33, 25, 4] </ref> require memory stores for each continuation function. <p> Tail calls: Hanson [20] showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack. Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") <ref> [14, 22, 4, 24] </ref>. Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19].
Reference: [5] <author> Andrew W. Appel and David B. MacQueen. </author> <title> Standard ML of New Jersey. </title> <editor> In Martin Wirsing, editor, </editor> <booktitle> Third Int'l Symp. on Prog. Lang. Implementation and Logic Programming, </booktitle> <pages> pages 1-13, </pages> <address> New York, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated [9, 11, 2]. The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler <ref> [5] </ref> are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore [18, 12].
Reference: [6] <author> Andrew W. Appel and Zhong Shao. </author> <title> Callee-save registers in continuation-passing style. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 5(3) </volume> <pages> 191-221, </pages> <year> 1992. </year>
Reference-contexts: Function returns are implemented in the same way because they are essentially calls to continuation functions, if represented in CPS. A closure can be any combination of registers and memory data structures that gives access to the free variables <ref> [25, 6] </ref>. The compiler is free to choose a closure representation that fl To appear in ACM Conference on Lisp and Functional Programming, June 1994 minimizes stores (closure creation), fetches (to access free variables), and memory use (reachable data). We have developed a new algorithm for choosing good closure representations. <p> These assumptions no longer hold, for three reasons: 1. As we will show in Section 4, because most parts of continuation closures are allocated in callee-save registers <ref> [6] </ref>, the extra memory write and read at each call can often be avoided. With the help of compile-time control and data flow information, the combination of shared closures and callee-save registers can often be comparable to or even better than stack allocation [7]. 2. <p> The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques [26, 6, 33, 30, 20, 22], using a simple and general framework expressed in continuation-passing and closure-passing style <ref> [4, 6] </ref>. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> Variables not live after the call may be allocated to caller-save registers which cuts down on register-saving traffic. We wanted to adapt this idea to our continuation-passing intermediate representation. We did so as follows <ref> [6] </ref>: each CPS-converted user function f is passed its ordinary arguments, a continuation function c 0 , and k extra arguments c 1 ; :::; c k . <p> One could also say that the continuation is represented in k + 1 registers (c 0 ; :::; c k ) instead of in just one pointer to a memory-resident closure. In our previous work <ref> [6] </ref>, we outlined this framework and demonstrated that it could reduce allocation and memory traffic. However, we did not have a really good algorithm to exploit the flexibility that callee-save registers provide. Closure creation and use can also be represented using the CPS language itself [4, 24]. <p> Because their call sites are not known at compile time, most continuation functions have to use the uniform convention, i.e., always in k callee-save registers <ref> [6] </ref>. In special cases, some continuation functions can be represented differently; this will be briefly discussed in Section 5.3. * For known functions, since their call sites are all known at compile time, their closures (or environments) may be allocated completely in registers. <p> We compare the performance of two compilers, using our Old algorithm <ref> [2, 6] </ref> and the New algorithm described in this paper. The Old algorithm uses a hybrid scheme: it uses linked closure representation if it is space safe, otherwise it uses flat closure representation. Both the Old and New compilers satisfy the "safe for space complexity" rule. <p> Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19]. We had shown how to represent callee-save registers in continuation-passing style <ref> [6, 2] </ref> but our new algorithm does a much better job of it. Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated [9, 11, 2].
Reference: [7] <author> Andrew W. Appel and Zhong Shao. </author> <title> An empirical and analytic study of stack vs. heap cost for languages with closures. </title> <type> Technical Report CS-TR-450-94, </type> <institution> Princeton University, Department of Computer Science, Princeton, NJ, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: With the help of compile-time control and data flow information, the combination of shared closures and callee-save registers can often be comparable to or even better than stack allocation <ref> [7] </ref>. 2. In a companion paper [7], we show that stacks do not have a significantly better locality of reference than heap-allocated activation records, even in a modern cache memory hierarchy. Stacks do have a much better write miss ratio, but not a much better read miss ratio. <p> With the help of compile-time control and data flow information, the combination of shared closures and callee-save registers can often be comparable to or even better than stack allocation <ref> [7] </ref>. 2. In a companion paper [7], we show that stacks do not have a significantly better locality of reference than heap-allocated activation records, even in a modern cache memory hierarchy. Stacks do have a much better write miss ratio, but not a much better read miss ratio. <p> Stacks do have a much better write miss ratio, but not a much better read miss ratio. But on many modern machines, the write miss penalty is approximately zero <ref> [23, 16, 7] </ref>. 3. The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. <p> Stacks do have a much better write miss ratio, but not a much better read miss ratio. But on many modern machines, the write miss penalty is approximately zero [23, 16, 7]. 3. The amortized cost of collection can be very low <ref> [1, 7] </ref>, especially with modern generational garbage collection techniques [36]. <p> Most stack-frame implementations also violate SSC, since dead variables remain in the frame until a function returns. This can be avoided by associating a descriptor with each return address, showing which variables are live; but this complicates the garbage collector <ref> [7, 8] </ref>. Obeying SSC can require extra copying of pointer values from an old closure that contains them (but also contains values not needed in a new context) into a new closure. <p> With our new closure analysis technique to make good use of callee-save registers, heap-allocated activation records can be made almost as efficient as stack allocation <ref> [7] </ref>. The idea is that we can always allocate most parts of the current activation record in callee-save registers. With careful lifetime analysis, register save/restore around several function calls can often be eliminated or amalgamated, so function calls in sequence need just write one heap record.
Reference: [8] <author> Lennart Augustsson. </author> <title> Garbage collection in the &lt; -; g &gt;- machine. </title> <type> Technical Report PMG memo 73, </type> <institution> Dept. of Computer Sciences, Chalmers University of Technology, Gote-borg, Sweden, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Most stack-frame implementations also violate SSC, since dead variables remain in the frame until a function returns. This can be avoided by associating a descriptor with each return address, showing which variables are live; but this complicates the garbage collector <ref> [7, 8] </ref>. Obeying SSC can require extra copying of pointer values from an old closure that contains them (but also contains values not needed in a new context) into a new closure. <p> Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated [9, 11, 2]. The Chalmers Lazy-ML compiler <ref> [8] </ref> and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore [18, 12].
Reference: [9] <author> Henry G. Baker. </author> <title> The buried binding and stale binding prob-lems of LISP 1.5. </title> <note> unpublished, undistributed paper, </note> <month> June </month> <year> 1976. </year>
Reference-contexts: We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it. Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated <ref> [9, 11, 2] </ref>. The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore [18, 12].
Reference: [10] <author> Luca Cardelli. </author> <title> Compiling a functional language. </title> <booktitle> In Proc. of the 1984 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 208-217, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: All recent versions of SML/NJ have obeyed the "safe for space complexity" (SSC) rule, and users really did notice enclosing function's closure. 2 A flat closure <ref> [10] </ref> is a record that holds only the free variables needed by the function.
Reference: [11] <author> David R. Chase. </author> <title> Safety considerations for storage allocation optimizations. </title> <booktitle> In Proc. ACM SIGPLAN '88 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 1-9, </pages> <address> New York, June 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In 1988, Chase <ref> [11] </ref> observed that certain storage allocation optimization may convert a program that runs robustly into one that does not, due to the requirement of larger fraction of memory than the program actually needs. <p> We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it. Safe space complexity: The notion that certain compiler optimizations can cause space leaks by remembering too much is old, but only recently appreciated <ref> [9, 11, 2] </ref>. The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore [18, 12].
Reference: [12] <author> Fred C. Chow. </author> <title> Minimizing register usage penalty at procedure calls. </title> <booktitle> In Proc. ACM SIGPLAN '88 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 85-94, </pages> <address> New York, June 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers <ref> [12, 19] </ref>. We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it. <p> The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore <ref> [18, 12] </ref>. A stack of regions: Tofte and Talpin have demonstrated an analysis that can avoid garbage collection entirely [35], but unfortunately it does not satisfy the "safe for space complexity" rule. 8 Conclusions Our new closure conversion algorithm is a great success.
Reference: [13] <author> William D Clinger, Anne H Hartheimer, and Eric M Ost. </author> <title> Implementation strategies for continuations. </title> <booktitle> In 1988 ACM Conference on Lisp and Fucntional Programming, </booktitle> <pages> pages 124-131, </pages> <address> New York, June 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") [14, 22, 4, 24]. Efficient call/cc: Many have tried to make call/cc effi cient <ref> [15, 13, 21] </ref>. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19]. We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it.
Reference: [14] <author> G. Cousineau, P. L. Curien, and M. Mauny. </author> <title> The categorical abstract machine. </title> <editor> In J. P. Jouannaud, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <volume> LNCS Vol 201, </volume> <pages> pages 50-64, </pages> <address> New York, 1985. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Tail calls: Hanson [20] showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack. Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") <ref> [14, 22, 4, 24] </ref>. Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19].
Reference: [15] <author> Olivier Danvy. </author> <title> Memory allocation and higher-order functions. </title> <booktitle> In Proceedings of the SIGPLAN'87 Symposium on Interpreters and Interpretive Techniques, </booktitle> <pages> pages 241-252. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") [14, 22, 4, 24]. Efficient call/cc: Many have tried to make call/cc effi cient <ref> [15, 13, 21] </ref>. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19]. We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it.
Reference: [16] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs using copying garbage collection. </title> <booktitle> In Proc. 21st Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, </booktitle> <pages> pages 1-14. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: Stacks do have a much better write miss ratio, but not a much better read miss ratio. But on many modern machines, the write miss penalty is approximately zero <ref> [23, 16, 7] </ref>. 3. The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36].
Reference: [17] <author> Lal George, Florent Guillaume, and John Reppy. </author> <title> A portable and optimizing backend for the SML/NJ compiler. </title> <booktitle> In Proceedings of the 1994 International Conference on Compiler Construction, page (to appear), </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: (e.g., register r) may use up all the available machine registers and cause unnecessary register spilling, but this can always be avoided by selectively keeping limited number of intermediate variables in the "lazy display" (registers). 4.6 Remarks Graph-coloring global register allocation and targeting, which have been implemented by Lal George <ref> [17] </ref>, will accomplish most control transfers (function calls) (such as line 12 and 13 in Figure 6) without any register-register moves. This allows a more flexible boundary between callee-save and caller-save registers than is normal in most compilers.
Reference: [18] <author> Carsten K. Gomard and Peter Sestoft. </author> <title> Globalization and live variables. </title> <booktitle> In Proceedings of the 1991 Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 166-177. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: The Chalmers Lazy-ML compiler [8] and the SML/NJ compiler [5] are the only ones we know of that guarantee "space safety." Globalization: Local variables of different functions with nonoverlapping live ranges can be allocated to the same register or global without any save/restore <ref> [18, 12] </ref>. A stack of regions: Tofte and Talpin have demonstrated an analysis that can avoid garbage collection entirely [35], but unfortunately it does not satisfy the "safe for space complexity" rule. 8 Conclusions Our new closure conversion algorithm is a great success.
Reference: [19] <author> Jr. Guy L. Steele and Gerald Jay Sussman. </author> <title> The dream of a lifetime: A lazy variable extent mechanism. </title> <booktitle> In Proceedings of the 1980 LISP Conference, </booktitle> <pages> pages 163-172, </pages> <address> Stanford, </address> <year> 1980. </year>
Reference-contexts: Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers <ref> [12, 19] </ref>. We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it.
Reference: [20] <author> Chris Hanson. </author> <title> Efficient stack allocation for tail-recursive languages. </title> <booktitle> In 1990 ACM Conference on Lisp and Fucntional Programming, </booktitle> <pages> pages 106-118, </pages> <address> New York, June 1990. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> Programs, in our scheme, tend to accumulate values in registers and only dump them into a closure at infrequent intervals. It may be useful to use more callee-save (and fewer caller-save) registers to optimize this. Our closure scheme handles tail calls very nicely, simply by re-arranging registers. Hanson <ref> [20] </ref> shows how complicated things become when it's necessary to re-arrange a stack frame. A source-language function that calls several other functions in sequence would, in previous CPS compilers (including our own) allocate a continuation closure for each call. <p> We combine all of these analyses (except stack allocation) and more. Tail calls: Hanson <ref> [20] </ref> showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack. Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") [14, 22, 4, 24].
Reference: [21] <author> Robert Hieb, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <booktitle> In Proc. ACM SIGPLAN '90 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 66-77, </pages> <address> New York, 1990. </address> <publisher> ACM Press. </publisher>
Reference-contexts: writes are made to it; this makes generational garbage collection and call/cc efficient, and also reduces the need for alias analysis in the compiler. * Because all closures are either allocated in the heap or in registers, first class continuations call/cc are very ef ficient, requiring no complicated stack hackery <ref> [21] </ref>. Our new closure allocation scheme does not use any run-time stack. Instead, all closure environments are either allocated in the heap or in registers. <p> Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") [14, 22, 4, 24]. Efficient call/cc: Many have tried to make call/cc effi cient <ref> [15, 13, 21] </ref>. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19]. We had shown how to represent callee-save registers in continuation-passing style [6, 2] but our new algorithm does a much better job of it.
Reference: [22] <author> Thomas Johnsson. </author> <title> Lambda Lifting: Transforming Programs to Recursive Equations. </title> <booktitle> In The Second International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 190-203, </pages> <address> New York, </address> <month> September </month> <year> 1985. </year> <note> Springer-Verlag. </note>
Reference-contexts: The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> This is because the lifetime of w and x (also g and y) does not overlap, so they can just share one callee-save register (i.e., J3 and K3, K2 and Q2). 5.2 Lambda lifting on known function Lambda lifting <ref> [22] </ref> is a well-known transformation that rewrites a program into an equivalent one in which no function has free variables. Lambda lifting on known func tions essentially corresponds to the special closure allocation strategy that allocates as many free variables in registers as possible. <p> Tail calls: Hanson [20] showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack. Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") <ref> [14, 22, 4, 24] </ref>. Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19].
Reference: [23] <author> Norman P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Stacks do have a much better write miss ratio, but not a much better read miss ratio. But on many modern machines, the write miss penalty is approximately zero <ref> [23, 16, 7] </ref>. 3. The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36].
Reference: [24] <author> Richard Kelsey and Paul Hudak. </author> <title> Realistic compilation by program transformation. </title> <booktitle> In Sixteenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 281-92, </pages> <address> New York, 1989. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In our previous work [6], we outlined this framework and demonstrated that it could reduce allocation and memory traffic. However, we did not have a really good algorithm to exploit the flexibility that callee-save registers provide. Closure creation and use can also be represented using the CPS language itself <ref> [4, 24] </ref>. We call this closure-passing style (CLO). The main difference between CLO and CPS is that functions in CLO do not contain free variables, so they can be translated directly into machine code. <p> Tail calls: Hanson [20] showed the complexity of implementing tail calls correctly and efficiently on a conven tional stack. Closure-passing style: Lambda notation has often been used to represent the results of closure analysis (this is also called "lambda lifting") <ref> [14, 22, 4, 24] </ref>. Efficient call/cc: Many have tried to make call/cc effi cient [15, 13, 21]. Callee-save registers: Dataflow analysis can help decide whether to put variables in caller-save or callee-save registers [12, 19].
Reference: [25] <author> D. Kranz, R. Kelsey, J. Rees, P. Hudak, J. Philbin, and N. Adams. </author> <title> ORBIT: An optimizing compiler for Scheme. </title> <booktitle> SIGPLAN Notices (Proc. Sigplan '86 Symp. on Compiler Construction), </booktitle> <volume> 21(7) </volume> <pages> 219-33, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Function returns are implemented in the same way because they are essentially calls to continuation functions, if represented in CPS. A closure can be any combination of registers and memory data structures that gives access to the free variables <ref> [25, 6] </ref>. The compiler is free to choose a closure representation that fl To appear in ACM Conference on Lisp and Functional Programming, June 1994 minimizes stores (closure creation), fetches (to access free variables), and memory use (reachable data). We have developed a new algorithm for choosing good closure representations. <p> This problem is solved by adding a closure which makes explicit the access to all nonlocal variables. Kranz <ref> [25, 26] </ref> showed that different kinds of functions should use different closure allocation strategies. <p> Where there are more than three free variables, some of the callee-save arguments must be heap-allocated records containing several variables each; thus, the CR closure-record appears as J1 in the call on line 19. Previous closure conversion algorithms <ref> [33, 25, 4] </ref> require memory stores for each continuation function. <p> Closure analysis: Steele [33] used continuation closures instead of "stack frames;" Rozas [30] and Kranz <ref> [25, 26] </ref> used closure analysis to choose specialized representations for different kinds of closures; Appel and Jim investigated closure-sharing strategies [3]. We combine all of these analyses (except stack allocation) and more.
Reference: [26] <author> David Kranz. </author> <title> ORBIT: An optimizing compiler for Scheme. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1987. </year>
Reference-contexts: The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> Unlike linked closures, the nesting level of safely linked closures never exceeds more than two, so they still enjoy very fast variable access time. 3 Continuations and Closures We will illustrate CPS-conversion (which is not new <ref> [33, 26, 2] </ref>), and our new closure analysis algorithm, on the example 3 In practice, both this and SSC can be relaxed a little because the asymptotic space complexity will not change if we retain some variables that can be proven of constant size at compile time. in Figure 4. <p> This problem is solved by adding a closure which makes explicit the access to all nonlocal variables. Kranz <ref> [25, 26] </ref> showed that different kinds of functions should use different closure allocation strategies. <p> If y's last use is much earlier than w's or x's, then the record (w; x; y) might not obey the SSC rule. Most closure conversion algorithms <ref> [2, 26, 33] </ref> start with a phase to gather the set of raw free variables for each function definition in E. <p> Notice that a variable can have different lut and fut numbers inside different function definitions (e.g., a in J and Q). 4.3 Closure strategy analysis Closure strategy analysis essentially determines where in the machine to allocate each closure. Unlike previous CPS compilers <ref> [26, 33] </ref>, we do not do any escape analysis 7 because we simply do not use a runtime stack. Our closure strategy analysis only decides how many slots (i.e., registers) each closure is going to use, denoted by S (f ) for each function f . <p> We use the "lazy display" technique used by Kranz <ref> [26] </ref>, so that loads of common paths can be shared. <p> Many compilers identify special control structures at compile time, and assign each of them a special closure allocation strategy. For example, in Kranz's Orbit compiler <ref> [26] </ref>, all tail recursions are asssigned a so-called "stack/loop" strategy, and all general recursions are assigned a "stack/recursion" strategy. <p> Lambda lifting on known func tions essentially corresponds to the special closure allocation strategy that allocates as many free variables in registers as possible. But this special strategy does not always generate very efficient code <ref> [26] </ref>. <p> Closure analysis: Steele [33] used continuation closures instead of "stack frames;" Rozas [30] and Kranz <ref> [25, 26] </ref> used closure analysis to choose specialized representations for different kinds of closures; Appel and Jim investigated closure-sharing strategies [3]. We combine all of these analyses (except stack allocation) and more.
Reference: [27] <author> P. J. Landin. </author> <title> The mechanical evaluation of expressions. </title> <journal> Computer Journal, </journal> <volume> 6(4) </volume> <pages> 308-20, </pages> <year> 1964. </year>
Reference-contexts: Appel [2] also noticed that programs using linked closures 1 , 1 A linked closure <ref> [27] </ref> is a record that contains the bound variables of the enclosing function, together with a pointer to the or stack-allocated activation records, may cause a compiled program to use much more memory. fun f (v,w,x,y,z) = let val u = hd (v) let fun i () = w+x+y+z+3 end end
Reference: [28] <author> Xavier Leroy. </author> <title> Unboxed objects and polymorphic typing. </title> <booktitle> In Nineteenth Annual ACM Symp. on Principles of Prog. Languages, </booktitle> <address> New York, Jan 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Both the Old and New compilers satisfy the "safe for space complexity" rule. Both compilers represent continuation closures using three callee-save registers. Both compilers use representation analysis <ref> [28] </ref> to allow arguments being passed in registers. The "lazy display" technique is implemented in both compilers, however it is used more effectively in the New compiler because of its more extensive use of shared Program Size Description Barnes-Hut 3036 N-body problem solver. Boyer 919 Standard theorem-prover benchmark.
Reference: [29] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: () = w+x+y+z+3 end end end fun big n = if n&lt;1 then [0] else n :: big (n-1) fun loop (n,res) = if n&lt;1 then res else (let val s = f (big (N),0,0,0,0)() in loop (n-1,s::res) end) val result = loop (N,[]) For example, consider the Standard ML <ref> [29] </ref> program in of f (...)() yields a closure s for h that contains just a few integers u, w, x, y, and z; the final result (e.g., result) contains N copies of the closure s for h, thus it uses at most O (N ) space.
Reference: [30] <author> Guillermo Juan Rozas. </author> <title> Liar, an algol-like compiler for scheme. </title> <type> S.B. thesis, </type> <institution> MIT Dept. of Computer Science and Electrical Engineering, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> Closure analysis: Steele [33] used continuation closures instead of "stack frames;" Rozas <ref> [30] </ref> and Kranz [25, 26] used closure analysis to choose specialized representations for different kinds of closures; Appel and Jim investigated closure-sharing strategies [3]. We combine all of these analyses (except stack allocation) and more.
Reference: [31] <author> Barbara G. Ryder and Marvin C. Paull. </author> <title> Elimination algorithms for data flow analysis. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Cycles in the graph imply loops or recursions (e.g., the path from h to J to Q). The nested hierarchies of loops and recursions in E can be revealed by running the Tarjan interval analysis 6 algorithm <ref> [34, 31] </ref> on G, assuming G is a reducible flow graph. For the purpose of our closure analysis, this control flow information can be used to choose closure representations that allow more efficient variable accesses in frequently-executed program fragments (e.g., loops). <p> closure representation analysis phase; we use the true free variables to denote the 5 Shivers [32] presented more sophisticated techniques that can find even better approximations of control flow information. 6 Given a flow graph G, a Tarjan interval is essentially a single-entry, strongly-connected subgraph of G; the interval analysis <ref> [31] </ref> partitions the set of nodes in G into disjoint intervals, with each interval representing a proper loop (or recursion) layer.
Reference: [32] <author> Olin Shivers. </author> <title> Control-Flow Analysis of Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1991. </year> <month> CMU-CS-91-145. </month>
Reference-contexts: This function can be defined in the ordinary way (by fun), and will presumably be invoked by the callee in order to continue the computation. been hoisted out of the loop because it is loop-invariant <ref> [32] </ref>. Such optimizations are performed after CPS-conversion, but before the closure analysis that is the subject of this paper. To ease the presentation, we use capital letters to denote continuations (e.g., C, J, and Q). <p> These free variables are called raw free variables because some of them may be substituted by a set of other free variables later during the closure representation analysis phase; we use the true free variables to denote the 5 Shivers <ref> [32] </ref> presented more sophisticated techniques that can find even better approximations of control flow information. 6 Given a flow graph G, a Tarjan interval is essentially a single-entry, strongly-connected subgraph of G; the interval analysis [31] partitions the set of nodes in G into disjoint intervals, with each interval representing a
Reference: [33] <author> Guy L. Steele. Rabbit: </author> <title> a compiler for Scheme. </title> <type> Technical Report AI-TR-474, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: Before a function call, context information is saved from registers into a "frame." In a compiler based on Continuation-Passing Style (CPS), this "frame" is the closure of a continuation function <ref> [33] </ref>. In a CPS-based compiler, a closure environment is constructed at each function (or continuation) definition site; it provides runtime access to bindings of variables free in the function (or continuation) body. <p> The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques [36]. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques <ref> [26, 6, 33, 30, 20, 22] </ref>, using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6]. Our new algorithm extensively exploits the use of compile-time control and data flow information to optimize closure allocation strategies and representations. <p> Unlike linked closures, the nesting level of safely linked closures never exceeds more than two, so they still enjoy very fast variable access time. 3 Continuations and Closures We will illustrate CPS-conversion (which is not new <ref> [33, 26, 2] </ref>), and our new closure analysis algorithm, on the example 3 In practice, both this and SSC can be relaxed a little because the asymptotic space complexity will not change if we retain some variables that can be proven of constant size at compile time. in Figure 4. <p> Where there are more than three free variables, some of the callee-save arguments must be heap-allocated records containing several variables each; thus, the CR closure-record appears as J1 in the call on line 19. Previous closure conversion algorithms <ref> [33, 25, 4] </ref> require memory stores for each continuation function. <p> If y's last use is much earlier than w's or x's, then the record (w; x; y) might not obey the SSC rule. Most closure conversion algorithms <ref> [2, 26, 33] </ref> start with a phase to gather the set of raw free variables for each function definition in E. <p> Notice that a variable can have different lut and fut numbers inside different function definitions (e.g., a in J and Q). 4.3 Closure strategy analysis Closure strategy analysis essentially determines where in the machine to allocate each closure. Unlike previous CPS compilers <ref> [26, 33] </ref>, we do not do any escape analysis 7 because we simply do not use a runtime stack. Our closure strategy analysis only decides how many slots (i.e., registers) each closure is going to use, denoted by S (f ) for each function f . <p> Most of the reduction in heap allocation is from the continuation closures; closure analysis does nothing to reduce the allocation of records and arrays. 7 Comparison with Other Schemes Our work on closure analysis is related to, and influenced by, many other research results. Closure analysis: Steele <ref> [33] </ref> used continuation closures instead of "stack frames;" Rozas [30] and Kranz [25, 26] used closure analysis to choose specialized representations for different kinds of closures; Appel and Jim investigated closure-sharing strategies [3]. We combine all of these analyses (except stack allocation) and more.
Reference: [34] <author> Robert E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Science, </journal> <volume> 9(3) </volume> <pages> 355-365, </pages> <month> December </month> <year> 1974. </year>
Reference-contexts: Cycles in the graph imply loops or recursions (e.g., the path from h to J to Q). The nested hierarchies of loops and recursions in E can be revealed by running the Tarjan interval analysis 6 algorithm <ref> [34, 31] </ref> on G, assuming G is a reducible flow graph. For the purpose of our closure analysis, this control flow information can be used to choose closure representations that allow more efficient variable accesses in frequently-executed program fragments (e.g., loops).
Reference: [35] <author> Mads Tofte and Jean-Pierre Talpin. </author> <title> Implementation of the typed call-by-value -calculus using a stack of regions. </title> <booktitle> In Proc. 21st Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, </booktitle> <pages> pages 188-201. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: A stack of regions: Tofte and Talpin have demonstrated an analysis that can avoid garbage collection entirely <ref> [35] </ref>, but unfortunately it does not satisfy the "safe for space complexity" rule. 8 Conclusions Our new closure conversion algorithm is a great success. The closure conversion algorithm itself is faster than our previous algorithm.
Reference: [36] <author> David M. Ungar. </author> <title> The Design and Evaluation of a High Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: But on many modern machines, the write miss penalty is approximately zero [23, 16, 7]. 3. The amortized cost of collection can be very low [1, 7], especially with modern generational garbage collection techniques <ref> [36] </ref>. The major contribution of our paper is a "safe for space" closure conversion algorithm that integrates and improves most previous closure analysis techniques [26, 6, 33, 30, 20, 22], using a simple and general framework expressed in continuation-passing and closure-passing style [4, 6].
References-found: 36

