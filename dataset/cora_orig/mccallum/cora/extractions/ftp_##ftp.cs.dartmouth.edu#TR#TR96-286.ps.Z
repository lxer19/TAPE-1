URL: ftp://ftp.cs.dartmouth.edu/TR/TR96-286.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR96-286/
Root-URL: http://www.cs.dartmouth.edu
Title: The Galley Parallel File System  
Author: Nils Nieuwejaar David Kotz 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Note: Available at  
Pubnum: PCS-TR96-286  
Email: fnils,dfkg@cs.dartmouth.edu  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR96-286.ps.Z  
Abstract: Most current multiprocessor file systems are designed to use multiple disks in parallel, using the high aggregate bandwidth to meet the growing I/O requirements of parallel scientific applications. Many multiprocessor file systems provide applications with a conventional Unix-like interface, allowing the application to access multiple disks transparently. This interface conceals the parallelism within the file system, increasing the ease of programmability, but making it difficult or impossible for sophisticated programmers and libraries to use knowledge about their I/O needs to exploit that parallelism. In addition to providing an insufficient interface, most current multiprocessor file systems are optimized for a different workload than they are being asked to support. We introduce Galley, a new parallel file system that is intended to efficiently support realistic scientific multiprocessor workloads. We discuss Galley's file structure and application interface, as well as the performance advantages offered by that interface. 
Abstract-found: 1
Intro-found: 1
Reference: [Are91] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: Another instance where this type of file structure may be useful is in the problem of genome-sequence comparison. This problem requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs. <p> Another instance where this type of file structure may be useful is in the problem of genome-sequence comparison. This problem requires searching a large database to find approximate matches between strings <ref> [Are91] </ref>. The raw database used in [Are91] contained thousands of genetic sequences, each of which was composed of hundreds or thousands of bases. To reduce the amount of time required to identify potential matches, the authors constructed an index of the database that was specific to their needs.
Reference: [BBH95] <author> Sandra Johnson Baylor, Caroline B. Benveniste, and Yarson Hsu. </author> <title> Performance evaluation of a parallel I/O architecture. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 404-413, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, multiprocessor file-system workloads frequently include many small requests.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Galley is targeted at distributed memory, MIMD machines such as IBM's SP-2 or Intel's Paragon. 2 3 File Structure Most existing multiprocessor file systems are based on a Unix-like model <ref> [BGST93, Pie89, LIN + 93] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue requests to read or write data contiguous subranges of that sequence of bytes. <p> CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other multiprocessor file systems with this style of interface are SUNMOS and its successor, PUMA [WMR + 94], sfs [LIN + 93], and CMMD <ref> [BGST93] </ref>. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95]. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley [Nie96]. Other libraries currently being implemented provide Panda [SCJ + 95] and Vesta [CFP + 95] interfaces, as well as ViC*, a variant of C* designed for out-of-core computations <ref> [CC94] </ref>. 4.2 I/O Processors Galley's I/O servers, illustrated in Figure 3, are composed of several units, which are described in detail below. Each unit is implemented as a separate thread. Furthermore, each IOP also has one thread designated to handle incoming I/O requests for each compute processor.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: Administrators and advanced 22 on the y-axis. 23 users may also choose to interact with PIOFS's underlying parallel file system, which is based on the Vesta file system <ref> [CF94, CFP + 95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra Johnson Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <pages> pages 222-248, </pages> <year> 1995. </year>
Reference-contexts: One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley [Nie96]. Other libraries currently being implemented provide Panda [SCJ + 95] and Vesta <ref> [CFP + 95] </ref> interfaces, as well as ViC*, a variant of C* designed for out-of-core computations [CC94]. 4.2 I/O Processors Galley's I/O servers, illustrated in Figure 3, are composed of several units, which are described in detail below. Each unit is implemented as a separate thread. <p> This step requires that a single message be sent to the IOP that will be responsible for maintaining the metadata for the new file. The responsible IOP is chosen by applying a simple hash function to the file name. Vesta uses a similar scheme <ref> [CFP + 95] </ref>. The second step is to create subfiles on each of the appropriate IOPs. This step requires that a message be sent to each IOP, asking that a subfile-header block be assigned to the file. <p> Administrators and advanced 22 on the y-axis. 23 users may also choose to interact with PIOFS's underlying parallel file system, which is based on the Vesta file system <ref> [CF94, CFP + 95] </ref>. Files in Vesta are two-dimensional, and are composed of multiple cells, each of which is a sequence of basic striping units. BSUs are essentially records, or fixed-sized sequences of bytes. Like Galley's subfiles, each cell resides on a single disk.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: To avoid the limitations of the linear file model, Galley does not impose a declustering strategy on an application's data. Instead, Galley provides applications with the ability to fully control this declustering according to their own needs. This control is particularly important when implementing I/O-optimal algorithms <ref> [CK93] </ref>. Applications are also able to explicitly indicate which disk they wish to access in each request. To allow this behavior, files are composed of one or more subfiles, which may be directly addressed by the application.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blu-menthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface <ref> [HER + 95] </ref>. In PPFS, however, the basic transfer unit is an application-defined record rather than a byte. PPFS maps requests against the logical, linear stream of records to an underlying two-dimensional model, indexed with a (disk, record) pair.
Reference: [HP91] <institution> Hewlett Packard. </institution> <note> HP97556/58/60 5.25-inch SCSI Disk Drives Technical Reference Manual, second edition, </note> <month> June </month> <year> 1991. </year> <title> HP Part number 5960-0115. </title>
Reference-contexts: To avoid these inflated results, we examined Galley's performance using a simulation of an HP 97560 SCSI hard disk, which has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s <ref> [HP91] </ref>. Our implementation of the disk model was based on earlier implementations [RW94, KTR94] 2 . Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year> <note> http://www.erc.msstate.edu/hpff/report.html. </note>
Reference-contexts: pattern could represent either a one-dimensional partitioning of data or the series of accesses we would expect to see if a two-dimensional matrix were stored on disk in row-major order, and the application distributed the rows of the matrix across the compute nodes in a BLOCK fashion (using HPF terminology <ref> [HPF93] </ref>). There are two different ways a partitioned access pattern at the file level can map onto access patterns at the IOP level.
Reference: [IBM94] <author> IBM. </author> <title> AIX Version 3.2 General Programming Concepts, </title> <booktitle> twelfth edition, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: This interface gives applications the ability to submit multiple simple or strided requests at once. 12 5.3.4 List Requests Finally, in addition to these structured operations, Galley provides a simple, more general file interface, called the list interface, which has functionality similar to the POSIX lio listio () interface <ref> [IBM94] </ref>. This interface allows an application to simply specify an array of (file offset, memory offset, size) triples that it would like transferred between memory and disk. This interface is useful for applications with access patterns that do not have any inherently regular structure.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Several recent analyses of production file-system workloads on multiprocessors running primarily scientific applications show that many of the assumptions that guided the development of most multiprocessor file systems were incorrect <ref> [KN94, NK96a, PEK + 95] </ref>. It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91].
Reference: [KR94] <author> Thomas T. Kwan and Daniel A. Reed. </author> <title> Performance of the CM-5 scalable file system. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 156-165, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, multiprocessor file-system workloads frequently include many small requests.
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: To avoid these inflated results, we examined Galley's performance using a simulation of an HP 97560 SCSI hard disk, which has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s [HP91]. Our implementation of the disk model was based on earlier implementations <ref> [RW94, KTR94] </ref> 2 . Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. <p> Galley is targeted at distributed memory, MIMD machines such as IBM's SP-2 or Intel's Paragon. 2 3 File Structure Most existing multiprocessor file systems are based on a Unix-like model <ref> [BGST93, Pie89, LIN + 93] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue requests to read or write data contiguous subranges of that sequence of bytes. <p> CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other multiprocessor file systems with this style of interface are SUNMOS and its successor, PUMA [WMR + 94], sfs <ref> [LIN + 93] </ref>, and CMMD [BGST93]. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95].
Reference: [MHQ96] <author> Jason A. Moore, Phil Hatcher, and Michael J. Quinn. </author> <title> Efficient data-parallel files via automatic mode detection. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-14, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Under Galley, this index could be stored in one fork, while the database itself could be stored in a second fork. A final example of the use of forks is Stream*, a parallel file abstraction for the data-parallel language, C* <ref> [MHQ96] </ref>. Briefly, Stream* divides a file into three distinct segments, each of which corresponds to a particular set of access semantics. While the current implementation of Stream* stores all the segments in a single file, one could use a different fork for each segment.
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>.
Reference: [Nie96] <author> Nils Nieuwejaar. </author> <title> A linear file model and an implementation of the NHT-1 I/O application benchmark on the Galley Parallel File System. </title> <type> Technical Report In progress, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: Although applications may interact directly with Galley's interface, we expect that most applications will use a higher-level library or language layered on top of the Galley run-time library. One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley <ref> [Nie96] </ref>. <p> The higher-level interfaces offered by Galley are summarized below. These interfaces are described in greater detail, and examples are provided, in <ref> [NK96a, Nie96] </ref>. <p> An example of such an application is given in <ref> [Nie96] </ref>. For those applications, we provide a nested-batched interface.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Galley's DiskManager does not attempt to prefetch data for two reasons. First, indiscriminate prefetching can cause thrashing in the buffer cache <ref> [Nit92] </ref>. Second, prefetching is based on the assump 8 tion that the system can intelligently guess what an application is going to request next. <p> Indeed, most do not even examine the performance of requests of fewer than many kilobytes <ref> [Nit92, BBH95, KR94] </ref>. As discussed above, multiprocessor file-system workloads frequently include many small requests. <p> While many of these were similar to the traditional Unix-style file system, there have been also several more ambitious attempts. Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, and its successor, PFS, are examples of multiprocessor file systems that use a linear file model and provide applications with a Unix-like interface. Both systems provide limited support to parallel applications in the form of file pointers that may be shared by all the processes in the application.
Reference: [NK96a] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 9, </booktitle> <pages> pages 205-223. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Several recent analyses of production file-system workloads on multiprocessors running primarily scientific applications show that many of the assumptions that guided the development of most multiprocessor file systems were incorrect <ref> [KN94, NK96a, PEK + 95] </ref>. It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91]. <p> These primitives are limited to read ()ing and write ()ing consecutive regions of a file. As discussed above, recent studies show that these primitives are not sufficient to meet the needs of many parallel applications <ref> [NK96a, NKP + 95] </ref>. Specifically, parallel scientific applications frequently make many small requests to a file, with strided access patterns. We define two types of strided patterns. <p> A nested-strided access pattern is similar to a simple-strided pattern, but rather than repeating a single request at regular intervals, the application repeats either a simple-strided or nested-strided segment at regular intervals. Studies show that both simple-strided and nested-strided patterns are common in parallel, scientific applications <ref> [NK96a, NKP + 95] </ref>. Galley provides three interfaces that allow applications to explicitly make regular, structured requests such as those described above, as well as one interface for unstructured requests. <p> The higher-level interfaces offered by Galley are summarized below. These interfaces are described in greater detail, and examples are provided, in <ref> [NK96a, Nie96] </ref>.
Reference: [NK96b] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: When testing an earlier version of Galley we found that with large numbers of IOPs, the network congestion at the CPs was so great that the CPs were unable to receive data and issue new requests to the IOPs in a timely fashion <ref> [NK96b] </ref>. As a result, the DiskManagers on the IOPs were unable to make intelligent disk scheduling decisions, causing excess disk-head seeks and thrashing of the on-disk cache.
Reference: [NKP + 95] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: number CCR-9404919 and by NASA Ames under agreement numbers NCC 2-849 and NAG 2-936. running a variety of applications in a variety of scientific domains, on two architectures, under both data-parallel and control-parallel programming models, show that many applications make many small, regular, but non-consecutive requests to the file system <ref> [NKP + 95] </ref>. These studies suggest that the workload that most multiprocessor file systems were optimized for is very different than the workloads they are actually being asked to serve. <p> The drawback of this approach is that most multiprocessor file systems use a declustering unit size measured in kilobytes (e.g., 4 KB in Intel's CFS [Pie89]), but our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes <ref> [NKP + 95] </ref>. This disparity between the request size and the declustering unit size means that most of the individual requests generated by parallel applications are not being executed in parallel. <p> These primitives are limited to read ()ing and write ()ing consecutive regions of a file. As discussed above, recent studies show that these primitives are not sufficient to meet the needs of many parallel applications <ref> [NK96a, NKP + 95] </ref>. Specifically, parallel scientific applications frequently make many small requests to a file, with strided access patterns. We define two types of strided patterns. <p> A nested-strided access pattern is similar to a simple-strided pattern, but rather than repeating a single request at regular intervals, the application repeats either a simple-strided or nested-strided segment at regular intervals. Studies show that both simple-strided and nested-strided patterns are common in parallel, scientific applications <ref> [NK96a, NKP + 95] </ref>. Galley provides three interfaces that allow applications to explicitly make regular, structured requests such as those described above, as well as one interface for unstructured requests.
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Several recent analyses of production file-system workloads on multiprocessors running primarily scientific applications show that many of the assumptions that guided the development of most multiprocessor file systems were incorrect <ref> [KN94, NK96a, PEK + 95] </ref>. It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks [Pie89, PFDJ89, LIN + 93, MK91].
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: It was generally assumed that scientific applications designed to run on a multiprocessor would behave in the same fashion as scientific applications designed to run on sequential and vector supercomputers: accessing large files in large, consecutive chunks <ref> [Pie89, PFDJ89, LIN + 93, MK91] </ref>. <p> Galley is targeted at distributed memory, MIMD machines such as IBM's SP-2 or Intel's Paragon. 2 3 File Structure Most existing multiprocessor file systems are based on a Unix-like model <ref> [BGST93, Pie89, LIN + 93] </ref>. Under this model, a file is seen as an addressable, linear sequence of bytes. Applications can issue requests to read or write data contiguous subranges of that sequence of bytes. <p> The drawback of this approach is that most multiprocessor file systems use a declustering unit size measured in kilobytes (e.g., 4 KB in Intel's CFS <ref> [Pie89] </ref>), but our workload characterization studies show that the typical request size in a parallel application is much smaller: frequently under 200 bytes [NKP + 95]. <p> While many of these were similar to the traditional Unix-style file system, there have been also several more ambitious attempts. Intel's Concurrent File System (CFS) <ref> [Pie89, Nit92] </ref>, and its successor, PFS, are examples of multiprocessor file systems that use a linear file model and provide applications with a Unix-like interface. Both systems provide limited support to parallel applications in the form of file pointers that may be shared by all the processes in the application.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: To avoid these inflated results, we examined Galley's performance using a simulation of an HP 97560 SCSI hard disk, which has an average seek time of 13.5 ms and a maximum sustained throughput of 2.2 MB/s [HP91]. Our implementation of the disk model was based on earlier implementations <ref> [RW94, KTR94] </ref> 2 . Among the factors simulated by our model are head-switch time, track-switch time, SCSI-bus overhead, controller overhead, rotational latency, and the disk cache. <p> To validate our model, we used a trace-driven simulation, using data provided by Hewlett-Packard and used by Ruemmler and Wilkes in their study. 3 Comparing the results of this trace-driven simulation with the measured results from the actual disk, we obtained a demerit figure (see <ref> [RW94] </ref> for a discussion of this measure) of 5.0%, 2 The source code for this disk simulator is available online at http://www.cs.dartmouth.edu/~nils/disk.html. 3 Kindly provided to us by John Wilkes and HP.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: One such library implements a Unix-like file model, which should reduce the effort required to port legacy applications to Galley [Nie96]. Other libraries currently being implemented provide Panda <ref> [SCJ + 95] </ref> and Vesta [CFP + 95] interfaces, as well as ViC*, a variant of C* designed for out-of-core computations [CC94]. 4.2 I/O Processors Galley's I/O servers, illustrated in Figure 3, are composed of several units, which are described in detail below.
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the 1990 Winter USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: The DiskMan-ager maintains a list of blocks that the CacheManager has requested to be read or written. As new requests arrive from the CacheManager, they are placed into the list according to the disk scheduling algorithm. The DiskManager currently uses a Cyclical Scan algorithm <ref> [SCO90] </ref>. When a block has been read from disk, the DiskManager updates the cache status of that block's buffer from `not ready' to `ready', increases its reference count, and adds it to the requesting thread's ready queue. Galley's DiskManager does not attempt to prefetch data for two reasons.
Reference: [SW95] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition to storing data in the traditional sense, many libraries also need to store persistent, library-specific `metadata' independently of the data proper. One example of such a library would be a compression library similar to that described in <ref> [SW95] </ref>, which compresses a data file in multiple independent chunks. Such a library could store the compressed data chunks in one fork and index information in another. 4 portion of the file residing on disk 0 is shown in greater detail than the portions on the other two disks.
Reference: [TG96] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: One such example is the two-dimensional, cyclically-shifted block layout scheme for matrices, shown in Figure 1, which was designed for SOLAR, a portable, out-of-core linear-algebra library <ref> [TG96] </ref>. This data layout is intended to efficiently support a wide variety of out-of-core algorithms. In particular, it allows blocks of rows and columns to be transferred efficiently, as well as square or nearly-square submatrices. <p> To allow this behavior, files are composed of one or more subfiles, which may be directly addressed by the application. Each subfile resides entirely on a single disk, and no disk contains more 3 described in <ref> [TG96] </ref>. In this example there are 6 disks, logically arranged into a 2-by-3 grid, and a 6-by-12 block matrix. The number in each square indicates the disk on which that block is stored. than one subfile from any file.
Reference: [WMR + 94] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 56-65, </pages> <year> 1994. </year>
Reference-contexts: CFS and PFS provide several modes, each of which provides the applications with a different set of semantics governing how the file pointers are shared. Other multiprocessor file systems with this style of interface are SUNMOS and its successor, PUMA <ref> [WMR + 94] </ref>, sfs [LIN + 93], and CMMD [BGST93]. Like the systems mentioned above, PPFS provides the end user with a linear file that is accessed with primitives that are similar to the traditional read ()/write () interface [HER + 95].
References-found: 31

