URL: http://www.isi.edu/~moriarty/papers/moriarty.obstacle.ps.gz
Refering-URL: http://www.isi.edu/~moriarty/mypapers.html
Root-URL: http://www.isi.edu
Email: moriarty,risto@cs.utexas.edu  
Title: Evolving Obstacle Avoidance Behavior in a Robot Arm  
Author: David E. Moriarty and Risto Miikkulainen 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Existing approaches for learning to control a robot arm rely on supervised methods where correct behavior is explicitly given. It is difficult to learn to avoid obstacles using such methods, however, because examples of obstacle avoidance behavior are hard to generate. This paper presents an alternative approach that evolves neural network controllers through genetic algorithms. No input/output examples are necessary, since neuro-evolution learns from a single performance measurement over the entire task of grasping an object. The approach is tested in a simulation of the OSCAR-6 robot arm which receives both visual and sensory input. Neural networks evolved to effectively avoid obstacles at various locations to reach random target locations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baker, W. L., and Farrell, J. A. </author> <year> (1992). </year> <title> An introduction to connectionist learning control systems. </title> <booktitle> In Handbook of Intelligent Control, </booktitle> <pages> 35-63. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Cliff, D., Harvey, I., and Husbands, P. </author> <year> (1993). </year> <title> Explorations in evolutionary robotics. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2 </volume> <pages> 73-110. </pages>
Reference: <author> Feddema, J. T., and Lee, G. C. S. </author> <year> (1990). </year> <title> Adaptive image feature prediction and control for visual tracking with a hand-eye coordinated camera. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 20(5). </volume>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: It is unclear how such examples could be generated without a path-planning al gorithm (Lumelsky 1987), which requires significant domain knowledge of the robot and its environment. An alternative to supervised learning is to use a reinforcement learning method, such as Q-learning (Watkins and Dayan 1992) or genetic algorithms <ref> (Goldberg 1989) </ref>, to form the control policy. In reinforcement learning, no input/output examples are required, and thus no path planning algorithms are necessary to generate intermediate arm positions.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: <author> Kawato, M. </author> <year> (1990). </year> <title> Computational schemes and neural network models for formation and control of multi-joint arm trajectory. In Neural Networks for Control. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kuperstein, M. </author> <year> (1991). </year> <title> INFANT neural controller for adaptive sensory-motor coordination. Neural Networks, </title> <type> 4(2). </type>
Reference: <author> Lumelsky, V. J. </author> <year> (1987). </year> <title> Algorithmic and complexity issues of robot motion in an uncertain environment. </title> <journal> Journal of Complexity, </journal> <volume> 3 </volume> <pages> 146-182. </pages>
Reference-contexts: In domains with obstacles, this is not always possible because the arm must move around an obstacle. Explicitly generating the necessary intermediate joint positions is extremely difficult and requires significant domain knowledge <ref> (Lumelsky 1987) </ref>. Without such knowledge, it is not possible to learn obstacle avoidance behaviors through supervised methods. This paper presents an alternative learning control system that evolves neural networks through genetic algorithms. Neuro-evolution does not require explicit training examples and learns multiple joint rotations implicitly through evolution. <p> To produce such behavior using a supervised learning approach, training examples must demonstrate movement to intermediate arm positions (e.g. above the block). It is unclear how such examples could be generated without a path-planning al gorithm <ref> (Lumelsky 1987) </ref>, which requires significant domain knowledge of the robot and its environment. An alternative to supervised learning is to use a reinforcement learning method, such as Q-learning (Watkins and Dayan 1992) or genetic algorithms (Goldberg 1989), to form the control policy.
Reference: <author> Miller, W. T. </author> <year> (1989). </year> <title> Real-time application of neural networks for sensor-based control of robots with vision. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 825-831. </pages>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32. </pages>
Reference-contexts: Neuro-evolution can thus form network controllers that can adapt in uncertain environments. Evolving neuro-control was tested in a sophisticated robot arm simulation of the OSCAR-6 anthromoporphic robot. The evolution was based on the Hierarchical SANE system <ref> (Moriarty and Miikkulainen 1996a, 1996b) </ref>. Networks were evolved to maneuver the arm to random target locations while avoiding obstacles. Given both camera-based visual and infrared sensory input, the networks learned to effectively combine both target reaching and obstacle avoidance strategies. <p> Symbiotic evolution has been evaluated in several tasks including the standard pole-balancing benchmark where it outperformed existing neuro-evolution and reinforcement learning approaches <ref> (Moriarty and Miikkulainen 1996a) </ref>. In contrast to standard neuro-evolution algorithms that evolve a population of neural networks, in SANE two separate populations are evolved: a population of neurons and a population of network blueprints.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996b). </year> <title> Hierarchical evolution of neural networks. </title> <type> Technical Report AI96-242, </type> <institution> Department of Computer Science, The University of Texas at Austin. </institution>
Reference-contexts: Each neuron receives a fitness according to how well the top five networks in which it participates perform in the task. A very aggressive genetic selection and recombination strategy is used to quickly build new structures in both the neuron and blueprint populations (see <ref> (Moriarty and Miikkulainen 1996b) </ref> for details). SANE offers two important advantages over other neuro-evolution approaches. First, it maintains diverse populations. Because several different types of neurons are necessary to build an effective neural network, there is inherent evolutionary pressure to form neurons that perform different functions. <p> Instead of searching for complete networks all at once, solutions to smaller problems (good neurons) are evolved, which can be combined to form an effective full solution (a network). 1 For the remainder of the paper, the name SANE will refer to the hierarchical version described in <ref> (Moriarty and Miikkulainen 1996b) </ref> The numbers indicate the joints which are to be controlled. 5 Evaluation 5.1 Experimental Setup The Simderella 2.0 package written by van der Smagt (1994) was used as the robot arm simulator in these experiments.
Reference: <author> Nolfi, S., Floreano, D., Miglino, O., and Mondada, F. </author> <year> (1994). </year> <title> How to evolve autonomous robots: </title> <booktitle> Differ ent approaches in evolutionary robotics. In Artificial Life IV. </booktitle> <address> Cambridge, MA. </address>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1995). </year> <title> Learning to adapt to changing environments in evolving neural networks. </title> <type> Technical Report 95-15, </type> <institution> Department of Neural Systems and Artificial Life, Institute of Psychology, CNR - Rome. </institution>
Reference: <author> Papanikolopoulos, N. P., and Khosla, P. K. </author> <year> (1993). </year> <title> Adaptive robotic visual tracking: Theory and experiments. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 38(3) </volume> <pages> 429-444. </pages>
Reference: <author> Sanderson, A. C., and Weiss, L. E. </author> <year> (1983). </year> <title> Adaptive visual servo control of robots. </title> <editor> In Pugh, A., editor, </editor> <title> Robot Vision, </title> <address> 107-116. New York: </address> <publisher> Springer-Verlag. </publisher> <editor> van der Smagt, P. </editor> <year> (1994). </year> <title> Simderella: A robot simulator for neuro-controller design. </title> <journal> Neurocomputing, </journal> <volume> 6(2). </volume> <editor> van der Smagt, P. </editor> <year> (1995). </year> <title> Visual Robot Arm Guidance using Neural Networks. </title> <type> PhD thesis, </type> <institution> The University of Amsterdam, </institution> <address> Amsterdam, The Netherlands. </address>
Reference: <author> Walter, J. A., Martinez, T. M., and Schulten, K. J. </author> <year> (1991). </year> <title> Industrial robot learns visuo-motor coordination by means of neural-gas network. </title> <editor> In Kohonen, T., editor, </editor> <booktitle> Artfificial Neural Networks, </booktitle> <volume> vol. 1. </volume> <month> Ams-terdam. </month>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference-contexts: It is unclear how such examples could be generated without a path-planning al gorithm (Lumelsky 1987), which requires significant domain knowledge of the robot and its environment. An alternative to supervised learning is to use a reinforcement learning method, such as Q-learning <ref> (Watkins and Dayan 1992) </ref> or genetic algorithms (Goldberg 1989), to form the control policy. In reinforcement learning, no input/output examples are required, and thus no path planning algorithms are necessary to generate intermediate arm positions.
Reference: <author> Weiss, L. E., Sanderson, A. C., and Neumann, C. P. </author> <year> (1987). </year> <title> Dynamic sensor-based control of robots with visual feedback. </title> <journal> Journal of Robotics and Automation, RA-3. </journal>
Reference: <author> Werbos, P. J. </author> <year> (1992). </year> <title> Neurocontrol and supervised learning: An overview and evaluation. </title> <booktitle> In Handbook of Intelligent Control, </booktitle> <pages> 65-89. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: Supervised learning, however, requires training examples that demonstrate correct mappings from input to output. Training examples are normally generated by flailing the arm while recording joint movements and final arm positions <ref> (Werbos 1992) </ref>. The supervised approach is sufficient for learning basic hand-eye coordination in domains with unrestricted movement, however, in uncertain or obstacle-filled domains the supervised approach fails. The main problem is that in the supervised training the arm is always moved to the target location in a single joint rotation. <p> In any supervised learning application, it is crucial that the training corpus contains a good representative sample of the desired behavior. The most common approach for generating training examples is to flail the arm and record the resulting joint and hand positions <ref> (Werbos 1992) </ref>. For example, if the joints are initially in position ~ J and a random rotation ~ R results in hand position ~ H, a training example of the form Input : ~ J ; ~ H; Output : ~ R can be constructed. <p> In addition, much of our work will continue to focus on methods for improving the general efficiency of a neuro-evolution search. The neural controller described in this paper is a fixed adaptive controller <ref> (Werbos 1992) </ref>; once the controller is evolved it does not change. However, it would be desirable to build a controller that can adapt online to take advantage of domain specific information.
Reference: <author> Whitley, D., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neuro-control problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. </pages>
Reference: <author> Wijesoma, S. W., Wolfe, D. F. H., and Richards, R. J. </author> <year> (1993). </year> <title> Eye-to-hand coordination for vision-guided robot control applications. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 12(1) </volume> <pages> 65-78. </pages>
Reference: <author> Yamauchi, B. M., and Beer, R. D. </author> <year> (1993). </year> <title> Sequential behavior and learning in evolved dynamical neural networks. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2 </volume> <pages> 219-246. </pages>
References-found: 22

