URL: http://www.cs.brown.edu/research/ai/dynamics/tutorial/Postscript/SingularValueDecomposition.ps
Refering-URL: http://www.cs.brown.edu/research/ai/dynamics/tutorial/Documents/Syllabus.html
Root-URL: http://www.cs.brown.edu/
Title: Singular Value Decomposition A Primer  
Author: Sonia Leach 
Note: DRAFT VERSION  
Address: Providence, RI 02912  
Affiliation: Department of Computer Science Brown University  
Abstract-found: 0
Intro-found: 1
Reference: [And86] <author> Fobert F. V. Anderson. </author> <title> Introduction to Linear Algebra. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Section 4 lists a number of interesting applications and Section 5 concludes the paper with a discussion of the advantages and disadvantages of using the SVD. 2 Definition of the SVD In this section, we assume a familiarity with the basic terminology of linear algebra, and refer the reader to <ref> [And86] </ref> for a more complete coverage. We restrict our attention to matrices of real numbers and refer the reader to [DD88] for a discussion of the SVD using complex numbers. This presentation is largely adapted from [FMM77].
Reference: [DD88] <editor> P. Dewilde and Ed. F. Deprettere. </editor> <title> Singular value decomposition: An introduction. </title> <editor> In Ed. F. Deprettere, editor, </editor> <booktitle> SVD and Signal Processing: Algorithms, Applications, and Architectures, </booktitle> <pages> pages 3-41. </pages> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1988. </year>
Reference-contexts: We restrict our attention to matrices of real numbers and refer the reader to <ref> [DD88] </ref> for a discussion of the SVD using complex numbers. This presentation is largely adapted from [FMM77]. <p> The Frobenius norm is then k A k F = @ i=1 j=1 1 1 : Given the SVD of a matrix A, these norms can easily be computed. Proofs of the following facts are given in <ref> [GVL83, DD88] </ref>. Let U V T be the SVD of M fi N matrix A, where fs 1 ; s 2 ; :::; s k g; k N are the non-zero singular values in . <p> The solution B = A k will be unique when s k+1 &lt; s k . Proofs of these facts can be found in <ref> [DD88, GVL83] </ref>. We see then that the SVD of A produces a sequence of approximations to A of successive ranks A i = U i V T , where i is the rank i version of obtained by setting the last m i singular values to zero.
Reference: [Dep88] <editor> Ed. F. Deprettere, editor. </editor> <title> SVD and Signal Processing: Algorithms, Analysis, and Applications. </title> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1988. </year>
Reference-contexts: Many fundamental aspects of linear algebra rely on determining the rank of a matrix, making the SVD an important and widely-used technique. This primer serves as a short introduction to the SVD and its applications. More comprehensive coverage can be found in numerous references, such as <ref> [GVL83, Dep88, Vac91] </ref>. Organization of the paper is as follows. Section 2 introduces the definition of the SVD, followed by a discussion of the properties of the components of the SVD. Section 3 explores further properties of the SVD and provides a geometric interpretation of the singular values. <p> A host of active research efforts address these problems. Further examples of the use of SVD in the field of Signal Processing, as well as discussions of implementation algorithms and architectures, can be found in <ref> [Vac91, Dep88] </ref>.
Reference: [FMM77] <author> George E. Forsythe, Michael A. Malcolm, and Cleve B. Moler. </author> <booktitle> Computer Methods for Mathematical Computations, </booktitle> <pages> pages 201-235. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1977. </year>
Reference-contexts: We restrict our attention to matrices of real numbers and refer the reader to [DD88] for a discussion of the SVD using complex numbers. This presentation is largely adapted from <ref> [FMM77] </ref>. Using the superscript T to denote the transpose of a vector or matrix, we say two vectors x and y are orthogonal if x T y = 0: In two or three dimensional space, this simply means that the vectors are perpendicular. <p> Geometrically, this set defines a k-dimensional ellipsoid embedded in an M -dimensional space, where k is the number of nonzero singular values. Figure 1 depicts the situation when M = N = k = 2 <ref> [FMM77] </ref>. The lengths of the axes of the ellipsoid are the singular values of A, and in the general case, the major and minor axes are given by s max and s min respectively. <p> Additionally, multiplication of u i v T i with a vector x requires only M + N operations, instead of M fl N <ref> [FMM77] </ref> 3.3 SVD and Linear Independence Another use of the SVD provides a measure, called a condition number, which is related to the measure of linear independence between the column vectors of the matrix. <p> The solution to the original problem is then x = V z <ref> [FMM77] </ref>. 4.2 Noisy Signal Filtering Problems in signal processing often use linear models for signals. In ideal (noise-free) conditions, the measurment data can be arranged in a matrix, where the matrix is known to be rank-deficient.
Reference: [GVL83] <author> Gene H. Golub and Charles F. Van Loan. </author> <booktitle> Matrix Computations, </booktitle> <pages> pages 16-21, 293. </pages> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1983. </year>
Reference-contexts: Many fundamental aspects of linear algebra rely on determining the rank of a matrix, making the SVD an important and widely-used technique. This primer serves as a short introduction to the SVD and its applications. More comprehensive coverage can be found in numerous references, such as <ref> [GVL83, Dep88, Vac91] </ref>. Organization of the paper is as follows. Section 2 introduces the definition of the SVD, followed by a discussion of the properties of the components of the SVD. Section 3 explores further properties of the SVD and provides a geometric interpretation of the singular values. <p> Furthermore, it can be shown that there exist (non-unique) matrices U and V such that s 1 s 2 s N 0 <ref> [GVL83] </ref>. Henceforth we will assume the SVD has such a property. The quantities s i are called the singular values of A, and the columns of U and V are called the left and right singular vectors, respectively. <p> The Frobenius norm is then k A k F = @ i=1 j=1 1 1 : Given the SVD of a matrix A, these norms can easily be computed. Proofs of the following facts are given in <ref> [GVL83, DD88] </ref>. Let U V T be the SVD of M fi N matrix A, where fs 1 ; s 2 ; :::; s k g; k N are the non-zero singular values in . <p> The solution B = A k will be unique when s k+1 &lt; s k . Proofs of these facts can be found in <ref> [DD88, GVL83] </ref>. We see then that the SVD of A produces a sequence of approximations to A of successive ranks A i = U i V T , where i is the rank i version of obtained by setting the last m i singular values to zero.
Reference: [Sau94] <author> Tim Sauer. </author> <title> Time series prediction by using delayed coordinate embedding. </title> <editor> In Andreas S. Weigend and Neil A. Gershenfeld, editors, </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <address> Adgdison-Wesley, </address> <year> 1994. </year>
Reference-contexts: decomposed into the orthogonal components U k V T , which is the rank k subspace corresponding to the signal subspace, and U nk V T , which corresponds to the orthogonal subspace defining the noise components [Sch91]. 4.3 Time Series Analysis The technique of delay coordinate embedding, used by <ref> [Sau94] </ref> for time series analysis, also uses the SVD. The algorithm constructs a multidimensional model of the data from a sequence of one dimensional observations. An M dimensional vector is constructed by sliding a window of length M over consecutive observations in the data sequence.
Reference: [Sch91] <author> Louis L. Scharf. </author> <title> The SVD and reduced-rank signal processing. </title> <editor> In R. Vaccaro, editor, </editor> <booktitle> SVD and Signal Processing II: Algorithms, Applications, and Architectures, </booktitle> <pages> pages 3-31. </pages> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: the SVD as above, we see that the original data matrix A is decomposed into the orthogonal components U k V T , which is the rank k subspace corresponding to the signal subspace, and U nk V T , which corresponds to the orthogonal subspace defining the noise components <ref> [Sch91] </ref>. 4.3 Time Series Analysis The technique of delay coordinate embedding, used by [Sau94] for time series analysis, also uses the SVD. The algorithm constructs a multidimensional model of the data from a sequence of one dimensional observations.
Reference: [Vac91] <author> R. Vaccaro, </author> <title> editor. SVD and Signal Processing II: Algorithms, Analysis, and Applications. </title> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: Many fundamental aspects of linear algebra rely on determining the rank of a matrix, making the SVD an important and widely-used technique. This primer serves as a short introduction to the SVD and its applications. More comprehensive coverage can be found in numerous references, such as <ref> [GVL83, Dep88, Vac91] </ref>. Organization of the paper is as follows. Section 2 introduces the definition of the SVD, followed by a discussion of the properties of the components of the SVD. Section 3 explores further properties of the SVD and provides a geometric interpretation of the singular values. <p> Despite its usefulness, however, there are a number of drawbacks, as mentioned by <ref> [Vac91] </ref>. For problems that can be solved by simpler techniques, such as the Fourier Transform, or QR decomposition, use of the SVD may be unduly expensive computationally. <p> A host of active research efforts address these problems. Further examples of the use of SVD in the field of Signal Processing, as well as discussions of implementation algorithms and architectures, can be found in <ref> [Vac91, Dep88] </ref>.
Reference: [VDM88] <author> Joos Vandewalle and Bart De Moor. </author> <title> A variety of applications of singular value decomposition in identification and signal processing. </title> <editor> In Ed. F. Deprettere, editor, </editor> <booktitle> SVD and Signal Processing: Algorithms, Applications, and Architectures, </booktitle> <pages> pages 43-91. </pages> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1988. </year>
Reference-contexts: Suppose we want to measure the maximal increase in relative inaccuracy for the worst position of b and error db, when solving for x in the system Ax = b. The answer is precisely the condition number <ref> [VDM88] </ref>. E = cond (A) = max b;db k dy k = k y k s max For the above reason, matrices with large condition numbers are said to be ill-conditioned.
Reference: [WG94] <author> Andreas S. Weigend and Neil A. Gershenfeld. </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1994. </year>
References-found: 10

