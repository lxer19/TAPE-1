URL: http://elysium.cs.ucdavis.edu/~nico/seminar/papers/icdcs98.ps.gz
Refering-URL: http://elysium.cs.ucdavis.edu/~nico/seminar/seminar.html
Root-URL: http://www.cs.ucdavis.edu
Phone: Telephone:  
Address: 445 Hoes Lane P.O. Box 1331 Piscataway, NJ 08855-1331, USA.  
Affiliation: Service Center  
Note: Copyright from the IEEE. Contact: Manager, Copyrights and Permissions IEEE  +Intl. 908-562-3966.  
Abstract: c fl 1998 IEEE. Published in the Proceedings of ICDCS'98, May 1998 Amsterdam, The Nether-lands. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Greg Burns, Raja Daoud, and James Vaigl. LAM: </author> <title> An open cluster environment for MPI. </title> <institution> Ohio Supercomputing Center. Available at ftp://ftp.osc.edu/pub/lam, </institution> <year> 1994. </year>
Reference-contexts: Optimizing Communication Usually, MPI processes communicate over TCP/IP connections. But if some processes are located on the the same machine, we use shared memory for communication. In the Quarterware derived MPI, multiple communication media coexist at the same time, unlike in MPICH [5] and LAM <ref> [1] </ref>. Discussion We implemented these optimizations by customizing only three components in our implementation with just 490 lines of C++ code. The resulting performance is described in Sec. 4.4. <p> The resulting performance is described in Sec. 4.4. Layered structure and dependences among memory, communication, and process management make it difficult to implement such optimizations in conventional implementations like MPICH [5] or LAM <ref> [1] </ref>. Neither of these substrates implement the optimizations that we have discussed. 4 Performance Measurements In this section, we discuss the performance of mid-dleware implemented using our framework and show that the performance of Quarterware based middleware equals or exceeds the corresponding native implementations.
Reference: [2] <author> Roy Campbell, Nayeem Islam, Peter Madany, and David Raila. </author> <title> Experiences designing and implementing Choices: an object-oriented system in C++. </title> <journal> Communications of the ACM, </journal> <volume> 36(9) </volume> <pages> 117-126, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Components are readily combined using established software engineering practices, such as reducing architectural mismatch [3], ensuring cohesion, and reducing coupling [12]. Here, we concentrate on performance. Research has shown that good system performance requires end-to-end, system and subsystem optimizations in addition to optimizing independent components <ref> [2, 14] </ref>. Quarterware components follow the design principles described in [15] to enable end-to-end optimizations. Two representative examples are discussed below. Optimizing Memory Management The goals of memory management optimizations include reducing copying, reducing total memory usage, reducing dynamic allocations, and caching.
Reference: [3] <author> David Garlan, Robert Allen, and John Ockerbloom. </author> <title> Architectural mismatch or, why it's hard to build systems out of existing parts. </title> <booktitle> In Proceedings of the 17th International Conference on Software Engineering (ICSE-17), </booktitle> <pages> pages 179-185, </pages> <year> 1995. </year>
Reference-contexts: The Receiver and Invoker components implement the protocols and multiplexing. 2.2 Components and Optimizations Two primary issues arise in composing components: ease of combination and performance. Components are readily combined using established software engineering practices, such as reducing architectural mismatch <ref> [3] </ref>, ensuring cohesion, and reducing coupling [12]. Here, we concentrate on performance. Research has shown that good system performance requires end-to-end, system and subsystem optimizations in addition to optimizing independent components [2, 14]. Quarterware components follow the design principles described in [15] to enable end-to-end optimizations. <p> Other related work includes research on composable systems <ref> [3] </ref> and design patterns for distributed programming [22]. 6 Conclusions Middleware builders aim to build efficient implementations of features common to many applications. But no particular set of features can suffice for all applications.
Reference: [4] <author> Aniruddha Gokhale and Douglas C. Schmidt. </author> <title> Evaluating CORBA latency and scalability over high-speed networks. </title> <booktitle> In Proceedings of the 17th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The reason for high 2-way latency is inefficient demultiplexing and connection management in Orbix. Schmidt et. al. reach similar conclusions in a detailed performance study of Orbix and Visibroker <ref> [4] </ref>. 4.2.2 Bandwidth Intent This experiment shows that the memory management optimizations discussed in Sec. 2.2 reduce copying and dynamic memory allocations.
Reference: [5] <author> William Gropp and Ewing Lusk. </author> <title> User's guide for mpich, a portable implementation of MPI. </title> <type> Technical Report ANL-96/6, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Optimizing Communication Usually, MPI processes communicate over TCP/IP connections. But if some processes are located on the the same machine, we use shared memory for communication. In the Quarterware derived MPI, multiple communication media coexist at the same time, unlike in MPICH <ref> [5] </ref> and LAM [1]. Discussion We implemented these optimizations by customizing only three components in our implementation with just 490 lines of C++ code. The resulting performance is described in Sec. 4.4. <p> The resulting performance is described in Sec. 4.4. Layered structure and dependences among memory, communication, and process management make it difficult to implement such optimizations in conventional implementations like MPICH <ref> [5] </ref> or LAM [1]. Neither of these substrates implement the optimizations that we have discussed. 4 Performance Measurements In this section, we discuss the performance of mid-dleware implemented using our framework and show that the performance of Quarterware based middleware equals or exceeds the corresponding native implementations. <p> Recent CORBA specification (Feb. 1998) also provides for interceptors. Suggestions for customization have appeared in the literature on distributed object systems. Various systems customize argument passing, method dispatch, object allocation, and object placement. MPICH <ref> [5] </ref> implementors also argue for isolating the transport functionality in the form of abstract devices. These suggestions are applicable to individual features. In practice, they are needed together. Our design not only allows them to be used together, but (a) Ethernet (b) ATM also provides a framework for end-to-end optimizations.
Reference: [6] <author> Graham Hamilton, Michael L. Powell, and James G. Mitchell. Subcontract: </author> <title> a flexible base for distributed programming. </title> <booktitle> Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <volume> 27(5) </volume> <pages> 69-79, </pages> <year> 1993. </year>
Reference-contexts: The OORPC system [25], a customizable RPC implementation, also shares some of our ideas and demonstrates the benefits of RPC customization. We have also used Quarterware based implementations for CORBA extensions for real-time and fault-tolerance [19], and object group management for SUN Netra Proxy servers [18]. The Spring system <ref> [6] </ref> encapsulates object invocation and object reference marshaling policies in sub 13 This corresponds to a vector of MPI typemap f (T, 0), (MPI UB, 8192)g where T is a vector of 8191 bytes. contracts.
Reference: [7] <author> N. C. Hutchinson and Larry L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: As a result, we can easily assemble many different middleware with performance that exceeds the performance of corresponding native implementations. The work presented here was also inspired by the pioneering work on decomposition of networking protocols in the x-kernel <ref> [7] </ref>, customization research in the operating system community including our work on Choices operating system, the SPIN operating system from University of Washington, and Exokernel from MIT.
Reference: [8] <author> Nayeem Islam. </author> <title> Customized Message Passing and Scheduling for Parallel and Distributed Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <month> May </month> <year> 1994. </year> <month> UIUCDCS-R-94-1856. </month>
Reference-contexts: Third, the architecture facilitates customization for performance based on end-to-end optimizations. The need for customizable systems has been well documented in the literature <ref> [8] </ref>. Several research projects have shown that one may need to customize various aspects of middleware for flexbility or performance. (See Sec. 5 for a complete discussion of previous work.) Recognizing this, several vendors provide extra customization features to standard middleware implementations. <p> Each component implements one of the variations. In this Section, we discuss the derivation of components and briefly describe how these components are designed to enable systemwide, end-to-end optimizations. 2.1 Deriving Components Experience suggests the following classification of middleware features <ref> [8, 16, 9] </ref>. * Data Marshalling and Unmarshaling: Data marshaling policies depend on the kind of data being marshaled. System requirements also influence the marshaling policy, e.g., heterogeneity necessitates a common data format.
Reference: [9] <author> Eric Jul. </author> <title> Separation of distribution and objects. </title> <booktitle> In Proceedings of the ECOOP'93 Workshop on Object-Based Distributed Programming, volume 791 of Lecture Notes in Computer Science, </booktitle> <pages> pages 47-54. </pages> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Each component implements one of the variations. In this Section, we discuss the derivation of components and briefly describe how these components are designed to enable systemwide, end-to-end optimizations. 2.1 Deriving Components Experience suggests the following classification of middleware features <ref> [8, 16, 9] </ref>. * Data Marshalling and Unmarshaling: Data marshaling policies depend on the kind of data being marshaled. System requirements also influence the marshaling policy, e.g., heterogeneity necessitates a common data format.
Reference: [10] <author> MPI Forum. </author> <title> The Message Passing Standard, </title> <month> June </month> <year> 1995. </year> <note> version 1.1. </note>
Reference-contexts: Examples of derived implementations include basic CORBA [11], MPI <ref> [10] </ref>, and RMI [21]. Performance results from Sec. 4 show that the Quarterware-derived middleware perform as well or better than the corresponding native middleware implementations. <p> First, we derive the core features of CORBA [11] as an example of basic Quarterware facilities (Sec. 3.1). In Sec. 3.2, we specialize the CORBA implementation in a different direction, we modify some components and re-use the rest to derive an RMI [21] implementation. Our last example, MPI <ref> [10] </ref> (Sec. 3.3) explores another dimension of communication middleware. MPI follows a message passing model, as opposed to the RPC model followed by CORBA and RMI. It targets data and compute intensive parallel applications that require low latency and high bandwidth. <p> As discussed above, we can easily accommodate this change. However, the RMI implementation described in [24] combines transport, protocol and dispatching in the same layer and changing that implementation would require more effort. 3.3 Composing MPI MPI <ref> [10] </ref>, the Message Passing Interface is a standard library for message passing programs. The library provides facilities for process communication, data description, and process grouping and naming. For the purposes of this paper, we are interested in the implementation of MPI process communication. <p> So we specialize the Invoker to use the scatter-gather I/O facilities to avoid copying. Receiver-side Optimizations In a generic remote method call, the system allocates memory for the data transmitted during the call. But in MPI, receiver processes are required to allocate memory for incoming 4 MPI Specification <ref> [10] </ref>, Sec. 3.7.2. 5 MPI Specification [10], Sec. 3.12. data. We customize the Receiver to read incoming data directly into the application buffers by delaying actual reading of data until its final destination has been determined (direct demultiplexing). Recall that we used the same technique for RMI (Sec. 3.2). <p> Receiver-side Optimizations In a generic remote method call, the system allocates memory for the data transmitted during the call. But in MPI, receiver processes are required to allocate memory for incoming 4 MPI Specification <ref> [10] </ref>, Sec. 3.7.2. 5 MPI Specification [10], Sec. 3.12. data. We customize the Receiver to read incoming data directly into the application buffers by delaying actual reading of data until its final destination has been determined (direct demultiplexing). Recall that we used the same technique for RMI (Sec. 3.2).
Reference: [11] <author> Object Management Group. </author> <title> The Common Object Request Broker: Architecture and Specification, </title> <month> August </month> <year> 1996. </year> <note> Document PTC/96-08-04, Revision 2.0. </note>
Reference-contexts: Examples of derived implementations include basic CORBA <ref> [11] </ref>, MPI [10], and RMI [21]. Performance results from Sec. 4 show that the Quarterware-derived middleware perform as well or better than the corresponding native middleware implementations. <p> We optimize communication using specialized Invoker and Receiver together to decide the connection management policy, and to choose the appropriate Transport. 3 Composing Middleware Using Quarterware In this section, we show how we derive different middleware using the Quarterware components. First, we derive the core features of CORBA <ref> [11] </ref> as an example of basic Quarterware facilities (Sec. 3.1). In Sec. 3.2, we specialize the CORBA implementation in a different direction, we modify some components and re-use the rest to derive an RMI [21] implementation. Our last example, MPI [10] (Sec. 3.3) explores another dimension of communication middleware. <p> We show how (a) Client Side (b) Server Side we use MPI semantics to optimize MPI for such applications, resulting in better performance than native, optimized MPI implementations. 3.1 Composing CORBA The Common Object Request Broker Architecture (CORBA <ref> [11] </ref>) is a standard for distributed object computing. It defines the Interface Definition Language (IDL), its mapping to different programming languages, a marshaling format and a wire protocol called IIOP 1 , and semantics of remote invocations. <p> We allow the clients and the servers to dynamically negotiate applicable optimizations. In our approach, multiple optimizations co-exist in the server for different clients. The need for customizing middleware has been recognized in commercial products and middleware research. The CORBA standard <ref> [11] </ref> provides for limited customization: Server objects interact with the ORBs through Object Adaptors (OAs). An OA makes services such as databases appear to be objects. ORB implementations go beyond this and provide their own customization features.
Reference: [12] <author> Roger S. Pressman. </author> <title> Software Engineering: A Practitioner's Approach. </title> <address> Mc Graw Hill, </address> <note> 3rd edition edition, </note> <year> 1992. </year>
Reference-contexts: The Receiver and Invoker components implement the protocols and multiplexing. 2.2 Components and Optimizations Two primary issues arise in composing components: ease of combination and performance. Components are readily combined using established software engineering practices, such as reducing architectural mismatch [3], ensuring cohesion, and reducing coupling <ref> [12] </ref>. Here, we concentrate on performance. Research has shown that good system performance requires end-to-end, system and subsystem optimizations in addition to optimizing independent components [2, 14]. Quarterware components follow the design principles described in [15] to enable end-to-end optimizations. Two representative examples are discussed below.
Reference: [13] <author> Roger Riggs, Jim Waldo, Ann Wollrath, and Kr-ishna Bharat. </author> <title> Pickling state in the java system. </title> <journal> Computing Systems, </journal> <volume> 9(4) </volume> <pages> 313-329, </pages> <address> 1996. </address> <publisher> Usenix. </publisher>
Reference-contexts: The C++ version is useful where a Java implementation would be either too slow or infeasible, e.g., in network appliances like routers and switches. The principal characteristics of RMI relevant to our implementation are as follows: * RMI has its own wire protocol. * RMI uses Java Object Serialization <ref> [20, 13] </ref> for marshaling arguments and results. As described in Sec. 2.1, Invoker, Receiver and Marshaler components implement these features in Quarterware. Accordingly we customize them to implement the RMI specific variations. Wire Protocol The Invoker Receiver pair implements the wire protocol (Sec. 2.1). <p> Making objects serializable carries an implicit agreement that the object state can be exposed and manipulated <ref> [13] </ref>. Discussion Our components based design permits features and optimizations beyond the standard RMI. For example, we support multicast groups by using the multicast Transport that we already have. The standard RMI implementation (from JDK1.1.4) would require extensive modifications to support multicast.
Reference: [14] <author> J. H. Saltzer, D. P. Reed, and D. D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 277-288, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Components are readily combined using established software engineering practices, such as reducing architectural mismatch [3], ensuring cohesion, and reducing coupling [12]. Here, we concentrate on performance. Research has shown that good system performance requires end-to-end, system and subsystem optimizations in addition to optimizing independent components <ref> [2, 14] </ref>. Quarterware components follow the design principles described in [15] to enable end-to-end optimizations. Two representative examples are discussed below. Optimizing Memory Management The goals of memory management optimizations include reducing copying, reducing total memory usage, reducing dynamic allocations, and caching.
Reference: [15] <author> Aamod Sane, Ashish Singhai, and Roy Campbell. </author> <title> End-to-end considerations in framework design. </title> <booktitle> In Proceedings of the 12th European Conference on Object-Oriented Programming (ECOOP), Lecture Notes in Computer Science, page (to appear). </booktitle> <publisher> Springer Verlag, </publisher> <month> July </month> <year> 1998. </year>
Reference-contexts: Consider for example, provision of interceptors, smart stubs, and smart agents in commercial CORBA ORBs. Compared with these efforts, our work is distinguished by its component approach: the Quarterware architecture assembles a set of components to construct and optimize diverse middleware. Furthermore, our architecture enables end-to-end optimizations (see <ref> [15] </ref>), so that the composed middleware can outperform monolithic ones (Sec. 4 discusses performance results). Overview We proceed as follows. Section 2 introduces the Quarterware architecture and briefly alludes to how this architecture enables end-to-end optimizations. (The details of optimization principles are described elsewhere [15].) In Sec. 3 we demonstrate the <p> our architecture enables end-to-end optimizations (see <ref> [15] </ref>), so that the composed middleware can outperform monolithic ones (Sec. 4 discusses performance results). Overview We proceed as follows. Section 2 introduces the Quarterware architecture and briefly alludes to how this architecture enables end-to-end optimizations. (The details of optimization principles are described elsewhere [15].) In Sec. 3 we demonstrate the versatility of the architecture by describing how we derived implementations of a variety of middleware by specializing Quarterware components. Examples of derived implementations include basic CORBA [11], MPI [10], and RMI [21]. <p> Here, we concentrate on performance. Research has shown that good system performance requires end-to-end, system and subsystem optimizations in addition to optimizing independent components [2, 14]. Quarterware components follow the design principles described in <ref> [15] </ref> to enable end-to-end optimizations. Two representative examples are discussed below. Optimizing Memory Management The goals of memory management optimizations include reducing copying, reducing total memory usage, reducing dynamic allocations, and caching. We implement such optimizations by specializing Marshaler , Invoker and Receiver .
Reference: [16] <author> Douglas C. Schmidt. </author> <title> Principles and patterns of high-performance and real-time distributed object computing. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> page 11, </pages> <address> Santa Barbara, Cal-ifornia, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Each component implements one of the variations. In this Section, we discuss the derivation of components and briefly describe how these components are designed to enable systemwide, end-to-end optimizations. 2.1 Deriving Components Experience suggests the following classification of middleware features <ref> [8, 16, 9] </ref>. * Data Marshalling and Unmarshaling: Data marshaling policies depend on the kind of data being marshaled. System requirements also influence the marshaling policy, e.g., heterogeneity necessitates a common data format. <p> Similar to our work, they build a set of components that can be assembled to provide various flavors of DCOM. Another closely related work is on TAO ORB <ref> [16] </ref> by Schmidt et. al. They abstract ORB functionality as design patterns and use them to customize their ORB for hard real-time features. The OORPC system [25], a customizable RPC implementation, also shares some of our ideas and demonstrates the benefits of RPC customization.
Reference: [17] <author> Ashish Singhai, Swee Lim, and Sanjay R. Radia. </author> <note> CRMI - RMI in C++. (in preparation), </note> <year> 1998. </year>
Reference-contexts: To address this, we delay unmarshaling until method dispatch (direct demultiplexing). Arguments and Results Because our RMI is implemented in C++, argument and result handling in our RMI is limited in the following ways <ref> [17] </ref>. * Only the arguments and results whose types are known at compile time can be (un)marshaled. * In general, we can not decipher objects that override the default Java serialization (by implementing java.io.Serializable or java.io.
Reference: [18] <author> Ashish Singhai, Swee Lim, and Sanjay R. Radia. </author> <title> The SCALR framework for internet services. In Proceedings of the 28th Fault-Tolerant Computing Symposium (FTCS-28), </title> <note> page (to appear), </note> <month> June </month> <year> 1998. </year>
Reference-contexts: The OORPC system [25], a customizable RPC implementation, also shares some of our ideas and demonstrates the benefits of RPC customization. We have also used Quarterware based implementations for CORBA extensions for real-time and fault-tolerance [19], and object group management for SUN Netra Proxy servers <ref> [18] </ref>. The Spring system [6] encapsulates object invocation and object reference marshaling policies in sub 13 This corresponds to a vector of MPI typemap f (T, 0), (MPI UB, 8192)g where T is a vector of 8191 bytes. contracts.
Reference: [19] <author> Ashish Singhai, Aamod Sane, and Roy Camp-bell. </author> <title> Reflective ORBs: Support for robust, time-critical distribution. </title> <booktitle> In Proceedings of the ECOOP'97 Workshop on Reflective Real-Time Object-Oriented Programming and Systems, volume (to appear) of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <month> June </month> <year> 1997. </year>
Reference-contexts: The OORPC system [25], a customizable RPC implementation, also shares some of our ideas and demonstrates the benefits of RPC customization. We have also used Quarterware based implementations for CORBA extensions for real-time and fault-tolerance <ref> [19] </ref>, and object group management for SUN Netra Proxy servers [18]. The Spring system [6] encapsulates object invocation and object reference marshaling policies in sub 13 This corresponds to a vector of MPI typemap f (T, 0), (MPI UB, 8192)g where T is a vector of 8191 bytes. contracts.
Reference: [20] <author> Sun Microsystems Inc. </author> <title> Object Serialization Specification, </title> <note> 1997. Available at http://java.sun.com/products/jdk/rmi/ doc/serial-spec/serialTOC.doc.html%. </note>
Reference-contexts: The C++ version is useful where a Java implementation would be either too slow or infeasible, e.g., in network appliances like routers and switches. The principal characteristics of RMI relevant to our implementation are as follows: * RMI has its own wire protocol. * RMI uses Java Object Serialization <ref> [20, 13] </ref> for marshaling arguments and results. As described in Sec. 2.1, Invoker, Receiver and Marshaler components implement these features in Quarterware. Accordingly we customize them to implement the RMI specific variations. Wire Protocol The Invoker Receiver pair implements the wire protocol (Sec. 2.1).
Reference: [21] <author> Sun Microsystems Inc. </author> <title> Remote Method Invocation Specification, </title> <note> 1997. Available at http://java.sun.com/products/jdk/1.1/ docs/guide/rmi/spec/rmiTOC.doc.htm%l. </note>
Reference-contexts: Examples of derived implementations include basic CORBA [11], MPI [10], and RMI <ref> [21] </ref>. Performance results from Sec. 4 show that the Quarterware-derived middleware perform as well or better than the corresponding native middleware implementations. <p> First, we derive the core features of CORBA [11] as an example of basic Quarterware facilities (Sec. 3.1). In Sec. 3.2, we specialize the CORBA implementation in a different direction, we modify some components and re-use the rest to derive an RMI <ref> [21] </ref> implementation. Our last example, MPI [10] (Sec. 3.3) explores another dimension of communication middleware. MPI follows a message passing model, as opposed to the RPC model followed by CORBA and RMI. It targets data and compute intensive parallel applications that require low latency and high bandwidth. <p> On the other hand, for a system with few, concurrent objects (e.g., a relational database, wrapped to look like an object) we use a singleton Invoker with thread-per-message policy. 3.2 Composing RMI 2 Java Remote Method Invocation (RMI <ref> [21, 24] </ref>) provides distributed object support for Java applications. Using Quarterware, we derive a C++ version of RMI that interoperates with Java implementations. The C++ version is useful where a Java implementation would be either too slow or infeasible, e.g., in network appliances like routers and switches.
Reference: [22] <author> John M. Vlissides, James O. Coplien, and Nor-man L. Kerth, </author> <title> editors. Pattern Languages of Program Design 2. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Other related work includes research on composable systems [3] and design patterns for distributed programming <ref> [22] </ref>. 6 Conclusions Middleware builders aim to build efficient implementations of features common to many applications. But no particular set of features can suffice for all applications. Therefore, application programmers are forced to implement new features on top of the mid-dleware, or alter the middleware implementation.
Reference: [23] <author> Y. M. Wang and Woei-Jyh Lee. COMERA: </author> <title> COM extensible remoting architecture. </title> <booktitle> In Proceedings of the 4th Conference on Object-Oriented Technologies and Systems (COOTS). Usenix, </booktitle> <month> April </month> <year> 1998. </year>
Reference-contexts: LAM performance is the same as before because it makes copies in both cases. 5 Related Work The work closest to our work is on COMERA <ref> [23] </ref>, a Quarterware architecture for DCOM by Wang et. al. Similar to our work, they build a set of components that can be assembled to provide various flavors of DCOM. Another closely related work is on TAO ORB [16] by Schmidt et. al.
Reference: [24] <author> Ann Wollrath, Roger Riggs, and Jim Waldo. </author> <title> A distributed object model for the Java system. </title> <journal> Computing Systems, </journal> <volume> 9(4) </volume> <pages> 265-290, </pages> <address> 1996. </address> <publisher> Usenix. </publisher>
Reference-contexts: On the other hand, for a system with few, concurrent objects (e.g., a relational database, wrapped to look like an object) we use a singleton Invoker with thread-per-message policy. 3.2 Composing RMI 2 Java Remote Method Invocation (RMI <ref> [21, 24] </ref>) provides distributed object support for Java applications. Using Quarterware, we derive a C++ version of RMI that interoperates with Java implementations. The C++ version is useful where a Java implementation would be either too slow or infeasible, e.g., in network appliances like routers and switches. <p> The standard RMI implementation (from JDK1.1.4) would require extensive modifications to support multicast. Recently, it has been announced that future RMI implementations will also be based on CORBA IIOP. As discussed above, we can easily accommodate this change. However, the RMI implementation described in <ref> [24] </ref> combines transport, protocol and dispatching in the same layer and changing that implementation would require more effort. 3.3 Composing MPI MPI [10], the Message Passing Interface is a standard library for message passing programs. The library provides facilities for process communication, data description, and process grouping and naming.
Reference: [25] <author> Matthew J. Zelesko and David R. Cheriton. </author> <title> Specializing object-oriented RPC for functionality and performance. </title> <booktitle> In Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 175-187, </pages> <year> 1996. </year>
Reference-contexts: Another closely related work is on TAO ORB [16] by Schmidt et. al. They abstract ORB functionality as design patterns and use them to customize their ORB for hard real-time features. The OORPC system <ref> [25] </ref>, a customizable RPC implementation, also shares some of our ideas and demonstrates the benefits of RPC customization. We have also used Quarterware based implementations for CORBA extensions for real-time and fault-tolerance [19], and object group management for SUN Netra Proxy servers [18].
References-found: 25

