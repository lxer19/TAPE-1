URL: http://www.cs.utexas.edu/users/rvdg/class/jpdc_dense.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/class/materials.html
Root-URL: http://www.cs.utexas.edu
Title: SCALABILITY ISSUES AFFECTING THE DESIGN OF A DENSE LINEAR ALGEBRA LIBRARY 1  
Author: Jack J. Dongarra zx Robert A. van de Geijn David W. Walker P. O. David W. Walker P. O. 
Address: 107 Ayres Hall Knoxville, TN 37996-1301  Austin, TX 78712  Box 2008, Bldg. 6012 Oak Ridge, TN 37831-6367  Box 2008 Oak Ridge, TN 37831-6367  
Affiliation: Department of Computer Science University of Tennessee  Department of Computer Sciences University of Texas  Mathematical Sciences Section Oak Ridge National Laboratory  Oak Ridge National Laboratory  
Note: Submitted to Journal of Parallel and Distributed Computing  Corresponding author:  (office)  1 This work was supported in part by ARPA under contract number DAAL03-91-C-0047 ad ministered by ARO, and in part by DOE under contract number DE-AC05-84OR21400.  
Pubnum: TECHNICAL PAPER  
Email: walker@msr.epm.ornl.gov (email)  
Phone: (615) 574-7401  (615) 574-0680 (fax)  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen. </author> <title> Lapack: A portable linear algebra library for high-performance computers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 1-10. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year> <month> 23 </month>
Reference-contexts: In addition, we shall discuss how the design goals for the library, particularly the performance requirements, influenced the implementation of the core routines. The scalable library we are developing for multicomputers will be fully compatible with the LA-PACK library for vector and shared memory computers <ref> [1, 2, 12, 17] </ref>, and is therefore called ScaLAPACK. LAPACK was designed to implement the earlier EISPACK and LINPACK linear algebra libraries efficiently on shared memory, vector supercomputers, and to improve the robustness of some of the algorithms.
Reference: [2] <author> E. Anderson, Z. Bai, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year>
Reference-contexts: In addition, we shall discuss how the design goals for the library, particularly the performance requirements, influenced the implementation of the core routines. The scalable library we are developing for multicomputers will be fully compatible with the LA-PACK library for vector and shared memory computers <ref> [1, 2, 12, 17] </ref>, and is therefore called ScaLAPACK. LAPACK was designed to implement the earlier EISPACK and LINPACK linear algebra libraries efficiently on shared memory, vector supercomputers, and to improve the robustness of some of the algorithms.
Reference: [3] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn. </author> <title> Basic Linear Algebra Communication Subprograms. </title> <booktitle> In Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 287-290. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: This optimization is essential to the scalable performance of the library routines. The fundamental building blocks of the ScaLAPACK library are distributed memory versions of the Level 2 and Level 3 BLAS, and a set of Basic Linear Algebra Communication Subprograms (BLACS) <ref> [3, 21] </ref> for perform ing communication tasks that arise frequently in parallel linear algebra computations.
Reference: [4] <author> E. Anderson, A. Benzoni, J. J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn. </author> <title> LAPACK for distributed memory architectures: Progress report. </title> <booktitle> In Parallel Processing for Scientific Computing, Fifth SIAM Conference. </booktitle> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [5] <author> C. C. Ashcraft. </author> <title> The distributed solution of linear systems using the torus wrap data mapping. </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-147, Boeing Computer Services, </institution> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [6] <author> C. C. Ashcraft. </author> <title> A taxonamy of distributed dense LU factorization methods. </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-161, Boeing Computer Services, </institution> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [7] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, part i. </title> <editor> In R. Sincovec, editor, </editor> <booktitle> Proceedings of Sixth SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM Press, </publisher> <year> 1993. </year>
Reference-contexts: Another important and active research area is the development of re-usable software for multi-computers in the form of libraries and "tool-kits" <ref> [7, 23, 39] </ref>. Linear algebra|in particular, the solution of linear systems of equations|lies at the heart of most calculations in scientific computing.
Reference: [8] <author> R. P. Brent. </author> <title> The LINPACK benchmark on the AP 1000: Preliminary report. </title> <booktitle> In Proceedings of the 2nd CAP Workshop, </booktitle> <month> NOV </month> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [9] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker. </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: This approach permits locality of reference to be preserved. ScaLAPACK also uses block-partitioned algorithms to ensure good performance on MIMD distributed memory concurrent computers, by minimizing the frequency of data movement between different levels of the memory hierarchy <ref> [9, 10] </ref>. For such machines the memory hierarchy includes the off-processor memory of other processors, in addition to the hierarchy of registers, cache, and local memory on each processor, so the block partitioned approach is particularly useful in reducing the startup cost associated with interprocessor communication. <p> In addition to the LAPACK-compatible interface, we are also experimenting with developing interfaces for LAPACK and ScaLAPACK that are compatible 5 with Fortran 90 <ref> [9] </ref> and C++ [20]. 2.3 Range-Of-Use Range-of-use may be gauged by how numerically stable the algorithms are over a range of input problems, and the range of data structures the library will support.
Reference: [10] <author> J. Choi, J. J. Dongarra, and D. W. Walker. </author> <title> The design of scalable software libraries for distributed memory concurrent computers. </title> <booktitle> In Proceedings of the CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> 1993. </year> <note> To be published. </note>
Reference-contexts: This approach permits locality of reference to be preserved. ScaLAPACK also uses block-partitioned algorithms to ensure good performance on MIMD distributed memory concurrent computers, by minimizing the frequency of data movement between different levels of the memory hierarchy <ref> [9, 10] </ref>. For such machines the memory hierarchy includes the off-processor memory of other processors, in addition to the hierarchy of registers, cache, and local memory on each processor, so the block partitioned approach is particularly useful in reducing the startup cost associated with interprocessor communication.
Reference: [11] <author> T. Cwik, J. Patterson, and D. Scott. </author> <title> Electromagnetic scattering calculations on the intel touchstone delta. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 538-542. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: As noted by Cwik, Patterson, and Scott, "simulations using integral equation methods are fundamentally limited by the amount of available memory," which has led them to develop an out-of-core solver for their electromagnetic scattering problem <ref> [11] </ref>. Moreover, the largest dense LU factorization problems run in-core on the Intel Delta system are solved in less than 12 minutes [13]. In Figure 1 memory constrains the problem size and forms the lefthand boundary of the window-of-interest.
Reference: [12] <author> J. Demmel, J. J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, and D. Sorensen. </author> <title> Prospectus for the development of a linear algebra library for high performance computers. </title> <type> 24 Technical Report 97, </type> <institution> Argonne National Laboratory, Mathematics and Computer Science Division, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: In addition, we shall discuss how the design goals for the library, particularly the performance requirements, influenced the implementation of the core routines. The scalable library we are developing for multicomputers will be fully compatible with the LA-PACK library for vector and shared memory computers <ref> [1, 2, 12, 17] </ref>, and is therefore called ScaLAPACK. LAPACK was designed to implement the earlier EISPACK and LINPACK linear algebra libraries efficiently on shared memory, vector supercomputers, and to improve the robustness of some of the algorithms.
Reference: [13] <author> J. J. Dongarra. </author> <title> LINPACK benchmark: Performance of various computers using standard linear equations software. </title> <journal> Supercomputing Review, </journal> <volume> 5(3) </volume> <pages> 54-63, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Moreover, the largest dense LU factorization problems run in-core on the Intel Delta system are solved in less than 12 minutes <ref> [13] </ref>. In Figure 1 memory constrains the problem size and forms the lefthand boundary of the window-of-interest.
Reference: [14] <author> J. J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: When complete, ScaLAPACK will extend LAPACK to distributed memory concurrent computers. This section gives an overview of the functionality provided by LAPACK. LAPACK, which is based on the successful LINPACK <ref> [14] </ref> and EISPACK [27, 41] libraries, is portable and efficient across the range of large-scale, shared-memory, general-purpose computers.
Reference: [15] <author> J. J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A proposal for a set of level 3 basic linear algebra subprograms. </title> <type> Technical Report 88, </type> <institution> Argonne National Laboratory, Mathematics and Computer Science Division, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: This was done by recasting the algorithms in block-partitioned form, so that the bulk of the computation is performed by matrix-matrix operations using the Level 3 Basic Linear Algebra Subprograms (BLAS) <ref> [15, 16] </ref>. This approach permits locality of reference to be preserved. ScaLAPACK also uses block-partitioned algorithms to ensure good performance on MIMD distributed memory concurrent computers, by minimizing the frequency of data movement between different levels of the memory hierarchy [9, 10].
Reference: [16] <author> J. J. Dongarra, I. Duff, J. Du Croz, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM TOMS, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This was done by recasting the algorithms in block-partitioned form, so that the bulk of the computation is performed by matrix-matrix operations using the Level 3 Basic Linear Algebra Subprograms (BLAS) <ref> [15, 16] </ref>. This approach permits locality of reference to be preserved. ScaLAPACK also uses block-partitioned algorithms to ensure good performance on MIMD distributed memory concurrent computers, by minimizing the frequency of data movement between different levels of the memory hierarchy [9, 10].
Reference: [17] <author> J. J. Dongarra, I. S. Duff, and D. C. Sorensen H. A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: In addition, we shall discuss how the design goals for the library, particularly the performance requirements, influenced the implementation of the core routines. The scalable library we are developing for multicomputers will be fully compatible with the LA-PACK library for vector and shared memory computers <ref> [1, 2, 12, 17] </ref>, and is therefore called ScaLAPACK. LAPACK was designed to implement the earlier EISPACK and LINPACK linear algebra libraries efficiently on shared memory, vector supercomputers, and to improve the robustness of some of the algorithms.
Reference: [18] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey, and D. W. Walker. </author> <title> A proposal for a user-level message passing interface in a distributed memory environment. </title> <type> Technical Report TM-12231, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: A number of key software technologies are helping to accelerate the more widespread use of massively parallel computers in these areas. These include the development of parallelizing compilers, parallel language extensions such as Fortran D [25] and High Performance Fortran [33], the adoption of a standard message passing interface <ref> [18] </ref>, support for parallel constructs and operations, such as guard layers and object migration, and a variety of tools for debugging, and visualizing/analyzing performance on massively parallel computers. <p> In addition, the ScaLAPACK library also currently includes routines for the concurrent solution of triangular systems. The message passing performed in these routines is based on the PICL interface [28], however, when the Message Passing Interface (MPI) standard <ref> [18] </ref> is complete we intend to rewrite the distributed BLAS and BLACS in terms of this, which should make the library more easily portable.
Reference: [19] <author> J. J. Dongarra and S. Ostrouchov. </author> <title> LAPACK block factorization algorithms on the Intel iPSC/860. </title> <type> Technical Report CS-90-115, </type> <institution> University of Tennessee at Knoxville, Computer Science Department, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [20] <author> J. J. Dongarra, R. Pozo, and D. W. Walker. </author> <title> An object oriented design for high performance linear algebra on distributed memory architectures. </title> <booktitle> In Proceedings of Object Oriented Numerics Conference, </booktitle> <year> 1993. </year> <note> To be published. </note>
Reference-contexts: In addition to the LAPACK-compatible interface, we are also experimenting with developing interfaces for LAPACK and ScaLAPACK that are compatible 5 with Fortran 90 [9] and C++ <ref> [20] </ref>. 2.3 Range-Of-Use Range-of-use may be gauged by how numerically stable the algorithms are over a range of input problems, and the range of data structures the library will support.
Reference: [21] <author> J. J. Dongarra and R. A. van de Geijn. </author> <title> Two-dimensional basic linear algebra communication subprograms. </title> <note> Technical Report LAPACK working note 37, </note> <institution> Computer Science Department, University of Tennessee, Knoxville, TN, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: This optimization is essential to the scalable performance of the library routines. The fundamental building blocks of the ScaLAPACK library are distributed memory versions of the Level 2 and Level 3 BLAS, and a set of Basic Linear Algebra Communication Subprograms (BLACS) <ref> [3, 21] </ref> for perform ing communication tasks that arise frequently in parallel linear algebra computations.
Reference: [22] <author> J. J. Dongarra and R. A. van de Geijn. </author> <title> Reduction to condensed form for the eigenvalue problem on distributed memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 973-982, </pages> <year> 1992. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [23] <author> J. J. Dongarra, R. A. van de Geijn, and D. W. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <editor> In J. H. Saltz, editor, </editor> <booktitle> Proceedings of the 1992 Scalable High Performance Computing Comference. </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Another important and active research area is the development of re-usable software for multi-computers in the form of libraries and "tool-kits" <ref> [7, 23, 39] </ref>. Linear algebra|in particular, the solution of linear systems of equations|lies at the heart of most calculations in scientific computing.
Reference: [24] <author> A. Edelman. </author> <title> Large dense numerical linear algebra in 1993: The parallel computing influence. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 7(2), </volume> <year> 1993. </year> <month> 25 </month>
Reference-contexts: An alternative approach is to measure scalability in terms of performance per processor at fixed computational work per processor. This latter approach provides a useful scalability metric when runtime constrains the problem size [30, 31]. Large, dense linear algebra computations mostly arise in three-dimensional boundary element problems <ref> [24] </ref>, and for such problems memory, rather than runtime, usually constrains the problem size [42].
Reference: [25] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C-W. Tseng, and M-Y. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report CRPC-TR90079, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: A number of key software technologies are helping to accelerate the more widespread use of massively parallel computers in these areas. These include the development of parallelizing compilers, parallel language extensions such as Fortran D <ref> [25] </ref> and High Performance Fortran [33], the adoption of a standard message passing interface [18], support for parallel constructs and operations, such as guard layers and object migration, and a variety of tools for debugging, and visualizing/analyzing performance on massively parallel computers.
Reference: [26] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: The way in which the data is distributed (or decomposed) over the memory hierarchy of a computer is of fundamental importance to these factors. Concurrent efficiency, *, is defined as the concurrent speedup per processor <ref> [26] </ref>, where the concurrent speedup is the execution time, T seq , for the best sequential algorithm running on one processor of the concurrent computer, divided by the execution time, T , of the parallel algorithm running on N p processors. <p> We shall assume 0 m &lt; M and 0 p &lt; P . Two common decompositions are the block and the cyclic decompositions <ref> [26, 45] </ref>. The block decomposition assigns contiguous entries in the global vector to the processes in blocks. m 7! ( bm=Lc ; m mod L ) ; (3) where L = dM=P e.
Reference: [27] <author> B. S. Garbow, J. M. Boyle, J. J. Dongarra, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: When complete, ScaLAPACK will extend LAPACK to distributed memory concurrent computers. This section gives an overview of the functionality provided by LAPACK. LAPACK, which is based on the successful LINPACK [14] and EISPACK <ref> [27, 41] </ref> libraries, is portable and efficient across the range of large-scale, shared-memory, general-purpose computers.
Reference: [28] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> A user's guide to PICL: a portable instrumented communication library. </title> <type> Technical Report TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: In addition, the ScaLAPACK library also currently includes routines for the concurrent solution of triangular systems. The message passing performed in these routines is based on the PICL interface <ref> [28] </ref>, however, when the Message Passing Interface (MPI) standard [18] is complete we intend to rewrite the distributed BLAS and BLACS in terms of this, which should make the library more easily portable.
Reference: [29] <author> A. Gupta and V. Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year> <note> Also available as technical report TR 90-20 from the Computer Science Department, </note> <institution> University of Minnesota, </institution> <address> Minneapolis, MN 55455. </address>
Reference-contexts: For routines that iterate, such as eigen--solvers, the number of iterations, and hence the execution time, depends not only on the problem size, but also on other characteristics of the input data, such as condition number. A parallel algorithm is said to be highly scalable <ref> [29, 36] </ref> if the concurrent efficiency depends on the problem size and number of processors only through their ratio. This ratio is simply the memory requirement of the problem per processor, often referred to as the granularity.
Reference: [30] <author> J. Gustafson. </author> <title> Reevaluating amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: An alternative approach is to measure scalability in terms of performance per processor at fixed computational work per processor. This latter approach provides a useful scalability metric when runtime constrains the problem size <ref> [30, 31] </ref>. Large, dense linear algebra computations mostly arise in three-dimensional boundary element problems [24], and for such problems memory, rather than runtime, usually constrains the problem size [42].
Reference: [31] <author> J. Gustafson, D. Rover, S. Elbert, and M. Carter. </author> <title> The design of a scalable, fixed-time computer benchmark. </title> <journal> Journal of Parallel and Distributed Systems, </journal> <volume> 12 </volume> <pages> 388-401, </pages> <year> 1991. </year>
Reference-contexts: An alternative approach is to measure scalability in terms of performance per processor at fixed computational work per processor. This latter approach provides a useful scalability metric when runtime constrains the problem size <ref> [30, 31] </ref>. Large, dense linear algebra computations mostly arise in three-dimensional boundary element problems [24], and for such problems memory, rather than runtime, usually constrains the problem size [42].
Reference: [32] <author> B. Hendrickson and D. Womble. </author> <title> The torus-wrap mapping for dense matrix computations on massively parallel computers. </title> <type> Technical Report SAND92-0792, </type> <institution> Sandia National Laboratories, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions. <p> The ff term is due to the communications necessary for pivoting and broadcasting. The final term is due to the volume of communication that lies in the critical path of execution. In <ref> [32] </ref>, it is shown theoretically that the minimum communication required for an LU factorization that balances the workload is (P + Q)=(P Q)O (N 2 )fi.
Reference: [33] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 0.4, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: A number of key software technologies are helping to accelerate the more widespread use of massively parallel computers in these areas. These include the development of parallelizing compilers, parallel language extensions such as Fortran D [25] and High Performance Fortran <ref> [33] </ref>, the adoption of a standard message passing interface [18], support for parallel constructs and operations, such as guard layers and object migration, and a variety of tools for debugging, and visualizing/analyzing performance on massively parallel computers. <p> The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) <ref> [33] </ref>, and has been previously used, in one form or another, by several researchers (see [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3.
Reference: [34] <author> C.-T. Ho and S. L. Johnsson. </author> <title> Distributed routing algorithms for broadcasting and personalized communication in hypercubes. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 640-648. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: The logarithmic factor in the last term of Eq. 25 can be removed by using a more sophisticated broadcast like the EDST algorithm in <ref> [34] </ref>, at the expense of a higher cost for communication startup. To examine scalability, first assume that P = 1.
Reference: [35] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers. </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, UK, </address> <year> 1981. </year>
Reference-contexts: for solving linear least squares problem, computing the singular value decomposition, for the eigenvalue problem (general, symmetric, and generalized problem). 3.2 Target Architectures The EISPACK and LINPACK software libraries were designed for supercomputers in use in the 1970's and early 1980's, such as the CDC-7600, Cyber 205, and Cray-1 computers <ref> [35] </ref>. These machines featured multiple functional units that were pipelined to get good performance. The CDC-7600 was basically a high-performance scalar computer, while the Cyber 205 and Cray-1 were early vector computers.
Reference: [36] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <type> Technical report, </type> <institution> TR-91-18, Computer Science Department, University of Minn esota, </institution> <month> June </month> <year> 1991. </year> <note> To appear in Journal of Parallel and Distributed Computing, 1994. A short 26 version of the paper appears in the Proceedings of the 1991 International Conference on Su--percomputing, Germany, and as an invited paper in the Proceedings of 29th Annual Allerton Conference on Communuication, Control and Computing, Urbana,IL, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: For routines that iterate, such as eigen--solvers, the number of iterations, and hence the execution time, depends not only on the problem size, but also on other characteristics of the input data, such as condition number. A parallel algorithm is said to be highly scalable <ref> [29, 36] </ref> if the concurrent efficiency depends on the problem size and number of processors only through their ratio. This ratio is simply the memory requirement of the problem per processor, often referred to as the granularity.
Reference: [37] <author> W. Lichtenstein and S. L. Johnsson. </author> <title> Block-cyclic dense linear algebra. </title> <type> Technical Report TR-04-92, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [38] <author> Y. Saad and M. H. Schultz. </author> <title> Parallel direct methods for solving banded linear systems. </title> <type> Technical Report YALEU/DCS/RR-387, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1985. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [39] <author> A. J. Skjellum and C. Baldwin. </author> <title> The multicomputer toolbox: Scalable parallel libraries for large-scale concurrent applications. </title> <type> Technical report, </type> <institution> Numerical Mathematics Group, Lawrence Livermore National Laboratory, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Another important and active research area is the development of re-usable software for multi-computers in the form of libraries and "tool-kits" <ref> [7, 23, 39] </ref>. Linear algebra|in particular, the solution of linear systems of equations|lies at the heart of most calculations in scientific computing.
Reference: [40] <author> A. J. Skjellum and A. Leung. </author> <title> LU factorization of sparse, unsymmetric, Jacobian matrices on multicomputers. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 328-337. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions.
Reference: [41] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: When complete, ScaLAPACK will extend LAPACK to distributed memory concurrent computers. This section gives an overview of the functionality provided by LAPACK. LAPACK, which is based on the successful LINPACK [14] and EISPACK <ref> [27, 41] </ref> libraries, is portable and efficient across the range of large-scale, shared-memory, general-purpose computers.
Reference: [42] <author> X.-H. Sun and L. </author> <title> Hi. Scalable problems and memory-bounded speedup. </title> <journal> Journal of Parallel and Distributed Systems, </journal> <volume> 19(1) </volume> <pages> 27-37, </pages> <year> 1993. </year>
Reference-contexts: This latter approach provides a useful scalability metric when runtime constrains the problem size [30, 31]. Large, dense linear algebra computations mostly arise in three-dimensional boundary element problems [24], and for such problems memory, rather than runtime, usually constrains the problem size <ref> [42] </ref>. As noted by Cwik, Patterson, and Scott, "simulations using integral equation methods are fundamentally limited by the amount of available memory," which has led them to develop an out-of-core solver for their electromagnetic scattering problem [11].
Reference: [43] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: Our target machines are, therefore, medium and large grainsize advanced-architecture computers. These include "traditional" shared memory, vector supercomputers, such as the Cray Y-MP and C90, and MIMD distributed memory concurrent supercomputers, such as the Intel Paragon, and Thinking Machines' CM-5 <ref> [43] </ref>, and the more recently announced IBM SP1 and Cray T3D concurrent systems. Since these machines have only very recently become available most of the ongoing development of the ScaLAPACK library is being performed on a 128-node Intel iPSC/860 hypercube and on the 512-node Intel Touchstone Delta system.
Reference: [44] <author> R. A. van de Geijn. </author> <title> Massively parallel LINPACK benchmark on the Intel Touchstone Delta and iPSC/860 systems. </title> <institution> Computer Science report TR-91-28, Univ. of Texas, </institution> <year> 1991. </year>
Reference-contexts: The block cyclic decomposition is one of the data distributions supported by High Performance Fortran (HPF) [33], and has been previously used, in one form or another, by several researchers (see <ref> [4, 5, 6, 8, 19, 22, 32, 37, 38, 40, 44] </ref> for examples of its use). The block cyclic decomposition is illustrated with an example in Figure 3. In decomposing an M fi N matrix we apply independent block cyclic decompositions in the row and column directions. <p> Indeed, changing the blocksize by one can easily affect the performance on a single compute node by a factor of two <ref> [44] </ref>. As a result, r should be treated as a constant that depends on the implementation of the matrix-matrix multiply, and the hardware. The optimal ratio P=Q could be predicted by the model. For our algorithms, communication within rows can be pipelined, and therefore partially hidden by computation. <p> For our algorithms, communication within rows can be pipelined, and therefore partially hidden by computation. Computation within a column of the template is tightly coupled, making communication more difficult to hide. As a result, an optimal ratio will be attained when P &lt; Q <ref> [44] </ref>. 5.2 Experiments on the iPSC/860 In this section, we show how experimental results support the theoretical scalability results by reporting performance attained by our algorithms on the Intel iPSC/860. The Intel iPSC/860 is a parallel architecture with up to 128 processing nodes.
Reference: [45] <author> E. F. Van de Velde. </author> <title> Data redistribution and concurrency. </title> <journal> Parallel Computing, </journal> <volume> 16, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: We shall assume 0 m &lt; M and 0 p &lt; P . Two common decompositions are the block and the cyclic decompositions <ref> [26, 45] </ref>. The block decomposition assigns contiguous entries in the global vector to the processes in blocks. m 7! ( bm=Lc ; m mod L ) ; (3) where L = dM=P e.
References-found: 45

