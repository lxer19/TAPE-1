URL: ftp://ftp.eecs.umich.edu/people/durfee/aaai96ws-am-vd.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/durfee/vita.html
Root-URL: http://www.cs.umich.edu
Email: jmvidal@umich.edu  
Title: Building Agent Models in Economic Societies of Agents  
Author: Jose M. Vidal and Edmund H. Durfee 
Date: May 10, 1996  
Address: 1101 Beal Avenue, Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Abstract: We build an economic society of agents in which buyers and sellers compete with each other, and try to increase their total values and total profits, respectively. We give a precise method for the construction of these agents and for the incremental incorporation of modeling capabilities, following the intuitions behind the Recursive Modeling Method (RMM). These agents were built and run in a simulated economic society. Early test results show that deeper (i.e. 1-level) models are more effective in heterogeneous societies than in homogeneous ones, and that price volatility is a good predictor of the relative ben efits of deeper models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> George A. Akerlof. </author> <title> The market for `lemons': Quality uncertainty and the market mechanism. </title> <journal> The Quaterly Journal of Economics, </journal> <pages> pages 488-500, </pages> <month> August </month> <year> 1970. </year>
Reference: [2] <author> Daniel E. Atkins, William P. Birmingham, Edmund H. Durfee, Eric J. Glover, Tracy Mullen, Elke A. Rundensteiner, Elliot Soloway, Jose M. Vidal, Raven Wallace, and Michael P. Wellman. </author> <title> Toward inquiry-based education through interacting software agents. </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1996. </year> <note> http://www.computer.org/pubs/computer/dli/- r50069/r50069.htm </note> . 
Reference-contexts: Both of these extension endeavor to bring our model closer to the reality of a multi-agent system. In the same vein, we are already 17 implementing some of the agent capabilities shown in this paper, into the UMDL <ref> [2] </ref> multi-agent system. In the long run, another offshoot of this research could be a better characterization of the type of environments and how they allow/inhibit "cheating" behavior on different agent populations.
Reference: [3] <author> Robert Axelrod. </author> <title> The evolution of strategies in the iterated prisoner's dilemma. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year> <month> forthcoming. </month>
Reference: [4] <author> Edmund H. Durfee, Piotr J. Gmytrasiewicz, and Jeffrey S. Rosenschein. </author> <title> The utility of embedded communications and the emergence of protocols. </title> <booktitle> In Proceedings of the 13th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1994. </year>
Reference: [5] <author> Joshua M. Epstein and Robert L. Axtell. </author> <title> Growing Artifical Societies: Social Science from the Bottom Up. </title> <publisher> Brookings Institution, </publisher> <year> 1996. </year> <note> Description at http://www.brook.edu/pub/books/ARTIFSOC.HTM . 18 </note>
Reference: [6] <author> Piotr J. Gmytrasiewics and Edmund H. Durfee. </author> <title> A rigorous, operational formalization of recursive modeling. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 125-132, </pages> <year> 1995. </year>
Reference: [7] <author> Piotr J. Gmytrasiewicz. </author> <title> On reasoning about other agents. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: We divide the agents into classes that correspond to their modeling capabilities. The hierarchy we present is inspired by RMM <ref> [7] </ref>, but is function-based rather than matrix-based, and includes learning. We start with agents with no models (also referred to as 0-level agents), who must base their actions purely on their inputs and the rewards they receive. They are not aware that there are other agents out there.
Reference: [8] <author> Tracy Mullen and Michael. P. Wellman. </author> <title> Some issues in the design of market-oriented agents. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference: [9] <author> Jeffrey S. Rosenschein and Gilad Zlotkin. </author> <title> Rules of Encounter. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: The rewards, we are finding, start to diminish as the other agents become "smarter". It would be very useful to characterize the type of environments and agent populations that, combined, foster such antisocial behavior (see <ref> [9] </ref>), especially as interest in multi-agent systems grows.
Reference: [10] <author> Stuart Russell. </author> <title> Rationality and intelligence. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 950-957, </pages> <year> 1995. </year>
Reference: [11] <author> Stuart Russell and Eric Wefald. </author> <title> Do The Right Thing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference: [12] <author> Yoav Shoham. </author> <title> Agent-oriented programming. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 51-92, </pages> <year> 1993. </year>
Reference: [13] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In general, agents get some input, take an action, then receive some reward. This is the same basic framework under which most learning mechanism are presented. We decided to use a form of reinforcement learning <ref> [13] </ref> [17] for implementing this kind of learning in our agents, since it is a simple method and the domain is simple enough for it to do a reasonable job. Both buyers and sellers will use the equations in the next sections for determining what actions to take.
Reference: [14] <author> Milind Tambe and P. S. Rosenbloom. </author> <title> Agent tracking in real-time dynamic environments. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference: [15] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Task planning agents in the UMDL. </title> <booktitle> In Proceedings of the 1995 Intelligent Agents Workshop, </booktitle> <year> 1995. </year> <note> http://ai.eecs.umich.edu/people/jmvidal/papers/tpa/tpa.html . 19 </note>
Reference: [16] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Using recursive agent models ef-fectively. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> http://ai.eecs.umich.edu/- people/jmvidal/papers/lr-rmm2/lr-rmm2.html </note> . 
Reference: [17] <author> Christopher J. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In general, agents get some input, take an action, then receive some reward. This is the same basic framework under which most learning mechanism are presented. We decided to use a form of reinforcement learning [13] <ref> [17] </ref> for implementing this kind of learning in our agents, since it is a simple method and the domain is simple enough for it to do a reasonable job. Both buyers and sellers will use the equations in the next sections for determining what actions to take.
References-found: 17

