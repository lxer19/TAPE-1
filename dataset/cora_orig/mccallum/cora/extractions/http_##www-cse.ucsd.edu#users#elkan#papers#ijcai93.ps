URL: http://www-cse.ucsd.edu/users/elkan/papers/ijcai93.ps
Refering-URL: http://www-cse.ucsd.edu/users/elkan/
Root-URL: 
Title: Estimating the Accuracy of Learned Concepts  
Author: Timothy L. Bailey and Charles Elkan 
Address: La Jolla, California 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: August 1993.  
Note: Published in Proceedings of the International Joint Conference on Artificial Intelligence, (IJCAI'93), pp. 895-900, Chambery, France,  
Abstract: This paper investigates alternative estimators of the accuracy of concepts learned from examples. In particular, the cross-validation and 632 bootstrap estimators are studied, using synthetic training data and the foil learning algorithm. Our experimental results contradict previous papers in statistics, which advocate the 632 bootstrap method as superior to cross-validation. Nevertheless, our results also suggest that conclusions based on cross-validation in previous machine learning papers are unreliable. Specifically, our observations are that (i) the true error of the concept learned by foil from independently drawn sets of examples of the same concept varies widely, (ii) the estimate of true error provided by cross-validation has high variability but is approximately unbiased, and (iii) the 632 bootstrap estimator has lower variability than cross-validation, but is systematically biased.
Abstract-found: 1
Intro-found: 1
Reference: [ Crawford, 1989 ] <author> Stuart L. Crawford. </author> <title> Extensions to the cart algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31 </volume> <pages> 197-217, </pages> <year> 1989. </year>
Reference-contexts: Other learning algorithms with which the 632 bootstrap method has been reported to work well, notably Fisher's linear discriminant method used in [ Fitzmaurice et al., 1991 ] and [ Efron, 1983 ] and the cart decision tree algorithm used in <ref> [ Crawford, 1989 ] </ref> , are strongly influenced by duplicates. 5 Discussion We studied the performance of cross-validation and 632 bootstrap as estimators of the accuracy of concept definitions learned from synthetic data by foil. We observed the following. (i) The true accuracy Err of learned concepts has high variance.
Reference: [ Efron, 1979 ] <author> Bradley Efron. </author> <title> Bootstrap methods: An other look at the jackknife. </title> <journal> Annals of Statistics, </journal> <volume> 7 </volume> <pages> 1-26, </pages> <year> 1979. </year>
Reference-contexts: In much work in machine learning the distribution is just over the attributes, and the class is assumed to be deterministically dependent on the attributes. Our definition subsumes the usual definition as a special case. the training set X itself as the basis for estimating Err. <ref> [ Efron, 1979 ] </ref> shows that the three methods known as bootstrap, jackknife and cross-validation are mathematically related. Bootstrap is a nonparametric maximum likelihood estimator, 2 jackknife is a quadratic approximation to bootstrap, and cross-validation is similar in form and value to jackknife.
Reference: [ Efron, 1983 ] <author> Bradley Efron. </author> <title> Estimating the error rate of prediction rules: Improvement on cross-validation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 78(382) </volume> <pages> 316-331, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction The problem of concept induction (also known as the classification problem [ Kononenko and Bratko, 1991 ] and known as the prediction problem in the statistical literature <ref> [ Efron, 1983 ] </ref> ) is perhaps the most intensively studied topic in machine learning. <p> This is sometimes referred to as the generalization error of the concept definition since it measures how well the concept definition generalizes to examples the learning algorithm did not see. The definition of true error. The following definitions are essentially the same as those in <ref> [ Efron, 1983 ] </ref> . Suppose a learning algorithm constructs prediction rule (t; X) from training set X. <p> Bootstrap is a nonparametric maximum likelihood estimator, 2 jackknife is a quadratic approximation to bootstrap, and cross-validation is similar in form and value to jackknife. Later work <ref> [ Efron, 1983 ] </ref> argues empirically and analytically that a modified bootstrap method known as 632 bootstrap is superior. Therefore, we focused on cross-validation and 632 bootstrap estimators in our work. The cross-validation method. Cross-validation estimates Err by reserving part of the training set for testing the learned theory. <p> Other methods which use smaller subsets for training, in particular v-fold cross-validation where v &lt; n, should intuitively be poorer estimates of Err when the number of training examples available is small. The 632 bootstrap method. The 632 bootstrap technique <ref> [ Efron, 1983 ] </ref> for estimating Err creates a "re-sample" from a training set by choosing n samples with replacement from the training set. Resamples are typically multisets. <p> In practice, e b is averaged over many resamples. Previous comparisons of methods. The 632 bootstrap method is reported in <ref> [ Efron, 1983 ] </ref> to estimate true error better in five experiments than several other methods including the original bootstrap method and cross-validation. <p> The bias of cross-validation as an estimator of Err, on the other hand, is very small. The variance of 632 bootstrap is generally much less than that of cross-validation. This has always been a primary reason for considering bootstrap methods over cross-validation <ref> [ Efron, 1983 ] </ref> . The scatter plot experiments show the basic pitfalls of cross-validation as a method of estimating Err. <p> The 632 bootstrap method can thus lose its main advantage over cross-validation when the value of Err is small. Explaining the failure of 632 bootstrap. The poor performance of 632 bootstrap is surprising in view of earlier, positive reports in the statistical literature <ref> [ Fitzmaurice et al., 1991; Efron, 1983 ] </ref> . However, that work measured the accuracy of 632 bootstrap for training data generated from two multivariate normal classes. <p> Other learning algorithms with which the 632 bootstrap method has been reported to work well, notably Fisher's linear discriminant method used in [ Fitzmaurice et al., 1991 ] and <ref> [ Efron, 1983 ] </ref> and the cart decision tree algorithm used in [ Crawford, 1989 ] , are strongly influenced by duplicates. 5 Discussion We studied the performance of cross-validation and 632 bootstrap as estimators of the accuracy of concept definitions learned from synthetic data by foil.
Reference: [ Fitzmaurice et al., 1991 ] <author> G. M. Fitzmaurice, W. J. Krzanowski, and D. J. </author> <title> Hand. A Monte Carlo study of the 632-bootstrap estimator of error rate. </title> <journal> Journal of Classification, </journal> <volume> 8(2) </volume> <pages> 239-250, </pages> <year> 1991. </year>
Reference-contexts: The 632 bootstrap method is reported in [ Efron, 1983 ] to estimate true error better in five experiments than several other methods including the original bootstrap method and cross-validation. More recent and comprehensive experiments using linear discriminant classifiers confirm the good performance of the 632 bootstrap method <ref> [ Fitzmaurice et al., 1991; Sanchez and Cepeda, 1989 ] </ref> . <p> The 632 bootstrap method can thus lose its main advantage over cross-validation when the value of Err is small. Explaining the failure of 632 bootstrap. The poor performance of 632 bootstrap is surprising in view of earlier, positive reports in the statistical literature <ref> [ Fitzmaurice et al., 1991; Efron, 1983 ] </ref> . However, that work measured the accuracy of 632 bootstrap for training data generated from two multivariate normal classes. <p> Other learning algorithms with which the 632 bootstrap method has been reported to work well, notably Fisher's linear discriminant method used in <ref> [ Fitzmaurice et al., 1991 ] </ref> and [ Efron, 1983 ] and the cart decision tree algorithm used in [ Crawford, 1989 ] , are strongly influenced by duplicates. 5 Discussion We studied the performance of cross-validation and 632 bootstrap as estimators of the accuracy of concept definitions learned from synthetic
Reference: [ Haussler, 1988 ] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: 0 ), which is the expectation Err = E F Q [Y 0 ; (T 0 ; X)]: Readers familiar with PAC learning theory will notice that this definition of true error rate subsumes the definition of the error of a hypothesis with respect to a target concept given in <ref> [ Haussler, 1988 ] </ref> and often used by PAC theorists. The difference is that the PAC framework usually assumes that the examples are either "in" or "out" of a hypothesis: that the hypothesis and target concept are deterministic.
Reference: [ Kononenko and Bratko, 1991 ] <author> Igor Kononenko and Ivan Bratko. </author> <title> Information-based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of concept induction (also known as the classification problem <ref> [ Kononenko and Bratko, 1991 ] </ref> and known as the prediction problem in the statistical literature [ Efron, 1983 ] ) is perhaps the most intensively studied topic in machine learning.
Reference: [ Quinlan, 1990 ] <author> John R. Quinlan. </author> <title> Learning logical defi nitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: The FOIL algorithm. This algorithm <ref> [ Quinlan, 1990 ] </ref> produces concept definitions which are sets of function-free Datalog-with-negation clauses. Training sets given to foil are encoded as relation extensions, i.e. as lists of ground tuples.
Reference: [ Sanchez and Cepeda, 1989 ] <author> J. M. Prada Sanchez and X. L. Otero Cepeda. </author> <title> The use of smooth bootstrap techniques for estimating the error rate of a prediction rule. </title> <journal> Communications in Statistics-Simulation and Computation, </journal> <volume> 18(3) </volume> <pages> 1169-1186, </pages> <year> 1989. </year>
Reference-contexts: The 632 bootstrap method is reported in [ Efron, 1983 ] to estimate true error better in five experiments than several other methods including the original bootstrap method and cross-validation. More recent and comprehensive experiments using linear discriminant classifiers confirm the good performance of the 632 bootstrap method <ref> [ Fitzmaurice et al., 1991; Sanchez and Cepeda, 1989 ] </ref> .
Reference: [ Towell et al., 1990 ] <author> G. G. Towell, J. W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, Mas-sachusetts, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Cross-validation is the method most often used, fl This work was supported in part by the National Science Foundation under Award No. IRI-9110813. Timothy Bailey is supported by an NIH Human Genome Project predoctoral training grant. as for example in <ref> [ Towell et al., 1990 ] </ref> and [ Weinstein et al., 1992 ] .
Reference: [ Weinstein et al., 1992 ] <author> John N. Weinstein et al. </author> <title> Neu ral computing in cancer drug development: Predicting mechanism of action. </title> <journal> Science, </journal> <volume> 258 </volume> <pages> 447-451, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Cross-validation is the method most often used, fl This work was supported in part by the National Science Foundation under Award No. IRI-9110813. Timothy Bailey is supported by an NIH Human Genome Project predoctoral training grant. as for example in [ Towell et al., 1990 ] and <ref> [ Weinstein et al., 1992 ] </ref> . However several Monte Carlo studies of cross-validation and alternative methods have been done [ Fitzmaurice et al., 1991; Sanchez and Cepeda, 1989; Efron, 1983 ] , which tend to indicate that cross-validation is inferior to the other methods.
Reference: [ Weiss and Kulikowski, 1991 ] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer systems that learn: Classification and prediction methods from statistics, neural nets, machine learning, and expert systems. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference: [ Weiss, 1991 ] <author> Sholom M. Weiss. </author> <title> Small sample error rate estimation for k-NN classifiers. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(3) </volume> <pages> 285-289, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: In experiments using nearest neighbor classifiers, both cross-validation and the 632 bootstrap method have been reported to perform poorly, and a composite method has been suggested <ref> [ Weiss, 1991; Weiss and Ku-likowski, 1991 ] </ref> . Linear classifiers and nearest neighbor methods are very different from symbolic concept induction methods such as foil or decision tree algorithms. In almost all symbolic learning work cross-validation has been used to estimate the accuracy of learned concepts. <p> The concepts learned by foil are, however, different with and without duplicates, as can be seen by the fact that the curves for e b are different. It is worth noting that the one-nearest-neighbor classification method, with which the 632 bootstrap method is reported to perform poorly <ref> [ Weiss, 1991 ] </ref> , is a learning algorithm for which by definition duplicates in the training set have no influence.
References-found: 12

