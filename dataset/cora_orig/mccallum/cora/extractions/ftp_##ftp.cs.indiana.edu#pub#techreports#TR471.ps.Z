URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR471.ps.Z
Refering-URL: http://www.cs.indiana.edu/ftp/techreports/index.html
Root-URL: http://www.cs.indiana.edu
Title: A NEW MODEL FOR SOLVING THE DATA DISTRIBUTION PROBLEM  
Author: Thomas J. Loos 
Degree: Submitted to the faculty of the University Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: December 1996  
Affiliation: Department of Computer Science Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Geist, et al, </author> <title> PVM: A User's Guide and Tutorial for Networked Parallel Computing, </title> <publisher> MIT Press, </publisher> <address> Boston, MA, </address> <note> first ed., </note> <year> 1994. </year>
Reference-contexts: It is the dominant parallel computing interface standard, replacing Parallel Virtual Machine (PVM) <ref> [1] </ref>. 2.2.1 What is MPI? MPI is a standard defining a parallel communications paradigm it is not a implementation. Standardization allows for portable code between different parallel hardware platforms and between different MPI implementations. The wide acceptance of the standard is due in part to its readability [56].
Reference: [2] <author> V. Adve, </author> <title> Analyzing the Behavior and Performance of Parallel Programs, </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, </institution> <month> Oc-tober </month> <year> 1993. </year>
Reference-contexts: This work used previous timing results to predict future timing results; a natural choice for a frame-by-frame renderer, where the input changes a small amount from frame to frame. It did not readily account for changes in the rendering algorithm or computational environment. Adve <ref> [2] </ref> and Xu, Zhang, and Sun [62] also use a modeling strategy based on combining empirical observations. They identify segments of a program by first locating communication and synchronization points and computing a task graph. <p> This provides an indication that metrics such as edges cut which do not take synchronization into account will not provide good TPI estimates. Future work includes using a more sophisticated process state model, such as described in <ref> [2] </ref>, that could be added into the AET calculation. The AET simulator could be rewritten as separate from the actual solver code using only one simulator process, allowing the use of the AET metric as a cost function in a data distribution algorithm.
Reference: [3] <author> O. Axelsson and G. Lindskog, </author> <title> On the Eigenvalue Distribution of a Class of Preconditioning Methods, </title> <journal> Numer. Math., </journal> <volume> 48 (1986), </volume> <pages> pp. 479-498. </pages>
Reference-contexts: It is impossible in general to estimate the number of iterations a preconditioned nonsymmetric iterative solver will take. Although upper bounds have been established for the number of conjugate gradient iterations needed for some simple problems with known eigenvalue distributions <ref> [3] </ref>, there are no realistic estimates for practical problems. Three additional complicating factors also arise. First, the targeted systems are nonsymmetric in value.
Reference: [4] <author> S. Barnard and H. Simon, </author> <title> A Fast Multilevel Implementation of Recursive Spectral Bisection for Partitioning Unstructured Problems, in Concurrency: Practice and Experience, </title> <booktitle> also in The Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 101-117. </pages>
Reference-contexts: This suggests using a global method to obtain an initial partitioning and then use a local method to refine the partitioning. Also, the graph itself can be simplified or coarsened to allow for faster partitioning <ref> [40, 4, 29] </ref>, which can greatly improve the speed of a SPECT algorithm on a large mesh. The general MLP algorithm is: 1. Refine/coarsen graph if desired. 2. while (not done) do 3. Let P = global_partitioning_algorithm (G) 4. Let P' = local_partitioning_cleanup_algorithm (G,P) 5. done = finished (P') 6.
Reference: [5] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst, </author> <title> Templates for 132 the Solution of Linear Systems: Buildings Blocks for Iterative Methods, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> first ed., </note> <year> 1994. </year>
Reference-contexts: A contribution of this dissertation is the HMPE parallel preconditioned iterative solver package. HMPE implements the CGSTAB [58] and GMRES [51] solver algorithms, the block Jacobi <ref> [5] </ref> and block SSOR [61] preconditioning algorithms, and five factorization schemes for the diagonal (intraprocessor) blocks of A. <p> HMPE performance results are given in Chapter 4. The rest of this chapter is organized as follows: Section 3.2 describes some preconditioned linear system solution methods, primarily taken from <ref> [5, 50] </ref>. Section 3.3 describes some of the matrix data structures commonly used, Section 3.4 defines and describes the use of computational kernels in HMPE.
Reference: [6] <author> R. Blau, </author> <title> Performance Evaluation for Computer Image Synthesis Systems, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of California - Berkeley, </institution> <month> December </month> <year> 1992. </year> <note> Available as UCB Tech. Rep. CSD-93-736. </note>
Reference-contexts: Since is assumed to be made up of a list of high-level kernels, which are in turn made up of low-level kernels, the estimate of the time p l 's spends running is the sum of the low-level kernel estimates E. 2.3.4 Related Work Blau's <ref> [6] </ref> work uses a run-time estimate as input to a partitioning algorithm used by a computer rendering system. This work used previous timing results to predict future timing results; a natural choice for a frame-by-frame renderer, where the input changes a small amount from frame to frame.
Reference: [7] <author> R. Bramley and T. Loos, EMILY: </author> <title> A Visualization Tool for Large Sparse Matrices, </title> <type> Tech. Rep. TR 412A, </type> <institution> Indiana University Computer Science Department, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Finally, the AET is the average of all process TPI estimates. 4.4 HMPE Performance Results The two example matrices are shown in Figures 4.2 and 4.3 by means of the visualization tool Emily <ref> [7] </ref>. The matrices arise from computational fluid dynamics problems; the steady matrix is from a industrial 3D problem and has order 22 926 with 1 365 036 NN Z and the BFS matrix comes from a 2D test problem of order 20 284 and 452 752 N NZ.
Reference: [8] <author> R. Bramley and V. M. nkov, </author> <title> Low Rank Off-Diagonal Block Preconditioners for Solving Sparse Linear Systems on Parallel Computers, </title> <type> Tech. Rep. TR 446, </type> <institution> Indiana University Computer Science Department, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: Since BSSOR is a fairly robust preconditioner, it may be required to solve difficult systems regardless of cost. Parallelizable versions of BSSOR based on using low rank approximations of off-diagonal blocks combine the robustness of BSSOR with the parallelism of BDIAG <ref> [8] </ref>. 3.3 Data Structures As stated in Section 3.1, there are two usual ways to store a matrix: dense and sparse. Dense structures are easier to use from a programmer's point of view, but use more memory and CPU cycles in computing the result zero elements.
Reference: [9] <author> R. Bramley and X. Wang, SPLIB: </author> <title> A Library of Iterative Methods for Sparse Linear Systems, </title> <type> Tech. Rep. 454, </type> <institution> Indiana University-Bloomington, Bloomington, </institution> <note> IN 47405, </note> <year> 1995. </year>
Reference-contexts: The high-level kernels provide the solver interface to the low level kernels and perform estimation if necessary. Low-level kernels are generally simple loops or wrappers for library (i.e. BLAS, SPLIB <ref> [9] </ref>, or MPI) routines. For example, HMPE has evolved from a non-blocked uniprocessor solver to a blocked parallel solver. Originally, the high-level kernels were written as wrappers for calls to underlying kernels, which led to a non-blocked uniprocessor code. <p> (r); fi = temp / rnorm; rnorm = temp; axpy2 (fi; r; d); k = k + 1; endwhile end B.2 The Bi-Conjugate Gradient Stabilized (CGSTAB) Algorithm This statement and the GMRES statement are based on the HMPE code, both of which are in turn based on the SPLIB library <ref> [9] </ref> code.
Reference: [10] <author> T. Bui, C. Heigham, C. Jones, and T. Leighton, </author> <title> Improving the Performance of the Kernighan-Lin and Simulated Annealing Graph Bisection Algorithms, </title> <booktitle> in Proceedings of the 26th ACM/IEE Design Automation Conference, </booktitle> <year> 1989, </year> <pages> pp. 775-781. </pages>
Reference-contexts: KL's performance is heavily dependent on the given initial partitioning. It is frequently used to help "clean up" global methods [33, 29]. Other improvements and modifications have been made to KL which have made it the "recognized champion among classical approaches to the graph bisection problem" <ref> [10] </ref>. 1.4.3 Linear (LIN) and Scattered (SCAT) Graph Partition ing Methods Both the LIN and SCAT methods are based on simple assignment based on the numerical ordering of the nodes. <p> In this problem, SA's cost function is a partition's number of edges cut. SA can be shown to converge to an optimal solution with probability approaching 1. [37, p. 868]. To practically test this claim, several authors have compared SA with KL and other algorithms [37] [38] <ref> [10] </ref> [26] and have generally found that SA gives reasonably good partitionings in practice but is sensitive to the values of the initial temperature and control values of the temperature schedule, and has considerably longer run times than the other algorithms.
Reference: [11] <author> T. N. Bui and C. Jones, </author> <title> Finding Good Approximate Vertex and Edge Parti--tions is NP-Hard, </title> <journal> Information Processing Letters, </journal> <volume> 42 (1992), </volume> <pages> pp. </pages> <month> 153-159. </month> <title> [12] , A Heuristic for Reducing Fill In Sparse Matrix Factorization, </title> <booktitle> Proceedings of the Sixth SIAM Conference on Parallel Processing, </booktitle> <year> (1993), </year> <pages> pp. 445-452. </pages>
Reference-contexts: The resulting problem is called the graph partitioning problem, which is important in a number of fields [28], including VLSI layout, run-time scheduling for parallel processing, and, as indicated, the solution of linear systems on parallel computers. Since it is an NP-hard problem <ref> [11] </ref>, many heuristic approaches to the general graph partitioning problem have been proposed. After a data partitioning is generated, the problem of mapping the partitions onto processors must be solved.
Reference: [13] <author> C. Ashcraft and J.W.H. Liu, </author> <title> Using Domain Decomposition to Find Graph Bisectors, </title> <type> Tech. Rep. </type> <institution> CS-95-08, York University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Graph-based metrics for quality other than the number of edges cut have been proposed (see Ashcroft and Liu <ref> [13] </ref> and Rothberg [23] for discussions and comparisons of these metrics.) During the mapping phase, the number of edges cut can also be weighted to allow for variable interprocessor communications costs caused by interprocessor connection topology.
Reference: [14] <institution> Center for Innovative Computer Applications, Indiana University, </institution> <note> A Guide to the Paragon XP/S-A7 Supercomputer at Indiana Univerity. http://www.cica.indiana.edu/iu hpc/paragon/paragon.text.html (Visited June, </note> <year> 1996), </year> <month> October </month> <year> 1994. </year>
Reference-contexts: They are interconnected using a version SP High Performance Switch, a multi-stage packet switched Omega switch providing about 80 MB/second bandwidth with a minimum of 4 paths between nodes. The Intel Paragon used <ref> [14] </ref> is a model XP/S-A7 with 92 general purpose and 4 I/O and service nodes connected in a 12 fi 8 mesh. <p> HMPE is implemented in C++ using the MPI [56] communication library and has been tested on the SGI Power Challenge [54] shared memory and the IBM SP-2 [35] and Intel Paragon <ref> [14] </ref> distributed memory computers. The solvers in HMPE are coded in terms of a small number of kernels listed in 40 Appendix A. In HMPE, kernels are further broken down into high-level kernels and low-level kernels, which allows HMPE to use the modeling strategy described in Section 2.3.
Reference: [15] <author> P. Cousot, </author> <title> Abstract Interpretation, </title> <journal> ACM Computing Surveys, </journal> <volume> 28 (1996), </volume> <pages> pp. 324-328. </pages>
Reference-contexts: Both model fairly simple programs whose task graphs are known in advance. The static estimation portions of this strategy (i.e. the timing models) can be viewed as a case of abstract interpretation as applied to resource estimation for parallel programs. Abstract interpretation <ref> [15] </ref> is a general term for a compile-time analysis of a function or program used for estimating its properties. The term originated with researchers in functional and logic programming interested in type checking and semantics.
Reference: [16] <author> D. E. Culler et al, </author> <title> LogP: A Practical Model of Parallel Computation, </title> <journal> Communications of the ACM, </journal> <year> (1996), </year> <pages> pp. 78-85. </pages>
Reference-contexts: linearly related to the number of values (nvals) transmitted; that is, t comm = t overhead + nvals fi t gap (1.1) where t overhead is the cost to initiate communication between processors, including message buffer loading and unloading, and t gap is the average cost to transmit one value <ref> [16] </ref>.
Reference: [17] <author> E. D. Dahl, </author> <title> Mapping and Compiled Communication on the Connection Machine System, </title> <booktitle> in Proceedings of the Fifth Distributed Memory Computer Conference, </booktitle> <editor> D. W. Walker and Q. F. Stout, eds., </editor> <publisher> Los Alamitos, </publisher> <address> CA, April 1990, </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 134 </pages>
Reference-contexts: SA has also been used to solve the mapping problem [60] <ref> [17] </ref>.
Reference: [18] <author> J. J. Dongarra, S. W. Otto, M. Snir, and D. Walker, </author> <title> An Introduction to the MPI Standard, </title> <type> Tech. Rep. </type> <address> http://www.osc.edu/lam.html (Visited Octo-ber, </address> <year> 1996), </year> <institution> The University of Tennessee-Knoxville, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: A distributed shared memory processor replaces each node of a distributed memory computer with a shared memory multiprocessor. Both of these types of hybrid shared/distributed memory computers are expected to be more widespread in the future. 2.2 The MPI Standard MPI is a communication and synchronization interface standard <ref> [56, 18] </ref> for parallel computers based on message passing used in the HMPE solver. It is the dominant parallel computing interface standard, replacing Parallel Virtual Machine (PVM) [1]. 2.2.1 What is MPI? MPI is a standard defining a parallel communications paradigm it is not a implementation.
Reference: [19] <author> I. Duff and G. Meurant, </author> <title> The Effect of Ordering on Preconditioned Conjugate Gradients, </title> <journal> BIT, </journal> <volume> 29 (1989), </volume> <pages> pp. 635-657. </pages>
Reference-contexts: In many cases a preconditioner can actually increase the number of iterations required. Finally, each partitioning implicitly defines a reordering of the matrix, with subsequent changes in the order of operations and quality of preconditioning <ref> [19] </ref>. Table 4.1 1 shows the number of iterations required by a uniprocessor implementation of CGSTAB for the matrix sherman3 from the Harwell-Boeing collection of test matrices. Only the order of summation used in computing the dense dot products was varied; the matrix partitioning and matrix-vector products were fixed.
Reference: [20] <author> I. S. Duff, A. M. Erisman, and J. K. Reid, </author> <title> Direct Methods For Sparse Matrices, </title> <publisher> Oxford University Press, </publisher> <address> New York, NY, </address> <note> first ed., </note> <year> 1986. </year>
Reference-contexts: Section 3.3 describes some of the matrix data structures commonly used, Section 3.4 defines and describes the use of computational kernels in HMPE. Section 3.5 concludes and summarizes the chapter. 3.2 Linear System Solution Methods There are two classes of linear systems solution methods: direct and iterative methods <ref> [20, 50] </ref>. Direct methods factor A into A = LU , where L is lower triangular, and U is upper triangular and then solve Ax = (LU )x = b. During factorization, the 41 entries of L and U overwrite the entries of A. <p> However, no single iterative solver guarantees convergence for a general A. Direct methods that operate on sparse matrices have been developed <ref> [20] </ref>, but this dissertation will concentrate on their use as preconditioning methods for iterative solvers. Preconditioning is used to increase the probability an iterative solver will converge by transforming A to an easier system to solve.
Reference: [21] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> Users Guide for the Harwell-Boeing Sparse Matrix Collection (Release I), </title> <type> Tech. Rep. </type> <institution> TR/PA/92/86, Cedex and Boeing Computer Services, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The distribution assigns each process ownership of one or more block rows/vector blocks in a cyclic fashion based on NPROCS. The matrix A is input, either from a file in Harwell-Boeing format <ref> [21] </ref> or from an internal routine partsev that generates finite difference operator matrices, depending on the control parameter listed above. The matrix input routine uses the partition and distribution to input A, one block row at a time by its owning process, and determine communications requirements.
Reference: [22] <author> L. C. Dutto, W. G. Habashi, and M. Fortin, </author> <title> Parallelizable Block Diagonal Preconditioners for the Compressible Navier-Stokes Equations, </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, 117 (1994), </booktitle> <pages> pp. 15-47. </pages>
Reference-contexts: Most current work in solver estimation targets solvers with limited applicability, such as direct methods [27] or simple iterative methods with block diagonal preconditioners <ref> [22] </ref>. Unfortunately, in practice, those solver algorithms either require more memory than is currently available, or simply fail to converge for realistic 3D problems. Some popular solver algorithms, whose performance is assumed to be dominated by kernel execution times, are described in Chapter 3. 3.
Reference: [23] <author> E. Rothberg, </author> <title> Exploring the Tradeoff Between Imbalance and Separator Size in Nested Dissection Ordering, </title> <type> Tech. Rep. </type> <note> http://reality.sgi.com/employees/rothberg asd/ndimbal.ps (Visited October, </note> <year> 1996), </year> <title> Silicon Graphics, </title> <publisher> Inc., </publisher> <month> January </month> <year> 1996. </year> <month> 135 </month>
Reference-contexts: Graph-based metrics for quality other than the number of edges cut have been proposed (see Ashcroft and Liu [13] and Rothberg <ref> [23] </ref> for discussions and comparisons of these metrics.) During the mapping phase, the number of edges cut can also be weighted to allow for variable interprocessor communications costs caused by interprocessor connection topology.
Reference: [24] <author> V. Faber and T. Manteuffel, </author> <title> Necessary and Sufficient Conditions for the Existence of a Conjugate Gradient Method, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21 (1984), </volume> <pages> pp. 352-361. </pages>
Reference-contexts: The implementation of the unprecondi-tioned CG algorithm above uses only three additional vectors: d; r; and w. One of the three properties of orthogonality, minimization, and short recurrences must be 45 given up for all but trivial non-SPD matrices <ref> [24] </ref>. Both the GMRES and CGSTAB algorithms described below are applicable for general, nonsymmetric, non positive definite matrices; GMRES gives up short recurrences, and CGSTAB gives up the minimization property. 3.2.1.2 The GMRES Algorithm.
Reference: [25] <author> C. M. Fiduccia and R. M. Mattheyses, </author> <title> A Linear Time Heuristic for Improving Network Partitions, </title> <booktitle> in Proceedings of the 19th ACM/IEEE Design Automation Conference, </booktitle> <year> 1982, </year> <pages> pp. 175-181. </pages>
Reference-contexts: The original KL algorithm has O (V 2 log V) runtime [41]. Versions of KL with O (V) runtime have been found by both Fiduccia and Mattheyes <ref> [25] </ref> and Shiraishi and Hirose [53] both algorithms achieved the linear run time by use of clever data structures to keep careful track of the gain values and the vertices whose gain values needed updating. 15 This is the most commonly used local graph partitioning algorithm; that is, it looks at
Reference: [26] <author> S. W. Hammond, </author> <title> Mapping Unstructured Grid Computations to Massively Parallel Computers, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rensselaer Poly-techic Institute, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: C can also be thought of as a processor graph. Assume each processor in C is running at most one process, so P N T is a simplified version of the processor graph discussed in <ref> [26] </ref>. <p> Since it is an NP-hard problem [11], many heuristic approaches to the general graph partitioning problem have been proposed. After a data partitioning is generated, the problem of mapping the partitions onto processors must be solved. The mapping problem, which corresponds to the graph embedding problem, is NP-complete <ref> [26, p. 2] </ref>, so heuristic approaches are also used to solve the mapping problem. 1.4 Summary of Graph Partitioning Algorithms The current solution to the data distribution problem method is to use a graph partitioning algorithm to divide the data, then apply a mapping algorithm to map the partition sets onto <p> Six 11 graph partitioning methods are presented below which were used for thesis results. 1.4.1 Graph Partitioning Definitions The following definitions are taken from those given in [33, 31], and primarily from <ref> [26, pp. 1-39] </ref>. 1.4.1.1 General Graph Theoretical Definitions. A graph G is defined as G = (V; E), where V is a set of vertices, V = jV j, and E ae V fi V is the set of edges with v 1 ; v 2 2 E. <p> Hendrickson and Leland [29] use a multilevel approach (described below) to improve the quality of partitioning generated by the spectral method. This method has been found <ref> [33, 30, 26, 60] </ref> to give good partitionings; in fact, SPECT gives the lowest number of edges cut partitionings of any purely global method. <p> In this problem, SA's cost function is a partition's number of edges cut. SA can be shown to converge to an optimal solution with probability approaching 1. [37, p. 868]. To practically test this claim, several authors have compared SA with KL and other algorithms [37] [38] [10] <ref> [26] </ref> and have generally found that SA gives reasonably good partitionings in practice but is sensitive to the values of the initial temperature and control values of the temperature schedule, and has considerably longer run times than the other algorithms.
Reference: [27] <author> M. T. Heath, E. Ng, and B. W. Peyton, </author> <title> Parallel Algorithms for Sparse Linear Systems, in Parallel Algorithms for Matrix Computations, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991, </year> <pages> pp. 83-124. </pages>
Reference-contexts: Representation of the solver should minimally include the amount and 23 range of data values, kernel operations of the solver and the execution time of those kernels. Most current work in solver estimation targets solvers with limited applicability, such as direct methods <ref> [27] </ref> or simple iterative methods with block diagonal preconditioners [22]. Unfortunately, in practice, those solver algorithms either require more memory than is currently available, or simply fail to converge for realistic 3D problems.
Reference: [28] <author> B. Hendrickson and R. Leland, </author> <title> An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations, </title> <type> Tech. Rep. SAND 92-1460, </type> <institution> Sandia National Laboratories, </institution> <month> September </month> <year> 1992. </year> <title> [29] , A Multilevel Algorithm for Partitioning Graphs, </title> <type> Tech. Rep. SAND 93-1301, </type> <institution> Sandia National Laboratories, </institution> <month> October </month> <year> 1993. </year> <title> [30] , Multidimensional Spectral Load Balancing, </title> <type> Tech. Rep. SAND 93-0071, </type> <institution> San-dia National Laboratories, </institution> <month> January </month> <year> 1993. </year> <title> 136 [31] , The Chaco User's Guide Version 1.0, </title> <type> Tech. Rep. SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: A fair amount of research has been put into the general problem of partitioning data, assuming the data are representable as a graph. The resulting problem is called the graph partitioning problem, which is important in a number of fields <ref> [28] </ref>, including VLSI layout, run-time scheduling for parallel processing, and, as indicated, the solution of linear systems on parallel computers. Since it is an NP-hard problem [11], many heuristic approaches to the general graph partitioning problem have been proposed.
Reference: [32] <author> M. Hestenes and E. </author> <title> Stiefel, Methods of Conjugate Gradients for Solving Linear Systems, </title> <journal> Journal of Research National Bureau of Standards, </journal> <volume> 49 (1952), </volume> <pages> pp. 409-436. </pages>
Reference-contexts: HMPE supports two block preconditioning algorithms. The implementation assumes 42 each diagonal block is factored using incomplete LU factorization methods first de-scribed in [61]. 3.2.1 Iterative Solvers Three solver algorithms are described below: the Conjugate Gradient (CG) <ref> [32] </ref>, Generalized Minimum Residual (GMRES) [51], and Conjugate Gradient Stabilized (CGSTAB) [58]. Iterative solvers generate residual sequences that hopefully converge toward the true solution x: These sequences are written as fr i g; i 0. <p> See [50] for a good reference on iterative solvers. 43 3.2.1.1 The Conjugate Gradient (CG) Algorithm . The CG algorithm <ref> [32] </ref> as-sumes the input matrix A and the preconditioning matrix M are both symmetric positive definite (SPD). The method generates approximations to the solution x i , corresponding residual vectors r i , and search vectors d i .
Reference: [33] <author> Y. F. Hu and R. J. Blake, </author> <title> Numerical Experiences with Partitioning of Unstructured Meshes, </title> <booktitle> Parallel Computing, 20 (1994), </booktitle> <pages> pp. 815-829. </pages>
Reference-contexts: Graph partitioning algorithms as applied to the data distribution problem commonly attempt to minimize the number of edges cut. Six 11 graph partitioning methods are presented below which were used for thesis results. 1.4.1 Graph Partitioning Definitions The following definitions are taken from those given in <ref> [33, 31] </ref>, and primarily from [26, pp. 1-39]. 1.4.1.1 General Graph Theoretical Definitions. <p> KL's performance is heavily dependent on the given initial partitioning. It is frequently used to help "clean up" global methods <ref> [33, 29] </ref>. <p> This description is paraphrased from <ref> [33, pp. 817-821] </ref>. Let the mesh to be partitioned be described by G = (V; E). Assume the V vertices in the mesh are numbered 1 through V. <p> Hendrickson and Leland [29] use a multilevel approach (described below) to improve the quality of partitioning generated by the spectral method. This method has been found <ref> [33, 30, 26, 60] </ref> to give good partitionings; in fact, SPECT gives the lowest number of edges cut partitionings of any purely global method.
Reference: [34] <author> K. Hwang and F. A. Briggs, </author> <title> Computer Architecture and Parallel Processing, </title> <publisher> Mc-Graw Hill Book Company, </publisher> <address> New York, NY, </address> <note> first ed., </note> <year> 1984. </year>
Reference-contexts: significantly alters the cost per operation. t est is for N items defined by: 78 t est = &gt; &gt; &gt; &lt; t small ; N S minft limit ; (St small + (N S)t large )=N g; N &gt; S: This formula is derived from the standard performance equation <ref> [34, p. 57] </ref> of a two-level memory hierarchy: t est = Ht small + (1 H)t large (4.1) where H is the hit ratio for the cache. For a data item that completely fits in the cache (i.e.
Reference: [35] <author> International Business Machines Corporation, </author> <title> The RS/6000 SP Specification Sheet, </title> <type> Tech. Rep. </type> <note> http://www.austin.ibm.com/cgi-bin/systems/sp2.pl (Visited October, 1996), </note> <institution> International Business Machines Corporation, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: HMPE is implemented in C++ using the MPI [56] communication library and has been tested on the SGI Power Challenge [54] shared memory and the IBM SP-2 <ref> [35] </ref> and Intel Paragon [14] distributed memory computers. The solvers in HMPE are coded in terms of a small number of kernels listed in 40 Appendix A.
Reference: [36] <author> R. Jain, </author> <title> The Art of Computer Systems Performance Analysis: Techniques for Measurement, Simulation, and Modeling, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <note> first ed., </note> <year> 1991. </year>
Reference-contexts: The model does not require detailed hardware specifications or assembly-level run-time traces. It is fundamentally influenced by the ideas given in <ref> [36, pp. 30-33] </ref> to give a model that combines analysis, simulation, and direct measurements. 2.3.1 Machine Definition A machine M running a parallel program is defined to be the total parallel execution environment used to execute .
Reference: [37] <author> D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Schevon, </author> <title> Optimization by Simulated Annealing: An Experimental Evaluation; Part I, Graph Partitioning, </title> <journal> Operations Research, </journal> <volume> 37 (1989), </volume> <pages> pp. 865-892. 137 </pages>
Reference-contexts: SA is "motivated by an analogy to the behavior of physical systems in the presence of a heat bath" <ref> [37, p. 865] </ref>. <p> Each temperature value is used for a fixed number of iterations (the temperature's "length"). In this problem, SA's cost function is a partition's number of edges cut. SA can be shown to converge to an optimal solution with probability approaching 1. <ref> [37, p. 868] </ref>. <p> In this problem, SA's cost function is a partition's number of edges cut. SA can be shown to converge to an optimal solution with probability approaching 1. [37, p. 868]. To practically test this claim, several authors have compared SA with KL and other algorithms <ref> [37] </ref> [38] [10] [26] and have generally found that SA gives reasonably good partitionings in practice but is sensitive to the values of the initial temperature and control values of the temperature schedule, and has considerably longer run times than the other algorithms.
Reference: [38] <author> A. B. Kahng, </author> <title> Fast Hypergraph Partition, </title> <booktitle> in Proceedings of the 26th ACM/IEE Design Automation Conference, </booktitle> <year> 1989, </year> <pages> pp. 762-766. </pages>
Reference-contexts: In this problem, SA's cost function is a partition's number of edges cut. SA can be shown to converge to an optimal solution with probability approaching 1. [37, p. 868]. To practically test this claim, several authors have compared SA with KL and other algorithms [37] <ref> [38] </ref> [10] [26] and have generally found that SA gives reasonably good partitionings in practice but is sensitive to the values of the initial temperature and control values of the temperature schedule, and has considerably longer run times than the other algorithms.
Reference: [39] <author> G. Karypis and V. Kumar, METIS: </author> <title> Unstructured Graph Partitioning and Sparse Matrix Ordering System, </title> <note> Version 2.0. http://www.cs.umn.edu/ karypis/metis/manual.ps (Visited October, </note> <year> 1996), </year> <month> August </month> <year> 1995. </year> <title> [40] , Multilevel k-way Partitioning Scheme for Irregular Graphs, </title> <type> Tech. Rep. </type> <institution> TR-95-064, University of Minnesota Computer Science Dept., </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: It is the "state of the art" partitioning algorithm [12, 63, 29], commonly used with SPECT along with graph coarsening as the global partitioning algorithm and KL as the local cleanup algorithm. Karypis and Kumar <ref> [40, 39] </ref> investigated and implemented greedy initial partitioning schemes which they state work better than SPECT. 1.5 A New Model for the Data Distribution Prob lem Although mathematically pleasing, the number of edges cut does not provide even a qualitatively correct relative measure of the costs of a data distribution, as
Reference: [41] <author> B. W. Kernighan and S. Lin, </author> <title> An Efficient Heuristic Procedure for Partitioning Graphs, </title> <journal> Bell Systems Technical Journal, </journal> <volume> 49 (1970), </volume> <pages> pp. 291-307. </pages>
Reference-contexts: See Figure 1.3 for a diagram of the partitioning problem. Note that a partitioning method can be thought of as generating either a bipartitioning or a partition graph. 1.4.2 The Kernighan-Lin (KL) Partitioning Algorithm The key idea of the Kernighan-Lin <ref> [41] </ref> algorithm (KL) is to look at the gain in moving a vertex from one partition to the other. <p> The algorithm proceeds by swapping pairs of elements with the best net change in gain until either the total gain of the system is 0 or a predefined maximum number of iterations has been reached. The original KL algorithm has O (V 2 log V) runtime <ref> [41] </ref>.
Reference: [42] <author> B. Lisper and J. Collard, </author> <title> Extent Analysis of Data Fields, </title> <type> Tech. Rep. </type> <institution> TRITA-IT-9403, Swedish Royal Institute of Technology, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The term originated with researchers in functional and logic programming interested in type checking and semantics. Other program resources can be determined using abstract interpretation techniques, such as the memory utilization of the matrix-matrix multiplication operation <ref> [42] </ref>. 2.3.5 Critique of this Performance Modeling Strategy The performance modeling strategy described above is applicable to systems with a moderate number of pre-defined kernel operations.
Reference: [43] <author> T. Loos and R. Bramley, </author> <title> MPI Performance on the SGI Power Challenge, </title> <booktitle> in Proceedings of the Second MPI Developer's Conference, IEEE Computer Society Technical Committee on Distributed Processing, </booktitle> <year> 1996, </year> <pages> pp. 203-206. </pages>
Reference-contexts: The collective timing model models binary collective communications algorithms by scaling a cached timing model estimate with a value proportional to the logarithm of the number of processors executing the kernel and is further described in <ref> [43] </ref>. A general piecewise-linear model is used when the other two models do not apply.
Reference: [44] <author> B. Noble and J. W. Daniel, </author> <title> Applied Linear Algebra, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <note> second ed., 1977. 138 </note>
Reference-contexts: LU factorization is based on Gaussian elimination, while retaining the multipliers in the L matrix and the pivots are scaled and stored along the diagonal of the factored matrix <ref> [44] </ref>. To outline the LU factorization process, let A = L 0 + D + U 0 , with L 0 the strictly lower triangular part of A, U 0 the strictly upper triangular part of A, and D the diagonal of A.
Reference: [45] <author> Ohio Supercomputing Center, </author> <title> MPI Primer/Developing with LAM, </title> <type> Tech. Rep. </type> <institution> ftp://ftp.osc.edu/pub/lam/lam60.doc.ps, The Ohio State University, </institution> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: Standardization allows for portable code between different parallel hardware platforms and between different MPI implementations. The wide acceptance of the standard is due in part to its readability [56]. Free implementations 31 <ref> [59, 45] </ref> are available that run on most parallel platforms along with native imple-mentations for Cray, IBM, and SGI computers, 2 to name a few.
Reference: [46] <author> M. Papakhian, </author> <title> Hardware Configuration of the STARRS/SP Systems. </title> <note> Document available at http://browndwarf.ucs.indiana.edu/STARRS.hardware.html (Visited October, </note> <year> 1996), </year> <month> October </month> <year> 1996. </year>
Reference-contexts: The distributed computers used for testing the dissertation code are the IBM SP2 and the Intel Paragon. The SP2 can be configured to use between 2 and 512 CPUs. The SP-2 used <ref> [46] </ref> for the results presented below has 24 RS/6000 SP Thin-2 Node processors. Each processor has a 67MHz clock with a two- or three-tier RAM hierarchy, depending on configuration.
Reference: [47] <author> R. Parsons and D. Quinlan, </author> <title> Run-time Recognition of Task Parallelism Within the P++ Parallel Array Class Library, </title> <booktitle> in Proceedings of the Workshop of Scalable Libraries Conference, </booktitle> <institution> Mississippi State University, </institution> <year> 1993. </year>
Reference-contexts: A high-level kernel is assumed to consist of calls to one or more of K low-level kernels, each running on some input. This breakdown of high and low level kernels is based on a coding strategy described in <ref> [47] </ref> of parallel high-level kernels comprised of calls to (uniprocessor) low-level kernels. <p> Stating a solver in terms of kernel operations on matrix/vector data structures (implemented 67 as classes in C++) allows for changing kernel or class implementation while maintain-ing the solver code <ref> [47] </ref>. In HMPE, the kernels listed in Appendix A are considered high-level kernels which are in turn implemented in terms of other high-level kernels and low-level kernels. The high-level kernels provide the solver interface to the low level kernels and perform estimation if necessary.
Reference: [48] <author> A. Pothen, H. D. Simon, and K.-P. Liou, </author> <title> Partitioning Sparse Matrices with Eigenvalues of Graphs, </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 11 (1990), </volume> <pages> pp. 430-452. </pages>
Reference-contexts: The graph G cut = ([P 1 cut [ P 2 is bipartite and Pothen et. al. <ref> [48] </ref> use an algorithm for calculating the minimum cover of the bipartite graph to find an approximate minimum edge cut. Hendrickson and Leland [29] use a multilevel approach (described below) to improve the quality of partitioning generated by the spectral method.
Reference: [49] <author> P. C. Robertson, </author> <title> Visualizing Color Gamuts: A User Interface for the Effective Use of Perceptual Color Spaces in Data Displays, </title> <journal> Computer Graphics and Applications, </journal> <volume> 8 (1988), </volume> <pages> pp. 50-64. </pages>
Reference-contexts: BDIAG exhibits super-linear speedup because the matrix blocks fit into the caches of the individual processors, leading to faster access times. The parallelism inhibiting effects of BSSOR's triangular solves are 83 4.3 are shown partitioned into 8 block rows using linear partitioning with Robertson's <ref> [49] </ref> color map. 84 85 (top) and BFS (bottom) using BDIAG (left) and BSSOR (right) preconditioning. Super-linear speedup for BDIAG is due to cache effects. Actual results are labeled Act and estimation results are labeled Est. 86 (top) and BFS (bottom) using BDIAG (left) and BSSOR (right) preconditioning.
Reference: [50] <author> Y. Saad, </author> <title> Iterative Methods for Sparse Linear Systems, </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, MA, </address> <note> first ed., 1996. 139 </note>
Reference-contexts: In this case, the data distribution problem is the problem of determining the data layout or distribution among the processors so that the iterative linear system solver execution time is minimized. It is usually viewed as a specific instance of the graph partitioning problem <ref> [50] </ref>. The data distribution problem is to minimize the amount of time solving a sparse linear system on a parallel computer given the following: 1. A (large) sparse linear system Ax = b, with a n fi n matrix A and a n fi 1 vector b given. <p> HMPE performance results are given in Chapter 4. The rest of this chapter is organized as follows: Section 3.2 describes some preconditioned linear system solution methods, primarily taken from <ref> [5, 50] </ref>. Section 3.3 describes some of the matrix data structures commonly used, Section 3.4 defines and describes the use of computational kernels in HMPE. <p> Section 3.3 describes some of the matrix data structures commonly used, Section 3.4 defines and describes the use of computational kernels in HMPE. Section 3.5 concludes and summarizes the chapter. 3.2 Linear System Solution Methods There are two classes of linear systems solution methods: direct and iterative methods <ref> [20, 50] </ref>. Direct methods factor A into A = LU , where L is lower triangular, and U is upper triangular and then solve Ax = (LU )x = b. During factorization, the 41 entries of L and U overwrite the entries of A. <p> The transpose-free variation of Lanc-zos bi-orthogonalization used in CGSTAB generates residual vectors of the form r k = k (A)OE k (A)r 0 , where k and OE k are polynomials in A. See <ref> [50] </ref> for a good reference on iterative solvers. 43 3.2.1.1 The Conjugate Gradient (CG) Algorithm . The CG algorithm [32] as-sumes the input matrix A and the preconditioning matrix M are both symmetric positive definite (SPD). <p> M is assumed to be generated using an incom-plete LU factorization of A; that is, M = LU where L is a unit lower triangular matrix and U is an upper triangular matrix. There are a variety of methods to incompletely factor a matrix <ref> [50, pp. 265-298] </ref>, but only the ILU (s) factorization method was used as described below. Setup costs are not included in the timing results. Preconditioner application is the calculation of z = M 1 y by solving the system M z = y for z. <p> For the ILU (s) factorization process, the decision to keep a fill entry is determined by an entry's fill level lev ij . The definition <ref> [50, p.280] </ref> of the initial fill level lev ij of an element a ij of a sparse matrix A is lev ij = &gt; &gt; &gt; &lt; 0 if a ij 6= 0 or i = j 1 otherwise Every time a ij is modified by Equation 3.2 during the LU
Reference: [51] <author> Y. Saad and M. Schultz, </author> <title> GMRES: A Generalized Minimal Residual Algo--rithm for Solving Nonsymmetric Linear Systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 856-869. </pages>
Reference-contexts: A contribution of this dissertation is the HMPE parallel preconditioned iterative solver package. HMPE implements the CGSTAB [58] and GMRES <ref> [51] </ref> solver algorithms, the block Jacobi [5] and block SSOR [61] preconditioning algorithms, and five factorization schemes for the diagonal (intraprocessor) blocks of A. <p> HMPE supports two block preconditioning algorithms. The implementation assumes 42 each diagonal block is factored using incomplete LU factorization methods first de-scribed in [61]. 3.2.1 Iterative Solvers Three solver algorithms are described below: the Conjugate Gradient (CG) [32], Generalized Minimum Residual (GMRES) <ref> [51] </ref>, and Conjugate Gradient Stabilized (CGSTAB) [58]. Iterative solvers generate residual sequences that hopefully converge toward the true solution x: These sequences are written as fr i g; i 0.
Reference: [52] <author> J. R. Shewchuk, </author> <title> An Introduction to the Conjugate Gradient Method Without the Agonizing Pain, </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-125, Carnegie Mellon University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: It can be shown that this choice of fi makes d i orthogonal to Ad j ; j = 0; 1; : : : ; i 1 and r i orthogonal to r j ; j = 0; 1; : : : ; i 1. See <ref> [52] </ref> for a lucid description of the CG algorithm, its underlying principles, and proofs of these results for the unpreconditioned algorithm.
Reference: [53] <author> H. Shiraishi and F. Hirose, </author> <title> Efficient Placement and Routing for Masterslice LSI, </title> <booktitle> in Proceedings of the 17th Design Automation Conference, </booktitle> <year> 1980, </year> <pages> pp. 458-464. </pages>
Reference-contexts: The original KL algorithm has O (V 2 log V) runtime [41]. Versions of KL with O (V) runtime have been found by both Fiduccia and Mattheyes [25] and Shiraishi and Hirose <ref> [53] </ref> both algorithms achieved the linear run time by use of clever data structures to keep careful track of the gain values and the vertices whose gain values needed updating. 15 This is the most commonly used local graph partitioning algorithm; that is, it looks at information from only a small
Reference: [54] <author> Silicon Graphics Inc., </author> <title> The Power Challenge Technical Report, </title> <type> Tech. Rep. </type> <address> http://www.sgi.com/Products/software/PDF/pwr-chlg (Visited June, </address> <year> 1996), </year> <title> Silicon Graphics, </title> <publisher> Inc., </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Generally, shared memory computers have a relatively small number of CPUs to minimize shared memory contention. They have better communications performance for those CPUs and are generally easier systems for code development than distributed memory computers. An example of this architecture type is the SGI Power Challenge <ref> [54] </ref>, which is equipped with either of the MIPS R8000 or MIPS R10000 CPU chips and up to 16 GB of global memory, connected by a 1.2 GB/second common bus that uses piggy-back reads. <p> The Power Challenge used for the disseration results has 10 75 MHz R8000 chips and a three tier RAM hierarchy: a 16 KB cache on the integer unit, which is 28 unused during floating point loads and stores <ref> [54, Ch. 3, p. 47] </ref>, a 4 MB second-level off-chip "data streaming" cache, and a 2 GB global shared memory. For larger problems, shared memory array computers can be used. A shared memory array is several shared memory computers by a relatively fast inter-processor network. <p> HMPE is implemented in C++ using the MPI [56] communication library and has been tested on the SGI Power Challenge <ref> [54] </ref> shared memory and the IBM SP-2 [35] and Intel Paragon [14] distributed memory computers. The solvers in HMPE are coded in terms of a small number of kernels listed in 40 Appendix A.
Reference: [55] <author> H. S. Stone, </author> <title> High Performance Computer Architecture, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second ed., </note> <year> 1990. </year>
Reference-contexts: The cached timing model implements a slightly modified version of the standard cache cost function <ref> [55, p. 38] </ref>, described in the next paragraph. Cached timing models are used for modeling the majority of kernels whose performance is influenced by memory performance.
Reference: [56] <author> The Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <type> Tech. Rep. </type> <institution> UT-CS-94-230, University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1994. </year> <title> 140 [57] , MPI-2: Extensions to the Message-Passing Interface. </title> <note> http://www.cs.wisc.edu/lederman/mpi2/mpi2-report.ps.Z (Visited October, </note> <year> 1996), </year> <month> October </month> <year> 1996. </year>
Reference-contexts: A distributed shared memory processor replaces each node of a distributed memory computer with a shared memory multiprocessor. Both of these types of hybrid shared/distributed memory computers are expected to be more widespread in the future. 2.2 The MPI Standard MPI is a communication and synchronization interface standard <ref> [56, 18] </ref> for parallel computers based on message passing used in the HMPE solver. It is the dominant parallel computing interface standard, replacing Parallel Virtual Machine (PVM) [1]. 2.2.1 What is MPI? MPI is a standard defining a parallel communications paradigm it is not a implementation. <p> Standardization allows for portable code between different parallel hardware platforms and between different MPI implementations. The wide acceptance of the standard is due in part to its readability <ref> [56] </ref>. Free implementations 31 [59, 45] are available that run on most parallel platforms along with native imple-mentations for Cray, IBM, and SGI computers, 2 to name a few. <p> HMPE implements the CGSTAB [58] and GMRES [51] solver algorithms, the block Jacobi [5] and block SSOR [61] preconditioning algorithms, and five factorization schemes for the diagonal (intraprocessor) blocks of A. HMPE is implemented in C++ using the MPI <ref> [56] </ref> communication library and has been tested on the SGI Power Challenge [54] shared memory and the IBM SP-2 [35] and Intel Paragon [14] distributed memory computers. The solvers in HMPE are coded in terms of a small number of kernels listed in 40 Appendix A.
Reference: [58] <author> H. van der Vorst, </author> <title> Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the Solution of Nonsymmetric Linear Systems, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 631-644. </pages>
Reference-contexts: A contribution of this dissertation is the HMPE parallel preconditioned iterative solver package. HMPE implements the CGSTAB <ref> [58] </ref> and GMRES [51] solver algorithms, the block Jacobi [5] and block SSOR [61] preconditioning algorithms, and five factorization schemes for the diagonal (intraprocessor) blocks of A. <p> HMPE supports two block preconditioning algorithms. The implementation assumes 42 each diagonal block is factored using incomplete LU factorization methods first de-scribed in [61]. 3.2.1 Iterative Solvers Three solver algorithms are described below: the Conjugate Gradient (CG) [32], Generalized Minimum Residual (GMRES) [51], and Conjugate Gradient Stabilized (CGSTAB) <ref> [58] </ref>. Iterative solvers generate residual sequences that hopefully converge toward the true solution x: These sequences are written as fr i g; i 0. <p> CGSTAB uses short recur rences and maintains a form of the orthogonality constraints called bi-orthogonality, but it does not keep the residual minimization property. The fast and smoothly con verging variant of the algorithm <ref> [58] </ref> is listed below.
Reference: [59] <author> W. Gropp and E. Lusk, </author> <title> User's Guide for MPICH, a Portable Implementation of MPI. </title> <note> http://www.mcs.anl.gov/mpi/mpiuserguide/paper.html (Visited October, </note> <year> 1996), </year> <month> February </month> <year> 1996. </year>
Reference-contexts: Standardization allows for portable code between different parallel hardware platforms and between different MPI implementations. The wide acceptance of the standard is due in part to its readability [56]. Free implementations 31 <ref> [59, 45] </ref> are available that run on most parallel platforms along with native imple-mentations for Cray, IBM, and SGI computers, 2 to name a few.
Reference: [60] <author> R. Williams, </author> <title> Performance of Dynamic Load Balancing Algorithms for Unstructured Mesh Calculations, </title> <journal> Concurrency, </journal> <volume> 3 (1991), </volume> <pages> pp. 457-481. </pages>
Reference-contexts: Hendrickson and Leland [29] use a multilevel approach (described below) to improve the quality of partitioning generated by the spectral method. This method has been found <ref> [33, 30, 26, 60] </ref> to give good partitionings; in fact, SPECT gives the lowest number of edges cut partitionings of any purely global method. <p> SA has also been used to solve the mapping problem <ref> [60] </ref> [17].
Reference: [61] <author> A. Yeremin and L. </author> <title> Kolotilina, On a Family of Two-Level Precondition-ings of the Incomplete Block Factorization Type, </title> <journal> Sov. J. Numer. Anal. Math. Modeling, </journal> <volume> 1 (1986), </volume> <pages> pp. 293-320. </pages>
Reference-contexts: A contribution of this dissertation is the HMPE parallel preconditioned iterative solver package. HMPE implements the CGSTAB [58] and GMRES [51] solver algorithms, the block Jacobi [5] and block SSOR <ref> [61] </ref> preconditioning algorithms, and five factorization schemes for the diagonal (intraprocessor) blocks of A. HMPE is implemented in C++ using the MPI [56] communication library and has been tested on the SGI Power Challenge [54] shared memory and the IBM SP-2 [35] and Intel Paragon [14] distributed memory computers. <p> Preconditioning is commonly performed by using a second matrix, M , known as the preconditioning matrix. HMPE supports two block preconditioning algorithms. The implementation assumes 42 each diagonal block is factored using incomplete LU factorization methods first de-scribed in <ref> [61] </ref>. 3.2.1 Iterative Solvers Three solver algorithms are described below: the Conjugate Gradient (CG) [32], Generalized Minimum Residual (GMRES) [51], and Conjugate Gradient Stabilized (CGSTAB) [58]. Iterative solvers generate residual sequences that hopefully converge toward the true solution x: These sequences are written as fr i g; i 0.
Reference: [62] <author> Zhichen Xu, Xiaodong Zhang, and Lin Sun, </author> <title> Semi-Empirical Multiprocessor Performance Predictions, </title> <type> Tech. Rep. </type> <institution> TR-96-05-01, University of Texas, San Antonio, High Performance Comp. and Software Lab., </institution> <year> 1996. </year>
Reference-contexts: It did not readily account for changes in the rendering algorithm or computational environment. Adve [2] and Xu, Zhang, and Sun <ref> [62] </ref> also use a modeling strategy based on combining empirical observations. They identify segments of a program by first locating communication and synchronization points and computing a task graph.
Reference: [63] <author> H. B. Zhou, </author> <title> Two Stage m-way Graph Partitioning, </title> <booktitle> Parallel Computing, 19 (1993), </booktitle> <pages> pp. 378-406. </pages> <note> Curriculum Vitae Thomas Loos was born in Omaha, Nebraska on August 31, 1964. He received his B.S. in Computer Science from the University of Nebraska in 1986 and his M.S. </note> <institution> in Computer Studies from North Carolina State University in 1987. </institution>
Reference-contexts: Undo any alterations made in Steps 1 and 6. This is the most general method available, since any global method and any local 22 method can be used in this algorithm and can both be iterative methods. It is the "state of the art" partitioning algorithm <ref> [12, 63, 29] </ref>, commonly used with SPECT along with graph coarsening as the global partitioning algorithm and KL as the local cleanup algorithm.
References-found: 57

