URL: ftp://ftp.idiap.ch/pub/reports/1997/rr97-17.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: E  IDIAP Martigny Valais Suisse Neural Network Adaptations to Hardware Implementations  
Author: E R H E R R P Perry Moerland Emile Fiesler 
Note: internet  published in E. Fiesler and R. Beale, editors, Handbook of Neural Computation, E1.2:1-13. Institute of Physics Publishing and  
Address: I  P.O.Box 592 Martigny Valais Switzerland  Publishing, New York.  
Affiliation: I  for Perceptive Artificial Intelligence  Oxford University  
Pubnum: IDIAP-RR 97-17  
Email: e-mail secretariat@idiap.ch  e-mail: Perry.Moerland@idiap.ch  
Phone: phone +41 27 721 77 11 fax +41 27 721 77 12  
Date: January 97  
Web: http://www.idiap.ch  
Abstract-found: 0
Intro-found: 1
Reference: [Abramson-93] <author> S. Abramson, D. Saad, and E. Marom. </author> <title> Training a Neural Network with Ternary Weights Using the CHIR Algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 997-1000, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: fi nite precision. [Grossman-90] 1 1 Adaptation of both weights and the internal representation of the neurons. [Reyneri-91] 9-10 1 1 Batch backpropagation with a near-optimum learning rate. [Xie-92] 10 - 2 Weight perturbation with gain adaptation. [Xie-92] 9 - 2 Combination of weight perturbation and a partial random search. <ref> [Abramson-93] </ref> 2 3 A slight modification of [Grossman-90] to train sparsely connected Heaviside networks. [Sakaue-93] 8-10 - 2 A weighted error function in the backpropagation algorithm based on an overestimation of the error. [Hollis-94] 13 1 Weight perturbation with an adaptive gain and learning rate. [Jabri-94] 6 1 1 Semi-parallel weight
Reference: [Ackley-85] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A Learning Algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169, </pages> <year> 1985. </year>
Reference-contexts: Associative memory, see for example [Hopfield-82] [Verleysen-89] 2 1 A linear programming learning algorithm for associative memories. [Johannet-92] 9-11 1 Integer arithmetics for learning in associative memory. [Hendrich-96] 1 1 Associative memory with binary weights and a good storage capacity. Boltzmann network <ref> [Ackley-85] </ref> [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights.
Reference: [Alspector-92] <author> J. Alspector, A. Jayakumar, and S. Luma. </author> <title> Experimental Evaluation of Learning in a Neural Microsystem. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS91), </booktitle> <volume> vol. 4, </volume> <pages> pp. 871-878, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1992. </year>
Reference-contexts: An interesting example of a hardware implementation is Bellcore's implementation of a Boltzmann machine and Mean-Field learning, which allows on-chip learning with only 5-bit weights <ref> [Alspector-92] </ref>. Recently, a weight discretization algorithm for an associative memory with binary f1,+1g weights has been implemented on a digital VLSI chip [Hendrich-96]. <p> Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. <ref> [Alspector-92] </ref> 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights. <p> Another characteristic of the Boltzmann Machine is the use of simulated annealing to gradually increase the gain of a neuron's activation function. In Bellcore's implementation of a Boltzmann Machine this annealing schedule has been replaced by a gradual decrease of additive noise <ref> [Alspector-92] </ref>, while the main idea of Mean Field Theory learning is to replace the annealing strategy by a deterministic approximation. 5 Summary and Conclusions In this section an overview has been given of a variety of adaptations of neural network learning to enable their successful hardware implementation.
Reference: [Alspector-93] <author> J. Alspector, R. Meir, B. Yuhas, and A. Jayakumar. </author> <title> A Parallel Gradient Descent Method for Learning in Analog VLSI Neural Networks. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS92), </booktitle> <volume> vol. 5, </volume> <pages> pp. 836-844, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1993. </year>
Reference-contexts: The sequential nature of these simple perturbation algorithms has led to more intricate variants which perform some of the calculations in parallel. A simultaneous perturbation of all weights is a promising alternative <ref> [Alspector-93] </ref> [Cauwenberghs-93], even when for a reliable estimate of the gradient the results of several perturbations should be averaged or a very small and accurate perturbation is required. Other variants use a semi-parallel perturbation scheme like chain-rule perturbation [Hollis-94], fan-out or fan-in-out perturbation [Jabri-94], and summed weight neuron perturbation [Flower-93].
Reference: [Annema-95] <author> A. J. Annema and H. Wallinga. </author> <title> Analog Weight Adaptation Hardware. </title> <journal> Neural Processing Letters, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 1-4, </pages> <year> 1995. </year>
Reference-contexts: The occurrence of additive offsets in the multiplications and especially in weight adaptations do pose serious problems which are not easily overcome by on-chip learning [Dolenko-95]. A possible solution is the use of some dedicated hardware in the weight adaptation circuitry which enables offset-compensation <ref> [Annema-95] </ref>. 3.2 Non-Ideal Response Computations performed in hardware are approximations of the mathematical operations assumed to be ideal in neural network models. This affects in particular the analog implementation of a linear multiplication and the implementation of a non-linear activation function like the widely-used standard sigmoid.
Reference: [Asanovic-91] <author> K. Asanovic and N. Morgan. </author> <title> Experimental Determination of Precision Requirements for BackPropagation Training of Artificial Neural Networks. </title> <booktitle> Proceedings of the Second International Conference Mi-croNeuro'91, </booktitle> <pages> pp. 9-15, </pages> <editor> (U. Ramacher, U. Ruckert, and J. A. Nossek, eds.), </editor> <address> Munchen, Germany, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: This can often be solved IDIAP-RR 97-17 4 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world <ref> [Asanovic-91] </ref> 16 - 1 Coarse weight quantization in the back propagation algorithm. [Holt-93] 14-16 2 An error analysis of backpropagation with fi nite precision. [Grossman-90] 1 1 Adaptation of both weights and the internal representation of the neurons. [Reyneri-91] 9-10 1 1 Batch backpropagation with a near-optimum learning rate. [Xie-92] 10
Reference: [Austin-94] <author> J. Austin. </author> <title> A Review of RAM Based Neural Networks. </title> <booktitle> Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 58-66, </pages> <address> Turin, Italy, </address> <month> September 26-28, </month> <year> 1994, </year> <note> ISBN 0-8186-6710-9. </note>
Reference-contexts: Another example is the class of RAM-based networks which can be easily implemented with standard available components. A recent overview of RAM-based networks and related implementation aspects is given in <ref> [Austin-94] </ref>. Various hardware-friendlier alternatives have been proposed for several neural network learning rules, especially with the objective to enable on-chip learning.
Reference: [Balzer-91] <author> W. Balzer, M. Takahashi, J. Ohta, K. Kyuma. </author> <title> Weight Quantization in Boltzmann Machines. </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 405-409, </pages> <year> 1991. </year>
Reference-contexts: Associative memory, see for example [Hopfield-82] [Verleysen-89] 2 1 A linear programming learning algorithm for associative memories. [Johannet-92] 9-11 1 Integer arithmetics for learning in associative memory. [Hendrich-96] 1 1 Associative memory with binary weights and a good storage capacity. Boltzmann network [Ackley-85] <ref> [Balzer-91] </ref> 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights.
Reference: [Battiti-94] <author> R. Battiti and G. Tecchiolli. TOTEM: </author> <title> A Digital Processor for Neural Networks and Reactive Tabu Search. </title> <booktitle> Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 17-25, </pages> <address> Turin, Italy, </address> <month> September 26-28, </month> <year> 1994, </year> <note> ISBN 0-8186-6710-9. </note>
Reference-contexts: Some of these weight discretization algorithms have already proven their usefulness in hardware implementations. Battiti's reactive tabu search, for example, has been implemented in the TOTEM processor and successfully applied to a triggering problem in high energy physics with a weight accuracy as low as 4 bits <ref> [Battiti-94] </ref>. <p> Designers of digital neurocomputers, for example, profit from the fact that the required weight accuracy for backpropagation training is around 16 bits [Mauduit-92]. An example of a successful implementation of a weight discretization algorithm is Battiti's TOTEM-chip which uses a weight accuracy of 4 bits <ref> [Battiti-94] </ref>. Compared to the state-of-the-art in digital neural network implementations, the design of analog neural network implementations with non-idealities like component non-uniformity, non-ideal responses, and system noise, is still in a more experimental state.
Reference: [Battiti-95] <author> R. Battiti and G. Tecchiolli. </author> <title> Training Neural Nets with the Reactive Tabu Search. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 5, </volume> <pages> pp. 1185-1200, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: 8-10 - 2 A weighted error function in the backpropagation algorithm based on an overestimation of the error. [Hollis-94] 13 1 Weight perturbation with an adaptive gain and learning rate. [Jabri-94] 6 1 1 Semi-parallel weight perturbation algorithms. [Simard-94] 16 - 1 Backpropagation without multiplication; gradients and states of power-of-two <ref> [Battiti-95] </ref> 1-8 1 2 Heuristic method for solving combinatorial optimization problems. [Dundar-95] 10 2 Backpropagation with forced weight updates.
Reference: [Beiu-95] <author> V. Beiu. </author> <title> VLSI Complexity of Discrete Neural Networks. </title> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: This scheme has also been applied to gradient values, activation values, and learning rates [Hollis-94] [Simard-94]. Work on limiting the number of weight levels has also been done in the design of Heaviside networks for the computation of boolean functions (majority, parity, comparison, addition) and for the two-spiral problem <ref> [Beiu-95] </ref> [Beiu-96.1]. Beiu's concern is to minimize the total number of bits required to represent the weights of a network, since this is a realistic measure of the complexity of VLSI implementations.
Reference: [Beiu-96.1] <author> V. Beiu. </author> <title> Direct Synthesis of Neural Networks. </title> <booktitle> Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 257-264, </pages> <address> Lausanne, Switzerland, </address> <month> February 12-14, </month> <year> 1996. </year>
Reference-contexts: Work on limiting the number of weight levels has also been done in the design of Heaviside networks for the computation of boolean functions (majority, parity, comparison, addition) and for the two-spiral problem [Beiu-95] <ref> [Beiu-96.1] </ref>. Beiu's concern is to minimize the total number of bits required to represent the weights of a network, since this is a realistic measure of the complexity of VLSI implementations.
Reference: [Beiu-96.2] <author> V. Beiu. </author> <title> Entropy Bounds for Classification Algorithms. </title> <booktitle> Neural Network World, </booktitle> <volume> vol. 6, </volume> <pages> pp. 497-505, </pages> <year> 1996. </year> <pages> IDIAP-RR 97-17 11 </pages>
Reference-contexts: Moreover, it opens up the possibility to compare results obtained by learning algorithms with the entropy (number of bits) upper bounds of the data set <ref> [Beiu-96.2] </ref>. Finally, we would like to point out that a comparative benchmarking study of quantization effects on different neural network models and the improvements that can be obtained by weight discretization algorithms has not yet been done.
Reference: [Brandt-94] <author> R. D. Brandt and F. Lin. </author> <title> Supervised Learning in Neural Networks without Explicit Error Back-Propagation. </title> <booktitle> Proceedings of the Thirty-Second Allerton Conference on Communication, Control, and Computing, </booktitle> <pages> pp. 294-303, </pages> <address> Monticello, Illinois, </address> <month> September 28-30, </month> <year> 1994. </year>
Reference-contexts: Although it is not a steepest descent rule, it is still guaranteed that the weights are updated in the descent direction. Another local learning rule has been developed in <ref> [Brandt-94] </ref> which uses only the rates of change of the outgoing weights of a neuron. One of their algorithms is mathematically equivalent to the backpropagation algorithm, but the measurement of the rates of change of the weights could be hard to implement.
Reference: [Cairns-94] <author> G. Cairns and L. Tarassenko. </author> <title> Learning with Analogue VLSI MLPs. </title> <booktitle> Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 67-76, </pages> <address> Turin, Italy, </address> <month> September 26-28, </month> <year> 1994, </year> <note> ISBN 0-8186-6710-9. </note>
Reference-contexts: In this section, examples of these non-idealities are presented, together with their effects on the learning behaviour of neural networks. 3.1 Component Non-Uniformity Variations between the on-chip components, such as multipliers <ref> [Cairns-94] </ref> and the read-out of optical weight matrices [Robinson-92], are inevitable in analog hardware. These non-uniformities are particularly troublesome when the training of the network is done off-chip without taking these component variations into account [Frye-91]. <p> This is also intuitively clear because the use of the analog circuit in the forward pass incorporates the non-uniformities in the learning process. This has been confirmed by experimental results, for example for on-chip learning in backpropagation networks <ref> [Cairns-94] </ref> [Dolenko-95]. Their research indicates that backpropagation learning can adapt to the non-uniformity of multiplier gains which are caused by fab IDIAP-RR 97-17 6 rication inaccuracies. <p> Therefore, simple non-linear multipliers are often preferable and are used in both electronic [Lont-92] [Hollis-94] [Reyneri-95] and optical implementations [Robinson-92] [Neiberg-94]. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. While in <ref> [Cairns-94] </ref> [Dolenko-95] the straightforward use of non-linear multipliers in simulations of on-chip learning in analog backpropagation networks leads to satisfactory results, in [Lont-92] the standard backpropagation algorithm fails to converge with non-linear synapses. <p> As can be seen in table 3, weight perturbation also has a good performance with limited precision weights [Xie-92]. Moreover, it is more robust against non-idealities occurring in analog hardware: non-uniformity, non-ideal circuit response, and noise <ref> [Cairns-94] </ref>. The reason for this is that in this algorithm modeling of activation functions and multipliers does not need to be done, since these form an integral part of the training algorithm.
Reference: [Campbell-95] <author> C. Campbell and C. Perez Vincente. </author> <title> The Target Switch Algorithm: A Constructive Learning Procedure for Feed-Forward Neural Networks. </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 6, </volume> <pages> pp. 1245-1264, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights. Cascade topology [Fahlman-90] [Hoehfeld-92] 12 2 1 Coarse weight quantization in the cascade cor relation algorithm. [Hoehfeld-92] 6 2 1 Cascade correlation with probabilistic round ing and variable gain. <ref> [Campbell-95] </ref> 1 2 1 A constructive algorithm for Heaviside cas cade networks. Table 4: Weight discretization in other neural network models. 3 Hardware Non-Idealities Both in analog electronic and optical neural network implementations, computation suffers from drawbacks which do not play an important role in digital hardware.
Reference: [Card-92] <author> H. C. Card and C. R. Schneider. </author> <title> Analog CMOS Neural Circuits In Situ Learning. </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 103-124, </pages> <year> 1992. </year>
Reference-contexts: These non-uniformities are particularly troublesome when the training of the network is done off-chip without taking these component variations into account [Frye-91]. It is, however, widely claimed that chip-in-the-loop or on-chip learning can compensate to a considerable extent for these non-uniformities <ref> [Card-92] </ref>. This is also intuitively clear because the use of the analog circuit in the forward pass incorporates the non-uniformities in the learning process. This has been confirmed by experimental results, for example for on-chip learning in backpropagation networks [Cairns-94] [Dolenko-95].
Reference: [Cauwenberghs-93] <author> G. Cauwenberghs. </author> <title> A Fast Stochastic Error-Descent Algorithm for Supervised Learning and Optimization. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS92), </booktitle> <volume> vol. 5, </volume> <pages> pp. 244-251, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1993. </year>
Reference-contexts: The sequential nature of these simple perturbation algorithms has led to more intricate variants which perform some of the calculations in parallel. A simultaneous perturbation of all weights is a promising alternative [Alspector-93] <ref> [Cauwenberghs-93] </ref>, even when for a reliable estimate of the gradient the results of several perturbations should be averaged or a very small and accurate perturbation is required. Other variants use a semi-parallel perturbation scheme like chain-rule perturbation [Hollis-94], fan-out or fan-in-out perturbation [Jabri-94], and summed weight neuron perturbation [Flower-93].
Reference: [Chua-88] <author> L. O. Chua and L. Yang. </author> <title> Cellular Neural Networks: </title> <journal> Theory. IEEE Transactions on Circuits and Systems, </journal> <volume> vol. 35, </volume> <pages> pp. 1257-1272, </pages> <year> 1988. </year>
Reference: [Chua-93] <author> L. O. Chua and T. Roska. </author> <title> The CNN Paradigm. </title> <journal> IEEE Transactions on Circuits and Systems-I: Fundamental Theory and Applications, </journal> <volume> volume 40, no. 3, </volume> <pages> pp. 147-156, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: An example of the second class are cellular neural networks which are of special interest for VLSI implementation because of their sparse local connectivity: every unit of the network is a simple analog processor that interacts only with its neighbouring units; see <ref> [Chua-93] </ref> for a survey. Another example is the class of RAM-based networks which can be easily implemented with standard available components. A recent overview of RAM-based networks and related implementation aspects is given in [Austin-94].
Reference: [Coggins-94] <author> R. Coggins and M. Jabri. Wattle: </author> <title> A Trainable Gain Analogue VLSI Neural Network. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS93), </booktitle> <volume> vol. 6, </volume> <pages> pp. 874-881, </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo CA, </address> <year> 1994. </year>
Reference-contexts: Various strategies have been proposed to perform this gain adaptation, ranging from heuristics based on the average value of the incoming connections to a neuron [Hoehfeld-92] [Xie-92], to approaches that use some form of gradient descent to train the gains [Tang-93] <ref> [Coggins-94] </ref>. In some training algorithms the weight values have been limited to powers-of-two [White-92] [Tang-93] [Marchesi-93]. The main advantage of this technique is that all costly multiplications can be replaced by easy to implement shift operations.
Reference: [Dogaru-96] <author> R. Dogaru, A. T. Murgan, S. Ortmann, and M. Glesner. </author> <title> A Modified RBF Neural Network for Efficient Current-Mode VLSI Implementation. </title> <booktitle> Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 265-270, </pages> <address> Lausanne, Switzerland, </address> <month> February 12-14, </month> <year> 1996. </year>
Reference-contexts: Another approach is the use of a simplified activation function, for example the replacement of the Gaussian function in radial basis networks by a triangular one <ref> [Dogaru-96] </ref>, leading to a simplified hardware implementation. Additional difficulties arise when the activation functions are implemented by optical hardware, as for example liquid crystal light valves.
Reference: [Dolenko-95] <author> B. K. Dolenko and H. C. Card. </author> <title> Tolerance to Analog Hardware of On-Chip Learning in Backpropagation Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 5, </volume> <pages> pp. 1045-1052, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: This is also intuitively clear because the use of the analog circuit in the forward pass incorporates the non-uniformities in the learning process. This has been confirmed by experimental results, for example for on-chip learning in backpropagation networks [Cairns-94] <ref> [Dolenko-95] </ref>. Their research indicates that backpropagation learning can adapt to the non-uniformity of multiplier gains which are caused by fab IDIAP-RR 97-17 6 rication inaccuracies. The occurrence of additive offsets in the multiplications and especially in weight adaptations do pose serious problems which are not easily overcome by on-chip learning [Dolenko-95]. <p> <ref> [Dolenko-95] </ref>. Their research indicates that backpropagation learning can adapt to the non-uniformity of multiplier gains which are caused by fab IDIAP-RR 97-17 6 rication inaccuracies. The occurrence of additive offsets in the multiplications and especially in weight adaptations do pose serious problems which are not easily overcome by on-chip learning [Dolenko-95]. A possible solution is the use of some dedicated hardware in the weight adaptation circuitry which enables offset-compensation [Annema-95]. 3.2 Non-Ideal Response Computations performed in hardware are approximations of the mathematical operations assumed to be ideal in neural network models. <p> Therefore, simple non-linear multipliers are often preferable and are used in both electronic [Lont-92] [Hollis-94] [Reyneri-95] and optical implementations [Robinson-92] [Neiberg-94]. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. While in [Cairns-94] <ref> [Dolenko-95] </ref> the straightforward use of non-linear multipliers in simulations of on-chip learning in analog backpropagation networks leads to satisfactory results, in [Lont-92] the standard backpropagation algorithm fails to converge with non-linear synapses.
Reference: [Dundar-95] <author> G. Dundar and K. Rose. </author> <title> The Effects of Quantization on Multilayer Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 6, </volume> <pages> pp. 1446-1451, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Since most research has been performed for multilayer feedforward networks, these will be discussed separately from the other IDIAP-RR 97-17 2 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world [Holt-93] 8 1 Finite precision error analysis for the forward re trieving pass. <ref> [Dundar-95] </ref> 10 2 Statistical model of weight quantization in sig moidal networks. [Piche-95] 6-10 2 Statistical analysis of the effects of weight errors upon an ensemble of multilayer networks. Table 1: Weight discretization in multilayer neural networks: off-chip learning. <p> based on an overestimation of the error. [Hollis-94] 13 1 Weight perturbation with an adaptive gain and learning rate. [Jabri-94] 6 1 1 Semi-parallel weight perturbation algorithms. [Simard-94] 16 - 1 Backpropagation without multiplication; gradients and states of power-of-two [Battiti-95] 1-8 1 2 Heuristic method for solving combinatorial optimization problems. <ref> [Dundar-95] </ref> 10 2 Backpropagation with forced weight updates. Table 3: Weight discretization in multilayer neural networks: on-chip learning. by allowing a dynamic rescaling of the weights (and hence the weight range) by adapting the gain fi of the activation function.
Reference: [Fahlman-90] <author> S. E. Fahlman and C. Lebiere. </author> <booktitle> The Cascade-Correlation Learning Architecture. Advances in Neural Information Processing Systems (NIPS89), </booktitle> <volume> vol. 2, </volume> <pages> pp. 524-532, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights. Cascade topology <ref> [Fahlman-90] </ref> [Hoehfeld-92] 12 2 1 Coarse weight quantization in the cascade cor relation algorithm. [Hoehfeld-92] 6 2 1 Cascade correlation with probabilistic round ing and variable gain. [Campbell-95] 1 2 1 A constructive algorithm for Heaviside cas cade networks.
Reference: [Fiesler-88] <author> E. Fiesler, A. Choudry, and H. J. Caulfield. </author> <title> Weight Discretization in Backward Error Propagation Neural Networks. Neural Networks, </title> <booktitle> special supplement with "Abstracts of the First Annual INNS Meeting", </booktitle> <volume> vol. 1, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1988. </year>
Reference-contexts: Table 1: Weight discretization in multilayer neural networks: off-chip learning. Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world <ref> [Fiesler-88] </ref> [Fiesler-90] 2-3 3 Forward pass with discrete weights, backward pass with continuous weights. [Marchesi-93] 3-4 1 1 Power-of-two weights in the forward pass and an adaptive learning rate. [Tang-93] 3-4 1 Power-of-two weights and adaptive gain of the activation function. <p> One of the first, and perhaps most successful, weight discretization techniques is of the chip-in-the-loop kind <ref> [Fiesler-88] </ref> [Fiesler-90]. It is suitable for feedforward neural networks, easy to implement, and very flexible in that it can handle a large range of discretizations up to the precision of a few bits only (Table 2).
Reference: [Fiesler-90] <author> E. Fiesler, A. Choudry, and H. J. Caulfield. </author> <title> A Weight Discretization paradigm for Optical Neural Networks. </title> <booktitle> Proceedings of the International Congress on Optical Science and Engineering, </booktitle> <volume> vol. SPIE 1281, </volume> <pages> pp. 164-173, </pages> <address> SPIE, Bellingham, Washington, </address> <year> 1990, </year> <note> ISBN: 0-8194-0328-8. </note>
Reference-contexts: Table 1: Weight discretization in multilayer neural networks: off-chip learning. Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world [Fiesler-88] <ref> [Fiesler-90] </ref> 2-3 3 Forward pass with discrete weights, backward pass with continuous weights. [Marchesi-93] 3-4 1 1 Power-of-two weights in the forward pass and an adaptive learning rate. [Tang-93] 3-4 1 Power-of-two weights and adaptive gain of the activation function. <p> One of the first, and perhaps most successful, weight discretization techniques is of the chip-in-the-loop kind [Fiesler-88] <ref> [Fiesler-90] </ref>. It is suitable for feedforward neural networks, easy to implement, and very flexible in that it can handle a large range of discretizations up to the precision of a few bits only (Table 2). The basic idea is to start with a normal neural network with continuous valued weights.
Reference: [Flower-93] <author> B. Flower and M. Jabri. </author> <title> Summed Weight Neuron Perturbation: An O(N ) Improvement over Weight Perturbation. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS92), </booktitle> <volume> vol. 5, </volume> <pages> 212-219, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1993. </year>
Reference-contexts: Other variants use a semi-parallel perturbation scheme like chain-rule perturbation [Hollis-94], fan-out or fan-in-out perturbation [Jabri-94], and summed weight neuron perturbation <ref> [Flower-93] </ref>. These semi-parallel techniques perturb simultaneously all the weights feeding into or leaving one neuron. An experimental comparison of these perturbation algorithms with an analog multi-layer perceptron chip (Kakadu) in-the-loop showed that the semi-parallel techniques are best suited for effective learning when the accuracy is low [Jabri-94].
Reference: [Frye-91] <author> R. C. Frye, E. A. Rietman, and C. C. Wong. </author> <title> Back-Propagation Learning and Nonidealities in Analog Neural Network Hardware. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 110-117, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: These non-uniformities are particularly troublesome when the training of the network is done off-chip without taking these component variations into account <ref> [Frye-91] </ref>. It is, however, widely claimed that chip-in-the-loop or on-chip learning can compensate to a considerable extent for these non-uniformities [Card-92]. This is also intuitively clear because the use of the analog circuit in the forward pass incorporates the non-uniformities in the learning process.
Reference: [Fukushima-80] <author> K. Fukushima. </author> <title> Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position. </title> <journal> Biological Cybernetics, </journal> <volume> vol. 36, </volume> <pages> pp. 193-202, </pages> <year> 1980. </year>
Reference-contexts: Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron <ref> [Fukushima-80] </ref> [White-92] 3 1 Uses power-of-two weights. Cascade topology [Fahlman-90] [Hoehfeld-92] 12 2 1 Coarse weight quantization in the cascade cor relation algorithm. [Hoehfeld-92] 6 2 1 Cascade correlation with probabilistic round ing and variable gain. [Campbell-95] 1 2 1 A constructive algorithm for Heaviside cas cade networks.
Reference: [Grossman-90] <author> T. Grossman. </author> <title> The CHIR Algorithm for Feedforward Networks with Binary Weights. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS89), </booktitle> <volume> vol. 2, </volume> <pages> pp. 516-523, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: This can often be solved IDIAP-RR 97-17 4 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world [Asanovic-91] 16 - 1 Coarse weight quantization in the back propagation algorithm. [Holt-93] 14-16 2 An error analysis of backpropagation with fi nite precision. <ref> [Grossman-90] </ref> 1 1 Adaptation of both weights and the internal representation of the neurons. [Reyneri-91] 9-10 1 1 Batch backpropagation with a near-optimum learning rate. [Xie-92] 10 - 2 Weight perturbation with gain adaptation. [Xie-92] 9 - 2 Combination of weight perturbation and a partial random search. [Abramson-93] 2 3 A <p> of both weights and the internal representation of the neurons. [Reyneri-91] 9-10 1 1 Batch backpropagation with a near-optimum learning rate. [Xie-92] 10 - 2 Weight perturbation with gain adaptation. [Xie-92] 9 - 2 Combination of weight perturbation and a partial random search. [Abramson-93] 2 3 A slight modification of <ref> [Grossman-90] </ref> to train sparsely connected Heaviside networks. [Sakaue-93] 8-10 - 2 A weighted error function in the backpropagation algorithm based on an overestimation of the error. [Hollis-94] 13 1 Weight perturbation with an adaptive gain and learning rate. [Jabri-94] 6 1 1 Semi-parallel weight perturbation algorithms. [Simard-94] 16 - 1 Backpropagation
Reference: [Hendrich-96] <author> N. Hendrich. </author> <title> A Scalable Architecture for Binary Couplings Attractor Neural Networks. </title> <booktitle> Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 117-124, </pages> <address> Lausanne, Switzerland, February 12-14, </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA USA, </address> <year> 1996. </year>
Reference-contexts: An interesting example of a hardware implementation is Bellcore's implementation of a Boltzmann machine and Mean-Field learning, which allows on-chip learning with only 5-bit weights [Alspector-92]. Recently, a weight discretization algorithm for an associative memory with binary f1,+1g weights has been implemented on a digital VLSI chip <ref> [Hendrich-96] </ref>. The pattern storage capacity that can be obtained with this learning rule is good (0.4 times the number of neurons) and the algorithm is suited for on-chip learning. <p> Associative memory, see for example [Hopfield-82] [Verleysen-89] 2 1 A linear programming learning algorithm for associative memories. [Johannet-92] 9-11 1 Integer arithmetics for learning in associative memory. <ref> [Hendrich-96] </ref> 1 1 Associative memory with binary weights and a good storage capacity. Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights.
Reference: [Hoehfeld-92] <author> M. H. Hoehfeld and S. Fahlman. </author> <title> Learning with Limited Numerical Precision Using the Cascade-Correlation Algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 4, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: It has been shown by simulations that as soon as the range of the weights decreases below a certain value, which depends on the problem at hand, the training fails to converge because of the clipping of the weight values <ref> [Hoehfeld-92] </ref>. <p> Various strategies have been proposed to perform this gain adaptation, ranging from heuristics based on the average value of the incoming connections to a neuron <ref> [Hoehfeld-92] </ref> [Xie-92], to approaches that use some form of gradient descent to train the gains [Tang-93] [Coggins-94]. In some training algorithms the weight values have been limited to powers-of-two [White-92] [Tang-93] [Marchesi-93]. <p> Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights. Cascade topology [Fahlman-90] <ref> [Hoehfeld-92] </ref> 12 2 1 Coarse weight quantization in the cascade cor relation algorithm. [Hoehfeld-92] 6 2 1 Cascade correlation with probabilistic round ing and variable gain. [Campbell-95] 1 2 1 A constructive algorithm for Heaviside cas cade networks. <p> Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning. Neocognitron [Fukushima-80] [White-92] 3 1 Uses power-of-two weights. Cascade topology [Fahlman-90] <ref> [Hoehfeld-92] </ref> 12 2 1 Coarse weight quantization in the cascade cor relation algorithm. [Hoehfeld-92] 6 2 1 Cascade correlation with probabilistic round ing and variable gain. [Campbell-95] 1 2 1 A constructive algorithm for Heaviside cas cade networks.
Reference: [Holler-89] <author> M. Holler, S. Tam, H. Castro, and R. Benson. </author> <title> An Electrically Trainable Artificial Neural Network (ETANN) with 10240 `Floating Gate' Synapses. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (IJCNN89), </booktitle> <volume> vol. 2, </volume> <pages> pp. 191-196, </pages> <address> Washington DC, </address> <year> 1989. </year>
Reference-contexts: An example of an analog electronic implementation is Intel's Electrically Trainable Analog Neural Network (ETANN), which can perform an impressive two billion weight multiplications per second. The accuracy of its weights and neurons, however, can be compared with a resolution of only seven bits <ref> [Holler-89] </ref>. Since hardware implementations are characterized by a low numerical precision, it is essential to study its effects on the recall and training of the various neural network models.
Reference: [Hollis-94] <author> P. W. Hollis and J. J. Paulos. </author> <title> A Neural Network Learning Algorithm Tailored for VLSI Implementation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 5, </volume> <pages> pp. 784-791, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: perturbation with gain adaptation. [Xie-92] 9 - 2 Combination of weight perturbation and a partial random search. [Abramson-93] 2 3 A slight modification of [Grossman-90] to train sparsely connected Heaviside networks. [Sakaue-93] 8-10 - 2 A weighted error function in the backpropagation algorithm based on an overestimation of the error. <ref> [Hollis-94] </ref> 13 1 Weight perturbation with an adaptive gain and learning rate. [Jabri-94] 6 1 1 Semi-parallel weight perturbation algorithms. [Simard-94] 16 - 1 Backpropagation without multiplication; gradients and states of power-of-two [Battiti-95] 1-8 1 2 Heuristic method for solving combinatorial optimization problems. [Dundar-95] 10 2 Backpropagation with forced weight updates. <p> In some training algorithms the weight values have been limited to powers-of-two [White-92] [Tang-93] [Marchesi-93]. The main advantage of this technique is that all costly multiplications can be replaced by easy to implement shift operations. This scheme has also been applied to gradient values, activation values, and learning rates <ref> [Hollis-94] </ref> [Simard-94]. Work on limiting the number of weight levels has also been done in the design of Heaviside networks for the computation of boolean functions (majority, parity, comparison, addition) and for the two-spiral problem [Beiu-95] [Beiu-96.1]. <p> The use of a linear multiplier with a reasonable operating range leads to a large area penalty in VLSI implementations. Therefore, simple non-linear multipliers are often preferable and are used in both electronic [Lont-92] <ref> [Hollis-94] </ref> [Reyneri-95] and optical implementations [Robinson-92] [Neiberg-94]. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. <p> Instead, Lont proposes to incorporate non-linear multipliers in the formulation of the backpropagation rule, which leads to good results. A disadvantage of this approach is that an accurate model of the on-chip multiplier is needed. This can be alleviated by chain-rule perturbation learning <ref> [Hollis-94] </ref>, which only performs a forward pass through a multilayer network and hence incorporates the hardware characteristics directly into the training. A solution sometimes applied in optical networks is the use of an additional weight mask which complements and thereby compensates for the non-linearities in the multiplier [Neiberg-94]. <p> A simultaneous perturbation of all weights is a promising alternative [Alspector-93] [Cauwenberghs-93], even when for a reliable estimate of the gradient the results of several perturbations should be averaged or a very small and accurate perturbation is required. Other variants use a semi-parallel perturbation scheme like chain-rule perturbation <ref> [Hollis-94] </ref>, fan-out or fan-in-out perturbation [Jabri-94], and summed weight neuron perturbation [Flower-93]. These semi-parallel techniques perturb simultaneously all the weights feeding into or leaving one neuron.
Reference: [Holt-93] <author> J. L. Holt and J.-N. Hwang. </author> <title> Finite Error Precision Analysis of Neural Network Hardware Implementations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 3, </volume> <pages> pp. 1380-1389, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Since most research has been performed for multilayer feedforward networks, these will be discussed separately from the other IDIAP-RR 97-17 2 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world <ref> [Holt-93] </ref> 8 1 Finite precision error analysis for the forward re trieving pass. [Dundar-95] 10 2 Statistical model of weight quantization in sig moidal networks. [Piche-95] 6-10 2 Statistical analysis of the effects of weight errors upon an ensemble of multilayer networks. <p> This can often be solved IDIAP-RR 97-17 4 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world [Asanovic-91] 16 - 1 Coarse weight quantization in the back propagation algorithm. <ref> [Holt-93] </ref> 14-16 2 An error analysis of backpropagation with fi nite precision. [Grossman-90] 1 1 Adaptation of both weights and the internal representation of the neurons. [Reyneri-91] 9-10 1 1 Batch backpropagation with a near-optimum learning rate. [Xie-92] 10 - 2 Weight perturbation with gain adaptation. [Xie-92] 9 - 2 Combination
Reference: [Hopfield-82] <author> J. J. </author> <title> Hopfield. Neural Networks and Physical Systems with Emergent Collective Computational Abilities. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> vol. 79, no. 8, </volume> <pages> pp. 2554-2558, </pages> <address> Washington DC, </address> <month> April </month> <year> 1982. </year> <pages> IDIAP-RR 97-17 12 </pages>
Reference-contexts: Associative memory, see for example <ref> [Hopfield-82] </ref> [Verleysen-89] 2 1 A linear programming learning algorithm for associative memories. [Johannet-92] 9-11 1 Integer arithmetics for learning in associative memory. [Hendrich-96] 1 1 Associative memory with binary weights and a good storage capacity.
Reference: [Jabri-92] <author> M. Jabri and B. Flower. </author> <title> Weight Perturbation: An Optimal Architecture and Learning Technique for Analog VLSI Feedforward and Recurrent Multilayer Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 154-157, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The two main variants of this class of algorithms are node perturbation which is based on the perturbation of the input value of a neuron, as for example the Madaline-3 rule [Widrow-90], and weight perturbation, see for example <ref> [Jabri-92] </ref>.
Reference: [Jabri-94] <institution> Practical Performance and Credit Assignment Efficiency of Analog Multi-Layer Perceptron Perturbation Based Training Algorithms. SEDAL Technical Report 1-7-94, Systems Engineering and Design Automation Laboratory, Sydney University Electrical Engineering, </institution> <address> NSW 2006 Australia, </address> <year> 1994. </year>
Reference-contexts: Recently, an analog electronic chip (Kakadu) has been applied successfully to some classification problems by training it with the combined search algorithm and semi-parallel weight perturbation algorithms using only a 6-bit weight accuracy <ref> [Jabri-94] </ref> [Leong-95]. 2.2 Quantization Effects in Other Neural Network Models Also for other neural network models the effects of a coarse quantization of the weight values on recall and learning have been investigated. <p> and a partial random search. [Abramson-93] 2 3 A slight modification of [Grossman-90] to train sparsely connected Heaviside networks. [Sakaue-93] 8-10 - 2 A weighted error function in the backpropagation algorithm based on an overestimation of the error. [Hollis-94] 13 1 Weight perturbation with an adaptive gain and learning rate. <ref> [Jabri-94] </ref> 6 1 1 Semi-parallel weight perturbation algorithms. [Simard-94] 16 - 1 Backpropagation without multiplication; gradients and states of power-of-two [Battiti-95] 1-8 1 2 Heuristic method for solving combinatorial optimization problems. [Dundar-95] 10 2 Backpropagation with forced weight updates. <p> Other variants use a semi-parallel perturbation scheme like chain-rule perturbation [Hollis-94], fan-out or fan-in-out perturbation <ref> [Jabri-94] </ref>, and summed weight neuron perturbation [Flower-93]. These semi-parallel techniques perturb simultaneously all the weights feeding into or leaving one neuron. <p> These semi-parallel techniques perturb simultaneously all the weights feeding into or leaving one neuron. An experimental comparison of these perturbation algorithms with an analog multi-layer perceptron chip (Kakadu) in-the-loop showed that the semi-parallel techniques are best suited for effective learning when the accuracy is low <ref> [Jabri-94] </ref>. The fan-in-out technique showed the best generalization and training convergence results when the weights and weight updates were quantized to 6 bits. 4.2 Local Learning Algorithms The implementation of a learning rule can be greatly simplified if it only uses information that is locally available [Palmieri-93].
Reference: [Jim-94] <author> K. Jim, C. L. Giles, and B. G. </author> <title> Horne. Synaptic Noise in Dynamically-Driven Recurrent Neural Networks: Convergence and Generalization. </title> <type> Technical report UMIACS-TR-94-89 / CS-TR-3322, </type> <institution> Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 USA, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It is demonstrated both analytically and experimentally that this synaptic noise improves the network's fault tolerance to weight damage, generalization to unseen patterns, and training time. Similar results have been obtained when injecting additive noise into the weights of recurrent neural networks <ref> [Jim-94] </ref>. 4.5 Other Hardware-Friendly Neural Network Models Although the majority of neural hardware is concerned with the implementation of multilayer networks, because of their wide-ranging applicability, most other popular neural network models have also been implemented in hardware.
Reference: [Johannet-92] <author> A. Johannet, L. Personnaz, G. Dreyfus, J.-D. Gascuel, and M. Weinfeld. </author> <title> Specification and Implementation of a Digital Hopfield-Type Associative Memory with On-Chip Training. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> volume 3, number 4, </volume> <pages> pp. 529-539, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Associative memory, see for example [Hopfield-82] [Verleysen-89] 2 1 A linear programming learning algorithm for associative memories. <ref> [Johannet-92] </ref> 9-11 1 Integer arithmetics for learning in associative memory. [Hendrich-96] 1 1 Associative memory with binary weights and a good storage capacity. Boltzmann network [Ackley-85] [Balzer-91] 6-8 2 Coarse quantization of the weights during learning. [Alspector-92] 5 2 Coarse weight quantization for Boltzmann and mean-field learning.
Reference: [Judd-93] <author> S. Judd and P. W. Munro. </author> <title> Nets with Unreliable Hidden Nodes Learn Error-Correcting Codes. </title> <booktitle> Advances in Neural Information Processing Systems (NIPS92), </booktitle> <volume> vol. 5, </volume> <pages> pp. 89-96, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1993. </year>
Reference-contexts: Some research has also been devoted to the robustness of a network to unreliable neurons. This unreliability can consist of sign inversions of hidden neuron values <ref> [Judd-93] </ref> or destruction of hidden neurons [Kerlirzin-95]. While neural networks trained by standard learning algorithms are not inherently fault-tolerant, the incorporation of the expected faults in the training phase leads to remarkable improvements.
Reference: [Kerlirzin-95] <author> P. Kerlirzin and P. Refregier. </author> <title> Theoretical Investigation of the Robustness of Multilayer Perceptrons: Analysis of the Linear Case and Extension to Nonlinear Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 3, </volume> <pages> pp. 560-571, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Some research has also been devoted to the robustness of a network to unreliable neurons. This unreliability can consist of sign inversions of hidden neuron values [Judd-93] or destruction of hidden neurons <ref> [Kerlirzin-95] </ref>. While neural networks trained by standard learning algorithms are not inherently fault-tolerant, the incorporation of the expected faults in the training phase leads to remarkable improvements. <p> An illustration of this fact is an adaptation of the backpropagation learning rule that uses only a random subset of hidden neurons for each iteration. The trained network is far more robust to the destruction of hidden neurons and shows performance comparable to the noiseless case <ref> [Kerlirzin-95] </ref>. This is closely related to the injection of random noise in the weight values during the training of a multilayer neural network, whose effects have been elaborately discussed by Murray and Edwards [Murray-94].
Reference: [Kohonen-89] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory, 3rd edition, </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: The accuracies listed in Table 1 to 4 are therefore highly biased by the different benchmarks that were used by the various authors. IDIAP-RR 97-17 5 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world Self-organizing map, see for example <ref> [Kohonen-89] </ref> [Kohonen-93] 3-4 - 1 Quantization of input values during recall. [Rueping-94] 4 2 1 Power-of-two adaptation factor and quantized weights. [Thiran-94 ] 5 1 Uses a conical neighbourhood function instead of a rectangular one.
Reference: [Kohonen-93] <author> T. Kohonen. </author> <title> Things You Haven't Heard about the Self-Organizing Map. </title> <booktitle> Proceedings of the 1993 IEEE International Conference on Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1147-1156, </pages> <address> San Francisco, California, March 28-April 1, </address> <year> 1993, </year> <note> ISBN: 0-7803-0999-5. </note>
Reference-contexts: The accuracies listed in Table 1 to 4 are therefore highly biased by the different benchmarks that were used by the various authors. IDIAP-RR 97-17 5 Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world Self-organizing map, see for example [Kohonen-89] <ref> [Kohonen-93] </ref> 3-4 - 1 Quantization of input values during recall. [Rueping-94] 4 2 1 Power-of-two adaptation factor and quantized weights. [Thiran-94 ] 5 1 Uses a conical neighbourhood function instead of a rectangular one.
Reference: [Leong-95] <author> P. H. W. Leong and M. A. Jabri. </author> <title> A Low-Power VLSI Arrhythmia Classifier. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 6, </volume> <pages> pp. 1435-1445, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Recently, an analog electronic chip (Kakadu) has been applied successfully to some classification problems by training it with the combined search algorithm and semi-parallel weight perturbation algorithms using only a 6-bit weight accuracy [Jabri-94] <ref> [Leong-95] </ref>. 2.2 Quantization Effects in Other Neural Network Models Also for other neural network models the effects of a coarse quantization of the weight values on recall and learning have been investigated. <p> Compared to the state-of-the-art in digital neural network implementations, the design of analog neural network implementations with non-idealities like component non-uniformity, non-ideal responses, and system noise, is still in a more experimental state. Implementations have therefore been limited to small-scale networks <ref> [Leong-95] </ref> and it is yet to be shown whether reliable large networks can be realized in practice by analog techniques. <p> The development of hardware-friendly learning rules that form an alternative for algorithms which are intricate to implement, like the backpropagation algorithm, is therefore essential. The efficacy of perturbation algorithms illustrates the usefulness of this approach and the first implementations using these training algorithms are emerging <ref> [Leong-95] </ref>.
Reference: [Lont-92] <author> J. Lont and W. Guggenbuhl. </author> <title> Analog CMOS Implementation of a Multilayer Perceptron with Nonlinear Synapses. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 385-392, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The use of a linear multiplier with a reasonable operating range leads to a large area penalty in VLSI implementations. Therefore, simple non-linear multipliers are often preferable and are used in both electronic <ref> [Lont-92] </ref> [Hollis-94] [Reyneri-95] and optical implementations [Robinson-92] [Neiberg-94]. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. While in [Cairns-94] [Dolenko-95] the straightforward use of non-linear multipliers in simulations of on-chip learning in analog backpropagation networks leads to satisfactory results, in [Lont-92] the standard <p> both electronic <ref> [Lont-92] </ref> [Hollis-94] [Reyneri-95] and optical implementations [Robinson-92] [Neiberg-94]. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. While in [Cairns-94] [Dolenko-95] the straightforward use of non-linear multipliers in simulations of on-chip learning in analog backpropagation networks leads to satisfactory results, in [Lont-92] the standard backpropagation algorithm fails to converge with non-linear synapses. Instead, Lont proposes to incorporate non-linear multipliers in the formulation of the backpropagation rule, which leads to good results. A disadvantage of this approach is that an accurate model of the on-chip multiplier is needed. <p> Another problem for analog hardware is the requisite of an activation function that is similar to the standard sigmoid. The incorporation of a model of a sigmoid-like hardware activation function in the training algorithm can compensate for some inaccuracy <ref> [Lont-92] </ref>. This is another example of the opportunism that often plays a role in the design of neural hardware: search for the hidden advantages of apparent drawbacks and try to exploit these instead of trying to approximate the existing mathematical model as close as possible.
Reference: [Lyon-96] <author> R. F. Lyon and L. S. Yaeger. </author> <title> On-Line Hand-Printing Recognition with Neural Networks. </title> <booktitle> Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 201-212, </pages> <address> Lausanne, Switzerland, </address> <month> February 12-14, </month> <year> 1996. </year>
Reference-contexts: This scheme is repeated until convergence is obtained. This flexible weight discretization method has been successfully used in the development of the Apple Newton <ref> [Lyon-96] </ref>, and in optical neural networks at Mitsubishi, Japan [Takahashi-91] and in Switzerland [Saxena-95] [Moerland-96.2]. A similar approach has been applied to design neural networks restricted to single power-of-two weights (see section 2.3) [Marchesi-93] [Tang-93].
Reference: [Marchesi-93] <author> M. Marchesi, G. Orlandi, F. Piazza, and A. Uncini. </author> <title> Fast Neural Networks Without Multipliers. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 53-62, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Table 1: Weight discretization in multilayer neural networks: off-chip learning. Reference Accuracy # of Benchmarks Remarks (in bits) Artificial Real-world [Fiesler-88] [Fiesler-90] 2-3 3 Forward pass with discrete weights, backward pass with continuous weights. <ref> [Marchesi-93] </ref> 3-4 1 1 Power-of-two weights in the forward pass and an adaptive learning rate. [Tang-93] 3-4 1 Power-of-two weights and adaptive gain of the activation function. Table 2: Weight discretization in multilayer neural networks: chip-in-the-loop learning. neural network paradigms. <p> This flexible weight discretization method has been successfully used in the development of the Apple Newton [Lyon-96], and in optical neural networks at Mitsubishi, Japan [Takahashi-91] and in Switzerland [Saxena-95] [Moerland-96.2]. A similar approach has been applied to design neural networks restricted to single power-of-two weights (see section 2.3) <ref> [Marchesi-93] </ref> [Tang-93]. On-chip learning Here, the training of the neural network is done entirely on-chip which offers the possibility of continuous training. This means in specific that at least the weight values are represented with only a limited precision. <p> In some training algorithms the weight values have been limited to powers-of-two [White-92] [Tang-93] <ref> [Marchesi-93] </ref>. The main advantage of this technique is that all costly multiplications can be replaced by easy to implement shift operations. This scheme has also been applied to gradient values, activation values, and learning rates [Hollis-94] [Simard-94].
Reference: [Mauduit-92] <author> N. Mauduit, M. Duranton, J. Gobert, and J.-A. Sirat. Lneuro 1.0: </author> <title> A Piece of Hardware LEGO for Building Neural Network Systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> volume 3, number 3, </volume> <pages> pages 414-422, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, in Philips' L-Neuro 1.0 architecture, which allows the implementation of feedforward networks and on-chip backpropagation training, 16-bit weights are used during the training process and only 4-bit or 8-bit weights are employed during recall <ref> [Mauduit-92] </ref>. An example of an analog electronic implementation is Intel's Electrically Trainable Analog Neural Network (ETANN), which can perform an impressive two billion weight multiplications per second. The accuracy of its weights and neurons, however, can be compared with a resolution of only seven bits [Holler-89]. <p> These estimations of the required accuracy for well-known learning algorithms and several of the weight discretization algorithms described are already in use in some large-scale hardware implementations. Designers of digital neurocomputers, for example, profit from the fact that the required weight accuracy for backpropagation training is around 16 bits <ref> [Mauduit-92] </ref>. An example of a successful implementation of a weight discretization algorithm is Battiti's TOTEM-chip which uses a weight accuracy of 4 bits [Battiti-94].
Reference: [Moerland-95] <author> P. Moerland, E. Fiesler, and I. Saxena. </author> <title> The Effects of Optical Thresholding in Backpropagation Neural Networks. </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN95), </booktitle> <volume> vol. 2, </volume> <pages> pp. 339-343, </pages> <address> Paris, France, </address> <month> October 9-13, </month> <year> 1995. </year>
Reference-contexts: A nice and simple way to solve this problem is by using an adapted backpropagation learning rule that is based on a simple and precise relationship between the gain and two other network parameters [Thimm-96], which compensates for a non-standard gain without any additional hardware, and shows superior results <ref> [Moerland-95] </ref>. 4 Hardware-Friendly Learning Algorithms In this section a variety of learning algorithms that are well suited for hardware implementations of neural networks are presented. These hardware-friendly learning algorithms [Moerland-96.1] can de divided into two classes, namely: 1.
Reference: [Moerland-96.1] <author> P. Moerland and E. Fiesler. </author> <title> Hardware-FriendlyLearning Algorithms for Neural Networks: An Overview. </title> <booktitle> Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pp. 117-124, </pages> <address> Lausanne, Switzerland, </address> <month> February 12-14, </month> <year> 1996. </year>
Reference-contexts: These hardware-friendly learning algorithms <ref> [Moerland-96.1] </ref> can de divided into two classes, namely: 1. Adaptations of existing neural network learning rules that facilitate their hardware implement ation.
Reference: [Moerland-96.2] <author> P. Moerland, E. Fiesler and I. Saxena. </author> <title> Discrete All-Positive Multilayer Perceptrons for Optical Implementations. </title> <address> IDIAP-RR 97-02, IDIAP, Martigny, Switzerland, </address> <note> February 1997 (accepted for publication in Optical Engineering). </note>
Reference-contexts: This scheme is repeated until convergence is obtained. This flexible weight discretization method has been successfully used in the development of the Apple Newton [Lyon-96], and in optical neural networks at Mitsubishi, Japan [Takahashi-91] and in Switzerland [Saxena-95] <ref> [Moerland-96.2] </ref>. A similar approach has been applied to design neural networks restricted to single power-of-two weights (see section 2.3) [Marchesi-93] [Tang-93]. On-chip learning Here, the training of the neural network is done entirely on-chip which offers the possibility of continuous training.
Reference: [Moreno-94] <author> J. M. Moreno Arostegui. </author> <title> VLSI Architectures for Evolutive Neural Models. </title> <type> Ph.D. Thesis, </type> <institution> Technical University of Catalunya, Department of Electronics Engineering, </institution> <address> Barcelona, Spain, </address> <year> 1994. </year>
Reference-contexts: The basis of these algorithms is often formed by a perceptron algorithm that is used to adapt the weights into the freshly added neurons. Recently, some digital and mixed analog/digital architectures have been designed to be suitable for the implementation of a range of these constructive algorithms <ref> [Moreno-94] </ref>. 4.4 Robustness In section 3 several examples have already been given of the robustness of neural networks to hardware non-idealities. Some research has also been devoted to the robustness of a network to unreliable neurons.
Reference: [Murray-94] <author> A. F. Murray and P. J. Edwards. </author> <title> Enhanced MLP Performance and Fault Tolerance Resulting from Synaptic Weight Noise During Training. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 5, </volume> <pages> pp. 792-802, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: This is closely related to the injection of random noise in the weight values during the training of a multilayer neural network, whose effects have been elaborately discussed by Murray and Edwards <ref> [Murray-94] </ref>. It is demonstrated both analytically and experimentally that this synaptic noise improves the network's fault tolerance to weight damage, generalization to unseen patterns, and training time.
Reference: [Neiberg-94] <author> L. Neiberg and D. Casasent. </author> <title> High-Capacity Neural Networks on Nonideal Hardware. </title> <journal> Applied Optics, </journal> <volume> vol. 33, no. 32, </volume> <pages> pp. 7665-7675, </pages> <year> 1994. </year>
Reference-contexts: The use of a linear multiplier with a reasonable operating range leads to a large area penalty in VLSI implementations. Therefore, simple non-linear multipliers are often preferable and are used in both electronic [Lont-92] [Hollis-94] [Reyneri-95] and optical implementations [Robinson-92] <ref> [Neiberg-94] </ref>. The claims on the learning behaviour of a neural network with non-linear multipliers are rather contradictory. <p> A solution sometimes applied in optical networks is the use of an additional weight mask which complements and thereby compensates for the non-linearities in the multiplier <ref> [Neiberg-94] </ref>. Another problem for analog hardware is the requisite of an activation function that is similar to the standard sigmoid. The incorporation of a model of a sigmoid-like hardware activation function in the training algorithm can compensate for some inaccuracy [Lont-92].
Reference: [Palmieri-93] <author> F. Palmieri, J. Zhu, and C. Chang. </author> <title> Anti-Hebbian Learning in Topologically Constrained Linear Networks: </title>
Reference-contexts: The fan-in-out technique showed the best generalization and training convergence results when the weights and weight updates were quantized to 6 bits. 4.2 Local Learning Algorithms The implementation of a learning rule can be greatly simplified if it only uses information that is locally available <ref> [Palmieri-93] </ref>. This feature minimizes the amount of wiring and communication. Since the backpropagation algorithm is not local, several local learning algorithms have been designed that avoid a global backpropagation of error signals. An example is an anti-Hebbian learning algorithm that is suitable for optical neural networks [Psaltis-93].
References-found: 57

