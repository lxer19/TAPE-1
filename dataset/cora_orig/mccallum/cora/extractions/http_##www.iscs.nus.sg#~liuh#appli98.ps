URL: http://www.iscs.nus.sg/~liuh/appli98.ps
Refering-URL: 
Root-URL: 
Email: fliuh,rudysg@iscs.nus.edu.sg  
Phone: Tel: (+65) 874-6563; Fax: (+65) 779-4580  
Title: Incremental Feature Selection  
Author: Huan Liu and Rudy Setiono 
Keyword: Pattern Recognition, Machine Learning, Feature Selection, Dimensionality Reduction  
Note: To appear in Applied Intelligence, Kluwer Academic Publishers.  
Address: Ridge, Singapore 119260  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Feature selection is a problem of finding relevant features. When the number of features of a dataset is large and its number of patterns is huge, an effective method of feature selection can help in dimensionality reduction. An incremental probabilistic algorithm is designed and implemented as an alternative to the exhaustive and heuristic approaches. Theoretical analysis is given to support the idea of the probabilistic algorithm in finding an optimal or near-optimal subset of features. Experimental results suggest that (1) the probabilistic algorithm is effective in obtaining optimal/suboptimal feature subsets; (2) its incremental version expedites feature selection further when the number of patterns is large and can scale up without sacrificing the quality of selected features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.W. Aha. </author> <title> Tolerating noisy, irrelevant and novel attributes in instance-bassed learning algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36(1), </volume> <year> 1992. </year>
Reference-contexts: Third, irrelevant and redundant features may confuse a learning algorithm by obscuring the distribution of the small set of truly relevant features. In addition, irrelevant and redundant features require an exponential increase in data storage requirements <ref> [1] </ref>. This is because with more features, much more data is required for effective induction. For instance, in a binary domain, the extra m irrelevant/relevant features would require 2 m times more patterns to describe the whole data.
Reference: [2] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Most feature selection methods [9, 12, 8] can be grouped into two categories: exhaustive or heuristic search for an optimal set of M features. For example, Almuallim and Dietterich's FOCUS algorithm <ref> [2] </ref> starts with an empty feature set and carries out exhaustive search until it finds a minimal set of features that is sufficient to construct a hypothesis consistent with a given set of examples. It works on binary, noise-free data. <p> An example is dataset 1 (CorrAL) in Section 5 which is reproduced from [8]. 2 They proposed three heuristic algorithms to speed up the searching <ref> [2] </ref>. This is because selecting a minimal subset is a known intractable problem, and in practice, we often have to trade off the optimality of a solution for less time spent on searching. There are many heuristic feature selection algorithms. <p> Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [23], FRINGE [21] and C4.5 [24]. The results in <ref> [2] </ref> suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in [5]. The latest development of feature selection in pattern recognition can be found in [7]. <p> The inconsistency criterion is a conservative way of achieving the "class separability" which is commonly used in pattern recognition as the basic selection criterion [6]. A limited version of this was first proposed by <ref> [2] </ref> as the MIN-FEATURES bias on a binary domain. Instead of aiming to maximize the class separability, our measure tries to maintain the original class separability of the data. The inconsistency criterion is also in line with information-theoretic in all the experiments in this paper.
Reference: [3] <author> M. Boddy and T.L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67(2) </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: Both algorithms are simple to implement and fast to obtain results. By predefining fl according to prior knowledge, LVI and LVF can handle noisy data, as shown in the case of Monk3. Both can deal with multiple class values. Another feature of LVF is related to so-called anytime algorithms <ref> [3] </ref> that are algorithms whose quality of results improves gradually as computational time increases. LVF prints out a possible solution whenever it is found; afterwards LVF reports either a better subset or equally good ones.
Reference: [4] <author> G. Brassard and P. Bratley. </author> <title> Fundamentals of Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1996. </year>
Reference-contexts: At the end of the paper, we provide relevant discussion. 2 The checking can be done in O (P ), where P is the number of patterns, by using a hashing method. 3 3 Probabilistic Feature Selection The proposed probabilistic approach is a Las Vegas Algorithm <ref> [4] </ref>. Las Vegas algorithms make probabilistic choices to help guide them more quickly to a correct solution. One kind of Las Vegas algorithms uses randomness to guide their search in such a way that a correct solution is guaranteed even if unfortunate choices are made. <p> Another similar type of algorithms is Monte Carlo algorithms in which it is often possible to reduce the error probability arbitrarily at the cost of a slight increase in computing time (refer to page 341 in <ref> [4] </ref>). In this work, LVF (Las Vegas Filter) 3 is more suitable since probabilities of generating distinct subsets are the same. The time performance of a Las Vegas algorithm may not be better than that of some heuristic algorithms.
Reference: [5] <author> M. Dash and H. Liu. </author> <title> Feature selection methods for classifications. Intelligent Data Analysis: </title> <journal> An International Journal, </journal> <volume> 1(3), </volume> <year> 1997. </year>
Reference-contexts: The number of possible subsets grows exponentially. Researchers have designed different strategies in search of optimal subsets of d features (Branch and Bound [20] and its variations [26], many heuristic and stochastic methods <ref> [5, 7] </ref>). If we view 1 these feature selection algorithms from the perspective of using an induction algorithm, as pointed out in [8], the work on feature selection can be divided into filter and wrapper models. <p> Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [23], FRINGE [21] and C4.5 [24]. The results in [2] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in <ref> [5] </ref>. The latest development of feature selection in pattern recognition can be found in [7].
Reference: [6] <author> P.A. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall International, </publisher> <year> 1982. </year>
Reference-contexts: Our aim is to provide a simple and practical method that can select features for large datasets. In the following, we first review related work on feature selection. 2 Related Work The problem of feature selection has long been an active research topic within statistics and pattern recognition <ref> [30, 6, 7] </ref>, but most work in this area has dealt with linear regression [12] and is under assumptions that do not apply to most machine learning algorithms [8]. <p> The inconsistency criterion is a conservative way of achieving the "class separability" which is commonly used in pattern recognition as the basic selection criterion <ref> [6] </ref>. A limited version of this was first proposed by [2] as the MIN-FEATURES bias on a binary domain. Instead of aiming to maximize the class separability, our measure tries to maintain the original class separability of the data.
Reference: [7] <author> A. Jain and D. Zongker. </author> <title> Feature selection: Evaluation, application, and small sample performance. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(2) </volume> <pages> 153-158, </pages> <year> 1997. </year>
Reference-contexts: The number of possible subsets grows exponentially. Researchers have designed different strategies in search of optimal subsets of d features (Branch and Bound [20] and its variations [26], many heuristic and stochastic methods <ref> [5, 7] </ref>). If we view 1 these feature selection algorithms from the perspective of using an induction algorithm, as pointed out in [8], the work on feature selection can be divided into filter and wrapper models. <p> Our aim is to provide a simple and practical method that can select features for large datasets. In the following, we first review related work on feature selection. 2 Related Work The problem of feature selection has long been an active research topic within statistics and pattern recognition <ref> [30, 6, 7] </ref>, but most work in this area has dealt with linear regression [12] and is under assumptions that do not apply to most machine learning algorithms [8]. <p> The results in [2] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in [5]. The latest development of feature selection in pattern recognition can be found in <ref> [7] </ref>. To sum up, the exhaustive search approach is infeasible in practice; the heuristic search approach can reduce the search time significantly, but will fail on hard problems (e.g., the parity problem) or cannot remove redundant features.
Reference: [8] <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant feature and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann Publisher, </publisher> <year> 1994. </year>
Reference-contexts: Researchers have designed different strategies in search of optimal subsets of d features (Branch and Bound [20] and its variations [26], many heuristic and stochastic methods [5, 7]). If we view 1 these feature selection algorithms from the perspective of using an induction algorithm, as pointed out in <ref> [8] </ref>, the work on feature selection can be divided into filter and wrapper models. <p> feature selection. 2 Related Work The problem of feature selection has long been an active research topic within statistics and pattern recognition [30, 6, 7], but most work in this area has dealt with linear regression [12] and is under assumptions that do not apply to most machine learning algorithms <ref> [8] </ref>. Researchers [12, 8] pointed out that the most common assumption is monotonicity that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> Researchers <ref> [12, 8] </ref> pointed out that the most common assumption is monotonicity that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long time to run before data is reduced. Most feature selection methods <ref> [9, 12, 8] </ref> can be grouped into two categories: exhaustive or heuristic search for an optimal set of M features. <p> It works on binary, noise-free data. Its time complexity is O (min (N M ; 2 N )). 1 The monotonicity assumption is not valid for many induction algorithms used in machine learning. An example is dataset 1 (CorrAL) in Section 5 which is reproduced from <ref> [8] </ref>. 2 They proposed three heuristic algorithms to speed up the searching [2]. This is because selecting a minimal subset is a known intractable problem, and in practice, we often have to trade off the optimality of a solution for less time spent on searching. <p> These datasets are either commonly used in comparison or 8 having known relevant features. All but two (CorrAL and Parity5+5) datasets can be obtained from the UCI Repository [18]. Artificial Data: * CorrAL The data was designed in <ref> [8] </ref>. There are six binary features, A 0 ; A 1 ; B 0 ; B 1 ; I; and C. Feature I is irrelevant, feature C is correlated to the class label 75% of the time.
Reference: [9] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long time to run before data is reduced. Most feature selection methods <ref> [9, 12, 8] </ref> can be grouped into two categories: exhaustive or heuristic search for an optimal set of M features. <p> This is because selecting a minimal subset is a known intractable problem, and in practice, we often have to trade off the optimality of a solution for less time spent on searching. There are many heuristic feature selection algorithms. The Relief algorithm <ref> [9] </ref> assigns a "relevance"weight to each feature, which is meant to denote the relevance of a feature to the target concept. <p> Relief samples patterns randomly from the training set and updates the relevance values based on the difference between the selected pattern and the two nearest patterns of the same and opposite classes. According to <ref> [9] </ref>, Relief assumes two-class classification problems and does not help with redundant features. If most of the given features are relevant to the concept (including redundant features), it would select most of them even though only a fraction of them is necessary for concept description.
Reference: [10] <author> R. Kohavi. </author> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Standford University, Stanford, </institution> <address> CA, </address> <year> 1995. </year>
Reference-contexts: If a favorite induction algorithm is available, LVF can be easily transformed into LVW. The experimental results show that LVW is much slower than LVF. This finding is consistent with the results reported in <ref> [10] </ref>. 15 There may be a problem with using inconsistency as a feature selection criterion when one feature alone (such as social security number) can guarantee that there is no inconsistency in the data. Obviously, this feature is irrelevant for rule induction.
Reference: [11] <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 284-292. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Here f M and f N are value vectors of respective feature vectors F M and F N <ref> [11] </ref>. As the dimensionality of a domain expands, the number of features increases. In general, the role of feature selection is three-fold: 1. simplifying data description; 2. reducing the task of data collection; and 3. improving the quality of problem solving. <p> Although having more features enhances discriminating power in representation, having excessive features would introduce many difficulties for induction algorithms <ref> [11] </ref>. First, the time required by an induction algorithm often grows dramatically with the number of features, rendering the algorithm impractical for problems with a large number of features.
Reference: [12] <author> P. Langley. </author> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press, </publisher> <year> 1994. </year> <month> 17 </month>
Reference-contexts: One problem with the wrapper model is that it is restricted by the time complexity of a learning algorithm <ref> [12] </ref>. This time complexity is dependent on the number of features. Often the wrapper methods are prohibitively expensive to run and can be intractable for a very large number of features. <p> In the following, we first review related work on feature selection. 2 Related Work The problem of feature selection has long been an active research topic within statistics and pattern recognition [30, 6, 7], but most work in this area has dealt with linear regression <ref> [12] </ref> and is under assumptions that do not apply to most machine learning algorithms [8]. Researchers [12, 8] pointed out that the most common assumption is monotonicity that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> Researchers <ref> [12, 8] </ref> pointed out that the most common assumption is monotonicity that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long time to run before data is reduced. Most feature selection methods <ref> [9, 12, 8] </ref> can be grouped into two categories: exhaustive or heuristic search for an optimal set of M features.
Reference: [13] <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of the Seventh IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 388-391, </pages> <year> 1995. </year>
Reference-contexts: Another run of LVF with the other features will identify the right set of features. LVF only works on discrete features since it relies on the inconsistency calculation. One way is to apply a discretization algorithm (e.g., Chi2 <ref> [13] </ref>) to discretize the continuous features first before one runs LVF. Other possibilities are (1) to simply treat a continuous feature as a discrete one in some cases; and (2) to apply LVF only to the discrete features when the number of features is large. More work is needed.
Reference: [14] <author> H. Liu and R. Setiono. </author> <title> Feature selection and classification a probabilistic wrapper approach. </title> <booktitle> In Proceedings of the Ninth International Conference on Industrial and Engineering Applications of AI and ES, </booktitle> <pages> pages 419-424, </pages> <year> 1996. </year>
Reference-contexts: In order to verify that a filter feature selector can easily be turned to a wrapper one, LVW is built to prove the case <ref> [14] </ref>. If a favorite induction algorithm is available, LVF can be easily transformed into LVW. The experimental results show that LVW is much slower than LVF.
Reference: [15] <author> H. Liu and R. Setiono. </author> <title> A probabilistic approach to feature selection a filter solution. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of International Conference on Machine Learning (ICML-96), </booktitle> <pages> pages 319-327. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: To sum up, the exhaustive search approach is infeasible in practice; the heuristic search approach can reduce the search time significantly, but will fail on hard problems (e.g., the parity problem) or cannot remove redundant features. A probabilistic approach is proposed as an alternative <ref> [15] </ref> in selecting the optimal/suboptimal subset (s) of features. In the context of large sized databases, however, it would still take considerably long time to check if a subset is valid or not 2 .
Reference: [16] <author> H. Liu and W.X. Wen. </author> <title> Concept learning through feature selection. </title> <booktitle> In Proceedings of First Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <pages> pages 293-297, </pages> <year> 1993. </year>
Reference-contexts: The PRESET algorithm [19] is another heuristic feature selector that uses the theory of Rough Sets to rank the features heuristically, assuming a noise-free binary domain. In order to consider higher order relations among the features, Liu and Wen <ref> [16] </ref> suggest the use of high order information gains to select features.
Reference: [17] <author> P. Mehra, L.A. Rendell, and B.W. Wah. </author> <title> Principled constructive induction. </title> <booktitle> In Proceedings of IJCAI, </booktitle> <pages> pages 651-656, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Geometrically, this constraint can be interpreted <ref> [17] </ref> to mean that (i) such a feature takes on nearly identical values for all examples of the same class, and (ii) it takes on some different values for all examples of the other class.
Reference: [18] <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <address> http://www.ics.uci.edu/~mlearn/MLRepository.html . Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1996. </year>
Reference-contexts: These datasets are either commonly used in comparison or 8 having known relevant features. All but two (CorrAL and Parity5+5) datasets can be obtained from the UCI Repository <ref> [18] </ref>. Artificial Data: * CorrAL The data was designed in [8]. There are six binary features, A 0 ; A 1 ; B 0 ; B 1 ; I; and C. Feature I is irrelevant, feature C is correlated to the class label 75% of the time.
Reference: [19] <author> M. Modrzejewski. </author> <title> Feature selection using rough sets theory. </title> <editor> In P.B. Brazdil, editor, </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 213-226, </pages> <year> 1993. </year>
Reference-contexts: If most of the given features are relevant to the concept (including redundant features), it would select most of them even though only a fraction of them is necessary for concept description. The PRESET algorithm <ref> [19] </ref> is another heuristic feature selector that uses the theory of Rough Sets to rank the features heuristically, assuming a noise-free binary domain. In order to consider higher order relations among the features, Liu and Wen [16] suggest the use of high order information gains to select features.
Reference: [20] <author> P.M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computer, </journal> <volume> C-26(9):917-922, </volume> <month> September </month> <year> 1977. </year>
Reference-contexts: For N features, if d of them are relevant, an exhaustive approach to finding the optimal d features would require examining P d N subsets. The number of possible subsets grows exponentially. Researchers have designed different strategies in search of optimal subsets of d features (Branch and Bound <ref> [20] </ref> and its variations [26], many heuristic and stochastic methods [5, 7]). If we view 1 these feature selection algorithms from the perspective of using an induction algorithm, as pointed out in [8], the work on feature selection can be divided into filter and wrapper models.
Reference: [21] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [23], FRINGE <ref> [21] </ref> and C4.5 [24]. The results in [2] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in [5]. The latest development of feature selection in pattern recognition can be found in [7].
Reference: [22] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: The inconsistency criterion aims to retain the discriminating power of the data for multiple classes after feature selection. 3.2 Theoretical analysis Our analysis shows that LVF can give a good solution, or an optimal solution if MAX TRIES is sufficiently large. With a good pseudo random number generator <ref> [22] </ref>, selecting an optimal subset of M features can be considered as sampling without replacement.
Reference: [23] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 <ref> [23] </ref>, FRINGE [21] and C4.5 [24]. The results in [2] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in [5]. The latest development of feature selection in pattern recognition can be found in [7].
Reference: [24] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [23], FRINGE [21] and C4.5 <ref> [24] </ref>. The results in [2] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. A more detailed survey can be found in [5]. The latest development of feature selection in pattern recognition can be found in [7]. <p> If not, four features are selected: the above chosen 3 plus A1. ture selection through a learning algorithm. Among many choices, we chose C4.5 <ref> [24] </ref> and NBC [29] in our experiments because (1) C4.5 is a decision tree induction algorithm that works well on most datasets as reported by many researchers; and (2) it employs a heuristic to find the simplest tree structures.
Reference: [25] <author> T.W. Rauber. </author> <title> Inductive Pattern Classification Methods Features- Sensors. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering, Universidade Nova de Lisboa, Lisboa, </institution> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The selection process repeats until a solution is found. If no subset is found, the whole set is returned as a solution. 5 Empirical Study The error probability plays the most important role in the feature selection algorithms. Ultimately, it is always used as a meta-selection criterion <ref> [25] </ref>. That is, regardless of different feature selection algorithms, the subset with the lowest estimated error will always be selected for classification tasks. An error is caused by a wrongly classified pattern.
Reference: [26] <author> W. Siedlecki and J. Sklansky. </author> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 197-220, </pages> <year> 1988. </year>
Reference-contexts: The number of possible subsets grows exponentially. Researchers have designed different strategies in search of optimal subsets of d features (Branch and Bound [20] and its variations <ref> [26] </ref>, many heuristic and stochastic methods [5, 7]). If we view 1 these feature selection algorithms from the perspective of using an induction algorithm, as pointed out in [8], the work on feature selection can be divided into filter and wrapper models. <p> Tables 3 shows that results are consistent with the known fact that there are no bad features from the standpoint of Bayesian decision rules <ref> [26] </ref>. In all the datasets tested using NBC, only table sizes are all reduced (except Monk2) due to feature selection; error rates are not significantly changed in seven out of nine datasets.
Reference: [27] <editor> S.B. Thrun, et al. </editor> <title> The Monk's problems: A performance comarison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: C4.5 chose feature C as the root. This is an example of datasets in which if a feature like C is removed, a more accurate tree will result. * Monk1, Monk2, Monk3 The datasets were taken from <ref> [27] </ref>. They have six features. The training datasets provided were used for feature selection. Monk1 and Monk3 only need three features to describe the target concepts, but Monk2 requires all the six. The training data of Monk3 contains some noise.
Reference: [28] <author> S. Watanabe. </author> <title> Pattern Recognition: Human and Mechanical. </title> <publisher> Wiley Inter-science, </publisher> <year> 1985. </year>
Reference-contexts: The reader may do as we have done in another version of LVF to link MAX TRIES to the percentage of the total search space (2 N ) according to the desired quality of selected features. 5 considerations <ref> [28] </ref> which suggest that using a feature that is good for discrimi-nation provides compact descriptions of each of the two classes, and that these descriptions are maximally distinct.
Reference: [29] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems That Learn. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: If not, four features are selected: the above chosen 3 plus A1. ture selection through a learning algorithm. Among many choices, we chose C4.5 [24] and NBC <ref> [29] </ref> in our experiments because (1) C4.5 is a decision tree induction algorithm that works well on most datasets as reported by many researchers; and (2) it employs a heuristic to find the simplest tree structures.
Reference: [30] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year> <title> the reference. The difference between any two samples (e.g., 10% vs. 100%, or 20% vs. 100%) is most significant (light grey), significant (dark grey), or insignificant (black). 20 as the reference. The difference between any two samples (e.g., 10% vs. 100%, or 20% vs. 100%) is most significant (light grey), significant (dark grey), or insignificant (black). </title> <type> 21 </type>
Reference-contexts: Our aim is to provide a simple and practical method that can select features for large datasets. In the following, we first review related work on feature selection. 2 Related Work The problem of feature selection has long been an active research topic within statistics and pattern recognition <ref> [30, 6, 7] </ref>, but most work in this area has dealt with linear regression [12] and is under assumptions that do not apply to most machine learning algorithms [8].
References-found: 30

