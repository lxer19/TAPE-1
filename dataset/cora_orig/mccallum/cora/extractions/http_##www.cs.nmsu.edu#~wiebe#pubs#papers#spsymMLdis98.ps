URL: http://www.cs.nmsu.edu/~wiebe/pubs/papers/spsymMLdis98.ps
Refering-URL: http://www.cs.nmsu.edu/~wiebe/pubs/index.html
Root-URL: http://www.cs.nmsu.edu
Email: wiebe,kmckeeve@cs.nmsu.edu  
Title: Collocational Properties in Probabilistic Classifiers for Discourse Categorization  Applying Machine Learning to Discourse  
Author: Janyce M. Wiebe and Kenneth J. McKeever 
Date: March, 1998.  
Note: Processing, AAAI-98 Spring Symposia Series, Stanford, California,  
Web: http://www.cs.nmsu.edu/~wiebe,~kmckeeve  
Address: Las Cruces, NM 88003  
Affiliation: Dept. of Computer Science and the Computing Research Laboratory New Mexico State University  
Abstract: Properties can be mapped to features in a machine learning algorithm in different ways, potentially yielding different results. In previous work, we experimented with various approaches to organizing colloca-tional properties into features in a probabilistic classifier. It was found that one type of organization in particular, which is rarely used in NLP, allows one to take advantage of infrequent but high quality properties for an abstract discourse interpretation task. Based on an analysis of the experimental results, this paper suggests criteria for recognizing beneficial ways to include collocational information in probabilistic classifiers. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Badsberg, J. </author> <year> 1995. </year> <title> An Environment for Graphical Models. </title> <type> Ph.D. </type> <institution> diss., Aalborg University. </institution>
Reference-contexts: As different algorithms perform better on different tasks, our method provides a good alternative method for experimentation. The model search procedure can be performed using the public domain program CoCo <ref> (Badsberg 1995) </ref>. This platform allows one to experiment with many parameters, such as the order of search and the goodness-of-fit criterion used (Pedersen, Bruce, and Wiebe 1997). Such experiments can give one additional insight into the relative importance of variables and the interdependencies among them.
Reference: <author> Barnden, J.A. </author> <year> 1992. </year> <title> Belief in Metaphor: Taking Com-monsense Psychology Seriously. </title> <booktitle> Computational Intelligence 8 (3): </booktitle> <pages> 520-552. </pages>
Reference-contexts: Direct speech sentences, for example, which purport to present something close to what was said, often require introductory indirect speech sentences to integrate them into the discourse. The language used to describe private states and speech events is rich and varied <ref> (Barnden 1992) </ref>, and the classification is highly dependent on the discourse context. In a strong speech event context, for example, typical private state or action terms refer to speech events. Examples are agree, attack, estimate, concede, explore, and guide.
Reference: <author> Beeferman, D.; Berger, A.; and Lafferty, J. </author> <year> 1996. </year> <title> Text Segmentation Using Exponential Models. </title> <booktitle> Proceedings of the Conference on Empirical Methods in Discourse (EMNLP-2), </booktitle> <pages> 35-46. </pages> <institution> Association for Computational Linguistics. </institution>
Reference: <author> Berger, A.; Della Pietra, A.; and Della Pietra, V. </author> <year> 1996. </year>
Reference-contexts: The method permits the use of many features of different kinds, including n-gram properties as well as the types of features typically included in Maximum Entropy models <ref> (Berger, A. Della Pietra, and V. Della Pietra 1996) </ref> and Decision Trees (Breiman et al. 1984).
References-found: 4

