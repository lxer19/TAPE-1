URL: http://www.cs.utah.edu/~riloff/psfiles/aij.ps
Refering-URL: http://www.cs.utexas.edu/users/nl-acq/paper-history.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains  
Author: Ellen Riloff 
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science, University of Utah,  
Date: August 1996.  
Note: To appear in AI Journal, Vol. 85, Elsevier Publishers, cover date  
Abstract: A primary goal of natural language processing researchers is to develop a knowledge-based natural language processing (NLP) system that is portable across domains. However, most knowledge-based NLP systems rely on a domain-specific dictionary of concepts, which represents a substantial knowledge-engineering bottleneck. We have developed a system called AutoSlog that addresses the knowledge-engineering bottleneck for a task called information extraction. AutoSlog automatically creates domain-specific dictionaries for information extraction, given an appropriate training corpus. We have used AutoSlog to create a dictionary of extraction patterns for terrorism, which achieved 98% of the performance of a hand-crafted dictionary that required approximately 1500 person-hours to build. In this paper, we describe experiments with AutoSlog in two additional domains: joint ventures and microelectronics. We compare the performance of AutoSlog across the three domains, discuss the lessons learned about the generality of this approach, and present results from two experiments which demonstrate that novice users can generate effective dictionaries using AutoSlog.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ayuso, S. Boisen, H. Fox, H. Gish, R. Ingria, and R. Weischedel. </author> <title> BBN PLUM: Description of the PLUM System as Used for MUC-4. </title> <booktitle> In Proceedings 36 of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 177-185, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [2] <author> J. G. Carbonell. </author> <title> Subjective Understanding: Computer Models of Belief Systems. </title> <type> PhD thesis, Research Report 150, </type> <institution> Computer Science Department, Yale University, </institution> <year> 1979. </year>
Reference: [3] <author> J. G. Carbonell. </author> <title> Towards a Self-Extending Parser. </title> <booktitle> In Proceedings of the 17th Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 3-7, </pages> <year> 1979. </year>
Reference-contexts: AutoSlog does not use a semantic feature dictionary at all. Other researchers have worked on the general problem of automated dictionary construction. FOUL-UP [10] was one of the earliest AI systems that automatically learned the meanings of unknown words. The POLITICS <ref> [3] </ref> system also contained a mechanism for learning definitions for unknown words. Both FOUL-UP and POLITICS learned information about unknown words by examining contextual expectations derived from other words in the sentence.
Reference: [4] <author> R. E. Cullingford. </author> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD thesis, Research Report 116, </type> <institution> Computer Science Department, Yale University, </institution> <year> 1978. </year>
Reference: [5] <author> Gerald DeJong. </author> <title> An Overview of the FRUMP System. </title> <editor> In W. Lehnert and M. Ringle, editors, </editor> <booktitle> Strategies for Natural Language Processing, </booktitle> <pages> pages 149-177. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1982. </year>
Reference: [6] <author> Gerald DeJong and R. Mooney. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference: [7] <author> William Dolan, Lucy Vanderwende, and Stephen D. Richardson. </author> <title> Automatically Deriving Structured Knowledge Bases from On-Line Dictionaries. </title> <booktitle> In Proceedings of the First Conference of the Pacific Association for Computational Linguistics, </booktitle> <pages> pages 5-14, </pages> <year> 1993. </year>
Reference: [8] <author> D. H. Fisher. </author> <title> Knowledge Acquisition Via Incremental Conceptual Clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference: [9] <author> W. Francis and H. Kucera. </author> <title> Frequency Analysis of English Usage. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, MA, </address> <year> 1982. </year>
Reference-contexts: As input, AutoSlog requires an annotated training corpus for the domain and a few hours of manual filtering. However, NLP systems often rely on other types of tagged corpora, such as part-of-speech tagging or phrase structure bracketing (e.g., the Brown Corpus <ref> [9] </ref> and the Penn Treebank [22]). Furthermore, 35 corpus tagging for AutoSlog is less demanding than other forms of tagging because it is smaller in scope, and only the targeted information needs to be tagged (in contrast to syntactic tagging for which every word or phrase must be tagged).
Reference: [10] <author> R. H. Granger. FOUL-UP: </author> <title> A Program that Figures Out Meanings of Words from Context. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 172-178, </pages> <year> 1977. </year>
Reference-contexts: Second, PALKA relies on the semantic features associated with words to identify the extraction patterns. AutoSlog does not use a semantic feature dictionary at all. Other researchers have worked on the general problem of automated dictionary construction. FOUL-UP <ref> [10] </ref> was one of the earliest AI systems that automatically learned the meanings of unknown words. The POLITICS [3] system also contained a mechanism for learning definitions for unknown words. Both FOUL-UP and POLITICS learned information about unknown words by examining contextual expectations derived from other words in the sentence.
Reference: [11] <author> Philip J. Hayes and Steven P. Weinstein. Construe-TIS: </author> <title> A System for Content-Based Indexing of a Database of News Stories. </title> <booktitle> In Proceedings of the Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <pages> pages 49-64. </pages> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference: [12] <author> Jerry R. Hobbs, Douglas Appelt, Mabry Tyson, John Bear, and David Israel. </author> <title> SRI International: Description of the FASTUS System Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 268-275, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [13] <author> P. Jacobs and U. Zernik. </author> <title> Acquiring Lexical Knowledge from Text: A Case Study. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 739-744, </pages> <year> 1988. </year>
Reference-contexts: The POLITICS [3] system also contained a mechanism for learning definitions for unknown words. Both FOUL-UP and POLITICS learned information about unknown words by examining contextual expectations derived from other words in the sentence. RINA <ref> [13] </ref> is a language acquisition system that used multiple examples and a variety of knowledge sources to create dictionary entries for unknown words. All of these systems started with a "partial lexicon", and assumed that most of the words in the sentence were already defined.
Reference: [14] <author> J. Kim and D. Moldovan. </author> <title> Acquisition of Semantic Patterns for Information Extraction from Corpora. </title> <booktitle> In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 171-176, </pages> <address> Los Alamitos, CA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 37 </pages>
Reference-contexts: Most information extraction systems rely on a dictionary of extraction patterns that must be hand-coded for each domain [12,15,1]. However, a system called PALKA <ref> [14] </ref> has also been developed to automatically acquire patterns for information extraction.
Reference: [15] <author> G. Krupka, P. Jacobs, L. Rau, L. Childs, and I. Sider. GE NLTOOLSET: </author> <title> Description of the System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 177-185, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [16] <author> W. Lehnert. </author> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <editor> In J. Barnden and J. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 1, </volume> <pages> pages 135-164. </pages> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year>
Reference-contexts: Fig. 1. A MUC-4 terrorism text 1.2 The CIRCUS Sentence Analyzer The natural language processing group at the University of Massachusetts participated in MUC-3, MUC-4, and MUC-5 using a conceptual sentence analyzer called CIRCUS <ref> [16] </ref>. The heart of CIRCUS is a domain-specific dictionary of concept nodes. A concept node is essentially a case frame that is activated by certain linguistic expressions and extracts information from the surrounding text. Figure 2 shows a sample sentence and an instantiated concept node produced by CIRCUS.
Reference: [17] <author> W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. </author> <title> University of Massachusetts: Description of the CIRCUS System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 282-288, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [18] <author> W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. </author> <title> University of Massachusetts: MUC-4 Test Results and Analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 151-158, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [19] <author> W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and R. Williams. </author> <title> University of Massachusetts: Description of the CIRCUS System as Used for MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 223-233, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: All of the information extraction done by CIRCUS happens through concept nodes, so it is essential to have a concept node dictionary that provides good coverage of the domain. The UMass/MUC-3 system <ref> [19] </ref> used a concept node dictionary for the terrorism domain that was constructed by hand. Although the hand-crafted dictionary performed well 1 , we estimate that it required approximately 1500 person-hours to build. Furthermore, creating concept nodes by hand required system developers who were experienced with CIRCUS.
Reference: [20] <author> W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and R. Williams. </author> <title> University of Massachusetts: MUC-3 Test Results and Analysis. </title> <booktitle> In Proceedings of the Third Message Understanding Conference (MUC-3), </booktitle> <pages> pages 116-119, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As a result, the UMass/MUC-3 system was not portable across domains. To apply the system to a new domain, the entire knowledge engineering process had to be repeated. 1 The UMass/MUC-3 system had the highest combined recall and precision of all the MUC-3 systems <ref> [20] </ref>. 5 2 Automated Dictionary Construction Using AutoSlog 2.1 Motivation Building a concept node dictionary by hand was tedious and time-consuming, but in retrospect we realized that the process mainly involved looking for gaps in the dictionary and then creating definitions to fill those gaps.
Reference: [21] <author> W. Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peterson, F. Feng, C. Dolan, and S. Goldman. UMass/Hughes: </author> <title> Description of the CIRCUS System as Used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-291, </pages> <address> San Francisco, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Choosing 25 texts at random, we manually inspected the intermediate output and found that CIRCUS had extracted information with 68% recall and 54% precision. Obviously, much of the information was deleted or confounded by subsequent components (see <ref> [21] </ref> for more details). After discourse analysis, our official scores for these 25 texts were 32% recall and 45% precision, which is consistent with the overall results. <p> We eliminated entity and product/service definitions simply because they dominated the dictionary. 14 One complication was that the UMass/MUC-5 system includes two modules, TTG and Maytag, that used the original MUC-5 concept node dictionary for training (see <ref> [21] </ref>). Ideally, we should have retrained these components for each run with the new dictionary. We did retrain TTG, but we did not retrain Maytag.
Reference: [22] <author> M. Marcus, B. Santorini, and M. Marcinkiewicz. </author> <title> Building a Large Annotated Corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: As input, AutoSlog requires an annotated training corpus for the domain and a few hours of manual filtering. However, NLP systems often rely on other types of tagged corpora, such as part-of-speech tagging or phrase structure bracketing (e.g., the Brown Corpus [9] and the Penn Treebank <ref> [22] </ref>). Furthermore, 35 corpus tagging for AutoSlog is less demanding than other forms of tagging because it is smaller in scope, and only the targeted information needs to be tagged (in contrast to syntactic tagging for which every word or phrase must be tagged).
Reference: [23] <author> M. Mauldin. </author> <title> Retrieval Performance in FERRET: A Conceptual Information Retrieval System. </title> <booktitle> In Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 347-355, </pages> <year> 1991. </year>
Reference: [24] <author> T. M. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference: [25] <author> S. Montemagni and L. Vanderwende. </author> <title> Structural Patterns vs. String Patterns for Extracting Semantic Information from Dictionaries. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING-92), </booktitle> <pages> pages 546-552, </pages> <year> 1992. </year> <month> 38 </month>
Reference: [26] <institution> Proceedings of the Third Message Understanding Conference (MUC-3), </institution> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [27] <institution> Proceedings of the Fourth Message Understanding Conference (MUC-4), </institution> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Acceptance rates for the terrorism dictionary system (with the hand-crafted dictionary) and the AutoSlog version using the MUC-4 scoring program <ref> [27] </ref>. The results appear in Figure 14. System/Test Set Recall Precision F-measure MUC-4/TST3 46 56 50.51 AutoSlog/TST3 43 56 48.65 MUC-4/TST4 44 40 41.90 AutoSlog/TST4 39 45 41.79 Fig. 14. Comparative results The MUC-4 scoring program generated recall and precision scores as well as an f-measure score. <p> We evaluated each dictionary by removing the hand-crafted dictionary from the UMass/MUC-4 system and replacing it with one of the student dictionaries. Then we ran the new system on the two blind test sets TST3 and TST4 (see Section 2.4), and scored the output using the MUC-4 scoring program <ref> [27] </ref>. combined results for both TST3 and TST4). For the sake of comparison, we included the scores produced by the hand-crafted terrorism dictionary, denoted as MUC-4. Two of these data points are somewhat anomalous. <p> The best student dictionary (disregarding Student X) achieved an f-measure of 43.82 29 on TST3, which would have placed it fifth in the MUC-4 rankings (see <ref> [27] </ref>). Only four of the seventeen MUC-4 systems achieved higher scores. The student dictionary that obtained the lowest score on TST3 (35.57) would have ranked eighth in MUC-4. So all of the student dictionaries achieved TST3 scores better than half of the MUC-4 participants.
Reference: [28] <institution> Proceedings of the Fifth Message Understanding Conference (MUC-5), </institution> <address> San Francisco, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, the analysts were considered to be experts in the joint ventures domain because they were among those who manually encoded the answer key templates for the MUC-5 corpus <ref> [28] </ref>. This experiment represents a more realistic example of how dictionaries would likely be constructed for new domains. It is more realistic to expect to find people who are experts in a particular subject, than to find people who are experienced in natural language processing (much less CIRCUS in particular). <p> Scores for the analysts' dictionaries All three dictionaries achieved similar scores. Overall, both of the analysts' dictionaries achieved slightly higher f-measures than the MUC-5 dictionary. The error rates (ERR) for all three dictionaries were identical (see <ref> [28] </ref> for a description of the error rate measure), but the dictionaries filtered by the analysts achieved slightly higher recall and lower precision than the MUC-5 dictionary.
Reference: [29] <author> J. R. Quinlan. </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 80-106, </pages> <year> 1986. </year>
Reference: [30] <author> E. Riloff. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 811-816. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Using AutoSlog, the UMass/MUC-4 system was the first system that could acquire domain-specific extraction patterns automatically [17,18]. In previous work, we showed that AutoSlog could create effective extraction patterns for the domain of terrorism <ref> [30] </ref>. A dictionary generated by Au-toSlog for the terrorism domain achieved 98% of the performance of a hand-crafted dictionary that required approximately 1500 person-hours to build. The heuristics used by AutoSlog are domain-independent linguistic rules, but it was unclear whether these heuristics would be effective in other domains.
Reference: [31] <author> E. Riloff. </author> <title> Information Extraction as a Basis for Portable Text Classification Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts Amherst, </institution> <year> 1994. </year>
Reference-contexts: For the sake of completeness, we will briefly mention a few other changes. We replaced the original pp-attachment algorithm with a frequency-based pp 18 attachment algorithm (see <ref> [31] </ref> for details).
Reference: [32] <author> E. Riloff and W. Lehnert. </author> <title> Information Extraction as a Basis for High-Precision Text Classification. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 296-333, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: However, we are currently working on a new version of AutoSlog, called AutoSlog-TS, that does not need detailed text annotations at all but just a corpus of preclassified texts [34]. We have also shown that information extraction can be used to achieve high-precision text classification <ref> [32] </ref>, so the dictionaries produced by AutoSlog are useful for other language processing tasks as well. We have shown that novices can use AutoSlog effectively with only minimal training.
Reference: [33] <author> E. Riloff and W. G. Lehnert. </author> <title> A Dictionary Construction Experiment with Domain Experts. </title> <booktitle> In Proceedings of the TIPSTER Text Program (Phase I), </booktitle> <pages> pages 257-259, </pages> <address> San Francisco, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The second experiment involved two government analysts who manually filtered a dictionary produced by AutoSlog for the joint ventures domain <ref> [33] </ref>. In contrast to the previous experiment, the government analysts had no background in natural language processing at all, or any experience with CIRCUS or the UMass/MUC-5 system.
Reference: [34] <author> E. Riloff and J. Shoen. </author> <title> Automatically Acquiring Conceptual Patterns Without an Annotated Corpus. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> pages 148-161, </pages> <year> 1995. </year>
Reference-contexts: However, we are currently working on a new version of AutoSlog, called AutoSlog-TS, that does not need detailed text annotations at all but just a corpus of preclassified texts <ref> [34] </ref>. We have also shown that information extraction can be used to achieve high-precision text classification [32], so the dictionaries produced by AutoSlog are useful for other language processing tasks as well. We have shown that novices can use AutoSlog effectively with only minimal training.
Reference: [35] <author> P. Utgoff. ID5: </author> <title> An Incremental ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 107-120, </pages> <year> 1988. </year>
Reference: [36] <author> R. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. </author> <title> Coping with Ambiguity and Unknown Words through Probabilistic Models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382, </pages> <year> 1993. </year> <month> 39 </month>
Reference-contexts: In contrast, AutoSlog builds new dictionary definitions completely from scratch and depends only on a part-of-speech lexicon, which can be readily obtained from machine-readable dictionaries or a statistical part-of-speech tagger (e.g., POST <ref> [36] </ref>). One exception is recent work on automatically deriving knowledge from on-line dictionaries (see [7,25]). This research applies syntactic and lexical patterns to the entries in an on-line dictionary to derive semantic relationships between words.
References-found: 36

