URL: http://theory.lcs.mit.edu/~robdep/PS/wdag97-CDS97.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~robdep/papers.html
Root-URL: 
Email: chlebus@mimuw.edu.pl  robdep@theory.lcs.mit.edu  aas@eng2.uconn.edu  
Phone: 2  3  
Title: Performing tasks on restartable message-passing processors  
Author: Bogdan S. Chlebus and Roberto De Prisco and Alex A. Shvartsman 
Address: Banacha 2, 02-097 Warszawa, Poland.  545 Technology Square NE43-368, Cambridge, MA 02139, USA.  191 Auditorium Road, U-155, Storrs, CT 06269, USA.  
Affiliation: 1 Instytut Informatyki, Uniwersytet Warszawski,  Laboratory for Computer Science, Massachusetts Institute of Technology,  Department of Computer Science and Engineering, University of Connecticut,  
Abstract: This work presents new algorithms for the "Do-All" problem that consists of performing t tasks reliably in a message-passing synchronous system of p fault-prone processors. The algorithms are based on an aggressive coordination paradigm in which multiple coordinators may be active as the result of failures. The first algorithm is tolerant of f &lt; p stop-failures and it does not allow restarts. It has the available processor steps complexity S = O((t + p log p= log log p) log f) and the message complexity M = O(t + p log p= log log p + f p). Unlike prior solutions, our algorithm uses redundant broadcasts when encountering failures and, for large f, it has better S complexity. This algorithm is used as the basis for another algorithm which tolerates any pattern of stop-failures and restarts. This new algorithm is the first solution for the Do-All problem that efficiently deals with processor restarts. Its available processor steps complexity is S = O((t + p log p + f ) minflog p; log f g), and its message complexity is M = O(t + p log p + f p), where f is the number of failures.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. De Prisco, A. Mayer, and M. Yung, </author> <title> "Time-Optimal Message-Efficient Work Performance in the Presence of Faults," </title> <booktitle> in Proc. 13th ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994, </year> <pages> pp. 161-172. </pages>
Reference-contexts: This allows algorithm optimizations which keep the number of messages small, because processors can afford to wait to obtain sufficient information by not receiving messages in specific time intervals. De Prisco, Mayer and Yung also consider the Do-All problem without processor restarts in their study <ref> [1] </ref>. Their goal is the development of fast and message-efficient algorithms. The work measure they consider is the available processor steps S (introduced by Kanellakis and Shvartsman [6]). <p> Our contributions. In this paper we solve the Do-All problem in the setting where the p processors are subject to dynamic stop-failures and restarts. The complexity concerns in this paper follow the criteria established in <ref> [1] </ref>. We seek algorithmic efficiency with respect to both the work, expressed as available processor steps S, and the communication, expressed as the message complexity M . We want to minimize S, having M as small as possible. <p> Our algorithm AN is more efficient in terms of S than the algorithms of <ref> [1] </ref> and [3] when f , p and t are comparable; the algorithm also has efficient message complexity. <p> Lastly, our approach presents a new venue for optimizing Do-All solutions and for beating the (t + (f + 1) p) lower bound of stage-checkpointing algorithms <ref> [1] </ref>. 4 All logarithms are to the base 2; the expression "log f" stands for 1 when f &lt; 2 and log 2 f otherwise. Review of prior work. Dwork, Halpern and Waarts [2] developed the first algorithms for the Do-All problem. <p> The message complexity is quadratic in p for the fault-free case, and in the presence of a failure pattern of f &lt; p failures, the message complexity degrades to fi (f p 2 ). De Prisco, Mayer and Yung <ref> [1] </ref> present an algorithm which has the available processor steps O (t + (f + 1)p) and message complexity O ((f + 1)p). The available processor steps and communication efficiency approach requires keeping all the processors busy doing tasks, simultaneously controlling the amount of communication. <p> De Prisco, Mayer and Yung were the first to report results on Do-All algorithms in the fail-stop case using this efficiency approach. To avoid the quadratic upper bound for S substantial processing slackness (p t t) is assumed. In <ref> [1] </ref> a lower bound of (t + (f + 1)p) for algorithms that use the stage-checkpointing strategy is proved. However there are algorithmic strategies that have the potential of circumventing the quadratic bound. Consider the following scenarios. <p> One interesting result of our paper is showing that an algorithm can be developed which has both the available processor steps which is always subquadratic, and the number of messages which is quadratic only for f comparable to p, even with restarts. The algorithm in <ref> [1] </ref> is designed so that at each step there is at most one coordinator; if the current coordinator fails then the next available processor takes over, according to a time-out strategy. <p> Eventually there remains only one processor which has to perform all the tasks, because it has never received any messages, this gives the remaining (t) part. A related lower-bound argument for stage-checkpointing strategies is formally presented in <ref> [1] </ref>. Moreover, when processor restarts allowed, any algorithm that relies on a single coordinator for information gathering might not terminate (the adversary can always kill the current coordinator, keeping alive all the other processors so that no progress is made). <p> Another important algorithm was developed by Galil, Mayer and Yung [3]. Working in the context of Byzantine agreement with stop-failures (for which they establish a message-optimal solution), they improved the message complexity of <ref> [1] </ref> to O (f p " + minff + 1; log pgp), for any positive ", while achieving the available processor steps complexity of O (t + (f + 1) p).
Reference: 2. <author> C. Dwork, J. Halpern, O. Waarts, </author> <title> "Performing Work Efficiently in the Presence of Faults", </title> <note> to appear in SIAM J. on Computing, </note> <author> prelim. </author> <title> vers. appeared as Accomplishing Work in the Presence of Failures in Proc. </title> <booktitle> 11th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 91-102, </pages> <year> 1992. </year>
Reference-contexts: The research of the third author was substantially done at the Mas-sachusetts Institute of Technology. The research of the first and the third authors was partly done while visiting Heinz Nixdorf Institut, Universitat-GH Paderborn. by Dwork, Halpern and Waarts in their pioneering work <ref> [2] </ref>. They developed several efficient algorithms for this problem in the setting where the processors are subject to fail-stop (or crash) failures and where the tasks can be performed using the at-least-once execution semantics (i.e., the tasks either are or can be made idempotent). In the setting of [2], the cost <p> pioneering work <ref> [2] </ref>. They developed several efficient algorithms for this problem in the setting where the processors are subject to fail-stop (or crash) failures and where the tasks can be performed using the at-least-once execution semantics (i.e., the tasks either are or can be made idempotent). In the setting of [2], the cost of local computation, whether performing low-level administrative tasks or idling, is considered to be negligible compared to the costs of performing each of the t tasks. <p> Review of prior work. Dwork, Halpern and Waarts <ref> [2] </ref> developed the first algorithms for the Do-All problem. <p> Another algorithm in <ref> [2] </ref> (protocol C) has effort O (t + p log p). This includes optimal work of O (t + p), message complexity of O (p log p), and time O (p 2 (t + p)2 t+p ).
Reference: 3. <author> Z. Galil, A. Mayer, and M. Yung, </author> <title> "Resolving Message Complexity of Byzantine Agreement and Beyond," </title> <booktitle> in Proc. 36th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1995, </year> <pages> pp. 724-733. </pages>
Reference-contexts: The authors successfully pursue algorithmic efficiency in terms of what they call the lexicographic optimization of complexity measures. This means firstly achieving efficient work, then efficient communication complexity. A similar approach to efficiency is pursued by Galil, Mayer and Yung <ref> [3] </ref> who also derive a very efficient Do-All solution for stop-failures. Our contributions. In this paper we solve the Do-All problem in the setting where the p processors are subject to dynamic stop-failures and restarts. The complexity concerns in this paper follow the criteria established in [1]. <p> Our algorithm AN is more efficient in terms of S than the algorithms of [1] and <ref> [3] </ref> when f , p and t are comparable; the algorithm also has efficient message complexity. Both algorithm AN and algorithm AR come within a log f (and log p) factor of the lower bounds [6] for any algorithms that balance loads of surviving processors in each constant-time step. <p> Moreover, when processor restarts allowed, any algorithm that relies on a single coordinator for information gathering might not terminate (the adversary can always kill the current coordinator, keeping alive all the other processors so that no progress is made). Another important algorithm was developed by Galil, Mayer and Yung <ref> [3] </ref>.
Reference: 4. <author> V. Hadzilacos and S. Toueg, </author> <title> "Fault-Tolerant Broadcasts and Related Problems," in Distributed Systems, </title> <editor> 2nd Ed., S. Mullender, Ed., </editor> <publisher> Addison-Wesley and ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Indeed the real advantage of this approach is that it can be naturally extended to deal with processor failures and restarts, with graceful deterioration of performance. The improvements in S, however, come at a cost. Both of our algorithms assume reliable multicast <ref> [4] </ref>. Prior solutions do not make this assumption, although they do not solve the problem of processor restarts. The availability of reliable broadcast simplifies solutions for non-restartable processors, but dealing with processor restarts remains a challenge even when such broadcast is available. <p> Secondly, by separating the concerns between the reliability of processors and the underlying communication medium, we are able to formulate solutions at a higher level of modularity so that one can take advantage of efficient reliable broadcast algorithms (cf. <ref> [4] </ref>) without altering the overall algorithmic approach. <p> This condition rules out thrashing adversaries that repeatedly stop and restart processors in such a way that any progress made by the computation is lost (like in the above example). We assume that reliable multicast <ref> [4] </ref> is available. With reliable multicast a processor q can send a message to any set P P of processors in its send substep. All processors in P that are operational during the entire following receive substep receive the message sent by q.
Reference: 5. <author> P.C. Kanellakis, D. Michailidis, A.A. Shvartsman, </author> <title> "Controlling Memory Access Concurrency in Efficient Fault-Tolerant Parallel Algorithms", </title> <journal> Nordic J. of Computing, </journal> <volume> vol. 2, </volume> <pages> pp. 146-180, </pages> <year> 1995 </year> <month> (prel. </month> <title> vers. </title> <booktitle> in WDAG-7, </booktitle> <pages> pp. 99-114, </pages> <year> 1993). </year>
Reference-contexts: Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis [8], Martel, Park and Subramonian [11] and Kedem, Palem, Rabin and Raghunathan [9]. Kanellakis, Michailidis and Shvartsman <ref> [5] </ref> developed a technique for controlling redundant concurrent access to shared memory in algorithms with processor stop-failures. This is done with the help of a structure they call processor priority tree. In this work we use a similar structure in the qualitatively different message-passing setting.
Reference: 6. <author> P.C. Kanellakis and A.A. Shvartsman, </author> <title> "Efficient Parallel Algorithms Can Be Made Robust," </title> <journal> Distributed Computing, </journal> <volume> vol. 5, </volume> <pages> pp. 201-217, </pages> <year> 1992; </year> <month> prel. </month> <title> version in Proc. </title> <booktitle> of the 8th ACM Symp. on Principles of Distributed Computing, </booktitle> <year> 1989, </year> <pages> pp. 211-222. </pages>
Reference-contexts: De Prisco, Mayer and Yung also consider the Do-All problem without processor restarts in their study [1]. Their goal is the development of fast and message-efficient algorithms. The work measure they consider is the available processor steps S (introduced by Kanellakis and Shvartsman <ref> [6] </ref>). This measure accounts for all steps taken by the processors, that is, the steps involved in performing the Do-All tasks and any other computation steps taken by the processors. Optimization of S leads to fast algorithms whose performance degrades gracefully with failures. <p> Both algorithm AN and algorithm AR come within a log f (and log p) factor of the lower bounds <ref> [6] </ref> for any algorithms that balance loads of surviving processors in each constant-time step. We achieve this by deploying an aggressive processor coordination strategy, in which more than one processor may assume the role of the coordinator , the processor whose responsibility is to ensure the progress of the computation. <p> The Do-All problem for the shared-memory model of computation, where it is called Write-All , was introduced and studied by Kanellakis and Shvarts-man <ref> [6, 7] </ref>. Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis [8], Martel, Park and Subramonian [11] and Kedem, Palem, Rabin and Raghunathan [9]. <p> Let S a be the part of S spent during all the attended phases and S u be the part of S spent during all the unattended phases. Hence S is S a + S u . The following lemma uses the construction by Martel <ref> [10, 6] </ref>. Lemma 8. In any execution of algorithm AN, S a = O (t + p log p= log log p). Lemma 9. In any execution of algorithm AN, S u = O (S a log f ). Theorem 10. <p> Thus the work of algorithm AN is within a log f (and hence also log p) factor of the lower bound of (t + p log p= log log p) <ref> [6] </ref> for any algorithm that performs tasks by balancing loads of surviving processors in each time step. For each attended phase ff i 2 A, let d i be some distinguished active coordinator, we refer to d i as the designated coordinator of phase ff i .
Reference: 7. <author> P.C. Kanellakis and A.A. Shvartsman, </author> <title> Fault-Tolerant Parallel Computation, ISBN 0-7923-9922-6, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1997. </year>
Reference-contexts: The Do-All problem for the shared-memory model of computation, where it is called Write-All , was introduced and studied by Kanellakis and Shvarts-man <ref> [6, 7] </ref>. Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis [8], Martel, Park and Subramonian [11] and Kedem, Palem, Rabin and Raghunathan [9].
Reference: 8. <author> Z.M. Kedem, K.V. Palem, and P. Spirakis, </author> <title> "Efficient Robust Parallel Computations," </title> <booktitle> Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 138-148, </pages> <year> 1990. </year>
Reference-contexts: The Do-All problem for the shared-memory model of computation, where it is called Write-All , was introduced and studied by Kanellakis and Shvarts-man [6, 7]. Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis <ref> [8] </ref>, Martel, Park and Subramonian [11] and Kedem, Palem, Rabin and Raghunathan [9]. Kanellakis, Michailidis and Shvartsman [5] developed a technique for controlling redundant concurrent access to shared memory in algorithms with processor stop-failures. This is done with the help of a structure they call processor priority tree.
Reference: 9. <author> Z.M. Kedem, K.V. Palem, M.O. Rabin, A. Raghunathan, </author> <title> "Efficient Program Transformations for Resilient Parallel Computation via Randomization," </title> <booktitle> in Proc. 24th ACM Symp. on Theory of Comp., </booktitle> <pages> pp. 306-318, </pages> <year> 1992. </year>
Reference-contexts: Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis [8], Martel, Park and Subramonian [11] and Kedem, Palem, Rabin and Raghunathan <ref> [9] </ref>. Kanellakis, Michailidis and Shvartsman [5] developed a technique for controlling redundant concurrent access to shared memory in algorithms with processor stop-failures. This is done with the help of a structure they call processor priority tree. In this work we use a similar structure in the qualitatively different message-passing setting.
Reference: 10. <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: Let S a be the part of S spent during all the attended phases and S u be the part of S spent during all the unattended phases. Hence S is S a + S u . The following lemma uses the construction by Martel <ref> [10, 6] </ref>. Lemma 8. In any execution of algorithm AN, S a = O (t + p log p= log log p). Lemma 9. In any execution of algorithm AN, S u = O (S a log f ). Theorem 10.
Reference: 11. <author> C. Martel, R. Subramonian, and A. Park, </author> <title> "Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs," </title> <booktitle> in Proc. 32d IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> 1990. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Parallel computation using the iterated Do-All paradigm is the subject of several subsequent papers, most notably the work of Kedem, Palem and Spirakis [8], Martel, Park and Subramonian <ref> [11] </ref> and Kedem, Palem, Rabin and Raghunathan [9]. Kanellakis, Michailidis and Shvartsman [5] developed a technique for controlling redundant concurrent access to shared memory in algorithms with processor stop-failures. This is done with the help of a structure they call processor priority tree.
References-found: 11

