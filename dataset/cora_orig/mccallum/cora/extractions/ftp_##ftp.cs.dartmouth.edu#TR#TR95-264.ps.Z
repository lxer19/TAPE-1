URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-264.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-264/
Root-URL: http://www.cs.dartmouth.edu
Title: Process Migration for Heterogeneous Distributed Systems  
Author: Matt Bishop Mark Valence Leonard F. Wisniewski 
Date: August 21, 1995  
Address: Hanover, NH  
Affiliation: Department of Computer Science University of California at Davis  Sassafras Software Inc.  Department of Computer Science Dartmouth College  
Pubnum: Dartmouth PCS-TR95-264  
Abstract: The policies and mechanisms for migrating processes in a distributed system become more complicated in a heterogeneous environment, where the hosts may differ in their architecture and operating systems. These distributed systems include a large quantity and great diversity of resources which may not be fully utilized without the means to migrate processes to the idle resources. In this paper, we present a graph model for single process migration which can be used for load balancing as well as other non-traditional scenarios such as migration during the graceful degradation of a host. The graph model provides the basis for a layered approach to implementing the mechanisms for process migration in a Heterogeneous Migration Facility (HMF). HMF provides the user with a library to automatically migrate processes and checkpoint data.
Abstract-found: 1
Intro-found: 1
Reference: [AF89] <author> Yeshayahu Artsy and Raphael Finkel. </author> <title> Designing a process migration facility: The Charlotte experience. </title> <journal> Computer, </journal> <volume> 22(9) </volume> <pages> 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The THERE programming language supports the construction of execution environments for services, the definition of interfaces for clients to remote services, and system-independent communication between client and server. Charlotte is a message-based distributed operating system. Each host runs a kernel that handles short-term scheduling and IPC <ref> [AF89] </ref>. The designers of Charlotte placed the migration policy outside the kernel in a utility. Thus, Charlotte provides the flexibility to easily change the migration policy while keeping it close enough to the kernel for efficient data exchange. Mermaid extends distributed shared memory to a heterogeneous environment [ZSM90].
Reference: [Bis88] <author> Matt Bishop. </author> <title> An application of a fast Data Encryption Standard implementation. </title> <journal> Computing Systems, </journal> <volume> 3(1) </volume> <pages> 221-254, </pages> <month> Summer </month> <year> 1988. </year>
Reference-contexts: We have also rewritten the password cracker of <ref> [Bis88] </ref> using HMF.
Reference: [BL88] <author> Brian N. Bershad and Henry M. Levy. </author> <title> A remote computation facility for a heterogeneous environment. </title> <journal> Computer, </journal> <volume> 21(5) </volume> <pages> 50-60, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The copy-on-reference mechanism assumes that a process will only access a small fraction of its address space during remote execution. The Heterogeneous Environment for Remote Execution (THERE) provides a "meta-service" to simplify the adaptation of non-networked, non-heterogeneous applications to a distributed heterogeneous environment <ref> [BL88] </ref>. In this context, the source and destination hosts are clients and servers, respectively. The THERE programming language supports the construction of execution environments for services, the definition of interfaces for clients to remote services, and system-independent communication between client and server. Charlotte is a message-based distributed operating system.
Reference: [DO87] <author> Fred Douglis and John Ousterhout. </author> <title> Process migration in the Sprite operating system. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 18-25, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Distributed Library differs from standard remote execution facilities by allowing a remote environment to remain established over multiple remote procedure invocations. The Sprite distributed operating system accommodates process migration in a network file system environment <ref> [DO91, DO87] </ref>. Sprite uses a low-latency kernel-to-kernel remote procedure call facility. Each process originates on a home node which provides many location-dependent services (e.g., the time of day). When a migration occurs, the source host sends dirty pages to the 3 network file server. <p> Residual dependencies Leaving residual state of a process on its source machine reduces the immediate cost of migration. If a process must perform certain functions on the source machine <ref> [DO87, DO91] </ref> or if the source machine is responsible for forwarding messages after the migration completes [PM83], the cost of the migration includes these additional costs. The edge weights should account for the future costs accrued in accessing and maintaining this residual state.
Reference: [DO91] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Distributed Library differs from standard remote execution facilities by allowing a remote environment to remain established over multiple remote procedure invocations. The Sprite distributed operating system accommodates process migration in a network file system environment <ref> [DO91, DO87] </ref>. Sprite uses a low-latency kernel-to-kernel remote procedure call facility. Each process originates on a home node which provides many location-dependent services (e.g., the time of day). When a migration occurs, the source host sends dirty pages to the 3 network file server. <p> Residual dependencies Leaving residual state of a process on its source machine reduces the immediate cost of migration. If a process must perform certain functions on the source machine <ref> [DO87, DO91] </ref> or if the source machine is responsible for forwarding messages after the migration completes [PM83], the cost of the migration includes these additional costs. The edge weights should account for the future costs accrued in accessing and maintaining this residual state.
Reference: [Hac89] <author> Anna Hac. </author> <title> A distributed algorithm for performance improvement through file replication, file migration, and process migration. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1459-1470, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Condor is flexible enough to use on a variety of Unix platforms in exchange for the performance penalty of residing outside the kernel. 3 Graph model of single process migration Current theoretical research couples process migration with load balancing <ref> [Hac89] </ref>. Queuing theory provides the basis for these models since it is well-suited for modeling load balancing [Kle75].
Reference: [Kle75] <author> Leonard Kleinrock. </author> <title> Queuing Systems Volume I: Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Queuing theory provides the basis for these models since it is well-suited for modeling load balancing <ref> [Kle75] </ref>. We introduce a new model which encompasses load balancing as well as other useful purposes for process migration such as remote processing, graceful degradation, and efficient access to resources other than CPU time. Queue-based systems do not allow the analysis of these latter uses of migration.
Reference: [LS92] <author> Michael Litzkow and Marvin Solomon. </author> <title> Supporting checkpointing and process migration outside the Unix kernel. </title> <booktitle> In Proceedings of the USENIX Winter 1992 Technical Conference, </booktitle> <pages> pages 283-290, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: When a migration occurs, the source host sends dirty pages to the 3 network file server. After resuming on the destination host, the migrated process demand-pages from the network file server. Condor provides facilities outside the kernel for checkpointing and process migration <ref> [LS92] </ref>. Condor is flexible enough to use on a variety of Unix platforms in exchange for the performance penalty of residing outside the kernel. 3 Graph model of single process migration Current theoretical research couples process migration with load balancing [Hac89].
Reference: [MvRT + 90] <author> Sape J. Mullender, Guido van Rossum, Andrew S. Tanenbaum, Robbert van Renesse, and Hans van Staveren. </author> <title> Amoeba: A distributed operating system for the 1990s. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 44-53, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For distributed systems which contain a homogeneous pool of processors to achieve a high degree of concurrency in the computation, any lightly-loaded host becomes a candidate for receiving a migrateable process <ref> [vRvST89, MvRT + 90] </ref>. Since the processing speed and priority of the process are likely to be the same on each host, the load at a particular host is the primary factor in determining the vertex weight corresponding to that host.
Reference: [NBL + 88] <author> David Notkin, Andrew P. Black, Edward D. Lazowska, Henry M. Levy, Jan Sanislo, and John Zahorjan. </author> <title> Interconnecting heterogeneous computer systems. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 258-273, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Since heterogeneous distributed systems are typically spread out over large geographic areas (e.g., wide area networks), they usually possess a large quantity of raw computing power as well as a greater diversity of resources <ref> [NBL + 88] </ref>. Migrating a process between two different host architectures and operating systems involves many complex tasks which include the translation and transfer of state as well as the coordination of the source and destination hosts.
Reference: [Nic87] <author> David A. Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium of Operating System Principles, </booktitle> <pages> pages 5-12, </pages> <month> November </month> <year> 1987. </year> <month> 19 </month>
Reference-contexts: The system assigns low values to the hosts that can execute the real-time process and high values to the hosts that cannot accommodate the process. After a user leaves a host, many processes become idle, freeing up resources for migrating processes <ref> [Nic87] </ref>. Upon returning to the host, the user may want migrated processes to be evicted. Thus, the vertex weight for that host becomes infinity for each of the migrated processes.
Reference: [PM83] <author> Michael L. Powell and Barton P. Miller. </author> <title> Process migration in DEMOS/MP. </title> <booktitle> In Pro--ceedings of the Ninth ACM Symposium of Operating System Principles, </booktitle> <pages> pages 110-119, </pages> <year> 1983. </year>
Reference-contexts: We shall revisit many of these key features throughout the rest of the paper. 2 Demos/MP is a distributed operating system with a migration mechanism for location--transparent reliable interprocess messages <ref> [PM83] </ref>. A process sends a message to a link. A link is a protected global process address accessed via a local name space. A destination host accepts a process and subsequently allocates space for the process state. <p> Residual dependencies Leaving residual state of a process on its source machine reduces the immediate cost of migration. If a process must perform certain functions on the source machine [DO87, DO91] or if the source machine is responsible for forwarding messages after the migration completes <ref> [PM83] </ref>, the cost of the migration includes these additional costs. The edge weights should account for the future costs accrued in accessing and maintaining this residual state. The graph model represents this situation by leaving an inactive copy of the process at the source host.
Reference: [RR81] <author> Richard F. Rashid and George G. Robertson. </author> <title> Accent: A communication oriented network operating system kernel. </title> <booktitle> In Proceedings of the Eighth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 64-75, </pages> <year> 1981. </year>
Reference-contexts: Once the number of dirty pages is small or constant, the source host freezes the process to allow the copy of the remaining dirty pages. Accent is a distributed computing environment with closely integrated IPC and virtual memory facilities <ref> [RR81, Zay87] </ref>. In Accent, after a migration occurs, the destination host demand-pages from the source host in a copy-on-reference manner, reducing the time to initially transfer the state. The copy-on-reference mechanism assumes that a process will only access a small fraction of its address space during remote execution. <p> We also consider the complexity of the IPC implementation when evaluating the IPC cost. This consideration assigns costs to the overhead of sending a message which may be quite high in systems with complex IPC facilities such as Accent <ref> [RR81] </ref>. We define the message-send overhead S i as the cost (in microseconds) of sending a message from vertex i. Similarly, we define the message-receive overhead R i as the cost (in microseconds) of receiving a message at vertex i.
Reference: [TLC85] <author> Marvin M. Theimer, Keith A. Lantz, and David R. Cheriton. </author> <title> Preemptable remote execution facilities for the V-system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium of Operating System Principles, </booktitle> <pages> pages 2-12, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The original site maintains the names of all processes that are currently executing remotely. The V system includes process and memory management facilities in the kernel as well as a program manager outside the kernel <ref> [TLC85] </ref>. The program manager makes the migration policy decision whether to accept a migrating process. The V system uses pre-copying to transfer the state of the migrating process to the destination host. That is, a process continues to execute during the copy of its address space. <p> For the copy-on-reference mechanism used in Accent [Zay87], the transfer cost only includes the cost of the pages accessed after the migration. Each transferred page, however, includes a separate overhead cost. For a system such as V <ref> [TLC85] </ref>, a cost may be associated with a page multiple times if that page becomes dirty during pre-copying. The vertex weights, however, would be lower since pre-copying reduces the amount of remaining execution for the process. <p> The destination host, however, could immediately migrate the process again to another host to balance the load. In these situations, we consider the first processor to accept a migration as the approximate least-loaded machine <ref> [TLC85] </ref>. Thus, the distance of a host from the current host may be the determining factor in adjusting the vertex weights appropriately. Distance becomes a more significant factor in a LAN or WAN. <p> This overhead would be considered a part of the total overhead (of other processes) running on the host. The complexity of the policy determines whether it may be a burden on a host. A simple policy such as the first-response policy of the V system <ref> [TLC85] </ref> requires little additional overhead. 10 However, a complicated load balancing policy could require a substantial portion of processing time. In this case, a centralized dedicated server relieves the hosts from this overhead.
Reference: [vRvST89] <author> Robbert van Renesse, Hans van Staveren, and Andrew S. Tanenbaum. </author> <title> Performance of the Amoeba distributed operating system. </title> <journal> Software Practice and Experience, </journal> <volume> 19(3) </volume> <pages> 223-234, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: For distributed systems which contain a homogeneous pool of processors to achieve a high degree of concurrency in the computation, any lightly-loaded host becomes a candidate for receiving a migrateable process <ref> [vRvST89, MvRT + 90] </ref>. Since the processing speed and priority of the process are likely to be the same on each host, the load at a particular host is the primary factor in determining the vertex weight corresponding to that host.
Reference: [WM85] <author> Yung-Terng Wang and Robert J.T. Morris. </author> <title> Load sharing in distributed systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 34(3) </volume> <pages> 204-217, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: All host machines in a distributed system possess raw computing power, at the very least. Load balancing 1 attempts to better utilize this raw computing power <ref> [WM85] </ref>.
Reference: [WPE + 83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. </author> <title> The LOCUS distributed operating system. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 49-70, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: The transfer of the process state and program (code, data and stack) occurs from source kernel to destination kernel. After the migration, Demos/MP leaves a forwarding address on the source host. LOCUS provides a Unix-compatible remote execution facility <ref> [WPE + 83] </ref>. LOCUS uses global names consisting of the original host of a process and a local identifier generated by the remote execution site. The original site maintains the names of all processes that are currently executing remotely.
Reference: [XDR] <institution> External Data Representation Standard: Protocol Specification. RFC1050, ARPA Network Information Center. </institution>
Reference-contexts: The goals of this design include no kernel support, minimal daemon, system and root processes, modularity, compatability, and completeness. The four levels of implementation are the following: 1. Library calls on specific structures. Essentially, the user uses a translation package for specific structures, such as XDR <ref> [XDR] </ref>. 2. Library calls on arbitrary structures. This level of support abstracts the lower-level XDR. All structures are automatically available to be saved. The user must still explicitly invoke the calls to save the structures. 3. Structure tracking. This level of support abstracts the lower-level library.
Reference: [Yam90] <author> Michael J. Yamasaki. </author> <title> Distributed Library. </title> <type> Technical Report RNR-90-008, </type> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Mermaid provides support for converting data to the appropriate type before use by a particular host. The Mermaid project demonstrates the necessities, performance, and limitations involved in data conversion. The Distributed Library establishes sessions in remote execution environments across a network with various types of machines <ref> [Yam90] </ref>. Distributed Library differs from standard remote execution facilities by allowing a remote environment to remain established over multiple remote procedure invocations. The Sprite distributed operating system accommodates process migration in a network file system environment [DO91, DO87]. Sprite uses a low-latency kernel-to-kernel remote procedure call facility. <p> Thus, the vertex weight for that host becomes infinity for each of the migrated processes. A heterogeneous distributed system provides a large number of different resources each of which may be connected to a specific host <ref> [Yam90] </ref>. When a process requests a specific resource which is not available on its current host, it must migrate to another host with that resource.
Reference: [Zay87] <author> Edward R. Zayas. </author> <title> Attacking the process migration bottleneck. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium of Operating System Principles, </booktitle> <pages> pages 13-22, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Once the number of dirty pages is small or constant, the source host freezes the process to allow the copy of the remaining dirty pages. Accent is a distributed computing environment with closely integrated IPC and virtual memory facilities <ref> [RR81, Zay87] </ref>. In Accent, after a migration occurs, the destination host demand-pages from the source host in a copy-on-reference manner, reducing the time to initially transfer the state. The copy-on-reference mechanism assumes that a process will only access a small fraction of its address space during remote execution. <p> For the copy-on-reference mechanism used in Accent <ref> [Zay87] </ref>, the transfer cost only includes the cost of the pages accessed after the migration. Each transferred page, however, includes a separate overhead cost. For a system such as V [TLC85], a cost may be associated with a page multiple times if that page becomes dirty during pre-copying. <p> The benefits of demand 7 translation are very similar to those of demand paging. For example, when using a network file server, demand translation occurs at each page fault. In a copy-on-reference scheme such as Accent <ref> [Zay87] </ref>, a translation occurs only when the destination host needs a page from the source machine or the network file server. If the migration mechanism transfers the entire virtual address space, pages can remain untranslated on the destination host until referenced.
Reference: [ZSM90] <author> Songnian Zhou, Michael Stumm, and Tim McInerney. </author> <title> Extending distributed shared memory to heterogeneous environments. </title> <booktitle> In Proceedings of the 10th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 30-37, </pages> <year> 1990. </year> <month> 20 </month>
Reference-contexts: The designers of Charlotte placed the migration policy outside the kernel in a utility. Thus, Charlotte provides the flexibility to easily change the migration policy while keeping it close enough to the kernel for efficient data exchange. Mermaid extends distributed shared memory to a heterogeneous environment <ref> [ZSM90] </ref>. Mermaid provides support for converting data to the appropriate type before use by a particular host. The Mermaid project demonstrates the necessities, performance, and limitations involved in data conversion. The Distributed Library establishes sessions in remote execution environments across a network with various types of machines [Yam90].
References-found: 21

