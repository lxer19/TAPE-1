URL: http://polaris.cs.uiuc.edu/reports/1432.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: AUTOMATIC ARRAY PRIVATIZATION AND DEMAND-DRIVEN SYMBOLIC ANALYSIS  
Author: BY PENG TU 
Degree: B.S.Eng., Shanghai Jiao Tong University, 1984 M.S.Eng., Shanghai Jiao Tong University, 1987 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [ABCF88] <author> F. Allen, M. Burke, R. Cytron, and J. Ferrante. </author> <title> An overview of the ptran analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (5):617-640, October 1988. 
Reference-contexts: If the recurrence can be transformed into an equivalent expression that only depends on the loop indices, replacing the induction variable with the equivalent expression enables the loop to be executed in parallel. Various techniques have been developed for linear induction variable recognition [Ken81][ASU86] <ref> [ABCF88] </ref>. Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks [CKPK90], the compiler needs more symbolic analysis capabilities [BE94b]. <p> If the recurrence can be transformed into an equivalent expression that depends only on the loop index, then replacing the induction variable with the equivalent expression enables the loop to be executed in parallel. Various techniques have been developed for linear induction variable recognition [Ken81][ASU86] <ref> [ABCF88] </ref>. Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks [CKPK90], the compiler needs more symbolic analysis capabilities [BE94b].
Reference: [AH90] <author> Z. Ammarguellat and W. L. Harrison. </author> <title> Automatic recognition of induction variables and recurrence relations by abstract interpretation. </title> <booktitle> In Proc. of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-295. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: They use a join function to compute the intersection of possible values at the confluence nodes of the flow graph to cut down the amount of maintained information. Again the predicated conditional information is lost in the join function. Abstract interpretation has also been used in <ref> [AH90] </ref> for induction variable substitution. Most systems use symbolic global forward substitution, or propagation, to compute values and conditions at all points in the program.
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Given a CF G = (N; E) and its dominator tree, DT , edges in E from descendents to ancestors in DT are called back edges. Given a back edge n ! d, the natural loop <ref> [ASU86] </ref> of the edge is the subgraph of d plus the set of nodes that can reach n without going through d. <p> The dominator tree can be computed in O (Eff (E; N)) time using the dominator algorithm of Lengauer and Tarjan [LT79], or in O (E) time using a more 65 complicated algorithm of Harel [Har85]. The df n can be computed in linear time <ref> [ASU86] </ref>. The df n number has the property of df n (idom (v)) &lt; df n (v) for each node v 6= Entry. The algorithm is shown in Figure 3.4. This algorithm uses the following data structures: * '(fl) is a boolean array, one for each node v. <p> To determine if two expressions are congruent, we need to transform the expres 90 sions into some sort of canonical form. In many cases, pattern matching along will not to be sufficient. Rewriting transformations, such as constant folding, arithmetic simplification and normalization, and value numbering <ref> [ASU86] </ref>, are standard techniques to normalize the expressions into a canonical form.
Reference: [AWZ88] <author> B. Alpern, M. N. Wegman, and F. K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proc. of the 15th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <year> 1988. </year>
Reference-contexts: Forward substitution uses data flow analysis to determine invariant expressions and substitutes variables with invariant expressions. 3.1.1 Static Single Assignment Form Recently, Static Single Assignment (SSA) has been proposed to represent the data value flow property of programs <ref> [AWZ88, RWZ88] </ref>. In the SSA form, each variable in the original program is replaced with several new variables such that only one assignment can reach each use. <p> GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) [BMO90]. It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions <ref> [AWZ88, Hav93] </ref>; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization [TP95b, TP93]. <p> The result also proves the desired property the values. 4.3.2 Comparison of Symbolic Expressions The symbolic expression may still contain fl functions after path projection. In symbolic analysis, we sometimes need to compare these expressions. Alpern, Wegman, and Zadeck <ref> [AWZ88] </ref> define a congruence relation between expressions containing assignments. Definition 4.5 Two expressions are congruent if and only if: * They have the same gating functions, and * The arguments of the gating functions are congruent. <p> Using the GSA sparse representation and the demand-driven approach, our approach can perform more aggressive analysis only on demand. The SSA form has been used to determine the equivalence of symbolic variables and to construct global value graph in a program <ref> [AWZ88] </ref> [RWZ88]. Nascent [Wol92] uses SSA to do a comprehensive analysis of recurrences. Their representation does not include the gating predicate. The SSA representation in Nascent is through an explicit representation of use-def chains, which they call a demand-driven form of SSA.
Reference: [Ban76] <author> Utpal Banerjee. </author> <title> Data Dependence in Ordinary Programs. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Nov., </month> <year> 1976. </year>
Reference-contexts: Because the cost of solving such a system of equations and inequalities is too high for most production compilers, various dependence tests, such as Banerjee's test <ref> [Ban76, Ban88] </ref> and GCD test [Ban88], have been developed to identify sufficient conditions guaranteeing that no integer solutions exist and, hence, no dependences.
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Because the cost of solving such a system of equations and inequalities is too high for most production compilers, various dependence tests, such as Banerjee's test <ref> [Ban76, Ban88] </ref> and GCD test [Ban88], have been developed to identify sufficient conditions guaranteeing that no integer solutions exist and, hence, no dependences. <p> Because the cost of solving such a system of equations and inequalities is too high for most production compilers, various dependence tests, such as Banerjee's test [Ban76, Ban88] and GCD test <ref> [Ban88] </ref>, have been developed to identify sufficient conditions guaranteeing that no integer solutions exist and, hence, no dependences. For most dependence tests to work accurately, it is often necessary to know at compile time the values of the coefficients in the subscript expressions and the loop bounds. <p> It also presents some examples which require more sophisticated analy 14 sis techniques, such as determining symbolic values in the presence of conditional statements, loops, and index arrays. Section 2.5 summarizes this chapter. 2.2 Background and Definition Data dependence <ref> [Ban88] </ref> specifies the precedence constraints in the execution of statements in a program due to data producer/consumer relations. Anti-dependence and output dependence are memory-related, or false dependences, because they are not caused by the flow of values from one statement to another, but rather by the reuse of memory locations. <p> Otherwise it is profitable. When there is more than one subscript of A in P RI b (L), we need to test if there is dependence between each pair of subscripted variables. We can use Banerjee's Test <ref> [Ban88] </ref> to determine if within the loop boundaries two references are referred to the same location.
Reference: [Ban90] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. 3rd Workshop on Programming Languages and Compilers for Parallel Computing. </booktitle> <publisher> Pitman/MIT Press, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Some well-known control flow transformations include: loop interchanging to change the granularity of parallel computation; loop distribution to separate parallel computation from sequential computation in a loop; and, loop skewing to change dependence directions. Techniques such as unimodular transformation <ref> [Ban90] </ref> combine several loop transformations into one transformation. Besides changing the computation schedule to achieve a desirable parallelism, control 6 flow transformation such as loop tiling [Wol87] can also be used to improve the data locality of computation. Data flow transformation techniques change the data value flow.
Reference: [BCFH89] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing [TP92]. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization <ref> [BCFH89] </ref>, scalar renaming [CF87], and array expansion [PW86] [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program. <p> Hence our model does not include copy-in; that is, we do not allow values to be copied from the global array to the private array. While copy-in is sometimes used for scalar variable in parallelizing compilers <ref> [BCFH89] </ref>, it is problematical when used for arrays for the following reasons: * Copying-in a large array can be very expensive.
Reference: [BE94a] <author> William Blume and Rudolf Eigenmann. </author> <title> The range test: A dependence test for symbolic, non-linear expressions. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis <ref> [BE94a] </ref>; and demand-driven symbolic analysis for array privatization [TP95b, TP93]. In the SSA representation, -functions of a single type are placed at the join nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. <p> The examples illustrate how these techniques improve the effectiveness of array privatization. The techniques are also useful in improving the accuracy of dependence analysis <ref> [BE94b, BE94a] </ref>.
Reference: [BE94b] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic analysis techniques needed for the effective parallelization of the perfect benchmarks. </title> <type> Technical Report 1332, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Because dependence tests check the sufficient conditions of data independence, when the tests failed due to unknown symbolic values in subscript expressions or loop bounds, compilers have to assume the existence of dependence. This limitation can often be overcome by using symbolic analysis techniques <ref> [HP91, BE94b, TP95b] </ref>. 1.2.3 Program Transformation Program transformation techniques used in parallelizing compilers [Pad79, PW86] can be classified into two classes: control flow transformation and data flow transformation. Control flow transformations change the control of the program and hence the schedule of computation. <p> Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks [CKPK90], the compiler needs more symbolic analysis capabilities <ref> [BE94b] </ref>. <p> Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks [CKPK90], the compiler needs more symbolic analysis capabilities <ref> [BE94b] </ref>. These techniques include the analysis of symbolic range to prove independent array accesses for dependence analysis, induction and reduction recognition to transform flow dependence, and scalar and array privatization to eliminate memory-related dependences. <p> The examples illustrate how these techniques improve the effectiveness of array privatization. The techniques are also useful in improving the accuracy of dependence analysis <ref> [BE94b, BE94a] </ref>.
Reference: [BEF + 94] <author> Bill Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence Rauchwerger, Peng Tu, and 126 Stephen Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <booktitle> In Proc. 7th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Several research compilers, such as Parafrase-2 [HP92] and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris <ref> [BEF + 94] </ref> restructuring compiler, symbolic analysis is also used for dependence analysis, symbolic range propagation, and array privatization [BEF + 94][TP93]. Simple symbolic analysis techniques are used regularly in traditional optimizing compilers. Constant propagation detects occurrences of symbolic variables that have constant values. <p> Several research compilers, such as Parafrase-2 [HP92] and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris <ref> [BEF + 94] </ref> restructuring compiler, symbolic analysis is also used for dependence analysis, symbolic range propagation, and array privatization [BEF + 94][TP93]. Simple symbolic analysis techniques are used regularly in traditional optimizing compilers. Constant propagation detects occurrences of symbolic variables that have constant values. Also, common subexpression elimination determines the equivalence of two symbolic expressions to avoid redundant computation. <p> In the target parallel program, each loop is annotated with its privatizable arrays and last value assignment conditions which specify when the values of a private array should be copied out to a global array. The algorithm has been implemented in the Polaris parallelizing compiler <ref> [BEF + 94] </ref>. Our work on automatic array privatization presents the following new results: * We use data flow-based analysis for array reference. Compared with the dependence analysis-based approach [MAL92], which has to employ parametric integer programming in its most general case, our approach is more efficient. <p> 0 ; fl (Q; A 9 ; )); fl (R; A 4 ; )); and for node 6 (statement 14), fl (P; fl (R; ; A 3 ); fl (Q; ; A 14 )). 3.7 Implementation and Measurement We implemented the algorithm using path compression in the Polaris restructuring compiler <ref> [BEF + 94] </ref>. The simple algorithm uses only path compression and has a complexity of O (E log N ). Table 3.1 shows the characteristics of the programs we tested from Perfect Benchmarks [CKPK90] 74 in terms of the number of edges and the number of subroutines. <p> Recent developments in parallelizing compilers have resulted in the increased use of the symbolic analysis technique to facilitate parallelism detection and program transformation. Several research compilers, such as Parafrase-2 [HP92], and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris <ref> [BEF + 94] </ref> restructuring compiler, we use symbolic analysis for dependence analysis, symbolic range propagation, and array privati-zation [BEF + 94] [TP93]. Simple symbolic analysis techniques are used regularly in the traditional optimizing compilers. Constant propagation detects symbolic variables that are equivalent to constants. <p> Several research compilers, such as Parafrase-2 [HP92], and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris <ref> [BEF + 94] </ref> restructuring compiler, we use symbolic analysis for dependence analysis, symbolic range propagation, and array privati-zation [BEF + 94] [TP93]. Simple symbolic analysis techniques are used regularly in the traditional optimizing compilers. Constant propagation detects symbolic variables that are equivalent to constants. Also, common subexpression elimination determines the equivalence of two symbolic expressions to avoid redundant computation.
Reference: [Ber66] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15 </volume> <pages> 757-762, </pages> <year> 1966. </year>
Reference-contexts: For instance, in the following loop, the order of iterations is semantically irrelevant and all iterations can be executed in parallel with each other. DO I = 1, N ENDDO While determining the semantic validity of concurrently executing two segments of a sequential program is an undecidable problem <ref> [Ber66] </ref>, there are sufficient conditions for parallel execution based on dependence relations. The dependence relations characterize the semantical 3 constraints on the execution order of a program. They are determined by the way memory locations are read and written and the control flows during the sequential execution of the program.
Reference: [BMO90] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The program dependence web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proceedings of the SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The Gated Single-Assignment (GSA) form is an extension of the SSA representation [CFR + 91]. GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) <ref> [BMO90] </ref>. It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization [TP95b, TP93]. <p> The function selects the last value of X computed by the loop. The function, as defined in <ref> [BMO90] </ref>, awkwardly handles loops with a zero-trip count. <p> The new algorithm is more efficient and simpler than the existing algorithms for GSA construction 47 <ref> [BMO90, Hav93] </ref>. Because SSA is a special case of GSA, our algorithm can also be used for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same -function placement procedure as in the SSA construction [CFR + 91]. <p> The first step is the same -function placement procedure as in the SSA construction [CFR + 91]. In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a -function and transform the -function into a gating function. The original GSA conversion algorithm <ref> [BMO90] </ref> uses a Program Dependence Graph (PDG) [FOW87] as its input program representation. Havlak developed another algorithm [Hav93] to construct a variant of the GSA, known as Thinned GSA. <p> Gat 55 ing functions were introduced by Ballance, Maccabe, and Ottenstein <ref> [BMO90] </ref> to capture the control conditions that guard the paths to a -function. The three types of gating function discussed in section 3.2 are replacing -functions at different types of join nodes in a CF G. <p> In <ref> [BMO90] </ref>, the conversion to GSA is done after -placement. The algorithm works by expanding each node into a GSA gating tree that contains the control information for the different reaching definitions. The translation for each node may potentially scan all the edges in the flow graph. <p> The symbolic analysis technique is based on the Static Single Assignment (SSA) form [CFR + 91] of program representation. We use the more elaborate version of the SSA, known as the Gated Single Assignment (GSA) form <ref> [BMO90] </ref>, as presented in Chapter 3. The value of a symbolic variable is represented by a symbolic expression involving other symbolic variables, constants, and gating functions in the GSA. We present algorithms to build and analyze the symbolic expressions. <p> Our demand-driven backward substitution in GSA and our use of control dependences to project values are unique. 108 The currently most used algorithm for building SSA form is given in [CFR + 91]. GSA was introduced in <ref> [BMO90] </ref> as a part of the Program Dependence Web (PDW), which is an extension of the Program Dependence Graph (PDG) [FOW87]. An algorithm for constructing GSA from PDG and SSA is given in [BMO90]. Havlak [Hav93] develops another algorithm for building GSA from a program control flow graph and SSA. <p> GSA was introduced in <ref> [BMO90] </ref> as a part of the Program Dependence Web (PDW), which is an extension of the Program Dependence Graph (PDG) [FOW87]. An algorithm for constructing GSA from PDG and SSA is given in [BMO90]. Havlak [Hav93] develops another algorithm for building GSA from a program control flow graph and SSA.
Reference: [CF87] <author> Ron Cytron and Jeanne Ferante. </author> <title> What's in a name? or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proc. 1987 International Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing [TP92]. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization [BCFH89], scalar renaming <ref> [CF87] </ref>, and array expansion [PW86] [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program.
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Ken-neth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The Gated Single-Assignment (GSA) form is an extension of the SSA representation <ref> [CFR + 91] </ref>. GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) [BMO90]. <p> Because SSA is a special case of GSA, our algorithm can also be used for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same -function placement procedure as in the SSA construction <ref> [CFR + 91] </ref>. In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a -function and transform the -function into a gating function. The original GSA conversion algorithm [BMO90] uses a Program Dependence Graph (PDG) [FOW87] as its input program representation. <p> SSA form, def-use information is represented by at most E def-use chains, where E is the number of edges in the control flow graph. 3.3.1 Constructing SSA Form An efficient algorithm for constructing SSA with a minimal number of -functions was originally designed by Cytron, Ferrante, Rosen, Wegman, and Zadeck <ref> [CFR + 91] </ref>. In general, two separate steps are required to translate a program into SSA form: * -function placement. Some trivial -functions V (W 1 ; W 2 ; : : : ; W p ) are inserted at certain join nodes in the program flow graph. * Renaming. <p> In a brute-force approach of applying the -function placement condition, as we find more nodes needing -functions to V , the set of nodes needing the -functions will gradually increase until it stabilizes. The algorithm in <ref> [CFR + 91] </ref> achieves the same results as this brute-force approach, but with less time complexity. The -function placement algorithm in [CFR + 91] uses a concept of dominance frontier. <p> The algorithm in <ref> [CFR + 91] </ref> achieves the same results as this brute-force approach, but with less time complexity. The -function placement algorithm in [CFR + 91] uses a concept of dominance frontier. <p> A fundamental result proven in <ref> [CFR + 91] </ref> states that if ' is the set of assignment nodes for a variable V , then DF + (') is the minimum set of nodes needing -function assignment nodes for V . The algorithm in [CFR + 91] for placing the -functions for one variable is O (N <p> A fundamental result proven in <ref> [CFR + 91] </ref> states that if ' is the set of assignment nodes for a variable V , then DF + (') is the minimum set of nodes needing -function assignment nodes for V . The algorithm in [CFR + 91] for placing the -functions for one variable is O (N 2 ) in the worst case, because the size of the iterated dominance frontiers can be O (N 2 ). It often appears to be linear, however, when applied to real programs. <p> The algorithm in Figure 3.1 is from <ref> [CFR + 91] </ref>, which renames all the mentions of variables. We include it here for the purpose of demonstrating how to interface it with our GSA placement algorithm to be presented in this chapter. <p> We prove that the -nodes computed are the same as those using the iterated dominance frontier algorithm in <ref> [CFR + 91] </ref>. <p> This chapter presents a demand-driven technique to do symbolic analysis for parallelizing compilers and a data flow framework for conditional array def-use analysis. The symbolic analysis technique is based on the Static Single Assignment (SSA) form <ref> [CFR + 91] </ref> of program representation. We use the more elaborate version of the SSA, known as the Gated Single Assignment (GSA) form [BMO90], as presented in Chapter 3. <p> The roles of Entry and Exit are also reversed. From the definition, it is clear that control dependences are the dominance frontiers in the RCF G. Hence, the algorithm in <ref> [CFR + 91] </ref> for building dominance frontiers can be used to build the control dependences using RCF G. Definition 4.4 The iterative control dependences of a node X is the transitive closure of its control dependences. <p> We can use the GSA representation to find out the value of JPLUS (J) at statement U. To this end, we use an extension of the SSA representation to include arrays in the following way <ref> [CFR + 91] </ref>: 1. Create a new array name for each array assignment; 2. Use the subscript to identify which element is assigned; and 3. Replace the assignment with an update function ff (array; subscript; value). <p> GSA has also been used in Parascope to build the global value graph [Hav93]. Our demand-driven backward substitution in GSA and our use of control dependences to project values are unique. 108 The currently most used algorithm for building SSA form is given in <ref> [CFR + 91] </ref>. GSA was introduced in [BMO90] as a part of the Program Dependence Web (PDW), which is an extension of the Program Dependence Graph (PDG) [FOW87]. An algorithm for constructing GSA from PDG and SSA is given in [BMO90]. <p> Their technique can be used to eliminate redundant dependences in the gating functions. Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph [TP95a] (Chapter 3). Control dependence was introduced in [FOW87] as part of PDG. The SSA construction paper <ref> [CFR + 91] </ref> also has an algorithm for building control dependences. The algorithm in [CFR + 91] for building iterative dominance frontiers can be used on the reverse flow graph to build the iterative post-dominance frontiers (iterative control dependences) used in this paper. 4.9 Summary Symbolic analysis is important to parallelizing <p> Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph [TP95a] (Chapter 3). Control dependence was introduced in [FOW87] as part of PDG. The SSA construction paper <ref> [CFR + 91] </ref> also has an algorithm for building control dependences. The algorithm in [CFR + 91] for building iterative dominance frontiers can be used on the reverse flow graph to build the iterative post-dominance frontiers (iterative control dependences) used in this paper. 4.9 Summary Symbolic analysis is important to parallelizing compilers.
Reference: [CH78] <author> P. Cousot and N. Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Constant propagation detects occurrences of symbolic variables that have constant values. Also, common subexpression elimination determines the equivalence of two symbolic expressions to avoid redundant computation. Symbolic analysis has also been used to prove assertions for program verification and debugging <ref> [CH78] </ref>. Recent experiments on the effectiveness of parallelizing compilers have found that symbolic variables present problems for dependence analysis and parallelization transformations. The most common cases are induction variables. <p> Constant propagation detects symbolic variables that are equivalent to constants. Also, common subexpression elimination determines the equivalence of two symbolic expressions to avoid redundant computation. Symbolic analysis has also been used to prove assertions for program verification and debugging <ref> [CH78] </ref>. 78 Recent experiments on the effectiveness of parallelizing compilers have found that symbolic variables present problems for dependence analysis and parallelization transformations. The most common case is induction variable. <p> We also use the iterative control dependences to represent path conditions. Symbolic analysis is also related to the problem of automatic proof of invariant assertions in program. Symbolic execution can be considered as abstract interpretation [CH92]. In <ref> [CH78] </ref>, abstract interpretation is used to discover the linear relationships between variables. It can be used to propagate symbolic linear expressions as possible values for symbolic variables. However, the predicate guarding the conditional possible values is not included in the representation 107 and cannot be propagated.
Reference: [CH92] <author> P. Cousot and N. Halbwachs. </author> <title> Abstract interpretation and application to logic programs. </title> <journal> Journal of Logic Programming, </journal> <volume> 13(2-3):103-179, </volume> <year> 1992. </year>
Reference-contexts: We also use the iterative control dependences to represent path conditions. Symbolic analysis is also related to the problem of automatic proof of invariant assertions in program. Symbolic execution can be considered as abstract interpretation <ref> [CH92] </ref>. In [CH78], abstract interpretation is used to discover the linear relationships between variables. It can be used to propagate symbolic linear expressions as possible values for symbolic variables. However, the predicate guarding the conditional possible values is not included in the representation 107 and cannot be propagated.
Reference: [Che89] <author> Ding-Kai Chen. </author> <title> MAXPAR: An execution driven simulator for studying parallel systems. </title> <type> MS thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <month> October </month> <year> 1989. </year> <type> CSRD Report 917. </type>
Reference: [CHT79] <author> T. E. Cheatham, G. H. Holloway, and J. A. Townley. </author> <title> Symbolic evaluation and the analysis of programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(4) </volume> <pages> 402-417, </pages> <year> 1979. </year>
Reference-contexts: Corresponding to each path there is a path condition, i.e. a boolean expression that is true if the path is executed [CR85] <ref> [CHT79] </ref>. These techniques have exponential time and space requirements which limit their applicability in practical situations. The GSA representation of symbolic expression in this paper is a new way to represent predicated path values. With GSA, the condition and value are represented in a compact way.
Reference: [CK88a] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: This simplification of array analysis is not enough to satisfy the requirement for array privatization and parallelization. The notion of subarray we use in this thesis is an extension of the regular section used by others <ref> [CK88a] </ref>. Using subarrays, we can represent triangular sections and banded sections, as well as the strips, grids, columns, rows, and blocks of an array.
Reference: [CK88b] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference: [CKPK90] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer performance evaluation and the perfect benchmarks. </title> <booktitle> In Proceedings of ICS, </booktitle> <address> Amsterdam, Netherlands, </address> <pages> pages 162-174, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Various techniques have been developed for linear induction variable recognition [Ken81][ASU86] [ABCF88]. Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks <ref> [CKPK90] </ref>, the compiler needs more symbolic analysis capabilities [BE94b]. <p> The simple algorithm uses only path compression and has a complexity of O (E log N ). Table 3.1 shows the characteristics of the programs we tested from Perfect Benchmarks <ref> [CKPK90] </ref> 74 in terms of the number of edges and the number of subroutines. It also shows the total execution time in seconds on a SUN-10 workstation for building the gating functions. Program TRFD, with 662 edges, requires a fraction of a second. <p> Various techniques have been developed for linear induction variable recognition [Ken81][ASU86] [ABCF88]. Recently, more aggressive symbolic analysis techniques have been developed to detect and replace non-linear induction variables [HP92][Wol92]. However, to successfully parallelize a broad range of application programs, such as the Perfect Benchmarks <ref> [CKPK90] </ref>, the compiler needs more symbolic analysis capabilities [BE94b]. These techniques include the analysis of symbolic range to prove independent array accesses for dependence analysis, induction and reduction recognition to transform flow dependence, and scalar and array privatization to eliminate memory-related dependences. <p> A read shadow for a private variable is locally created for each loop iteration. Different iterations have different shadows for a private variable; hence, the anti-dependences are broken across iterations. Optimal execution time is computed by ignoring all the read shadow variables. Six programs in the Perfect Benchmark Club <ref> [CKPK90] </ref> were instrumented. Before the instrumentation, simple induction variable substitutions are applied to the programs to reduce the amount of flow dependences. The loop level speedup results are reported in Table 5.1.
Reference: [CR85] <author> L. A. Clarke and D. J. Richardson. </author> <title> Applications of symbolic evaluation. </title> <journal> Journal of Systems and Software, </journal> <volume> 5(1) </volume> <pages> 15-35, </pages> <year> 1985. </year> <month> 127 </month>
Reference-contexts: Corresponding to each path there is a path condition, i.e. a boolean expression that is true if the path is executed <ref> [CR85] </ref> [CHT79]. These techniques have exponential time and space requirements which limit their applicability in practical situations. The GSA representation of symbolic expression in this paper is a new way to represent predicated path values. With GSA, the condition and value are represented in a compact way.
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic par--allelization of four Perfect-Benchmark programs. </title> <booktitle> In Proc. 4-th Workshop on Programming Languages and Compilers for Parallel Computing. </booktitle> <publisher> Pitman/MIT Press, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Today's parallelizing compilers have only limited success in detecting parallelism in large application programs. Sometimes, this is due to some necessary information being unknown at compile time; most often, though, it is due to the lack of sophisticated analysis techniques in the compilers <ref> [EHLP91] </ref>. The major tasks of parallelizing compilers can be divided into two parts [PKL80, PW86]: dependence analysis and restructuring transformation. Dependence analysis determines the inherent parallelism in the sequential programs. <p> Previous work <ref> [EHLP91] </ref> has shown that these techniques are very important for the automatic parallelization of large application programs. 1.4.1 Array Privatization As mentioned in Section 1.2.3, privatization eliminates memory-related dependences. <p> By providing a distinct instance of a variable to each processor, privatization can eliminate memory-related dependence. Previous studies on the effectiveness of automatic program parallelization show that privatization is one of the most effective transformations for the exploitation of parallelism <ref> [EHLP91] </ref>. A related technique, called expansion [PW86], transforms each reference to a particular scalar into a reference to a vector element in such a way that each thread accesses a different vector element. When applied to an array, expansion creates a new dimension for the array. <p> Moreover, our array section analysis can handle nonlinear subscripts that cannot be handled by the integer programming approach. * The algorithm proceeds from the bottom up, which allows us to easily extend the algorithm to program call trees for interprocedural analysis. Early experience <ref> [EHLP91] </ref> shows that interprocedural array reference analysis is necessary in many cases for success ful array privatization in large application programs. * We distinguish private arrays whose last value assignments can be determined statically from those whose last values have to be assigned dynamically at runtime. <p> This separation facilitates efficient code generation and can potentially identify more private arrays than other algorithms can identify. * To evaluate its effectiveness, we test the algorithm on the programs in the Perfect Benchmarks. We compare the automatic privatization with the manual privatization described 13 in <ref> [EHLP91] </ref>. <p> We compared the number of private arrays found by the algorithm with that of the manual array privatization reported in <ref> [EHLP91] </ref>. The result is shown in Table 2.1. The first column reports the number of private arrays identified by both manual and automatic privatization. The second column reports the number of private arrays identified by manual privatization but not by automatic privatization. <p> Our experiments have indicated that the algorithms can privatize most of the arrays pri vatized by hand in <ref> [EHLP91] </ref>.
Reference: [Far77] <author> R. Farrow. </author> <title> Efficient on-line evaluation of functions defined on paths in trees. </title> <type> Technical report, </type> <institution> Rice University, Dept. Math. Sci., </institution> <year> 1977. </year>
Reference-contexts: A simple method has an O (E log (N )) time bound; a sophisticated off-line algorithm maintaining balanced subtrees has an O (Eff (E; N)) time bound. Yet another on-line O (Eff (E; N )) method, called stratified path compression by Farrow <ref> [Far77, Tar79] </ref>, can also be used. This GSA algorithm is also almost as efficient as the best known algorithms for -function placement in SSA conversion. 48 The rest of this chapter is divided into the following sections. In Section 3.3, we introduce some background and notations.
Reference: [Fea88] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proc. 1988 ACM Int'l Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing [TP92]. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization [BCFH89], scalar renaming [CF87], and array expansion [PW86] <ref> [Fea88] </ref>. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program.
Reference: [For93] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification (draft). </title> <type> Technical report, </type> <institution> High Performance Fortran Forum, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The privatization pass adds the following directives to the loop to identify privatizable variables and those that need last value assignments: C$DIR INDEPENDENT C$DIR PRIVATE A (1:N) C$DIR LAST VALUE A (1:N) WHEN (I.EQ.N) 15 The INDEPENDENT directive is borrowed from HPF <ref> [For93] </ref>. It specifies that the iterations of loop S1 are independent. There are two directives for a private array. The PRIVATE directive declares the privatizable arrays in the loop where each iteration can allocate its own copies of the arrays.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependency graph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a -function and transform the -function into a gating function. The original GSA conversion algorithm [BMO90] uses a Program Dependence Graph (PDG) <ref> [FOW87] </ref> as its input program representation. Havlak developed another algorithm [Hav93] to construct a variant of the GSA, known as Thinned GSA. Because Havlak's algorithm uses program control flow graph as it input program representation, it is simpler to explain than the original PDG-based algorithm. <p> Because the path condition at p is (P = true), the value of JUP 2 at p can be determined to be JMAX-1. To compute the path condition for each statement, we need to use the concept of iterative control dependences. Control dependences <ref> [FOW87] </ref> are essentially the dominance frontiers in the reverse graph of the control flow graph. <p> GSA was introduced in [BMO90] as a part of the Program Dependence Web (PDW), which is an extension of the Program Dependence Graph (PDG) <ref> [FOW87] </ref>. An algorithm for constructing GSA from PDG and SSA is given in [BMO90]. Havlak [Hav93] develops another algorithm for building GSA from a program control flow graph and SSA. <p> Their technique can be used to eliminate redundant dependences in the gating functions. Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph [TP95a] (Chapter 3). Control dependence was introduced in <ref> [FOW87] </ref> as part of PDG. The SSA construction paper [CFR + 91] also has an algorithm for building control dependences.
Reference: [Gir91] <author> Milind Baburao Girkar. </author> <title> Functional Parallelism Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> Univ. of Illionis at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Girkar and Polychronopoulos developed a related technique to find the minimum set of essential data and control dependences in Hierarchical Task Graph (HTG) and then use the dependences to derive execution conditions for tasks <ref> [GP92, Gir91] </ref>. Their technique can be used to eliminate redundant dependences in the gating functions. Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph [TP95a] (Chapter 3). Control dependence was introduced in [FOW87] as part of PDG.
Reference: [GJ79] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of the NP-Completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: Incorporating the path condition in the analysis provides us with more power than GSA or SSA alone. 4.3.1 Bounds of Symbolic Expression Whether P C P is undecidable, in general, and NP-Complete <ref> [GJ79] </ref> when all the conditions are boolean variables. Path projection is most useful when the path condition contains the same predicate as that in a gating function. When the number of boolean variables in P C and SE is small, it is still practical to compute the path projection.
Reference: [GP92] <author> Milind Girkar and Constantine D. Polychronopoulos. </author> <title> The HTG: An Intermediate Representation for Programs Based on Control and Data Dependences. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Girkar and Polychronopoulos developed a related technique to find the minimum set of essential data and control dependences in Hierarchical Task Graph (HTG) and then use the dependences to derive execution conditions for tasks <ref> [GP92, Gir91] </ref>. Their technique can be used to eliminate redundant dependences in the gating functions. Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph [TP95a] (Chapter 3). Control dependence was introduced in [FOW87] as part of PDG.
Reference: [GSW] <author> M. P. Gerlek, E. Stoltz, and M. Wolfe. </author> <title> Beyond induction variables: Detecting and classifying sequences using a demand-driven ssa form. </title> <note> to appear on, ACM TPLAS. </note>
Reference-contexts: Several research compilers, such as Parafrase-2 [HP92] and Nascent <ref> [GSW] </ref>, use symbolic analysis to identify and transform induction variables. In the Polaris [BEF + 94] restructuring compiler, symbolic analysis is also used for dependence analysis, symbolic range propagation, and array privatization [BEF + 94][TP93]. Simple symbolic analysis techniques are used regularly in traditional optimizing compilers. <p> Second, conditional analysis is necessary to analyze the conditionally defined and conditionally used arrays. Recent developments in parallelizing compilers have resulted in the increased use of the symbolic analysis technique to facilitate parallelism detection and program transformation. Several research compilers, such as Parafrase-2 [HP92], and Nascent <ref> [GSW] </ref>, use symbolic analysis to identify and transform induction variables. In the Polaris [BEF + 94] restructuring compiler, we use symbolic analysis for dependence analysis, symbolic range propagation, and array privati-zation [BEF + 94] [TP93]. Simple symbolic analysis techniques are used regularly in the traditional optimizing compilers.
Reference: [Har85] <author> P. Harel. </author> <title> A linear time algorithm for finding dominators in flowgraphs and related problems. </title> <booktitle> In Proc. of the 17th ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1985. </year>
Reference-contexts: The dominator tree can be computed in O (Eff (E; N)) time using the dominator algorithm of Lengauer and Tarjan [LT79], or in O (E) time using a more 65 complicated algorithm of Harel <ref> [Har85] </ref>. The df n can be computed in linear time [ASU86]. The df n number has the property of df n (idom (v)) &lt; df n (v) for each node v 6= Entry. The algorithm is shown in Figure 3.4.
Reference: [Hav93] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <booktitle> In Proc. 6rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) [BMO90]. It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions <ref> [AWZ88, Hav93] </ref>; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization [TP95b, TP93]. <p> The new algorithm is more efficient and simpler than the existing algorithms for GSA construction 47 <ref> [BMO90, Hav93] </ref>. Because SSA is a special case of GSA, our algorithm can also be used for SSA construction. The existing algorithms for building the GSA follow two steps. The first step is the same -function placement procedure as in the SSA construction [CFR + 91]. <p> In the second step, the GSA conversion algorithms collect the control dependences of the definitions reaching a -function and transform the -function into a gating function. The original GSA conversion algorithm [BMO90] uses a Program Dependence Graph (PDG) [FOW87] as its input program representation. Havlak developed another algorithm <ref> [Hav93] </ref> to construct a variant of the GSA, known as Thinned GSA. Because Havlak's algorithm uses program control flow graph as it input program representation, it is simpler to explain than the original PDG-based algorithm. But both algorithms essentially follow the same strategy. <p> Their approach for constructing strongly connected regions for recurrences in the graph is similar to our backward substitution of names. GSA has also been used in Parascope to build the global value graph <ref> [Hav93] </ref>. Our demand-driven backward substitution in GSA and our use of control dependences to project values are unique. 108 The currently most used algorithm for building SSA form is given in [CFR + 91]. <p> GSA was introduced in [BMO90] as a part of the Program Dependence Web (PDW), which is an extension of the Program Dependence Graph (PDG) [FOW87]. An algorithm for constructing GSA from PDG and SSA is given in [BMO90]. Havlak <ref> [Hav93] </ref> develops another algorithm for building GSA from a program control flow graph and SSA.
Reference: [HDH + 94] <author> K. Hayashi, T. Doi, T. Horie, Y. Koyanagi, O. Shiraki, N. Imamura, T. Shimizu, H. Ishihata, and T. Shindo. </author> <title> Ap1000+: Architectural support of put/get interface for parallelizing compiler. </title> <booktitle> In Proc. of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: DSM systems allow users to write programs using shared-memory style of programming. A DSM may be implemented with hardware support such as the Cray T3D, Fujitsu AP1000+ <ref> [HDH + 94] </ref>, or in software [NL91]. 120 A DSM system should support a virtual global memory on the distributed memory system. Any processor may access any location in the global memory space, but each processor owns a specific region of the global address space.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year> <month> 128 </month>
Reference-contexts: Interprocedural array def-use analysis can be used to relieve the problems. For example, the subroutine array def-use section summary, collected by the techniques as described in Chapter 2, can be used for array privatization and data dependence analysis <ref> [HK91] </ref>. Integration of different passes is also an important area. Currently, the data dependence analysis pass duplicates some of the work done by the array privatization pass. It may be beneficiary to integrate both passes more closely to avoid redundant analysis and improve the efficiency.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and Ch.-W. Tseng. </author> <title> Compiler support for machine--independent parallel programming in Fortran D. </title> <institution> Technical Report Rice COMP TR91-149, Department of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The array sections in a RANGEWRIT-TEN assertion may need to be PUT into the remote memory if they are not local. Contrary to the distributed memory model with owner-compute rule <ref> [HKT91] </ref>, here computations in the loop can be scheduled using iteration space partitioning as in a shared memory machines. We use the matrix multiplication program in Figure 5.1 to illustrate the use of the assertions to generate GET operations.
Reference: [HP91] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <booktitle> Parallel and Distributed Computing: Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 310-330, </pages> <year> 1991. </year>
Reference-contexts: Because dependence tests check the sufficient conditions of data independence, when the tests failed due to unknown symbolic values in subscript expressions or loop bounds, compilers have to assume the existence of dependence. This limitation can often be overcome by using symbolic analysis techniques <ref> [HP91, BE94b, TP95b] </ref>. 1.2.3 Program Transformation Program transformation techniques used in parallelizing compilers [Pad79, PW86] can be classified into two classes: control flow transformation and data flow transformation. Control flow transformations change the control of the program and hence the schedule of computation.
Reference: [HP92] <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic program analysis and optimization for parallelizing compilers. </title> <booktitle> In Proc. 5rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Several research compilers, such as Parafrase-2 <ref> [HP92] </ref> and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris [BEF + 94] restructuring compiler, symbolic analysis is also used for dependence analysis, symbolic range propagation, and array privatization [BEF + 94][TP93]. Simple symbolic analysis techniques are used regularly in traditional optimizing compilers. <p> Second, conditional analysis is necessary to analyze the conditionally defined and conditionally used arrays. Recent developments in parallelizing compilers have resulted in the increased use of the symbolic analysis technique to facilitate parallelism detection and program transformation. Several research compilers, such as Parafrase-2 <ref> [HP92] </ref>, and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris [BEF + 94] restructuring compiler, we use symbolic analysis for dependence analysis, symbolic range propagation, and array privati-zation [BEF + 94] [TP93]. Simple symbolic analysis techniques are used regularly in the traditional optimizing compilers.
Reference: [JP93] <author> R. Johnson and K. Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proc. the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: It often appears to be linear, however, when applied to real programs. The authors give evidence that the size of the iterated dominance frontiers is usually O (N ). Johnson and Pingali <ref> [JP93, JPP94] </ref> proposed another algorithm to place -functions in O (E) time. Later, Cytron and Ferrante proposed an almost-linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao [SG94] have developed another O (E) time algorithm.
Reference: [JPP94] <author> R. Johnson, D. Pearson, and K. Pingali. </author> <title> The program structure tree: Computing control regions in linear time. </title> <booktitle> In Proc. the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: It often appears to be linear, however, when applied to real programs. The authors give evidence that the size of the iterated dominance frontiers is usually O (N ). Johnson and Pingali <ref> [JP93, JPP94] </ref> proposed another algorithm to place -functions in O (E) time. Later, Cytron and Ferrante proposed an almost-linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao [SG94] have developed another O (E) time algorithm.
Reference: [KBC + 74] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Mu-raoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: Traditional data flow analysis can recognize subscripted variables only as accesses to the whole array. However, that is not sufficient for our purposes. The work of D. Kuck, L. Lamport, and others in early 1970s <ref> [KMC72, KBC + 74, Lam74, Tow76] </ref> was a major breakthrough since they were the first to take into account subscript expressions in the dependence analysis. The basic ideas that were developed at that time still dominate the research of automatic parallelization today. Subscript analysis examines each subscript individually.
Reference: [Ken81] <author> K. Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <title> Program Flow Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference: [KMC72] <author> D. J. Kuck, Y. Muraoka, and S. C. Chen. </author> <title> On the number of operations simultaneously executable in fortran-like programs and their resulting speed-up. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21 </volume> <pages> 1293-1310, </pages> <year> 1972. </year>
Reference-contexts: Traditional data flow analysis can recognize subscripted variables only as accesses to the whole array. However, that is not sufficient for our purposes. The work of D. Kuck, L. Lamport, and others in early 1970s <ref> [KMC72, KBC + 74, Lam74, Tow76] </ref> was a major breakthrough since they were the first to take into account subscript expressions in the dependence analysis. The basic ideas that were developed at that time still dominate the research of automatic parallelization today. Subscript analysis examines each subscript individually.
Reference: [Kum88] <author> M. Kumar. </author> <title> Measuring parallelism in computation-intensive science/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9) </volume> <pages> 5-40, </pages> <year> 1988. </year>
Reference-contexts: are identified and eliminated. * Parallelism after privatization: The maximum speedups that can be obtained after the privatization techniques in Chapter 2 have been applied to eliminate the memory related dependences. 113 To avoid machine-dependent effects, such as overhead costs, we use the critical path analysis technique introduced by Kumar <ref> [Kum88] </ref>. This technique measures the length of a program's runtime critical path on an ideal machine where only the arithmetic operations consume time and unlimited number of processors are available.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communication of ACM, </journal> <volume> 17 </volume> <pages> 83-93, </pages> <year> 1974. </year>
Reference-contexts: Traditional data flow analysis can recognize subscripted variables only as accesses to the whole array. However, that is not sufficient for our purposes. The work of D. Kuck, L. Lamport, and others in early 1970s <ref> [KMC72, KBC + 74, Lam74, Tow76] </ref> was a major breakthrough since they were the first to take into account subscript expressions in the dependence analysis. The basic ideas that were developed at that time still dominate the research of automatic parallelization today. Subscript analysis examines each subscript individually.
Reference: [Li92] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of ICS'92, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: Recently there have been several papers on array privatization <ref> [Li92] </ref> [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program.
Reference: [LT79] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 121-141, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: We also assume that each node in the CF G is assigned a depth-first number df n. The dominator tree can be computed in O (Eff (E; N)) time using the dominator algorithm of Lengauer and Tarjan <ref> [LT79] </ref>, or in O (E) time using a more 65 complicated algorithm of Harel [Har85]. The df n can be computed in linear time [ASU86]. The df n number has the property of df n (idom (v)) &lt; df n (v) for each node v 6= Entry.
Reference: [MAL92] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proc. 5rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Recently there have been several papers on array privatization [Li92] <ref> [MAL92] </ref> [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program. <p> The algorithm has been implemented in the Polaris parallelizing compiler [BEF + 94]. Our work on automatic array privatization presents the following new results: * We use data flow-based analysis for array reference. Compared with the dependence analysis-based approach <ref> [MAL92] </ref>, which has to employ parametric integer programming in its most general case, our approach is more efficient. Our technique can handle arbitrary control flows inside loop body, while dependence analysis-based approach can handle only linear control structures.
Reference: [MW77] <author> J. H. Morris and B. Wegbreit. </author> <title> Subgoal induction. </title> <journal> Communication of ACM, </journal> <volume> 20(4) </volume> <pages> 209-222, </pages> <year> 1977. </year>
Reference-contexts: However, the restricted information may not be accurate enough to be of any use. The demand-driven approach in this paper is a special case of the more general approach of Subgoal Induction <ref> [MW77] </ref>. Two facts make it possible: (1) In the SSA representation, the variable name is unique and the use-def chain is embedded in the unique variable name; (2) In GSA, backward substitution can be done by following the use-def chain in the name without going through the flow graph.
Reference: [NL91] <author> B Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algo-rithms. </title> <booktitle> Computer, </booktitle> <pages> pages 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: DSM systems allow users to write programs using shared-memory style of programming. A DSM may be implemented with hardware support such as the Cray T3D, Fujitsu AP1000+ [HDH + 94], or in software <ref> [NL91] </ref>. 120 A DSM system should support a virtual global memory on the distributed memory system. Any processor may access any location in the global memory space, but each processor owns a specific region of the global address space.
Reference: [Pad79] <author> D. A. Padua. </author> <title> First Quarterly Report for the Lawrence Livermore Laboratory. </title> <type> Technical Report 8, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Oct., </month> <year> 1979. </year>
Reference-contexts: This limitation can often be overcome by using symbolic analysis techniques [HP91, BE94b, TP95b]. 1.2.3 Program Transformation Program transformation techniques used in parallelizing compilers <ref> [Pad79, PW86] </ref> can be classified into two classes: control flow transformation and data flow transformation. Control flow transformations change the control of the program and hence the schedule of computation.
Reference: [Pad89] <author> David A. Padua. </author> <title> The Delta Program Manipulation system | Preliminary design. </title> <type> CSRD Report 808, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: After reshaping, we can determine that DEF X covers USE Y . Therefore, A (1 : 10; 1 : 10) is privatizable in the loop. This interprocedural analysis is implemented in the DELTA <ref> [Pad89] </ref> system (a predecessor of Polaris implemented in SETL language). In the current Polaris system, we choose to use the inlining instead of interprocedural analysis.
Reference: [PKL80] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-Speed Multiprocessors and Compilation Techniques. </title> <journal> Special Issue on Parallel Processing, IEEE Trans. on Computers, </journal> <volume> C-29(9):763-776, </volume> <month> Sept., </month> <year> 1980. </year>
Reference-contexts: Sometimes, this is due to some necessary information being unknown at compile time; most often, though, it is due to the lack of sophisticated analysis techniques in the compilers [EHLP91]. The major tasks of parallelizing compilers can be divided into two parts <ref> [PKL80, PW86] </ref>: dependence analysis and restructuring transformation. Dependence analysis determines the inherent parallelism in the sequential programs.
Reference: [PP92] <author> Paul Petersen and David Padua. </author> <title> Machine-independent evaluation of parallelizing compilers. In Advanced Compilation Techniques for Novel Architectures, </title> <month> January </month> <year> 1992. </year>
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Sometimes, this is due to some necessary information being unknown at compile time; most often, though, it is due to the lack of sophisticated analysis techniques in the compilers [EHLP91]. The major tasks of parallelizing compilers can be divided into two parts <ref> [PKL80, PW86] </ref>: dependence analysis and restructuring transformation. Dependence analysis determines the inherent parallelism in the sequential programs. <p> This limitation can often be overcome by using symbolic analysis techniques [HP91, BE94b, TP95b]. 1.2.3 Program Transformation Program transformation techniques used in parallelizing compilers <ref> [Pad79, PW86] </ref> can be classified into two classes: control flow transformation and data flow transformation. Control flow transformations change the control of the program and hence the schedule of computation. <p> Cross-iteration anti-dependences arise because there is only one copy of A for the loop. Replicating X to create a private copy per iteration would eliminate the cross-iteration anti-dependences. Another related technique, known as scalar expansion <ref> [Wol78, PW86] </ref>, can also be applied to expand the X into an array to eliminate the anti-dependences. While today's commercial compilers are capable of privatizing (or expanding) scalar variables, they are not capable of privatizing arrays. <p> By providing a distinct instance of a variable to each processor, privatization can eliminate memory-related dependence. Previous studies on the effectiveness of automatic program parallelization show that privatization is one of the most effective transformations for the exploitation of parallelism [EHLP91]. A related technique, called expansion <ref> [PW86] </ref>, transforms each reference to a particular scalar into a reference to a vector element in such a way that each thread accesses a different vector element. When applied to an array, expansion creates a new dimension for the array. <p> Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing [TP92]. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization [BCFH89], scalar renaming [CF87], and array expansion <ref> [PW86] </ref> [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program.
Reference: [RND77] <author> E. M. Reingold, J. Nievergelt, and N. Deo. </author> <title> Combinatorial Algorithms, Theory and Practice. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1977. </year>
Reference-contexts: General recurrence relations are solved by trial-and-error techniques and therefore, difficult to implement in a compiler. But for an important class of recurrences, general solution techniques do exist. Linear recurrences with constant coefficients <ref> [RND77] </ref> are recurrences that have the following general form: x i = a 1 x i1 + a 2 x i2 + : : : + a k x ik + b (4:13) where a 1 ; a 2 ; : : :; a k are constant coefficients and b is
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proc. the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: In the transformed program, it is easier for them to allocate a register to a scalar X. The transformation can also reduce the amount of false sharing in multiprocessor caches. In a distributed memory system with owner computes rule [ZBG88][CK88b] <ref> [RP89] </ref>, the transformed program effectively transfers the ownership of A (I) to iteration I; hence the processor scheduled to execute the iteration I can execute operations in S2 even if it does not own A (I).
Reference: [RWZ88] <author> B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Global value numbers and redundant computation. </title> <booktitle> In Proc. of the 15th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <year> 1988. </year>
Reference-contexts: Forward substitution uses data flow analysis to determine invariant expressions and substitutes variables with invariant expressions. 3.1.1 Static Single Assignment Form Recently, Static Single Assignment (SSA) has been proposed to represent the data value flow property of programs <ref> [AWZ88, RWZ88] </ref>. In the SSA form, each variable in the original program is replaced with several new variables such that only one assignment can reach each use. <p> Using the GSA sparse representation and the demand-driven approach, our approach can perform more aggressive analysis only on demand. The SSA form has been used to determine the equivalence of symbolic variables and to construct global value graph in a program [AWZ88] <ref> [RWZ88] </ref>. Nascent [Wol92] uses SSA to do a comprehensive analysis of recurrences. Their representation does not include the gating predicate. The SSA representation in Nascent is through an explicit representation of use-def chains, which they call a demand-driven form of SSA.
Reference: [SG94] <author> V.C. Sreedhar and G.R. Gao. </author> <title> Computing -nodes in linear time using dj-graph. </title> <type> Technical Report Technical Report,ACAPS Technical Memo 75, </type> <institution> McGill University, School of Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Johnson and Pingali [JP93, JPP94] proposed another algorithm to place -functions in O (E) time. Later, Cytron and Ferrante proposed an almost-linear time O (Eff (E)) using path compression. Recently, Sreedhar and Gao <ref> [SG94] </ref> have developed another O (E) time algorithm.
Reference: [Tar79] <author> Robert Endre Tarjan. </author> <title> Applications of path compression on balanced trees. </title> <journal> Journal of ACM, </journal> <volume> 26(4) </volume> <pages> 690-715, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: Our new algorithm to be presented constructs and places the gating functions from a program control flow graph in a single step. In our algorithm, SSA and GSA constructions are unified under a single process of gating path construction. It uses the path compression technique <ref> [Tar79] </ref> to reduce the total number of visits to the edges in the flow graph. Tarjan describes two ways to implement the path compression. <p> A simple method has an O (E log (N )) time bound; a sophisticated off-line algorithm maintaining balanced subtrees has an O (Eff (E; N)) time bound. Yet another on-line O (Eff (E; N )) method, called stratified path compression by Farrow <ref> [Far77, Tar79] </ref>, can also be used. This GSA algorithm is also almost as efficient as the best known algorithms for -function placement in SSA conversion. 48 The rest of this chapter is divided into the following sections. In Section 3.3, we introduce some background and notations. <p> The path expression computed by EV AL (e) is later used to update G fl (v) to represent the path from the loop back edge e. G fl (v) is used to build a function for the loop. Using Tarjan's technique for operations on a forest <ref> [Tar79] </ref>, we define the following operations on the forest of subtrees in a dominator tree: * EV AL (e): Let e = (w; v). <p> Because the sequence of EV AL and LIN K can be easily determined beforehand, the off-line algorithm in <ref> [Tar79] </ref> can also be used to achieve O (Eff (E; N)) time complexity. 70 Theorem 3.3 The time complexity of the algorithm is O (Eff (E; N )). For each node v, the algorithm computes the (v), GP (v), and G fl (v). <p> On average, GSA constructions cost 15 percent more than SSA constructions. 76 3.8 Conclusions In this chapter, we present an almost-linear time algorithm to place and build gating functions in a single step for GSA construction. The algorithm is based on the well-known path compression technique <ref> [Tar79] </ref>. Our technique reveals the relationship between the gating path and the iterative dominance frontier in the flow graph and provides a new perspective of the SSA and GSA representation. As compared with GSA algorithms, the new algorithm is more concise and more efficient.
Reference: [Tar81a] <author> Robert Endre Tarjan. </author> <title> Fast algorithm for solving path problems. </title> <journal> Journal of the ACM, </journal> <volume> 28(3) </volume> <pages> 594-614, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Irreducible graphs can also be handled by computing a path sequence for each dominator strong components. Whereas the two existing GSA algorithms handle only reducible graph, the algorithm here can be extended to handle irreducible graphs. Interested readers should refer to <ref> [Tar81a] </ref> for details on building path expressions for irreducible graph. In the merge phase, the algorithm follows the topsort order and computes for each child of u a path expression GP (v) representing all the gating paths of v. <p> This algorithm is a variant of Tarjan's fast algorithm for solving path problems using dom-inator strong components decomposition <ref> [Tar81a] </ref>. Its correctness can be derived from the following Lemma, which we quote without proof here. We will work through an example to illustrate the algorithm. Lemma 3.7 [Tar81a] * For edges e = (w; v) in CF G such that w 6= u, the corresponding path expression in the ListP <p> This algorithm is a variant of Tarjan's fast algorithm for solving path problems using dom-inator strong components decomposition <ref> [Tar81a] </ref>. Its correctness can be derived from the following Lemma, which we quote without proof here. We will work through an example to illustrate the algorithm. Lemma 3.7 [Tar81a] * For edges e = (w; v) in CF G such that w 6= u, the corresponding path expression in the ListP (v) computed by the derive phase is an unambiguous path expression representing exactly the paths from subroot (w) to v that end with e and contain only proper
Reference: [Tar81b] <author> Robert Endre Tarjan. </author> <title> A unified approach to path problems. </title> <journal> Journal of the ACM, </journal> <volume> 28(3) </volume> <pages> 577-593, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: A path expression <ref> [Tar81b] </ref> P of type (u; v) is a simple regular expression over E such that every string in (P ) is a path from node u to node v (where (P ) represents the string generated by the regular expression P ).
Reference: [Tow76] <author> Ross Albert Towle. </author> <title> Control and Data Dependence for Program Transformations. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Mar., </month> <year> 1976. </year>
Reference-contexts: Traditional data flow analysis can recognize subscripted variables only as accesses to the whole array. However, that is not sufficient for our purposes. The work of D. Kuck, L. Lamport, and others in early 1970s <ref> [KMC72, KBC + 74, Lam74, Tow76] </ref> was a major breakthrough since they were the first to take into account subscript expressions in the dependence analysis. The basic ideas that were developed at that time still dominate the research of automatic parallelization today. Subscript analysis examines each subscript individually.
Reference: [TP92] <author> Peng Tu and David Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proc. 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, in ACM SIGPLAN Notices, </booktitle> <month> January, </month> <year> 1993, </year> <month> September </month> <year> 1992. </year>
Reference-contexts: Because the access to a private variable is inherently local, privatization reduces the communication and facilitates data distribution. Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing <ref> [TP92] </ref>. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization [BCFH89], scalar renaming [CF87], and array expansion [PW86] [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. <p> processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing <ref> [TP92] </ref>. 12 Previous work on eliminating memory-related dependence focused on scalar expansion [Wol82], scalar privatization [BCFH89], scalar renaming [CF87], and array expansion [PW86] [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program. <p> This transformation can facilitate data distribution, reduce communication, and improve load balance <ref> [TP92] </ref>. 28 Algorithm Profitability Test Input: P RI b for loop L: with index i 2 [p : q : t] Output: P RO, arrays profitable for privatization P RO foreach A (r) 2 P RI b do ALL A fA (r) : A (r) 2 P RI b g foreach
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proc. 6rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Some of the limitations are due to the lack of symbolic analysis techniques to handle subarray boundaries and the inability to handle conditionally defined and used privatizable arrays. The results in this chapter were obtained first, most of which were previously reported in <ref> [TP93] </ref>. The limitations just discussed led to subsequent work on the GSA program representation [TP95a], GSA-based demand-driven symbolic analysis [TP95b], and conditional data flow analysis. This subsequent work will be presented in Chapters 3 and 4. The rest of the chapter is organized as follows. <p> It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization <ref> [TP95b, TP93] </ref>. In the SSA representation, -functions of a single type are placed at the join nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. The condition under which a definition reachs a join node is not represented in the -function. <p> Several research compilers, such as Parafrase-2 [HP92], and Nascent [GSW], use symbolic analysis to identify and transform induction variables. In the Polaris [BEF + 94] restructuring compiler, we use symbolic analysis for dependence analysis, symbolic range propagation, and array privati-zation [BEF + 94] <ref> [TP93] </ref>. Simple symbolic analysis techniques are used regularly in the traditional optimizing compilers. Constant propagation detects symbolic variables that are equivalent to constants. Also, common subexpression elimination determines the equivalence of two symbolic expressions to avoid redundant computation.
Reference: [TP95a] <author> Peng Tu and David Padua. </author> <title> Efficient building and placing of gating functions. </title> <booktitle> In Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The results in this chapter were obtained first, most of which were previously reported in [TP93]. The limitations just discussed led to subsequent work on the GSA program representation <ref> [TP95a] </ref>, GSA-based demand-driven symbolic analysis [TP95b], and conditional data flow analysis. This subsequent work will be presented in Chapters 3 and 4. The rest of the chapter is organized as follows. <p> Their technique can be used to eliminate redundant dependences in the gating functions. Recently, we developed an algorithm to efficiently construct the GSA directly from the control flow graph <ref> [TP95a] </ref> (Chapter 3). Control dependence was introduced in [FOW87] as part of PDG. The SSA construction paper [CFR + 91] also has an algorithm for building control dependences.
Reference: [TP95b] <author> Peng Tu and David Padua. </author> <title> Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers. </title> <booktitle> In Proc. ACM 9th International Conference on Supercomputing'95, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Because dependence tests check the sufficient conditions of data independence, when the tests failed due to unknown symbolic values in subscript expressions or loop bounds, compilers have to assume the existence of dependence. This limitation can often be overcome by using symbolic analysis techniques <ref> [HP91, BE94b, TP95b] </ref>. 1.2.3 Program Transformation Program transformation techniques used in parallelizing compilers [Pad79, PW86] can be classified into two classes: control flow transformation and data flow transformation. Control flow transformations change the control of the program and hence the schedule of computation. <p> The results in this chapter were obtained first, most of which were previously reported in [TP93]. The limitations just discussed led to subsequent work on the GSA program representation [TP95a], GSA-based demand-driven symbolic analysis <ref> [TP95b] </ref>, and conditional data flow analysis. This subsequent work will be presented in Chapters 3 and 4. The rest of the chapter is organized as follows. Section 2.2 is an overview of the issues in automatic array privatization and gives an example that motivates this work. <p> It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization <ref> [TP95b, TP93] </ref>. In the SSA representation, -functions of a single type are placed at the join nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. The condition under which a definition reachs a join node is not represented in the -function.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Compiler optimizations, such as prefetching and updating, are also very important for overlapping communication and computation in order to hide communication latencies. For efficient implementation of DSM, most systems support active message <ref> [vECGS92] </ref> which contains a notification mechanism for the arrival of data.
Reference: [Wol78] <author> Michael Joseph Wolfe. </author> <title> Techniques for Improving the Inherent Parallelism in Programs. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> July, </month> <year> 1978. </year>
Reference-contexts: Cross-iteration anti-dependences arise because there is only one copy of A for the loop. Replicating X to create a private copy per iteration would eliminate the cross-iteration anti-dependences. Another related technique, known as scalar expansion <ref> [Wol78, PW86] </ref>, can also be applied to expand the X into an array to eliminate the anti-dependences. While today's commercial compilers are capable of privatizing (or expanding) scalar variables, they are not capable of privatizing arrays.
Reference: [Wol82] <author> Michael Joseph Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> Technical Report UIUCDCS-R-82-1105, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Since private instances of a variable are spread among all the active processors, privatization provides opportunities to spread computation among the processors and thus improve load balancing [TP92]. 12 Previous work on eliminating memory-related dependence focused on scalar expansion <ref> [Wol82] </ref>, scalar privatization [BCFH89], scalar renaming [CF87], and array expansion [PW86] [Fea88]. Recently there have been several papers on array privatization [Li92] [MAL92] [TP92]. In this chapter, we present an algorithm for automatically generating an annotated parallel program from a sequential program.
Reference: [Wol87] <author> Michael Wolfe. </author> <title> Loop Optimization vs. Vectorization. </title> <booktitle> Proc. of 1987 Int'l. Conf. on Supercomputing, </booktitle> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: Techniques such as unimodular transformation [Ban90] combine several loop transformations into one transformation. Besides changing the computation schedule to achieve a desirable parallelism, control 6 flow transformation such as loop tiling <ref> [Wol87] </ref> can also be used to improve the data locality of computation. Data flow transformation techniques change the data value flow. Two most important transformations are privatization and idiom replacement. These transformations change the flow data values to reduce the number of data dependences and increase parallelism.
Reference: [Wol92] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> ACM PLDI'92, </booktitle> <year> 1992. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) [BMO90]. It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches [WZ91]; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution <ref> [Wol92] </ref>; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization [TP95b, TP93]. In the SSA representation, -functions of a single type are placed at the join nodes of a program flow graph to represent different definitions of a variable reaching from different incoming edges. <p> We present algorithms to build and analyze the symbolic expressions. It combines demand-driven interpretation of GSA and path conditions to analyze complicated symbolic expressions. This technique has been implemented in the Polaris 80 compiler. Combined with the known SSA-based induction variable analysis techniques <ref> [Wol92] </ref>, it serves as a tool for solving the following symbolic analysis problems in parallelizing compilers: * Refining the symbolic values of expressions using program control dependence relations. * Determining the relationship between symbolic expressions. * Identifying recurrences, computing their closed form expressions, and determining sym bolic upper and lower bounds. <p> When P is always true and A is a linear reference to an array, for example A=X (i), J 1 (i) is a sum reduction over X. Wolfe and others <ref> [Wol92] </ref> developed a comprehensive technique to classify and solve recurrence sequences using a graph representation of SSA. In this graph representation, a recurrence 94 is characterized by a Strongly Connected Region (SCR) of use-def chains. The backward sub-stitution technique for functions is equivalent to Wolfe's SCR approach. <p> Using the GSA sparse representation and the demand-driven approach, our approach can perform more aggressive analysis only on demand. The SSA form has been used to determine the equivalence of symbolic variables and to construct global value graph in a program [AWZ88] [RWZ88]. Nascent <ref> [Wol92] </ref> uses SSA to do a comprehensive analysis of recurrences. Their representation does not include the gating predicate. The SSA representation in Nascent is through an explicit representation of use-def chains, which they call a demand-driven form of SSA.
Reference: [WZ91] <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: GSA was introduced by Ballance, Maccabe and 45 Ottenstein as a part of Program Dependence Web (PDW) [BMO90]. It is a convenient represen-tation for several program analysis and optimization techniques, including constant propagation with conditional branches <ref> [WZ91] </ref>; equality of symbolic expressions [AWZ88, Hav93]; induction variable substitution [Wol92]; symbolic dependence analysis [BE94a]; and demand-driven symbolic analysis for array privatization [TP95b, TP93].
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference: [ZC91] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: A definition of variable v in a basic block S is said to be outward exposed if it is the last definition 20 of v in S. A use of v is outward exposed if S does not contain a definition of v before the use <ref> [ZC91] </ref>. Definition 2.4 Let S be a basic block and V AR be the set of scalar variables, subscripted variables, and subarrays in the program. Henceforth these are called variables. 1. DEF (S) := fv 2 V AR : v has an outward exposed definition in S g 2.

References-found: 76

