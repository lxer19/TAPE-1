URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/tcm/www/mowry94.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: http://www.cs.cmu.edu
Title: TOLERATING LATENCY THROUGH SOFTWARE-CONTROLLED DATA PREFETCHING  
Author: Todd C. Mowry 
Degree: a dissertation submitted to the department of electrical engineering and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: March 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> Proc. of the 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [2] <author> S. Adve and M. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: While conceptually intuitive and elegant, sequential consistency imposes severe restrictions on the outstanding accesses that a process may have, thus restricting the buffering and pipelining allowed. In contrast, relaxed consistency models <ref> [2, 18, 26, 27] </ref> permit accesses to be buffered and pipelined, provided that explicit synchronization events are identified and ordered properly. Once again, however, the main benefit of these relaxed Chapter 1. <p> Assuming that a program is "properly labeled" [27] or "data-race-free" <ref> [2] </ref>, synchronization statements should exist between the time when one processor modifies data and a second processor reads that data. Therefore, the compiler interprets explicit synchronization as a hint that data communication may be taking place.
Reference: [3] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: As a result, most commercial RISC microprocessors provide support for cache hierarchies, including on-chip primary instruction and data caches. The benefits of caches in multiprocessors have also been recognized, where despite the complication of keeping shared writable data coherent [6], a number of multiprocessors with caches have been implemented <ref> [3, 41, 47, 55] </ref>. Therefore caches are an integral part of the memory latency solution, and the remaining techniques we discuss build upon caching as a foundation. 1.2.2 Locality Optimizations Locality optimizations attempt to make caches more effective by restructuring computation to enhance data locality. <p> The advantages of software-controlled prefetching are that only a small amount of hardware support is necessary, and a broader class of reference patterns can be covered than simply constant stride accesses (e.g., indirect references, such as in sparse-matrix code). 1.2.5 Multithreading be used to tolerate latency <ref> [3, 36, 44, 85] </ref>. In this figure, context #1 suffers a cache miss when it attempts to load location A. At this time, context #1 is swapped out and the processor begins to execute context #2. <p> Finally, hardware-coherent caches help reduce memory latency by allowing shared writable data to be cached and replicated. Examples of such machines include Kendall Square Research's KSR1 [41], Stanford's DASH [54], and MIT's Alewife <ref> [3] </ref>. One of the keys to achieving high utilization of a multiprocessor is effectively overlapping communication and computation. Message-passing machines allow the programmer to do this by sending explicit non-blocking messages. <p> While prefetching is one technique for hiding read latency, another Chapter 5. Architectural Issues 183 technique is for the processor to support multiple hardware contexts <ref> [3, 36, 39, 73, 85] </ref> (also known as multithreading). As we mentioned earlier in Section 1.2.5, multithreading has two advantages over prefetching. First, it can handle arbitrarily complex access patterns|even cases where it is impossible to predict the accesses ahead of time (and therefore prefetching cannot succeed).
Reference: [4] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126-138, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [5] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [6] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: As a result, most commercial RISC microprocessors provide support for cache hierarchies, including on-chip primary instruction and data caches. The benefits of caches in multiprocessors have also been recognized, where despite the complication of keeping shared writable data coherent <ref> [6] </ref>, a number of multiprocessors with caches have been implemented [3, 41, 47, 55].
Reference: [7] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: Chapter 1. Introduction 9 We will discuss those schemes only briefly now, and will examine them in greater detail later in Section 5.3.1. Perhaps the most sophisticated of these techniques is the one proposed by Baer and Chen <ref> [7] </ref>. With this scheme, the processor maintains a history table to keep track of the types of reference patterns it is seeing. If it detects a pattern of constant-stride access behavior for a particular instruction, it will attempt to prefetch ahead for that reference in the future. <p> Lee [53] proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable. He found that the effectiveness of the scheme was limited by branch prediction and by synchronization. Baer and Chen <ref> [7] </ref> proposed a scheme that uses a history buffer to detect constant-stride access patterns. In their scheme, a "lookahead PC" speculatively walks through the program ahead of the normal PC using branch prediction. When the lookahead PC finds a matching stride entry in the table, it issues a prefetch. <p> prediction is achieved in one of three ways: (i) assume that there is abundant spatial locality (as in the schemes Porterfield studied [64]); (ii) decode ahead in the instruction stream (as in Lee's proposal [53]); or (iii) maintain a history of past access patterns (as in Baer and Chen's proposal <ref> [7] </ref>). The idea Chapter 5. Architectural Issues 178 behind the first technique is that whenever a cache line is accessed, neighboring cache lines will be accessed in the near future, so they should also be brought into the cache.
Reference: [8] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year> <note> 194 Bibliography 195 </note>
Reference-contexts: The performance of the benchmarks was simulated by instrumenting the MIPS object code using pixie [74] and piping the resulting trace into our detailed cache simulator. due to memory accesses for 13 uniprocessor programs taken from the SPEC [77], SPLASH [72], and NAS Parallel <ref> [8] </ref> benchmark suites. Many of the programs spend a significant amount of time on memory accesses. In fact, 8 out of the 13 programs spend more than half of their time stalled for memory accesses. <p> This collection includes NASA7 and TOMCATV from the SPEC benchmarks [77], OCEAN-a uniprocessor version of a SPLASH benchmark [72], and CG (conjugate gradient), EP ("embarassingly parallel"|a Monte Carlo simulation), IS (integer sort), MG (multigrid) from the NAS Parallel Benchmarks <ref> [8] </ref>. Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA).
Reference: [9] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This section summarizes their work and relates it to ours. Porterfield <ref> [9, 64] </ref> was the first to explore software-controlled prefetching for uniprocessors. He proposed a compiler algorithm for inserting prefetches into dense-matrix codes. He implemented his algorithm as a preprocessing pass that inserted prefetching into the source code. His initial algorithm prefetched all array references in inner loops one iteration ahead. <p> Ideally, isolating the cache miss instances will not increase the instruction overhead. One of the advantages of having implemented the prefetching schemes in the compiler is that we can quantify this instruction overhead. Previous studies have only been able to estimate instruction overhead <ref> [9] </ref>. Chapter 3.
Reference: [10] <author> A. Carle, K. Kennedy, U. Kremer, and J. Mellor-Crummey. </author> <title> Automatic data layout for distributed-memory machines in the D programming environment. In Proceedings of AP'93 International Workshop on Automatic Distributed Memory Parallelization, Automatic Data Distribution and Automatic Parallel Performance Prediction, </title> <booktitle> Saarbrucken, </booktitle> <address> Germany, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [11] <author> B. Chapman, P. Hehrota, and H. Zima. </author> <title> Programming in vienna fortran. </title> <booktitle> In Third Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 121-160, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [12] <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [13] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitec-tures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of Microcomputing 24, </booktitle> <year> 1991. </year>
Reference-contexts: The complexity of the compiler algorithm presented in this work illustrates how difficult the compiler's job becomes when binding rather than non-binding prefetches are used. This is in sharp contrast with the simplicity of our non-binding prefetching algorithm, presented later in Section 4.1. Chen et al. <ref> [13] </ref> investigated prefetching for non-numerical codes. They attempted to move address generation back as far as possible before loads to hide a small cache miss latency (10 cycles), and found mixed results. <p> We will discuss this approach qualitatively in this subsection. The motivation presented in previous proposals for a prefetch target buffer is that by keeping the prefetched data separate, it cannot interfere with data in the normal cache (which is referred to as the cache "pollution" problem) <ref> [13, 42] </ref>. While this may be true, this approach has a number of drawbacks.
Reference: [14] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman. </author> <title> A vliw architecture for a trace scheduling compiler. </title> <booktitle> In Proc. Second Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 180-192, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: One manifestation of this is that several of the scalar machines designed for scientific computation did not use caches <ref> [14, 16] </ref>. This thesis investigates a technique called software-controlled prefetching which mitigates the impact of long cache miss penalties, thereby helping to unlock the full potential of microprocessor-based systems.
Reference: [15] <author> K.D. Cooper, M.W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2), </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: Since our prefetching algorithm does not look across procedure boundaries, it cannot recognize the temporal locality of the pivot column, and therefore prefetches it each time the procedure is called. Overcoming this limitation statically would require not only interprocedural analysis, but also the ability to either inline or clone <ref> [15] </ref> the procedure body so that prefetches can be scheduled for only the first use of a pivot column. Another possibility is to use dynamic information in the form of hardware miss counters, thus allowing the procedure to adapt dynamically to whether the pivot column is in the cache. <p> To isolate only the coherence misses, we would like to prefetch a panel only the first time it is accessed by a given processor. This cannot occur normally, but would be possible with either inlining or procedure cloning <ref> [15] </ref>. (Note that a very similar situation occurs in LU.) For LOCUS, however, the answer is no, coherence misses Chapter 4. Prefetching for Multiprocessors 112 cannot be isolated. <p> On the other hand, if a column can fit in the cache, then the pivot column will only suffer misses the first time it is referenced. However, since this code is in a separate procedure, and since our compiler does not perform procedure cloning <ref> [15] </ref>, the only static option is to prefetch the column all the time. Once again, Chapter 5.
Reference: [16] <author> J. C. Dehnert, P. Y.-T. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the cydra 5. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 26-38, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: One manifestation of this is that several of the scalar machines designed for scientific computation did not use caches <ref> [14, 16] </ref>. This thesis investigates a technique called software-controlled prefetching which mitigates the impact of long cache miss penalties, thereby helping to unlock the full potential of microprocessor-based systems.
Reference: [17] <author> M. Dubois, L. Barroso, Y.-S. Chen, and K. </author> <title> Oner. Scalability problems in multiprocessors with private caches. </title> <booktitle> In Proceedings of Parallel Architecture and Languages Europe '92, </booktitle> <pages> pages 211-230, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme [63, 70, 79, 46]. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself <ref> [17, 52] </ref>, thereby permitting a larger number of outstanding misses. For the uniprocessor architecture used in Chapter 3, the lockup-free cache supports a single load or store miss (since loads and stores 7 directly stall the processor) and up to sixteen prefetch misses.
Reference: [18] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year> <note> Bibliography 196 </note>
Reference-contexts: While conceptually intuitive and elegant, sequential consistency imposes severe restrictions on the outstanding accesses that a process may have, thus restricting the buffering and pipelining allowed. In contrast, relaxed consistency models <ref> [2, 18, 26, 27] </ref> permit accesses to be buffered and pipelined, provided that explicit synchronization events are identified and ordered properly. Once again, however, the main benefit of these relaxed Chapter 1.
Reference: [19] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 377-381, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: However, increasing the cache line size is not the most effective form of prefetching, since memory bandwidth is wasted whenever useless data is brought into the cache [64]. In addition, long cache lines can aggravate miss rates in shared-memory multiprocessors by causing unnecessary amounts of false sharing <ref> [19, 81] </ref>. As we have seen already in Figures 1.2 and 1.3, a significant amount of latency remains despite the prefetching benefit of multi-word cache lines. Another form of prefetching could occur with non-blocking loads [66]. <p> In addition, while it may be tractable to understand when particular data items are shared among processors (i.e. true sharing), it is more difficult to predict the coherence misses that only occur because separate items fall within the same cache line (i.e. false sharing) <ref> [19, 81] </ref>. Finally, when the compiler is dealing with explicitly-parallelized programs, as is the case in our experiments 1 , it is difficult (if not impossible) for the compiler to extract the communication patterns, since much of this semantic information is contained only in the programmer's head.
Reference: [20] <author> M. Berry et al. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD 827, </type> <institution> Center for Supercomputing Research and Development, Illinois, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Contention for the task queue is not a problem since each routing task is fairly large-grain. For our experiments we run the largest circuit provided with the application, Primary2.grin, which contains 3817 wires and a 1290-by-20 cost array. WATER is adapted from the Perfect Club Benchmarks <ref> [20] </ref> and performs N-body molecular dynamics simulation of the forces and potentials in a system of water molecules to predict some physical properties of water in a liquid state. The primary data structure is a large array of Chapter 4.
Reference: [21] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1991. </year>
Reference-contexts: This first substep is a simplified version of algorithms proposed previously <ref> [21, 24, 64] </ref>. Once the amount of data accessed by each loop has been estimated, the second substep is comparing these amounts with the expected cache capacity to decide which loops are localized (i.e. Is Localized in Figure 2.7).
Reference: [22] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [23] <author> D. Gannon and W. Jalby. </author> <title> The influence of memory hierarchy on algorithm organization: Programming FFTs on a vector multiprocessor. In The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [24] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: If so, then the reference has spatial reuse. Group Reuse For reuse among different array references, Gannon et al. observe that data reuse is exploitable only if the references are uniformly generated; that is, references whose array index expressions differ in at most the constant term <ref> [24] </ref>. For example, references B [j][0] and B [j+1][0] in the former case, B [j][0] is accessing data brought into the cache by B [j+1][0] during the previous j iteration, making it very likely that this reuse will result in locality. <p> This first substep is a simplified version of algorithms proposed previously <ref> [21, 24, 64] </ref>. Once the amount of data accessed by each loop has been estimated, the second substep is comparing these amounts with the expected cache capacity to decide which loops are localized (i.e. Is Localized in Figure 2.7).
Reference: [25] <author> A. George, J. Liu, and E. Ng. </author> <title> User's guide for SPARSPAK: Waterloo sparse linear equations package. </title> <type> Technical Report CS-78-30, </type> <institution> Department of Computer Science, University of Waterloo, </institution> <year> 1980. </year>
Reference-contexts: Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA). In addition, for our study in Section 3.5 on prefetching indirect references, we also evaluate MP3D (another uniprocessor version of a SPLASH benchmark) and SPARSPAK <ref> [25] </ref> (a sparse matrix application), since these applications contain many indirect references. Table 3.1 provides a brief summary of the applications, including their input data sets, and Table 3.2 shows some general characteristics of the applications. <p> The processors remove panels from this task queue and perform all of their associated modifications, which in turn causes other panels to be placed on the task queue. The principal data structure is the sparse matrix itself, which is stored in a compressed format similar to that of SPARSPAK <ref> [25] </ref>. The primary operation that is performed repeatedly is adding a multiple of one column to another column. Contention occurs for the task queue and the modified columns, which are protected by locks.
Reference: [26] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: While conceptually intuitive and elegant, sequential consistency imposes severe restrictions on the outstanding accesses that a process may have, thus restricting the buffering and pipelining allowed. In contrast, relaxed consistency models <ref> [2, 18, 26, 27] </ref> permit accesses to be buffered and pipelined, provided that explicit synchronization events are identified and ordered properly. Once again, however, the main benefit of these relaxed Chapter 1. <p> Introduction 7 Without Prefetching With Prefetching Load A fi Fetch A Load B fi Fetch B Load A fi Load B fi Prefetch A fi Prefetch B fi Fetch A Fetch B Executing Instructions Stalled Waiting for Data Time consistency models is hiding write latency <ref> [26] </ref>. To address read latency effectively, we must look beyond buffering and pipelining. 1.2.4 Prefetching The two main techniques for tolerating read latency as well as write latency are prefetching and multithreading.
Reference: [27] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: While conceptually intuitive and elegant, sequential consistency imposes severe restrictions on the outstanding accesses that a process may have, thus restricting the buffering and pipelining allowed. In contrast, relaxed consistency models <ref> [2, 18, 26, 27] </ref> permit accesses to be buffered and pipelined, provided that explicit synchronization events are identified and ordered properly. Once again, however, the main benefit of these relaxed Chapter 1. <p> Although the compiler cannot precisely understand the communication patterns in the explicitly-parallelized codes used in our experiments, one thing that serves as a useful hint is the explicit synchronization statements inserted by the programmer to ensure that shared data are accessed safely. Assuming that a program is "properly labeled" <ref> [27] </ref> or "data-race-free" [2], synchronization statements should exist between the time when one processor modifies data and a second processor reads that data. Therefore, the compiler interprets explicit synchronization as a hint that data communication may be taking place. <p> The latency shown for writes is the time for retiring the request from the write buffer. This latency is the time for acquiring exclusive ownership of the line, which does not necessarily include the time for receiving acknowledgment messages from invalidations, since the release consistency model is used <ref> [27] </ref>. 4.2.2 Applications In this subsection we describe the computational structure of the parallel applications used in this chapter. This information is useful in later sections for understanding the performance results. <p> While conceptually intuitive, this model imposes severe restrictions on the buffering and pipelining of memory accesses. One of the least strict models is the release consistency model (RC) <ref> [27] </ref>. It requires that synchronization accesses in the program be identified and classified as either acquires (e.g., locks) or releases (e.g., unlocks).
Reference: [28] <author> A. J. Goldberg. </author> <title> Multiprocessor Performance Debugging and Memory Bottlenecks. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year> <note> Bibliography 197 </note>
Reference-contexts: This information can be collected at various levels of granularity. For example, the Mtool utility <ref> [28] </ref> collects information at the granularity of a single loop nesting. This is useful for identifying which loop nests suffer the most from latency, and which loop nests are insignificant.
Reference: [29] <author> S. R. Goldschmidt and H. Davis. </author> <title> Tango introduction and tutorial. </title> <type> Technical Report CSL-TR-90-410, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: The simulations are based on a 16 processor configuration. The architecture simulator is tightly coupled to the Chapter 4. Prefetching for Multiprocessors 96 Tango-Lite reference generator (the threads-based successor to the process-based Tango reference generator) <ref> [29] </ref> to assure a correct interleaving of accesses. For example, a process doing a read operation is blocked until that read completes, where the latency of the read is determined by the architecture simulator. Operating system references are not modeled.
Reference: [30] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [31] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-Directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: Gornish, Granston and Veidenbaum <ref> [31, 32] </ref> presented an algorithm for determining the earliest time when it is safe to prefetch shared data in a multiprocessor with software-controlled cache coherency. Since the prefetches are binding, all control and data dependencies must be carefully considered. <p> This algorithm improves upon several previous proposals that focused on dense-matrix uniprocessor codes <ref> [31, 42, 64] </ref>. In addition, this algorithm handles indirect references, which frequently occur in sparse-matrix codes, and targets large-scale shared memory multiprocessors as well as uniprocessors. * A detailed evaluation of the prefetching algorithm based on a full compiler implementation. <p> As a result, compiler algorithms that insert binding prefetches spend most of their effort worrying about whether prefetching is legal <ref> [31] </ref>, and are often forced to be conservative due to complications such as imperfect memory disambiguation and explicitly-parallelized code. To make significant headway in prefetching for multiprocessors, we must move beyond the distractions of correctness and focus instead on the deeper performance issues. <p> However, both of these approaches have drawbacks, which we will briefly discuss. The large block prefetch approach was studied by Gornish et al. <ref> [31, 32] </ref> in the context of a software-coherent shared-memory multiprocessor with binding prefetches. The potential problem of large block prefetches is that data may arrive in the cache too early, thereby exposing it to possible replacement before it can be used.
Reference: [32] <author> E. H. Gornish. </author> <title> Compile time analysis for data prefetching. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Gornish, Granston and Veidenbaum <ref> [31, 32] </ref> presented an algorithm for determining the earliest time when it is safe to prefetch shared data in a multiprocessor with software-controlled cache coherency. Since the prefetches are binding, all control and data dependencies must be carefully considered. <p> However, both of these approaches have drawbacks, which we will briefly discuss. The large block prefetch approach was studied by Gornish et al. <ref> [31, 32] </ref> in the context of a software-coherent shared-memory multiprocessor with binding prefetches. The potential problem of large block prefetches is that data may arrive in the cache too early, thereby exposing it to possible replacement before it can be used.
Reference: [33] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This information is useful in later sections for understanding the performance results. The parallel applications we use consist of the entire SPLASH suite [72] plus the LU-decomposition application that we used in earlier studies <ref> [33, 61] </ref>. These applications are representative of Chapter 4. Prefetching for Multiprocessors 92 Table 4.1: Latency for various memory system operations in processor clock cycles (1 pclock = 30 ns).
Reference: [34] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> College of Engineering, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [35] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [36] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The advantages of software-controlled prefetching are that only a small amount of hardware support is necessary, and a broader class of reference patterns can be covered than simply constant stride accesses (e.g., indirect references, such as in sparse-matrix code). 1.2.5 Multithreading be used to tolerate latency <ref> [3, 36, 44, 85] </ref>. In this figure, context #1 suffers a cache miss when it attempts to load location A. At this time, context #1 is swapped out and the processor begins to execute context #2. <p> While prefetching is one technique for hiding read latency, another Chapter 5. Architectural Issues 183 technique is for the processor to support multiple hardware contexts <ref> [3, 36, 39, 73, 85] </ref> (also known as multithreading). As we mentioned earlier in Section 1.2.5, multithreading has two advantages over prefetching. First, it can handle arbitrarily complex access patterns|even cases where it is impossible to predict the accesses ahead of time (and therefore prefetching cannot succeed).
Reference: [37] <author> L. J. Hendren. </author> <title> Parallelizing Programs with Recursive Data Structures. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Could the compiler insert these same prefetches automatically? With the help of sophisticated pointer analysis to recognize the tree structures and the order in which they are traversed <ref> [37] </ref>, Chapter 4. <p> However, analyzing recursive data structures may be quite difficult since it usually requires pointer analysis, which is a difficult problem for the compiler in general <ref> [37, 51] </ref>. Rather than relying strictly upon static information, another possibility is to make use of dynamic information. Dynamic information could be used either at compile-time, through the use of feedback information, or at run-time, by generating adaptive code. We will discuss both possibilities in this subsection. Chapter 5.
Reference: [38] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling fortran d for mimd distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [39] <author> R. A. </author> <title> Iannucci. Toward a dataflow/von Neumann hybrid architecture. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 131-140, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: While prefetching is one technique for hiding read latency, another Chapter 5. Architectural Issues 183 technique is for the processor to support multiple hardware contexts <ref> [3, 36, 39, 73, 85] </ref> (also known as multithreading). As we mentioned earlier in Section 1.2.5, multithreading has two advantages over prefetching. First, it can handle arbitrarily complex access patterns|even cases where it is impossible to predict the accesses ahead of time (and therefore prefetching cannot succeed).
Reference: [40] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year> <note> Bibliography 198 </note>
Reference-contexts: Therefore minimizing the degree of associativity is an important concern. One option is to have a set-associative primary cache, where addresses are mapped to N-way associative sets. Another approach is to keep the cache direct-mapped but also add a "victim cache" <ref> [40] </ref> off to the side. A victim cache is a small buffer containing the last several lines replaced from the cache.
Reference: [41] <institution> Kendall Square Research. Kendall Square Research 1 (KSR1) Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: As a result, most commercial RISC microprocessors provide support for cache hierarchies, including on-chip primary instruction and data caches. The benefits of caches in multiprocessors have also been recognized, where despite the complication of keeping shared writable data coherent [6], a number of multiprocessors with caches have been implemented <ref> [3, 41, 47, 55] </ref>. Therefore caches are an integral part of the memory latency solution, and the remaining techniques we discuss build upon caching as a foundation. 1.2.2 Locality Optimizations Locality optimizations attempt to make caches more effective by restructuring computation to enhance data locality. <p> Finally, hardware-coherent caches help reduce memory latency by allowing shared writable data to be cached and replicated. Examples of such machines include Kendall Square Research's KSR1 <ref> [41] </ref>, Stanford's DASH [54], and MIT's Alewife [3]. One of the keys to achieving high utilization of a multiprocessor is effectively overlapping communication and computation. Message-passing machines allow the programmer to do this by sending explicit non-blocking messages.
Reference: [42] <author> A. C. Klaiber and H. M. Levy. </author> <title> Architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 43-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Also, the more sophisticated scheme was not automated, since the overflow iterations were calculated by hand, and did not take cache line reuse into account. Despite leaving many important questions unanswered, Porterfield's work demonstrated that software-controlled prefetching was a promising technique that warranted further exploration. Klaiber and Levy <ref> [42] </ref> extended Porterfield's work by recognizing the need to prefetch more than a single iteration ahead. They included several memory system parameters in their equation for how many iterations ahead to prefetch, and inserted prefetches by hand at the assembly-code level. <p> This algorithm improves upon several previous proposals that focused on dense-matrix uniprocessor codes <ref> [31, 42, 64] </ref>. In addition, this algorithm handles indirect references, which frequently occur in sparse-matrix codes, and targets large-scale shared memory multiprocessors as well as uniprocessors. * A detailed evaluation of the prefetching algorithm based on a full compiler implementation. <p> We will discuss this approach qualitatively in this subsection. The motivation presented in previous proposals for a prefetch target buffer is that by keeping the prefetched data separate, it cannot interfere with data in the normal cache (which is referred to as the cache "pollution" problem) <ref> [13, 42] </ref>. While this may be true, this approach has a number of drawbacks.
Reference: [43] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [44] <editor> J. S. Kowalik, editor. </editor> <title> Parallel MIMD Computation : The HEP Supercomputer and Its Applications. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The advantages of software-controlled prefetching are that only a small amount of hardware support is necessary, and a broader class of reference patterns can be covered than simply constant stride accesses (e.g., indirect references, such as in sparse-matrix code). 1.2.5 Multithreading be used to tolerate latency <ref> [3, 36, 44, 85] </ref>. In this figure, context #1 suffers a cache miss when it attempts to load location A. At this time, context #1 is swapped out and the processor begins to execute context #2.
Reference: [45] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-85, </pages> <year> 1981. </year>
Reference-contexts: Buffering read accesses is more difficult because unlike writes, the processor typically cannot proceed until the read access completes, since it needs the data that is being read. With non-blocking loads and a lockup-free cache <ref> [45] </ref>, it is possible to buffer and pipeline reads. A non-blocking load means that rather than stalling at the time the load is performed, the processor postpones stalling until the data is actually used. A lockup-free cache permits multiple outstanding cache misses. <p> Both levels of the cache are lockup-free <ref> [45] </ref> in the sense that multiple prefetches can be outstanding along with either a single load or store miss. 1 The primary cache is checked in the cycle the prefetch instruction is executed. If the line is already in the cache, the prefetch is discarded. <p> The interface includes read and write buffers. The write buffer is 16 entries deep. Reads can bypass writes in the write buffer if the memory consistency model allows this. Both the first and second level caches are lockup-free <ref> [45] </ref>, direct-mapped, and use 16 byte lines. The bus bandwidth of the node bus is 133 Mbytes/sec, and the peak network bandwidth is approximately 120 Mbytes/sec into and 120 Mbytes/sec out of each node. <p> It is essential that the bandwidth of the memory hierarchy be increased to support the extra demand imposed by prefetching. An important step toward increasing memory hierarchy bandwidth is allowing multiple outstanding cache misses, which is referred to as having a lockup-free cache <ref> [45] </ref>. 6 This added bandwidth makes it possible to hide latency by overlapping memory accesses with other memory accesses, not just computation. This subsection is organized as follows. We begin by discussing issues associated with implementing a lockup-free cache, and relate them to our uniprocessor architecture. <p> For loads, the requested data must be forwarded directly to a register|thus requiring state to associate each outstanding access with the register (s) waiting for the value|and any future uses of that register must interlock if the value has not returned yet. Kroft <ref> [45] </ref> presented the original lockup-free cache design, which adds structures called "miss information/status handling registers" (MSHRs) to keep track of outstanding misses. Each MSHR contains enough state to handle one or more accesses of any type to a single memory line.
Reference: [46] <author> J. Kubiatowicz, D. Chaiken, and A. Agarwal. </author> <title> Closing the window of vulnerability in multiphase memory transactions. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-284, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme <ref> [63, 70, 79, 46] </ref>. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself [17, 52], thereby permitting a larger number of outstanding misses.
Reference: [47] <author> D. J. Kuck, E. S. Davidson, D. H. Lawrie, and A. H. Sameh. </author> <title> Experimental Parallel Computing Architectures: </title> <booktitle> Volume 1 Special Topics in Supercomputing, chapter Parallel Supercomputing Today and the Cedar Approach, </booktitle> <pages> pages 1-23. </pages> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: As a result, most commercial RISC microprocessors provide support for cache hierarchies, including on-chip primary instruction and data caches. The benefits of caches in multiprocessors have also been recognized, where despite the complication of keeping shared writable data coherent [6], a number of multiprocessors with caches have been implemented <ref> [3, 41, 47, 55] </ref>. Therefore caches are an integral part of the memory latency solution, and the remaining techniques we discuss build upon caching as a foundation. 1.2.2 Locality Optimizations Locality optimizations attempt to make caches more effective by restructuring computation to enhance data locality.
Reference: [48] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In Proc. ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Now that our compiler knows what to prefetch and has isolated those instances in the code with minimal overhead, the final step is to schedule the prefetches the proper amount of time in advance to hide the memory latency through software pipelining. 2.4.2 Software Pipelining Software pipelining <ref> [48, 65] </ref> is a technique that allows us to hide memory latency by overlapping the prefetches for a future iteration with the computation of the current iteration. While this transformation is straightforward, the key parameter is how far in advance the prefetches should be scheduled.
Reference: [49] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In this case, prefetched data tends to be replaced from the cache shortly after it arrives, so ideally it should arrive "just in time". Therefore, the lowest prefetch latency (100 cycles) offers the best the performance, as we see in that cause this behavior <ref> [49] </ref>. In general, we observe that it is better to be conservative with the prefetch latency parameter. Clearly if the value is not large enough to hide latency, it will always hurt performance. If we specify more latency than is actually experienced, it hurts performance only if data gets displaced.
Reference: [50] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):241-248, </volume> <month> September </month> <year> 1979. </year> <note> Bibliography 199 </note>
Reference-contexts: Therefore tolerating read latency through buffering and pipelining is not especially promising. Buffering and pipelining accesses in a multiprocessor is complicated by the restrictions placed on the causality of accesses in different processors. In the strictest case, known as sequential or strong consistency <ref> [50] </ref>, all accesses to shared data must appear as though the different processes were interleaved on a sequential machine. While conceptually intuitive and elegant, sequential consistency imposes severe restrictions on the outstanding accesses that a process may have, thus restricting the buffering and pipelining allowed. <p> Therefore it may be necessary to restrict the types of buffering and pipelining that are permitted. These restrictions are dictated by the memory consistency model supported by the multiprocessor. Several memory consistency models have been proposed. The strictest model is that of sequential consistency (SC) <ref> [50] </ref>. It requires the execution of a parallel program to appear as some interleaving of the execution of the parallel processes on a sequential machine. While conceptually intuitive, this model imposes severe restrictions on the buffering and pipelining of memory accesses.
Reference: [51] <author> W. Landi, B. G. Ryder, and S. Zhang. </author> <title> Interprocedural modification side effect analysis with pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 56-67, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: However, analyzing recursive data structures may be quite difficult since it usually requires pointer analysis, which is a difficult problem for the compiler in general <ref> [37, 51] </ref>. Rather than relying strictly upon static information, another possibility is to make use of dynamic information. Dynamic information could be used either at compile-time, through the use of feedback information, or at run-time, by generating adaptive code. We will discuss both possibilities in this subsection. Chapter 5.
Reference: [52] <author> J. P. Laudon. </author> <title> Architectural and Implementation Tradeoffs for Multiple-Context Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: Lockup-Free Cache A lockup-free cache is a common requirement for most latency-hiding techniques, including prefetching, relaxed consistency models, non-blocking loads, and multithreading. The complexity of implementing a lockup-free cache depends on which of these techniques it is intended to support (as described in detail by Laudon <ref> [52] </ref>). <p> The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme [63, 70, 79, 46]. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself <ref> [17, 52] </ref>, thereby permitting a larger number of outstanding misses. For the uniprocessor architecture used in Chapter 3, the lockup-free cache supports a single load or store miss (since loads and stores 7 directly stall the processor) and up to sixteen prefetch misses.
Reference: [53] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: This prefetching occurs through a "lookahead PC", which walks ahead of the actual PC using branch prediction. The lookahead PC is used to look up these future instructions in the history table to see whether they should be prefetched. Another scheme proposed by Lee <ref> [53] </ref> attempted to decode future instructions using a lookahead buffer to detect memory references. One advantage of strictly hardware-based schemes is that they do not incur any instruction overhead, unlike software-controlled prefetching (which we will discuss next). <p> Porterfield [64] evaluated several cacheline-based hardware prefetching schemes. In some cases they were quite effective at reducing miss rates, but at the same time they often increased memory traffic substantially. Lee <ref> [53] </ref> proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable. He found that the effectiveness of the scheme was limited by branch prediction and by synchronization. Baer and Chen [7] proposed a scheme that uses a history buffer to detect constant-stride access patterns. <p> In the various hardware-based prefetching schemes that have been proposed, this prediction is achieved in one of three ways: (i) assume that there is abundant spatial locality (as in the schemes Porterfield studied [64]); (ii) decode ahead in the instruction stream (as in Lee's proposal <ref> [53] </ref>); or (iii) maintain a history of past access patterns (as in Baer and Chen's proposal [7]). The idea Chapter 5.
Reference: [54] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We conducted a similar experiment to evaluate the impact of memory latency on large-scale shared-memory multiprocessors by simulating the entire SPLASH [72] parallel application suite on an architecture resembling the Stanford DASH multiprocessor <ref> [54] </ref>. The architecture we model includes 16 R3000 processors running at 33 MHz, two-level cache hierarchies (64 Kbytes/256 Kbytes), and a low-latency interconnection network. Miss latencies for loads range from 15 cycles to the secondary cache to over 130 cycles for "remote dirty" lines. <p> We now place this related work in the context of our own research. Porterfield's work on software-controlled prefetching for uniprocessors was done concurrently with our investigation of non-binding software-controlled prefetching for multiprocessors as part of the DASH project <ref> [54] </ref>. Chapter 1. Introduction 15 Our study, where we inserted prefetches by hand, was the first to consider non-binding prefetch-ing for multiprocessors [61]. The compiler algorithm presented in this dissertation partially overlapped the work by Porterfield and by Klaiber and Levy, but has a number of key differences. <p> Finally, hardware-coherent caches help reduce memory latency by allowing shared writable data to be cached and replicated. Examples of such machines include Kendall Square Research's KSR1 [41], Stanford's DASH <ref> [54] </ref>, and MIT's Alewife [3]. One of the keys to achieving high utilization of a multiprocessor is effectively overlapping communication and computation. Message-passing machines allow the programmer to do this by sending explicit non-blocking messages. <p> a line, and to eliminate unnecessary bandwidth consumption in read-modify-write situations. 4.2 Experimental Framework This section presents the architectural assumptions we make, the benchmark applications, and the simulation environment used to obtain performance results. 4.2.1 Architectural Assumptions For this study, we have chosen an architecture that resembles the DASH multiprocessor <ref> [54] </ref>, a large-scale cache-coherent machine that has been built at Stanford. Figure 4.5 shows the high-level organization of the simulated architecture. The architecture consists of several processing nodes connected through a low-latency scalable interconnection network. Physical memory is distributed among the nodes.
Reference: [55] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hennessy, Mark Horowitz, and Monica Lam. </author> <title> Design of Scalable Shared-Memory Multiprocessors: The DASH Approach. </title> <booktitle> In Proceedings of COMPCON'90, </booktitle> <pages> pages 62-67, </pages> <year> 1990. </year>
Reference-contexts: As a result, most commercial RISC microprocessors provide support for cache hierarchies, including on-chip primary instruction and data caches. The benefits of caches in multiprocessors have also been recognized, where despite the complication of keeping shared writable data coherent [6], a number of multiprocessors with caches have been implemented <ref> [3, 41, 47, 55] </ref>. Therefore caches are an integral part of the memory latency solution, and the remaining techniques we discuss build upon caching as a foundation. 1.2.2 Locality Optimizations Locality optimizations attempt to make caches more effective by restructuring computation to enhance data locality.
Reference: [56] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 213-221, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Another approach is for the compiler to determine both the data and computation decompositions <ref> [4, 5, 10, 12, 34, 35, 56] </ref>. While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so.
Reference: [57] <editor> E. Lusk, R. Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: One application (OCEAN) is written in Fortran, and the others are written in C. The Argonne National Laboratory macro package <ref> [57] </ref> is used to provide synchronization and sharing primitives. Table 4.2 provides a brief summary of the applications, along with their input data sets, and Table 4.3 shows some general characteristics of the applications when 16 processors are used (as is the case throughout this chapter).
Reference: [58] <author> D. E. Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: While locality optimizations are quite useful when they work, their applicability is somewhat limited because not only must there be a better way to structure the code (which is not always the case), but it also must be legal to do so. Whenever dependence analysis <ref> [58] </ref> is inexact, Chapter 1. Introduction 6 the compiler usually has to be conservative and assume that dependencies could be violated if the code was restructured. In practice, this means that these types of optimizations are not frequently applicable (as we will observe later in Section 3.4).
Reference: [59] <author> J. D. McDonald and D. Baganoff. </author> <title> Vectorization of a particle simulation method for hypersonic rarified flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Each processor waits until a column has been produced, and then that column is used to modify all columns that the processor owns. Once a processor completes a column, it releases any processors waiting for that column. For our experiments we performed LU-decomposition on a 200x200 matrix. MP3D <ref> [59] </ref> is a 3-dimensional particle simulator. It is used to study the pressure and temperature profiles created as an object flies at high speed through the upper atmosphere.
Reference: [60] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3) </volume> <pages> 153-165, </pages> <year> 1969. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [61] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <year> 1991. </year> <note> Bibliography 200 </note>
Reference-contexts: These applications are interesting both because they tend to suffer from memory latency, due to the large size of the arrays and the often wide separation between reuses, and because the access patterns are regular and predictable, which gives prefetching a good chance of success <ref> [61] </ref>. Next we extend this algorithm to handle indirect array references, which are another important case and are common to sparse-matrix codes. Finally, we address multiprocessing, where memory latencies can be quite Chapter 1. Introduction 13 large. <p> Porterfield's work on software-controlled prefetching for uniprocessors was done concurrently with our investigation of non-binding software-controlled prefetching for multiprocessors as part of the DASH project [54]. Chapter 1. Introduction 15 Our study, where we inserted prefetches by hand, was the first to consider non-binding prefetch-ing for multiprocessors <ref> [61] </ref>. The compiler algorithm presented in this dissertation partially overlapped the work by Porterfield and by Klaiber and Levy, but has a number of key differences. First, our uniprocessor algorithm is more comprehensive in the types of locality it optimizes for, including temporal, spatial, and group locality. <p> This information is useful in later sections for understanding the performance results. The parallel applications we use consist of the entire SPLASH suite [72] plus the LU-decomposition application that we used in earlier studies <ref> [33, 61] </ref>. These applications are representative of Chapter 4. Prefetching for Multiprocessors 92 Table 4.1: Latency for various memory system operations in processor clock cycles (1 pclock = 30 ns). <p> In this section, we examine applications with hand-inserted prefetching, discussing cases where the compiler was successful (Section 4.5.1), and also cases where the compiler failed (Section 4.5.2). 4.5.1 Cases Where the Compiler Succeeded In an earlier study, we inserted prefetches by hand into MP3D, LU, and PTHOR <ref> [61] </ref>. In the case of PTHOR, the access patterns are irregular and difficult to prefetch even by hand|we will discuss that case later in Section 4.5.2. In contrast, the other two cases (MP3D and LU) have regular and predictable access patterns. <p> The problem (as in BARNES) is that PTHOR is an irregular computation containing mainly pointers to linked lists, which are beyond the scope of our algorithm. The approach we take to inserting prefetches by hand was described in detail in an earlier study <ref> [61] </ref>, and we briefly summarize that strategy here. One of the main data structures in PTHOR is the element record, which stores all information about the type and state of a logic element. <p> Our results showed absolutely no difference in performance. This is partly because the lockup-free cache (which allows up to eight outstanding misses for the multiprocessor architecture) handles requests quickly enough that prefetches are rarely delayed behind writes. In an earlier study where we did not use a lockup-free cache <ref> [61] </ref>, the performance advantage of having a separate prefetch issue buffer was also rather small. Therefore the choice of separate or combined buffers should be dictated by whichever is easier to implement, since both schemes offer similar performance. <p> In addition, the bursts in network traffic caused by large block requests can lead to queueing delays. We observed these negative effects when using large block prefetches in LU during an earlier study <ref> [61] </ref>. While hardware-based prefetching schemes can stream data into the cache more evenly, they suffer from a number of disadvantages which we will describe in detail later in Section 5.3.1. To improve upon both of these techniques, we introduce the notion of programmable streams.
Reference: [62] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> volume 27, </volume> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Avoiding these unnecessary prefetches is an important goal of our compiler algorithm, which we describe in the next several sections. 2.2 Overview of Algorithm We have developed a compiler algorithm that selectively prefetches only those references that are likely to suffer cache misses <ref> [62] </ref>.
Reference: [63] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme <ref> [63, 70, 79, 46] </ref>. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself [17, 52], thereby permitting a larger number of outstanding misses.
Reference: [64] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. <p> This is most useful when there is abundant spatial locality, such as when iterating across an array in a unit-stride manner. However, increasing the cache line size is not the most effective form of prefetching, since memory bandwidth is wasted whenever useless data is brought into the cache <ref> [64] </ref>. In addition, long cache lines can aggravate miss rates in shared-memory multiprocessors by causing unnecessary amounts of false sharing [19, 81]. As we have seen already in Figures 1.2 and 1.3, a significant amount of latency remains despite the prefetching benefit of multi-word cache lines. <p> This section summarizes their work and relates it to ours. Porterfield <ref> [9, 64] </ref> was the first to explore software-controlled prefetching for uniprocessors. He proposed a compiler algorithm for inserting prefetches into dense-matrix codes. He implemented his algorithm as a preprocessing pass that inserted prefetching into the source code. His initial algorithm prefetched all array references in inner loops one iteration ahead. <p> This algorithm improves upon several previous proposals that focused on dense-matrix uniprocessor codes <ref> [31, 42, 64] </ref>. In addition, this algorithm handles indirect references, which frequently occur in sparse-matrix codes, and targets large-scale shared memory multiprocessors as well as uniprocessors. * A detailed evaluation of the prefetching algorithm based on a full compiler implementation. <p> This first substep is a simplified version of algorithms proposed previously <ref> [21, 24, 64] </ref>. Once the amount of data accessed by each loop has been estimated, the second substep is comparing these amounts with the expected cache capacity to decide which loops are localized (i.e. Is Localized in Figure 2.7). <p> Group (and not False None leading reference) (a) Code Before Peeling for (i = 0; i &lt; n; i++) f f (i); g (b) Code After Peeling f (0); for (i = 1; i &lt; n; i++) f g spatial locality requires the loop to be either unrolled or strip-mined <ref> [64] </ref> to isolate one in every l iterations. Figures 2.13 and 2.14 show the generic schemas for unrolling and strip-mining a loop, respectively. In general, the unrolling transformation is preferable for small or moderate values of l. <p> Although block prefetches may be helpful when there is spatial locality, they may hurt performance in the absence of spatial locality by displacing useful data and wasting memory bandwidth. This negative effect has been observed in previous studies where additional consecutive cache lines were automatically prefetched by the hardware <ref> [64] </ref>. Another potential downside is that bringing the additional lines into the cache earlier increases their chance of being displaced before use. <p> Note that as block prefetching increases the number of iterations between prefetches, it will eventually become more attractive to use strip mining <ref> [64] </ref> (as described earlier in Section 2.4.1) rather than unrolling to do the loop splitting, since the negative effects of loop unrolling dominate once they are unrolled too many times. Block prefetching is a straightforward extension of normal prefetching. <p> Finally, we evaluate multithreading in Section 5.3.3, which is a technique for hiding latency by exploiting parallelism across multiple threads of execution. 5.3.1 Hardware-Controlled Prefetching While software-controlled prefetching requires support from both hardware and software, several schemes have been proposed that are strictly hardware-based. Porterfield <ref> [64] </ref> evaluated several cacheline-based hardware prefetching schemes. In some cases they were quite effective at reducing miss rates, but at the same time they often increased memory traffic substantially. Lee [53] proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable. <p> In the various hardware-based prefetching schemes that have been proposed, this prediction is achieved in one of three ways: (i) assume that there is abundant spatial locality (as in the schemes Porterfield studied <ref> [64] </ref>); (ii) decode ahead in the instruction stream (as in Lee's proposal [53]); or (iii) maintain a history of past access patterns (as in Baer and Chen's proposal [7]). The idea Chapter 5.
Reference: [65] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some Scheduling Techniques and an Easily Schedulable Horizontal Architecture for High Performance Scientific Computing. </title> <booktitle> In Proceedings of the 14th Annual Workshop on Microprogramming, </booktitle> <pages> pages 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: Now that our compiler knows what to prefetch and has isolated those instances in the code with minimal overhead, the final step is to schedule the prefetches the proper amount of time in advance to hide the memory latency through software pipelining. 2.4.2 Software Pipelining Software pipelining <ref> [48, 65] </ref> is a technique that allows us to hide memory latency by overlapping the prefetches for a future iteration with the computation of the current iteration. While this transformation is straightforward, the key parameter is how far in advance the prefetches should be scheduled.
Reference: [66] <author> A. Rogers and K. Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> volume 27, </volume> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: As we have seen already in Figures 1.2 and 1.3, a significant amount of latency remains despite the prefetching benefit of multi-word cache lines. Another form of prefetching could occur with non-blocking loads <ref> [66] </ref>. With a non-blocking load, rather than stalling when the load is executed, any stalls are postponed until the load result is actually needed.
Reference: [67] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [68] <author> J. Rose. Locusroute: </author> <title> A parallel global router for standard cells. </title> <booktitle> In Design Automation Conference, </booktitle> <pages> pages 189-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Contention occurs for the task queue and the modified columns, which are protected by locks. For our experiments we ran bcsstk15 which is a 3948-by-3948 matrix with 56,934 non-zeroes in the matrix and 647,274 non-zeroes in the factor. LOCUS (our abbreviation for "LocusRoute") <ref> [68] </ref> is a high-quality global router for VLSI standard cells that has been used to design real integrated circuits. The parallelism in LOCUS comes from routing multiple wires concurrently.
Reference: [69] <author> E. Rothberg and A. Gupta. </author> <title> Techniques for improving the performance of sparse factorization on multiprocessor workstations. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: The program is parallelized by statically dividing the particles equally among the processors. The main synchronization consists of barriers between each time step. For our experiments we ran MP3D with 100,000 particles, a 32x6x32 space array, and simulated 5 time steps. CHOLESKY <ref> [69] </ref> performs sparse Cholesky factorization using a dynamic version of the supernodal fan-out method. The matrix is divided into supernodes (sets of columns with identical non-zero structures), which are further divided into conveniently-sized chunks called panels.
Reference: [70] <author> C. Scheurich and M. Dubois. </author> <title> Lockup-free caches in high-performance multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(1) </volume> <pages> 25-36, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme <ref> [63, 70, 79, 46] </ref>. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself [17, 52], thereby permitting a larger number of outstanding misses.
Reference: [71] <author> J. P. Singh and J. L. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experience, results and implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1) </volume> <pages> 27-48, </pages> <year> 1992. </year> <note> Bibliography 201 </note>
Reference-contexts: Table 4.2 provides a brief summary of the applications, along with their input data sets, and Table 4.3 shows some general characteristics of the applications when 16 processors are used (as is the case throughout this chapter). OCEAN <ref> [71] </ref> simulates the role of eddy and boundary currents in influencing large-scale ocean movements. It uses Successive Over Relaxation (SOR) to solve two-dimensional grids Chapter 4. Prefetching for Multiprocessors 93 Table 4.3: General statistics for the multiprocessor applications.
Reference: [72] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The performance of the benchmarks was simulated by instrumenting the MIPS object code using pixie [74] and piping the resulting trace into our detailed cache simulator. due to memory accesses for 13 uniprocessor programs taken from the SPEC [77], SPLASH <ref> [72] </ref>, and NAS Parallel [8] benchmark suites. Many of the programs spend a significant amount of time on memory accesses. In fact, 8 out of the 13 programs spend more than half of their time stalled for memory accesses. <p> In fact, 8 out of the 13 programs spend more than half of their time stalled for memory accesses. We conducted a similar experiment to evaluate the impact of memory latency on large-scale shared-memory multiprocessors by simulating the entire SPLASH <ref> [72] </ref> parallel application suite on an architecture resembling the Stanford DASH multiprocessor [54]. The architecture we model includes 16 R3000 processors running at 33 MHz, two-level cache hierarchies (64 Kbytes/256 Kbytes), and a low-latency interconnection network. <p> We begin by discussing how the prefetching compiler algorithm described in Chapters 2 and 3 is modified to address the issues unique to multiprocessing, and then evaluate its effect on the performance of the entire SPLASH <ref> [72] </ref> application suite. We also compare compiler-inserted prefetching with hand-inserted prefetching to see whether the compiler is living up to its potential, and to discover methods for further improvement. Chapter 5 explores the architectural issues associated with prefetching, and is divided into three distinct sections. <p> This collection includes NASA7 and TOMCATV from the SPEC benchmarks [77], OCEAN-a uniprocessor version of a SPLASH benchmark <ref> [72] </ref>, and CG (conjugate gradient), EP ("embarassingly parallel"|a Monte Carlo simulation), IS (integer sort), MG (multigrid) from the NAS Parallel Benchmarks [8]. Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA). <p> This information is useful in later sections for understanding the performance results. The parallel applications we use consist of the entire SPLASH suite <ref> [72] </ref> plus the LU-decomposition application that we used in earlier studies [33, 61]. These applications are representative of Chapter 4. Prefetching for Multiprocessors 92 Table 4.1: Latency for various memory system operations in processor clock cycles (1 pclock = 30 ns).
Reference: [73] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: While prefetching is one technique for hiding read latency, another Chapter 5. Architectural Issues 183 technique is for the processor to support multiple hardware contexts <ref> [3, 36, 39, 73, 85] </ref> (also known as multithreading). As we mentioned earlier in Section 1.2.5, multithreading has two advantages over prefetching. First, it can handle arbitrarily complex access patterns|even cases where it is impossible to predict the accesses ahead of time (and therefore prefetching cannot succeed).
Reference: [74] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: In this simple model, we assume that all instructions execute in a single cycle and that all instructions hit in the primary instruction cache. The performance of the benchmarks was simulated by instrumenting the MIPS object code using pixie <ref> [74] </ref> and piping the resulting trace into our detailed cache simulator. due to memory accesses for 13 uniprocessor programs taken from the SPEC [77], SPLASH [72], and NAS Parallel [8] benchmark suites. Many of the programs spend a significant amount of time on memory accesses. <p> The performance of the resulting object code is simulated by using the MIPS pixie utility <ref> [74] </ref> to generate an instrumented version of the code, and then piping the resulting trace into our detailed cache simulator. Our simulator makes the simplifying assumption that all instructions execute in a single cycle and that all instructions hit in the primary instruction cache.
Reference: [75] <author> M. D. Smith. </author> <title> Support for Speculative Execution in High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Control-flow feedback (also known as "branch profiling") records how frequently each control path in the program is executed, thereby measuring branch outcome frequencies. Such information is often used in aggressive instruction-scheduling algorithms where it is important to schedule code beyond branches <ref> [75] </ref>. Control-flow feedback would be useful in our prefetching compiler algorithm (described in Chapter 2) when computing the localized iteration space for loop nests that include symbolic loop bounds. <p> As this occurs, the cost of even the current level of instruction overhead will diminish relative to the latency-hiding benefit of each useful prefetch. The second important trend is continued improvements in the ability of processors to exploit instruction-level parallelism through techniques such as superscalar processing <ref> [75] </ref>. Since prefetch instructions can always be executed in parallel with other operations (because no other operations depend upon their completion), they should benefit well from the exploitation of instruction-level parallelism. Therefore the absolute overhead of processing prefetch instructions is likely to decrease.
Reference: [76] <author> L. Soule and A. Gupta. </author> <title> Parallel Distributed-Time Logic Simulation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 6(6) </volume> <pages> 32-48, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Due to symmetry, each processor only computes the interaction between a molecule it owns and half the other molecules. We run WATER with 512 molecules through 2 time steps. PTHOR <ref> [76] </ref> is a parallel logic simulator based on the Chandy-Misra simulation algorithm. Unlike centralized-time algorithms, this algorithm does not rely on a single global time during simulation. The primary data structures associated with the simulator are the logic elements (e.g.
Reference: [77] <author> SPEC. </author> <title> The SPEC Benchmark Report. </title> <publisher> Waterside Associates, </publisher> <address> Fremont, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: The performance of the benchmarks was simulated by instrumenting the MIPS object code using pixie [74] and piping the resulting trace into our detailed cache simulator. due to memory accesses for 13 uniprocessor programs taken from the SPEC <ref> [77] </ref>, SPLASH [72], and NAS Parallel [8] benchmark suites. Many of the programs spend a significant amount of time on memory accesses. In fact, 8 out of the 13 programs spend more than half of their time stalled for memory accesses. <p> This collection includes NASA7 and TOMCATV from the SPEC benchmarks <ref> [77] </ref>, OCEAN-a uniprocessor version of a SPLASH benchmark [72], and CG (conjugate gradient), EP ("embarassingly parallel"|a Monte Carlo simulation), IS (integer sort), MG (multigrid) from the NAS Parallel Benchmarks [8].
Reference: [78] <author> G. L. Steele. </author> <title> Proposal for alignment and distribution directives in HPF. Draft presented at HPF Forum meeting, </title> <month> June </month> <year> 1992. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [79] <author> P. Stenstrom, F. Dahlgren, and L. Lundberg. </author> <title> A lockup-free multiprocessor cache design. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 246-250, </pages> <year> 1991. </year>
Reference-contexts: The majority of subsequent lockup-free cache proposals have been a variation of this original MSHR scheme <ref> [63, 70, 79, 46] </ref>. An alternative approach is to maintain the state of outstanding misses in the cache tag array itself [17, 52], thereby permitting a larger number of outstanding misses.
Reference: [80] <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: The prefetching algorithm is implemented in the SUIF (Stanford University Intermediate Form) compiler, which includes many of the standard optimizations and generates code competitive with the MIPS 2.10 compiler <ref> [80] </ref>. Using this compiler system, we have been able to generate fully functional and optimized code with prefetching. By simulating the code with a detailed architectural model, we can evaluate the effect of prefetching on overall system performance. <p> other data. (This is a source-level representation of the actual code generated by our compiler for this case). 2.5.2 Implementation Experience We implemented our prefetching algorithm in the SUIF (Stanford University Intermediate Form) compiler, which includes many of the standard optimizations and generates code competitive with the MIPS 2.10 compiler <ref> [80] </ref>. The experience of actually implementing our algorithm raised some interesting issues. The first issue is when prefetching should occur relative to the other optimizations performed by the compiler.
Reference: [81] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: However, increasing the cache line size is not the most effective form of prefetching, since memory bandwidth is wasted whenever useless data is brought into the cache [64]. In addition, long cache lines can aggravate miss rates in shared-memory multiprocessors by causing unnecessary amounts of false sharing <ref> [19, 81] </ref>. As we have seen already in Figures 1.2 and 1.3, a significant amount of latency remains despite the prefetching benefit of multi-word cache lines. Another form of prefetching could occur with non-blocking loads [66]. <p> In addition, while it may be tractable to understand when particular data items are shared among processors (i.e. true sharing), it is more difficult to predict the coherence misses that only occur because separate items fall within the same cache line (i.e. false sharing) <ref> [19, 81] </ref>. Finally, when the compiler is dealing with explicitly-parallelized programs, as is the case in our experiments 1 , it is difficult (if not impossible) for the compiler to extract the communication patterns, since much of this semantic information is contained only in the programmer's head.
Reference: [82] <author> P.-S. Tseng. </author> <title> A Parallelizing Compiler for Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
Reference: [83] <author> D. M. Tullsen and S. J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Because of the difficulty of prefetching highly irregular access patterns and the relatively small expected gains, we chose not to focus on this class of applications during this dissertation. Tullsen and Eggers <ref> [83] </ref> post-processed reference traces to evaluate the performance of an "oracle" prefetching scheme on a bus-based multiprocessor architecture with limited bandwidth. They observed that if an application is already bandwidth-limited, prefetching cannot improve performance.
Reference: [84] <author> W.-D. Weber. </author> <title> Scalable Directories for Cache-Coherent Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1993. </year> <note> Bibliography 202 </note>
Reference-contexts: Therefore the question is how to scale the machine parameters so as to get realistic performance estimates. A thorough examination of this question has been presented by Weber <ref> [84] </ref>. Weber uses variational analysis (i.e. observing the effects of varying cache size and problem size parameters on performance) and application-specific knowledge to choose appropriate cache sizes given "smaller-than-real" problem sizes for the SPLASH applications. This analysis provides the basis for our own decisions on how to scale the caches.
Reference: [85] <author> W.-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The advantages of software-controlled prefetching are that only a small amount of hardware support is necessary, and a broader class of reference patterns can be covered than simply constant stride accesses (e.g., indirect references, such as in sparse-matrix code). 1.2.5 Multithreading be used to tolerate latency <ref> [3, 36, 44, 85] </ref>. In this figure, context #1 suffers a cache miss when it attempts to load location A. At this time, context #1 is swapped out and the processor begins to execute context #2. <p> While prefetching is one technique for hiding read latency, another Chapter 5. Architectural Issues 183 technique is for the processor to support multiple hardware contexts <ref> [3, 36, 39, 73, 85] </ref> (also known as multithreading). As we mentioned earlier in Section 1.2.5, multithreading has two advantages over prefetching. First, it can handle arbitrarily complex access patterns|even cases where it is impossible to predict the accesses ahead of time (and therefore prefetching cannot succeed). <p> However, the number of contexts is constrained by hardware costs and available parallelism in the application. Previous studies have shown that given processor caches, the interval between long-latency operations (i.e. cache misses) becomes fairly large, allowing just a handful of contexts to hide most of the latency <ref> [85] </ref>. The second factor is the context switch overhead. If the overhead is a sizable fraction of the typical run lengths (time between misses), a significant fraction of time may be wasted switching contexts. Shorter context switch times, however, require a more complex processor.
Reference: [86] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: We will later insert prefetches such that a reference is prefetched whenever its prefetch predicate is true. These first two substeps have been adapted from Wolf and Lam's data locality optimizing algorithm <ref> [86, 87] </ref>, while the third substep is unique to prefetching. Although analyzing data locality is essential for both locality optimizations and prefetching, the differing goals of the two techniques lead to differences in how the analysis is applied. <p> Therefore if we had an infinitely large cache, which would retain data perfectly, reuse would be equivalent to locality. Our reuse analysis step is nearly identical to that proposed by Wolf and Lam <ref> [86, 87] </ref>, which we will briefly summarize in this subsection. Our terminology is slightly different, however, since we tailor our reuse categories to more closely match our prefetching algorithm.
Reference: [87] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: One important example of a locality-improving transformation is blocking (also known as tiling) <ref> [1, 22, 23, 30, 60, 64, 87] </ref>, which works as follows. Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. <p> Rather than operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. Other useful transformations include unimodular loop transforms such as interchange, skewing and reversal <ref> [87] </ref>. Since these optimizations improve the code's data locality, they not only reduce the effective memory access time but also reduce the memory bandwidth requirement. For multiprocessors, the concept of data locality can be extended to minimize not only accesses to main memory, but also communication between processors. <p> We will later insert prefetches such that a reference is prefetched whenever its prefetch predicate is true. These first two substeps have been adapted from Wolf and Lam's data locality optimizing algorithm <ref> [86, 87] </ref>, while the third substep is unique to prefetching. Although analyzing data locality is essential for both locality optimizations and prefetching, the differing goals of the two techniques lead to differences in how the analysis is applied. <p> Therefore if we had an infinitely large cache, which would retain data perfectly, reuse would be equivalent to locality. Our reuse analysis step is nearly identical to that proposed by Wolf and Lam <ref> [86, 87] </ref>, which we will briefly summarize in this subsection. Our terminology is slightly different, however, since we tailor our reuse categories to more closely match our prefetching algorithm. <p> We represent the shape of the set of iterations that use the same data by a reuse vector space <ref> [87] </ref>. The remainder of this subsection describes how this mathematical representation is used to compute temporal, spatial, and group reuse. 2 "Memory line" refers to cache-line-sized blocks of contiguous memory, which are the unique elements that can be mapped into cache entries. Chapter 2. <p> Rather than trying to represent exactly which reuses would result in cache hits, we adopt Wolf and Lam's approach of capturing only the dimensionality of the iteration space that has data locality <ref> [87] </ref>. We define the localized iteration space to be the set of loops that can exploit reuse. <p> Our compiler can do both things automatically by first applying locality optimizations and then inserting prefetches. We compiled each of the benchmarks with the locality optimizer enabled <ref> [87] </ref>. In two of the cases (GMTRY and VPENTA), there was a significant improvement in locality, and thus performance. Both of these cases are presented in Figure 3.7.
Reference: [88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The most popular approach to this complex optimization problem is for the programmer to explicitly specify the data decomposition through directives in the programming language <ref> [11, 38, 43, 67, 78, 82, 88] </ref>, while the compiler is responsible for decomposing the computation. Another approach is for the compiler to determine both the data and computation decompositions [4, 5, 10, 12, 34, 35, 56].
References-found: 88

