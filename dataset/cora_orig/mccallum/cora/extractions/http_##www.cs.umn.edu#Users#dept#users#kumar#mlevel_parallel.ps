URL: http://www.cs.umn.edu/Users/dept/users/kumar/mlevel_parallel.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: fkarypis, kumarg@cs.umn.edu  
Title: Parallel Algorithm for Multilevel Graph Partitioning and Sparse Matrix Ordering  
Author: George Karypis and Vipin Kumar 
Date: Last updated on October 9, 1997  
Address: Minneapolis, MN 55455,  
Affiliation: University of Minnesota, Department of Computer Science/ Army HPC Research Center  at 5:08pm  
Note: A  
Pubnum: Technical Report: 95-036  
Abstract: Appears in the Journal of Parallel and Distributed Computing A short version of this paper appears in International Parallel Processing Symposium 1996 The serial algorithms described in this paper are implemented by the `METIS: Unstructured Graph Partitioning and Sparse Matrix Ordering System'. METIS is available on WWW at URL: http://www.cs.umn.edu/karypis/metis Abstract In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. A key feature of our parallel formulation (that distinguishes it from other proposed parallel formulations of multilevel algorithms is that it partitions the vertices of the graph into p p parts while distributing the overall adjacency matrix of the graph among all p processors. This mapping results in substantially smaller communication than one-dimensional distribution for graphs with relatively high degree, especially if the graph is randomly distributed among the processors. We also present a parallel algorithm for computing a minimal cover of a bipartite graph which is a key operation for obtaining a small vertex separator that is useful for computing the fill reducing ordering of sparse matrices. Our parallel algorithm achieves a speedup of up to 56 on 128 processors for moderate size problems, further reducing the already moderate serial run time of multilevel schemes. Furthermore, the quality of the produced partitions and orderings are comparable to those produced by the serial multilevel algorithm that has been shown to outperform both spectral partitioning and multiple minimum degree. fl This work was supported by NSF CCR-9423082, by the Army Research Office contract DA/DAAH04-95-1-0538, by the IBM Partnership Award, and by Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute, Cray Research Inc, and by the Pittsburgh Supercomputing Center. Related papers are available via WWW at URL: http://www.cs.umn.edu/karypis 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Stephen T. Barnard. Pmrsb: </author> <title> Parallel multilevel recursive spectral bisection. </title> <booktitle> In Supercomputing 1995, </booktitle> <year> 1995. </year>
Reference-contexts: In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. A key feature of our parallel formulation (that distinguishes it from other proposed parallel formulations of multilevel algorithms <ref> [2, 1, 24, 14] </ref>) is that it partitions the vertices of the graph into p p parts while distributing the overall adjacency matrix of the graph among all p processors. <p> Due to the two-dimensional mapping scheme used in the parallel formulation, its asymptotic speedup is limited to O. p/ because the matching operation is performed only on the diagonal processors. In contrast, for one-dimensional mapping scheme used in <ref> [24, 1, 14] </ref>, the asymptotic speedup can be O. p/ for large enough graphs. However, this two-dimensional mapping has the following advantages. First, the actual speedup on graphs with large average degrees is quite good as shown in Figure 8.
Reference: [2] <author> Stephen T. Barnard and Horst Simon. </author> <title> A parallel implementation of multilevel recursive spectral bisection for application to adaptive unstructured meshes. </title> <booktitle> In Proceedings of the seventh SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 627-632, </pages> <year> 1995. </year>
Reference-contexts: In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. A key feature of our parallel formulation (that distinguishes it from other proposed parallel formulations of multilevel algorithms <ref> [2, 1, 24, 14] </ref>) is that it partitions the vertices of the graph into p p parts while distributing the overall adjacency matrix of the graph among all p processors.
Reference: [3] <author> Stephen T. Barnard and Horst D. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <booktitle> In Proceedings of the sixth SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 711-718, </pages> <year> 1993. </year>
Reference: [4] <author> T. Bui and C. Jones. </author> <title> A heuristic for reducing fill in sparse matrix factorization. </title> <booktitle> In 6th SIAM Conf. Parallel Processing for Scientific Computing, </booktitle> <pages> pages 445-452, </pages> <year> 1993. </year>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Recently, a new class of multilevel graph partitioning techniques was introduced by Bui & Jones <ref> [4] </ref> and Hendrickson & Leland [12], and further studied by Karypis & Kumar [16, 15, 13]. These multilevel schemes provide excellent graph partitionings and have moderate computational complexity. <p> The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms <ref> [4, 12, 7, 22] </ref>. The rest of the paper is organized as follows. Section 2 surveys the different types of graph partitioning algorithms that are widely used today. <p> The graph G D .V ; E/ is first coarsened down to a few thousand vertices (coarsening phase), a bisection of this much smaller graph is computed (initial partitioning phase), and then this partition is projected back towards the original graph (uncoarsening phase), by periodically refining the partition <ref> [4, 12, 16] </ref>. Since the finer graph has more degrees of freedom, such refinements improve the quality of the partitions. This process, is graphically illustrated in Figure 1. <p> Graph G lC1 is constructed from G l by finding a maximal matching M l E l of G l and collapsing together the vertices that are incident on each edge of the matching. Maximal matchings can be computed in different ways <ref> [4, 12, 16, 15] </ref>. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme [4, 12]. <p> Maximal matchings can be computed in different ways [4, 12, 16, 15]. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme <ref> [4, 12] </ref>. In this scheme vertices are visited in random order, and for each unmatched vertex we randomly match it with one of its unmatched neighbors. An alternative matching scheme that we have found to be quite effective is called heavy-edge matching (HEM) [16, 13].
Reference: [5] <author> Pedro Diniz, Steve Plimpton, Bruce Hendrickson, and Robert Leland. </author> <title> Parallel algorithms for dynamically partitioning unstructured grids. </title> <booktitle> In Proceedings of the seventh SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 615-620, </pages> <year> 1995. </year>
Reference-contexts: The key idea behind our parallel refinement algorithm is to select a group of vertices to swap from one part to the other instead of selecting a single vertex. Refinement schemes that use similar ideas are described in <ref> [26, 5] </ref>;. However, our algorithm differs in two important ways from the other schemes: (i) it uses a different method for selecting vertices; (ii) it uses a two-dimensional partition to minimize communication. Consider a p p p processor grid on which graph G D .V ; E/ is distributed.
Reference: [6] <author> C. M. Fiduccia and R. M. Mattheyses. </author> <title> A linear time heuristic for improving network partitions. </title> <booktitle> In In Proc. 19th IEEE Design Automation Conference, </booktitle> <pages> pages 175-181, </pages> <year> 1982. </year>
Reference-contexts: A class of local refinement algorithms that tend to produce very good results are those that are based on the Kernighan-Lin (KL) partitioning algorithm [18] and their variants (FM) <ref> [6, 12, 16] </ref>. 3 Parallel Multilevel Graph Partitioning Algorithm There are two types of parallelism that can be exploited in the p-way graph partitioning algorithm based on the multilevel bisection described in Section 2. The first type of parallelism is due to the recursive nature of the algorithm.
Reference: [7] <author> J. Garbers, H. J. Promel, and A. Steger. </author> <title> Finding clusters in VLSI circuits. </title> <booktitle> In Proceedings of IEEE International Conference on Computer Aided Design, </booktitle> <pages> pages 520-523, </pages> <year> 1990. </year>
Reference-contexts: The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms <ref> [4, 12, 7, 22] </ref>. The rest of the paper is organized as follows. Section 2 surveys the different types of graph partitioning algorithms that are widely used today.
Reference: [8] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase <ref> [19, 8] </ref>. The multiple minimum degree ordering used almost exclusively in serial direct methods is not suitable for parallel direct methods, as it provides limited concurrency in the parallel factorization phase. The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. <p> Thus, the problem of performing a p-way partition is reduced to that of performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition [27, 15], it is used extensively due to its simplicity <ref> [8, 10] </ref>. The basic structure of the multilevel bisection algorithm is very simple.
Reference: [9] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(5) </volume> <pages> 502-520, </pages> <month> May </month> <year> 1997. </year> <note> Available on WWW at URL http://www.cs.umn.edu/karypis. </note>
Reference-contexts: A parallel graph partitioning algorithm can take advantage of the significantly higher amount of memory available in parallel computers. Furthermore, with recent development of highly parallel formulations of sparse Cholesky factorization algorithms <ref> [9, 17, 25] </ref>, numeric factorization on parallel computers can take much less time than the step for computing a fill-reducing ordering on a serial computer. <p> In particular, the isoefficiency [19] for 2D finite element graphs is O. p 1:5 log 3 p/, and for 3D finite element graphs is O. p log 2 p/. We have recently developed a highly parallel sparse direct factorization algorithm <ref> [17, 9] </ref>. the isoefficiency of this algorithm is O. p 1:5 / for both 2D and 3D finite element graphs.
Reference: [10] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: Thus, the problem of performing a p-way partition is reduced to that of performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition [27, 15], it is used extensively due to its simplicity <ref> [8, 10] </ref>. The basic structure of the multilevel bisection algorithm is very simple.
Reference: [11] <author> Bruce Hendrickson and Robert Leland. </author> <title> The chaco user's guide, version 1.0. </title> <type> Technical Report SAND93-2339, </type> <institution> Sandia National Laboratories, </institution> <year> 1993. </year>
Reference-contexts: However, even for this problem, when larger partitions are considered, the relative difference in the edge-cut decreases; and for the of 128-way partition, parallel multilevel does slightly better than the serial multilevel. with small edge-cuts. We used the Chaco <ref> [11] </ref> graph partitioning package to produce the MSB partitions. As before, any bars above the baseline indicate that the parallel algorithm generates partitions with higher edge-cuts. From this figure we see that the quality of the parallel algorithm is almost never worse than that of the MSB algorithm.
Reference: [12] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <type> Technical Report SAND93-1301, </type> <institution> Sandia National Laboratories, </institution> <year> 1993. </year>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Recently, a new class of multilevel graph partitioning techniques was introduced by Bui & Jones [4] and Hendrickson & Leland <ref> [12] </ref>, and further studied by Karypis & Kumar [16, 15, 13]. These multilevel schemes provide excellent graph partitionings and have moderate computational complexity. Even though these multilevel algorithms are quite fast compared with spectral methods, parallel formulations of multilevel partitioning algorithms are needed for the following reasons. <p> The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms <ref> [4, 12, 7, 22] </ref>. The rest of the paper is organized as follows. Section 2 surveys the different types of graph partitioning algorithms that are widely used today. <p> The graph G D .V ; E/ is first coarsened down to a few thousand vertices (coarsening phase), a bisection of this much smaller graph is computed (initial partitioning phase), and then this partition is projected back towards the original graph (uncoarsening phase), by periodically refining the partition <ref> [4, 12, 16] </ref>. Since the finer graph has more degrees of freedom, such refinements improve the quality of the partitions. This process, is graphically illustrated in Figure 1. <p> Graph G lC1 is constructed from G l by finding a maximal matching M l E l of G l and collapsing together the vertices that are incident on each edge of the matching. Maximal matchings can be computed in different ways <ref> [4, 12, 16, 15] </ref>. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme [4, 12]. <p> Maximal matchings can be computed in different ways [4, 12, 16, 15]. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme <ref> [4, 12] </ref>. In this scheme vertices are visited in random order, and for each unmatched vertex we randomly match it with one of its unmatched neighbors. An alternative matching scheme that we have found to be quite effective is called heavy-edge matching (HEM) [16, 13]. <p> A class of local refinement algorithms that tend to produce very good results are those that are based on the Kernighan-Lin (KL) partitioning algorithm [18] and their variants (FM) <ref> [6, 12, 16] </ref>. 3 Parallel Multilevel Graph Partitioning Algorithm There are two types of parallelism that can be exploited in the p-way graph partitioning algorithm based on the multilevel bisection described in Section 2. The first type of parallelism is due to the recursive nature of the algorithm.
Reference: [13] <author> G. Karypis and V. Kumar. </author> <title> Analysis of multilevel graph partitioning. </title> <type> Technical Report TR 95-037, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year> <note> Also available on WWW at URL http://www.cs.umn.edu/karypis. A short version appears in Supercomputing 95. 20 </note>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Recently, a new class of multilevel graph partitioning techniques was introduced by Bui & Jones [4] and Hendrickson & Leland [12], and further studied by Karypis & Kumar <ref> [16, 15, 13] </ref>. These multilevel schemes provide excellent graph partitionings and have moderate computational complexity. Even though these multilevel algorithms are quite fast compared with spectral methods, parallel formulations of multilevel partitioning algorithms are needed for the following reasons. <p> In this scheme vertices are visited in random order, and for each unmatched vertex we randomly match it with one of its unmatched neighbors. An alternative matching scheme that we have found to be quite effective is called heavy-edge matching (HEM) <ref> [16, 13] </ref>. The HEM matching scheme computes a matching M l , such that the weight of the edges in M l is high. The heavy-edge matching is computed using a randomized algorithm as follows. The vertices are again visited in random order.
Reference: [14] <author> G. Karypis and V. Kumar. </author> <title> Parallel multilevel k-way partitioning scheme for irregular graphs. </title> <type> Technical Report TR 96-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1996. </year> <note> Also available on WWW at URL http://www.cs.umn.edu/karypis. A short version appears in Supercomputing 96. </note>
Reference-contexts: In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. A key feature of our parallel formulation (that distinguishes it from other proposed parallel formulations of multilevel algorithms <ref> [2, 1, 24, 14] </ref>) is that it partitions the vertices of the graph into p p parts while distributing the overall adjacency matrix of the graph among all p processors. <p> Due to the two-dimensional mapping scheme used in the parallel formulation, its asymptotic speedup is limited to O. p/ because the matching operation is performed only on the diagonal processors. In contrast, for one-dimensional mapping scheme used in <ref> [24, 1, 14] </ref>, the asymptotic speedup can be O. p/ for large enough graphs. However, this two-dimensional mapping has the following advantages. First, the actual speedup on graphs with large average degrees is quite good as shown in Figure 8.
Reference: [15] <author> G. Karypis and V. Kumar. </author> <title> Multilevel k-way partitioning scheme for irregular graphs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> Accepted for publication, 1997. Also available on WWW at URL http://www.cs.umn.edu/karypis. </note>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Recently, a new class of multilevel graph partitioning techniques was introduced by Bui & Jones [4] and Hendrickson & Leland [12], and further studied by Karypis & Kumar <ref> [16, 15, 13] </ref>. These multilevel schemes provide excellent graph partitionings and have moderate computational complexity. Even though these multilevel algorithms are quite fast compared with spectral methods, parallel formulations of multilevel partitioning algorithms are needed for the following reasons. <p> After log p phases, graph G is partitioned into p parts. Thus, the problem of performing a p-way partition is reduced to that of performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition <ref> [27, 15] </ref>, it is used extensively due to its simplicity [8, 10]. The basic structure of the multilevel bisection algorithm is very simple. <p> Graph G lC1 is constructed from G l by finding a maximal matching M l E l of G l and collapsing together the vertices that are incident on each edge of the matching. Maximal matchings can be computed in different ways <ref> [4, 12, 16, 15] </ref>. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme [4, 12].
Reference: [16] <author> G. Karypis and V. Kumar. </author> <title> A fast and highly quality multilevel scheme for partitioning irregular graphs. </title> <journal> SIAM Journal on Scientific Computing, </journal> <note> to appear. Also available on WWW at URL http://www.cs.umn.edu/karypis. A short version appears in Intl. Conf. on Parallel Processing 1995. </note>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Recently, a new class of multilevel graph partitioning techniques was introduced by Bui & Jones [4] and Hendrickson & Leland [12], and further studied by Karypis & Kumar <ref> [16, 15, 13] </ref>. These multilevel schemes provide excellent graph partitionings and have moderate computational complexity. Even though these multilevel algorithms are quite fast compared with spectral methods, parallel formulations of multilevel partitioning algorithms are needed for the following reasons. <p> Furthermore, the quality of the produced partitions and orderings are comparable to those produced by the serial multilevel algorithm that has been shown to outperform both spectral partitioning and multiple minimum degree <ref> [16] </ref>. The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms [4, 12, 7, 22]. <p> of the produced partitions and orderings are comparable to those produced by the serial multilevel algorithm that has been shown to outperform both spectral partitioning and multiple minimum degree <ref> [16] </ref>. The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms [4, 12, 7, 22]. The rest of the paper is organized as follows. Section 2 surveys the different types of graph partitioning algorithms that are widely used today. <p> The graph G D .V ; E/ is first coarsened down to a few thousand vertices (coarsening phase), a bisection of this much smaller graph is computed (initial partitioning phase), and then this partition is projected back towards the original graph (uncoarsening phase), by periodically refining the partition <ref> [4, 12, 16] </ref>. Since the finer graph has more degrees of freedom, such refinements improve the quality of the partitions. This process, is graphically illustrated in Figure 1. <p> Graph G lC1 is constructed from G l by finding a maximal matching M l E l of G l and collapsing together the vertices that are incident on each edge of the matching. Maximal matchings can be computed in different ways <ref> [4, 12, 16, 15] </ref>. The method used to compute the matching greatly affects both the quality of the partitioning, and the time required during the uncoarsening phase. One simple scheme for computing a matching is the random matching (RM) scheme [4, 12]. <p> In this scheme vertices are visited in random order, and for each unmatched vertex we randomly match it with one of its unmatched neighbors. An alternative matching scheme that we have found to be quite effective is called heavy-edge matching (HEM) <ref> [16, 13] </ref>. The HEM matching scheme computes a matching M l , such that the weight of the edges in M l is high. The heavy-edge matching is computed using a randomized algorithm as follows. The vertices are again visited in random order. <p> A class of local refinement algorithms that tend to produce very good results are those that are based on the Kernighan-Lin (KL) partitioning algorithm [18] and their variants (FM) <ref> [6, 12, 16] </ref>. 3 Parallel Multilevel Graph Partitioning Algorithm There are two types of parallelism that can be exploited in the p-way graph partitioning algorithm based on the multilevel bisection described in Section 2. The first type of parallelism is due to the recursive nature of the algorithm. <p> We use the GGGP algorithm described in <ref> [16] </ref> to partition the coarsest graph. We perform a small number of GGGP runs starting from different random vertices and the one with the smaller edge-cut is selected as the partition. <p> Here we describe our parallel implementation of the refinement step. For refining the coarser graphs that reside on a single processor, we use the boundary Kernighan-Lin refinement algorithm (BKLR) described in <ref> [16] </ref>. However, the BKLR algorithm is sequential in nature and it cannot be used in its current form to efficiently refine a partition when the graph is distributed among a grid of processors. <p> This figure (along with Figure 6) also indicates that our serial multilevel algorithm outperforms the MSB algorithm. An extensive comparison between our serial multilevel algorithm and MSB, can be found in <ref> [16] </ref>. Tables 2 and 3 also show the run time of the parallel algorithm and the serial algorithm, respectively. A number of conclusions can be drawn from these results. First, as p increases, the time required for the p-way partition on p 15 processors decreases.
Reference: [17] <author> George Karypis and Vipin Kumar. </author> <title> Fast sparse Cholesky factorization on scalable parallel computers. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A short version appears in the Eighth Symposium on the Frontiers of Massively Parallel Computation, 1995. Available on WWW at URL http://www.cs.umn.edu/karypis. </note>
Reference-contexts: A parallel graph partitioning algorithm can take advantage of the significantly higher amount of memory available in parallel computers. Furthermore, with recent development of highly parallel formulations of sparse Cholesky factorization algorithms <ref> [9, 17, 25] </ref>, numeric factorization on parallel computers can take much less time than the step for computing a fill-reducing ordering on a serial computer. <p> For example, on a 1024-processor Cray T3D, some matrices can be factored in a few seconds using our parallel sparse Cholesky factorization algorithm <ref> [17] </ref>, but serial graph partitioning (required for ordering) takes several minutes for these problems. In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. <p> In particular, the isoefficiency [19] for 2D finite element graphs is O. p 1:5 log 3 p/, and for 3D finite element graphs is O. p log 2 p/. We have recently developed a highly parallel sparse direct factorization algorithm <ref> [17, 9] </ref>. the isoefficiency of this algorithm is O. p 1:5 / for both 2D and 3D finite element graphs.
Reference: [18] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <year> 1970. </year>
Reference-contexts: A class of local refinement algorithms that tend to produce very good results are those that are based on the Kernighan-Lin (KL) partitioning algorithm <ref> [18] </ref> and their variants (FM) [6, 12, 16]. 3 Parallel Multilevel Graph Partitioning Algorithm There are two types of parallelism that can be exploited in the p-way graph partitioning algorithm based on the multilevel bisection described in Section 2.
Reference: [19] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: A key step in each iteration of these methods is the multiplication of a sparse matrix and a (dense) vector. Partitioning the graph that corresponds to matrix A, is used to significantly reduce the amount of communication <ref> [19] </ref>. If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase [19, 8]. <p> If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase <ref> [19, 8] </ref>. The multiple minimum degree ordering used almost exclusively in serial direct methods is not suitable for parallel direct methods, as it provides limited concurrency in the parallel factorization phase. The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. <p> The vertices of the graph G 0 D .V 0 ; E 0 / are distributed among this processor grid using a cyclic mapping <ref> [19] </ref>. The vertices V 0 are partitioned into p p subsets, V 0 0 ; V 1 p 0 . Processor P i; j stores the edges of E 0 between the subsets of vertices V i 0 and V 0 . <p> Since the size of these vectors are much larger than p p, this broadcast can be performed in time linear to the message size, by performing a one-to-all personalized broadcast followed by an all-to-all broadcast (Problem 3.24 in <ref> [19] </ref>). <p> The communication overhead of parallel ordering over all p processors is O.n p p log p/, which can be subsumed by the serial complexity of Cholesky factorization provided n is large enough relative to p. In particular, the isoefficiency <ref> [19] </ref> for 2D finite element graphs is O. p 1:5 log 3 p/, and for 3D finite element graphs is O. p log 2 p/.
Reference: [20] <author> J. W.-H. Liu. </author> <title> Modification of the minimum degree algorithm by multiple elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 </volume> <pages> 141-153, </pages> <year> 1985. </year>
Reference-contexts: On p processors, the ordering is computed by using nested dissection 16 for the first log p levels, and then multiple minimum degree <ref> [20] </ref> (MMD) is used to order the submatrices stored locally on each processor.
Reference: [21] <author> Michael Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: Similar problems arise when trying to parallelize algorithms based on depth-first traversal of the graph. Another possibility is to adapt some of the algorithms that have been developed for the PRAM model. In particular the algorithm of Luby <ref> [21] </ref> for computing the maximal independent set can be used to find a matching. However, parallel formulations of this type of algorithms also have high communication overhead because each processor p i needs to communicate with all other processors that contain neighbors of the nodes local at p i .
Reference: [22] <author> R. Ponnusamy, N. Mansour, A. Choudhary, and G. C. Fox. </author> <title> Graph contraction and physical optimization methods: a quality-cost tradeoff for mapping data on parallel computers. </title> <booktitle> In International Conference of Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: The parallel formulation in this paper is described in the context of the serial multilevel graph partitioning algorithm presented in [16]. However, nearly all of the discussion in this paper is applicable to other multilevel graph partitioning algorithms <ref> [4, 12, 7, 22] </ref>. The rest of the paper is organized as follows. Section 2 surveys the different types of graph partitioning algorithms that are widely used today.
Reference: [23] <author> A. Pothen and C-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: A boundary induced separator can be easily constructed by simply choosing the smaller of A and B. However, a different separator can be constructed using a minimum cover algorithm for bipartite graphs <ref> [23] </ref> that contains subsets of vertices from both A and B. In many cases, this new separator S may have 20% to 40% fewer vertices than either A or B. <p> Since, the size of the vertex separator directly affects the fill and thus, the time required to factor the matrix, small separators are extremely important. The worst-case complexity of the minimum cover algorithm is O.j A [ Bj 2 / <ref> [23] </ref>. However, A [ B can be fairly large (O.jV j 2=3 /) for three-dimensional finite element graphs; hence, this step needs to be performed in parallel.
Reference: [24] <author> Padma Raghavan. </author> <title> Parallel ordering using edge contraction. </title> <type> Technical Report CS-95-293, </type> <institution> Department of Computer Science, University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: In this paper we present a parallel formulation of the multilevel graph partitioning and sparse matrix ordering algorithm. A key feature of our parallel formulation (that distinguishes it from other proposed parallel formulations of multilevel algorithms <ref> [2, 1, 24, 14] </ref>) is that it partitions the vertices of the graph into p p parts while distributing the overall adjacency matrix of the graph among all p processors. <p> Due to the two-dimensional mapping scheme used in the parallel formulation, its asymptotic speedup is limited to O. p/ because the matching operation is performed only on the diagonal processors. In contrast, for one-dimensional mapping scheme used in <ref> [24, 1, 14] </ref>, the asymptotic speedup can be O. p/ for large enough graphs. However, this two-dimensional mapping has the following advantages. First, the actual speedup on graphs with large average degrees is quite good as shown in Figure 8.
Reference: [25] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: A parallel graph partitioning algorithm can take advantage of the significantly higher amount of memory available in parallel computers. Furthermore, with recent development of highly parallel formulations of sparse Cholesky factorization algorithms <ref> [9, 17, 25] </ref>, numeric factorization on parallel computers can take much less time than the step for computing a fill-reducing ordering on a serial computer.
Reference: [26] <author> J. E. Savage and M. G. Wloka. </author> <title> Parallelism in graph partitioning. </title> <journal> J. Par. Dist. Computing, </journal> <volume> 13 </volume> <pages> 257-272, </pages> <year> 1991. </year>
Reference-contexts: The key idea behind our parallel refinement algorithm is to select a group of vertices to swap from one part to the other instead of selecting a single vertex. Refinement schemes that use similar ideas are described in <ref> [26, 5] </ref>;. However, our algorithm differs in two important ways from the other schemes: (i) it uses a different method for selecting vertices; (ii) it uses a two-dimensional partition to minimize communication. Consider a p p p processor grid on which graph G D .V ; E/ is distributed.
Reference: [27] <author> Horst D. Simon and Shang-Hua Teng. </author> <title> How good is recursive bisection? Technical Report RNR-93-012, NAS Systems Division, </title> <booktitle> Moffet Field, </booktitle> <address> CA, </address> <year> 1993. </year>
Reference-contexts: After log p phases, graph G is partitioned into p parts. Thus, the problem of performing a p-way partition is reduced to that of performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition <ref> [27, 15] </ref>, it is used extensively due to its simplicity [8, 10]. The basic structure of the multilevel bisection algorithm is very simple.
References-found: 27

