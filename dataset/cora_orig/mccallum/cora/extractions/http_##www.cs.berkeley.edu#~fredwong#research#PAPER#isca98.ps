URL: http://www.cs.berkeley.edu/~fredwong/research/PAPER/isca98.ps
Refering-URL: http://www.cs.berkeley.edu/~fredwong/cs252/index.html
Root-URL: http://www.cs.berkeley.edu
Title: Architectural Requirements and Scalability of the NAS Parallel Benchmarks  
Author: Frederick C. Wong, Richard P. Martin, Remzi H. Arpaci-Dusseau and David E. Culler 
Date: January 14, 1998  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: We present a study of the architectural requirements and scalability of the NAS Parallel Benchmarks. We find that many complex factors affect performance. Of these, the increase in computational efficiency, a direct result of decreasing working sets and increasing total cache size, is the most significant. The resulting improvement in computational efficiency can serve to offset the costs of communication and synchronization, hence yielding linear or even super-linear speedups. Communication performance is often much lower than that would be expected from micro-benchmarks, as a result of queuing effects in the both the software and hardware. We also find that network bisection bandwidth is the limiting factor for two of the benchmarks because of their all-to-all communication patterns. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gheith Abandah. </author> <title> Characterizing Shared-Memory Applications: A Case Study of the NAS Parallel Benchmarks. </title> <type> Technical Report HPL-97-24, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: The applications in [8] have a much wider range of communication to computation ratios than we found in the NPB. 19 A study of the NPB 1.0 suite for the Convex SPP <ref> [1] </ref> shows similar computational characteristics of those studied here. The Convex suite had a similar high percentage of load/store instructions, and an even lower frequency of branches.
Reference: [2] <institution> Argonne National Laboratory. MPICH-A Portable Implmentation of MPI, </institution> <year> 1997. </year> <note> http://www.mcs.- anl.gov/mpi/mpich. </note>
Reference-contexts: Each has a single Myricom network interface card attached to the S-Bus I/O bus. The machines are interconnected with ten 8-port Myrinet [4] switches in a fat-tree type topology. All the NAS benchmarks communicate via MPI, version 1. Our implementation of MPI is based on the MPICH reference implementation <ref> [2] </ref>. All ADI calls are mapped to Active Messages [20], and the layer is highly tuned for performance. The resulting MPI implementation has a fixed set-up cost of 50 microseconds and a peak bandwidth of 30 MB/s.
Reference: [3] <author> David H. Bailey, T. Harris, Rob Van der Wigngaart, William Saphir, Alex Woo, and Maurice Yarrow. </author> <title> The NAS Parallel Benchmarks 2.0. </title> <type> Technical Report NAS-95-010, </type> <institution> NASA Ames Research Center, </institution> <year> 1995. </year>
Reference-contexts: To date, every vendor of large parallel machines has presented NPB version 1.0 results [9]. With the recent convergence of parallel machines and the introduction of a standard programming model (MPI), the NAS group created version 2.2 of the benchmark suite <ref> [3] </ref>. In contrast to the vendor-specific implementations of version 1.0, NPB 2.2 presents a consistent, portable, and readily available workload to parallel machine designers, analogous to the SPEC benchmarks for single-processor machines. <p> None of the benchmarks performs a significant amount of I/O. In addition, both BT and SP require a square number of processors (e.g. 1,4,9,16,25,36), while the rest of the benchmarks required a power of two (e.g. 1,4,8,16,32) number of processors. More detail on all benchmarks can be found in <ref> [3] </ref>. Both the SGI and UltraSPARC platforms experience an improvement in ComputeEfficiency ; we seek to understand exactly when this benefit takes place, and how significant is its impact. In this work, we present a quantitative study of computation, communication, and synchronization costs under CPS scaling.
Reference: [4] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> MyrinetA Gigabet-per-Second Local-Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-38, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Each has a single Myricom network interface card attached to the S-Bus I/O bus. The machines are interconnected with ten 8-port Myrinet <ref> [4] </ref> switches in a fat-tree type topology. All the NAS benchmarks communicate via MPI, version 1. Our implementation of MPI is based on the MPICH reference implementation [2]. All ADI calls are mapped to Active Messages [20], and the layer is highly tuned for performance.
Reference: [5] <author> Eric Brewer and Bradley C. Kuszmaul. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <volume> volume 8, </volume> <pages> pages 858-67, </pages> <address> Cancun, MX, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Although the average load on the network is low, the large messages sizes and bursty behavior of the NPB result in even small messages experiencing long delays. Better communication scheduling, perhaps with techniques found in <ref> [5] </ref>, may be required to obtain higher efficiency. 5 Related Work The performance of the NPB 2.1 codes is reported in [18] for several different platforms. Although the qualitative conclusions they reach are similar to ours, little quantitative data on the internal structure of the NPB is presented.
Reference: [6] <author> Bob Cmelik and Doug Keppel. Shade: </author> <title> A Fast Instruction-set Simulator for Execution Profiling. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: To solve this problem, we employ the Shade simulation environment <ref> [6] </ref>. Shade provides a SPARC-based virtual machine to an application program, as well as certain kernel functionality required to run most programs. We use Shade in a novel manner, running independent instances of the simulator across the workstations of our cluster.
Reference: [7] <author> David E. Culler, Lok Tin Liu, Richard P. Martin, and Chad O. Yoshikawa. </author> <title> Assessing Fast Network Interfaces. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 35-43, </pages> <month> February </month> <year> 1996. </year> <month> 21 </month>
Reference-contexts: scaling performance for IS and FT. 15 delivered by the MPI layer vs. the total time spent in sends and waiting for each application as a function of the number of processors. 4.3.1 Delivered Efficiency Message layer performance is often quoted as the delivered performance under a few communication micro-benchmarks <ref> [7, 11] </ref>. However, micro-benchmarks measure communication performance under ideal conditions, and we wish to quantify the communication performance under a real workload. Unfortunately, quantifying communication efficiency is difficult. Under a message-passing model, both communication and synchronization costs are intertwined at many levels.
Reference: [8] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Atchitectural Requirements of Parallel Scientific Applications with Explict Communication. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Interestingly, the study found that much smaller grain sizes were acceptable up to a large (1000) number of nodes. This is direct opposition to the results we present here; under problem-constrained scaling, much larger grain sizes are necessary to amortize the overheads of parallelism. Cypher <ref> [8] </ref> describes the characteristics of a set of substantial message passing applications. Qualitatively, the applications are similar to NAS in that they were developed in the context of heavy-weight message-passing libraries, and were highly floating-point intensive. <p> Also, the range of processor studies, into the 100's of nodes, and time-scales (hours) of the applications are quite different from those of the NPB presented here. The applications in <ref> [8] </ref> have a much wider range of communication to computation ratios than we found in the NPB. 19 A study of the NPB 1.0 suite for the Convex SPP [1] shows similar computational characteristics of those studied here.
Reference: [9] <author> Leonardo Dagum David H. Bailey, Eric Barszcz and Horst D. Simon. </author> <title> NAS Parallel Benchmark Results. </title> <type> Technical Report RNR-93-016, </type> <institution> NASA Ames Research Center, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction The NAS Parallel Benchmarks (NPB) are widely used to evaluate parallel machines. To date, every vendor of large parallel machines has presented NPB version 1.0 results <ref> [9] </ref>. With the recent convergence of parallel machines and the introduction of a standard programming model (MPI), the NAS group created version 2.2 of the benchmark suite [3].
Reference: [10] <author> Rob F. Van der Wigngaart. </author> <title> Efficient Implementatin of a 3-Dimensional ADI Method. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 102-111, </pages> <year> 1993. </year>
Reference-contexts: The multi-partition scheme used in BT and SP results in a six-way exchange because each processor owns 'cubes' of the data-set <ref> [10] </ref>. Each processor exchanges boundary conditions along the six faces of its cubes. MG presents a more complex multi-way exchange, but it is clear from the figure that the communication is also balanced. Finally, the even coloring of FT and IS correspond to all-to-all exchanges.
Reference: [11] <author> Jack Dongarra and Tom Dunigan. </author> <title> Message-Passing Performance of Various Computers. </title> <type> Technical Report UT-CS-95-299, </type> <institution> University of Tennessee, Department of Computer Science, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: scaling performance for IS and FT. 15 delivered by the MPI layer vs. the total time spent in sends and waiting for each application as a function of the number of processors. 4.3.1 Delivered Efficiency Message layer performance is often quoted as the delivered performance under a few communication micro-benchmarks <ref> [7, 11] </ref>. However, micro-benchmarks measure communication performance under ideal conditions, and we wish to quantify the communication performance under a real workload. Unfortunately, quantifying communication efficiency is difficult. Under a message-passing model, both communication and synchronization costs are intertwined at many levels.
Reference: [12] <author> Mark Hill. </author> <title> The Dinero Cache Simulator, </title> <month> August </month> <year> 1985. </year> <note> http://www.cs.wisc.edu/ larus/warts.html. </note>
Reference-contexts: For all traces, the benchmarks are run for a single time-step. Other experiments have revealed that behavior across processors and time steps is nearly identical for these benchmarks. After a trace is produced, we use the Dinero cache simulator <ref> [12] </ref> to simulate the desired cache configuration. 3 Algorithmic Characteristics In this section, we describe the baseline characteristics and algorithmic scaling properties of the NAS benchmarks. We find that the NAS codes are well-tuned and mostly floating-point intensive.
Reference: [13] <author> Richard P. Martin, Amin M. Vahdat, David E. Culler, and Thomas E. Anderson. </author> <title> The Effects of Latency, Overhead and Bandwidth in a Cluster Architecture. </title> <booktitle> In Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Within the realm of interest, there is an order of magnitude difference in the average size of a message. Interestingly, the smallest messages are still on the order of 1000 bytes, which is a substantially larger grain than found in many other parallel benchmarks <ref> [13, 21] </ref>. Because of their all-to-all pattern, for both IS and FT , the total number of messages increases as the square of the number of processors, and thus the per-processor message count is linear in the number of processors.
Reference: [14] <author> John D. McCalpin. </author> <title> Sustainable Memory Bandwidth in Current High-Performance Computers. </title> <note> http://reality.sgi.com/employees/mccalpin, 1995. </note>
Reference: [15] <institution> NASA Ames Research Center. </institution> <note> NPB 2 Detailed Results, 1997. http://science.nas.nasa.gov/Software/- NPB/NPB2Results. </note>
Reference-contexts: We believe that the reported increases in overall efficiencies [18] as problem size scales are due to this effect. Given that the Origin is still able to obtain super-linear speedups on a number of the NPB codes for the class B and C sizes <ref> [15] </ref>, increases in memory system efficiency for larger problem sizes are possible. However, this may require larger caches than for the class A sizes reported here. We have shown that scalability is a complex phenomenon.
Reference: [16] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 5 presents related work, and in Section 6, we conclude. 2 Methodology Understanding the performance under scaling of large, parallel codes is a difficult problem <ref> [16] </ref>. Ideally, one would like to run the benchmarks on real machines, but this option precludes a detailed study of differing hardware characteristics such as cache size and other parameters. Simulations are problematic, because they limit the size of the problem and can potentially miss long-term effects.
Reference: [17] <author> Edward Rotherberg, Jawwinder Pal Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes and Node Granuarity Issues for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: More importantly, we see that all NAS programs are highly structured: communication is infrequent and consists of large transfers. The communication to computation ratio is roughly equivalent to the less frequently communicating SPLASH-2 codes. A study of various computational kernels' scaling behavior is presented in <ref> [17] </ref>. However, the study focuses on memory-constrained and time-constrained scaling, rather than the problem-size constrained scaling. Interestingly, the study found that much smaller grain sizes were acceptable up to a large (1000) number of nodes.
Reference: [18] <author> William Saphir, Alex Woo, and Maurice Yarrow. </author> <title> NAS Parallel Benchmark 2.1 Results. </title> <type> Technical Report NAS-96-010, </type> <institution> NASA Ames Research Center, </institution> <year> 1996. </year>
Reference-contexts: Better communication scheduling, perhaps with techniques found in [5], may be required to obtain higher efficiency. 5 Related Work The performance of the NPB 2.1 codes is reported in <ref> [18] </ref> for several different platforms. Although the qualitative conclusions they reach are similar to ours, little quantitative data on the internal structure of the NPB is presented. <p> Communication scheduling is clearly an issue. Finally, there is a need to extend this work to larger problem sizes, such as those found in class B and C. Scaling to larger problem sizes will lower the communication to computation ratio. We believe that the reported increases in overall efficiencies <ref> [18] </ref> as problem size scales are due to this effect. Given that the Origin is still able to obtain super-linear speedups on a number of the NPB codes for the class B and C sizes [15], increases in memory system efficiency for larger problem sizes are possible.
Reference: [19] <author> Jaswinder Pal Singh, John Hennessy, and Anoop Gupta. </author> <title> Scaling Parallel Programs for Multiprocessor: Methodolgy and Examples. </title> <journal> Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> jul </month> <year> 1993. </year>
Reference-contexts: Much has been written about the mathematical techniques in these codes, but the understanding of their architectural requirements is at best incomplete. The NAS parallel benchmarks use Constant Problem Size (CPS) scaling as the method of evaluating parallel machine performance <ref> [19] </ref>. Under CPS scaling, the problem size is fixed as processors are added. that scaling characteristics across platforms are quite different.
Reference: [20] <author> Thorsten von Eicken, David E. Culler, Steh C. Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: The machines are interconnected with ten 8-port Myrinet [4] switches in a fat-tree type topology. All the NAS benchmarks communicate via MPI, version 1. Our implementation of MPI is based on the MPICH reference implementation [2]. All ADI calls are mapped to Active Messages <ref> [20] </ref>, and the layer is highly tuned for performance. The resulting MPI implementation has a fixed set-up cost of 50 microseconds and a peak bandwidth of 30 MB/s. To break down the performance of the benchmarks, we add instrumentation code to the MPI layer.
Reference: [21] <author> Steven Cameron Woo, Moriwoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Within the realm of interest, there is an order of magnitude difference in the average size of a message. Interestingly, the smallest messages are still on the order of 1000 bytes, which is a substantially larger grain than found in many other parallel benchmarks <ref> [13, 21] </ref>. Because of their all-to-all pattern, for both IS and FT , the total number of messages increases as the square of the number of processors, and thus the per-processor message count is linear in the number of processors. <p> Indeed, for FT and IS , the ratio is constant. However, the there is an order-of-magnitude variation in communication to computation ratios across the benchmarks. The communication to computation ration, while non-trivial, is still on the lower end of those reported in the SPLASH-2 suite <ref> [21] </ref>. Overall, we have seen that most codes perform more work as processors increase, usually a 5% to 20% increase. Further, all programs perform a substantial amount of communication, and all but two show an increase in the total amount of data communicated under scaling. <p> Whereas many studies, such as those of SPLASH-2 <ref> [21] </ref>, have shown the knees of the working set on a given number of processors, we also show how the working sets change as processors are scaled. <p> Although two of the platforms in their study exhibited super-linear speedup (the SGI PowerChallenge and the Cray T3D), they did not investigate or explain this effect. Perhaps the closest study to our own is the SPLASH-2 work <ref> [21] </ref>. The NAS suite, however, differs significantly from the SPLASH-2 benchmarks in two important ways. First, the NAS codes use explicit message passing, whereas SPLASH codes are for shared-memory machines. More importantly, we see that all NAS programs are highly structured: communication is infrequent and consists of large transfers.
References-found: 21

