URL: http://www.ai.univie.ac.at/~bernhard/kdd97.ps.gz
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Email: stefan@ai.univie.ac.at  bernhard@cs.waikato.ac.nz  Christoph.Helma@univie.ac.at  
Title: Mining for Causes of Cancer: Machine Learning Experiments at Various Levels of Detail  
Author: Stefan Kramer Bernhard Pfahringer Christoph Helma 
Address: Schottengasse 3 A-1010 Vienna, Austria  Private Bag 3105 Hamilton, New Zealand  Borschkegasse 8a A-1090 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  Computer Science Dept., University of Waikato  Institute for Tumor Biology Cancer Research, University of Vienna  
Abstract: This paper presents first results of an interdisciplinary project in scientific data mining. We analyze data about the carcinogenicity of chemicals derived from the carcinogenesis bioassay program performed by the US National Institute of Environmental Health Sciences. The database contains detailed descriptions of 6823 tests performed with more than 330 compounds and animals of different species, strains and sexes. The chemical structures are described at the atom and bond level, and in terms of various relevant structural properties. The goal of this paper is to investigate the effects that various levels of detail and amounts of information have on the resulting hypotheses, both quantitatively and qualitatively. We apply relational and propositional machine learning algorithms to learning problems formulated as regression or as classification tasks. In addition, these experiments have been conducted with two learning problems which are at different levels of detail. Quantitatively, our experiments indicate that additional information not necessarily improves accuracy. Qualitatively, a number of potential discoveries have been made by the algorithm for Relational Regression because it can utilize all the information contained in the relations of the database as well as in the numerical dependent variable. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Auer, P.; Maass, W.; and Holte, R. </author> <year> 1995. </year> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The ILP setting includes additional low-level structural details about atoms and bonds. Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we used C4.5 (Quinlan 1993), and T2 <ref> (Auer, Maass, & Holte 1995) </ref>, which induces 2-level decision trees. FOIL (Quinlan 1990) and Progol (Muggleton 1995) are state-of-the-art ILP algorithms. M5 (Quinlan 1992) learns regression trees with linear regression models in the leaves. SRT (Kramer 1996) learns relational regression trees.
Reference: <author> Dzeroski, S. </author> <year> 1995. </year> <title> Numerical Constraints and Learnabil-ity in Inductive Logic Programming. </title> <type> Ph.D. Dissertation, </type> <institution> University of Ljubljana, Ljubljana, Slovenija. </institution>
Reference-contexts: All rights reserved. carcinogenicity. SARs are models that predict the activity of chemicals in organisms from the molecular structure. Formally, the problem is to predict numbers from "relational structures" (such as labeled graphs), a problem also known as Relational Regression <ref> (Dzeroski 1995) </ref>. The data used were derived from the carcino-genesis bioassay program, a long-term research study performed by the US National Institute of Environmental Health Sciences (NIEHS). Related Work Several SAR studies (e.g., (Hirst, King, & Sternberg 1994)) using ILP methods have been published.
Reference: <author> Fayyad, U.; Haussler, D.; and Stolorz, P. </author> <year> 1996. </year> <title> KDD for science data analysis: Issues and examples. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> 50-56. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Introduction 1 In science data analysis <ref> (Fayyad, Haussler, & Stolorz 1996) </ref>, we benefit from the luxury of precision of the data and the availability of domain knowledge, but often scientific data are complex and highly structured. Therefore "a flat-file form of the data is unlikely to be useful"(Fayyad, Haussler, & Stolorz 1996).
Reference: <author> Gold, L. </author> <year> 1995. </year> <booktitle> Sixth plot of the carcinogenicity potency in the general literature 1989 to 1990 and by the national toxicology program 1990 to 1993. Environmental Health Perspectives 103 (Suppl7):1-122. </booktitle>
Reference: <author> Hirst, J.; King, R.; and Sternberg, M. </author> <year> 1994. </year> <title> Quantitative structure-activity relationships by neural networks and inductive logic programming. </title> <booktitle> the inhibition of dihy-drofolate reductase by pyrimidines. Journal of Computer-Aided Molecular Design 8 </booktitle> <pages> 405-420. </pages>
Reference-contexts: The data used were derived from the carcino-genesis bioassay program, a long-term research study performed by the US National Institute of Environmental Health Sciences (NIEHS). Related Work Several SAR studies (e.g., <ref> (Hirst, King, & Sternberg 1994) </ref>) using ILP methods have been published. Generally, the comparisons of ILP algorithms with other approaches (linear regression, neural networks) showed no statistically significant differences in predictive accuracies, but ILP-generated theories tend to be more comprehensible.
Reference: <author> King, R., and Srinivasan, A. </author> <year> 1997. </year> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives. </title>
Reference-contexts: This work is also much in the spirit of studies comparing various methods (FOIL vs. Pro-gol (Srinivasan, Muggleton, & King 1995), propositional learning vs. relational learning (Srinivasan et al. 1996)) in the domain of mutagenicity. <ref> (King & Srinivasan 1997) </ref> report on the application of Progol to one of the databases also used here. Description of the Data In this section we describe the datasets used in our experiments "as is", without the data engineering steps to define the learning problems. <p> Description of the Data In this section we describe the datasets used in our experiments "as is", without the data engineering steps to define the learning problems. Our starting point are two databases: The first one <ref> (King & Srinivasan 1997) </ref>(abbreviated by K&S), contains information about the carcinogenicity of 330 compounds, as classified by the NIEHS. <p> Next we present the major discoveries and findings from our experiments. One of the authors is an expert in toxicology, and interprets the theories induced by the learning algorithms. 2 The experiment with Progol has been described in <ref> (King & Srinivasan 1997) </ref>. Approach Algorithm Accuracy Rel. E. Default 55.00% Ames Test 63.00% Propositional C4.5 prune 58.79% Classification C4.5 rules 60.76% T2 65.00% Propositional M5 69.93% 0.98 Regression Relational FOIL 25.15% Classification Progol 63.00% Relational SRT 72.46% 0.14 Regression Table 4: Quantitative results for chemicals obtained by 5-fold cross-validation.
Reference: <author> Kramer, S. </author> <year> 1996. </year> <title> Structural regression trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96). </booktitle>
Reference-contexts: For propositional classification, we used C4.5 (Quinlan 1993), and T2 (Auer, Maass, & Holte 1995), which induces 2-level decision trees. FOIL (Quinlan 1990) and Progol (Muggleton 1995) are state-of-the-art ILP algorithms. M5 (Quinlan 1992) learns regression trees with linear regression models in the leaves. SRT <ref> (Kramer 1996) </ref> learns relational regression trees. Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5). For the chemicals, we did not observe big differences in accuracy except for Relational Regression. SRT achieves (with statistical significance) the best accuracy for the chemicals.
Reference: <editor> Muggleton, S., ed. </editor> <booktitle> 1992. Inductive Logic Programming. </booktitle>
Reference: <institution> London, </institution> <address> U.K.: </address> <publisher> Academic Press. </publisher>
Reference: <author> Muggleton, S. </author> <year> 1995. </year> <title> Inverse Entailment and Progol. </title> <booktitle> New Generation Computing 13 </booktitle> <pages> 245-286. </pages>
Reference-contexts: Generally, the comparisons of ILP algorithms with other approaches (linear regression, neural networks) showed no statistically significant differences in predictive accuracies, but ILP-generated theories tend to be more comprehensible. This work is also much in the spirit of studies comparing various methods (FOIL vs. Pro-gol <ref> (Srinivasan, Muggleton, & King 1995) </ref>, propositional learning vs. relational learning (Srinivasan et al. 1996)) in the domain of mutagenicity. (King & Srinivasan 1997) report on the application of Progol to one of the databases also used here. <p> Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we used C4.5 (Quinlan 1993), and T2 (Auer, Maass, & Holte 1995), which induces 2-level decision trees. FOIL (Quinlan 1990) and Progol <ref> (Muggleton 1995) </ref> are state-of-the-art ILP algorithms. M5 (Quinlan 1992) learns regression trees with linear regression models in the leaves. SRT (Kramer 1996) learns relational regression trees. Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: <author> Quinlan, J. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 239-266. </pages>
Reference-contexts: The ILP setting includes additional low-level structural details about atoms and bonds. Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we used C4.5 (Quinlan 1993), and T2 (Auer, Maass, & Holte 1995), which induces 2-level decision trees. FOIL <ref> (Quinlan 1990) </ref> and Progol (Muggleton 1995) are state-of-the-art ILP algorithms. M5 (Quinlan 1992) learns regression trees with linear regression models in the leaves. SRT (Kramer 1996) learns relational regression trees. Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: <author> Quinlan, J. </author> <year> 1992. </year> <title> Learning with continuous classes. </title> <editor> In Adams, S., ed., </editor> <booktitle> Proceedings AI'92, </booktitle> <pages> 343-348. </pages> <address> Singapore: </address> <publisher> World Scientific. </publisher>
Reference-contexts: Since we are not aware of relational learning algorithms dealing with ordinal dependent variables, we formulated a regression problem by mapping the NIEHS assessment to f1; 0; 1g. The scale does not play a role because we evaluated the results by the relative error (see e.g. <ref> (Quinlan 1992) </ref>). The classification accuracy was calculated in the following way: if a regression rule predicts a negative value, then we predict "non-carcinogenic", else we predict "carcinogenic". For tests as examples, we derived the classification problem from the regression problem by discretiza-tion of the dependent variable. <p> Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we used C4.5 (Quinlan 1993), and T2 (Auer, Maass, & Holte 1995), which induces 2-level decision trees. FOIL (Quinlan 1990) and Progol (Muggleton 1995) are state-of-the-art ILP algorithms. M5 <ref> (Quinlan 1992) </ref> learns regression trees with linear regression models in the leaves. SRT (Kramer 1996) learns relational regression trees. Experimental Results First we discuss the quantitative results of the experiments (see Table 4 and Table 5).
Reference: <author> Quinlan, J. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The ILP setting includes additional low-level structural details about atoms and bonds. Algorithms Used In Table 3, we present the algorithms used for our comparative study. For propositional classification, we used C4.5 <ref> (Quinlan 1993) </ref>, and T2 (Auer, Maass, & Holte 1995), which induces 2-level decision trees. FOIL (Quinlan 1990) and Progol (Muggleton 1995) are state-of-the-art ILP algorithms. M5 (Quinlan 1992) learns regression trees with linear regression models in the leaves. SRT (Kramer 1996) learns relational regression trees.
Reference: <author> Srinivasan, A.; Muggleton, S.; King, R.; and Sternberg, M. </author> <year> 1996. </year> <title> Theories for mutagenicity: a study of first-order and feature based induction. </title> <journal> Artificial Intelligence 85(1-2):277-299. </journal>
Reference-contexts: This work is also much in the spirit of studies comparing various methods (FOIL vs. Pro-gol (Srinivasan, Muggleton, & King 1995), propositional learning vs. relational learning <ref> (Srinivasan et al. 1996) </ref>) in the domain of mutagenicity. (King & Srinivasan 1997) report on the application of Progol to one of the databases also used here.
Reference: <author> Srinivasan, A.; Muggleton, S.; and King, R. </author> <year> 1995. </year> <title> Comparing the use of background knowledge by Inductive Logic Programming systems. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> 199-230. </pages> <institution> Katholieke Universiteit Leuven. </institution>
Reference-contexts: Generally, the comparisons of ILP algorithms with other approaches (linear regression, neural networks) showed no statistically significant differences in predictive accuracies, but ILP-generated theories tend to be more comprehensible. This work is also much in the spirit of studies comparing various methods (FOIL vs. Pro-gol <ref> (Srinivasan, Muggleton, & King 1995) </ref>, propositional learning vs. relational learning (Srinivasan et al. 1996)) in the domain of mutagenicity. (King & Srinivasan 1997) report on the application of Progol to one of the databases also used here.
References-found: 15

