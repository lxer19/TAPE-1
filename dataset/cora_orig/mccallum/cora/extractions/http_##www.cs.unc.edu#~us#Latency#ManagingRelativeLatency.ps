URL: http://www.cs.unc.edu/~us/Latency/ManagingRelativeLatency.ps
Refering-URL: http://www.cs.unc.edu/~us/latency.html
Root-URL: http://www.cs.unc.edu
Email: fjacobsjlivingstjstateg@cs.unc.edu  
Title: Managing Latency in Complex Augmented Reality Systems  
Author: Marco C. Jacobs Mark A. Livingston Andrei 
Keyword: CR Categories and Subject Descriptors: I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality; I.3.1 [Hardware Architecture]: Three-dimensional displays; I.3.6 [Methodology and Techniques]: Interaction Techniques. Additional Keywords: Augmented Reality, Latency Management, Ultrasound Echography.  
Address: Netherlands  
Affiliation: State University of North Carolina at Chapel Hill Delft University of Technology, the  
Abstract: Registration (or alignment) of the synthetic imagery with the real world is crucial in augmented reality (AR) systems. The data from user-input devices, tracking devices, and imaging devices need to be registered spatially and temporally with the user's view of the surroundings. Each device has an associated delay between its observations of the world and the moment when the AR display presented to the user appears to be affected by a change in the data. We call the differences in delay the relative latencies. Relative latency is a source of misregistration and should be reduced. We give general methods for handling multiple data streams with different latency values associated with them in a working AR system. We measure the latency differences (part of the system dependent set of calibrations), time-stamp on-host, adjust the moment of sampling, and interpolate or extrapolate data streams. By using these schemes, a more accurate and consistent view is computed and presented to the user. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Azuma, R., and Bishop, G. </author> <title> A Frequency-Domain analysis of head-motion prediction. </title> <booktitle> In SIGGRAPH 95 Conference Proceedings (Aug. </booktitle> <year> 1995), </year> <editor> R. Cook, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 401-408. </pages> <address> held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: Parallelization reduces latency for T comp , T render and T sync by increasing throughput. Wloka [16] dedicates a processor to sample the external data streams at a high frequency. Prediction of future head position and orientation can be used in order to reduce perceived delay <ref> [1] </ref>. This position and orientation is estimated by extrapolation of current and older values. This predicted value can then be used to generate an image for the time when the image will be displayed. Since T render is not a constant, it may be difficult to determine the extrapolation interval. <p> On the other hand, if a tracking system's acquisition path has higher latency than the video camera's acquisition path, we can use predictive tracking methods <ref> [1] </ref> to compensate for the difference. 4 CASE STUDY: AR ULTRASOUND SYSTEM Our testbed AR system is the ultrasound visualization system currently being developed at the University of North Carolina at Chapel Hill [13].
Reference: [2] <author> Azuma, R. T. </author> <title> A survey of augmented reality. </title> <booktitle> In Computer Graphics (SIGGRAPH '95 Proceedings, Course Notes #9: Developing Advanced Virtual Reality Applications) (Aug. </booktitle> <year> 1995), </year> <pages> pp. 1-38. </pages>
Reference-contexts: This view is created by compositing computer graphics with a view of the real world. The graphics must be generated in such a way that the user believes that the synthetic objects exist in the environment. Azuma <ref> [2] </ref> gives an introduction to the field of AR, the technology that can be used to achieve it, current existing applications, and the potential of the paradigm. 1.1 Motivation As has been recognized in previous AR systems, registration (or alignment) of the synthetic imagery with the real is crucial.
Reference: [3] <author> Bajura, M., and Neumann, U. </author> <title> Dynamic registration correction in video-based augmented reality systems. </title> <journal> IEEE Computer Graphics and Applications 15, </journal> <month> 5 (Sep </month> <year> 1995), </year> <pages> 52-60. </pages>
Reference-contexts: Regan et al. [10] render the scene on the faces of a cube. A head rotation causes an address offset for the final image to be taken from this cube. State et al. [12] and Bajura et al. <ref> [3] </ref> synchronize a video stream with head-tracking data by reducing the head-tracking error with the use of videometrically tracked landmarks. Bajura et al. [3] temporally synchronize the output of a rendering system and a video stream by buffering the lower latency video. 3 METHODS The naive way of writing an AR <p> A head rotation causes an address offset for the final image to be taken from this cube. State et al. [12] and Bajura et al. <ref> [3] </ref> synchronize a video stream with head-tracking data by reducing the head-tracking error with the use of videometrically tracked landmarks. Bajura et al. [3] temporally synchronize the output of a rendering system and a video stream by buffering the lower latency video. 3 METHODS The naive way of writing an AR application is to sample all the data streams at the start of a frame after which the application starts computing and rendering (Figure
Reference: [4] <author> Funkhouser, T. A., and S equin, C. H. </author> <title> Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. </title> <booktitle> In Computer Graphics (SIGGRAPH '93 Proceedings) (Aug. </booktitle> <year> 1993), </year> <editor> J. T. Kajiya, Ed., </editor> <volume> vol. 27, </volume> <pages> pp. 247-254. </pages>
Reference: [5] <author> Holloway, R. L. </author> <title> Viper: A quasi real-time virtual-environment application. </title> <type> Tech. Rep. </type> <institution> TR92-004, Department of Computer Science, The University of North Carolina, </institution> <year> 1992. </year>
Reference: [6] <author> Holloway, R. L. </author> <title> Registration errors in augmented reality systems. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> Dissertation TR95-016, Department of Computer Science, The University of North Carolina, </institution> <year> 1995. </year>
Reference-contexts: The model's depth values are used in order to achieve correct occlusion. We have eliminated a relative latency of 30ms in the tracking of the ultrasound probe. In analyzing registration error, Holloway <ref> [6] </ref> found that 1ms of latency can cause 1mm of registration error, so we have eliminated a potential source of up to 30mm of registration error. We of course introduce error in the prediction, but this is a small price.
Reference: [7] <author> Mark, W. R., Bishop, G., and McMillan, L. </author> <title> Post-rendering 3-d warping for latency compensation. </title> <booktitle> In 1997 Symposium on Interactive 3D Graphics (Apr. 1997), ACM SIGGRAPH. </booktitle>
Reference-contexts: This can introduce misregistration caused by relative latencies. An optical-see-through setup also suffers from the relative latency between the latent rendered images and the zero-latency real world. newer head position. This newer head position can be acquired after rendering or be approximated through prediction. Mark et al. <ref> [7] </ref> use a warping mechanism to reduce latency caused by bandwidth limitations. Regan et al. [10] render the scene on the faces of a cube. A head rotation causes an address offset for the final image to be taken from this cube.
Reference: [8] <author> Mine, M. </author> <title> Characterization of end-to-end delays in head-mounted displays. </title> <type> Tech. Rep. </type> <institution> TR93-001, Department of Computer Science, The University of North Carolina, </institution> <year> 1993. </year>
Reference: [9] <author> Olano, M., Cohen, J., Mine, M., and Bishop, G. </author> <title> Combatting rendering latency. </title> <booktitle> In 1995 Symposium on Interactive 3D Graphics (Apr. 1995), ACM SIG-GRAPH, </booktitle> <pages> pp. 19-24. </pages> <note> ISBN 0-89791-736-7. </note>
Reference-contexts: The application is constantly aware of time. Reducing latency here means reducing computation accuracy, which might not be always advisable. Most real-time graphics system are aimed at high throughput instead of low latency. High throughput is achieved through pipelining which results in latency. Olano et al. <ref> [9] </ref> specifically discuss a low-latency rendering system and a technique to reduce errors caused by delays in the display system. Parallelization reduces latency for T comp , T render and T sync by increasing throughput. Wloka [16] dedicates a processor to sample the external data streams at a high frequency.
Reference: [10] <author> Regan, M., and Pose, R. </author> <title> Priority rendering with a virtual reality address recalculation pipeline. </title> <booktitle> In Proceedings of SIGGRAPH '94 (Orlando, </booktitle> <address> Florida, </address> <month> July 24-29, </month> <year> 1994) </year> <month> (July </month> <year> 1994), </year> <editor> A. Glassner, Ed., </editor> <booktitle> Computer Graphics Proceedings, Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 155-162. </pages> <note> ISBN 0-89791-667-0. </note>
Reference-contexts: This newer head position can be acquired after rendering or be approximated through prediction. Mark et al. [7] use a warping mechanism to reduce latency caused by bandwidth limitations. Regan et al. <ref> [10] </ref> render the scene on the faces of a cube. A head rotation causes an address offset for the final image to be taken from this cube.
Reference: [11] <author> Sajedi, A. </author> <title> Private communication, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: We measured the end-to-end latency of the camera (40ms). Adding the relative latency between these two streams (30ms), we conclude that the results of the experiments are consistent with the Faro specification. The off-host latency of the Faro is specified as 67ms <ref> [11] </ref>. 4.3 Results First, we arranged the order of polling the devices to reflect the latencies measured in the previous section. 1. Read Faro and FOB trackers for prediction. 2. Capture camera video. 3. Read FOB tracker again. 4. Invoke vision-based hybrid tracker [12]. 5. Read Faro tracker again. 6.
Reference: [12] <author> State, A., Hirota, G., Chen, D. T., Garrett, W. F., and Livingston, M. A. </author> <title> Superior augmented reality registration by integrating landmark tracking and magnetic tracking. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings (Aug. </booktitle> <year> 1996), </year> <editor> H. Rushmeier, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 429-438. </pages> <address> held in New Orleans, Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: Mark et al. [7] use a warping mechanism to reduce latency caused by bandwidth limitations. Regan et al. [10] render the scene on the faces of a cube. A head rotation causes an address offset for the final image to be taken from this cube. State et al. <ref> [12] </ref> and Bajura et al. [3] synchronize a video stream with head-tracking data by reducing the head-tracking error with the use of videometrically tracked landmarks. <p> Read Faro and FOB trackers for prediction. 2. Capture camera video. 3. Read FOB tracker again. 4. Invoke vision-based hybrid tracker <ref> [12] </ref>. 5. Read Faro tracker again. 6. Capture ultrasound video. 7. Render world. video stream. The pulse generator triggers the LED, which in turn triggers the oscilloscope. The LED blinks and is seen in the camera image, which is subsequently seen by the photosensor. The photosensor also triggers the oscilloscope. <p> However, due to the noise and inaccuracy of the magnetic tracker signal the linear predictor is not precise enough. We therefore apply a vision-based corrector (C Flock , Figure 6) that relies on color-coded landmarks visible in the image <ref> [12] </ref>. This corrector typically eliminates relative latency (and inaccuracies in the tracking report), since its tracking information comes from the video image, to which we are synchronizing (s Camera ) (Plate 1, left; original, Plate 4, left; method applied).
Reference: [13] <author> State, A., Livingston, M. A., Hirota, G., Gar-rett, W. F., Whitton, M. C., and Fuchs, H. </author> <title> Technologies for augmented-reality systems: </title> <booktitle> Realizing ultrasound-guided needle biopsies. In SIGGRAPH 96 Conference Proceedings (Aug. </booktitle> <year> 1996), </year> <editor> H. Rushmeier, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Ad-dison Wesley, </publisher> <pages> pp. 439-446. </pages> <address> held in New Orleans, Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: We introduce these methods while keeping in mind that we are building a complete system, and thus must focus on our goal-providing a convincing and accurate illusion. We apply these methods to a previously described AR system for real-time ultrasound visualization <ref> [13] </ref> and demonstrate improved registration and visualization resulting from our latency management techniques. 1.3 Latency sources The following sources of delay have been classified [8][16][14]: Off-host delay: Duration between the occurrence of a physical event and its arrival on the host. (T offhost ) Computational delay: Time elapsed while the data <p> acquisition path has higher latency than the video camera's acquisition path, we can use predictive tracking methods [1] to compensate for the difference. 4 CASE STUDY: AR ULTRASOUND SYSTEM Our testbed AR system is the ultrasound visualization system currently being developed at the University of North Carolina at Chapel Hill <ref> [13] </ref>. This system is a video-see-through AR system designed for the medical procedure known as ultrasound-guided needle biopsy. The physician wears a head-mounted display fitted with two video cameras. A Flock of Birds (FOB) magnetic tracker from Ascension Technology Corporation is used to track the head-mounted cameras.
Reference: [14] <author> Taylor, V. E., Stevens, R., and Canfield, T. </author> <title> Performance models of interactive, immersive visualization for scientific applications. </title> <booktitle> In Proceedings of the International Workshop on High Performance Computing for Computer Graphics and Visualization (July 1995), </booktitle> <pages> pp. 238-252. </pages>
Reference: [15] <author> Wloka, M. M. </author> <title> Dissertation proposal: Time-critical graphics. </title> <type> Tech. Rep. </type> <institution> TR-CS-93-50, Computer Science Department, Brown University, </institution> <year> 1993. </year>
Reference: [16] <author> Wloka, M. M. </author> <title> Lag in multiprocessor virtual reality. Presence: Teleoperators and Virtual Environments 4, </title> <booktitle> 1 (Winter 1995), </booktitle> <pages> 50-63. </pages>
Reference-contexts: High throughput is achieved through pipelining which results in latency. Olano et al. [9] specifically discuss a low-latency rendering system and a technique to reduce errors caused by delays in the display system. Parallelization reduces latency for T comp , T render and T sync by increasing throughput. Wloka <ref> [16] </ref> dedicates a processor to sample the external data streams at a high frequency. Prediction of future head position and orientation can be used in order to reduce perceived delay [1]. This position and orientation is estimated by extrapolation of current and older values. <p> We would like to have autonomous sampling processes for each stream <ref> [16] </ref>. Every stream could then have its own (higher) sampling frequency making it possible to have more accurate interpolation and extrapolation functions. For video images this is hard, since video has high bandwidth and moving video around in memory costs time, but for tracking information this is easily feasible.
References-found: 16

