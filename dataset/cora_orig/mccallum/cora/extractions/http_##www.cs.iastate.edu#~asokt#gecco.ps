URL: http://www.cs.iastate.edu/~asokt/gecco.ps
Refering-URL: http://www.cs.iastate.edu/~asokt/papers.html
Root-URL: http://www.cs.iastate.edu
Email: yang@cs.iastate.edu  asokt@cs.iastate.edu  fjchen@cs.iastate.edu  honavar@cs.iastate.edu  
Title: Feature Subset Selection for Rule Induction Using RIPPER  
Author: Jihoon Yang Asok Tiyyagura Fajun Chen Vasant Honavar 
Keyword: CATEGORY: REAL WORLD APPLICA TIONS Abstract  
Address: Ames, IA 50011  Ames, IA 50011  Ames, IA 50011  Ames, IA 50011  
Affiliation: Computer Science Dept. Iowa State University  Computer Science Dept. Iowa State University  Computer Science Dept. Iowa State University  Computer Science Dept. Iowa State University  
Abstract: The choice of features or attributes used to represent patterns in the synthesis of pattern classifiers using machine learning algorithms has a strong impact on the accuracy of the classifier, the number of examples needed to attain a given classification accuracy on test data, the cost of classification, and the comprehensibility of the learned classifier. This presents us with a feature subset selection problem, namely, the selection of a subset of features from a much larger candidate set of features to represent patterns to be classified so as to optimize multiple criteria such as the accuracy and the cost of pattern classification. Evolutionary algorithms, because of their ability to find good solutions offer a promising approach to such a multi-criteria optimization problem. Results of experiments reported in this paper demonstrate that feature subset selection using a genetic algorithm results in substantial improvement in classification accuracy and comprehensibility, and substantial reduction in the cost of classification associated with pattern classifiers trained using RIPPER on a number of benchmark problems. RIPPER is a relatively fast algorithm for induction of pattern classification rules from labeled examples. Given the ability of RIPPER to induce rules from large, noisy datasets consisting of patterns that are encoded using binary, numeric, or nominal attributes, this makes GARIPPER, the proposed hybrid approach to rule induction, an attractive approach to data-driven knowledge discovery. 
Abstract-found: 1
Intro-found: 1
Reference: [Balakrishnan and Honavar, 1995] <author> Balakrishnan, K. and Honavar, V. </author> <year> (1995). </year> <title> Properties of genetic representations of neural architectures. </title> <booktitle> In Proceedings of WCNN'95 July 17-21 Washington D.C., </booktitle> <volume> volume 1, </volume> <pages> pages 807-813. </pages>
Reference-contexts: The genetic operators are usually designed to exploit the known properties of the genetic representation, the search space, and the optimization problem to be solved. Genetic operators enable the algorithm to explore the space of candidate solutions. See <ref> [Balakrishnan and Honavar, 1995] </ref> for a discussion of some desirable properties of genetic representations and operators. Mutation and crossover are two of the most commonly used operators that are used with genetic algorithms that represent individuals as binary strings.
Reference: [Balakrishnan and Honavar, 1996a] <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996a). </year> <title> Analysis of neurocon-trollers designed by simulated evolution. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <address> Washington, D.C. </address>
Reference: [Balakrishnan and Honavar, 1996b] <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996b). </year> <title> On sensor evolution in robotics. </title> <editor> In Koza, Goldberg, Fogel, and Riolo, editors, </editor> <booktitle> Proceedings of the 1996 Genetic Programming Conference - GP-96, </booktitle> <pages> pages 455-460. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Other examples of feature subset selection problem include large scale data mining applications, power system control [Zhou et al., 1997], construction of user interest profiles for text classification [Yang et al., 1998] and sensor subset selection in the design of autonomous robots <ref> [Balakrishnan and Honavar, 1996b] </ref>. The rest of the paper is organized as follows: Section 2 describes our approach that uses a Genetic Algorithm for RIPPER pattern classifiers. Section 3 explains the implementation details in our experiments.
Reference: [Balakrishnan and Honavar, 1996c] <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996c). </year> <title> Some experiments in the evolutionary synthesis of robotic neurocontrollers. </title> <booktitle> In Proceedings of the World Congress on Neural Networks (WCNN'96), </booktitle> <pages> pages 1035-1040, </pages> <address> San Diego, CA. </address>
Reference: [Banzaf et al., 1997] <author> Banzaf, W., Nordin, P., Keller, R., and Francone, F. </author> <year> (1997). </year> <title> Genetic Programming An Introduction. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: With a randomly chosen crossover position 4, the two strings 01101 and 11000 yield the offspring 01100 and 11001 as a result of crossover. Other genetic representations (e.g., matrices, LISP programs) require the use of appropriately designed genetic operators <ref> [Michalewicz, 1996, Mitchell, 1996, Banzaf et al., 1997] </ref>. The process of selection and application of genetic operators to generate successive generations of individuals is repeated until a satisfactory solution is found (or the search fails to do so within the time allocated).
Reference: [Fogel, 1995] <author> Fogel, D. </author> <year> (1995). </year> <title> Evolutionary Computation: Toward a New Philosophy of Machine Intelligence. </title> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ. </address>
Reference: [Frank and Witten, 1998] <author> Frank, E. and Witten, I. H. </author> <year> (1998). </year> <title> Generating accurate rule sets without global optimization. </title>
Reference-contexts: Other algorithms (e.g., RIPPER [W.Cohen, 1995]) di rectly induce rules from the training data using a separate-and-conquer approach. The learned ruleset is post-processed to discard (as in C4.5) or prune (as in RIPPER) some of the rules using various criteria to improve their classification accuracy on test data.Recently, <ref> [Frank and Witten, 1998] </ref> has proposed an approach to rule learning that combines aspects of decision tree learning and the separate-and-conquer approach to eliminate the need for post-processing of the learned ruleset. The design of RIPPER is an extension of IREP (Incremental Reduced Error Pruning) algorithm [Furnkranz and Widmer, 1994].
Reference: [Furnkranz and Widmer, 1994] <author> Furnkranz and Wid-mer (1994). </author> <title> Incremental reduced error pruning. </title> <booktitle> In Proceedings of the Eleventh Annual Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The design of RIPPER is an extension of IREP (Incremental Reduced Error Pruning) algorithm <ref> [Furnkranz and Widmer, 1994] </ref>. IREP tightly integrates reduced error pruning with separate-and-conquer strategy. IREP is a greedy rule induction algorithm which learns a rule at a time. The rule so learned covers a maximal subset of examples in its current training set.
Reference: [Goldberg, 1989] <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> New York. </address>
Reference-contexts: The individuals represent candidate solutions to the optimization problem being solved. A wide range of genetic representations (e.g., bit vectors, LISP programs, matrices, etc.) can be used to encode the individuals depending on the space of solutions that needs to be searched. In genetic algorithms <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>, the individuals are typically represented by n-bit binary vectors. The resulting search space corresponds to an n-dimensional boolean space. In the feature subset selection problem, each individual would represent a feature subset. <p> Evolutionary algorithms use some form of fitness-dependent probabilistic selection of individuals from the current population to produce individuals for the next generation. A variety of selection techniques have been explored in the literature. Some of the most common ones are fitness-proportionate selection, rank-based selection, and tournament-based selection <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>. The selected individuals are transformed using genetic operators to obtain new individuals that constitute the next generation. The genetic operators are usually designed to exploit the known properties of the genetic representation, the search space, and the optimization problem to be solved. <p> The current implementation of GARIPPER uses the accuracy of the learned ruleset (measured by ten-fold crossvalidation) as the fitness measure. Our experiments with GARIPPER were run using a genetic algorithm used standard mutation and crossover operators and the tournament selection strategy <ref> [Goldberg, 1989, Mitchell, 1996] </ref>.
Reference: [Helmer Guy, 1999] <author> Helmer Guy, Wong Johnny S, H. V. </author> <year> (1999). </year> <title> Automated discovery of concise predictive rules for intrusion detection. </title> <booktitle> In Proceedings of AAAI'99. </booktitle>
Reference-contexts: successfully used recently to select feature subsets for pattern classification tasks that arise in power system security assessment [Zhou et al., 1997], sensor subsets in the design of behavior and control structures for autonomous mobile robots [Balakrishnan and Honavar, 1996a, Balakrishnan and Honavar, 1996b, Balakrishnan and Honavar, 1996c], intrusion detection <ref> [Helmer Guy, 1999] </ref>. Additional experiments with GARIPPER in scientific knowledge discovery tasks in bioinformatics (e.g., discovery of protein structure-function relationships, carcinogenicity prediction, gene sequence identification) are currently in progress.
Reference: [Holland, 1992] <author> Holland, J. </author> <year> (1992). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: It can be shown that evolutionary algorithms of the sort outlined above simulate highly opportunistic and exploitative randomized search that explores high-dimensional search spaces rather effectively under certain conditions <ref> [Holland, 1992] </ref>. In practice, the performance of evolutionary algorithms depends on a number of factors including: the choice of genetic representation and operators, the fitness function, the details of the selection procedure, and the various user-determined parameters such as population size, probability of application of different genetic operators, etc.
Reference: [Keeney and Raiffa, 1976] <author> Keeney, R. and Raiffa, H. </author> <year> (1976). </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: programming [Koza, 1992]; Extensive experimental (and wherever feasible, theoretical) comparison of the performance of the proposed approach with that of conventional methods for feature subset selection; More principled design of multi-objective fitness functions for feature subset selection using domain knowledge as well as mathematically well-founded tools of multi-attribute utility theory; <ref> [Keeney and Raiffa, 1976] </ref>;find novel genetic operators which would improve the performance of GARIPPER approach. Acknowledgements This Research is partially supported by NSF-IRI9409580,ISU Council on international programs and EPRI,ISU Graduate College.
Reference: [Koller and Sahami, 1997] <author> Koller, D. and Sahami, M. </author> <year> (1997). </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <pages> pages 170-178. </pages>
Reference-contexts: The classifications for news articles were given based on their topics (6, 4 and 8 classes) following <ref> [Koller and Sahami, 1997] </ref>, resulting in three different datasets (Reuters1, Reuters2 and Reuters3), respectively. These datasets are also summarized in Table 1.
Reference: [Koza, 1992] <author> Koza, J. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Additional experiments with GARIPPER in scientific knowledge discovery tasks in bioinformatics (e.g., discovery of protein structure-function relationships, carcinogenicity prediction, gene sequence identification) are currently in progress. Some directions for future research include: Extension of feature subset selection by incorporating feature construction and genetic programming <ref> [Koza, 1992] </ref>; Extensive experimental (and wherever feasible, theoretical) comparison of the performance of the proposed approach with that of conventional methods for feature subset selection; More principled design of multi-objective fitness functions for feature subset selection using domain knowledge as well as mathematically well-founded tools of multi-attribute utility theory; [Keeney and
Reference: [Langley, 1995] <author> Langley, P. </author> <year> (1995). </year> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: The number of examples needed for learning a sufficiently accurate classification function: All other things being equal, the larger the number of features used to describe the patterns in a domain of interest, the larger is the number of examples needed to learn a classification function to a desired accuracy <ref> [Langley, 1995, Mitchell, 1997] </ref>. * The cost of performing classification using the learned classification function: In many practical applications e.g., medical diagnosis, patterns are described using observable symptoms as well as results of diagnostic tests. Different diagnostic tests might have different costs as well as risks associated with them.
Reference: [Michalewicz, 1996] <author> Michalewicz, Z. </author> <year> (1996). </year> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer-Verlag, </publisher> <address> New York, third edition. </address>
Reference-contexts: The individuals represent candidate solutions to the optimization problem being solved. A wide range of genetic representations (e.g., bit vectors, LISP programs, matrices, etc.) can be used to encode the individuals depending on the space of solutions that needs to be searched. In genetic algorithms <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>, the individuals are typically represented by n-bit binary vectors. The resulting search space corresponds to an n-dimensional boolean space. In the feature subset selection problem, each individual would represent a feature subset. <p> Evolutionary algorithms use some form of fitness-dependent probabilistic selection of individuals from the current population to produce individuals for the next generation. A variety of selection techniques have been explored in the literature. Some of the most common ones are fitness-proportionate selection, rank-based selection, and tournament-based selection <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>. The selected individuals are transformed using genetic operators to obtain new individuals that constitute the next generation. The genetic operators are usually designed to exploit the known properties of the genetic representation, the search space, and the optimization problem to be solved. <p> With a randomly chosen crossover position 4, the two strings 01101 and 11000 yield the offspring 01100 and 11001 as a result of crossover. Other genetic representations (e.g., matrices, LISP programs) require the use of appropriately designed genetic operators <ref> [Michalewicz, 1996, Mitchell, 1996, Banzaf et al., 1997] </ref>. The process of selection and application of genetic operators to generate successive generations of individuals is repeated until a satisfactory solution is found (or the search fails to do so within the time allocated).
Reference: [Mitchell, 1996] <author> Mitchell, M. </author> <year> (1996). </year> <title> An Introduction to Genetic algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The individuals represent candidate solutions to the optimization problem being solved. A wide range of genetic representations (e.g., bit vectors, LISP programs, matrices, etc.) can be used to encode the individuals depending on the space of solutions that needs to be searched. In genetic algorithms <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>, the individuals are typically represented by n-bit binary vectors. The resulting search space corresponds to an n-dimensional boolean space. In the feature subset selection problem, each individual would represent a feature subset. <p> Evolutionary algorithms use some form of fitness-dependent probabilistic selection of individuals from the current population to produce individuals for the next generation. A variety of selection techniques have been explored in the literature. Some of the most common ones are fitness-proportionate selection, rank-based selection, and tournament-based selection <ref> [Goldberg, 1989, Michalewicz, 1996, Mitchell, 1996] </ref>. The selected individuals are transformed using genetic operators to obtain new individuals that constitute the next generation. The genetic operators are usually designed to exploit the known properties of the genetic representation, the search space, and the optimization problem to be solved. <p> With a randomly chosen crossover position 4, the two strings 01101 and 11000 yield the offspring 01100 and 11001 as a result of crossover. Other genetic representations (e.g., matrices, LISP programs) require the use of appropriately designed genetic operators <ref> [Michalewicz, 1996, Mitchell, 1996, Banzaf et al., 1997] </ref>. The process of selection and application of genetic operators to generate successive generations of individuals is repeated until a satisfactory solution is found (or the search fails to do so within the time allocated). <p> The current implementation of GARIPPER uses the accuracy of the learned ruleset (measured by ten-fold crossvalidation) as the fitness measure. Our experiments with GARIPPER were run using a genetic algorithm used standard mutation and crossover operators and the tournament selection strategy <ref> [Goldberg, 1989, Mitchell, 1996] </ref>. <p> They are comparable to the typical values mentioned in the literature <ref> [Mitchell, 1996] </ref>.
Reference: [Mitchell, 1997] <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <address> New York. </address>
Reference-contexts: The number of examples needed for learning a sufficiently accurate classification function: All other things being equal, the larger the number of features used to describe the patterns in a domain of interest, the larger is the number of examples needed to learn a classification function to a desired accuracy <ref> [Langley, 1995, Mitchell, 1997] </ref>. * The cost of performing classification using the learned classification function: In many practical applications e.g., medical diagnosis, patterns are described using observable symptoms as well as results of diagnostic tests. Different diagnostic tests might have different costs as well as risks associated with them.
Reference: [Mooney et al., 1989] <author> Mooney, R., Shavlik, J., Towell, G., and Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 775-780. </pages> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Pattern classifiers that are induced by rule learning algorithms are often simpler and easier to comprehend by humans than those induced using genetic programming or most neural network approaches. Yet, results of experiments <ref> [Mooney et al., 1989] </ref> indicate that the classification accuracies of the classifiers induced using different approaches are often comparable. A variety of algorithms for rule induction from labeled examples have been proposed in the literature.
Reference: [Murphy and Aha, 1994] <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> Repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. </address>
Reference-contexts: No attempt was made to to choose the best parameter 4 EXPERIMENTS 4.1 Description of Datasets The experiments reported here used a wide range of real-world datasets from the machine learning data repository at the University of California at Irvine <ref> [Murphy and Aha, 1994] </ref> as well as a carefully constructed artificial dataset (3-bit parity) to explore the feasibility of using genetic algorithms for feature subset selection for neural network classifiers. The feature Table 1: Datasets used in the experiments.
Reference: [Quinlan, 1993] <author> Quinlan, R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference-contexts: A variety of algorithms for rule induction from labeled examples have been proposed in the literature. Some of them first construct a decision tree e.g., using C4.5 <ref> [Quinlan, 1993] </ref>, and then extract a set of classification rules from the decision tree. Other algorithms (e.g., RIPPER [W.Cohen, 1995]) di rectly induce rules from the training data using a separate-and-conquer approach.
Reference: [Rissanen, 1978] <author> Rissanen, J. </author> <year> (1978). </year> <title> Modelling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: RIPPER replaces v (Rule,PrunePos,PruneNeg) by v*(Rule,PrunePos,PruneNeg) pn p+n . The stopping criterion used in RIPPER is based on the total description length of the rule set and the examples which is motivated by the minimum description length (MDL) heuristic <ref> [Rissanen, 1978] </ref>. For each rule in rule set, say R i , A MDL heuristic is used in RIPPER decide whether to keep R i , or a pruned version of R i .
Reference: [Salton and McGill, 1983] <author> Salton, G. and McGill, M. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill, </publisher> <address> New York. </address>
Reference-contexts: The news articles were obtained from Reuters dataset. Each document is represented in the form of a vector of numeric weights for each of the words (terms) in the vocabulary. The weights correspond to the term frequency and inverse document frequency (TFIDF) <ref> [Salton and McGill, 1983, Yang et al., 1998] </ref> values for the corresponding words. The training sets for paper abstracts were generated based on the classification of the corresponding documents into two classes (interesting and not interesting) by two different individuals, resulting in two different data sets (Ab-stract1 and Abstract2).
Reference: [W.Cohen, 1995] <author> W.Cohen, W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <address> San Fransisco. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A variety of algorithms for rule induction from labeled examples have been proposed in the literature. Some of them first construct a decision tree e.g., using C4.5 [Quinlan, 1993], and then extract a set of classification rules from the decision tree. Other algorithms (e.g., RIPPER <ref> [W.Cohen, 1995] </ref>) di rectly induce rules from the training data using a separate-and-conquer approach. <p> All of the examples that are correctly labeled by the resulting rule are elminated from the training set. This process is repeated until a predertemined stopping condition is satisfied or the training set becomes empty. The IREP algorithm is shown in Figure 1 <ref> [W.Cohen, 1995] </ref>. procedure IREP (Pos,Neg) begin Ruleset: = while Pos6= do /* grow and prune a new rule */ split (Pos,Neg) into (GrowPos,GrowNeg) and (PrunePos,PruneNeg) Rule:=GrowRule (GrowPos,GrowNeg) Rule:=PruneRule (Rule,PrunePos,PruneNeg) /* stopping condition */ if the error rate of Rule on (PrunePos,PruneNeg) exceeds 50% then return Ruleset else add Rule to Ruleset <p> prune a new rule */ split (Pos,Neg) into (GrowPos,GrowNeg) and (PrunePos,PruneNeg) Rule:=GrowRule (GrowPos,GrowNeg) Rule:=PruneRule (Rule,PrunePos,PruneNeg) /* stopping condition */ if the error rate of Rule on (PrunePos,PruneNeg) exceeds 50% then return Ruleset else add Rule to Ruleset remove examples covered by Rule from (Pos,Neg) endif endwhile return Ruleset end RIPPER <ref> [W.Cohen, 1995] </ref> is an improvement over IREP.
Reference: [Yang et al., 1998] <author> Yang, J., Pai, P., Honavar, V., and Miller, L. </author> <year> (1998). </year> <title> Mobile intelligent agents for document classification and retrieval: A machine learning approach. </title> <booktitle> In 14th European Meeting on Cybernetics and Systems Research. Symposium on Agent Theory to Agent Implementation, </booktitle> <address> Vienna, Austria. </address>
Reference-contexts: Other examples of feature subset selection problem include large scale data mining applications, power system control [Zhou et al., 1997], construction of user interest profiles for text classification <ref> [Yang et al., 1998] </ref> and sensor subset selection in the design of autonomous robots [Balakrishnan and Honavar, 1996b]. The rest of the paper is organized as follows: Section 2 describes our approach that uses a Genetic Algorithm for RIPPER pattern classifiers. Section 3 explains the implementation details in our experiments. <p> The news articles were obtained from Reuters dataset. Each document is represented in the form of a vector of numeric weights for each of the words (terms) in the vocabulary. The weights correspond to the term frequency and inverse document frequency (TFIDF) <ref> [Salton and McGill, 1983, Yang et al., 1998] </ref> values for the corresponding words. The training sets for paper abstracts were generated based on the classification of the corresponding documents into two classes (interesting and not interesting) by two different individuals, resulting in two different data sets (Ab-stract1 and Abstract2).
Reference: [Zhou et al., 1997] <author> Zhou, G., McCalley, J., and Honavar, V. </author> <year> (1997). </year> <title> Power system security margin prediction using radial basis function networks. </title> <booktitle> In Proceedings of the 29th Annual North American Power Symposium, </booktitle> <institution> Laramie, Wyoming. </institution>
Reference-contexts: Other examples of feature subset selection problem include large scale data mining applications, power system control <ref> [Zhou et al., 1997] </ref>, construction of user interest profiles for text classification [Yang et al., 1998] and sensor subset selection in the design of autonomous robots [Balakrishnan and Honavar, 1996b]. <p> Techniques similar to the one discussed in this paper have been successfully used recently to select feature subsets for pattern classification tasks that arise in power system security assessment <ref> [Zhou et al., 1997] </ref>, sensor subsets in the design of behavior and control structures for autonomous mobile robots [Balakrishnan and Honavar, 1996a, Balakrishnan and Honavar, 1996b, Balakrishnan and Honavar, 1996c], intrusion detection [Helmer Guy, 1999].
References-found: 26

