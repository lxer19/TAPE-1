URL: http://pcl.cs.ucla.edu/papers/theses/akahn-MS.PS
Refering-URL: http://pcl.cs.ucla.edu/projects/sesame/
Root-URL: http://www.cs.ucla.edu
Title: Simulation of Parallel I/O Programs  
Degree: A thesis submitted in partial satisfaction of the requirements for the degree Master of Science in Computer Science by Andy Chih-Yuan Kahn  
Date: 1997  
Note: Parallel  
Address: Los Angeles  
Affiliation: University of California  
Abstract-found: 0
Intro-found: 1
Reference: [Bag94] <author> R. Bagrodia and Wen toh Liao. "Maisie: </author> <title> A Language for design of Efficient Discrete-Event Simulations." </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> April </month> <year> 1994. </year>
Reference-contexts: Nonetheless, the potential for significant speedup in the execution time of a simulation has led to the design of several algorithms for parallel simulation. They are broadly classified as either optimistic or conservative. Simulation languages such as Maisie <ref> [Bag94] </ref> can act as a tool as well as a parallel language to support both types of algorithms. This allows a user to utilize the simulation algorithm that best fits the application.
Reference: [BBB94] <author> Sandra Johnson Baylor, Caroline Benveniste, and Leo J. Beolhouwer. </author> <title> "A Methodology for Evaluating Parallel I/O Performance for Massively Parallel Processors." </title> <booktitle> In Proceedings of the 27th Annual Simulation Symposium, </booktitle> <pages> pp. 31-40, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: STARFISH's threaded I/O nodes is most similar to the multi-threaded capabilities of MPIO-SIM's underlying simulation kernel. Although the experiments in this thesis do 45 not create threads other than those of the target program, the detailed parallel file system simulation does [BDK97]. In <ref> [BBB94] </ref>, a hybrid methodology for evaluating the performance of parallel I/O subsystems was done. PIOS, a trace-driven I/O simulator, is used to calculate the performance of the I/O system for a subset of the problem to be evaluated, while an analytical model was used for the remainder.
Reference: [BBH96] <author> Sandra Johnson Baylor, Caroline Benveniste, and Yarsun Hsu. </author> <title> "Performance Evaluation of a Massively Parallel I/O Subsystem." </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, volume 362 of The Kluwer International Series in Engineering and Computer Science, chapter 13, </booktitle> <pages> pp. 293-311. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: In contrast, as mentioned before, MPIO-SIM uses the NAS BTIO Benchmarks to generate the workload. 2. Network Model: The Vulcan network is modeled <ref> [BBH96] </ref>. Each node in the network (called a Vulcan node) is connected to the Vulcan network switch. 3. I/O Node Model: Models the ionode by using data obtained by a single Vulcan I/O node simulator to determine the processing time of I/O requests.
Reference: [BDC91] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. "PRO-TEUS: </author> <title> A High-Performance Parallel-Architecture Simulator." </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <year> 1991. </year>
Reference-contexts: It provides a controlled environment in which the various components of the simulated system can be changed, and their results evaluated. However, sequential simulation of detailed large programs and systems become extremely time consuming <ref> [BDC91, DGH91, CDJ91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation time. Existing parallel program simulators are used to evaluate the performance of memory systems, interconnection networks, or processor architectures. <p> The most common method for simulation of a local event is by direct execution <ref> [BDC91, DGH91, CDJ91] </ref>. The LP, say lp i executes the LCB on the host machine, measuring its duration, say t, and advancing clock i by t. <p> This differs from MPIO-SIM in that we have used real MPI-IO programs from the NAS BTIO Benchmarks to generate our workload. The work done in [Kot97] uses the STARFISH [Kot96] simulator, which is 44 based on Proteus <ref> [BDC91] </ref>, a parallel architecture simulation engine. Proteus itself is a sequential simulator, and a parallel version exists [LW96], but has not been extended to simulate parallel I/O.
Reference: [BDK97] <author> Rajive Bagrodia, Stephen Docy, and Andy Kahn. </author> <title> "Parallel Simulation of Parallel File Systems and I/O Programs." </title> <booktitle> In Proceedings of 1997 Supercomputing Conference, </booktitle> <address> San Jose, CA, </address> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: In this scenario, this layer can simulate a parallel file system's cnodes, ionodes, disks, and caching strategies. This larger, and more complete simulation system is beyond the scope of this thesis. The reader is referred to <ref> [BDK97] </ref> for more details. 36 CHAPTER 5 Experiments and Results 5.1 Benchmarks The IBM SP2 at UCLA was selected both as the target machine as well as the host machine. Each node of the SP2 is a POWER2 node with 128Kb of cache, and 256Mb of main memory. <p> To correctly simulate the effects of disk contention, a more detailed disk model that simulates the queuing and contention at the disk, must be used. A study using this model along with modeling a parallel file system and caching effects can be found in <ref> [BDK97] </ref>. 43 CHAPTER 6 Conclusion In this thesis, we have presented the design and implementation issues of a simulation library for parallel I/O programs which use MPI-IO. Using the simulator, we show good speedups from parallel execution. <p> STARFISH's threaded I/O nodes is most similar to the multi-threaded capabilities of MPIO-SIM's underlying simulation kernel. Although the experiments in this thesis do 45 not create threads other than those of the target program, the detailed parallel file system simulation does <ref> [BDK97] </ref>. In [BBB94], a hybrid methodology for evaluating the performance of parallel I/O subsystems was done. PIOS, a trace-driven I/O simulator, is used to calculate the performance of the I/O system for a subset of the problem to be evaluated, while an analytical model was used for the remainder. <p> The I/O Node model does not correspond to anything in MPIO-SIM. Instead, it more closely resembles part of the detailed parallel file system model as found in <ref> [BDK97] </ref>. Lastly, Panda [SCJ95] provides a high-level collective I/O library interface. It implements server-directed I/O, which is disk-directed I/O at the logical file level, rather than the physical disk level. This method provides a high-level of portability by avoiding the difficult details of utilizing specific attributes for each underlying filesystem. <p> Integration with detailed file system models: Preliminary work which models the Vesta parallel file system [CF96] has already been done in <ref> [BDK97] </ref>, but more study still exists. 4. Implementation of other collective I/O algorithms: The addition of something such as disk-directed I/O would make the simulator more robust. <p> communicator * myrank : my rank in the communicator * fAvail : whether or not this slot is available for a new file handle to use * iolnfo : my ftype IOL * ind fp : my individual file pointer * s fd : the file descriptor used with PFS-SIM <ref> [BDK97] </ref> * aAllIol : array containing the ftype IOL's for all other members of the communicator IOL's are described three places: 1) later in the next section (Section B.4.1), 2) in [BDK97], and 3) in the thesis itself. <p> IOL * ind fp : my individual file pointer * s fd : the file descriptor used with PFS-SIM <ref> [BDK97] </ref> * aAllIol : array containing the ftype IOL's for all other members of the communicator IOL's are described three places: 1) later in the next section (Section B.4.1), 2) in [BDK97], and 3) in the thesis itself. The array of these structures is declared as: MPIO_Fhandle allfh [MAX_ENTITIES_PER_PROCESSOR][MAXFILES]; MAX ENTITIES PER PROCESSOR is the same as before, while MAXFILES indicates the maximum number of open files each thread may have. <p> B.5 Miscellaneous B.5.1 Creating Non-Target Program Threads By default, MPIO-SIM does not need to create any additional threads other than the target threads. However, when using the detailed file system simulation, as found in <ref> [BDK97] </ref>, it is necessary to create non-target threads: threads to represent the compute nodes, ionodes, and disk entities. To do this, one specifies the number of target threads and the number of ionodes on the command line when invoking the program (e.g., target 4 2).
Reference: [CDJ91] <author> R.G. Covington, S. Dwarkadas, J.R. Jump, J.B. Sinclair, and S. Madala. </author> <title> "The Efficient Simulation of Parallel Computer Systems." </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <year> 1991. </year>
Reference-contexts: It provides a controlled environment in which the various components of the simulated system can be changed, and their results evaluated. However, sequential simulation of detailed large programs and systems become extremely time consuming <ref> [BDC91, DGH91, CDJ91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation time. Existing parallel program simulators are used to evaluate the performance of memory systems, interconnection networks, or processor architectures. <p> The most common method for simulation of a local event is by direct execution <ref> [BDC91, DGH91, CDJ91] </ref>. The LP, say lp i executes the LCB on the host machine, measuring its duration, say t, and advancing clock i by t.
Reference: [CF96] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> "The Vesta Parallel File System." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(3) </volume> <pages> 225-264, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Since MPI is still an emerging and rapidly evolving standard, it would be beneficial to implement some of these constructs within the simulator to evaluate their impact. 47 3. Integration with detailed file system models: Preliminary work which models the Vesta parallel file system <ref> [CF96] </ref> has already been done in [BDK97], but more study still exists. 4. Implementation of other collective I/O algorithms: The addition of something such as disk-directed I/O would make the simulator more robust.
Reference: [Cha89] <author> K.M. Chandy and R. Sherman. </author> <title> "The Conditional Event Approach to Distributed Simulation." </title> <booktitle> In Distributed Simulation Conference, </booktitle> <address> Mi-ami, </address> <year> 1989. </year>
Reference-contexts: In the parallel implementation, each processor has its own event-list and a synchronization algorithm must be used to enforce the global ordering. For parallel simulation, the simulation kernel has the capability of using the traditional null-message protocol [Mis86], the conditional event protocol <ref> [Cha89] </ref>, or a combination of both [Jha93]. 4.3 MPISIM The MPISIM layer provides simulation of all MPI communication. This includes all the various point-to-point communication calls as well collective communication. We assume that communication between LP's is FIFO (First In, First Out).
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> "Improved Parallel I/O via a Two-Phase Run-time Access Strategy." </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel 67 Computer Systems, </booktitle> <pages> pp. 56-70, </pages> <address> Newport Beach, CA, </address> <year> 1993. </year> <note> Also pub-lished in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: There are three parameters for the target of collective buffering: 1) the number and/or identity of the target nodes, 2) the amount of target buffer space, and 3) the target data distribution. Collective buffering is best seen in Two-Phase I/O <ref> [dBC93] </ref>. In the first phase, data is permuted according to a conforming distribution. This is a permutation of the data across the cnode processes so that it coincides with the underlying file layout (the target data distribution as 12 mentioned above). The actual I/O is performed in the second phase. <p> This is a permutation of the data across the cnode processes so that it coincides with the underlying file layout (the target data distribution as 12 mentioned above). The actual I/O is performed in the second phase. Collective buffering has shown orders-of-magnitude performance improvement <ref> [dBC93, MPI96b] </ref>. In general, it does not require operating system support since the cnodes are doing the permuting and optimizing of I/O requests. Once permuted, cnodes simply make their requests to their designated ionode (s). This makes implementation easier than disk-directed I/O.
Reference: [DGH91] <author> H. Davis, S. R. Goldschmidt, and Hennessey. </author> <title> "Multiprocessor simulation and tracing using Tango." </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (ICPP'91), </booktitle> <pages> pp. </pages> <address> II99-II107, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: It provides a controlled environment in which the various components of the simulated system can be changed, and their results evaluated. However, sequential simulation of detailed large programs and systems become extremely time consuming <ref> [BDC91, DGH91, CDJ91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation time. Existing parallel program simulators are used to evaluate the performance of memory systems, interconnection networks, or processor architectures. <p> The most common method for simulation of a local event is by direct execution <ref> [BDC91, DGH91, CDJ91] </ref>. The LP, say lp i executes the LCB on the host machine, measuring its duration, say t, and advancing clock i by t.
Reference: [f2c90] <author> S.I.Feldman, David M. Gay, Mark W. Maimone, and N.L.Schryer. </author> <title> "A Fortran-to-C Converter." </title> <type> Technical report no. 149, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ 07974, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Both benchmarks exhibit numerous seek operations. The NAS BTIO Benchmarks are written in Fortran-77 using MPI and MPI-IO calls. Since MPIO-SIM currently only supports C programs, the benchmarks's Fortran code were successfully converted using f2c <ref> [f2c90] </ref>. The third benchmark we have used is a rudimentary out-of-core matrix multiplication application. The out-of-core algorithm itself was not written for efficiency, but rather, only to provide numerous reads and writes within the same program.
Reference: [Fuj90] <author> R. Fujimoto. </author> <title> "Parallel Discrete event simulation." </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: As a result, standard interfaces have been proposed, and one such interface is MPI-IO, an extension to the current MPI standard. 1.2 Parallel Simulation Overview Parallel (or distributed) discrete-event simulation refers to the execution of a discrete-event simulation on a parallel (or distributed) architecture <ref> [Fuj90] </ref>. Interest in exploiting parallelism has increased in a number of application areas: network simulations, computer architectures, digital circuits, and parallel programs. This is due in part by the greater availability of systems which are relatively lower in cost, yet still achieve good performance.
Reference: [Fuj93] <author> R. Fujimoto. </author> <title> "Parallel Discrete event simulation: </title> <journal> Will the field survive?" ORSA Journal on Computing, </journal> <volume> 5(3) </volume> <pages> 213-230, </pages> <year> 1993. </year>
Reference-contexts: This is due in part by the greater availability of systems which are relatively lower in cost, yet still achieve good performance. However, parallel simulation is still primarily used in the research community <ref> [Fuj93] </ref>. A major limiting factor for more widespread use is the complexity of implementing efficient parallel simulations. Nonetheless, the potential for significant speedup in the execution time of a simulation has led to the design of several algorithms for parallel simulation. They are broadly classified as either optimistic or conservative.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> "High Performance Fortran Language Specification." </title> <note> Available by anonymous ftp from titan.cs.rice.edu, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The experiments performed varies the file-access patterns, disk layout, number of cnodes, number of ionodes, and number of disks. Although the workload was synthetic, performance evaluation for the collective I/O schemes is based on the array distribution possibilities supported in High-Performance Fortran <ref> [Hig93] </ref>. This differs from MPIO-SIM in that we have used real MPI-IO programs from the NAS BTIO Benchmarks to generate our workload. The work done in [Kot97] uses the STARFISH [Kot96] simulator, which is 44 based on Proteus [BDC91], a parallel architecture simulation engine.
Reference: [Jha93] <author> Vikas Jha and Rajive Bagrodia. </author> <title> "Transparent Implementation of Conservative Algorithms in Parallel Simulation Languages." </title> <booktitle> In Winter Simulation Conference, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: In the parallel implementation, each processor has its own event-list and a synchronization algorithm must be used to enforce the global ordering. For parallel simulation, the simulation kernel has the capability of using the traditional null-message protocol [Mis86], the conditional event protocol [Cha89], or a combination of both <ref> [Jha93] </ref>. 4.3 MPISIM The MPISIM layer provides simulation of all MPI communication. This includes all the various point-to-point communication calls as well collective communication. We assume that communication between LP's is FIFO (First In, First Out).
Reference: [Kof95] <author> Stig Kofoed. </author> <title> "Portable Multitasking in C." </title> <journal> Dr. Dobb's Journal, </journal> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: In the simulator, there is no explicit function to create new threads (e.g., thread create ()). Threads are created using the method as outlined in <ref> [Kof95] </ref>, using setjmp () and longjmp (). Each thread is given an id number, which determines the function where each thread begins execution. For example, let's say we used target 4 2. This specifies 4 target threads and 2 ionodes.
Reference: [Kot91] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: There are multiple ways to organize multiple disks in order to achieve greater parallelism. We will use the following terms <ref> [Kot91] </ref>: * traditional: A traditional (sequential) file system may use more than one disk, but places each file entirely on one disk.
Reference: [Kot96] <author> David Kotz. </author> <title> "Tuning STARFISH." </title> <type> Technical Report PCS-TR96-296, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: This differs from MPIO-SIM in that we have used real MPI-IO programs from the NAS BTIO Benchmarks to generate our workload. The work done in [Kot97] uses the STARFISH <ref> [Kot96] </ref> simulator, which is 44 based on Proteus [BDC91], a parallel architecture simulation engine. Proteus itself is a sequential simulator, and a parallel version exists [LW96], but has not been extended to simulate parallel I/O.
Reference: [Kot97] <author> David Kotz. </author> <title> "Disk-directed I/O for MIMD Multiprocessors." </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Finally, at the highest level of parallelism is the language or algorithmic level. 8 The language hides the algorithm (described in the next section) and provides an interface to the parallel file system. There are many types of interfaces <ref> [Kot97] </ref>, but for the purposes of this thesis, we focus on MPI-IO. MPI-IO provides each processor its own "view" of the file, where the file data itself could be noncontiguous, but appears logically contiguous to the processor. <p> A straight-forward implementation of these I/O requests using current parallel I/O interfaces leads to uncoordinated accesses among the cnodes. This results in abysmal performance [NF95]. Two important pieces of information has been lost 9 in the process <ref> [Kot97] </ref>: 1) each cnode's request is actually part of a larger data transfer with other cnodes, and 2) all cnodes are collectively participating in the I/O transfer. Collective I/O algorithms are a solution to this problem. <p> A better solution would be to have the system automatically try to determine the best group size (or partioning scheme). An alternative to grouping is Disk-directed I/O <ref> [Kot97] </ref>. Disk-directed I/O tries to solve the problem of resource contention through minimizing disk access time by reducing the number of disk accesses and reducing the seek time at every access. One cnode collects all requests from all other cnodes, then sends these requests to all ionodes. <p> Although disk-directed I/O consistently performs better than non-collective I/O, it is harder to implement since optimizing requests at the ionode requires knowledge of the physical disk layout as well as support from the operating system. 11 From the experiments and simulations done in <ref> [Kot97] </ref>, disk-directed I/O achieved performance close to the limits of the disk hardware. This is due in part by its ability to optimize I/O requests at the ionodes. <p> Also, when communication is a limiting factor, the optimal I/O pattern for the ionodes is not the optimal communication pattern. This leads to network congestion <ref> [Kot97] </ref>. 2.4.2 Collective Buffering Collective buffering also tries to reduce resource contention by permuting data prior to performing the I/O operations in order to minimize the number of requests. <p> We have also shown the usefulness and extensibility of the simulator by evaluating the impact of alternative implementations of collective I/O on real MPI-IO programs, including benchmarks from the NAS BTIO Parallel I/O Benchmark suite. 6.1 Related Work Three approaches to collective I/O are evaluated in <ref> [Kot97] </ref>: traditional caching, two-phase I/O, and disk-directed I/O. Traditional caching mimics a traditional parallel file system, and does no collective I/O optimizations, since I/O requests are served as they arrive. The experiments performed varies the file-access patterns, disk layout, number of cnodes, number of ionodes, and number of disks. <p> Although the workload was synthetic, performance evaluation for the collective I/O schemes is based on the array distribution possibilities supported in High-Performance Fortran [Hig93]. This differs from MPIO-SIM in that we have used real MPI-IO programs from the NAS BTIO Benchmarks to generate our workload. The work done in <ref> [Kot97] </ref> uses the STARFISH [Kot96] simulator, which is 44 based on Proteus [BDC91], a parallel architecture simulation engine. Proteus itself is a sequential simulator, and a parallel version exists [LW96], but has not been extended to simulate parallel I/O.
Reference: [LW96] <author> U. Legedza and W. E. Weihl. </author> <title> "Reducing Synchronization Overhead in Parallel Simulation." </title> <booktitle> In 10th Workshop on Parallel and Distributed Simulation (PADS96), </booktitle> <pages> pp. 86-95, </pages> <month> May </month> <year> 1996. </year> <month> 68 </month>
Reference-contexts: The work done in [Kot97] uses the STARFISH [Kot96] simulator, which is 44 based on Proteus [BDC91], a parallel architecture simulation engine. Proteus itself is a sequential simulator, and a parallel version exists <ref> [LW96] </ref>, but has not been extended to simulate parallel I/O.
Reference: [Mis86] <author> J. Misra. </author> <title> "Distributed Discrete-Event Simulation." </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: In the parallel implementation, each processor has its own event-list and a synchronization algorithm must be used to enforce the global ordering. For parallel simulation, the simulation kernel has the capability of using the traditional null-message protocol <ref> [Mis86] </ref>, the conditional event protocol [Cha89], or a combination of both [Jha93]. 4.3 MPISIM The MPISIM layer provides simulation of all MPI communication. This includes all the various point-to-point communication calls as well collective communication. We assume that communication between LP's is FIFO (First In, First Out).
Reference: [MPI93] <author> MPI Forum. </author> <title> "MPI: A Message Passing Interface." </title> <booktitle> In Proceedings of 1993 Supercomputing Conference, </booktitle> <address> Portland, Washington, </address> <month> November </month> <year> 1993. </year> <title> [MPI96a] "MPI-IO: A Parallel File I/O Interface for MPI." The MPI-IO Committee, </title> <month> April </month> <year> 1996. </year> <note> Version 0.5. See WWW http://lovelace.nas.nasa.gov/MPI-IO/mpi-io-report.0.5.ps. </note>
Reference-contexts: In the next chapter, we present and describe a simulator for MPI-IO, a portable interface for parallel I/O programming. 13 CHAPTER 3 MPI and MPI-IO For the purposes of this thesis, MPI is a library of functions which provide a standard interface for portable parallel programming <ref> [MPI93] </ref>. The programming paradigm in MPI is based upon message passing as the communication method between processes. MPI-IO is a proposed extension to MPI to incorporate parallel I/O constructs [MPI96a].
Reference: [MPI96b] <author> PMPIO-A Portable Implementation of MPI-IO. "Samuel A. Fineberg and Parkson Wong and Bill Nitzberg and Chris Kuszmaul." </author> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: This is a permutation of the data across the cnode processes so that it coincides with the underlying file layout (the target data distribution as 12 mentioned above). The actual I/O is performed in the second phase. Collective buffering has shown orders-of-magnitude performance improvement <ref> [dBC93, MPI96b] </ref>. In general, it does not require operating system support since the cnodes are doing the permuting and optimizing of I/O requests. Once permuted, cnodes simply make their requests to their designated ionode (s). This makes implementation easier than disk-directed I/O. <p> MPIO-SIM maintains its own internal structure to represent datatypes. This structure consists of a list of base offsets and the length (number of bytes) of valid data at that offset. This structure is similar to I/O lists, as found in PMPIO, NAS's implementation of MPI-IO <ref> [MPI96b] </ref>, as well as type maps, as found in the MPI-IO implementation for the NEC Cenju-3 [SPB96]. We will refer to this 27 structure simply as an IOL. In MPI-IO, opening a file is a collective operation.
Reference: [NAS95] <author> D. Bailey, T. Harris, W. Saphir, R. v.d. Wijngaart, A. Woo, and M. Yarrow. </author> <title> "The NAS Parallel Benchmarks 2.0." </title> <type> Technical report nas-95-020, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA 94035-1000, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The workload for our experiments was provided by using three MPI/MPI-IO programs. The first two programs are from the NAS (Numerical Aerodynamic Simulation) BTIO Parallel I/O Benchmarks v0.1 1 . These I/O benchmarks are based on the BT benchmark from the NAS Parallel Benchmarks v2.0 <ref> [NAS95] </ref>, which do not have any I/O. The BT benchmark simulates computational fluid dynamics (CFD) applications that solve systems of equations resulting from an approximately factored implicit finite-difference discretization of the Navier-Stokes equations.
Reference: [NF95] <author> Bill Nitzberg and Samuel A. Fineberg. </author> <title> "Parallel I/O on Highly Parallel Systems| Supercomputing '95 Tutorial M6 Notes." </title> <type> Technical Report NAS-95-022, </type> <institution> NASA Ames Research Center, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The application distributes its data across each cnode's main memory to increase parallelism, improve load balancing, and minimize communication overhead <ref> [NF95] </ref>. Each cnode operates on its own piece of data, so from a file system's point of view, each cnode makes its own requests to read or write data. <p> The problem here is that "good" data distributions for the application tends to lead to small data blocks and non-sequential access to the data [NKP96]. A straight-forward implementation of these I/O requests using current parallel I/O interfaces leads to uncoordinated accesses among the cnodes. This results in abysmal performance <ref> [NF95] </ref>. Two important pieces of information has been lost 9 in the process [Kot97]: 1) each cnode's request is actually part of a larger data transfer with other cnodes, and 2) all cnodes are collectively participating in the I/O transfer. Collective I/O algorithms are a solution to this problem.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> "Performance of the iPSC/860 Concurrent File System." </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In data grouping, the data is partitioned and I/O requests are made according to the partitioned data. Advantages of grouping are: * An 8-times performance increase was achieved on the iPSC/860 CFS <ref> [Nit92] </ref> using grouping. This specific instance is an example which shows potential for grouping. * There is no need to determine the cause of the thrashing. <p> speedup was achieved in the parallel simulation. 5.3 Effect of Collective I/O Using a simple disk model, Figure 5.3 shows a comparison of the execution time as predicted by the simulator between the different collective I/O implementations. 40 Although node grouping shows up to 8-times improvement in one particular case <ref> [Nit92] </ref>, it has performed very poorly for the NAS Benchmark. This is mostly due to the fact that this particular problem size of the NAS Benchmark does not generate enough I/O requests to flood the interconnection network.
Reference: [NK96] <author> Nils Nieuwejaar and David Kotz. </author> <title> "The Galley Parallel File System." </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pp. 374-381, </pages> <address> Philadelphia, PA, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The simple disk model can be replaced with a more detailed disk model, such as the one found in <ref> [NK96] </ref>. For maximum detail, an additional simulation component for the target architecture's file system can be used. In this scenario, this layer can simulate a parallel file system's cnodes, ionodes, disks, and caching strategies. This larger, and more complete simulation system is beyond the scope of this thesis.
Reference: [NKP96] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> "File-Access Characteristics of Parallel Scientific Workloads." </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Each cnode operates on its own piece of data, so from a file system's point of view, each cnode makes its own requests to read or write data. However, this data is usually not logically contiguous in the file <ref> [NKP96] </ref>, hence a separate file system request must be made for each contiguous portion. Consequently, the file system receives numerous, concurrent small requests from many processors instead of a single, large request that would have occurred on a uniprocessor. <p> The problem here is that "good" data distributions for the application tends to lead to small data blocks and non-sequential access to the data <ref> [NKP96] </ref>. A straight-forward implementation of these I/O requests using current parallel I/O interfaces leads to uncoordinated accesses among the cnodes. This results in abysmal performance [NF95].
Reference: [PGK88] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> "A case for redundant arrays of inexpensive disks (RAID)." </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pp. 109-116, </pages> <address> Chicago, IL, June 1988. </address> <publisher> ACM Press. </publisher> <pages> 69 </pages>
Reference-contexts: To address this problem, several solutions have been proposed to increase I/O performance. Parallelization occurs at various levels. The two most commonly found and most successful are: Low level parallelization, as found in RAID <ref> [PGK88] </ref>, parallelizes I/O by dividing data blocks into smaller subblocks and storing the subblocks in a disk array. The idea here is twofold: 1) to speed up transfers through reading and writing to the disk array in parallel, and 2) to improve reliability by introducing redundancy. <p> However, although processor speed has increased, the input/output (I/O) subsystem has always been orders of magnitudes slower and has not shown as much improvement relative to processor speed. This widening gap in access-time is known as the I/O crisis <ref> [PGK88] </ref>. This problem may not be too apparent for the general computer user, but for numerous scientific applications, the effects can be devastating. These applications which require high-performance I/O include weather simulation/forcasting, computational fluid dynamics, seismic data processing, and tactical simulation. <p> Arranging the disks in various manners allows different data placement and data access among the disks. A great deal of previous work involves disk striping, where the file is interleaved across multiple disks and accessed in parallel to simultaneously read/write blocks of the file. The RAID group at Berkeley <ref> [PGK88] </ref> proposed and studied an organization consisting of redundant arrays of inexpensive disks. Depending on the configuration, multiple disks can be used for redundancy and fault-tolerance to increase reliability, or they can be used to stripe data across the disks for higher performance.
Reference: [Pra96] <author> Sundeep Prakash. </author> <title> Performance Prediction of Parallel Programs. </title> <type> Ph.d. dissertation, </type> <institution> Computer Science Dept, UCLA, </institution> <address> Los Angeles, CA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The simulation kernel provides sequential simulation for use on a uniprocessor, as well as parallel simulation using multiple synchronization protocols for a parallel architecture <ref> [Pra96] </ref>. * MPISIM: The simulation of MPI-IO commands requires the ability to simulate MPI communication commands. MPISIM is a multi-threaded MPI simulator that was developed for this purpose [Pra96]. * PIO-SIM: Simulates a subset of the individual and collective I/O constructs provided by MPI-IO. <p> kernel provides sequential simulation for use on a uniprocessor, as well as parallel simulation using multiple synchronization protocols for a parallel architecture <ref> [Pra96] </ref>. * MPISIM: The simulation of MPI-IO commands requires the ability to simulate MPI communication commands. MPISIM is a multi-threaded MPI simulator that was developed for this purpose [Pra96]. * PIO-SIM: Simulates a subset of the individual and collective I/O constructs provided by MPI-IO. PIO-SIM supports all operations except calls using the shared file pointer. <p> All point-to-point communication calls are implemented in terms four functions which comprise a core. This core set of functions consists of MPI Issend, MPI Ibsend, MPI Irecv, and MPI Wait <ref> [Pra96] </ref>. Collective communication is implemented as a set of point-to-point communication. For example, an MPI Bcast (broadcast) happens in two steps: First, (a) each process dynamically configures a tree which contains all processes in the communicator. The process's own position in the tree is determined by its rank. <p> In this situation, the information relevant to MPISIM will be made clear and described in the context of MPIO-SIM. For an overall description of the simulation model of MPISIM, please refer to <ref> [Pra96] </ref>. MPIO-SIM is based on v0.5 of the MPI-IO specification [MPI96a]. Thus, all syntax, most functionality, and most semantics correspond with v0.5. B.2 Datatypes When sending and receiving messages in MPI as well as reading and writing data in MPI-IO, there is concept of a datatype.
Reference: [SCJ95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> "Server-Directed Collective I/O in Panda." </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, December 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The I/O Node model does not correspond to anything in MPIO-SIM. Instead, it more closely resembles part of the detailed parallel file system model as found in [BDK97]. Lastly, Panda <ref> [SCJ95] </ref> provides a high-level collective I/O library interface. It implements server-directed I/O, which is disk-directed I/O at the logical file level, rather than the physical disk level. This method provides a high-level of portability by avoiding the difficult details of utilizing specific attributes for each underlying filesystem.
Reference: [SPB96] <author> Darren Sanders, Yoonho Park, and Maciej Brodowicz. </author> <title> "Implementation and performance of MPI-IO file access using MPI datatypes." </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This structure is similar to I/O lists, as found in PMPIO, NAS's implementation of MPI-IO [MPI96b], as well as type maps, as found in the MPI-IO implementation for the NEC Cenju-3 <ref> [SPB96] </ref>. We will refer to this 27 structure simply as an IOL. In MPI-IO, opening a file is a collective operation. Upon invocation, each target LP creates an IOL to represent its filetype (ftype IOL). Afterwards, each target LP exchanges its ftype IOL with all other target LP's.
References-found: 32

