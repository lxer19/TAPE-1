URL: ftp://ftp.idiap.ch/pub/papers/neural/moerland.me.aposteriori.ps.Z
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: Mixtures of Experts Estimate A Posteriori Probabilities  
Author: Perry Moerland 
Address: CP 592, 1920 Martigny, Switzerland  
Affiliation: IDIAP,  
Abstract: The mixtures of experts (ME) model offers a modular structure suitable for a divide-and-conquer approach to pattern recognition. It has a probabilistic interpretation in terms of a mixture model, which forms the basis for the error function associated with MEs. In this paper, it is shown that for classification problems the minimization of this ME error function leads to ME outputs estimating the a posteriori probabilities of class membership of the input vector. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Christopher M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: This soft-max function makes that the gating network outputs sum to unity and are non-negative; thus implementing the (soft) competition between the experts. A probabilistic interpretation of a ME can be given in the context of mixture models for conditional probability distributions (see section 6.4 in <ref> [1] </ref>): p (tjx) = j=1 where the OE j represent the conditional densities of target vector t for expert j. <p> ME error function which can be optimized using gradient descent or the Expectation-Maximization (EM) algorithm [7]. 3 Estimating Posterior Probabilities A standard way to motivate error functions is from the principle of maximum likelihood of the (independently distributed) training data fx n ; t n g (see section 6.1 in <ref> [1] </ref>): Y p (x n ; t n ) = n A cost function is then obtained by taking the negative logarithm of the likelihood (and dropping the term p (x n ) which does not depend on the network parameters): E = n The most suitable choice for the conditional <p> The solution of these equations will then result in expressions for g j (x) and y j (x) at the minimum of E (along the lines of section 6.1.3 of <ref> [1] </ref> for the sum-of-squares error function). Defining: E 0 = ln j=1 we are then interested in the following two functional derivatives set to zero. <p> For the gating network: ffiE ffiz j = @E 0 and for the expert network (using the chain rule): ffiE ffia jc = @E 0 Z X @E 0 @y jk p (tjx)p (x)dt = 0: (9) In section 6.4 of <ref> [1] </ref>, the partial derivatives for the gating network occurring in (8) have been calculated in the context of a gradient descent algorithm for the mixture model (3). <p> This is exactly the same as for the outputs of a network trained by minimizing the sum-of-squares or cross-entropy error functions <ref> [1] </ref>. It is a well-known result that for a classification problem with 1-of-c coding the conditional average of the target data is (see, for example, section 6.6 in [1]) : y c (x) = P (C c jx); so that the outputs of a ME do indeed estimate the a posteriori <p> This is exactly the same as for the outputs of a network trained by minimizing the sum-of-squares or cross-entropy error functions <ref> [1] </ref>. It is a well-known result that for a classification problem with 1-of-c coding the conditional average of the target data is (see, for example, section 6.6 in [1]) : y c (x) = P (C c jx); so that the outputs of a ME do indeed estimate the a posteriori probability that x belongs to class C c . 4 Discussion In section 3, it was assumed that the conditional density OE j (t n jx n )
Reference: 2. <author> H. Bourlard and N. Morgan. </author> <title> Links between Markov models and multi-layer per-ceptrons. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 502-510, </pages> <address> San Mateo CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction It is well-known that for artificial neural networks trained by minimizing sum-of-squares or cross-entropy error functions for a classification problem, the network outputs approximate the a posteriori probabilities of class membership <ref> [2] </ref>. This property is a very useful one, especially when the network outputs are to be used in a further decision-making stage (e.g. rejection thresholds) or integrated in other statistical pattern recognition methods (as in hybrid NN-HMMs).
Reference: 3. <author> J. S. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman Soulie and J. Herault, editors, Neurocomputing: </editor> <booktitle> Algorithms, Architectures, and Applications, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In order to ensure this probabilistic interpretation, the activation function for the outputs of the gating network is chosen to be the soft-max function <ref> [3] </ref>: g j = P m ; (2) where the z i are the gating network outputs before thresholding. This soft-max function makes that the gating network outputs sum to unity and are non-negative; thus implementing the (soft) competition between the experts.
Reference: 4. <author> Jurgen Fritsch, Michael Finke, and Alex Waibel. </author> <title> Context-dependent hybrid HME/HMM speech recognition using polyphone clustering decision trees. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <year> 1997. </year>
Reference: 5. <author> Mariano Giaquinta and Stefan Hildebrandt. </author> <title> Calculus of Variations. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: an integral: E = ln @ j=1 1 A p (t; x) dtdx; factoring the joint distribution: E = ln @ j=1 1 A p (tjx)p (x) dtdx: The interpretation of the ME outputs when this error function is minimized, can be obtained by setting to zero the functional derivatives <ref> [5] </ref> of E with respect to the gating network outputs z j (x) and the network outputs of expert j, a jc (x).
Reference: 6. <author> Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: 7. <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: Architecture of a mixture of experts network. networks and one gating network both having access to the input vector x; the gating network has one output g i per expert. The standard choices for gating and expert networks are generalized linear models <ref> [7] </ref> and multilayer perceptrons [9]. The output vector of a ME is the weighted (by the gating network outputs) mean of the expert outputs: y (x) = j=1 The gating network outputs g j (x) can be regarded as the probability that input x is attributed to expert j. <p> soft-max function in the gating network and the fact that the OE j are densities guarantee that the distribution is normalized: R As outlined in the next section this distribution forms the basis for the ME error function which can be optimized using gradient descent or the Expectation-Maximization (EM) algorithm <ref> [7] </ref>. 3 Estimating Posterior Probabilities A standard way to motivate error functions is from the principle of maximum likelihood of the (independently distributed) training data fx n ; t n g (see section 6.1 in [1]): Y p (x n ; t n ) = n A cost function is then
Reference: 8. <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Classification using hierarchical mixtures of experts. </title> <booktitle> In Proceedings 1994 IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 177-186, </pages> <address> Long Beach CA, 1994. </address> <publisher> IEEE Press. </publisher>
Reference: 9. <author> Andreas S. Weigend, Morgan Mangeas, and Ashok N. Srivastava. </author> <title> Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 373-399, </pages> <year> 1995. </year>
Reference-contexts: In particular, the gating network of a ME learns to partition the input space (in a soft way, so overlaps are possible) and attributes expert networks to these different regions. The divide-and-conquer approach has shown particularly useful in attributing experts to different regimes in piece-wise stationary time series <ref> [9] </ref> and modeling discontinuities in the input-output mapping. <p> Architecture of a mixture of experts network. networks and one gating network both having access to the input vector x; the gating network has one output g i per expert. The standard choices for gating and expert networks are generalized linear models [7] and multilayer perceptrons <ref> [9] </ref>. The output vector of a ME is the weighted (by the gating network outputs) mean of the expert outputs: y (x) = j=1 The gating network outputs g j (x) can be regarded as the probability that input x is attributed to expert j.
References-found: 9

