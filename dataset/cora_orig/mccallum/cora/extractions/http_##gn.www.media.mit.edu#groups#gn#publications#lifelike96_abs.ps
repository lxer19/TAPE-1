URL: http://gn.www.media.mit.edu/groups/gn/publications/lifelike96_abs.ps
Refering-URL: http://gn.www.media.mit.edu/groups/gn/publications.html
Root-URL: http://www.media.mit.edu
Email: kris@media.mit.edu, justine @media.mit.edu  
Title: Abstract from the Lifelike Computer Characters conference, Snowbird, Utah, 1996 WHY PUT AN AGENT IN
Author: Kristinn Thorisson and Justine Cassell 
Address: 20 Ames Street Cambridge, MA 02139  
Affiliation: MIT Media Lab  
Abstract: Although many different human characteristics have been put forth as the key to making humanoid agents lifelike (eg. emotional expression, fluid body movement, face and hand gestures, realistic skin color) the young field of synthetic computer characters has not seen much research comparing these different putatively most important characteristics of lifelike computer characters. Of course, research on the effectiveness of natural language based, humanoid agent systems, and on the role of believability in the construction of such systems, has to date been hampered by the lack of real computer systems capable of sustaining and supporting spoken dialogue with a human user. In this paper we describe a comparison of two commonly discussed features: emotional facial icons and nonverbal communicative behavior. This comparison was made possible by a platform that supports the construction of humanoid agents and allows various features of those agents to be turned off. We used a fully automated character generation system, capable of real-time, multimodal, faceto-face interaction with a user [Thrisson 1996], to assess users reactions to two commonly discussed human characteristics: facial emotional icons and nonverbal feedback about the interaction. We tested users reactions by way of a questionnaire assessing comfort with the interaction, but also by looking at the efficiency of the interaction, as measured by how many times users repeated themselves. Specifically, we compared users questionnaire responses to, and efficiency with a -1- content-only character (CONT), -2- a content + emotional facial icons character (EMO), and -3- a content + nonverbal communicative support character (ENV). The characters all appear on a normal-sized monitor beside a big screen projector, on which a graphical model of the solar system is displayed. The users can ask the character questions about planets and have it show them the planets. The characters in each condition are equally knowledgeable about the solar system, and their responses are equally rapid, but they provide the following different feedback: In the CONT condition the character gives verbal feedback only relating to the content of the dialogue; the EMO character gives the same verbal feedback and also smiles occasionally, when it has finished some action, and looks puzzled if it doesn't understand what the user says; the ENV character provides the same verbal feedback as CONT with the addition of behaviors relating to the process of dialogue: turning to and/or looking at the big screen or the user at the right times, giving nonverbal feedback to show when it decides to take the turn (when the user has finished making a request), and hand gestures that support its utterances (beat gestures and pointing at the planets when speaking). It also blinks, and drums with its fingers when its hand is at rest. The experiment was a repeated-measures design, with twelve subjects. Thus, all subjects interacted with all three characters. Two hypotheses were tested: -1- We expected to find no significant difference in ease of interaction or efficiency between the CONT and EMO conditions. That is, we didnt expect emotional facial icons to add anything to the interaction. -2- We expected to find a significant difference in ease and efficiency between the ENV condition and the other two conditions. In other words, we expected behaviors relating to the process of dialogue to 
Abstract-found: 1
Intro-found: 0
Reference: <author> Ekman, P. </author> <title> (1979) "About Brows: Emotional and Conversational Signals." </title> <editor> In M. von Crahach, K. Foppa, W. Lepenies & D. Ploog (eds.), </editor> <booktitle> Human Ethology, </booktitle> <pages> pp. 169-243. </pages>
Reference: <author> Hauptman, </author> <title> A.G. (1989) "Speech and Gesture for Graphic Image Manipulation." </title> <booktitle> In Proceedings of SIGCHI '89, </booktitle> <pages> pp. 241-245. </pages>
Reference: <author> Maes, P. </author> <title> (1994) "Agents that Reduce Work and Information Overload." </title> <journal> Communications of the ACM, </journal> <volume> 37(7), </volume> <pages> pp. 31-40, 146. </pages>
Reference: <author> Maulsby, D., D. Greenberg & R. Mandler. </author> <title> (1993) "Prototyping an Intelligent Agent through Wizard of Oz." </title> <booktitle> In Proceedings of InterCHI '93, </booktitle> <address> Amsterdam, </address> <month> April 24-29, </month> <pages> pp. 277-284. </pages>
Reference: <author> Ochsman, R.B. & A. Chapanis. </author> <title> (1974) "The Effects of 10 Communication Modes on the Behavior of Teams During Cooperative Problem Solving." </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 6, </volume> <pages> pp. 579-619. </pages>
Reference: <author> Thorisson, K.R. </author> <title> (1996) "Dialogue Control in Social Interface Agents." </title> <booktitle> In InterCHI Adjunct Proceedings '93, </booktitle> <address> Amsterdam, </address> <month> April 24-29, </month> <pages> pp. 876-881. </pages>
Reference: <author> Thorisson, K.R. </author> <title> (1996) "Communicative Humanoids: A Computational Model of Psychosocial Dialogue Skills." </title> <type> Doctoral Dissertation, </type> <institution> Massachusetts Institute of Technology, Media Laboratory, </institution> <month> September </month> <year> 1996. </year>
References-found: 7

