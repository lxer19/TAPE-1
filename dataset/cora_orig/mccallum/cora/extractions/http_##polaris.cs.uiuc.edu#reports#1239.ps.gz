URL: http://polaris.cs.uiuc.edu/reports/1239.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: A General Framework for Analyzing Shared-Memory Parallel Programs  
Author: Jyh-Herng Chow and Williams Ludwell Harrison III 
Address: Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Explicit parallelism greatly complicates the program semantics, especially if concurrent activities are allowed to interact with each other through shared variables. Existing compiler analysis and optimization techniques for sequential programs must be carefully examined before they can be safely applied to parallel programs. In this paper, we present a general framework for analyzing programs with cobegin parallelism, which is based on state space exploration. Reducing the explosion of state space is necessary to make compile-time analysis feasible. This state space explosion problem is relieved first by eliminating redundant interleaving, through the stubborn set theory. Then, abstract interpretation techniques are employed which provide systematic methods for folding related states for further state space reduction. With this framework, we have developed static analysis for obtaining program properties, such as side effects, data dependences, and object lifetimes. The information obtained facilitates program optimization, restructuring, and memory management. 
Abstract-found: 1
Intro-found: 1
Reference: [AH87] <editor> Samson Abramsky and Chris Hankin. </editor> <title> Abstract Interpretation of Declarative Languages. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1987. </year>
Reference-contexts: To fully understand this technique, one needs to know domain theories in formal program semantics. Thus, we only give a brief introduction in this section. A good source of further information on this topic can be found in <ref> [AH87] </ref> or [Bur91]. The way that abstract interpretation works is the following. First, we define a formal semantics, called standard semantics, for the given language. The standard semantics defines the meaning of a program, which all the later semantics must preserve.
Reference: [BHA86] <author> G.L. Burn, C.L. Hankin, and S. Abramsky. </author> <title> The Theory and Practice of Strictness Analysis for Higher Order Functions. In Programs as Data Objects, </title> <booktitle> volume 217 of Lecture Notes in Computer Science, </booktitle> <pages> pages 43-62. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis <ref> [BHA86] </ref>, sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91].
Reference: [Bur87] <author> G. L. Burn. </author> <title> Abstract Interpretation and the Parallel Evaluation of Functional Languages. </title> <type> PhD thesis, </type> <institution> University of London, </institution> <year> 1987. </year>
Reference-contexts: Because abstract interpretation is based on a formal mathematic/semantic model, the correctness of analysis (i.e., consistent with the language semantics) can be proved formally and easily if we follow some existing framework (e.g., <ref> [Bur87] </ref>). 4 A Framework The framework we will describe is based on abstract interpretation. Given a language L, we first define a formal semantics S = hD; Ei for it, where D is the set of semantic domains and E is the semantic function.
Reference: [Bur91] <author> Geoffery Burn. </author> <title> Lazy Functional Languages: Abstract Interpretation and Compilation. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: To fully understand this technique, one needs to know domain theories in formal program semantics. Thus, we only give a brief introduction in this section. A good source of further information on this topic can be found in [AH87] or <ref> [Bur91] </ref>. The way that abstract interpretation works is the following. First, we define a formal semantics, called standard semantics, for the given language. The standard semantics defines the meaning of a program, which all the later semantics must preserve. It also defines what is meant by a correct analysis.
Reference: [CC77] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract Interpretation: a Unified Lattice Model for Static Analysis of Program by Construction or Approximation of Fixpoints. </title> <booktitle> In ACM 4th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <year> 1977. </year>
Reference-contexts: Our framework is based on state space exploration and abstract interpretation <ref> [CC77] </ref>. Like other state space generation based analysis, the most important question is how to relieve the problem of state space explosion. We approach this problem in two ways. First, stubborn set theory [Val88, Val89, Val90] is applied to eliminate redundant inter-leavings. <p> The next section describes the abstract interpretation technique that allows further state space reduction and also forms a framework of our analysis. 3 Abstract Interpretation This work takes a semantic-based approach to the program analysis problem by using abstract interpretation <ref> [CC77] </ref>. To fully understand this technique, one needs to know domain theories in formal program semantics. Thus, we only give a brief introduction in this section. A good source of further information on this topic can be found in [AH87] or [Bur91]. <p> The seminal paper on abstract interpretation is <ref> [CC77] </ref>, which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [CC80] <author> Patrick Cousot and Radhia Cousot. </author> <title> Semantic Analysis of Communicating Sequential Processes. </title> <booktitle> In International Conference on Automata, Languages and Programming, volume 85 of Lecture Notes in Computer Science, </booktitle> <pages> pages 119-133, </pages> <year> 1980. </year>
Reference-contexts: This semantics essentially disallows any interaction through shared variables among concurrent activities, for the purpose of simplifying the effort of compiler analysis. Data flow analysis for communicating processes where process interaction occurs only through message passing can be found in Reif and Smolka [RS90] and Cousot and Cousot <ref> [CC80] </ref>. 9 Conclusion As multiprocessor systems become popular and easily accessible, there are reasons for pursuing parallel programming. However, parallel programs are often hard to understand and debug. A compiler must consider every possible interaction among concurrent activities, in order to apply a safe optimization.
Reference: [CH92] <author> Jyh-Herng Chow and Williams Ludwell Harrison III. </author> <title> Compile-Time Analysis of Parallel Programs that Share Memory. </title> <booktitle> In ACM 19th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <year> 1992. </year>
Reference-contexts: With this abstract interpretation framework, we have designed an analyzer for a language that supports first class functions, dynamic allocation, pointers, and cobegin parallelism (which can be nested) 5 . The formal syntax and semantics of the language can be found in <ref> [CH92] </ref>. Example 8 The program shown below, written in C-style (assume x,y are pointers to integer), can be handled in our language. 4 It is like taking in-line procedure expansion first and then analyzing the results as a whole. <p> The formalization of all the analyses and details about how to use procedure strings to derive these analyses can be found in <ref> [CH92] </ref>. 5.1 Side Effects We say function f makes a reference to an object if the evaluation of f reads or writes the object. The following gives the definition of side effect. <p> We will only describe abstraction for the domain of configurations and processes and indicate that the folding mechanism proposed in [Tay83] or [McD89] is in fact one of the possible abstractions under our abstract interpretation framework. Other abstract domains can be found in <ref> [CH92] </ref>. 6.1 Abstracting Configurations As mentioned in Example 3, we need to use abstraction to folds some of the configurations, so that the number of configurations can be further reduced.
Reference: [Deu90] <author> Alain Deutsch. </author> <title> On Determining Lifetime and Aliasing of Dynamically Allocated Data in Higher-order Functional Specifications. </title> <booktitle> In ACM 17th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 157-168, </pages> <year> 1990. </year>
Reference-contexts: We will show an example in Section 7. 7 Again, considering only a single execution path. 6 Abstraction This section describes some abstraction to the semantic domains, in order to reduce the complexity of the analysis. Various well-known abstraction maps on semantic domains can be found in <ref> [Deu90] </ref>. We will only describe abstraction for the domain of configurations and processes and indicate that the folding mechanism proposed in [Tay83] or [McD89] is in fact one of the possible abstractions under our abstract interpretation framework. <p> Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis <ref> [Deu90] </ref>, data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91]. One of the earlier works on compile-time analysis of parallel programs was done by Taylor [Tay83], who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs.
Reference: [HA89] <author> Williams L. Harrison III and Zahira Ammarguellat. </author> <title> The Design of Automatic Parallelizers for Symbolic and Numeric Programs. </title> <editor> In T. Ito and R.H. Halstead, editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, volume 441 of Lecture Notes in Computer Science, </booktitle> <pages> pages 235-253. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC <ref> [HA89, Har91] </ref>. One of the earlier works on compile-time analysis of parallel programs was done by Taylor [Tay83], who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs. In parallel programs debugging, detecting access anomalies is the essential work. <p> The task of compiler analysis can be much simplified for programming languages that restrict the accesses of shared variables among concurrent threads, for example, read-only or one-writer-multiple-readers, because in which the number of non-redundant interleaving can be reduced significantly. The proposed framework has being implemented, built upon the MIPRAC <ref> [Har91, HA89] </ref> compiler where abstract interpretations of sequential Fortran, C, and Scheme programs have been fully developed.
Reference: [Har89] <author> Williams Ludwell Harrison III. </author> <title> The Interprocedural Analysis and Automatic Parallelization of Scheme Programs. Lisp and Symbolic Computation: </title> <journal> An International Journal, </journal> 2(3/4):179-396, 1989. 
Reference-contexts: This will be clarified in the next section. To obtain these properties, our instrumented semantics employs a device called a procedure string <ref> [Har89] </ref> to record the procedural and concurrency movements (namely, entering a procedure/thread, or exiting from a procedure/thread) during the program interpretation. When an object is created, the procedure string at that point (called its birthdate) is recorded into that object. <p> More precisely, if we know the extent of objects, we can associate each function exit with a deallocation list of objects, as proposed in <ref> [Har89] </ref>. In a system with hierarchical memories, suppose each cobegin thread is executed in a processor. If we know an object will be referenced by another concurrent thread, then it should be allocated in the memory accessible to both threads. <p> Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL <ref> [Har89] </ref> and MIPRAC [HA89, Har91]. One of the earlier works on compile-time analysis of parallel programs was done by Taylor [Tay83], who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs. In parallel programs debugging, detecting access anomalies is the essential work.
Reference: [Har91] <author> Williams Ludwell Harrison III. </author> <title> Semantic Analysis of Symbolic Programs for Automatic Parallelization. </title> <note> book in preparation, </note> <year> 1991. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC <ref> [HA89, Har91] </ref>. One of the earlier works on compile-time analysis of parallel programs was done by Taylor [Tay83], who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs. In parallel programs debugging, detecting access anomalies is the essential work. <p> The task of compiler analysis can be much simplified for programming languages that restrict the accesses of shared variables among concurrent threads, for example, read-only or one-writer-multiple-readers, because in which the number of non-redundant interleaving can be reduced significantly. The proposed framework has being implemented, built upon the MIPRAC <ref> [Har91, HA89] </ref> compiler where abstract interpretations of sequential Fortran, C, and Scheme programs have been fully developed.
Reference: [HPR89] <author> Susan Horwitz, Phil Pfeiffer, and Thomas Reps. </author> <title> Dependence Analysis for Pointer Variables. </title> <booktitle> In ACM Sigplan Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 28-40, </pages> <year> 1989. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis [Hud87, JM89]), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence <ref> [HPR89] </ref>). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91]. One of the earlier works on compile-time analysis of parallel programs was done by Taylor [Tay83], who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs.
Reference: [Hud87] <author> Paul Hudak. </author> <title> A Semantic Model of Reference Counting and its Abstraction. </title> <editor> In S. Abramsky and C. Hankin, editors, </editor> <title> Abstract Interpretation of Declarative Languages. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1987. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis <ref> [Hud87, JM89] </ref>), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91].
Reference: [JM89] <author> Simon B. Jones and Daniel Le Metayer. </author> <title> Compile-Time Garbage Collection by Sharing Analysis. </title> <booktitle> In Proc. the Fourth International Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 54-74. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Since then, abstract interpretation has been used broadly in analyzing functional programs (e.g. strictness analysis [BHA86], sharing and lifetime analysis <ref> [Hud87, JM89] </ref>), or non-functional programs (e.g., sharing and lifetime analysis [Deu90], data dependence [HPR89]). It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91].
Reference: [Lam79] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: On the other hand, however, if (b) is the input program to the compiler, the compiler can safely parallelize all these four statements without violating the program semantics. 2 1 Throughout the paper, sequential consistency <ref> [Lam79] </ref> is assumed as the execution model 1 The goal of this work is to develop a general framework for obtaining various useful information at compile time about parallel programs where concurrent activities can interfere with each other through shared variables.
Reference: [LCL88] <author> F.J. Lin, P.M. Chu, and M.T. Liu. </author> <title> Protocal Verification Using Reachability Analysis: The State Space Explosion Problem and Relief Strategies. </title> <booktitle> In ACM SIGCOMM '87 Workshop on Frontier in Computer Communications Technology, Computer Communication Review 17(5), </booktitle> <pages> pages 126-135, </pages> <year> 1988. </year>
Reference-contexts: level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification [Ove81, Pnu86], Petri-net reachabil-ity analysis [Val88, Val89, Val90], and communication protocol verification <ref> [LCL88] </ref>. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [McD89] <author> Charles E. McDowell. </author> <title> A Practical Algorithm for Static Analysis of Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6(3) </volume> <pages> 515-536, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Section 4 describes our framework for analyzing parallel programs. Section 5 develops the analyses of side effects, data dependences, and object lifetimes. Section 6 presents methods for using abstractions to reduce the complexity of the analysis. We indicate that existing mechanisms, such as those in <ref> [McD89] </ref>, can be viewed as special cases of abstractions on certain domains under our framework. Applications using the obtained information are discussed in section 7. <p> There are advantages to using abstract interpretation. First, recall the complexity in analyzing parallel programs; some mechanism is needed to fold related states in order to reduce the cost. The folding mechanism is in fact a form of abstraction (for example, the clan in <ref> [McD89] </ref> is an abstract process). By abstract interpretation, there are many alternatives available for designing abstract semantics, and any of them automatically suggests a different folding mechanism. These choices are not easily visible and obtainable from traditional perspectives. <p> Various well-known abstraction maps on semantic domains can be found in [Deu90]. We will only describe abstraction for the domain of configurations and processes and indicate that the folding mechanism proposed in [Tay83] or <ref> [McD89] </ref> is in fact one of the possible abstractions under our abstract interpretation framework. <p> Since processes originated from the same cobegin branch execute the same program fragment, a transition made by the abstract process takes these (concrete) processes to their next states altogether. In fact, the above abstraction utilizes the two important observations in <ref> [McD89] </ref> when he describes his algorithm. 9 This abstract process corresponds to his clan concept, and the resulting configurations corresponds to his virtual concurrency states. 8 Taylor later describes a parceling mechanism that attempts to reduce the number of interleavings or determine if two threads can be analyzed separately, which corresponds <p> are: (1) two tasks executing the same sequence of statements often need not be distinguished for analysis purpose; (2) if several tasks are executing the same sequence of statements, it is often not necessary to know exactly how many of those tasks are at a certain point in their execution. <ref> [McD89] </ref> s4 s2 s4: s2: coend call f4 (); call f3 (); call f2 (); call f1 (); // cobegin cobegin s1: y=malloc (1); s2: *y=10; s3: x=malloc (1); s4: *x=*y; coend b2b1 s3 s1 7 Applications We give some examples illustrating how the information obtained by our analyses can be
Reference: [MH89] <author> Charles E. McDowell and David P. Helmbold. </author> <title> Debugging Concurrent Programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(4) </volume> <pages> 593-622, </pages> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: procedure g; ... jmp loop ... test r0 load r0, s loop: .... s3: A=1 (b)(a) segment 1 segment 2 s1: A=1 s2: y=B ... ... ... ... s2: y=B segment 2segment 1 (A=B=0) Compile-time analysis of parallel programs has been proposed for detecting access anomalies or assisting debugging (see <ref> [MH89] </ref> for a survey). However, there is a fundamental difference between static analysis for the purpose of debugging and program optimization. In the former, only well-synchronized programs (e.g., synchronized accesses to shared variables) are considered correct; unordered accesses to a shared variable are bugs to be detected. <p> In parallel programs debugging, detecting access anomalies is the essential work. Compile-time analyses for reporting possible access anomalies or assisting post-run debugging have been proposed by several researchers <ref> [MH89] </ref>. From the problem that consecutive memory accesses may be out of order in a multistage interconnection network, Shasha and Snir [SS88] investigate how to insert a minimum number of delays to enforce sequential consistency.
Reference: [Mis91] <author> Jayadev Misra. </author> <title> Loosely-Coupled Processes (Preliminary Version). </title> <booktitle> In PARLE 91, Vol. II, volume 506 of Lecture Notes in Computer Science, </booktitle> <pages> pages 1-26. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: As a result, the intended busy-waiting never succeeds. In fact, even the simplest optimization, like constant propagation, will fail if applied without modification. Most compilers avoid the risks by restricting data sharing between concurrent threads (e.g., the model proposed by Steele [Ste90] and loosely-coupled processes proposed by Misra <ref> [Mis91] </ref>), or not optimizing the parallel codes at all. We believe as more people write parallel programs, the demand to develop various compile-time analysis techniques for these parallel programs will become stronger. <p> Among them, Steele [Ste90] proposes a model requiring any two operations that are not causally related must commute", in order to make asynchronous programs safe in the sense that nondeterminism and data races are impossible. Misra <ref> [Mis91] </ref> proposes a similar concept, loosely-coupled processes, and argues that large-scale shared-variable programming is feasible only if processes are loosely-coupled".
Reference: [MP90] <author> S.P. Midkiff and D.A. Padua. </author> <title> Issues in the Optimization of Parallel Programs. </title> <booktitle> In Proceedings International Conference on Parallel Processing - Vol II Software, </booktitle> <pages> pages 105-113, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction The inclusion of concurrent constructs, such as cobegin or doall, greatly complicates the semantics of programming languages, especially if data sharing between concurrent threads is allowed. Traditional compiler optimization techniques for sequential programs are no longer directly applicable to parallel programs <ref> [MP90] </ref>, and their extension to the parallel counterpart is not trivial [SW91]. The difficulty comes mainly from the exponential growth of the complexity in analyzing every possible interaction among concurrent threads during execution. <p> Example 15 (further parallelization) The techniques in <ref> [SS88, MP90] </ref> can be easily extended to procedure calls. Let's consider an example in Figure 8, which is similar to the one in [SS88] except that simple assignments are replaced by function calls. <p> If no delay is necessary between statements i and j in a program segment, then the order of i and j can be arbitrary, namely their order can be interchanged. However, they address only the case of straight line codes. Midkiff, Padua and Cytron <ref> [MPC90, MP90] </ref> extend the above idea to deal with loops and array accesses,by constructing statement level conflict graphs whose arcs compactly describe a set of conflict instances. <p> It eventually tries to test if a system of equations of an s-level cycle has a solution, similar to the data dependence test. <ref> [MP90] </ref> also gives eleven examples illustrating how a compiler fails to generate correct codes for parallel programs.
Reference: [MPC90] <author> S.P. Midkiff, David Padua, and R.G. Cytron. </author> <title> Compiling Programs with User Parallelism. </title> <editor> In David Gelernter, Alexandru Nico-lau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel & Distributed Computing, </booktitle> <pages> pages 402-422. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: If no delay is necessary between statements i and j in a program segment, then the order of i and j can be arbitrary, namely their order can be interchanged. However, they address only the case of straight line codes. Midkiff, Padua and Cytron <ref> [MPC90, MP90] </ref> extend the above idea to deal with loops and array accesses,by constructing statement level conflict graphs whose arcs compactly describe a set of conflict instances.
Reference: [Ove81] <author> W.T. Overman. </author> <title> Verification of Concurent Systems: Function and Timing. </title> <type> PhD thesis, </type> <institution> University of California Los Angeles, </institution> <year> 1981. </year>
Reference-contexts: Now if we take advantage of the locality property, the configuration space can be greatly reduced as shown in Figure 5 (b), which contains only 13 configurations, while producing exactly the same set of result-configurations. 2 This method for space reduction is first introduced in Overman <ref> [Ove81] </ref>, and it is generalized and formalized by Val-mari [Val88, Val89, Val90], where he develops so called stubborn set theory. By ordinary state space generation, all enabled transitions at each step are investigated, and the corresponding successor states are generated. <p> The formal presentation of this theory is complicated, which can be found in the above references. We only gives an improved version of Overman's algorithm <ref> [Ove81] </ref> in section 2.3. There may exist several stubborn sets at an expanding step. In general, we prefer a stubborn set that contains the fewest number of enabled transitions, so that the smallest number of successors have to be generated. <p> Section 5.3 tells us b1 should be allocated at a level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification <ref> [Ove81, Pnu86] </ref>, Petri-net reachabil-ity analysis [Val88, Val89, Val90], and communication protocol verification [LCL88]. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [Pnu86] <author> Amir Pnueli. </author> <title> Applications of Temporal Logic to the Specification and Verification of Reactive Systems: A Survey of Current Trends. </title> <booktitle> In Current Thrends in Concurrency, volume 224 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: We say: Definition 4 (Critical Reference) A reference is a critical reference if it is a read to a variable which may be written by another thread, or it is a write to a variable which may be read or written by another thread <ref> [Pnu86] </ref>. Observation 5 (Virtual Coarsening) Atomic actions of a thread can be combined if they contain at most one critical reference. <p> Section 5.3 tells us b1 should be allocated at a level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification <ref> [Ove81, Pnu86] </ref>, Petri-net reachabil-ity analysis [Val88, Val89, Val90], and communication protocol verification [LCL88]. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [RS90] <author> John H. Reif and Scott A. Smolka. </author> <title> Data Flow Analysis of Distributed Communicating Processes. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 19(1) </volume> <pages> 1-30, </pages> <year> 1990. </year>
Reference-contexts: This semantics essentially disallows any interaction through shared variables among concurrent activities, for the purpose of simplifying the effort of compiler analysis. Data flow analysis for communicating processes where process interaction occurs only through message passing can be found in Reif and Smolka <ref> [RS90] </ref> and Cousot and Cousot [CC80]. 9 Conclusion As multiprocessor systems become popular and easily accessible, there are reasons for pursuing parallel programming. However, parallel programs are often hard to understand and debug. A compiler must consider every possible interaction among concurrent activities, in order to apply a safe optimization.
Reference: [SS88] <author> Dennis Shasha and Marc Snir. </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 283-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: optimization, the first assumption is that input programs are correct and thus any optimization should result in consistency with the programs. (In fact, there are chaotic or intended nondeterministic algorithms where accesses to the same shared variable need not be synchronized.) Example 1 Figure 2 (a) shows an example from <ref> [SS88] </ref>, where the two program segments are run concurrently. After the execution of the four statements 1 , (0,0),(1,0) and (1,1) are all legal values for (x,y). <p> Example 15 (further parallelization) The techniques in <ref> [SS88, MP90] </ref> can be easily extended to procedure calls. Let's consider an example in Figure 8, which is similar to the one in [SS88] except that simple assignments are replaced by function calls. <p> Example 15 (further parallelization) The techniques in [SS88, MP90] can be easily extended to procedure calls. Let's consider an example in Figure 8, which is similar to the one in <ref> [SS88] </ref> except that simple assignments are replaced by function calls. Suppose we want to see if these function calls can be further parallelized, and after our analysis, the pairs (s1; s4) and (s2; s3) have dependences. <p> In parallel programs debugging, detecting access anomalies is the essential work. Compile-time analyses for reporting possible access anomalies or assisting post-run debugging have been proposed by several researchers [MH89]. From the problem that consecutive memory accesses may be out of order in a multistage interconnection network, Shasha and Snir <ref> [SS88] </ref> investigate how to insert a minimum number of delays to enforce sequential consistency. The idea is that by choosing delays in a way such that the union of program arcs P (from statement orders) and execution arcs E (from conflict relations) is acyclic, then E is a correct execution.
Reference: [Ste90] <author> Guy L. Steele. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In ACM 17th Symposium on Principles of Programming Languages, </booktitle> <year> 1990. </year>
Reference-contexts: So, load r0,s is moved before the loop. As a result, the intended busy-waiting never succeeds. In fact, even the simplest optimization, like constant propagation, will fail if applied without modification. Most compilers avoid the risks by restricting data sharing between concurrent threads (e.g., the model proposed by Steele <ref> [Ste90] </ref> and loosely-coupled processes proposed by Misra [Mis91]), or not optimizing the parallel codes at all. We believe as more people write parallel programs, the demand to develop various compile-time analysis techniques for these parallel programs will become stronger. <p> Several programming models are proposed for restricting the use of shared variables and prohibiting process interaction through shared variables; as a result, some important classes of algorithms can not be programmed, such as mutual exclusion or shared variable synchronization. Among them, Steele <ref> [Ste90] </ref> proposes a model requiring any two operations that are not causally related must commute", in order to make asynchronous programs safe in the sense that nondeterminism and data races are impossible.
Reference: [SW91] <author> Harini Srinivasan and Michael Wolfe. </author> <title> Analyzing Programs with Explicit Parallelism. </title> <booktitle> In Proceedings of the Fourth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Traditional compiler optimization techniques for sequential programs are no longer directly applicable to parallel programs [MP90], and their extension to the parallel counterpart is not trivial <ref> [SW91] </ref>. The difficulty comes mainly from the exponential growth of the complexity in analyzing every possible interaction among concurrent threads during execution. To date, very little research has proposed solutions to this problem, and usually compilers simply produce the wrong codes. <p> Misra [Mis91] proposes a similar concept, loosely-coupled processes, and argues that large-scale shared-variable programming is feasible only if processes are loosely-coupled". Srinivasan and Wolfe <ref> [SW91] </ref> propose a copy-in/copy-out semantics for parallel constructs, where each concurrent thread gets a private copy of shared variables and all modifications to shared variables are made locally. This semantics essentially disallows any interaction through shared variables among concurrent activities, for the purpose of simplifying the effort of compiler analysis.
Reference: [Tay83] <author> Richard N. Taylor. </author> <title> A General-Purpose Algorithm for Analyzing Concurrent Programs. </title> <journal> Communication of ACM, </journal> <volume> 26(5) </volume> <pages> 362-376, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: Various well-known abstraction maps on semantic domains can be found in [Deu90]. We will only describe abstraction for the domain of configurations and processes and indicate that the folding mechanism proposed in <ref> [Tay83] </ref> or [McD89] is in fact one of the possible abstractions under our abstract interpretation framework. <p> This abstraction greatly reduces the complexity. For the example in Figure 3, the three dangling links will be merged together, resulting in only one configuration . This abstract configuration corresponds to the basic data struc ture, concurrency state, in <ref> [Tay83] </ref>. 8 6.2 Abstracting Processes Because our language allows dynamic process creation and nested parallelism, several instances of concurrent activities of a given cobegin may be created due to procedure calls or loops. <p> It is used in the parallelizing compiler PARCEL [Har89] and MIPRAC [HA89, Har91]. One of the earlier works on compile-time analysis of parallel programs was done by Taylor <ref> [Tay83] </ref>, who presented techniques for determining possible rendezvous, parallel actions and detecting infinite waits for Ada programs. In parallel programs debugging, detecting access anomalies is the essential work. Compile-time analyses for reporting possible access anomalies or assisting post-run debugging have been proposed by several researchers [MH89].
Reference: [Val88] <author> Antti Valmari. </author> <title> State Space Generation: Efficiency and Practicality. </title> <type> PhD thesis, </type> <institution> Tampere University of Technology, Publications 55, </institution> <year> 1988. </year>
Reference-contexts: Our framework is based on state space exploration and abstract interpretation [CC77]. Like other state space generation based analysis, the most important question is how to relieve the problem of state space explosion. We approach this problem in two ways. First, stubborn set theory <ref> [Val88, Val89, Val90] </ref> is applied to eliminate redundant inter-leavings. Then, abstract interpretation, a semantic-based lattice-theoretic approach to static program analysis, is employed for further state space reduction, which provide systematic methods for folding related states. <p> of the locality property, the configuration space can be greatly reduced as shown in Figure 5 (b), which contains only 13 configurations, while producing exactly the same set of result-configurations. 2 This method for space reduction is first introduced in Overman [Ove81], and it is generalized and formalized by Val-mari <ref> [Val88, Val89, Val90] </ref>, where he develops so called stubborn set theory. By ordinary state space generation, all enabled transitions at each step are investigated, and the corresponding successor states are generated. <p> The cost of the state space generation can be reduced significantly for parallel programs where accesses to shared variables do not occur frequently, and only a small set of variables is shared among concurrent processes. The power of this method is demon strated in <ref> [Val88] </ref>, where the state space for n dining philosophers is reduced from exponential to quadratic in n. 2.3 An Algorithm for State Space Generation Algorithm 1 At each expansion step, let r i and w i be the set of locations to be read and written in process i's next actions, <p> us b1 should be allocated at a level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification [Ove81, Pnu86], Petri-net reachabil-ity analysis <ref> [Val88, Val89, Val90] </ref>, and communication protocol verification [LCL88]. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [Val89] <author> Antti Valmari. </author> <title> Eliminating Redundant Interleaving During Concurrent Program Verification. </title> <booktitle> In PARLE '89, Parallel Architectures and Languages Europe, Volumn II: Parallel Languages, volume 366 of Lecture Notes in Computer Science, </booktitle> <pages> pages 89-103. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Our framework is based on state space exploration and abstract interpretation [CC77]. Like other state space generation based analysis, the most important question is how to relieve the problem of state space explosion. We approach this problem in two ways. First, stubborn set theory <ref> [Val88, Val89, Val90] </ref> is applied to eliminate redundant inter-leavings. Then, abstract interpretation, a semantic-based lattice-theoretic approach to static program analysis, is employed for further state space reduction, which provide systematic methods for folding related states. <p> of the locality property, the configuration space can be greatly reduced as shown in Figure 5 (b), which contains only 13 configurations, while producing exactly the same set of result-configurations. 2 This method for space reduction is first introduced in Overman [Ove81], and it is generalized and formalized by Val-mari <ref> [Val88, Val89, Val90] </ref>, where he develops so called stubborn set theory. By ordinary state space generation, all enabled transitions at each step are investigated, and the corresponding successor states are generated. <p> However, many of them are duplicated. a stubborn set of transitions is searched, and only the enabled transitions in it are used when successor states are generated <ref> [Val89] </ref>. The formal presentation of this theory is complicated, which can be found in the above references. We only gives an improved version of Overman's algorithm [Ove81] in section 2.3. There may exist several stubborn sets at an expanding step. <p> us b1 should be allocated at a level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification [Ove81, Pnu86], Petri-net reachabil-ity analysis <ref> [Val88, Val89, Val90] </ref>, and communication protocol verification [LCL88]. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
Reference: [Val90] <author> Antti Valmari. </author> <title> Stubborn Sets for Reduced State Space Generation. </title> <booktitle> In Advances in Petri Nets, volume 483 of Lecture Notes in Computer Science, </booktitle> <pages> pages 491-515. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Our framework is based on state space exploration and abstract interpretation [CC77]. Like other state space generation based analysis, the most important question is how to relieve the problem of state space explosion. We approach this problem in two ways. First, stubborn set theory <ref> [Val88, Val89, Val90] </ref> is applied to eliminate redundant inter-leavings. Then, abstract interpretation, a semantic-based lattice-theoretic approach to static program analysis, is employed for further state space reduction, which provide systematic methods for folding related states. <p> of the locality property, the configuration space can be greatly reduced as shown in Figure 5 (b), which contains only 13 configurations, while producing exactly the same set of result-configurations. 2 This method for space reduction is first introduced in Overman [Ove81], and it is generalized and formalized by Val-mari <ref> [Val88, Val89, Val90] </ref>, where he develops so called stubborn set theory. By ordinary state space generation, all enabled transitions at each step are investigated, and the corresponding successor states are generated. <p> us b1 should be allocated at a level of memory visible to both processors (since b1 is accessed by both threads) while b2 can be allocated locally. 2 8 Related Work State space reduction has been studied in many different fields, such as program verification [Ove81, Pnu86], Petri-net reachabil-ity analysis <ref> [Val88, Val89, Val90] </ref>, and communication protocol verification [LCL88]. The seminal paper on abstract interpretation is [CC77], which provides a fundamental unity among all apparently unrelated program analysis techniques", and argues that most program analysis techniques may be understood as abstract interpretations of programs".
References-found: 31

