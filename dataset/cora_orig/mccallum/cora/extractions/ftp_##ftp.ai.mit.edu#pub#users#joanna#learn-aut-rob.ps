URL: ftp://ftp.ai.mit.edu/pub/users/joanna/learn-aut-rob.ps
Refering-URL: http://www.ai.mit.edu/people/joanna/joanna.html
Root-URL: 
Title: Specialized Learning and the Design of Intelligent Agents  
Author: Joanna Bryson 
Date: September 18, 1997  
Abstract: The reactive approach to artificial intelligence has developed impressive demonstrations of life-like robots and simulations by breaking behavior into carefully engineered behavioral modules which react quickly to the environment due to specialized perception (Horswill 1997). The next step in scaling the complexity of reactive robots is to acknowledge and understand the role of state and learning in such systems. Learning is an integral part of perception and behavior; this article proposes it should also be specialized and modular. We present a system of thinking about state and learning, heuristics for determining the extent of specialization vs. adaptivity that should be used, and an example of an architecture that supports the design and development of such systems. We also briefly describe experiments in both simulated blocks world and mobile robotics.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1987), </year> <title> Pengi: An Implementation of a Theory of Activity, </title> <booktitle> in `Proceedings of the Sixth National Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Seattle, Washington, </address> <pages> pp. 196-201. </pages>
Reference-contexts: Although this approach runs counter to many intuitive notions of rationality and intelligence, it has proved effective for problems ranging from navigation-based robot tasks (Connell 1990) to playing video games <ref> (Agre & Chapman 1987) </ref>, modeling human perception and problem solving (Ballard, Hayhoe, Pook & Rao forthcoming). Behavior-based AI refers to an approach inspired by Minsky (1986), where many small, relatively simple elements of intelligence act in parallel, each handling their own area of expertise (Brooks 1991b, Mataric 1997).
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1988), </year> <title> What are Plans For?, AI memo 1050, </title> <publisher> MIT, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: For example, representing a blocks world problem may require individually labeling each block. Deictic variables provide a restricted set of labels such as the-block-I'm-holding or even the-green-block-I-most-recently-saw. Deictic variables were popularized in the work of Agre and Chapman <ref> (Agre & Chapman 1988, Chapman 1990) </ref> who used them to write a reactive system that successfully plays computer games. The ideas behind deictic variables can also be found in Minsky's pronemes (Minsky 1986), in visual attention Ullman (1984), and apparently in philosophical work of Heidegger (Dreyfus 1992).
Reference: <author> Ambler, A., Barrow, H., Brown, C., Burstall, R. & Popplestone, R. J. </author> <year> (1975), </year> <title> `A versatile system for computer controlled assembly', </title> <booktitle> Artificial Intelligence 6(2), </booktitle> <pages> 215-218. </pages>
Reference-contexts: However, in some ways our new agents look stupider than the robots of the 1970's such as Shakey (Nilsson 1984) and FREDDY <ref> (Ambler, Barrow, Brown, Burstall & Popplestone 1975) </ref>. We no longer have demonstrations involving assembly and manipulation as well as map building and navigation. Essentially, we have robots with a very limited set of skills that are capable identifying when those skills should be applied.
Reference: <author> Angle, C. M. </author> <year> (1989), </year> <title> Genghis, a Six Legged Walking Robot, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts. Bachelor's thesis. </institution>
Reference-contexts: The Polly vision-based obstacle-avoidance algorithm also determined speed and direction by the apparent distance to obstacles in each individual visual frame. (Horswill 1993)). However, even simple behaviors may require some kind of stored internal state, and often sequential control. For example, one of the most influential subsumption robots, Genghis <ref> (Angle 1989) </ref>, would back off and turn away from obstacles detected by bumping its antennae.
Reference: <author> Ballard, D. H., Hayhoe, M. M., Pook, P. K. & Rao, R. P. N. </author> <title> (forthcoming), `Deictic codes for the embodiment of cognition', </title> <journal> Brain and Behavioral Sciences. </journal> <volume> 12 Blumberg, </volume> <editor> B. M. </editor> <year> (1996), </year> <title> Old Tricks, New Dogs: Ethology and Interactive Creatures, </title> <type> PhD thesis, </type> <note> Learning and Common Sense Section. </note>
Reference: <author> Bonasso, R. P., Firby, R. J., Gat, E., Kortenkamp, D., Miller, D. P. & Slack, M. G. </author> <year> (1997), </year> <title> `Experiences with an architecture for intelligent, reactive agents', </title> <journal> Journal of Experimental & Theoretical Artificial Intelligence 9(2/3), </journal> <pages> 237-256. </pages>
Reference-contexts: Programming difficulties have been part of the motivation for the two and three layer architectures <ref> (Bonasso et al. 1997) </ref> and for the situated planning approach in general (Levison 1996). There are two main reasons for this problem. First, subsumption architecture and similar approaches have sensor information flow and control structure represented in the same connections between behaviors.
Reference: <author> Braitenberg, V. </author> <year> (1984), </year> <title> Vehicles: Experiments in Synthetic Psychology, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Thus at each moment every behavior records a particular state of execution, and this state helps determine what that behavior will do next. The most reactive behaviors do respond directly and continuously to factors in the environment. For example, in the first few Braitenburg Vehicles <ref> (Braitenberg 1984) </ref> motor speed was directly correlates to light or heat intensity. The Polly vision-based obstacle-avoidance algorithm also determined speed and direction by the apparent distance to obstacles in each individual visual frame. (Horswill 1993)).
Reference: <author> Brooks, R. A. </author> <year> (1989), </year> <title> `A Robot That Walks: Emergent Behavior from a Carefully Evolved Network', </title> <booktitle> Neural Computation 1(2), </booktitle> <pages> 253-262. </pages>
Reference-contexts: In fact, many of the behaviors take their inputs from reading the outputs of other behaviors, and similarly many express their own behavior by inhibiting or suppressing (overwriting) the inputs or outputs of other behaviors. (See <ref> (Brooks 1989) </ref> for example.) Trying to effect control and data flow through the same mechanisms leads to complications. One of the principles of subsumption architecture is that once a layer of competence is developed, it should never be altered for higher layers.
Reference: <author> Brooks, R. A. </author> <year> (1991a), </year> <title> Intelligence Without Reason, </title> <booktitle> in `Proceedings of the 1991 International Joint Conference on Artificial Intelligence', </booktitle> <pages> pp. 569-595. </pages>
Reference-contexts: Rather, the problem is a consequence of current technique and methodology. In particular, there is a need to focus on the use of learning in reactive architectures. The reactive robotics approach depends crucially on the practice of carefully engineering each behavior <ref> (Brooks 1991a, Wooldridge & Jennings 1995) </ref>. Consequently, it has tended to deprecate the use of learning and the presence of state. Practitioners argue that relying on previous experience must necessarily reduce a robot's reactivity to the environment, exposing it to danger and missed opportunities in a dynamic world. <p> These sections also introduce our own architecture, based on reactive hierarchical control and modular learning. The architecture's design reflects our analysis of state and learning, and experiments we have conducted illustrate it. 2 Learning in Animals Animals are our primary working example of what we consider intelligence to be <ref> (Brooks 1991a, McGonigle 1991) </ref>. Autonomous agent research more than most branches of artificial intelligence has always acknowledged the extent to which it exploits the solutions of natural intelligence (see for example Maes, Mataric, Meyer, Pollack & Wilson (1996)).
Reference: <author> Brooks, R. A. </author> <year> (1991b), </year> <title> `Intelligence without Representation', </title> <booktitle> Artificial Intelligence 47, </booktitle> <pages> 139-159. </pages>
Reference-contexts: Behavior-based AI refers to an approach inspired by Minsky (1986), where many small, relatively simple elements of intelligence act in parallel, each handling their own area of expertise <ref> (Brooks 1991b, Mataric 1997) </ref>. In theory, these simpler elements are both easier to design and more plausible from an evolutionary standpoint. The apparent complexity of intelligent behavior arises from two sources: the interaction between multiple units running in parallel, and the complexity of the environment the units are reacting to. <p> For example, the offices in a laboratory where a robot operates may be connected by a single hallway. The hallway is essentially the same environment whichever office the robot needs to enter next. 4 Learning and State In their struggle to minimize state, the architects of dominant reactive systems <ref> (Brooks 1991b, Maes 1989) </ref> have complicated both learning and control unnecessarily by confounding the flow of information with the flow of control. The methodology presented here divides the issues of control, or when a behavior is expressed, from perception, or how it is expressed. <p> Conventional ai systems hold control state in a plan, in which a program pointer determines what the next step should be. Finite state machines, often used in behavior-based ai <ref> (Brooks 1991b) </ref>, hold similar state. Deictic Variables Fixed variables which refer to a particular object of attention. Deictic variables allow a system to generalize over cases where particular plans embodied in control state may operate. <p> For more details of the architecture, see (Bryson in press). 5.1 Control State One of the myths of artificial intelligence is that the early experimenters in reactive planning advocated stateless systems. In fact, the fundamental units of subsumption architecture <ref> (Brooks 1991b) </ref> are augmented finite state machines. Thus at each moment every behavior records a particular state of execution, and this state helps determine what that behavior will do next. The most reactive behaviors do respond directly and continuously to factors in the environment.
Reference: <author> Bryson, J. </author> <title> (in press), Agent Architecture as Object Oriented Design, </title> <editor> in M. P. Singh, ed., </editor> <booktitle> `The Fourth International Workshop on Agent Theories, Architectures, and Languages (ATAL97)', </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Calvin, W. H. </author> <year> (1996), </year> <title> The Cerebral Code, </title> <publisher> MIT Press. </publisher>
Reference-contexts: The action sequences and competences in the control architecture were designed to support another kind of meta-learning, possibly referred to as thought. The idea is to allow evolutionary-type processes to derive new behaviors or solutions when old ones cannot be produced. (See for a similar theory <ref> (Calvin 1996) </ref>.) The simpler problems here are how to generate new thoughts and how to test and store a winning solution. The hard problems are how to tell when a new plan is needed, and how to recognize which solution wins.
Reference: <author> Carlson, N. R. </author> <year> (1994), </year> <title> Physiology of Behavior, 5 edn, </title> <publisher> Allyn and Bacon, </publisher> <address> Boston. </address>
Reference-contexts: The abstraction is clearest in humans: what we perceive is what we report, whereas what we sense is presumably more directly a consequence of the environment. Perception is dependent both on learning and on context <ref> (Carlson 1994) </ref>. We can learn to discriminate differences without having any explicit knowledge of what those differences are. In some cases, this learning requires no feedback, simply exposure to the appropriate stimulus (Sundareswaran & Vaina 1994).
Reference: <author> Chapman, D. </author> <year> (1987), </year> <title> `Planning for conjunctive goals', </title> <booktitle> Artificial Intelligence 32, </booktitle> <pages> 333-378. </pages>
Reference-contexts: A common response to this situation has been to create hybrid robotic architectures that supplement reactive lower layers with conventional AI planners (Bonasso, Firby, Gat, Kortenkamp, Miller & Slack 1997, Hexmoor 1995, Levison 1996). Planning, however, has been at best difficult to scale and at worst provably intractable <ref> (Chapman 1987, Horswill 1997, Wooldridge & Jennings 1995) </ref>. An alternative research path is to explore ways to expand the current envelope of competence in reactive robotics. <p> Although this approach runs counter to many intuitive notions of rationality and intelligence, it has proved effective for problems ranging from navigation-based robot tasks (Connell 1990) to playing video games <ref> (Agre & Chapman 1987) </ref>, modeling human perception and problem solving (Ballard, Hayhoe, Pook & Rao forthcoming). Behavior-based AI refers to an approach inspired by Minsky (1986), where many small, relatively simple elements of intelligence act in parallel, each handling their own area of expertise (Brooks 1991b, Mataric 1997).
Reference: <author> Chapman, D. </author> <year> (1989), </year> <title> `Penguins Can Make Cake', </title> <journal> AI Magazine 10(4), </journal> <pages> 51-60. </pages>
Reference-contexts: This demonstrates an interchange of control state and deictic state, with considerable combinatorial ramifications. 3 With the exception of the robot arm commands, which were quite unrealistically considered primitives both in Whitehead (1992) and our simulation. 8 As a final experiment, we constructed a copy demo similar to <ref> (Chapman 1989) </ref>. The demo requires building a stack to match a goal stack present in the environment. Coding this more general task took no more steps than coding the red on green task.
Reference: <author> Chapman, D. </author> <year> (1990), </year> <title> Vision, Instruction, and Action, </title> <type> Technical Report 1204, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, Cambridge, Massachusetts. </institution>
Reference: <author> Connell, J. </author> <year> (1990), </year> <title> Minimalist Mobile Robotics: A Colony-style Architecture for a Mobile Robot, </title> <publisher> Academic Press, </publisher> <address> Cambridge, Massachusetts. </address> <publisher> also MIT TR-1151. </publisher>
Reference-contexts: Although this approach runs counter to many intuitive notions of rationality and intelligence, it has proved effective for problems ranging from navigation-based robot tasks <ref> (Connell 1990) </ref> to playing video games (Agre & Chapman 1987), modeling human perception and problem solving (Ballard, Hayhoe, Pook & Rao forthcoming). <p> One of the principles of subsumption architecture is that once a layer of competence is developed, it should never be altered for higher layers. In Herbert <ref> (Connell 1990) </ref>, this principle broke down when a behavior was added to reach and grasp a soda can. The robot's arm broke its line of sight to the can, so the lower behavior that moved unless it saw a soda can would be activated in mid reach. <p> By making these competences part of a single drive element we ensure that they will never operate at the same time. Each drive element can be thought of as an attention resource. By having a 2 An action sequence would be appropriate for the exact behavior in <ref> (Connell 1990) </ref>. 6 control structure (whether a sequence or a competence) for the grasp, we remove the ambiguity of what failing to see the can means. If the robot is attending to the grasp process, then it has found the can it is grasping, and is not searching.
Reference: <author> Cooper, R., Shallice, T. & Farringdon, J. </author> <year> (1995), </year> <title> Symbolic and continuous processes in the automatic selection of actions, </title> <editor> in J. Hallam, ed., </editor> <title> `Hybrid Problems, Hybrid Solutions, </title> <booktitle> Frontiers in Artificial Intelligence and Applications', </booktitle> <publisher> IOS Press, Amsterdam, </publisher> <pages> pp. 27-37. </pages>
Reference: <author> Correia, L. & Steiger-Gar~c~ao, A. </author> <year> (1995), </year> <title> A Useful Autonomous Vehicle With a Hierarchical Behavior Control, </title> <editor> in F. Moran, A. Moreno, J. Merelo & P. Chacon, eds, </editor> <booktitle> `Advances in Artificial Life (Third European Conference on Artificial Life)', </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <pages> pp. 625-639. </pages>
Reference: <author> Dreyfus, H. L. </author> <year> (1992), </year> <title> What Computers Still Can't Do, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The ideas behind deictic variables can also be found in Minsky's pronemes (Minsky 1986), in visual attention Ullman (1984), and apparently in philosophical work of Heidegger <ref> (Dreyfus 1992) </ref>. Whitehead (1992) uses deictic variables to simplify reinforcement learning.
Reference: <author> Gallistel, C., Brown, A. L., Carey, S., Gelman, R. & Keil, F. C. </author> <year> (1991), </year> <title> Lessons From Animal Learning for the Study of Cognitive Development, </title> <editor> in S. Carey & R. Gelman, eds, </editor> <booktitle> `The Epigenesis of Mind', </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: to form associations; there are general laws of learning that apply equally to all domains of stimuli, responses, and reinforcers; the more frequent the pairings between the elements to be associated, the stronger the associative strength; the more proximate the members of an association pair, the more likely the learning <ref> (Gallistel, Brown, Carey, Gelman & Keil 1991) </ref>. Learning by association, also called "conditioning", does appear to be a general learning mechanism with parameters that hold across species, presumably indicating a common underlying mechanism. However, behaviorist research itself eventually demonstrated that animals cannot learn to associate any stimulus with any response.
Reference: <author> Garcia, J. & Koelling, R. A. </author> <year> (1966), </year> <title> `The relation of cue to consequence in avoidance learning', </title> <booktitle> Psychonomic Science 4, </booktitle> <pages> 123-124. </pages>
Reference-contexts: In related experiments, rats presented with "bad" water learned different cues for its badness depending on the consequences of drinking it. If drinking lead to shocks, they learned visual or auditory cues and if drinking lead to poisoning they learned taste or smell cues <ref> (Garcia & Koelling 1966) </ref>. These examples demonstrate highly specific, constrained and ecologically relevant learning mechanisms.
Reference: <author> Henson, R. N. A. </author> <year> (1996), </year> <title> Short-term Memory for Serial Order, </title> <type> PhD thesis, </type> <institution> St. John's College. </institution>
Reference-contexts: Neurophysiological research demonstrates that a change of context alters the indexing of the place cells in the hippocampus in rats (Wilson & McNaughton 1994). Recent psychological research indicates that human sequence learning can best be modeled by associating elements of the sequence graded relation to its start and end <ref> (Henson 1996) </ref>. Consolidated motor sequence memory has particular cells dedicated to units of behavior and to motor transitions (Tanji 1996). Presumably, then, some element of memory allows for this transduction by observing episodic memory. We are currently experimenting with navigational systems built on this premise.
Reference: <author> Hexmoor, H. H. </author> <year> (1995), </year> <title> Representing and Learning Routine Activities, </title> <type> PhD thesis, </type> <institution> State University of New York at Buffalo. </institution>
Reference: <author> Hineline, P. & Rachlin, H. </author> <year> (1969), </year> <title> `Escape and avoidance of shock by pigeons pecking a key', </title> <journal> Journal of Experimental Analysis of Behavior 12, </journal> <pages> 533-538. </pages>
Reference-contexts: However, behaviorist research itself eventually demonstrated that animals cannot learn to associate any stimulus with any response. Pigeons can learn to peck for food, but cannot learn to peck to avoid a shock. They can, however, learn to flap their wings to avoid a shock, but not for food <ref> (Hineline & Rachlin 1969) </ref>. In related experiments, rats presented with "bad" water learned different cues for its badness depending on the consequences of drinking it.
Reference: <author> Hinton, G. E. & Nowlan, S. J. </author> <year> (1987), </year> <title> `How Learning Can Guide Evolution', </title> <booktitle> Complex Systems 1, </booktitle> <pages> 495-502. </pages>
Reference-contexts: These cries are dedicated to pythons, martial eagles, and leopards. Baby vervets make cries from a very early age, but across more general 1 Evolutionary modeling research on the Baldwin Effect suggests that there is little evolutionary pressure to genetically hard code rules that are consistently learned <ref> (Hinton & Nowlan 1987) </ref>. 2 objects. For example, they may give the "eagle" cry for anything in the sky, the "leopard" cry for any animal, the "python" cry for a stick on the ground.
Reference: <author> Horswill, I. </author> <year> (1997), </year> <title> `Visual architecture and cognitive architecture', </title> <journal> Journal of Experimental & The--oretical Artificial Intelligence 9(2/3), </journal> <pages> 277-293. </pages>
Reference-contexts: A solution to this problem that is gaining acceptance is the use of deictic variables <ref> (Ballard et al. forthcoming, Horswill 1997) </ref>. A deictic variable is a permanent memory structure with changeable external reference that can be incorporated into plans. For example, representing a blocks world problem may require individually labeling each block. <p> Thus the reduction of complexity brought about by using deictic variables for a reactive system is not in terms of amount of data, but in the size of the control architecture, the number of behaviors, or the number of reactive plans <ref> (Horswill 1997, Rhodes 1996) </ref>. 5.2.1 Trials with the Reactive Hierarchical Control Architecture Pure reactive planning is closely related to deictic representations. In the example given earlier of Genghis avoiding an obstacle, the obstacle might be perceived externally as the deictic variable the-thing-I-just-hit.
Reference: <author> Horswill, I. D. </author> <year> (1993), </year> <title> Specialization of Perceptual Processes, </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts. </institution>
Reference-contexts: For example, in the first few Braitenburg Vehicles (Braitenberg 1984) motor speed was directly correlates to light or heat intensity. The Polly vision-based obstacle-avoidance algorithm also determined speed and direction by the apparent distance to obstacles in each individual visual frame. <ref> (Horswill 1993) </ref>). However, even simple behaviors may require some kind of stored internal state, and often sequential control. For example, one of the most influential subsumption robots, Genghis (Angle 1989), would back off and turn away from obstacles detected by bumping its antennae.
Reference: <author> Horswill, I. D. </author> <year> (1995), </year> <title> Visual routines and visual search, </title> <booktitle> in `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <address> Montreal. </address>
Reference-contexts: The first implementation of the Hierarchical Control Architecture was a blocks world simulation called Braniff. The simulation was based on Whitehead (1992), a reinforcement learning thesis using visual routine theory. The primitive behaviors were assumed to be grounded completely 3 in commands of the Visual Routine Processor described in <ref> (Horswill 1995) </ref>. Thus the behaviors themselves had no internal state beyond control. The state of the deictic variables was recorded in the simulated environment; in the robotic implementation that state would have been maintained by the vrp.
Reference: <author> Kaelbling, L. P. </author> <year> (1997), </year> <title> Why Robbie Can't Learn: The Difficulty of Learning in Autonomous Agents, </title> <booktitle> in `Proceedings of the 15th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Nagoya. </address> <booktitle> IJCAI Computers and Thought Award talk. </booktitle>
Reference-contexts: In fact, the more like a deictic variable perceptual learning can become, the better. Our thesis that intelligent agents can best be developed through the use of specialized learning modules is really a restatement of the well established result that learning is dependent on bias <ref> (Kaelbling 1997) </ref>. We attempt to maximize bias by minimizing learning and constraining each necessary adaptive element individually. 5.3.1 Modular Learning and Reactive Hierarchical Control Learning is applied when programming control is otherwise prohibitively complex.
Reference: <author> Karmiloff-Smith, A. </author> <year> (1992), </year> <title> Beyond Modularity, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Levison, L. </author> <year> (1996), </year> <title> Connecting Planning and Action via Object-Specific Reasoning, </title> <type> PhD thesis, </type> <institution> School of Engineering and Applied Science, Computer and Information Science Department. </institution>
Reference-contexts: Programming difficulties have been part of the motivation for the two and three layer architectures (Bonasso et al. 1997) and for the situated planning approach in general <ref> (Levison 1996) </ref>. There are two main reasons for this problem. First, subsumption architecture and similar approaches have sensor information flow and control structure represented in the same connections between behaviors. Ideally in subsumption architecture, each behavior should receive sensor information directly, so that each behavior is itself reactive.
Reference: <author> Lorenz, K. </author> <year> (1973), </year> <title> Foundations of Ethology, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Deictic Variables Fixed variables which refer to a particular object of attention. Deictic variables allow a system to generalize over cases where particular plans embodied in control state may operate. An example from ethology is the classic work on imprinting by Lorenz <ref> (Lorenz 1973) </ref>: a gosling will follow and learn from whatever mother-shaped thing it sees (possibly a baby-buggy) during a critical period after hatching. Upon reaching adulthood, a male goose will try to mate with similar objects. Perceptual Memory Specialized systems and representations where information can accumulate.
Reference: <author> Maes, P. </author> <year> (1989), </year> <title> How To Do the Right Thing, </title> <journal> A.I. </journal> <volume> Memo 1180, </volume> <publisher> MIT, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: However, such systems can also be difficult to program and to scale, even in situations where some explicit sequencing is set in terms of activation from precondition to dependency <ref> (Maes 1989) </ref>. 5.1.2 Reactive Hierarchical Control To address these issues we developed a control architecture. The architecture provides two primary types of control structures for organizing behaviors: action sequences referred to as laps (Learnable Action Patterns) and small reactive plans referred to as competences.
Reference: <author> Maes, P. </author> <year> (1990), </year> <title> Designing Autonomous Agents : Theory and Practice from Biology to Engineering and back, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Once normal behavior is resumed, the event will be forgotten. This disposal of information may appear wasteful, but in practice the information is difficult to record accurately, and is often transient. The superiority of the reactive approach to local navigation has been shown empirically <ref> (Maes 1990, Bonasso et al. 1997) </ref>. A more significant criticism, however, is that this approach exchanges the complexity of adequately representing the environment for complexity in designing the control. 5 5.1.1 Programming Control State Programming under the original reactive architectures is notoriously difficult (Wooldridge & Jen-nings 1995).
Reference: <editor> Maes, P., Mataric, M. J., Meyer, J.-A., Pollack, J. & Wilson, S. W., eds (1996), </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Malcolm, C. </author> <year> (1992), </year> <type> Personal communication. </type>
Reference-contexts: Malcolm's own research uses reactive, behavior-based primitives to support a planning system for assembly (Malcolm & Smithers 1990, Wilson 1996); he considers assembly too complex of a task for a strictly behavior-based system <ref> (Malcolm 1992) </ref>. Whitehead's thesis was chosen in particular because of its status as a well-known advance of reinforcement learning. Reinforcement learning develops a look-up table for behavior selection based on observed state. Our intention was to compare our own system with behavior arbitration as results from reinforcement learning.
Reference: <author> Malcolm, C. & Smithers, T. </author> <year> (1990), </year> <title> Symbol Grounding via a Hybrid Architecture in an Autonomous Assembly System, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back', </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 123-144. </pages>
Reference-contexts: Braniff was located in a blocks world to address a challenge to behavior-based AI by Chris Malcolm. Malcolm's own research uses reactive, behavior-based primitives to support a planning system for assembly <ref> (Malcolm & Smithers 1990, Wilson 1996) </ref>; he considers assembly too complex of a task for a strictly behavior-based system (Malcolm 1992). Whitehead's thesis was chosen in particular because of its status as a well-known advance of reinforcement learning.
Reference: <author> Mataric, M. J. </author> <year> (1997), </year> <title> `Behaviour-based control: examples from navigation, learning, and group behaviour', </title> <journal> Journal of Experimental & Theoretical Artificial Intelligence 9(2/3), </journal> <pages> 323-336. </pages>
Reference-contexts: 1 Introduction In some ways embodied agents appear significantly more intelligent now than they did a few decades ago. The reactive approach to robot intelligence (Steels 1994) allows us to create rapid, robust behaviors by replacing extensive planning and modeling with carefully engineered behaviors and continuous sensing of the environment <ref> (Mataric 1997, Maes 1990) </ref>. However, in some ways our new agents look stupider than the robots of the 1970's such as Shakey (Nilsson 1984) and FREDDY (Ambler, Barrow, Brown, Burstall & Popplestone 1975). We no longer have demonstrations involving assembly and manipulation as well as map building and navigation.
Reference: <author> McGonigle, B. </author> <year> (1991), </year> <title> Incrementing Intelligent Systems by Design, </title> <editor> in J.-A. Meyer & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats', </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pp. 478-485. </pages>
Reference: <author> McGonigle, B. & Chalmers, M. </author> <title> (forthcoming), The Growth of Intelligence in Complex Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Minsky, M. </author> <year> (1986), </year> <title> The Society of Mind, </title> <publisher> Simon and Schuster, </publisher> <address> New York, New York. </address>
Reference-contexts: Deictic variables were popularized in the work of Agre and Chapman (Agre & Chapman 1988, Chapman 1990) who used them to write a reactive system that successfully plays computer games. The ideas behind deictic variables can also be found in Minsky's pronemes <ref> (Minsky 1986) </ref>, in visual attention Ullman (1984), and apparently in philosophical work of Heidegger (Dreyfus 1992). Whitehead (1992) uses deictic variables to simplify reinforcement learning.
Reference: <author> Nilsson, N. </author> <year> (1984), </year> <title> Shakey the Robot, </title> <type> Technical note 323, </type> <institution> SRI International, Menlo Park, California. </institution>
Reference-contexts: However, in some ways our new agents look stupider than the robots of the 1970's such as Shakey <ref> (Nilsson 1984) </ref> and FREDDY (Ambler, Barrow, Brown, Burstall & Popplestone 1975). We no longer have demonstrations involving assembly and manipulation as well as map building and navigation. Essentially, we have robots with a very limited set of skills that are capable identifying when those skills should be applied.
Reference: <author> Norman, D. A. & Shallice, T. </author> <year> (1986), </year> <title> Attention to Action: Willed and Automatic Control of Behavior, </title> <editor> in R.Davidson, G. Schwartz & D. Shapiro, eds, `Consciousness and Self Regulation: </editor> <booktitle> Advances in Research and Theory', </booktitle> <volume> Vol. 4, </volume> <publisher> Plenum, </publisher> <address> New York, </address> <pages> pp. 1-18. </pages>
Reference: <author> Rhodes, B. </author> <year> (1996), </year> <title> PHISH-Nets: Planning Heuristically in Situated Hybrid Networks, </title> <type> Master's thesis, </type> <institution> MIT Media Lab. </institution>
Reference: <author> Seyfarth, R. M., Cheney, D. L. & Marler, P. </author> <year> (1980), </year> <title> `Monkey responses to three different alarm calls: Evidence of predator classification and semantic communication', </title> <booktitle> Science 14, </booktitle> <pages> 801-803. </pages>
Reference-contexts: For example, they may give the "eagle" cry for anything in the sky, the "leopard" cry for any animal, the "python" cry for a stick on the ground. They are born attending to the sorts of stimuli they need to be aware of, but learn fine discrimination experientially <ref> (Seyfarth, Cheney & Marler 1980) </ref>. Although animal learning is specialized, individual elements are not necessarily constrained to a single purpose or behavior. Gallistel et al. (1991) that a single adaptive solution or mechanism may be leveraged by multiple processes once established.
Reference: <author> Steels, L. </author> <year> (1994), </year> <title> Building Agents with Autonomous Behavior Systems, </title> <editor> in L. Steels & R. Brooks, eds, </editor> <title> `The `artificial life' route to `artificial intelligence'. Building situated embodied agents.', </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> New Haven. </address> <note> 14 Sundareswaran, </note> <author> V. & Vaina, L. M. </author> <year> (1994), </year> <title> Learning Direction in Global Motion: Two Classes of Psychophysically-Motivated Models, </title> <editor> in G. Tesauro, D. Touretzky & T. Leen, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 7', </booktitle> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: 1 Introduction In some ways embodied agents appear significantly more intelligent now than they did a few decades ago. The reactive approach to robot intelligence <ref> (Steels 1994) </ref> allows us to create rapid, robust behaviors by replacing extensive planning and modeling with carefully engineered behaviors and continuous sensing of the environment (Mataric 1997, Maes 1990).
Reference: <author> Tanji, J. </author> <year> (1996), </year> <title> Involvement of motor areas in the medial frontal cortex of primates in temporal sequencing of multiple movements, </title> <editor> in R. Caminiti, K. Hoffmann, F. Lacquaniti & J. Altman, eds, </editor> <title> `Vision and Movement: Mechanisms in the Cerebral Cortex', </title> <booktitle> Vol. 2, Human Frontier Science Program, Strasbourg, </booktitle> <pages> pp. 126-133. </pages>
Reference-contexts: Recent psychological research indicates that human sequence learning can best be modeled by associating elements of the sequence graded relation to its start and end (Henson 1996). Consolidated motor sequence memory has particular cells dedicated to units of behavior and to motor transitions <ref> (Tanji 1996) </ref>. Presumably, then, some element of memory allows for this transduction by observing episodic memory. We are currently experimenting with navigational systems built on this premise. The meta-state is simply further perceptual state. Karmiloff-Smith (1992) has proposed that development of skills depends on learning new representations for existing behaviors.
Reference: <author> Ullman, S. </author> <year> (1984), </year> <title> `Visual Routines', </title> <journal> Cognition 18, </journal> <pages> 97-159. </pages>
Reference: <author> Whitehead, S. D. </author> <year> (1992), </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action, </title> <type> Technical Report 406, </type> <institution> University of Rochester Computer Science, Rochester, NY. </institution>
Reference: <author> Wilson, M. & McNaughton, B. </author> <year> (1994), </year> <title> `Reactivation of Hippocampal Ensemble Memories During Sleep', </title> <booktitle> Science 261, </booktitle> <pages> 1227-1232. </pages>
Reference-contexts: Recent events are clearer simply because of decay caused by interference, which may be aggravated by change in 11 context. Neurophysiological research demonstrates that a change of context alters the indexing of the place cells in the hippocampus in rats <ref> (Wilson & McNaughton 1994) </ref>. Recent psychological research indicates that human sequence learning can best be modeled by associating elements of the sequence graded relation to its start and end (Henson 1996). Consolidated motor sequence memory has particular cells dedicated to units of behavior and to motor transitions (Tanji 1996).
Reference: <author> Wilson, M. S. </author> <year> (1996), </year> <title> `Reliability and Flexibility | A Mutually Exclusive Proble m for Robotic Assembly?', </title> <journal> IEEE Transactions on Robotics and Automation. </journal>
Reference: <author> Winston, P. H. </author> <year> (1992), </year> <booktitle> Artificial Intelligence, third edition edn, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts. </address>
Reference-contexts: Perception requires memory because very little information is available in each snapshot of sensory information. Meta-State State about other internal state. Research in mainstream ai suggests that learning about one's own behavior patterns and developing meta-rules is necessary for complex, human-like behavior <ref> (Winston 1992) </ref>. Certainly animal brains contain much recurrent wiring. This may be necessary for learning of complex patterns such as hierarchical behavior (McGonigle & Chalmers forthcoming). 4.2 Heuristics for Using Different Types of State The list above is ordered in descending reactivity.
Reference: <author> Wooldridge, M. & Jennings, N. R. </author> <year> (1995), </year> <title> `Intelligent Agents: </title> <journal> Theory and Practice', Knowledge Engineering Review. </journal> <volume> 15 </volume>
Reference-contexts: A more significant criticism, however, is that this approach exchanges the complexity of adequately representing the environment for complexity in designing the control. 5 5.1.1 Programming Control State Programming under the original reactive architectures is notoriously difficult <ref> (Wooldridge & Jen-nings 1995) </ref>. Programming difficulties have been part of the motivation for the two and three layer architectures (Bonasso et al. 1997) and for the situated planning approach in general (Levison 1996). There are two main reasons for this problem.
References-found: 54

