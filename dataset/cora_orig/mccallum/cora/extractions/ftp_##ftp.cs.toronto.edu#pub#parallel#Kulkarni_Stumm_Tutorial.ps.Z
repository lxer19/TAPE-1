URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kulkarni_Stumm_Tutorial.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Email: fkulki,stummg@eecg.toronto.edu  
Title: Loop and Data Transformations: A Tutorial  
Author: Dattatraya Kulkarni and Michael Stumm 
Keyword: Key Words: Dependence Analysis, Iteration and Data Spaces, Hierarchical Memory, Parallelism, Locality, Load Balance, Conventional and Unified Loop transformations, Data Alignment, Data Distributions.  
Date: June 1993  
Address: Toronto, Toronto, Canada, M5S 1A4  
Affiliation: Department of Computer Science and Department of Electrical and Computer Engineering University of  
Abstract: Technical Report CSRI-337, Computer Systems Research Institute, University of Toronto, June 1993. Abstract In this tutorial, we address the problem of restructuring a (possibly sequential) program to improve execution efficiency on parallel machines. This restructuring involves the transformation and partitioning of loop structures and data so as to improve parallelism, static and dynamic locality, and load balance. We present previous and ongoing work on loop and data transformations and motivate a unified framework. 
Abstract-found: 1
Intro-found: 1
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: For a example, the then clause of an if statement is control dependent on the branching condition. A statement that uses the value of a variable assigned by an earlier statement is data dependent on the earlier statement <ref> [Ban88] </ref>. The dependence relation between two statements constrains the order in which the statements may be executed. <p> In this document, we concern ourselves only with data dependence. 1 In this section, we briefly discuss the basic concepts in data dependence and the computational complexity of deciding the existence of a dependence. <ref> [Ban88] </ref> serves as a very good reference 1 Control dependence is important to identify functional level parallelism, and to choose between various candidates for data distribution, among other things. Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. <p> A loop of the above form with linear subscript functions is called an affine loop. Definition 5 (Iteration space) <ref> [Ban88] </ref> 2 I R n such that I = f (i 1 ; :::; i n ) j L 1 i 1 U 1 ; :::; L n (i 1 ; :::; i n1 ) i n U n (i 1 ; :::; i n1 )g; is an iteration space, where <p> The dependence matrix therefore is D = 1 0 2 Because the problem of determining the existence of a dependence is NP-complete, one often employs efficient algorithms that solve restricted cases or provide approximate solutions in practice. GCD test <ref> [Ban88] </ref> finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92].
Reference: [Ban90] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The enhancement we refer to here is the effective reuse taking cache and array sizes into consideration. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 12 thus be characterized by a transformation of the iteration space. Unimodular transformations <ref> [Ban90, WL90, Dow90, KKBP91, KKB91] </ref> are linear transformations on iteration spaces that give a unified view of all possible sequences of most of the existing transformations. <p> When the transformation is unimodular the above approach results in the transformed space that corresponds exactly to the original space. The unimodularity of the transformation matrix ensures that the stride in the transformed loop is unit as well. Banerjee <ref> [Ban90] </ref> computes the bounds for a two dimensional loop directly. Kumar and Kulkarni [KKB91] compute the bounds for a loop of any dimension by transforming the bounding hyper planes in the original loop and substituting for the extremes points of the polyhedron. <p> The concept of unimodular transformations for loop partitioning were introduced independently by Banerjee <ref> [Ban90] </ref>, and by Wolf and Lam [WL90]. Certain significant existence results concerning candidate and optimal unimodular transformations were presented by Dowling [Dow90]. The scope of his work is limited to transformations for inner loop parallelism in affine nested loops. The results are essentially existential and not constructive.
Reference: [BFKK] <author> V. Balasundaram, G. Fox, Ken Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <volume> volume 26. </volume>
Reference-contexts: This cost model can be quite detailed, using estimates for the cost of accesses used in the program <ref> [BFKK, Who92] </ref> or can be quite abstract. It is non-trivial to decide on the appropriate level of detail desired of the cost model. In the following paragraphs we discuss some of the approaches to derivation of array distributions. Exhaustive Search.
Reference: [CF87] <author> Ron Cytron and Jeanne Ferrante. </author> <booktitle> What's in a name? In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <year> 1987. </year>
Reference-contexts: The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences. They can be eliminated by renaming some of the variables <ref> [CF87] </ref>. For this reason we write S1 ffi S2 to mean flow dependence from S 1 to S 2 from now on.
Reference: [D'H89] <author> E.H. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant dependence vectors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 139-144, </pages> <year> 1989. </year>
Reference-contexts: Such a dynamic view is however known to be unnecessary while dealing with uniform and linear Kulkarni and Stumm Loop and Data Transformations: A Tutorial 23 dependence vectors. A number of researchers <ref> [D'H89, IT88, PC87, SF88] </ref> look for completely independent partitions in the iteration space of a nested loop.
Reference: [Dow90] <author> M.L. Dowling. </author> <title> Optimum code parallelization using unimodular transformations. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 155-171, </pages> <year> 1990. </year>
Reference-contexts: The enhancement we refer to here is the effective reuse taking cache and array sizes into consideration. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 12 thus be characterized by a transformation of the iteration space. Unimodular transformations <ref> [Ban90, WL90, Dow90, KKBP91, KKB91] </ref> are linear transformations on iteration spaces that give a unified view of all possible sequences of most of the existing transformations. <p> Unfortunately, the derivation of a unimodular matrix that satisfies the desired requirements is hard in general. The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances <ref> [Dow90] </ref>. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences [Dow90, KKB91]. A dependence matrix can provide a good indication as to the desired transformations. <p> Unfortunately, the derivation of a unimodular matrix that satisfies the desired requirements is hard in general. The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances [Dow90]. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences <ref> [Dow90, KKB91] </ref>. A dependence matrix can provide a good indication as to the desired transformations. <p> The concept of unimodular transformations for loop partitioning were introduced independently by Banerjee [Ban90], and by Wolf and Lam [WL90]. Certain significant existence results concerning candidate and optimal unimodular transformations were presented by Dowling <ref> [Dow90] </ref>. The scope of his work is limited to transformations for inner loop parallelism in affine nested loops. The results are essentially existential and not constructive.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <year> 1992. </year>
Reference-contexts: Li and Chen [LC91] assign 1 for dimensions appearing in for loops, * for multiple affinities to the same dimension, or 1 otherwise. 19 Gupta and Banerjee <ref> [GB92] </ref> use the cost of communication to arrive at a better cost measure of non-alignment. However, their method to estimate the cost is ad hoc: The cost of not aligning dimensions p and q of two different arrays is considered to be the cost of choosing some other arbitrary alignment. <p> Constraint Resolution. The observation that an array distribution can be specified in terms of several parameters, and that reference patterns in the program provide constraints on these parameters led Gupta and Banerjee <ref> [GB92] </ref> to a constraint-based approach. 23 The first step in this approach is to find a permutation alignment of arrays and thus identify the sets of dimensions of the arrays that are similarly distributed. <p> For example, to compute the quality measure of a constraining advocating cyclic distribution we need to know the rest of the parameters. Resorting to some default parameters weakens the detail of the architecture model. The default parameters often tend to be ad hoc. Gupta and Banerjee <ref> [GB92] </ref> have an implementation that shows the feasibility of the approach, albeit with limitations. The constraints could have a scoping rule that they are effective only in certain parts of the program. The detection and the type of the constraint is limited, and has room for improvement. Analytical Method.
Reference: [Ger89] <author> H.M. Gerndt. </author> <title> Automatic parallelization for distributed memory multiprocessing systems. </title> <type> Technical report, PhD thesis, </type> <institution> Bonn University, </institution> <address> FRG, </address> <year> 1989. </year>
Reference-contexts: However, some believe that the distribution is best done by the user. For this reason, extensions have been added to some languages like Fortran to enable one to express the distribution for an array <ref> [HKK + 91, KKBP90, Ger89] </ref>. 21 In particular, in a two dimensional case, we could think of distribution, as two lines, and difference in consecutive intercepts, just as we specified tiling.
Reference: [GJ79] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability, A guide to the theory of NP-Completeness. W.H. </title> <publisher> Freeman and Co., </publisher> <year> 1979. </year>
Reference-contexts: The weights for the sequential programs we have been considering happen to be 1 always and hence does not provide a good cost measure. 20 It is easy to see that MAXCUT <ref> [GJ79] </ref> can be reduced to the problem of partitioning the node set of CAG. The problem is tractable when the CAG is planar [GJ79]. It is interesting to find the characteristics of the programs that result in planar CAGs, and to investigate the fraction such programs form of the benchmarks. <p> programs we have been considering happen to be 1 always and hence does not provide a good cost measure. 20 It is easy to see that MAXCUT <ref> [GJ79] </ref> can be reduced to the problem of partitioning the node set of CAG. The problem is tractable when the CAG is planar [GJ79]. It is interesting to find the characteristics of the programs that result in planar CAGs, and to investigate the fraction such programs form of the benchmarks.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the fortran d programming system. </title> <type> Technical Report CRPC-TR91121, </type> <institution> Dept of computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: However, some believe that the distribution is best done by the user. For this reason, extensions have been added to some languages like Fortran to enable one to express the distribution for an array <ref> [HKK + 91, KKBP90, Ger89] </ref>. 21 In particular, in a two dimensional case, we could think of distribution, as two lines, and difference in consecutive intercepts, just as we specified tiling.
Reference: [HS92] <author> M. Holliday and M. Stumm. </author> <title> Performance evaluation of hierarchical ring-based shared memory mutliprocessors. </title> <type> Technical Report CS-1992-18, </type> <institution> Computer Science Department, Duke University, </institution> <year> 1992. </year>
Reference-contexts: IN order to predict the run time of a nested loop, it is necessary to have a good model of target architectures and the nested loops themselves. In particular, the existing architectural models for non-uniform memory access machines <ref> [SZ91, HS92, KKB92] </ref>, (with possible modifications,) might be able to serve this purpose. Dependences and data mapping, together with the mapping of iterations on the processor space can give us an estimate of the number of remote accesses.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: Such a dynamic view is however known to be unnecessary while dealing with uniform and linear Kulkarni and Stumm Loop and Data Transformations: A Tutorial 23 dependence vectors. A number of researchers <ref> [D'H89, IT88, PC87, SF88] </ref> look for completely independent partitions in the iteration space of a nested loop. <p> These techniques, which have been termed tiling <ref> [RS90, WL91, IT88] </ref> or grouping [KN89], serve the purpose of combining the communication associated with each group in order to save on the overhead for each communications. <p> The tile size can also be tuned to optimize the size of remote data transfers or the the size of the vector registers. Determining the loop bounds of a tiled loop is non-trivial and are usually realized either by conditionals inside the loop body <ref> [IT88] </ref> or by shifts [Ram92b, Kum93]. 5 Data Transformations Loop performance is dependent as much on data organization as it is on code organization. So far we have confined ourselves to reorganizing loop iteration spaces. In this section, we concentrate on reorganizing array structures.
Reference: [KKB91] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Generalized unimodular loop transformations for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <address> Chicago, MI, </address> <year> 1991. </year> <title> Kulkarni and Stumm Loop and Data Transformations: A Tutorial 52 </title>
Reference-contexts: The enhancement we refer to here is the effective reuse taking cache and array sizes into consideration. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 12 thus be characterized by a transformation of the iteration space. Unimodular transformations <ref> [Ban90, WL90, Dow90, KKBP91, KKB91] </ref> are linear transformations on iteration spaces that give a unified view of all possible sequences of most of the existing transformations. <p> The unimodularity of the transformation matrix ensures that the stride in the transformed loop is unit as well. Banerjee [Ban90] computes the bounds for a two dimensional loop directly. Kumar and Kulkarni <ref> [KKB91] </ref> compute the bounds for a loop of any dimension by transforming the bounding hyper planes in the original loop and substituting for the extremes points of the polyhedron. The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. <p> Unfortunately, the derivation of a unimodular matrix that satisfies the desired requirements is hard in general. The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances [Dow90]. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences <ref> [Dow90, KKB91] </ref>. A dependence matrix can provide a good indication as to the desired transformations. <p> The amount of parallelism is the size of the outer loop. Any dependences carried by the outer loop result in non-local accesses. Internalization <ref> [KKBP91, KKB91] </ref> transforms a loop so that as many dependences as possible are independent of the outer loop, and so that the outer loop is as large as possible. <p> The first row of U is the normal to this face, and hence perpendicular to n 1 linearly independent dependences; the second row is chosen to be normal to n 2 dependences so on. Further details can be found in <ref> [KKB91] </ref>. One can only internalize n 1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc.
Reference: [KKB92] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hierarchical parallel machines in polynomial time. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, </address> <year> 1992. </year>
Reference-contexts: Kulkarni and Kumar [KKBP91] introduced the notion of weight to a dependence to characterize the net volume of non-local accesses. They also provided metrics for parallelism and load imbalance for the two dimensional case. Internalization can be further generalized to mapping with multiple parallel levels <ref> [KKB92] </ref>. Locality can be improved by internalizing a dependence or a reference with reuse. In other words, internalization is a transformation that enhances parallelism and locality [KSa]. 3.4 Access Normalization Ideally, we want a processor to own all the data it needs in the course of its computation. <p> IN order to predict the run time of a nested loop, it is necessary to have a good model of target architectures and the nested loops themselves. In particular, the existing architectural models for non-uniform memory access machines <ref> [SZ91, HS92, KKB92] </ref>, (with possible modifications,) might be able to serve this purpose. Dependences and data mapping, together with the mapping of iterations on the processor space can give us an estimate of the number of remote accesses.
Reference: [KKBP90] <author> K.G. Kumar, D. Kulkarni, A. Basu, and A. Paulraj. Aldims: </author> <title> A language for mimd parallelism on distributed memory multiprocessors. </title> <booktitle> In PARCOM-90, </booktitle> <address> Pune, India, </address> <year> 1990. </year>
Reference-contexts: However, some believe that the distribution is best done by the user. For this reason, extensions have been added to some languages like Fortran to enable one to express the distribution for an array <ref> [HKK + 91, KKBP90, Ger89] </ref>. 21 In particular, in a two dimensional case, we could think of distribution, as two lines, and difference in consecutive intercepts, just as we specified tiling.
Reference: [KKBP91] <author> D. Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Ger-many, </address> <year> 1991. </year>
Reference-contexts: The enhancement we refer to here is the effective reuse taking cache and array sizes into consideration. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 12 thus be characterized by a transformation of the iteration space. Unimodular transformations <ref> [Ban90, WL90, Dow90, KKBP91, KKB91] </ref> are linear transformations on iteration spaces that give a unified view of all possible sequences of most of the existing transformations. <p> The parallelism at outer (inner) loop level, volume of communication, the average load, and load balances for the transformed loop can be specified in terms of the elements of the transformation matrix, dependences, and original loop bounds <ref> [KKBP91] </ref>. For example, we may want to find a transformation that minimizes the size of the outer loop level, because it is sequential. <p> The amount of parallelism is the size of the outer loop. Any dependences carried by the outer loop result in non-local accesses. Internalization <ref> [KKBP91, KKB91] </ref> transforms a loop so that as many dependences as possible are independent of the outer loop, and so that the outer loop is as large as possible. <p> One can only internalize n 1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc. Kulkarni and Kumar <ref> [KKBP91] </ref> introduced the notion of weight to a dependence to characterize the net volume of non-local accesses. They also provided metrics for parallelism and load imbalance for the two dimensional case. Internalization can be further generalized to mapping with multiple parallel levels [KKB92].
Reference: [KN89] <author> C.T. King and L.M. Ni. </author> <title> Grouping in nested loops for parallel execution on mul-ticomputers. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 31-38, </pages> <year> 1989. </year>
Reference-contexts: These techniques, which have been termed tiling [RS90, WL91, IT88] or grouping <ref> [KN89] </ref>, serve the purpose of combining the communication associated with each group in order to save on the overhead for each communications. Tiling also results in smaller and less complex iteration spaces for further partitioning, and can improve the cache performance by improving the amount of variable reuse.
Reference: [KSa] <author> D. Kulkarni and M. Stumm. </author> <title> Architecture specific optimal loop transformations. </title> <note> In In preparation. </note>
Reference-contexts: Internalization can be further generalized to mapping with multiple parallel levels [KKB92]. Locality can be improved by internalizing a dependence or a reference with reuse. In other words, internalization is a transformation that enhances parallelism and locality <ref> [KSa] </ref>. 3.4 Access Normalization Ideally, we want a processor to own all the data it needs in the course of its computation. In that case we wish to transform a loop so that it matches the existing layout of the data in the memory of the parallel system 12 .
Reference: [KSb] <author> D. Kulkarni and M. Stumm. </author> <title> On computation alignment. </title> <note> In In preparation. </note>
Reference-contexts: It can increase uniformity across different loops and thus achieve inter loop alignments. It can also improve the dynamic locality among other things. We defer the representation, techniques, legality criterion, different types of computation alignments, and details of applications of computation alignment to the following paper <ref> [KSb] </ref>. 7.3 Cure-all When a sequence of transformations is applied, then one of the transformations can achieve a desired objective such as improved parallelism, but it can undo the effects of an earlier transformation (which was applied to achieve a different objective).
Reference: [Kum93] <author> K.G. Kumar. </author> <type> Personal communication. </type> <year> 1993. </year>
Reference-contexts: The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. All of the above methods fail to produce exact bounds when the transformation is not unimodular. Ramanujam [Ram92a] and Kumar <ref> [Kum93] </ref> compute the new bounds for any non-singular transformation by stepping aside the non-existent iterations. For illustrative purposes, we compute the new bounds for a two dimensional loop directly in the following example. <p> As discussed in a later section, it is an open question as to whether it is possible to derive a transformation that provides the best trade-off parallelism, locality, and load balance. Although a unimodular transformation has its advantages, a loop transformation can be represented by any invertible integer matrix <ref> [LP92, Ram92a, Kum93] </ref>. A non-unimodular linear transformation is more general does not result in unit strides, and computing the transformed loop bounds is more involved. 10 3.3 Internalization Consider the execution of a nested loop on a hierarchical memory architecture, where nonuniform memory access times exist. <p> The tile size can also be tuned to optimize the size of remote data transfers or the the size of the vector registers. Determining the loop bounds of a tiled loop is non-trivial and are usually realized either by conditionals inside the loop body [IT88] or by shifts <ref> [Ram92b, Kum93] </ref>. 5 Data Transformations Loop performance is dependent as much on data organization as it is on code organization. So far we have confined ourselves to reorganizing loop iteration spaces. In this section, we concentrate on reorganizing array structures.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2), </volume> <year> 1974. </year>
Reference-contexts: To be a valid transformation the access matrix has to be invertible. The techniques to make the access matrix invertible do so at the cost of reduced normalization [LP92]. 3.5 Other relevant work The hyperplane or wavefront method suggested by Lamport <ref> [Lam74] </ref> to parallelize do loops is the most influential early work. This method attempts to find the optimum family of hyperplanes of parallelism in the iteration space, maximizing the number of iteration points on each hyperplane and minimizing the inter plane dependences.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed memory machines. </title> <journal> Journal of parallel and distributed computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: The transformation T : (ff; a ) could be either a general linear transformation, a permutation, an embedding, a shift or reflection. In this section we present heuristics for determining the permutation alignment of arrays in a program (due to <ref> [LC91] </ref>). Definition 12 (Permutation Alignment) The alignment function T : (ff; a ) from an n-dimensional data space to another is said to be a permutation if ff is a column permutation of I the identity matrix, and a is zero. <p> The computation for permutation alignment reduces to representing the constraints on the alignment and finding the groups of dimensions of different arrays that go together. Li and Chen <ref> [LC91] </ref> formulate the problem as one of partitioning the node set of component affinity graph defined below. The alignment constraints are restricted to be affinity relations between dimensions of arrays. <p> Kulkarni and Stumm Loop and Data Transformations: A Tutorial 31 The edges of the CAG are given weights corresponding to the penalty for not aligning. Li and Chen <ref> [LC91] </ref> assign 1 for dimensions appearing in for loops, * for multiple affinities to the same dimension, or 1 otherwise. 19 Gupta and Banerjee [GB92] use the cost of communication to arrive at a better cost measure of non-alignment. <p> A permutation alignment is a partitioning of the node set of CAG such that the total weight of edges between nodes in different partitions is minimum. The problem is clearly intractable. 20 Li and Chen <ref> [LC91] </ref> provide a heuristic algorithm. Example 8 Consider program segment for i = 1; n for k = 1; n endfor endfor endfor In a CAG (A,p) denotes p th dimension of array A. The partitions would be f (A,1),(B,2)g and f (A,2)(B,1)g.
Reference: [LP92] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1992. </year>
Reference-contexts: As discussed in a later section, it is an open question as to whether it is possible to derive a transformation that provides the best trade-off parallelism, locality, and load balance. Although a unimodular transformation has its advantages, a loop transformation can be represented by any invertible integer matrix <ref> [LP92, Ram92a, Kum93] </ref>. A non-unimodular linear transformation is more general does not result in unit strides, and computing the transformed loop bounds is more involved. 10 3.3 Internalization Consider the execution of a nested loop on a hierarchical memory architecture, where nonuniform memory access times exist. <p> To be a valid transformation the access matrix has to be invertible. The techniques to make the access matrix invertible do so at the cost of reduced normalization <ref> [LP92] </ref>. 3.5 Other relevant work The hyperplane or wavefront method suggested by Lamport [Lam74] to parallelize do loops is the most influential early work. <p> The above approach is very limited because communication free loops are rare. They do not provide a systematic way of arriving at the loop structure that corresponds to the data partitioning derived. Techniques used in access normalization <ref> [LP92] </ref> that we discussed previously can be applied to do this. A loop partitioning implicitly provides the data partitioning for the lhs array.
Reference: [LYZ90] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> An efficient data dependence analysis for parallelizing compilers. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 26-34, </pages> <year> 1990. </year>
Reference-contexts: Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 5 of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. These dependences are defined as follows. <p> GCD test [Ban88] finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information <ref> [LYZ90, WT92] </ref>. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity has much lower average complexity [Pug92]. A modified Fourier-Motzkin variable elimination [Sch86] produces exact solutions in a reasonable amount of time for many problems.
Reference: [Mac87] <author> M. Mace. </author> <title> Memory storage patterns in parallel processing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: For a program that has flow constraints, the problem of finding an optimal distribution is NP-hard even when the number of allowed distributions is constant, and a fixed number of operations are to be performed <ref> [Mac87] </ref>. The problem is tractable when there are no flow or output dependences involved, but then such programs are uninteresting and uncommon. The intractability of the problem has motivated research in finding good heuristic data distributions, and design tools to assist in the process.
Reference: [MHL91] <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 1-14, </pages> <address> Toronto, Ontario, Canada, </address> <year> 1991. </year> <title> Kulkarni and Stumm Loop and Data Transformations: A Tutorial 53 </title>
Reference-contexts: Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 5 of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. These dependences are defined as follows.
Reference: [OH92] <author> M.F.P O'Boyle and G.A. Hedayat. </author> <title> Data alignment: Transformations to reduce communication on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <publisher> IEE Press, </publisher> <address> Williamsburg, </address> <year> 1992. </year>
Reference-contexts: We confine ourselves to affine references and provide a representation, 13 The processor that owns the lhs in a statement computes it. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 26 pairwise alignment criteria, and general technique for alignment. The representation closely follows <ref> [OH92] </ref> being a good algebraic representation. For notational ease we consider a reference as though it were a column vector instead of a row vector. Let the dimension of an array A be N A . <p> A heuristic to increase the alignment is to maximize the dimension of S 0 <ref> [OH92] </ref>. Assuming the availability of a cost model, one has to enumerate all the possible alignments and choose the best. Moreover, arriving at a cost model (parameterized by the architectural features) at the right level of detail itself is a hard problem. 16 Algorithm 1 [OH92] Construct F A and F <p> the dimension of S 0 <ref> [OH92] </ref>. Assuming the availability of a cost model, one has to enumerate all the possible alignments and choose the best. Moreover, arriving at a cost model (parameterized by the architectural features) at the right level of detail itself is a hard problem. 16 Algorithm 1 [OH92] Construct F A and F B , and thus U , u, U T , C, c, C T . <p> Reduce the following to row echelon form. h U T jC T jI Place column i of C T as the row i of ff if it is independent of the others, otherwise choose an independent column of I in the row echelon form. Example 7 <ref> [OH92] </ref> Consider the alignment of array B to array A in the program segment below. 16 We will see that there exist heuristics for a restricted class of alignments, called permutations.
Reference: [PC87] <author> J.K. Peir and R. Cytron. </author> <title> Minimum distance a method for partitioning recur-resnces for multiprocessors. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 217-224, </pages> <year> 1987. </year>
Reference-contexts: Such a dynamic view is however known to be unnecessary while dealing with uniform and linear Kulkarni and Stumm Loop and Data Transformations: A Tutorial 23 dependence vectors. A number of researchers <ref> [D'H89, IT88, PC87, SF88] </ref> look for completely independent partitions in the iteration space of a nested loop.
Reference: [Pug92] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> In Communications of the ACM, </journal> <volume> volume 35, </volume> <pages> pages 102-114, </pages> <year> 1992. </year>
Reference-contexts: Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 5 of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. These dependences are defined as follows. <p> Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92]. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity has much lower average complexity <ref> [Pug92] </ref>. A modified Fourier-Motzkin variable elimination [Sch86] produces exact solutions in a reasonable amount of time for many problems. In the next section we discuss the loop transformation techniques assume that dependence distance information is available.
Reference: [Ram92a] <author> J. Ramanujam. </author> <title> Non-singular transformations of nested loops. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 214-223, </pages> <year> 1992. </year>
Reference-contexts: The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. All of the above methods fail to produce exact bounds when the transformation is not unimodular. Ramanujam <ref> [Ram92a] </ref> and Kumar [Kum93] compute the new bounds for any non-singular transformation by stepping aside the non-existent iterations. For illustrative purposes, we compute the new bounds for a two dimensional loop directly in the following example. <p> As discussed in a later section, it is an open question as to whether it is possible to derive a transformation that provides the best trade-off parallelism, locality, and load balance. Although a unimodular transformation has its advantages, a loop transformation can be represented by any invertible integer matrix <ref> [LP92, Ram92a, Kum93] </ref>. A non-unimodular linear transformation is more general does not result in unit strides, and computing the transformed loop bounds is more involved. 10 3.3 Internalization Consider the execution of a nested loop on a hierarchical memory architecture, where nonuniform memory access times exist.
Reference: [Ram92b] <author> J. Ramanujam. </author> <title> Tiling of iteration spaces for multicomputers. </title> <booktitle> In Proceedings of the Supercomputing 1992, </booktitle> <pages> pages 179-186, </pages> <year> 1992. </year>
Reference-contexts: The tile size can also be tuned to optimize the size of remote data transfers or the the size of the vector registers. Determining the loop bounds of a tiled loop is non-trivial and are usually realized either by conditionals inside the loop body [IT88] or by shifts <ref> [Ram92b, Kum93] </ref>. 5 Data Transformations Loop performance is dependent as much on data organization as it is on code organization. So far we have confined ourselves to reorganizing loop iteration spaces. In this section, we concentrate on reorganizing array structures.
Reference: [RS90] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling of iteration spaces for multicomputers. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages 179-186, </pages> <year> 1990. </year>
Reference-contexts: These techniques, which have been termed tiling <ref> [RS90, WL91, IT88] </ref> or grouping [KN89], serve the purpose of combining the communication associated with each group in order to save on the overhead for each communications.
Reference: [RS91] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <year> 1991. </year>
Reference-contexts: A second approach is to perform process directed data partitioning. A loop partitioning implicitly provides information on the distribution of some of the arrays. Again, the approach would be limited to loop level. Ramanujam and Sadayappan <ref> [RS91] </ref> formulate the task of data partitioning for arrays appearing in dependence free loops as a linear programming problem.
Reference: [Sch86] <author> A. Schrijver. </author> <title> Theory of linear and integer programming. </title> <publisher> Wiley, </publisher> <year> 1986. </year>
Reference-contexts: Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92]. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity has much lower average complexity [Pug92]. A modified Fourier-Motzkin variable elimination <ref> [Sch86] </ref> produces exact solutions in a reasonable amount of time for many problems. In the next section we discuss the loop transformation techniques assume that dependence distance information is available. <p> If S 0 = S U 1 is lower triangular then it is clear that the new loop bounds can be directly obtained from the rows of S 0 . In general, variable elimination techniques such as Fourier-Motzkin <ref> [Sch86] </ref> has to be applied on S 0 to obtain the new loop bounds. Suppose ff's and fi's are linear expressions in K 1 ; :::; K n1 , and a's and b's are constants.
Reference: [SF88] <author> W. Shang and J.A.B. Fortes. </author> <title> Independent partitioning of algorithms with uniform dependences. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 26-33, </pages> <year> 1988. </year>
Reference-contexts: Such a dynamic view is however known to be unnecessary while dealing with uniform and linear Kulkarni and Stumm Loop and Data Transformations: A Tutorial 23 dependence vectors. A number of researchers <ref> [D'H89, IT88, PC87, SF88] </ref> look for completely independent partitions in the iteration space of a nested loop.
Reference: [SZ91] <author> K.C. Sevcik and S. Zhou. </author> <title> Performance benefits and limitations of large numa mut-liprocessors. </title> <type> Technical Report CSRI-256, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: IN order to predict the run time of a nested loop, it is necessary to have a good model of target architectures and the nested loops themselves. In particular, the existing architectural models for non-uniform memory access machines <ref> [SZ91, HS92, KKB92] </ref>, (with possible modifications,) might be able to serve this purpose. Dependences and data mapping, together with the mapping of iterations on the processor space can give us an estimate of the number of remote accesses.
Reference: [Who92] <author> S. Wholey. </author> <title> Automatic data mapping for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: This cost model can be quite detailed, using estimates for the cost of accesses used in the program <ref> [BFKK, Who92] </ref> or can be quite abstract. It is non-trivial to decide on the appropriate level of detail desired of the cost model. In the following paragraphs we discuss some of the approaches to derivation of array distributions. Exhaustive Search. <p> The reference patterns can provide heuristics to prune the search. Statements in the program also provide the desired relation between distributions for different arrays. The search space is usually very large. It is hard to see how constraints between distributions for arrays can be accommodated in the transitions. <ref> [Who92] </ref> is an example of this approach. Constraint Resolution.
Reference: [WL90] <author> M.E. Wolf and M.S. Lam. </author> <title> An algorithmic approach to compound loop transformation. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <year> 1990. </year>
Reference-contexts: The enhancement we refer to here is the effective reuse taking cache and array sizes into consideration. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 12 thus be characterized by a transformation of the iteration space. Unimodular transformations <ref> [Ban90, WL90, Dow90, KKBP91, KKB91] </ref> are linear transformations on iteration spaces that give a unified view of all possible sequences of most of the existing transformations. <p> The concept of unimodular transformations for loop partitioning were introduced independently by Banerjee [Ban90], and by Wolf and Lam <ref> [WL90] </ref>. Certain significant existence results concerning candidate and optimal unimodular transformations were presented by Dowling [Dow90]. The scope of his work is limited to transformations for inner loop parallelism in affine nested loops. The results are essentially existential and not constructive.
Reference: [WL91] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 30-44, </pages> <address> Toronto, Ontario, Canada, </address> <year> 1991. </year>
Reference-contexts: These techniques, which have been termed tiling <ref> [RS90, WL91, IT88] </ref> or grouping [KN89], serve the purpose of combining the communication associated with each group in order to save on the overhead for each communications.
Reference: [WT92] <author> Michael Wolfe and Chau-Wen Tseng. </author> <title> The power test for data dependence. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 591-601, </pages> <year> 1992. </year>
Reference-contexts: Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. Kulkarni and Stumm Loop and Data Transformations: A Tutorial 5 of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. These dependences are defined as follows. <p> GCD test [Ban88] finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information <ref> [LYZ90, WT92] </ref>. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity has much lower average complexity [Pug92]. A modified Fourier-Motzkin variable elimination [Sch86] produces exact solutions in a reasonable amount of time for many problems.
References-found: 40

