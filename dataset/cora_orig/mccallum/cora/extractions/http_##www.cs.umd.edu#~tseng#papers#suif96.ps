URL: http://www.cs.umd.edu/~tseng/papers/suif96.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Improving the Compiler/Software DSM Interface:  
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  
Abstract: Preliminary Results Abstract Current parallelizing compilers for message-passing machines only support a limited class of data-parallel applications. One method for eliminating this restriction is to combine powerful shared-memory parallelizing compilers with software distributed-shared-memory (DSM) systems. Preliminary results show simply combining the parallelizer and software DSM yields very poor performance. The compiler/software DSM interface can be improved based on relatively little compiler input by: 1) combining synchronization and parallelism information communication on parallel task invocation, 2) employing customized routines for evaluating reduction operations, and 3) selecting a hybrid update protocol to presend data by flushing updates at barriers. These optimizations yield decent speedups for program kernels, but are not sufficient for entire programs. Based on our experimental results, we point out areas where additional compiler analysis and software DSM improvements are necessary to achieve good performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chiaken, K. Johnson, D. Kratz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The combination of ease of use and scalability of software is a key appeal of shared-memory compilers. A recent development that improves the desirability of compiling for software DSMs is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife <ref> [1] </ref>, Flash [18, 23], Typhoon [30, 31]). These architectures maintain a coherent shared address space on top of physically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [2] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory ma chines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The compiler must first apply communication analysis to detect nonlocal accesses. If the nonlocal data is not contiguous, then the compiler must insert code to copy the data to contiguous buffers (one for each processor). The placement copy code can be determined by data dependences using message vectorization <ref> [2, 19] </ref>. The compiler must also modify the code so data so nonlocal accesses are made to the buffers. 5.4 Message Library Support simple applications. Part of the problem is the underlying communication mechanism. The numbers in this paper reflect using UDP sockets as a communication substrate.
Reference: [3] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: To improve spatial locality of local data, the compiler may decide to reindex array references to make local sections of each array contiguous. However, scalar optimizations are required to clean up modulo and division operations inserted into array subscripts <ref> [3] </ref>. Since the compiler is already building a structure for all shared variables, it should also attempt to page align shared data to improve spatial locality at the page level. 5.3 Packing Nonlocal Data Software DSM systems may waste significant communication bandwidth for nonlocal data accesses with poor spatial locality.
Reference: [4] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support. The latest generation of software DSMs (e.g, Munin <ref> [4] </ref>, Blizzard/Tempest [13], CVM [20]) also support multiple coherence protocols and explicit messages on top of existing message-passing machines and networks of workstations.
Reference: [5] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceed ings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs [9, 11, 29], we are aware of only a few projects combining compilers and software DSMs. Bershad et al. <ref> [5] </ref> maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses. CVM, like most software DSMs, relies on the virtual memory system to detect shared memory updates. Results show that the software communication overhead usually dominates the memory management overhead.
Reference: [6] <author> W. Blume et al. </author> <title> Polaris: The next generation in parallelizing compilers,. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Evidence indicates parallelizing compilers for shared-memory machines are beginning to mature. Several research prototypes have been developed with powerful symbolic and interprocedural analyses that can automatically exploit parallelism in many numeric programs (e.g., SUIF [16, 36], Polaris <ref> [6] </ref>). These compilers generate shared-memory programs with parallel constructs such as doall loops and reduction routines. To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support. <p> The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. The SUIF compiler draws on a large body of work on techniques for identifying parallelism <ref> [6, 17, 35] </ref>. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing [7, 12, 34], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [7] <author> W. Bolosky and M. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: The SUIF compiler draws on a large body of work on techniques for identifying parallelism [6, 17, 35]. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing <ref> [7, 12, 34] </ref>, but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [8] <author> R. Bryant, P. Carini, H.-Y. Chang, and B. Rosenburg. </author> <title> Supporting structured shared virtual memory under Mach. </title> <booktitle> In Proceedings of the 2nd Mach Usenix Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: The data shows a large amount of data is being communicated, perhaps more than expected. 4.7 Page Alignment and Multiple Writers The numbers in this paper reflect a multi-writer protocol. CVM also supports several other protocols, including a single-writer protocol <ref> [8] </ref>. Multiple-writer protocols have several advantages. They alleviate the "ping-pong" effect when applications exhibit substantial write sharing (whether it be true sharing or false sharing).
Reference: [9] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Multiple-writer protocols. False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. Multiple-writer coherence protocols <ref> [9] </ref> avoid false sharing by allowing two or more processors to simultaneously modify their local copy of a shared page. Their modifications are merged at the next synchronization operation. In order to capture the modifications to a shared page, it is initially write-protected. <p> A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [9, 11, 29] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. [5] maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses.
Reference: [10] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: HPF is an enhanced Fortran 90 extended with annotations that specify how data should be partitioned across processors. Compilers have been developed (e.g., Fortran D [19], Paradigm [33]) that can translate HPF programs into message-passing programs for distributed-memory machines. Extensive compiler and run-time support (e.g., Chaos <ref> [10] </ref>) have also been developed to handle programs with complicated reference patterns, such as those found in adaptive sparse applications. Though HPF is a good solution for data-parallel applications, there are still a number of disadvantages to using HPF. <p> Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos <ref> [10] </ref>. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. The SUIF compiler draws on a large body of work on techniques for identifying parallelism [6, 17, 35].
Reference: [11] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These compilers generate shared-memory programs with parallel constructs such as doall loops and reduction routines. To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks <ref> [11] </ref>) which support a shared address space using operating systems support. The latest generation of software DSMs (e.g, Munin [4], Blizzard/Tempest [13], CVM [20]) also support multiple coherence protocols and explicit messages on top of existing message-passing machines and networks of workstations. <p> Processor q then piggybacks on the release-acquire message to p write notices for all intervals named in q's current vector timestamp but not in the vector timestamp it received from p. Experiments show alternative implementations of release consistency generally cause more communication than lazy release consistency <ref> [11] </ref>. Invalid, update, and hybrid protocols. Write notices indicate that a page has been modified in a particular interval, but do not contain the actual modifications. The timing of the actual data movement depends on whether an invalidate, an update, or a hybrid protocol is used [11]. <p> than lazy release consistency <ref> [11] </ref>. Invalid, update, and hybrid protocols. Write notices indicate that a page has been modified in a particular interval, but do not contain the actual modifications. The timing of the actual data movement depends on whether an invalidate, an update, or a hybrid protocol is used [11]. Most DSM systems use an invalidate protocol: the arrival of a write notice for a page causes the processor to invalidate its copy of that page. A subsequent access to that page causes an access miss, at which time the modifications are propagated to the local copy. <p> This effect can be countered using a variant of the update protocol called the hybrid protocol, which only sends updates for some pages, allowing other pages to be invalidated <ref> [11] </ref>. Multiple-writer protocols. False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. <p> A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [9, 11, 29] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. [5] maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses.
Reference: [12] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The SUIF compiler draws on a large body of work on techniques for identifying parallelism [6, 17, 35]. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing <ref> [7, 12, 34] </ref>, but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [13] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, A. Rogers, and D. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support. The latest generation of software DSMs (e.g, Munin [4], Blizzard/Tempest <ref> [13] </ref>, CVM [20]) also support multiple coherence protocols and explicit messages on top of existing message-passing machines and networks of workstations.
Reference: [14] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This model is inefficient because it implies communication on each write to a shared data item for which other cached copies exist. In comparison, a release consistency (RC) <ref> [14] </ref> memory consistency model permits a processor to delay making its changes to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of the modifications can thus be postponed until the next synchronization operation takes effect. <p> Programs produce the same results for the two memory models provided that (i) all synchronization operations use system-supplied primitives, and (ii) there is a release-acquire pair between conflicting ordinary accesses to the same memory location on different processors <ref> [14] </ref>. In practice, most shared-memory programs require little or no modifications to meet these requirements. Lazy release consistency. In lazy release consistency (LRC) [21], the propagation of modifications is postponed until the time of the acquire.
Reference: [15] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing [7, 12, 34], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs <ref> [15] </ref>. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. No implementation or experiments are provided.
Reference: [16] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an inter procedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Evidence indicates parallelizing compilers for shared-memory machines are beginning to mature. Several research prototypes have been developed with powerful symbolic and interprocedural analyses that can automatically exploit parallelism in many numeric programs (e.g., SUIF <ref> [16, 36] </ref>, Polaris [6]). These compilers generate shared-memory programs with parallel constructs such as doall loops and reduction routines. To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support.
Reference: [17] <author> M. W. Hall, S. Amarasinghe, and B. Murphy. </author> <title> Interprocedural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. The SUIF compiler draws on a large body of work on techniques for identifying parallelism <ref> [6, 17, 35] </ref>. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing [7, 12, 34], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [18] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J.-P. Singh, R. Simoni, K. Gharachorloo, J. Baxter, D. Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The performance impact of flexibility in the Stan-ford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The combination of ease of use and scalability of software is a key appeal of shared-memory compilers. A recent development that improves the desirability of compiling for software DSMs is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash <ref> [18, 23] </ref>, Typhoon [30, 31]). These architectures maintain a coherent shared address space on top of physically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [19] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Another solution is to use data-parallel languages such as High Performance Fortran (HPF) [22]. HPF is an enhanced Fortran 90 extended with annotations that specify how data should be partitioned across processors. Compilers have been developed (e.g., Fortran D <ref> [19] </ref>, Paradigm [33]) that can translate HPF programs into message-passing programs for distributed-memory machines. Extensive compiler and run-time support (e.g., Chaos [10]) have also been developed to handle programs with complicated reference patterns, such as those found in adaptive sparse applications. <p> Though the process is not as difficult as writing message-passing code, it may still be laborious for large legacy codes. Some compilers for distributed-memory machines can avoid this problem by automatically detecting data-parallelism in sequential programs (e.g., Fortran D <ref> [19] </ref>). More problematic is the fact that some applications may contain irregular data access patterns or parallelism not expressible in the data-parallel constructs found in HPF, such as trees and linked lists found in C. <p> The compiler must first apply communication analysis to detect nonlocal accesses. If the nonlocal data is not contiguous, then the compiler must insert code to copy the data to contiguous buffers (one for each processor). The placement copy code can be determined by data dependences using message vectorization <ref> [2, 19] </ref>. The compiler must also modify the code so data so nonlocal accesses are made to the buffers. 5.4 Message Library Support simple applications. Part of the problem is the underlying communication mechanism. The numbers in this paper reflect using UDP sockets as a communication substrate.
Reference: [20] <author> P. Keleher. </author> <title> Multiple writers considered harmful. </title> <type> Technical Report CS-TR-3543, </type> <institution> Dept. of Computer Science, University of Maryland at College Park, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support. The latest generation of software DSMs (e.g, Munin [4], Blizzard/Tempest [13], CVM <ref> [20] </ref>) also support multiple coherence protocols and explicit messages on top of existing message-passing machines and networks of workstations. <p> We illustrate the issues involved in the compiler/software DSM interface by listing the steps needed to retarget the Stanford SUIF parallelizing compiler [36] to the CVM software DSM system <ref> [20] </ref>. We then point out areas where the interface may be improved with relatively little additional compiler analysis. 3.1 Simple Interface A simple way to interface the SUIF compiler and CVM is to port the SUIF run-time system by using routines from CVM for thread startup, locks, and barriers.
Reference: [21] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In practice, most shared-memory programs require little or no modifications to meet these requirements. Lazy release consistency. In lazy release consistency (LRC) <ref> [21] </ref>, the propagation of modifications is postponed until the time of the acquire. At this time, the acquiring processor determines which modifications it needs to see according to the definition of release consistency.
Reference: [22] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Message-passing programs, however, must deal with separate address spaces, index translation, and explicit interprocessor communication. Writing efficient parallel programs thus require too much effort for most scientists and engineers. Another solution is to use data-parallel languages such as High Performance Fortran (HPF) <ref> [22] </ref>. HPF is an enhanced Fortran 90 extended with annotations that specify how data should be partitioned across processors. Compilers have been developed (e.g., Fortran D [19], Paradigm [33]) that can translate HPF programs into message-passing programs for distributed-memory machines.
Reference: [23] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simon, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The combination of ease of use and scalability of software is a key appeal of shared-memory compilers. A recent development that improves the desirability of compiling for software DSMs is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash <ref> [18, 23] </ref>, Typhoon [30, 31]). These architectures maintain a coherent shared address space on top of physically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [24] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Simply imitating the coherence protocol used by hardware shared-memory multiprocessors is inefficient due to the high communication overhead and large page-sized coherence units. Techniques developed to improve performance of software DSMs are lazy release consistency and multiple-writer protocols. Release consistency. In the conventional sequentially consistent (SC) memory <ref> [24] </ref> model implemented by most snoopy-cache, bus-based multiprocessors, modifications to shared memory must become visible to other processors immediately [24]. This model is inefficient because it implies communication on each write to a shared data item for which other cached copies exist. <p> Techniques developed to improve performance of software DSMs are lazy release consistency and multiple-writer protocols. Release consistency. In the conventional sequentially consistent (SC) memory <ref> [24] </ref> model implemented by most snoopy-cache, bus-based multiprocessors, modifications to shared memory must become visible to other processors immediately [24]. This model is inefficient because it implies communication on each write to a shared data item for which other cached copies exist.
Reference: [25] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: These compilers generate shared-memory programs with parallel constructs such as doall loops and reduction routines. To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy <ref> [25] </ref>, Treadmarks [11]) which support a shared address space using operating systems support. The latest generation of software DSMs (e.g, Munin [4], Blizzard/Tempest [13], CVM [20]) also support multiple coherence protocols and explicit messages on top of existing message-passing machines and networks of workstations.
Reference: [26] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. The SUIF compiler draws on a large body of work on techniques for identifying parallelism [6, 17, 35]. Previous researchers have examined shared-memory compilation issues such as improving locality <ref> [26] </ref> and reducing false sharing [7, 12, 34], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [27] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of DSM systems via compiler involvement. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: No implementation or experiments are provided. CVM uses a multi-writer release consistency protocol, so these optimizations are not as vital as for a sequentially-consistent single-writer protocol. Mirchandaney et al. described the design of a compiler for Treadmarks, a software DSM <ref> [27] </ref>. They propose section locks and broadcast barriers to guide eager updates of data, integrating send, recv and broadcast operations with the software DSM, and reductions based on multiple-writer protocols.
Reference: [28] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: CVM, like most software DSMs, relies on the virtual memory system to detect shared memory updates. Results show that the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs <ref> [28] </ref> on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 [30, 31]. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [10].
Reference: [29] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The run-time system may also support a variety of scheduling policies (e.g., block, round-robin, dynamic) for scheduling iterations of parallel loops to processors. 2.2 Software DSM Software distributed-shared-memory (DSM) systems provide a shared address space on top of physically distributed memory using software support <ref> [29] </ref>. Efficient implementations have been developed that run on commonly available Unix systems making them widely portable, even to standard UNIX workstations connected via ethernet or ATM networks. <p> A disadvantage of the distributed approach is that it requires additional messages. In the centralized approach, all reduction traffic is piggybacked on existing barrier messages. More experiments will be needed. 6 Related Work While there has been a large amount of research on software DSMs <ref> [9, 11, 29] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. [5] maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses.
Reference: [30] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The combination of ease of use and scalability of software is a key appeal of shared-memory compilers. A recent development that improves the desirability of compiling for software DSMs is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash [18, 23], Typhoon <ref> [30, 31] </ref>). These architectures maintain a coherent shared address space on top of physically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance. <p> Results show that the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs [28] on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 <ref> [30, 31] </ref>. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [10]. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance.
Reference: [31] <author> I. Schoinas, B. Falsafi, A. Lebeck, S. Reinhardt, J. Larus, and D. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The combination of ease of use and scalability of software is a key appeal of shared-memory compilers. A recent development that improves the desirability of compiling for software DSMs is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash [18, 23], Typhoon <ref> [30, 31] </ref>). These architectures maintain a coherent shared address space on top of physically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance. <p> Results show that the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs [28] on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 <ref> [30, 31] </ref>. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [10]. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance.
Reference: [32] <author> J.P. Singh, T. Joe, A. Gupta, and J. Hennessy. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: However, performance can be poor unless users extensively rewrite programs to avoid problems such as poor spatial locality and false sharing <ref> [32] </ref>. For distributed-memory machines, users can write message-passing programs that use a standard message-passing library such as PVM, P4, PARMACS, or MPI. Users can achieve high performance because they have total control over interprocessor communication and data layout.
Reference: [33] <author> E. Su, A. Lain, S. Ramaswamy, D. J. Palermo, E. W. Hodges IV, and P. Banerjee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Another solution is to use data-parallel languages such as High Performance Fortran (HPF) [22]. HPF is an enhanced Fortran 90 extended with annotations that specify how data should be partitioned across processors. Compilers have been developed (e.g., Fortran D [19], Paradigm <ref> [33] </ref>) that can translate HPF programs into message-passing programs for distributed-memory machines. Extensive compiler and run-time support (e.g., Chaos [10]) have also been developed to handle programs with complicated reference patterns, such as those found in adaptive sparse applications.
Reference: [34] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The SUIF compiler draws on a large body of work on techniques for identifying parallelism [6, 17, 35]. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing <ref> [7, 12, 34] </ref>, but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [35] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. The SUIF compiler draws on a large body of work on techniques for identifying parallelism <ref> [6, 17, 35] </ref>. Previous researchers have examined shared-memory compilation issues such as improving locality [26] and reducing false sharing [7, 12, 34], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [15].
Reference: [36] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Evidence indicates parallelizing compilers for shared-memory machines are beginning to mature. Several research prototypes have been developed with powerful symbolic and interprocedural analyses that can automatically exploit parallelism in many numeric programs (e.g., SUIF <ref> [16, 36] </ref>, Polaris [6]). These compilers generate shared-memory programs with parallel constructs such as doall loops and reduction routines. To exploit shared-memory compilers for message-passing machines, we rely on software distributed-shared-memory (DSM) systems (e.g., Ivy [25], Treadmarks [11]) which support a shared address space using operating systems support. <p> We illustrate the issues involved in the compiler/software DSM interface by listing the steps needed to retarget the Stanford SUIF parallelizing compiler <ref> [36] </ref> to the CVM software DSM system [20].
References-found: 36

