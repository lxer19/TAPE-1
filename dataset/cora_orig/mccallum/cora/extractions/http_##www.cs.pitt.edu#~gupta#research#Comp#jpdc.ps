URL: http://www.cs.pitt.edu/~gupta/research/Comp/jpdc.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: 
Email: gupta@cs.pitt.edu  
Title: SPMD Execution of Programs with Pointer-based Dynamic Data Structures  
Author: Rajiv Gupta 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: This paper discusses an approach for supporting SPMD (single-program, multiple-data) execution of programs with pointer-based data structures on distributed-memory machines. Through a combination of language design and new compilation techniques, static and dynamic implicit parallelism present in sequential programs based upon pointer-based data structures is exploited. Language support is provided for constructing and manipulating local as well as distributed data structures. The compiler translates the program for execution in SPMD mode in which each processor executes that part of the program which operates on the elements of distributed data structures local to the processor. Therefore the parallelism implicit in a sequential program is exploited. A novel approach for implementing pointers is presented in this paper which is based upon the generation of names for the nodes in a data structure. The name based strategy enables the dynamic distribution of data structures among the processors as well as the traversal of distributed data structures without inter-processor communication.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Balasundaram, B., Fox, G., Kennedy, K., and Kremer, U. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> Proc. Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. </booktitle> <address> Williamsburg, Virginia, </address> <year> 1991, </year> <pages> pp. 213-223. </pages>
Reference-contexts: In addition to the previous work already discussed, work dealing with loop level parallelism in scientific applications using arrays includes the Fortran D system for distributing arrays <ref> [13, 1] </ref>, techniques for identifying parallel loop iterations whose execution does not require communication [14], and identifying communication patterns for efficiently implementing interprocessor communication [21].
Reference: [2] <author> Bennett, J.K., Carter, J.B., and Zwaenepoel, W. Munin: </author> <title> distributed shared memory based on type-specific memory coherence. </title> <booktitle> Proc. Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. </booktitle> <address> Seattle, Washington, </address> <year> 1990, </year> <pages> pp. 168-176. </pages>
Reference-contexts: However, Linda forces the programmer to explicitly program each node. This requires considerable effort on the part of the user. Shared-data structures can be supported through the use of virtual shared-memory implemented on a distributed-memory machine through software coherence mechanisms <ref> [22, 2] </ref>. The overhead associated with software coherence mechanisms is a detriment to achieving high performance. The user may develop parallel applications using shared-memory for which coherence mechanisms may generate excessive communication. Thus, the above approaches do not satisfactorily address the issues of programmability and performance.
Reference: [3] <author> Callahan, D., and Kennedy, K. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <editor> J. </editor> <booktitle> Supercomputing. </booktitle> <address> 2 (Oct. </address> <year> 1988), </year> <pages> 151-169. </pages>
Reference-contexts: The compiler can also transform the program to expose parallelism. Thus, the compiler-based strategy provides us with an avenue for achieving programmability as well as good performance. Callahan and Kennedy have proposed a compilation strategy for the execution of programs in SPMD (single-program, multiple-data) mode on a distributed-memory machine <ref> [3] </ref>. The user specifies the distribution of the data structures and the compiler translates the program for SPMD execution. Every processor executes the same program but operates only on the elements local to the processor. <p> The compiler can also attempt compile-time resolution to reduce the overhead due to run-time resolution [24]. Previous work relating to the SPMD execution of programs on distributed-memory machines has only considered static arrays <ref> [3, 24] </ref>. However, language and compiler support for the SPMD execution 2 of programs that rely upon the use of pointer-based data structures built using pointers has not yet been developed. <p> Thus, the run-time resolution mechanism based upon node names, rather than node addresses, will not require inter-processor communication. A basic compilation strategy of features involving pointer variables is presented. We assume that the compilation strategy developed by Callahan and Kennedy <ref> [3] </ref> is being used for other features. Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. <p> It should be noted that although the hash table is replicated at each processor the nodes in data structure are not replicated. The hash table for a dynamic data structure is similar to the lookup table used by Callahan and Kennedy for describing the distribution of an array <ref> [3] </ref>. The local data structures are implemented in the traditional manner because each processor simply operates upon its copy of the data structure. For consistency, temporary pointer variables and pointer variables of a local data structure type are also represented as ds &lt; : [; ff] &gt;.
Reference: [4] <author> Cardelli, L., and Wegner, P. </author> <title> On understanding types, data abstraction, and polymorphism. </title> <journal> ACM Computing Surveys. </journal> <volume> 17, 4 (1985), </volume> <pages> 471-522. </pages>
Reference-contexts: Thus, the type of temp during the execution of S2 can be determined at compile-time. Using type inference techniques based upon combined forward and backward flow analysis of the program run-time overhead can be reduced <ref> [4] </ref>. - P and Q refer to data structures temp is a temporary pointer variable 25 S1: temp = P!ptr .....
Reference: [5] <author> Carriero, N., Gelernter, D., and Leichter, J. </author> <title> Distributed data structures in Linda. </title> <booktitle> Proc. ACM Symposium on Principles of Programming Languages. </booktitle> <address> St. Petersburg Beach, Florida, </address> <year> 1986, </year> <pages> pp. 236-242. </pages>
Reference-contexts: An added advantage of this approach is that the existing software developed for shared-memory machines fl Supported in part by the National Science Foundation through a Presidential Young Investigator Award CCR-9157371. need not be rewritten for a distributed-memory machine. A language such as Linda <ref> [5] </ref> provides the illusion of shared data structures on a distributed-memory machine. However, Linda forces the programmer to explicitly program each node. This requires considerable effort on the part of the user.
Reference: [6] <author> Chase, D.R., Wegman, M., and Zadeck, F.K. </author> <title> Analysis of pointers and structures. </title> <booktitle> Proc. ACM SIG-PLAN Conference on Programming Language Design and Implementation. </booktitle> <address> White Plains, New York, </address> <year> 1990, </year> <pages> pp. 296-310. </pages>
Reference-contexts: Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. The inclusion of parallel looping constructs [18] in the language and automatic parallelism detection techniques <ref> [12, 20, 6, 16] </ref> in the compiler can further enhance performance. The information regarding parallelism in a program can be used by the compiler to determine a data distribution appropriate for parallel execution.
Reference: [7] <author> Chen, M. </author> <title> A design methodology for synthesizing parallel algorithms and architectures. </title> <journal> J. Parallel Distributed Computing. </journal> <volume> 3, </volume> <month> 4 (Dec. </month> <year> 1986), </year> <pages> 461-491. </pages>
Reference-contexts: Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems <ref> [25, 7] </ref> and compilation techniques for reducing communication on SIMD and VLIW machines [17, 11]. In the next section we present language constructs which enable the user to communicate the important properties of a pointer-based data structure to the compiler.
Reference: [8] <author> Ferrante, J., Ottenstein, K., and Warren, J. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Trans. Programming Languages Systems. </journal> <volume> 9, </volume> <month> 3 (July </month> <year> 1987), </year> <pages> 319-349. </pages>
Reference-contexts: The compiler can propagate messages to the top of a code segment and combine these messages. The notion of control dependence will be useful in propagating communication operations across control structures and create opportunities for combining messages <ref> [8] </ref>. 5 Summary In this paper we addressed the problem of supporting dynamic data structures on distributed-memory machines using the SPMD approach for execution. The SPMD approach exploits static as well as dynamic parallelism implicit in a sequential program.
Reference: [9] <author> Gupta, R., Soffa, M.L., and Steele, T. </author> <title> Register allocation via clique separators. </title> <booktitle> Proc. ACM SIG-PLAN Conference on Programming Language Design and Implementation. </booktitle> <address> Portland, Oregon, </address> <year> 1989, </year> <pages> pp. 264-275. </pages>
Reference-contexts: In addition, a wide variety of graph-based algorithms can benefit from this work. For example, parallelism can be found in register allocation algorithms which are based upon the coloring of an interference graph <ref> [9] </ref> and data flow analysis algorithms which use the control flow graph [19]. The manner in which pointer-based data structures are manipulated (in contrast to static arrays) raises the following problems which must be addressed to enable SPMD execution of programs with pointer-based data structures.
Reference: [10] <author> Gupta, R. </author> <title> A fresh look at optimizing array bound checking. </title> <booktitle> Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation. </booktitle> <address> White Plains, New York, </address> <year> 1990, </year> <pages> pp. 272-282. </pages>
Reference-contexts: The analysis required to eliminate redundant communication operations is similar in nature to the techniques used to eliminate redundant array bound checks <ref> [10] </ref>. If the execution of a statement requires transmission of multiple values from one processor to another, then it may be cheaper to execute the statement at the processor which has the operands and then transmit the result to the other processor.
Reference: [11] <author> Gupta, R., and Soffa, </author> <title> M.L. Compile-time techniques for improving scalar access performance in parallel memories. </title> <journal> IEEE Trans. Parallel Distributed Systems. </journal> <volume> 2, </volume> <month> 2 (April </month> <year> 1991), </year> <pages> 138-148. </pages>
Reference-contexts: Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems [25, 7] and compilation techniques for reducing communication on SIMD and VLIW machines <ref> [17, 11] </ref>. In the next section we present language constructs which enable the user to communicate the important properties of a pointer-based data structure to the compiler. Next we develop a representation of pointer variables and distributed data structures suitable for a distributed-memory machine. <p> In this section we have described the main extensions required to handle aliased data structures. A more detailed description can be found in <ref> [11] </ref>. function TraverseP tr i ( ds, ) = n ( 1) + i + 1 (exists, , ff, orig) = GetNodeInfo (ds,) if not exists then return ( ds &lt; 0 : [ 0 ; 0] &gt; ) elseif (orig 6= 0) then return ( ds &lt; orig : [;
Reference: [12] <author> Hendren, L.J., and Nicolau, A. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Trans. Parallel Distributed Systems. </journal> <volume> 1, </volume> <month> 1 (Jan. </month> <year> 1990), </year> <pages> 35-47. </pages>
Reference-contexts: Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. The inclusion of parallel looping constructs [18] in the language and automatic parallelism detection techniques <ref> [12, 20, 6, 16] </ref> in the compiler can further enhance performance. The information regarding parallelism in a program can be used by the compiler to determine a data distribution appropriate for parallel execution.
Reference: [13] <author> Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., and Tseng, C.-W. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> Proc. Fourth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <address> Santa Clara, California, </address> <year> 1991. </year>
Reference-contexts: In addition to the previous work already discussed, work dealing with loop level parallelism in scientific applications using arrays includes the Fortran D system for distributing arrays <ref> [13, 1] </ref>, techniques for identifying parallel loop iterations whose execution does not require communication [14], and identifying communication patterns for efficiently implementing interprocessor communication [21].
Reference: [14] <author> Huang, C.-H., and Sadayappan, P. </author> <title> Communication-free hyperplane partitioning of nested loops. </title> <booktitle> Proc. Fourth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <address> Santa Clara, Califor-nia, </address> <year> 1991. </year>
Reference-contexts: In addition to the previous work already discussed, work dealing with loop level parallelism in scientific applications using arrays includes the Fortran D system for distributing arrays [13, 1], techniques for identifying parallel loop iterations whose execution does not require communication <ref> [14] </ref>, and identifying communication patterns for efficiently implementing interprocessor communication [21]. Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems [25, 7] and compilation techniques for reducing communication on SIMD and VLIW machines [17, 11].
Reference: [15] <author> Hudak, P., and Smith, L. </author> <title> Para-functional programming: a paradigm for programming multiprocessor systems. </title> <booktitle> Proc. ACM Symposium on Principles of Programming Languages. </booktitle> <address> St. Petersburg Beach, Florida, </address> <year> 1986, </year> <pages> pp. 243-254. </pages>
Reference-contexts: As illustrated by the example in Fig. 2 this option is useful because the user may have knowledge regarding mapping functions which are appropriate for the application at hand <ref> [15] </ref>. However, if the name generation and distribution strategies are not specified by the user one can be chosen by the compiler.
Reference: [16] <author> Klappholz, D., Kallis, A., and Kong, X. </author> <title> Refined C: An update. </title> <booktitle> Proc. Second Workshop on Languages and Compilers for Parallel Computing. </booktitle> <address> Urbana Champaign, Illinois, </address> <year> 1989. </year> <month> 28 </month>
Reference-contexts: Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. The inclusion of parallel looping constructs [18] in the language and automatic parallelism detection techniques <ref> [12, 20, 6, 16] </ref> in the compiler can further enhance performance. The information regarding parallelism in a program can be used by the compiler to determine a data distribution appropriate for parallel execution.
Reference: [17] <author> Knobe, K., Lukas, J., and Steele, G. </author> <title> Data optimization: allocation of arrays to reduce communi-cation on SIMD machines. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 8, 2 (1990), </volume> <pages> 102-118. </pages>
Reference-contexts: Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems [25, 7] and compilation techniques for reducing communication on SIMD and VLIW machines <ref> [17, 11] </ref>. In the next section we present language constructs which enable the user to communicate the important properties of a pointer-based data structure to the compiler. Next we develop a representation of pointer variables and distributed data structures suitable for a distributed-memory machine.
Reference: [18] <author> Koelbel, C., Mehrotra, P., and Rosendale, J.V. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> Proc. Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. </booktitle> <address> Seattle, Washington, </address> <year> 1990, </year> <pages> pp. 177-186. </pages>
Reference-contexts: The compiler creates a mapping between program variables and processors, generates code responsible for run-time resolution, and also generates code for carrying out interprocessor communication at run-time. In addition, the compiler can analyze the program and perform transformations which will increase parallelism and reduce interprocessor communication <ref> [18] </ref>. The compiler can also attempt compile-time resolution to reduce the overhead due to run-time resolution [24]. Previous work relating to the SPMD execution of programs on distributed-memory machines has only considered static arrays [3, 24]. <p> We assume that the compilation strategy developed by Callahan and Kennedy [3] is being used for other features. Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. The inclusion of parallel looping constructs <ref> [18] </ref> in the language and automatic parallelism detection techniques [12, 20, 6, 16] in the compiler can further enhance performance. The information regarding parallelism in a program can be used by the compiler to determine a data distribution appropriate for parallel execution.
Reference: [19] <author> Kramer, R., Gupta, R., and Soffa, </author> <title> M.L. Combining DAG: an approach to parallel data flow analysis. </title> <booktitle> Proc. Sixth International Parallel Processing Symposium. </booktitle> <address> Beverly Hills, California, </address> <year> 1992, </year> <pages> pp. 652-655. </pages>
Reference-contexts: In addition, a wide variety of graph-based algorithms can benefit from this work. For example, parallelism can be found in register allocation algorithms which are based upon the coloring of an interference graph [9] and data flow analysis algorithms which use the control flow graph <ref> [19] </ref>. The manner in which pointer-based data structures are manipulated (in contrast to static arrays) raises the following problems which must be addressed to enable SPMD execution of programs with pointer-based data structures.
Reference: [20] <author> Larus, J.R., and Hilfinger, </author> <title> P.N. Detecting conflicts between structure accesses. </title> <booktitle> Proc. ACM SIG-PLAN Conference on Programming language Design and Implementation. </booktitle> <address> Atlanta, Georgia, </address> <year> 1988, </year> <pages> pp. 21-34. </pages>
Reference-contexts: Although the SPMD execution approach exploits implicit parallelism present in sequential programs, it can benefit from information regarding parallelism present in a program. The inclusion of parallel looping constructs [18] in the language and automatic parallelism detection techniques <ref> [12, 20, 6, 16] </ref> in the compiler can further enhance performance. The information regarding parallelism in a program can be used by the compiler to determine a data distribution appropriate for parallel execution.
Reference: [21] <author> Li, J., and Chen, M. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Trans. Parallel Distrib. Systems. </journal> <volume> 2, </volume> <month> 3 (July </month> <year> 1991), </year> <pages> 361-376. </pages>
Reference-contexts: addition to the previous work already discussed, work dealing with loop level parallelism in scientific applications using arrays includes the Fortran D system for distributing arrays [13, 1], techniques for identifying parallel loop iterations whose execution does not require communication [14], and identifying communication patterns for efficiently implementing interprocessor communication <ref> [21] </ref>. Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems [25, 7] and compilation techniques for reducing communication on SIMD and VLIW machines [17, 11].
Reference: [22] <author> Li, K. </author> <title> Shared virtual memory on loosely coupled multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Yale University, </institution> <year> 1986. </year>
Reference-contexts: However, Linda forces the programmer to explicitly program each node. This requires considerable effort on the part of the user. Shared-data structures can be supported through the use of virtual shared-memory implemented on a distributed-memory machine through software coherence mechanisms <ref> [22, 2] </ref>. The overhead associated with software coherence mechanisms is a detriment to achieving high performance. The user may develop parallel applications using shared-memory for which coherence mechanisms may generate excessive communication. Thus, the above approaches do not satisfactorily address the issues of programmability and performance.
Reference: [23] <author> Mace, M. </author> <title> Memory storage patterns in parallel processing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: The degree of parallelism detected depends on the manner in which the data is distributed among the processors. Determining optimal data distribution automatically is an extremely difficult problem <ref> [23] </ref>. Therefore, language constructs that allow the user to specify the distribution of data structures are provided. The compiler creates a mapping between program variables and processors, generates code responsible for run-time resolution, and also generates code for carrying out interprocessor communication at run-time.
Reference: [24] <author> Rogers, A., and Pingali, K. </author> <title> Process decomposition through locality of reference. </title> <booktitle> Proc. ACM SIG-PLAN Conference on Programming language Design and Implementation. </booktitle> <address> Portland, Oregon, </address> <year> 1989, </year> <pages> pp. 69-80. </pages>
Reference-contexts: In addition, the compiler can analyze the program and perform transformations which will increase parallelism and reduce interprocessor communication [18]. The compiler can also attempt compile-time resolution to reduce the overhead due to run-time resolution <ref> [24] </ref>. Previous work relating to the SPMD execution of programs on distributed-memory machines has only considered static arrays [3, 24]. However, language and compiler support for the SPMD execution 2 of programs that rely upon the use of pointer-based data structures built using pointers has not yet been developed. <p> The compiler can also attempt compile-time resolution to reduce the overhead due to run-time resolution [24]. Previous work relating to the SPMD execution of programs on distributed-memory machines has only considered static arrays <ref> [3, 24] </ref>. However, language and compiler support for the SPMD execution 2 of programs that rely upon the use of pointer-based data structures built using pointers has not yet been developed.
Reference: [25] <author> Tseng, P.-S. </author> <title> Compiling programs for a linear systolic array. </title> <booktitle> Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation. </booktitle> <address> White Plains, New York, </address> <year> 1990, </year> <pages> pp. 311-321. 29 </pages>
Reference-contexts: Work has also also been done on the compilation of systolic applications for fine-grained distributed-memory systems <ref> [25, 7] </ref> and compilation techniques for reducing communication on SIMD and VLIW machines [17, 11]. In the next section we present language constructs which enable the user to communicate the important properties of a pointer-based data structure to the compiler.
References-found: 25

