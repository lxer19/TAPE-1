URL: ftp://ilk.kub.nl/pub/antalb/conll98-vdb-d.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: fantalb,walterg@kub.nl  
Title: Do Not Forget: Full Memory in Memory-Based Learning of Word Pronunciation  
Author: Antal van den Bosch and Walter Daelemans 
Address: P.O. Box 90153, NL-5000 LE Tilburg The Netherlands  
Affiliation: Tilburg University, ILK  
Abstract: Memory-based learning, keeping full memory of learning material, appears a viable approach to learning nlp tasks, and is often superior in generalisation accuracy to eager learning approaches that abstract from learning material. Here we investigate three partial memory-based learning approaches which remove from memory specific task instance types estimated to be exceptional. The three approaches each implement one heuristic function for estimating exceptionality of instance types: (i) typicality, (ii) class prediction strength, and (iii) friendly-neighbourhood size. Experiments are performed with the memory-based learning algorithm ib1-ig trained on English word pronunciation. We find that removing instance types with low prediction strength (ii) is the only tested method which does not seriously harm generalisation accuracy. We conclude that keeping full memory of types rather than tokens, and excluding minority ambiguities appear to be the only performance-preserving op timisations of memory-based learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., editor. </author> <year> 1997. </year> <title> Lazy learning. </title> <publisher> Dordrecht: Kluwer Academic Publishers. </publisher> <editor> reprinted from: </editor> <booktitle> Artificial Intelligence Review, </booktitle> <volume> 11 </volume> <pages> 1-5. </pages>
Reference-contexts: 1 Introduction Memory-based learning of classification tasks is a branch of supervised machine learning in which the learning phase consists simply of storing all encountered instances from a training set in memory <ref> (Aha, 1997) </ref>. Memory-based learning algorithms do not invest effort during learning in abstracting from the training data, such as eager-learning (e.g., decision-tree algorithms, rule-induction, or connectionist-learning algorithms, (Quinlan, 1993; Mitchell, 1997)) do. Rather, they defer investing effort until new instances are presented.
Reference: <author> Aha, D. W., D. Kibler, and M. Albert. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 37-66. </pages>
Reference-contexts: For example, the memory-based learning algorithm ib1-ig (Daelemans and Van den Bosch, 1992; Daelemans, Van den Bosch, and Weijters, 1997b), which extends the well-known ib1 algorithm <ref> (Aha, Kibler, and Albert, 1991) </ref> with an information-gain weighted similarity metric, has been demonstrated to perform adequately and, moreover, consistently and significantly better than eager-learning algorithms which do invest effort in abstraction during learning (e.g., decision-tree learning (Daelemans, Van den Bosch, and Weijters, 1997b; Quinlan, 1993), and connectionist learning (Rumelhart, Hinton, <p> Several functions for computing class-prediction strength have been proposed, e.g., as a criterion for removing instances in memory-based (k-nn) learning algorithms, such as ib3 <ref> (Aha, Ki-bler, and Albert, 1991) </ref> (cf. earlier work on edited k-nn (Wilson, 1972; Voisin and Devijver, 1987)); or for weighting instances in the Each algorithm (Salzberg, 1990; Cost and Salzberg, 1993). We chose to implement the straightforward class-prediction strength function as proposed in (Salzberg, 1990) in two steps.
Reference: <author> Breiman, L. </author> <year> 1996a. </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2). </volume>
Reference-contexts: Another explanation for the difference in performance between decision-tree, connectionist, and editing methods versus pure memory-based learning is that the former generally display high variance, which is the portion of the generalisation error caused by the unstability of the learning algorithm <ref> (Breiman, 1996a) </ref>. An algorithm is unstable when small perturbations in the learning material lead to large differences in induced models, and stable otherwise; pure memory-based learning algorithms are said to be very stable, and decision-tree algorithms and connectionist learning to be unstable (Breiman, 1996a). <p> caused by the unstability of the learning algorithm <ref> (Breiman, 1996a) </ref>. An algorithm is unstable when small perturbations in the learning material lead to large differences in induced models, and stable otherwise; pure memory-based learning algorithms are said to be very stable, and decision-tree algorithms and connectionist learning to be unstable (Breiman, 1996a). High variance is usually coupled with low bias, i.e., unstable learning algorithms with high variance tend to have few limitations in the freedom to approximate the task or function to be learned) (Breiman, 1996b).
Reference: <author> Breiman, L. </author> <year> 1996b. </year> <title> Bias, variance and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> University of Cali-fornia, Statistics Department, Berkeley, </institution> <address> CA. </address>
Reference-contexts: High variance is usually coupled with low bias, i.e., unstable learning algorithms with high variance tend to have few limitations in the freedom to approximate the task or function to be learned) <ref> (Breiman, 1996b) </ref>.
Reference: <author> Burnage, G., </author> <year> 1990. </year> <title> celex: A guide for users. Cen-tre for Lexical Information, </title> <address> Nijmegen. </address>
Reference: <author> Cost, S. and S. Salzberg. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78. </pages>
Reference-contexts: Future research The results of the present study suggest that the following questions be investigated in future research: * The tested criteria for editing can be employed as instance weights as in Each (Salzberg, 1990) and Pebls <ref> (Cost and Salzberg, 1993) </ref>, rather than as criteria for instance removal. Instance weighting, preserving pure memory-based learning, may add relevant information to similarity matching, and may improve ib1 ig's performance. * Different data sets of different sizes may contain different portions of atypical instances or minority ambiguities.
Reference: <author> Cover, T. M. and P. E. Hart. </author> <year> 1967. </year> <title> Nearest neighbor pattern classification. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27. </pages>
Reference: <author> Daelemans, W. </author> <year> 1996. </year> <title> Abstraction considered harmful: lazy learning of language processing. </title> <editor> In H. J. Van den Herik and A. Weijters, editors, </editor> <booktitle> Proceedings of the Sixth Belgian-Dutch Conference on Machine Learning, </booktitle> <pages> pages 3-12, </pages> <address> Maastricht, The Netherlands. matriks. </address>
Reference: <author> Daelemans, W., S. Gillis, and G. Durieux. </author> <year> 1994. </year> <title> The acquisition of stress: a data-oriented approach. </title> <journal> Computational Linguistics, </journal> <volume> 20(3) </volume> <pages> 421-451. </pages>
Reference: <author> Daelemans, W. and A. Van den Bosch. </author> <year> 1992. </year> <title> Gen-eralisation performance of backpropagation learning on a syllabification task. </title> <editor> In M. F. J. Drossaers and A. Nijholt, editors, TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing, </booktitle> <pages> pages 27-37, </pages> <institution> Enschede. Twente University. </institution>
Reference: <author> Daelemans, W., A. Van den Bosch, and A. Weij-ters. </author> <year> 1997a. </year> <title> Empirical learning of natural language processing tasks. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> , <volume> number 1224, </volume> <pages> pages 337-344. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: have also been named lazy, instance-based, exemplar-based, memory-based, case-based learning or reasoning (Stanfill and Waltz, 1986; Kolodner, 1993; Aha, Kibler, and Albert, 1991; Aha, 1997)) Memory-based learning has been demonstrated to yield accurate models of various natural language tasks such as grapheme-phoneme conversion, word stress assignment, part-of-speech tagging, and PP-attachment <ref> (Daelemans, Van den Bosch, and Weijters, 1997a) </ref>. <p> Consistently lower performances are obtained with algorithms that forget by constructing decision trees or connectionist networks, or by editing instance types. Generalisation accuracy appears to be related to the dimension lazy-eager learning; for the gs task (and for many other language tasks, <ref> (Daelemans, Van den Bosch, and Weijters, 1997a) </ref>), it is demonstrated that memory-based lazy learning leads to the best generalisation accuracies.
Reference: <author> Daelemans, W., A. Van den Bosch, and A. Weij-ters. </author> <year> 1997b. </year> <title> igtree: using trees for classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 407-423. </pages>
Reference-contexts: First, ib1-ig selects the class with the highest occurrence within the merged set of classes of the best-matching instance types. In case of occurrence ties, the classification is selected that has the highest overall occurrence in the training set. <ref> (Daelemans, Van den Bosch, and Weijters, 1997b) </ref>. 3.2 Setup We performed a series of experiments in which ib1-ig is applied to the gs data set, systematically edited according to each of the three tested criteria (plus the baseline random criterion) described in the next section. <p> Apart from the possibility that the lazy and eager learning algorithms investigated here and in earlier work do not have a strongly contrasting bias, we conjecture that the editing methods discussed here, and some specific decision-tree learning algorithms investigated earlier (i.e., igtree <ref> (Daelemans, Van den Bosch, and Weijters, 1997b) </ref>, a decision tree learning algorithm that is an approximate optimisation of ib1-ig) have a similar variance to that of ib1-ig; they are virtually as stable as ib1-ig.
Reference: <author> Devijver, P. .A. and J. Kittler. </author> <year> 1982. </year> <title> Pattern recognition. A statistical approach. </title> <address> London, UK: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This procedure is also known as 1-nn, i.e., a search for the single nearest neighbour, the simplest variant of k-nn <ref> (Devijver and Kittler, 1982) </ref>. The weighting function of ib1-ig, W (f i ), represents the information gain of feature f i . Weighting features in k-nn classifiers such as ib1-ig is an active field of research (cf. (Wettschereck, 1995; Wettschereck, Aha, and Mohri, 1997), for comprehensive overviews and discussion).
Reference: <author> Dietterich, T. G., H. Hild, and G. Bakiri. </author> <year> 1990. </year> <title> A comparison of id3 and backpropagation for English text-to-speech mapping. </title> <type> Technical Report 90-20-4, </type> <institution> Oregon State University. </institution>
Reference-contexts: The coding of the output as 159 atomic (`local') classes combining grapheme-phoneme conversion and stress assignment is one out of many types of output coding (Shavlik, Mooney, and Towell, 1991), e.g., distributed bit coding using articulatory features (Se-jnowski and Rosenberg, 1987), error-correcting output coding <ref> (Dietterich, Hild, and Bakiri, 1990) </ref>, or split discrete coding of grapheme-phoneme conversion and stress assignment (Van den Bosch, 1997). <p> at back-propagation learning (Rumelhart, Hinton, and Williams, 1986), using distributed output code, as the better performer as compared to id3 (Quinlan, 1986), a symbolic inductive-learning decision tree algorithm (Di-etterich, Hild, and Bakiri, 1990; Shavlik, Mooney, and Towell, 1991), unless id3 was equipped with error-correcting output codes and additional manual tweaks <ref> (Dietterich, Hild, and Bakiri, 1990) </ref>.
Reference: <author> Kolodner, J. </author> <year> 1993. </year> <title> Case-based reasoning. </title> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mitchell, T. </author> <year> 1997. </year> <title> Machine learning. </title> <address> New York, NY: </address> <publisher> McGraw Hill. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-206. </pages>
Reference-contexts: While these studies point at back-propagation learning (Rumelhart, Hinton, and Williams, 1986), using distributed output code, as the better performer as compared to id3 <ref> (Quinlan, 1986) </ref>, a symbolic inductive-learning decision tree algorithm (Di-etterich, Hild, and Bakiri, 1990; Shavlik, Mooney, and Towell, 1991), unless id3 was equipped with error-correcting output codes and additional manual tweaks (Dietterich, Hild, and Bakiri, 1990). <p> Weighting features in k-nn classifiers such as ib1-ig is an active field of research (cf. (Wettschereck, 1995; Wettschereck, Aha, and Mohri, 1997), for comprehensive overviews and discussion). Information gain is a function from information theory also used in id3 <ref> (Quinlan, 1986) </ref> and c4.5 (Quinlan, 1993). The information gain of a feature expresses its relative relevance compared to the other features when performing the mapping from input to classification.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> c4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, both redundancy and exceptionality cannot be computed trivially; heuristic functions are generally used to estimate them (e.g., functions from information theory <ref> (Quinlan, 1993) </ref>). The lower generalisation accuracies of both decision-tree and connectionist learning, compared to memory-based learning, on the abovementioned nlp tasks, suggest that these heuristic estimates may not be the best choice for learning nlp tasks. <p> Weighting features in k-nn classifiers such as ib1-ig is an active field of research (cf. (Wettschereck, 1995; Wettschereck, Aha, and Mohri, 1997), for comprehensive overviews and discussion). Information gain is a function from information theory also used in id3 (Quinlan, 1986) and c4.5 <ref> (Quinlan, 1993) </ref>. The information gain of a feature expresses its relative relevance compared to the other features when performing the mapping from input to classification.
Reference: <author> Rosch, E. and C. B. Mervis. </author> <year> 1975. </year> <title> Family resemblances: studies in the internal structure of categories. Cognitive Psychology, </title> <editor> 7:??-?? Rumelhart, D. E., G. E. Hinton, and R. J. Williams. </editor> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. Mc-Clelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press, </publisher> <pages> pages 318-362. </pages>
Reference-contexts: We adopt a definition from (Zhang, 1992), who proposes a typicality function. Zhang computes typicalities of instance types by taking both their feature values and their classifications into account (Zhang, 1992). He adopts the notions of intra-concept similarity and inter-concept similarity <ref> (Rosch and Mervis, 1975) </ref> to do this.
Reference: <author> Salzberg, S. </author> <year> 1990. </year> <title> Learning with nested generalised exemplars. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: We chose to implement the straightforward class-prediction strength function as proposed in <ref> (Salzberg, 1990) </ref> in two steps. <p> Future research The results of the present study suggest that the following questions be investigated in future research: * The tested criteria for editing can be employed as instance weights as in Each <ref> (Salzberg, 1990) </ref> and Pebls (Cost and Salzberg, 1993), rather than as criteria for instance removal.
Reference: <author> Sejnowski, T. J. and C. S. Rosenberg. </author> <year> 1987. </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: We define the task as the conversion of fixed-sized instances representing parts of words to a class representing the phoneme and the stress marker of the instance's middle letter. To generate the instances, windowing is used <ref> (Sejnowski and Rosenberg, 1987) </ref>. Table 1 displays example instances and their classifications generated on the basis of the sample word booking. Classifications, i.e., phonemes with stress markers (henceforth PSs), are denoted by composite labels.
Reference: <author> Shavlik, J. W., R. J. Mooney, and G. G. Towell. </author> <year> 1991. </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference-contexts: The task features 159 classes (combined phonemes and stress markers). The coding of the output as 159 atomic (`local') classes combining grapheme-phoneme conversion and stress assignment is one out of many types of output coding <ref> (Shavlik, Mooney, and Towell, 1991) </ref>, e.g., distributed bit coding using articulatory features (Se-jnowski and Rosenberg, 1987), error-correcting output coding (Dietterich, Hild, and Bakiri, 1990), or split discrete coding of grapheme-phoneme conversion and stress assignment (Van den Bosch, 1997).
Reference: <author> Stanfill, C. and D. Waltz. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the acm, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Van den Bosch, A. </author> <year> 1997. </year> <title> Learning to pronounce written words, a study in inductive language learning. </title> <type> Ph.D. thesis, </type> <institution> Universiteit Maastricht. </institution>
Reference-contexts: In this study, we chose a fixed window width of seven letters, which offers sufficient context information for adequate performance, though extension of the window decreases ambiguity within the data set <ref> (Van den Bosch, 1997) </ref>. The task, henceforth referred to as gs (grapheme-phoneme conversion and stress assignment) is similar to the nettalk task presented by Sejnowski and Rosenberg (1986), but is performed on a larger corpus of 77,565 English word-pronunciation pairs, extracted from the celex lexical data base (Bur-nage, 1990). <p> classes combining grapheme-phoneme conversion and stress assignment is one out of many types of output coding (Shavlik, Mooney, and Towell, 1991), e.g., distributed bit coding using articulatory features (Se-jnowski and Rosenberg, 1987), error-correcting output coding (Dietterich, Hild, and Bakiri, 1990), or split discrete coding of grapheme-phoneme conversion and stress assignment <ref> (Van den Bosch, 1997) </ref>. <p> Systematic experiments with the data also used in this paper have indicated that both back-propagation and decision-tree learning (using either distributed or atomic output coding) are consistently and significantly outperformed by memory-based learning of grapheme-phoneme conversion, stress assignment, and the combination of the two <ref> (Van den Bosch, 1997) </ref>, using atomic output coding. <p> latter results. 3 Algorithm and experimental setup 3.1 Memory-based learning in IB1-IG In the experiments reported here, we employ ib1-ig (Daelemans and Van den Bosch, 1992; Daelemans, Van den Bosch, and Weijters, 1997b), which has been demonstrated to perform adequately, and significantly better than eager-learning algorithms on the gs task <ref> (Van den Bosch, 1997) </ref>. ib1-ig constructs an instance base during learning. <p> We base this conjecture on the fact that the standard deviations of both decision-tree learning and memory-based learning trained and tested on the gs data are not only very small (in the order of 1=10 percents), but also hardly different (cf. <ref> (Van den Bosch, 1997) </ref> for details and examples). Only connectionist networks trained with back-propagation and decision-tree learning with pruning display larger standard deviations when accuracies are averaged over exper iments (Van den Bosch, 1997); the stable-unstable dimension might play a role there, but not in the difference between pure memory-based learning <p> on the gs data are not only very small (in the order of 1=10 percents), but also hardly different (cf. <ref> (Van den Bosch, 1997) </ref> for details and examples). Only connectionist networks trained with back-propagation and decision-tree learning with pruning display larger standard deviations when accuracies are averaged over exper iments (Van den Bosch, 1997); the stable-unstable dimension might play a role there, but not in the difference between pure memory-based learning and edited memory-based learning.
Reference: <author> Van den Bosch, A., W. Daelemans, and A. Weijters. </author> <year> 1996. </year> <title> Morphological analysis as classification: an inductive-learning approach. </title> <editor> In K. Oflazer and H. Somers, editors, </editor> <booktitle> Proceedings of the Second International Conference on New Methods in Natural Language Processing, NeMLaP-2, Ankara, Turkey, </booktitle> <pages> pages 79-89. </pages>
Reference: <author> Voisin, J. and P. A. Devijver. </author> <year> 1987. </year> <title> An application of the Multiedit-Condensing technique to the reference selection problem in a print recognition system. </title> <journal> Pattern Recognition, </journal> <volume> 5 </volume> <pages> 465-474. </pages>
Reference: <author> Wettschereck, D. </author> <year> 1995. </year> <title> A study of distance-based machine-learning algorithms. </title> <type> Ph.D. thesis, </type> <institution> Ore-gon State University. </institution>
Reference: <author> Wettschereck, D., D. W. Aha, and T. Mohri. </author> <year> 1997. </year> <title> A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 273-314. </pages>
Reference: <author> Wilson, D. </author> <year> 1972. </year> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> Institute of Electrical and Electronic Engineers Transactions on Systems, Man and Cybernetics, </journal> <volume> 2 </volume> <pages> 408-421. </pages>
Reference: <author> Wolpert, D. H. </author> <year> 1990. </year> <title> Constructing a generalizer superior to NETtalk via a mathematical theory of generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 445-452. </pages>
Reference: <author> Zhang, J. </author> <year> 1992. </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the International Machine Learning Conference 1992, </booktitle> <pages> pages 470-479. </pages>
Reference-contexts: We adopt a definition from <ref> (Zhang, 1992) </ref>, who proposes a typicality function. Zhang computes typicalities of instance types by taking both their feature values and their classifications into account (Zhang, 1992). He adopts the notions of intra-concept similarity and inter-concept similarity (Rosch and Mervis, 1975) to do this. <p> We adopt a definition from <ref> (Zhang, 1992) </ref>, who proposes a typicality function. Zhang computes typicalities of instance types by taking both their feature values and their classifications into account (Zhang, 1992). He adopts the notions of intra-concept similarity and inter-concept similarity (Rosch and Mervis, 1975) to do this. <p> An instance type is atypical when its intra-concept similarity is smaller than its inter-concept similarity, which results in a typicality between 0 and 1. Around typicality value 1, instances cannot be sensibly called typical or atypical; <ref> (Zhang, 1992) </ref> refers to such instances as boundary instances. In our experiments, we compute the typicality of all instance types in the training set, order them on their typicality, and remove 1%, 2%, 5%, and 10% of the instance types with the lowest typicality, i.e., the most atypical instance types. <p> Our implementation of ib1-ig described in (Daelemans and Van den Bosch, 1992; Daelemans, Van den Bosch, and Weijters, 1997b) already makes use of this knowledge, albeit partially (it stores class distributions with letter-window types). Our results also show that atypicality, non-typicality, and typicality <ref> (Zhang, 1992) </ref>, and friendly-neighbourhood size are all estimates of exceptionality that indicate the importance of instance types for classification, rather than their removability.
References-found: 31

