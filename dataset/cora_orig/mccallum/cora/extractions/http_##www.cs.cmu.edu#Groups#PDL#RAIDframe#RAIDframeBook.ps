URL: http://www.cs.cmu.edu/Groups/PDL/RAIDframe/RAIDframeBook.ps
Refering-URL: http://www.cs.cmu.edu/Groups/PDL/RAIDframe/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: RAIDframe: A Rapid Prototyping Tool for RAID Systems  
Author: William V. Courtright II Garth Gibson Mark Holland LeAnn Neal Reilly Jim Zelenka 
Address: 5000 Forbes Avenue Pittsburgh, Pennsylvania 15213-3891  
Web: http://www.cs.cmu.edu/Web/Groups/PDL/RAIDframe/RAIDframe.html  
Affiliation: Parallel Data Laboratory  School of Computer Science Carnegie Mellon University  
Date: 1.0, 29 August 1996  
Note: Version  
Abstract-found: 0
Intro-found: 0
Reference: <author> 134 RAIDframe: </author> <title> A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 </title>
Reference: [Blaum94] <author> Blaum, M., Brady, J., Bruck, J., and Menon, J., Evenodd: </author> <title> An Optimal Scheme for Tolerating Double Disk Failures in RAID Architectures, </title> <booktitle> Proceedings of the International Symposium of Computer Architecture (ISCA), </booktitle> <year> 1994, </year> <pages> pp. 245-54. </pages>
Reference-contexts: see this, assume that disk 2 in the RAID Level 3 diagram within Figure 4 has failed, and note that Multiple-failure tolerance can be achieved in RAID Level 3 by using more than one check disk and a more complex error-detecting/correcting code such as a Reed-Solomon [Peterson72] or MDS code <ref> [Burkhard93, Blaum94] </ref>. RAID Level 3 has very low storage overhead and provides very high data-transfer rates. Since user data is striped on a fine grain, each user access uses all the disks in the array, and hence only one access can be serviced at any one time. <p> An array constructed using such a code can tolerate (n-m) concurrent failures without losing data. The second, described fully by Blaum et al. <ref> [Blaum94] </ref>, clusters together sets of N-1 parity stripes where N is the number of disks in the array and stores two parity units per parity stripe.
Reference: [Brown72] <author> Bernstein, P. A., Hadzilacos, V., and Goodman, N. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: The best-known example of this is the channel-program approach used in the IBM System/370 architecture <ref> [Brown72] </ref>. At the time it was introduced, much of the internal workings of a disk drive were exposed to the system, requiring external control of arm positioning, sector searching, and data transfer.
Reference: [Burkhard93] <author> Burkhard, W. and J. Menon, </author> <title> Disk Array Storage System Reliability, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1993, </year> <pages> pp. 432-441. </pages>
Reference-contexts: see this, assume that disk 2 in the RAID Level 3 diagram within Figure 4 has failed, and note that Multiple-failure tolerance can be achieved in RAID Level 3 by using more than one check disk and a more complex error-detecting/correcting code such as a Reed-Solomon [Peterson72] or MDS code <ref> [Burkhard93, Blaum94] </ref>. RAID Level 3 has very low storage overhead and provides very high data-transfer rates. Since user data is striped on a fine grain, each user access uses all the disks in the array, and hence only one access can be serviced at any one time. <p> There are, however, three reasons why single-failure tolerance may not be adequate for all systems. First, recalling that the reliability of the array falls as the number of disks increases, the reliability of very large single-failure tolerating arrays may be unacceptable <ref> [Burkhard93] </ref>. Second, applications in which data loss has catastrophic consequences may mandate a higher degree of reliability than can be delivered using the RAID architectures described above. <p> In comparing the techniques according to the figures of merit, they show multiple-order-of-magnitude reliability enhancements in moving from single to multiple-fault toleration and achieve this using relatively low check-disk overheads ranging from 2% to 30%. Burkhard and Menon <ref> [Burkhard93] </ref> described two multiple-fault tolerating schemes as examples of maximum-distance-separable (MDS) codes [MacWilliams78]. The first uses a file-dispersal matrix to distribute a block of data (a file in their terminology) into n fragments such that any m &lt; n of them suffice to reconstruct the entire file.
Reference: [Buzen87] <author> Buzen, J.P. and Shum, A.W., </author> <title> A Unified Operational Treatment of RPS Reconnect Delays, </title> <journal> Performance Evaluation Review, </journal> <volume> Vol. 15, No. 1, </volume> <year> 1987. </year>
Reference-contexts: Reading and writing to and from the buffer, instead of directly between the media and the bus, also eliminates rotational-position-sensing (RPS) misses <ref> [Buzen87] </ref>, which occur in bufferless disks when the transfer path to the CPU is not available at the time the data arrives under the disk heads. The second purpose served by the buffer is as a cache memory [IBM0661, Maxtor89].
Reference: [Cabrera91] <author> Cabrera, L.-F. and D. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <journal> Computing Systems, </journal> <volume> Vol. 4, No. 4, </volume> <year> 1991, </year> <pages> pp. 405-439. </pages>
Reference-contexts: They evaluated the performance, overhead, and reliability of several variations on this idea and concluded that distributed RAID has many reliability advantages but performs poorly in the presence of failures. Other studies <ref> [Cabrera91, Hartman93] </ref> have extended this idea to network file systems that stripe data for performance.
Reference: [Cao93] <author> Cao P., Lim, S.B., Venkataraman, S., and Wilkes, J., </author> <title> The TickerTAIP Parallel RAID Architecture, </title> <booktitle> Proceedings of the International Symposium of Computer Architecture (ISCA), </booktitle> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference-contexts: Array controllers and disk buses are often duplicated (indicated by the dotted lines in the figure) so that they do not represent a single point of failure [Katzman77, Menon93]. The controller functionality can also be distributed among the disks of the array <ref> [Cao93] </ref>. As disks get smaller [Gibson92], the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 3b offers an alternative. It uses high-bandwidth, bidirectional serial links for disk interconnection. <p> Cao et al. <ref> [Cao93] </ref> described a disk-array architecture they call TickerTAIP that distributes the controller functionality among several loosely coupled controller nodes. Each node controls a relatively small set of disks (one SCSI string, for example) and communicates with the other nodes via a small, dedicated interconnect network.
Reference: [Cao94] <author> Cao, P., Lim, S. B., Venkataraman, S., and Wilkes, J. </author> <title> The TickerTAIP parallel RAID architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 12, No. 3. </volume> <month> August </month> <year> 1994, </year> <pages> pp. 236-269. </pages>
Reference-contexts: The programs are represented as a linear array of sequentially parsed primitive operations. Similar methods for abstracting the details of storage operations were recently proposed in the distributed, redundant-disk-array architecture called TickerTAIP <ref> [Cao94] </ref>. In TABLE 3.
Reference: [Chen90a] <author> Chen, P. et al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: Section 1.2.2 describes both of these approaches in detail. Studies have shown that, due to superior performance on small read and write operations, a mirrored array, also known as RAID Level 1, may deliver higher performance to many important workloads than can a parity-based array <ref> [Chen90a, Gray90] </ref>. Unfortunately, mirroring is substantially more expensiveits storage overhead for redundancy is 100%, whereas the overhead in a parity-encoded array is generally less than 25% and may be less than 10%. <p> This exibility allows the controller to improve throughput by selecting, for each user read operation, the disk that will incur the least positioning overhead [Bitton88, Bitton89]. This is frequently called the shortest-seek optimization and can improve read throughput by up to about 15% over RAID Level 0 <ref> [Chen90a] </ref>. In degraded mode, the controller sends user write operations that target a unit with one copy on the failed disk only to the surviving disk in the pair instead of to both. <p> They found substantial benefits to this approach. Size S avg positioning time disk xfer rate concurrency 1 ( ) 1 sector+= Redundant Disk Arrays: A Brief Overview 34 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 1.2.2.6.6 Disk-Array Performance Evaluation Chen et al. <ref> [Chen90a] </ref> tackled the thorny problem of comparing RAID Level 5 to RAID Level 1. The comparison is difficult to make because equating the number of actuators causes the array capacities to differ and vice versa.
Reference: [Chen90b] <author> Chen, P. and Patterson, D., </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 322-331. </pages>
Reference-contexts: Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next. The striping unit (or stripe unit) <ref> [Chen90b] </ref> is the maximum amount of consecutive data assigned to a single disk. The array controller has the freedom to set the striping unit arbitrarily; the unit can be as small as a single bit or byte, or as large as an entire disk. <p> Many disks currently on the market support this spindle synchronization. RAID Level 4 is identical to Level 3 except that the striping unit is relatively coarse-grained (perhaps 32KB or larger <ref> [Chen90b] </ref>), rather than a single bit or byte. The block of parity that protects a set of data units is called a parity unit. A set of data units and their corresponding parity unit is called a parity stripe. <p> The paper conceded that none of these problems are insurmountable in RAID arrays but asserted that designers cannot ignore the problem of retrofitting existing systems to use disk arrays. Chen and Patterson <ref> [Chen90b] </ref> developed simple rules of thumb for selecting the striping unit in a nonredundant disk array. They expect that these rules will hold, perhaps with some modification, for redundant arrays as well. <p> The RAIDframe simulator is built on top of the Berkeley Features RAIDframe: A Rapid Prototyping Tool for RAID Systems 57 Version 1.0 8/29/96 RaidSim simulator <ref> [Chen90b, Lee91] </ref>, which was further modified at CMU. In the simulator, the low-level disk operations are simulated by a configurable disk-geometry model instead of being executed by a real disk; the geometry model is configurable to a wide range of disks.
Reference: [Clark82] <author> Clarke, E. and Emerson, </author> <title> E.A. "Synthesis of synchronization skeletons for branching time temporal logic." </title> <booktitle> Proc. of the Workshop on Logic of Programs, </booktitle> <month> May </month> <year> 1981, </year> <institution> Yorktown Heights, NY. </institution> <note> Published as Lecture Notes in Computer Science, Vol. 131. </note> <institution> Wein, </institution> <address> Austria: </address> <publisher> Springer-Verlag, </publisher> <year> 1982, </year> <pages> pp. 52-71. </pages>
Reference: [Clark94] <author> Clarke, E., Grumberg, O., and Long, D. </author> <title> Model checking. </title> <booktitle> Proc. of the International Summer School on Deductive Program Design. Marktoberdorf, </booktitle> <address> Germany. </address> <month> July 26 - August 27, </month> <year> 1994. </year>
Reference: [Copeland89] <author> Copeland, G. and Keller, T., </author> <title> A Comparison of High-Availability Media Recovery Techniques, </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1989, </year> <pages> pp. 98-109. </pages>
Reference-contexts: Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring <ref> [Katzman77, Bitton88, Copeland89, Hsiao91] </ref>, or by parity encoding [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93]. In the former, one or more duplicate copies of each user data unit are stored on separate disks.
Reference: [Courtright94] <author> Courtright, W.V. and Gibson, G., </author> <title> Backward Error Recovery in Redundant Disk Arrays, </title> <booktitle> Proceedings of the 1994 Computer Measurement Group (CMG) Conference, </booktitle> <year> 1994, </year> <title> pp.63-74. RAIDframe: A Rapid Prototyping Tool for RAID Systems 135 Version 1.0 8/29/96 </title>
Reference-contexts: Error recovery is then mechanized without diminish The Importance of RAIDframe to the Research and Development Communities 8 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 ing performance or increasing overheadin contrast to traditional approaches which were manual and prone to error <ref> [Courtright94] </ref>. The programming abstraction RAIDframe uses is based on directed acyclic graphs (DAGs). A designer wishing to introduce a new architecture or optimize an existing architecture is able to achieve this goal by modifying the library of graphs and graph-invocation rules implemented in RAIDframe. <p> To do this, we abstract primitive operations with a wrapper that is responsible for creating the illusion of pass-fail devices, in which pass implies successful completion and fail implies the presence of a permanent fault <ref> [Courtright94] </ref>. By allowing primitive operations to return fail only when an unrecoverable device fault is detected, we are further able to restrict the class of errors observable by RAID operations to those that require handling at the array level.
Reference: [DEC86] <author> Digital Equipment Corporation, </author> <title> Digital Large System Mass Storage Handbook, </title> <year> 1986. </year>
Reference-contexts: Product manuals such as Digital Equipment Corporations Mass Storage Handbook <ref> [DEC86] </ref> provide more thorough descriptions of disk technology. This section describes disk-array structure and functionality in more detail because this information is essential to understanding the RAIDframe prototyping tool. 1.
Reference: [DISK/TREND94] <author> DISK/TREND, Inc., </author> <title> 1994 DISK/TREND Report: Disk Drive Arrays. </title> <address> 1925 Landings Drive, Mountain View, CA, SUM-3. </address>
Reference-contexts: Given how rapidly the market for Redundant Arrays of Independent Disks (RAID) [Patterson88] is growing <ref> [DISK/TREND94] </ref>, these architectures are clearly the storage technology of choice for meeting this demand. The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94].
Reference: [Drapeau94] <author> Drapeau, A., Shirriff, K., Hartman, J., Miller, E., Seshan, S., Katz, R., Patterson, D., Lee, E., Chen, P., and Gibson, G., </author> <title> RAID-II: A High-Bandwidth Network File Server, </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <year> 1994, </year> <pages> pp. 234-44. </pages>
Reference: [Fibre91] <institution> Fibre ChannelPhysical Layer, ANSI X3T9.3 Working Document, Revision 2.1, </institution> <month> May </month> <year> 1991. </year>
Reference: [Fujitsu2360] <author> Fujitsu Corporation, </author> <title> Model M2360A product information. </title>
Reference-contexts: There do exist a few disks that access multiple heads in parallel by careful management of head alignment <ref> [Fujitsu2360] </ref>, but these are not commodity products and typically have lower density and higher cost per megabyte than standard disks.
Reference: [Geist87] <author> Geist, R., Reynolds, R., and Pittard, E., </author> <title> Disk Scheduling in System V, </title> <publisher> ACM.. </publisher>
Reference-contexts: With CVSCAN, adding new queuing disciplines can be achieved simply by assigning new values to the two parameters. New disciplines can also be added to the disk-queue switch by specifying new function calls for create, enqueue, dequeue, promote, and peek. *For more information about CVSCAN, please refer to <ref> [Geist87] </ref>. 3.2.2.2 Disk-Geometry Database This database contains disk specifications used by the simulator.
Reference: [Gelsinger89] <author> Gelsinger, </author> <note> P.P., Gargini, P.A., </note> <author> Parker, G.H., and Yu, A.Y.C., </author> <title> Microprocessors circa 2000, </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1989, </year> <pages> pp. 43-74. </pages>
Reference-contexts: Microprocessors are increasing in computational power at a rate between 25 and 30 percent per year <ref> [Myers86, Gelsinger89] </ref>, and projections for future performance increases range even higher.
Reference: [Gibson89] <author> Gibson, G., Hellerstein, L., Karp, R., Katz, R. and Patterson, D., </author> <title> Coding Techniques for Handling Failures in Large Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1989, </year> <pages> pp. 123-132. </pages>
Reference-contexts: A system in which failed components are self-identifying is called an erasure channel, to distinguish it from an error channel, in which the locations of the errors are not known. An n-failure-detecting code for an error channel becomes an n-failure-correcting code when applied to an erasure channel <ref> [Gibson89, Peterson72] </ref>. RAID Level 3 takes advantage of this fact to reduce the storage overhead for redundancy still further. In RAID Level 3, user data is bit or byte-striped across the data disks, and a simple parity code is used to protect against data loss. <p> The drawback of multiple-failure toleration is that it degrades write performance: in an n-failure-tolerating array, every write operation must update at least n+1 disks so that some record of the write will remain should n of those n+1 disks fail <ref> [Gibson89] </ref>. Thus the write performance of the array decreases in proportion to any increase in n. TABLE 1. <p> Small Accesses Capacity Overhead (%) ConcurrencyRead Write RMW Read Write RMW 0 100 100 100 100 100 100 0 N 3 100 100 100 n/a n/a n/a 100/N 1 Redundant Disk Arrays: A Brief Overview 30 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 Gibson et al. <ref> [Gibson89] </ref> treated multiple-fault tolerance as an error-control coding problem [Peterson72].
Reference: [Gibson92] <author> Gibson, G., </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This means that current and future systems will achieve better I/O performance by increasing the number, rather than the performance, of the individual disks used <ref> [Patterson88, Gibson92] </ref>. As will be seen, this distinction is important in that it implies directly the need for improved data availability. <p> This corresponds to an annual rate of improvement of less than 5%. Increased processor performance leads directly to increased demand for I/O bandwidth <ref> [Gibson92, Kung86, Patterson88] </ref>. Since disk technology is not keeping pace with processor technology, it is necessary to use parallelism in the storage subsystem to meet the increasing demands for I/O bandwidth. <p> Gibson analyzed a set of disk-lifetime data to investigate the accuracy of the assumptions behind this calculation and found reasonable evidence to indicate that the lifetimes of the more mature of these products can be modeled by an exponential distribution <ref> [Gibson92, p. 113] </ref>. Working from this assumption, a 100-disk array composed of disks with a 300,000-hour mean-time-to-failure (typical for current disks) will experience a failure every 3000 hours, or about once every 125 days. <p> Array controllers and disk buses are often duplicated (indicated by the dotted lines in the figure) so that they do not represent a single point of failure [Katzman77, Menon93]. The controller functionality can also be distributed among the disks of the array [Cao93]. As disks get smaller <ref> [Gibson92] </ref>, the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 3b offers an alternative. It uses high-bandwidth, bidirectional serial links for disk interconnection. <p> We refer to the mapping of an applications logical unit of stored data to physical disk locations and associated ECC locations as the disk arrays layout. Fundamental to all disk arrays is the concept of striping consecutive units of user data across the disks of the array <ref> [Kim86, Livny87, Patterson88, Gibson92, Merchant92] </ref>. Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next. <p> Redundant Disk Arrays: A Brief Overview 20 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 comprising the array are divided into data disks and check disks. User data is bit or byte-striped across the data disks, and the check disks hold a Hamming error correcting code <ref> [Peterson72, Gibson92] </ref> computed over the data in the corresponding bits or bytes on the data disks. <p> This paper estimated the reliability of each such component and derived simple techniques for building redundancy into the controllers, cabling, cooling, etc. so as to maximize the overall system reliability. Modeling the reliability of disk arrays was one of the primary topics of Gibsons Ph.D. dissertation <ref> [Gibson92, Gibson93] </ref>. He analyzed all of the assumptions behind the simple equation given above, identified the conditions under which they do and do not hold, and derived new reliability models for conditions not previously covered.
Reference: [Gibson93] <author> Gibson, G. and Patterson, D., </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 17, </volume> <year> 1993, </year> <pages> pp. 4-27. </pages>
Reference-contexts: Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring [Katzman77, Bitton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> This paper estimated the reliability of each such component and derived simple techniques for building redundancy into the controllers, cabling, cooling, etc. so as to maximize the overall system reliability. Modeling the reliability of disk arrays was one of the primary topics of Gibsons Ph.D. dissertation <ref> [Gibson92, Gibson93] </ref>. He analyzed all of the assumptions behind the simple equation given above, identified the conditions under which they do and do not hold, and derived new reliability models for conditions not previously covered.
Reference: [Gibson95] <author> Gibson, G. A., Courtright, W.V., Holland, M., and Zelenka, J., RAID-frame: </author> <title> Rapid Prototyping for Disk Arrays, </title> <institution> CMU-CS-95-200, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference: [Gray81] <author> Gray, J., McJones, P., Blasgen, M., Lindsay, B., Lorie, R., Price, T., Putzolu, F., and Traiger, I. </author> <title> The recovery manager of the System R database manager. </title> <journal> Computing Surveys, </journal> <volume> Vol. 13, No. 2. </volume> <month> June </month> <year> 1981, </year> <pages> pp. 223-242. </pages>
Reference-contexts: More importantly, modeling with graphs has enabled us to simplify and automate error recovery. To do this, we employ an undo-redo error recovery scheme, similar to the one used in the System R recovery manager <ref> [Gray81] </ref>. In our approach, if a primitive operation fails at any time during the execution of a graph, the execution mechanism will automatically undo the effects of the previously completed primitives.
Reference: [Gray90] <author> Gray, G., Horst, B. and Walker, M., </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 148-160. </pages>
Reference-contexts: Section 1.2.2 describes both of these approaches in detail. Studies have shown that, due to superior performance on small read and write operations, a mirrored array, also known as RAID Level 1, may deliver higher performance to many important workloads than can a parity-based array <ref> [Chen90a, Gray90] </ref>. Unfortunately, mirroring is substantially more expensiveits storage overhead for redundancy is 100%, whereas the overhead in a parity-encoded array is generally less than 25% and may be less than 10%. <p> The choice is always made based on the characteristics of the expected workload. Gray, Horst, and Walker <ref> [Gray90] </ref> objected to the notion of striping the data across the disks comprising an array, arguing that fine-grain striping is inappropriate for transaction processing systems because it causes more than one arm to be used per disk request and that coarse-grain striping has several drawbacks when compared to non-striped arrays.
Reference: [Harker81] <author> Harker, J.M , Brede, D.W., Pattison, R.E., Santana, G.R., and Taft, L.G., </author> <title> A Quarter Century of Disk File Innovation, </title> <journal> IBM Journal of Research and Development, </journal> <volume> Vol. 25 no. 5, </volume> <year> 1981, </year> <pages> pp. </pages> <month> 677-689. </month> <title> 136 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 </title>
Reference-contexts: Bell [Bell89] projects supercomputer growth rates of about 150% per year. Disk drives, by way of contrast, have been increasing in performance at a much slower rate. Comparing the state of the art in 1981 <ref> [Harker81] </ref> to that in 1993 [Wood93] shows that the average seek time 1 for a disk drive improved from about 16 ms to about 10 ms, rotational latency from about 8.5 ms to about 5 ms, and data transfer rate from about 3 MB/sec (which was achieved only in the largest
Reference: [Hartman93] <author> Hartman, J. and Ousterhout, J., </author> <title> The Zebra Striped Network File system, </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1993. </year>
Reference-contexts: They evaluated the performance, overhead, and reliability of several variations on this idea and concluded that distributed RAID has many reliability advantages but performs poorly in the presence of failures. Other studies <ref> [Cabrera91, Hartman93] </ref> have extended this idea to network file systems that stripe data for performance.
Reference: [Holland92] <author> Holland, M. and Gibson, G., </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 23-25. </pages>
Reference: [Holland94] <author> Holland, M. </author> <title> On-line Data Reconstruction in Redundant Disk Arrays, </title> <institution> Carn-egie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: In this section, we provide a brief description of a disk-oriented algorithm (taken from <ref> [Holland94] </ref>) for reconstructing lost data into spare disk space. For a more complete discussion of reconstruction algorithms, including performance evaluations and optimizations of the disk-oriented algorithm, please refer to Chapter 4 in [Holland94]. 2.5.1 Disk-Oriented Reconstruction Not only must a single-fault-tolerant disk array recover from the loss of a disk, it <p> In this section, we provide a brief description of a disk-oriented algorithm (taken from <ref> [Holland94] </ref>) for reconstructing lost data into spare disk space. For a more complete discussion of reconstruction algorithms, including performance evaluations and optimizations of the disk-oriented algorithm, please refer to Chapter 4 in [Holland94]. 2.5.1 Disk-Oriented Reconstruction Not only must a single-fault-tolerant disk array recover from the loss of a disk, it should be able to effect this recovery without taking the system off-line. This is implemented by maintaining one or more on-line spare disks in the array.
Reference: [HPC3013] <author> HP Corporation, </author> <title> Disk Drive Model HP C3013 (Kittyhawk) product information. </title>
Reference-contexts: Seek time, rotational latency, and transfer rate are defined in Section 1.2.1. The Need for Improved Availability in the Storage Subsystem RAIDframe: A Rapid Prototyping Tool for RAID Systems 13 Version 1.0 8/29/96 smaller form factors: 2-1/2-inch drives are common in laptop computers [ST9096], and 1.3-inch drives are available <ref> [HPC3013] </ref>. One-inch-diameter disks should appear on the market by 1995 and should be common by about 1998. At a (conservative) projected recording density in excess of 1-2 GB per square inch [Wood93], one such disk should hold well over 2 GB of data. These tiny disks will enable very-large-scale arrays.
Reference: [Hsiao90] <author> Hsiao, H., and DeWitt, D., </author> <title> Chained Declustering: A New Availability Strategy for Multiprocessor Database Machines, </title> <booktitle> Proceedings of the International Data Engineering Conference, </booktitle> <year> 1990. </year>
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2, </type> <year> 1989. </year>
Reference-contexts: The second purpose served by the buffer is as a cache memory <ref> [IBM0661, Maxtor89] </ref>. Applications typically access files sequentially, and so the disks comprising a storage subsystem typically observe a sequential access pattern as well. Thus after each read operation, the disk controller will continue to read sequential data from the media into the buffer.
Reference: [IBM3380] <author> IBM Corporation, </author> <title> IBM 3380 Direct Access Storage Introduction, Manual GC26-4491-0, </title> <year> 1987. </year>
Reference-contexts: This has been, and continues to be, the primary motivation behind disk-array technology. 1.1.2 The Downsizing Trend in Disk Drives Prior to the early 1980s, storage technology was driven by the large-diameter (14-inch) drives <ref> [IBM3380, IBM3390] </ref> used by mainframes in large-scale computing environments such as banks, insurance companies, and airlines. These were the only drives that offered sufficient capacity to meet the requirements of these applications [Wood93]. This changed dramatically with the growth of the personal computer market.
Reference: [IBM3390] <author> IBM Corporation, </author> <title> IBM 3390 Direct Access Storage Introduction, Manual GC26-4573-0, </title> <year> 1989. </year>
Reference-contexts: This has been, and continues to be, the primary motivation behind disk-array technology. 1.1.2 The Downsizing Trend in Disk Drives Prior to the early 1980s, storage technology was driven by the large-diameter (14-inch) drives <ref> [IBM3380, IBM3390] </ref> used by mainframes in large-scale computing environments such as banks, insurance companies, and airlines. These were the only drives that offered sufficient capacity to meet the requirements of these applications [Wood93]. This changed dramatically with the growth of the personal computer market.
Reference: [IEEE89] <institution> Proposed IEEE Standard 802.6Distributed Queue Dual Bus (DQDB)-- Metropolitan Area Network, Draft D7, IEEE 802.6 Working Group, </institution> <year> 1989. </year>
Reference-contexts: Further, by making each link bidirectional, it provides two paths to each disk without duplicating buses. Standards for serial-interface disks have emerged (P1394 [IEEE93], Fibre Channel Fibre91], DQDB <ref> [IEEE89] </ref>) and Seagate has begun shipping drives with serial interfaces. As the cost of high-bandwidth serial connectivity is reduced, architectures similar to that of Figure 3b may supplant todays short, parallel bus-based arrays.
Reference: [IEEE93] <institution> IEEE High Performance Serial Bus Specification, </institution> <address> P1394/Draft 6.2v0, New York, NY, </address> <month> June, </month> <year> 1993. </year>
Reference-contexts: This architecture scales to large arrays more easily because it eliminates the need for the array controller to incorporate a large number of string controllers. Further, by making each link bidirectional, it provides two paths to each disk without duplicating buses. Standards for serial-interface disks have emerged (P1394 <ref> [IEEE93] </ref>, Fibre Channel Fibre91], DQDB [IEEE89]) and Seagate has begun shipping drives with serial interfaces. As the cost of high-bandwidth serial connectivity is reduced, architectures similar to that of Figure 3b may supplant todays short, parallel bus-based arrays.
Reference: [Katz93] <author> Katz, R., Chen, P., Drapeau, A., Lee, E., Lutz, K., Miller, E., Seshan, S., and Patterson, D., </author> <title> RAID-II: Design and Implementation of a Large Scale Disk Array Controller, </title> <booktitle> Symposium on Integrated Systems, </booktitle> <year> 1993. </year>
Reference-contexts: The controller may perform this XOR operation before the write is initiated or as the data ows down to the disks <ref> [Katz93] </ref>.
Reference: [Katzman77] <author> Katzman, J. </author> <title> System Architecture for Nonstop Computing, </title> <booktitle> Proceedings of the Computer Society International Conference (COMPCON 77), </booktitle> <year> 1977. </year>
Reference-contexts: Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring <ref> [Katzman77, Bitton88, Copeland89, Hsiao91] </ref>, or by parity encoding [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93]. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> Array controllers and disk buses are often duplicated (indicated by the dotted lines in the figure) so that they do not represent a single point of failure <ref> [Katzman77, Menon93] </ref>. The controller functionality can also be distributed among the disks of the array [Cao93]. As disks get smaller [Gibson92], the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 3b offers an alternative. <p> The remainder of this section briey introduces each of the levels, and subsequent sections provide additional details. RAID Level 1, also called mirroring or shadowing, is the standard technique used to achieve fault-tolerance in traditional data-storage subsystems <ref> [Katzman77, Bitton88] </ref>. The disks are grouped into mirror pairs, and one copy of each data block is stored on each of the disks in the pair.
Reference: [Kim86] <author> Kim, M., </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35, No. 11, </volume> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring [Katzman77, Bitton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> We refer to the mapping of an applications logical unit of stored data to physical disk locations and associated ECC locations as the disk arrays layout. Fundamental to all disk arrays is the concept of striping consecutive units of user data across the disks of the array <ref> [Kim86, Livny87, Patterson88, Gibson92, Merchant92] </ref>. Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next.
Reference: [Kung86] <author> Kung, H.T., </author> <title> Memory Requirements for Balanced Computer Architectures, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1986, </year> <pages> pp. 49-54. </pages>
Reference-contexts: This corresponds to an annual rate of improvement of less than 5%. Increased processor performance leads directly to increased demand for I/O bandwidth <ref> [Gibson92, Kung86, Patterson88] </ref>. Since disk technology is not keeping pace with processor technology, it is necessary to use parallelism in the storage subsystem to meet the increasing demands for I/O bandwidth.
Reference: [Lee91] <author> Lee, E. and Katz, R., </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. </pages> <month> 190-199. </month> <title> RAIDframe: A Rapid Prototyping Tool for RAID Systems 137 Version 1.0 8/29/96 </title>
Reference-contexts: This assures that the parity-update workload is as well balanced across the disks as the data-update workload. In RAID Level 5, there are a variety of ways to lay out data and parity such that parity is evenly distributed over the disks <ref> [Lee91] </ref>. The structure shown in Figure 4 is called the left-symmetric organization and is formed by first placing the parity units along the diagonal and then placing the consecutive user data units on consecutive disks at the lowest available offset on each disk. <p> The size of the striping unit increases as the con-currency rises in order to gradually reduce the probability that any particular access will use more than one disk arm. Lee and Katz <ref> [Lee91] </ref> described several different strategies for placing the parity units among the striped data units. <p> The RAIDframe simulator is built on top of the Berkeley Features RAIDframe: A Rapid Prototyping Tool for RAID Systems 57 Version 1.0 8/29/96 RaidSim simulator <ref> [Chen90b, Lee91] </ref>, which was further modified at CMU. In the simulator, the low-level disk operations are simulated by a configurable disk-geometry model instead of being executed by a real disk; the geometry model is configurable to a wide range of disks.
Reference: [Livny87] <author> Livny, M., Khoshafian, S., and Boral, H., </author> <title> Multi-disk Management Algorithms, </title> <booktitle> Proceedings of the ACM Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1987, </year> <pages> pp. 69-77. </pages>
Reference-contexts: We refer to the mapping of an applications logical unit of stored data to physical disk locations and associated ECC locations as the disk arrays layout. Fundamental to all disk arrays is the concept of striping consecutive units of user data across the disks of the array <ref> [Kim86, Livny87, Patterson88, Gibson92, Merchant92] </ref>. Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next.
Reference: [Long94] <author> Long, D., Montague, B., and Cabrera, L.-F., Swift/RAID: </author> <title> A Distributed Computing System, </title> <journal> Computing Systems, </journal> <volume> Vol 3., No. 7, </volume> <pages> pp. 333-359, </pages> <year> 1994. </year>
Reference: [MacWilliams78] <author> MacWilliams, F. and Sloane, N., </author> <title> Theory of Error-Correcting Codes, </title> <publisher> North Holland, </publisher> <year> 1978. </year>
Reference-contexts: Burkhard and Menon [Burkhard93] described two multiple-fault tolerating schemes as examples of maximum-distance-separable (MDS) codes <ref> [MacWilliams78] </ref>. The first uses a file-dispersal matrix to distribute a block of data (a file in their terminology) into n fragments such that any m &lt; n of them suffice to reconstruct the entire file. An array constructed using such a code can tolerate (n-m) concurrent failures without losing data.
Reference: [Maxtor89] <author> Maxtor Corporation, </author> <title> XT-8000S Product Specification and OEM Technical Manual, Document 1015586, </title> <year> 1989. </year>
Reference-contexts: The second purpose served by the buffer is as a cache memory <ref> [IBM0661, Maxtor89] </ref>. Applications typically access files sequentially, and so the disks comprising a storage subsystem typically observe a sequential access pattern as well. Thus after each read operation, the disk controller will continue to read sequential data from the media into the buffer.
Reference: [McKeown83] <author> McKeown, D., </author> <title> MAPS: The Organization of a Spatial Database System Using Imagery, Terrain, and Map Data, </title> <institution> Department of Computer Science Technical Report CMU-CS-83-136, Carnegie Mellon University, </institution> <year> 1983. </year>
Reference-contexts: The most visible example of this is in the emergence of digital audio and video applications such as video-on-demand [Rangan93]. Others include scientific visualization and large-object servers such as spatial databases <ref> [McKeown83, Stonebraker92] </ref>. These applications are all characterized by the fact that, if implemented on a large scale, their demands for storage and I/O bandwidth will far exceed the ability of current data storage subsystems to supply them.
Reference: [Menon89] <author> Menon, J. and Kasson, J., </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <institution> IBM Research Division Computer Science Report RJ 6928 (66034), </institution> <year> 1989. </year>
Reference-contexts: This degrades the performance of small write operations by a factor of four when compared to RAID Level 0. Several organizations have been proposed to address this problem. Menon and Kasson <ref> [Menon89, Menon92a] </ref> proposed a technique based on oating the data and/or parity units to different disk locations upon each update.
Reference: [Menon92a] <author> Menon, J. and Kasson, J., </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Unfortunately, mirroring is substantially more expensiveits storage overhead for redundancy is 100%, whereas the overhead in a parity-encoded array is generally less than 25% and may be less than 10%. Furthermore, several recent studies <ref> [Rosenblum91, Menon92a, Stodolsky94] </ref> demonstrated techniques that allow the small-write performance of parity-based arrays to approach and sometimes exceed that of mirroring. 1.2 Technology Background This section describes the structure and organization of modern disk drives and disk arrays; the subsection on disk technology has been kept to a minimum. <p> This degrades the performance of small write operations by a factor of four when compared to RAID Level 0. Several organizations have been proposed to address this problem. Menon and Kasson <ref> [Menon89, Menon92a] </ref> proposed a technique based on oating the data and/or parity units to different disk locations upon each update.
Reference: [Menon92b] <author> Menon, J. and Mattson, D., </author> <title> Comparison of Sparing Alternatives for Disk Arrays, </title> <booktitle> Proceedings of the International Symposium of Computer Architecture (ISCA), </booktitle> <year> 1992, </year> <pages> pp. 318-329. </pages>
Reference-contexts: This spare disk can be viewed as a system resource that is grossly underutilized; the throughput of the array could be increased if this disk is used to service user requests. Menon and Kasson <ref> [Menon92b] </ref> described and evaluated three alternatives for organizing the spare space in a RAID Level 5 disk array. The first, dedicated sparing, is the default approach of dedicating a single disk as the spare.
Reference: [Menon92c] <author> Menon, J. and Mattson, D., </author> <title> Performance of Disk Arrays in Transaction Processing Environments, </title> <booktitle> Conference on Distributed Computing Systems, </booktitle> <year> 1992, </year> <pages> pp. 302-309. </pages>
Reference-contexts: This reduces the number of disk operations required from four to three. This situation is very common in OLTP environments <ref> [TPCA89, Menon92c] </ref>. When the number of data units being updated exceeds half of one parity stripe, there is a more efficient mechanism for updating the parity.
Reference: [Menon93] <author> Menon, J. and Cortney, J., </author> <title> The Architecture of a Fault-Tolerant Cached RAID Controller, </title> <booktitle> Proceedings of the International Symposium of Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 76-86. </pages>
Reference-contexts: Array controllers and disk buses are often duplicated (indicated by the dotted lines in the figure) so that they do not represent a single point of failure <ref> [Katzman77, Menon93] </ref>. The controller functionality can also be distributed among the disks of the array [Cao93]. As disks get smaller [Gibson92], the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 3b offers an alternative. <p> The paper showed simulation results indicating that this technique can allow the performance of RAID Level 5 arrays to approach, or under certain conditions even exceed, that of mirroring. Menon and Cortney <ref> [Menon93] </ref> described the architecture of a controller that improves small-write performance by deferring the actual update operations for some period of time after the application performs the write. In this approach, the controller stores the data associated with a write in a nonvolatile, fault-tolerant cache memory in the array controller. <p> In this sense, this scheme is similar to the deferred-update techniques described by Menon and Cor-ney <ref> [Menon93] </ref> with the primary difference being that reads are not cached in Solworth and Orjis proposal, and the cache replacement policies are adapted to account for this. The authors do not address the question of whether some of the memory used for write-caching would be better used for read-caching.
Reference: [Merchant92] <author> Merchant, A. and Yu, P., </author> <title> Performance Analysis of A Dual Striping Strategy for Replicated Disk Arrays, </title> <booktitle> Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1992. </year>
Reference-contexts: We refer to the mapping of an applications logical unit of stored data to physical disk locations and associated ECC locations as the disk arrays layout. Fundamental to all disk arrays is the concept of striping consecutive units of user data across the disks of the array <ref> [Kim86, Livny87, Patterson88, Gibson92, Merchant92] </ref>. Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next. <p> The left-symmetric parity placement illustrated in the RAID Level 5 case of Figure 4 was among the best of the options. Merchant and Yu <ref> [Merchant92] </ref> noted that it is common for a database workload to consist of two components: transactions and ad hoc, read-only queries into the database. Transactions generate small, randomly distributed accesses into the array, whereas the ad hoc queries often scan significant portions of the database.
Reference: [Meyers78] <author> Meyers, G. J. </author> <title> Composite/Structured Design. </title> <address> New York: </address> <publisher> Nav Nostrand Reinhold Co., </publisher> <year> 1978. </year>
Reference-contexts: By treating RAID operations as programs, we are able to minimize the number of code changes required to extend the software. The best-known method for doing this is to create modular code which isolates functions that are known to change orthogonally with architecture <ref> [Meyers78] </ref>. FIGURE 8 Isolating Common Infrastructure As Figure 8 shows, the most obvious functions that vary with array architecture are data encoding, information layout, and operation structure.
Reference: [Mogi94] <author> Mogi, K., and Kitsuregawa, M., </author> <title> Dynamic Parity Stripe Reorganizations for RAID5 Disk Arrays, </title> <booktitle> Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1994, </year> <pages> pp. 17-26. </pages>
Reference-contexts: The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance <ref> [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>. While many of these proposals are promising, they have been largely evaluated only by simulation or analytic modeling. To understand the advantages and limitations of these new designs, it is essential for RAID architects to experiment with concrete implementations.
Reference: [Muntz90] <author> Muntz, R. and Lui, J., </author> <title> Performance Analysis of Disk Arrays Under Failure, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. </pages> <month> 162-173. </month> <title> 138 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 </title>
Reference: [Myers86] <author> Myers, G.J., Yu, A.Y.C., and House, </author> <title> D.L., Microprocessor Technology Trends, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 74, No. 12, </volume> <year> 1986. </year>
Reference-contexts: Microprocessors are increasing in computational power at a rate between 25 and 30 percent per year <ref> [Myers86, Gelsinger89] </ref>, and projections for future performance increases range even higher.
Reference: [Ng92] <author> Ng, S. and Mattson, R., </author> <title> Maintaining Good Performance in Disk Arrays During Failure Via Uniform Parity Group Distribution, </title> <booktitle> Proceedings of the First International Symposium on High-Performance Distributed Computing, </booktitle> <year> 1992, </year> <pages> pp. 260-269. </pages>
Reference: [Orji93] <author> Orji, C. and Solworth, J., </author> <title> Doubly Distorted Mirrors, </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1993, </year> <pages> pp. 307-316. </pages>
Reference-contexts: The authors do not address the question of whether some of the memory used for write-caching would be better used for read-caching. In two follow-up studies, Solworth and Orji proposed distorted mirrors [Solworth91] and doubly distorted mirrors <ref> [Orji93] </ref>. In the former, the controller updates data in place on the primary disk in a mirror pair but writes the data to any convenient location on the secondary drive. The controller maintains a data structure in memory describing the location of each block on the secondary drive.
Reference: [Park86] <author> Park, A. and Balasubramanian, K., </author> <title> Providing Fault Tolerance in Parallel Secondary Storage Systems, </title> <institution> Princeton University Technical Report CS-TR-057-86, </institution> <year> 1986. </year>
Reference-contexts: Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring [Katzman77, Bitton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., and Katz, R.A., </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Given how rapidly the market for Redundant Arrays of Independent Disks (RAID) <ref> [Patterson88] </ref> is growing [DISK/TREND94], these architectures are clearly the storage technology of choice for meeting this demand. The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94]. <p> This means that current and future systems will achieve better I/O performance by increasing the number, rather than the performance, of the individual disks used <ref> [Patterson88, Gibson92] </ref>. As will be seen, this distinction is important in that it implies directly the need for improved data availability. <p> This corresponds to an annual rate of improvement of less than 5%. Increased processor performance leads directly to increased demand for I/O bandwidth <ref> [Gibson92, Kung86, Patterson88] </ref>. Since disk technology is not keeping pace with processor technology, it is necessary to use parallelism in the storage subsystem to meet the increasing demands for I/O bandwidth. <p> Specifically, assuming the failure rates for a set of disks to be identical, independent, exponentially distributed random variables, a simple reliability calculation shows that the mean time to data loss for a group of N disks is only 1/N times as long as that of a single disk <ref> [Patterson88] </ref>. Gibson analyzed a set of disk-lifetime data to investigate the accuracy of the assumptions behind this calculation and found reasonable evidence to indicate that the lifetimes of the more mature of these products can be modeled by an exponential distribution [Gibson92, p. 113]. <p> Disk arrays typically incorporate some form of redundancy in order to protect against data loss when these failures occur. This is generally achieved either by disk mirroring [Katzman77, Bitton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Kim86, Park86, Patterson88, Gibson93] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> In the former, one or more duplicate copies of each user data unit are stored on separate disks. In the latter, commonly known as Redundant Arrays of Inexpensive 1 Disks (RAID) <ref> [Patterson88] </ref>, a portion of the arrays physical capacity is used to store an error-correcting code computed over the data stored in the array. Section 1.2.2 describes both of these approaches in detail. <p> We refer to the mapping of an applications logical unit of stored data to physical disk locations and associated ECC locations as the disk arrays layout. Fundamental to all disk arrays is the concept of striping consecutive units of user data across the disks of the array <ref> [Kim86, Livny87, Patterson88, Gibson92, Merchant92] </ref>. Striping is defined as breaking up the linear address space exported by the array controller into blocks of some size and assigning the consecutive blocks to consecutive disks rather than filling each disk with consecutive data before switching to the next. <p> Therefore, a fine-grain-striped array can service only one I/O at any one time but is capable of reading or writing the data at a very high rate. Patterson, Gibson, and Katz <ref> [Patterson88] </ref> classified redundant disk arrays into five types, called RAID Levels 1 through 5, based on the organization of redundant information and the layout of user data on the disks. This terminology has gained wide acceptance [RAID93] and is used throughout this document. <p> When a user write request updates data for which the parity has failed, the data is simply written in place since no parity-maintenance operations are possible. 1.2.2.4 Comparing the Performance of the RAID Levels Table 1, adapted from Patterson, Gibson, and Katz <ref> [Patterson88] </ref>, compares the fault-free performance and capacity overhead of the RAID levels. The values are all first-order approximations since there are a wide variety of effects related to seek distance, head synchronization, access patterns, etc., that inuence performance, but the table provides a baseline comparison. <p> The authors evaluated the architectures by implementing them in real hardware and applying synthetically generated workloads that varied in the parameters of interest. The results largely validated the simple model of Patterson et al. <ref> [Patterson88] </ref>, which is approximated in Table 1. <p> They further showed that due to the shortest-seek optimization, the RAID Level 1 outperformed the RAID Level 5 on small-access-dominated-workloads, whereas the reverse was true on large-access workloads due to more efficient write operations in RAID Level 5. 1.2.2.6.7 Reliability Modeling Patterson et. al. <ref> [Patterson88] </ref> derived a simple expression for the mean time to data loss (MTTDL) in a redundant disk array: where MTTF disk is the mean time to failure of a component disk; N groups is the number of independent groups in the array, each of which contains N diskspergroup disks, including the
Reference: [Peterson72] <author> Peterson, W. and Weldon Jr., E., </author> <title> Error-Correcting Codes, second edition, </title> <publisher> MIT Press, </publisher> <year> 1972. </year>
Reference-contexts: Redundant Disk Arrays: A Brief Overview 20 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 comprising the array are divided into data disks and check disks. User data is bit or byte-striped across the data disks, and the check disks hold a Hamming error correcting code <ref> [Peterson72, Gibson92] </ref> computed over the data in the corresponding bits or bytes on the data disks. <p> A system in which failed components are self-identifying is called an erasure channel, to distinguish it from an error channel, in which the locations of the errors are not known. An n-failure-detecting code for an error channel becomes an n-failure-correcting code when applied to an erasure channel <ref> [Gibson89, Peterson72] </ref>. RAID Level 3 takes advantage of this fact to reduce the storage overhead for redundancy still further. In RAID Level 3, user data is bit or byte-striped across the data disks, and a simple parity code is used to protect against data loss. <p> To see this, assume that disk 2 in the RAID Level 3 diagram within Figure 4 has failed, and note that Multiple-failure tolerance can be achieved in RAID Level 3 by using more than one check disk and a more complex error-detecting/correcting code such as a Reed-Solomon <ref> [Peterson72] </ref> or MDS code [Burkhard93, Blaum94]. RAID Level 3 has very low storage overhead and provides very high data-transfer rates. <p> Write RMW 0 100 100 100 100 100 100 0 N 3 100 100 100 n/a n/a n/a 100/N 1 Redundant Disk Arrays: A Brief Overview 30 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 Gibson et al. [Gibson89] treated multiple-fault tolerance as an error-control coding problem <ref> [Peterson72] </ref>.
Reference: [Polyzois93] <author> Polyzois, C., Bhide, A., and Dias, D., </author> <title> Disk Mirroring with Alternating Deferred Updates, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1993, </year> <pages> pp. 604-617. </pages>
Reference-contexts: The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance <ref> [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>. While many of these proposals are promising, they have been largely evaluated only by simulation or analytic modeling. To understand the advantages and limitations of these new designs, it is essential for RAID architects to experiment with concrete implementations. <p> In the latter (doubly distorted mirrors), the authors combined the ideas of a write-only cache and write-anywhere semantics on the secondary drive to eliminate the necessity that the cache be nonvolatile and fault-tolerant. Polyzois, Bhide, and Dias <ref> [Polyzois93] </ref> proposed a modification to the deferred-write technique in which the two disk arms in a mirror pair alternate between reading and writing. Deferred writes accumulate in the cache for some period of time, and then the controller batches them together and writes them out to one drive.
Reference: [RAID96] <author> RAID Advisory Board, </author> <title> The RAIDBook: A Source Book for RAID Technology, </title> <editor> 5th Ed., St. Peter, </editor> <address> Minnesota, </address> <year> 1996. </year>
Reference-contexts: The text has been updated and edited in minor ways to allow it to fit into the RAIDframe documentation. For a more thorough description of RAID technology, we recommend The RAIDbook: A Source Book for Disk Array Technology <ref> [RAID96] </ref>. 1.1 The Need for Improved Availability in the Storage Subsystem There exist several trends in the computer industry that are driving the design of storage subsystems toward higher levels of parallelism. <p> Because of industrial interest in using the RAID acronym and because of their concerns about the restrictiveness of its Inexpensive component, RAID is often reported as an acronym for Redundant Arrays of Independent Disks <ref> [RAID96] </ref>. Technology Background RAIDframe: A Rapid Prototyping Tool for RAID Systems 15 Version 1.0 8/29/96 FIGURE 1 Physical Components of a Disk Drive 1.2.1 Disk Technology stack of platters coated with magnetic media with data stored on all surfaces.
Reference: [Ramakrishnan92] <author> Ramakrishnan, K., Biswas, P., and Karedla, R., </author> <title> Analysis of File I/O Traces in Commercial Computing Environments, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference: [Rangan93] <author> Rangan, P.V., and Vin, H.M., </author> <title> Efficient Storage Techniques for Digital Continuous Multimedia, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 5, No. 4, </volume> <year> 1993. </year>
Reference-contexts: The most visible example of this is in the emergence of digital audio and video applications such as video-on-demand <ref> [Rangan93] </ref>. Others include scientific visualization and large-object servers such as spatial databases [McKeown83, Stonebraker92]. These applications are all characterized by the fact that, if implemented on a large scale, their demands for storage and I/O bandwidth will far exceed the ability of current data storage subsystems to supply them.
Reference: [Rosenblum91] <author> Rosenblum, M.. and J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference-contexts: Unfortunately, mirroring is substantially more expensiveits storage overhead for redundancy is 100%, whereas the overhead in a parity-encoded array is generally less than 25% and may be less than 10%. Furthermore, several recent studies <ref> [Rosenblum91, Menon92a, Stodolsky94] </ref> demonstrated techniques that allow the small-write performance of parity-based arrays to approach and sometimes exceed that of mirroring. 1.2 Technology Background This section describes the structure and organization of modern disk drives and disk arrays; the subsection on disk technology has been kept to a minimum. <p> This requires a high degree of predictability from the disks and makes the design difficult to verify, tune, and maintain. Another technique proposed to address the small-write problem is to eliminate them from the workload. The Log-Structured File System (LFS) <ref> [Rosenblum91, Seltzer93] </ref> has the potential to achieve this by organizing the file system as an append-only log. The motivation behind this file system is that a disk drive is able to service sequential accesses at about twenty times the bandwidth of random accesses.
Reference: [Rosenblum92] <author> Rosenblum, M., and Ousterhout, J.. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 10, No. 1., </volume> <month> February </month> <year> 1992, </year> <pages> pp. 26-52. </pages>
Reference: [Rudeseal92] <author> A. Rudeseal, </author> <title> Storage Technology Corporation, </title> <institution> Presentation at Carnegie Mellon University, </institution> <month> March 5, </month> <year> 1992. </year>
Reference-contexts: For example, DISK/TREND predicts that the redundant-disk-array market will exceed thirteen billion dollars by 1997 [DISK/ TREND94]. Storage Technology Corporation, traditionally a maker of large-form-factor IBM-compatible disk drives, has stopped developing disks altogether and is replac ing this product line by one based on disk arrays <ref> [Rudeseal92] </ref>. 1.1.3 The Advent of New, I/O-Intensive Applications Finally, increases in on-line storage capacity and commensurate decreases in per-megabyte cost enable new technologies that demand even higher levels of I/O performance.
Reference: [Schulze89] <author> Schulze,M., Gibson, G., Katz, R., and Patterson, D., </author> <title> How Reliable is a RAID? Proceedings of COMPCON, </title> <booktitle> 1989, </booktitle> <pages> pp. </pages> <month> 118-123. </month> <title> RAIDframe: A Rapid Prototyping Tool for RAID Systems 139 Version 1.0 8/29/96 </title>
Reference-contexts: In arrays that maintain one or more on-line spare disks, the repair time can be very short, a few minutes to half an hour, and so the mean time to data loss can be very long. Schulze et. al. <ref> [Schulze89] </ref> noted that the time until data loss due to multiple simultaneous disk failures, which is the only failure mode modeled by the above equation, is not an adequate measure of true reliability because the failure of other system components (array controllers, string controllers, cabling, air conditioning, etc.) can equally well
Reference: [Seltzer93] <author> Seltzer, M., Bostic, K., McKusick, M., and Staelin, C., </author> <title> An Implementation of a Log-Structured File System for UNIX, </title> <booktitle> Proceedings of the Winter USENIX Conference, </booktitle> <year> 1993, </year> <pages> pp. 201-220. </pages>
Reference-contexts: This requires a high degree of predictability from the disks and makes the design difficult to verify, tune, and maintain. Another technique proposed to address the small-write problem is to eliminate them from the workload. The Log-Structured File System (LFS) <ref> [Rosenblum91, Seltzer93] </ref> has the potential to achieve this by organizing the file system as an append-only log. The motivation behind this file system is that a disk drive is able to service sequential accesses at about twenty times the bandwidth of random accesses.
Reference: [Solworth90] <author> Solworth, J. and Orji, C., </author> <title> Write-Only Disk Caches, </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1990, </year> <pages> pp. 123-132. </pages>
Reference-contexts: Solworth and Orji proposed several variations on an organization to improve mirrored-array write performance. They first proposed implementing a large, nonvolatile, possibly fault-tolerant write-only disk cache dedicated exclusively to write operations <ref> [Solworth90] </ref>. In this scheme, the controller defers user write operations by holding the corresponding data in the cache until a user read operation moves the disk heads to the vicinity of the data to be written, at which time it destages the data to disk.
Reference: [Solworth91] <author> Solworth, J. and Orji, C., </author> <title> Distorted Mirrors, </title> <booktitle> Proceedings of the International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1991, </year> <pages> pp. 10-17. </pages>
Reference-contexts: The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance <ref> [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>. While many of these proposals are promising, they have been largely evaluated only by simulation or analytic modeling. To understand the advantages and limitations of these new designs, it is essential for RAID architects to experiment with concrete implementations. <p> The authors do not address the question of whether some of the memory used for write-caching would be better used for read-caching. In two follow-up studies, Solworth and Orji proposed distorted mirrors <ref> [Solworth91] </ref> and doubly distorted mirrors [Orji93]. In the former, the controller updates data in place on the primary disk in a mirror pair but writes the data to any convenient location on the secondary drive.
Reference: [STC94] <author> Storage Technology Corporation, </author> <title> Iceberg 9200 Storage System: Introduction, STK Part Number 307406101, Storage Technology Corporation, </title> <type> Corporate Technical Publications, </type> <address> 2270 South 88th Street, Louisville, CO 80028. </address>
Reference: [ST9096] <author> Seagate Corporation, </author> <title> Disk Drive Model ST9096 product information. </title>
Reference-contexts: The trend is toward even 1. Seek time, rotational latency, and transfer rate are defined in Section 1.2.1. The Need for Improved Availability in the Storage Subsystem RAIDframe: A Rapid Prototyping Tool for RAID Systems 13 Version 1.0 8/29/96 smaller form factors: 2-1/2-inch drives are common in laptop computers <ref> [ST9096] </ref>, and 1.3-inch drives are available [HPC3013]. One-inch-diameter disks should appear on the market by 1995 and should be common by about 1998. At a (conservative) projected recording density in excess of 1-2 GB per square inch [Wood93], one such disk should hold well over 2 GB of data.
Reference: [Stodolsky94] <author> Stodolsky, D., Gibson, G., Courtright, W.V., and Holland, M., </author> <title> A Redundant Disk Array Architecture for Efficient Small Writes, </title> <type> Technical Report No. </type> <institution> CMU-CS-94-170, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213-3890, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The increasing importance of RAID systems has led to a number of proposals for new architectures and algorithms, for example, designs emphasizing improved write performance <ref> [Menon92, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>. While many of these proposals are promising, they have been largely evaluated only by simulation or analytic modeling. To understand the advantages and limitations of these new designs, it is essential for RAID architects to experiment with concrete implementations. <p> Unfortunately, mirroring is substantially more expensiveits storage overhead for redundancy is 100%, whereas the overhead in a parity-encoded array is generally less than 25% and may be less than 10%. Furthermore, several recent studies <ref> [Rosenblum91, Menon92a, Stodolsky94] </ref> demonstrated techniques that allow the small-write performance of parity-based arrays to approach and sometimes exceed that of mirroring. 1.2 Technology Background This section describes the structure and organization of modern disk drives and disk arrays; the subsection on disk technology has been kept to a minimum. <p> new, 3 N Technology Background RAIDframe: A Rapid Prototyping Tool for RAID Systems 27 Version 1.0 8/29/96 FIGURE 7 Read and Write Operations in RAID Level 5 (rotated parity) The preread-and-then-write operation performed on the data unit is typically done atomically to minimize the positioning overhead incurred by the access <ref> [Stodolsky94] </ref>. This is also true for the parity unit. Since the old data must be available to perform the parity update, the data preread-and-write is typically allowed to complete (atomically) before the parity preread-and-write is started. <p> When the underlying storage mechanism is a disk array, the only writes that are encountered are large enough to span entire parity stripes, and thus the large-write optimization always applies. Stodolsky et. al. <ref> [Stodolsky94] </ref> adapted the ideas behind LFS to the problem of parity maintenance and proposed an approach based on logging the parity changes generated by each write operation rather than immediately updating the parity upon each user write.
Reference: [Stonebraker90] <author> Stonebraker, M. and Schloss, G., </author> <title> Distributed RAIDA New Multiple Copy Algorithm, </title> <booktitle> Proceedings of the IEEE Conference on Data Engineering, </booktitle> <year> 1990, </year> <pages> pp. 430-437. </pages>
Reference-contexts: It achieves this at lower disk cost than the standard approach of file duplication on multiple servers. Redundant Disk Arrays: A Brief Overview 36 RAIDframe: A Rapid Prototyping Tool for RAID Systems Version 1.0 8/29/96 Stonebraker and Schloss <ref> [Stonebraker90] </ref> proposed an organization that is essentially identical to RAID Level 5 with each disk replaced by a server in a network file system.
Reference: [Stonebraker92] <author> Stonebraker, M., </author> <title> An Overview of the Sequoia 2000 Project, </title> <booktitle> Proceedings of the Thirty-Seventh IEEE Computer Society International Conference (COMPCON), </booktitle> <year> 1992, </year> <pages> pp. 383-388. </pages>
Reference-contexts: The most visible example of this is in the emergence of digital audio and video applications such as video-on-demand [Rangan93]. Others include scientific visualization and large-object servers such as spatial databases <ref> [McKeown83, Stonebraker92] </ref>. These applications are all characterized by the fact that, if implemented on a large scale, their demands for storage and I/O bandwidth will far exceed the ability of current data storage subsystems to supply them.
Reference: [TMC87] <author> Thinking Machines Corporation, </author> <title> Connection Machine Model CM-2 Technical Summary, </title> <type> Thinking Machines Technical Report HA87-4, </type> <year> 1987. </year>
Reference-contexts: It can be extended to support multiple-failure toleration by using an n-failure-tolerating Hamming code, which of course increases the capacity overhead for redundancy and the computational overhead for computing the codes. Thinking Machines Corporations Data Vault storage subsystem <ref> [TMC87] </ref> employed RAID Level 2, but this organization ignores an important fact about failure modes in disk drives.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> 1989. </year>
Reference-contexts: This reduces the number of disk operations required from four to three. This situation is very common in OLTP environments <ref> [TPCA89, Menon92c] </ref>. When the number of data units being updated exceeds half of one parity stripe, there is a more efficient mechanism for updating the parity.
Reference: [Wing96] <author> Wing, J. and M. Vaziri-Farahani, </author> <title> Model Checking a Controller Algorithm for the RAID Level 5 System, </title> <note> unpublished paper. </note>
Reference-contexts: Furthermore, automating this task is now possible. Given that DAGs consist of well-defined primitives, it is possible to think of them as state machines. Through model checking, used to verify the correctness of state machines [Clarke82, Clarke94], RAID designs can be verified immediately, long before actual implementation begins <ref> [Wing96] </ref>. Verifying that RAID operations are correctly implemented requires that graphs meet three criteria. First, primitive operations must be valid.

References-found: 82

