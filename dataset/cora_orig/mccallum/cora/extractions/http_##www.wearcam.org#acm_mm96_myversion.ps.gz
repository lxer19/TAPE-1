URL: http://www.wearcam.org/acm_mm96_myversion.ps.gz
Refering-URL: http://www.wearcam.org/
Root-URL: 
Title: `SMART CLOTHING': WEARABLE MULTIMEDIA COMPUTING AND `PERSONAL IMAGING' TO RESTORE THE TECHNOLOGICAL BALANCE BETWEEN PEOPLE
Author: Steve Mann 
Keyword: augmented reality, mediated reality, ubiquitous computing, smart spaces, video surveillance, mobile multimedia, wearable computing, personal imaging, video orbits, pencigraphic image compositing.  *Words or phrases in single quotes  
Address: Building E15-389;  10 King's College Road, Room 2001,  Palo Alto.  
Affiliation: MIT  University of Toronto  HP labs,  
Note: Reprinted from ACM Multimedia 96, Boston, MA, November 1996, Pages 163-174, Copyright, ACM 1  author currently with  Research supported, in part, by  are those introduced by author here or elsewhere in the literature.  
Email: http://www.wearcam.org/ steve@media.mit.edu  
Phone: Tel. (416) 946-3387; Fax. (416) 971-2326  
Abstract: Current portable computers and PDAs fail to truly become part of our daily lives in the sense that we need to stop what we are doing and expend conscious effort to use them. They also do not have the situational awareness that they should have: while they are not being explicitly used, they are unable to remain attentive to possible ways to help the user. Environmental technology in the form of ubiquitous computing, ubiquitous surveillance, and smart spaces, has attempted to bring multimedia computing seamlessly into our daily lives, promising a future world with cameras and microphones everywhere, connected to invisible computing, always attentive to our every movement or conversation. This raises some serious privacy issues. Even if we ignore these issues, there is still a problem of user-control, customization, and reliance on an infrastructure that will not (and probably should not) become totally ubiquitous. In response to these problems, a personal, wearable, multimedia computer, with head-mounted camera(s)/display, sensors, etc. is proposed for use in day-to-day living within the surrounding social fabric of the individual. Examples of practical uses include: face identification (memory aid for names), way-finding via sequences of freeze-frames, shared visual memory/environment maps, and other personal note-taking together with visual images. Anecdotal personal experiences, over several years of use, are reported, and privacy issues are addressed, in particular, with a discussion of how personal `smart clothing' has counteracted or at least reached a healthy balance with environmental surveillance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mark Weiser. </author> <booktitle> Ubiquitous computing, </booktitle> <pages> 1988-1994. </pages> <address> http://sandbox.parc.xerox.com/ubicomp. </address>
Reference-contexts: These items are far more encumbering than would be a single item that performed all of the tasks that each was meant to do. Other developments, such as ubiquitous computing <ref> [1] </ref> have attempted to bring computing seamlessly into our daily lives. Ubiquitous multimedia computing and smart spaces [2][3][4] would seem to suggest a future world in which we're surrounded with computing, as well as cameras, microphones, and other forms of perceptual intelligence during all facets of our daily lives.
Reference: [2] <author> David Rokeby. </author> <title> Camera-based performance spaces, </title> <note> 1982. http://www.vividgroup.com/vivid/next.htm and http://www.interlog.com/~drokeby/. </note>
Reference: [3] <author> Francis MacDougall, Vincent John Vincent, and Su-san Wyshynski. </author> <title> Camera-based virtual reality, </title> <note> 1987. http://www.vividgroup.com/vivid/next.htm. </note>
Reference: [4] <author> Maes, Darrell, Blumberg, and Pentland. </author> <title> The alive system: Full-body interaction with animated autonomous agents. </title> <type> TR 257, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <year> 1994. </year>
Reference: [5] <author> Phil Patton. </author> <title> Caught. </title> <type> WIRED, </type> <month> January </month> <year> 1995. </year>
Reference-contexts: This symmetry is generally not the case with regard to surveillance systems like the ones used in the UK, which, although often installed by governments, are operated as closed (secret) systems, the imagery being unavailable to ordinary citizens. Phil Patton <ref> [5] </ref> discusses the surveillance dilemma, making reference to the ubiquitous "ceiling domes of wine-dark opacity", making mention that "many department stores use hidden cameras behind one-way mirrors in fitting rooms", and in general, that there is much more video surveillance than we might at first think.
Reference: [6] <author> LynNell Hancock, Claudia Kalb, and William Underhill. </author> <title> You don't have to smile. </title> <address> Newsweek, </address> <month> July 17 July 17, </month> <year> 1995. </year>
Reference: [7] <author> ADVANCED IMAGING. </author> <title> Imaging in the internet. Solutions for the Electronic Imaging Professional, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: Sophisticated machine vision algorithms are often used to track shoppers' activities <ref> [7] </ref> and make inferences about possible suspicious behaviour. (c) Machines with dark windows monitor users' activities, purportedly for the users' own protection, although organizations are often secretive about the exact nature of these systems (hence the use of very dark glass to hide the apparatus behind it). (d) `Smart toilets', with
Reference: [8] <author> R. A. Earnshaw, M. A. Gigante, and H Jones. </author> <title> Virtual reality systems. </title> <publisher> Academic press, </publisher> <year> 1993. </year>
Reference-contexts: Ivan Sutherland, a pioneer in computer graphics, described a head-mounted display with half-silvered mirrors so that the wearer could see a virtual world superimposed on reality <ref> [8] </ref> [9]. Sutherland's work, as well as more recent related work [10][11][12][13][14]is characterized by its tethered nature. The wearer is tethered to a workstation which is generally powered from an AC outlet | in this sense it differs from the `smart clothing' which is entirely battery operated and tetherless. 3.0.1.
Reference: [9] <author> I. Sutherland. </author> <title> A head-mounted three dimensional display. </title> <booktitle> In Proc. Fall Joint Computer Conference, </booktitle> <pages> pages 757-764, </pages> <year> 1968. </year>
Reference-contexts: Ivan Sutherland, a pioneer in computer graphics, described a head-mounted display with half-silvered mirrors so that the wearer could see a virtual world superimposed on reality [8] <ref> [9] </ref>. Sutherland's work, as well as more recent related work [10][11][12][13][14]is characterized by its tethered nature. The wearer is tethered to a workstation which is generally powered from an AC outlet | in this sense it differs from the `smart clothing' which is entirely battery operated and tetherless. 3.0.1.
Reference: [10] <author> S. Feiner, B. MacIntyre, and D. Seligmann. </author> <title> Knowledge-based augmented reality, </title> <month> Jul </month> <year> 1993. </year> <journal> Communications of the ACM, </journal> <volume> 36(7). </volume>
Reference: [11] <author> S. Feiner, B. MacIntyre, and D. Seligmann. </author> <title> Karma (knowledge-based augmented reality for maintenance assistance), </title> <note> 1993. http://www.cs.columbia.edu/graphics/projects/karma/ karma.html. </note>
Reference: [12] <author> Henry Fuchs, Mike Bajura, and Ryutarou Ohbuchi. </author> <title> Teaming ultrasound data with virtual reality in obstetrics. </title> <address> http://www.ncsa.uiuc.edu/Pubs/MetaCenter/SciHi93/ 1c.Highlights-BiologyC.html. </address>
Reference: [13] <editor> David Drascic. David drascic's papers and presentations, </editor> <year> 1993. </year> <note> http://vered.rose.utoronto.ca/people/david dir/ Bibliography.html. </note>
Reference: [14] <author> Paul Milgram. </author> <title> Augmented reality: A class of displays on the reality-virtuality continuum, </title> <booktitle> 1994. </booktitle> <address> http://vered.rose.toronto.edu/people/paul dir/ SPIE94/SPIE94.full.html. </address>
Reference: [15] <author> S. Finger, M. Terk, E. Subrahmanian, C. Kasabach, F. Prinz, D.P. Siewiorek, A. Smailagic, J. Stivorek, and L Weiss. </author> <title> Rapid design and manufacture of wearable computers. </title> <journal> Communications of the ACM, </journal> <pages> pages 63-70, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The wearer is tethered to a workstation which is generally powered from an AC outlet | in this sense it differs from the `smart clothing' which is entirely battery operated and tetherless. 3.0.1. OTHER RELATED WORK Other recent work in wearable computing <ref> [15] </ref> provides a task-specific system, in particular, a repair manual for use by soldiers. <p> The recent proliferation of wearable computers (there are about 5 companies making wearable computers now) suggests that we're moving in that direction. However, many of the applications of wearable computers so-far envisioned, such as the land warrior (military), the intelligent maintenance aid, or various applications in the workplace <ref> [15] </ref> might better be described as `smart uniforms'. A `smart uniform' is issued to a soldier or employee at the start of a job, and then taken away after the job is completed.
Reference: [16] <author> Joseph Hoshen, Jim Sennott, and Max Winkler. </author> <title> Keeping tabs on criminals. </title> <journal> IEEE SPECTRUM, </journal> <pages> pages 26-32, </pages> <month> Febru-ary </month> <year> 1995. </year>
Reference-contexts: the soldier focused on the task at hand, the only input is a knob and pushbutton, so that menu items from a specific program may be selected. `Smart clothing' differs from employer-owned technology, or technology controlled by an external entity (the most extreme case being devices used to track criminals <ref> [16] </ref>). In particular, it is owned, operated, and controlled by the wearer. It is primarily intended for day-to-day living within the surrounding social fabric of the individual [17]. <p> Indeed, examples of wearable technology at the extreme opposite to the personal wearable, are the wearable ID transponders that have been rejected by many employees, and the devices attached to criminals to keep track of them <ref> [16] </ref>. These devices are owned, operated, and controlled by a remote entity.
Reference: [17] <author> Steve Mann. Smart clothing: </author> <title> The shift to wearable computing. </title> <journal> Communications of the ACM, </journal> <pages> pages 23-24, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: In particular, it is owned, operated, and controlled by the wearer. It is primarily intended for day-to-day living within the surrounding social fabric of the individual <ref> [17] </ref>. Reprinted from ACM Multimedia 96, Boston, MA, November 1996, Pages 163-174, Copyright, ACM 4 4. `PERSONAL IMAGING' THEORETICAL BACKGROUND The theoretical background for personal imaging is based on regarding the camera as a measurement instrument, in particular, an array of directional lightmeters.
Reference: [18] <author> S. Mann. </author> <title> Compositing multiple pictures of the same scene. </title> <booktitle> In Proceedings of the 46th Annual IS&T Conference, </booktitle> <address> Cam-bridge, Massachusetts, </address> <month> May 9-14 </month> <year> 1993. </year> <institution> The Society of Imaging Science and Technology. </institution>
Reference-contexts: A fully automatic featureless parameter estimator for estimating the parameters of the true projective group of coordinate transformations (e.g. the parameters in (1)) has been proposed <ref> [18] </ref>. <p> With feature correspondences, the projective coordinate transformation may be determined but it was not until 1993 that a fully automatic featureless method of doing so appeared in the literature <ref> [18] </ref>. Other related work, such as Apple's Quick Time VR (QTVR) requires a special apparatus, comprising a tripod with a precisely calibrated rotating camera stage, so that camera movements are known.
Reference: [19] <author> Steve Mann. </author> <title> `pencigraphy' with AGC: Joint parameter estimation in both domain and range of functions in same orbit of the projective-Wyckoff group. </title> <type> Technical Report 384, </type> <institution> MIT Media Lab, Cambridge, Massachusetts, </institution> <month> Decem-ber </month> <year> 1994. </year> <title> also appears in: </title> <booktitle> IEEE International Conference on Image Processing (ICIP 96), </booktitle> <address> Lausanne, Switzerland, </address> <month> September </month> <year> 1996. </year>
Reference: [20] <author> S. Mann and R. W. </author> <title> Picard. Video orbits of the projective group; a simple approach to featureless estimation of parameters. </title> <type> TR 338, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <year> 1995. </year> <journal> IEEE Trans. Image Proc., </journal> <month> Sept </month> <year> 1997. </year>
Reference: [21] <author> Steve Mann and Simon Haykin. </author> <title> Time-frequency perspectives: The chirplet transform. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> San Francisco, CA, March 23-26, 1992. </address> <publisher> IEEE. </publisher>
Reference: [22] <author> William A. </author> <title> Radlinski American Society of Photogramme-try. Editor-in-chief: </title> <editor> Morris M. Thompson. Associate editors: Robert C. Eller and Julius L. Speert. </editor> <title> Manual of pho-togrammetry. Schaum's Outline Series. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> Falls Church, Va., </address> <note> 3d ed edition, 1966. 2 v. (xx, 1199 p.) illus. 27 cm. </note>
Reference-contexts: HISTORICAL BACKGROUND Efforts to combine multiple images of the same scene have been around for many years. Photogrammeters have combined images manually, in non-overlapping sections 1 . Pho-togrammetry is a well-developed field of study <ref> [22] </ref>. Artists, most notably, David Hockney [23], often assemble multiple pictures of the same scene by hand, using the medium expressively.
Reference: [23] <author> D. Hockney. </author> <title> Hockney on Photography. </title> <address> London Cape, Lon-don, </address> <year> 1988. </year> <title> Conversations with Paul Joyce. </title>
Reference-contexts: HISTORICAL BACKGROUND Efforts to combine multiple images of the same scene have been around for many years. Photogrammeters have combined images manually, in non-overlapping sections 1 . Pho-togrammetry is a well-developed field of study [22]. Artists, most notably, David Hockney <ref> [23] </ref>, often assemble multiple pictures of the same scene by hand, using the medium expressively.
Reference: [24] <author> A.M. Tekalp, M.K. Ozkan, and M.I. Sezan. </author> <title> High-resolution image reconstruction from lower-resolution image sequences and space-varying image restoration. </title> <booktitle> In Proc. of the Int. Conf. on Acoust., Speech and Sig. Proc., pages III-169, </booktitle> <address> San Francisco, CA, </address> <month> Mar. </month> <pages> 23-26, </pages> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Pho-togrammetry is a well-developed field of study [22]. Artists, most notably, David Hockney [23], often assemble multiple pictures of the same scene by hand, using the medium expressively. Research in combining multiple pictures electronically has previously relied on an pure-translational model <ref> [24] </ref> of the form x 2 = x 1 + b or an affine model [25] of the form x 2 = ax 1 +b. However, these models fail to capture the `chirping' effect (described in the previous section) which is clearly quite pronounced in almost any practical imaging situation.
Reference: [25] <author> M. Irani and S. Peleg. </author> <title> Improving Resolution by Image Registration. </title> <journal> CVGIP, </journal> <volume> 53 </volume> <pages> 231-239, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Artists, most notably, David Hockney [23], often assemble multiple pictures of the same scene by hand, using the medium expressively. Research in combining multiple pictures electronically has previously relied on an pure-translational model [24] of the form x 2 = x 1 + b or an affine model <ref> [25] </ref> of the form x 2 = ax 1 +b. However, these models fail to capture the `chirping' effect (described in the previous section) which is clearly quite pronounced in almost any practical imaging situation.
Reference: [26] <author> N1NLF. `wearstation': </author> <title> With today's technology, it is now possible to build a fully equipped ham radio station, complete with internet connection, into your clothing., </title> <month> January, </month> <year> 1996. </year>
Reference-contexts: EXPERIMENTAL APPARATUS Only a brief description of the apparatus is provided here; more detailed information can be found elsewhere in the literature <ref> [26] </ref>. The author's current apparatus, with miniature camera and miniature (one inch diag.) 24-bit color screen in the eyeglasses, and miniaturized Pentium 90 system (64M RAM, 1.2G hard drive, etc) in the waist bag is still a somewhat cumbersome prototype.
Reference: [27] <author> Steve Mann. Wearable, </author> <title> tetherless computer-mediated reality: WearCam as a wearable face-recognizer, and other applications for the disabled. </title> <type> TR 361, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cam-bridge, Ma, </address> <month> February 2 </month> <year> 1996. </year> <note> Also appears in AAAI Fall Symposium on Developing Assistive Technology for People with Disabilities, </note> <month> 9-11 November </month> <year> 1996, </year> <institution> MIT. </institution>
Reference-contexts: Other sensors such as infared and radar, enhance and extend the author's sensory capabilities, and have been used for various experiments in synthetic synesthesia, which might someday be of assistance to the visually challenged <ref> [27] </ref>. The author's apparatus is somewhat reminiscent of the "Winnebiko/Behemoth" work of Steve Roberts, N4RVE [28], except that it is built into clothing rather than a bicycle. 5.0.6. `PERSONAL IMAGING' `Personal imaging' is an important aspect of `smart clothing', thus the camera and display are important parts of the apparatus. <p> EXPERIMENTS The apparatus, a wearable multimedia computer system equipped camera (s), and wireless Internet connection [29], enabled experimentation with imaging applications in ordinary day-to-day situations, not just in a lab. Possible applications of this `WearCam' to the handicapped have been suggested <ref> [27] </ref>, in particular, its use as a personal visual assistant (PVA), and as a visual memory prosthetic has been suggested. <p> registration template (Fig 6) displayed on top of the video to facilitate manual alignment of the face, through rotating the head to aim the camera appropriately. (The cursors themselves were implemented as a JPEG image.) Further implementational details of the wearable face recognizer and visual memory prosthetic are available in <ref> [27] </ref> and [30]. 7.0.14.
Reference: [28] <author> N4RVE Steve Roberts. </author> <note> Nomadic research labs. http://www.microship.com. </note>
Reference-contexts: Other sensors such as infared and radar, enhance and extend the author's sensory capabilities, and have been used for various experiments in synthetic synesthesia, which might someday be of assistance to the visually challenged [27]. The author's apparatus is somewhat reminiscent of the "Winnebiko/Behemoth" work of Steve Roberts, N4RVE <ref> [28] </ref>, except that it is built into clothing rather than a bicycle. 5.0.6. `PERSONAL IMAGING' `Personal imaging' is an important aspect of `smart clothing', thus the camera and display are important parts of the apparatus.
Reference: [29] <author> S. Mann. Wearable Wireless Webcam, </author> <year> 1994. </year> <note> http://wearcam.org. </note>
Reference-contexts: A spinning color filter wheel was replaced with an electronic color shutter to reduce physical size and increase reliability by eliminating moving parts. 6. EXPERIMENTS The apparatus, a wearable multimedia computer system equipped camera (s), and wireless Internet connection <ref> [29] </ref>, enabled experimentation with imaging applications in ordinary day-to-day situations, not just in a lab. Possible applications of this `WearCam' to the handicapped have been suggested [27], in particular, its use as a personal visual assistant (PVA), and as a visual memory prosthetic has been suggested.
Reference: [30] <author> S. Mann. </author> <title> `mediated reality'. </title> <type> TR 260, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <year> 1994. </year>
Reference-contexts: Furthermore, with the current (or soon-to-be) availability of commercial systems such as Metricom, Wavelan, and Motorola, getting more `smart clothing' online will soon be trivial. Experiments with two WearCam users have allowed an exchange of viewpoint, so that each person sees through the other person's eyes <ref> [30] </ref>. Instead of just two people, suppose we have a community (network) of individuals wearing the apparatus. This could be a homogeneous community (all wearing the same form of the apparatus), but for simplicity of implementation, consider a heterogeneous community (Fig 5) wearing various prototypes of the apparatus. <p> RESULTS AND DISCUSSION 7.0.9. EDGERTONIAN EYES Early experiments with a variety of different `visual filters', involved experimenting with the WearCam apparatus in day-to-day activities <ref> [30] </ref>. Each of these filters provided a different visual reality. For one such filter, the author experimented by applying a repeating freeze-frame effect to WearCam. With this video sample and hold, It was found that nearly periodic patterns would appear to freeze at certain speeds. <p> It was found that the best overall configuration was to have the camera rotated 90 degrees (portrait) but with the display still in landscape orientation. Anecdotes on the author's experiences "living in a rot90 world" (reminiscent of George Stratton's upside-down glasses [40]) appear in <ref> [30] </ref>. <p> (Fig 6) displayed on top of the video to facilitate manual alignment of the face, through rotating the head to aim the camera appropriately. (The cursors themselves were implemented as a JPEG image.) Further implementational details of the wearable face recognizer and visual memory prosthetic are available in [27] and <ref> [30] </ref>. 7.0.14.
Reference: [31] <author> Bob Shaw. </author> <title> Light of Other Days. Analog, </title> <month> August </month> <year> 1966. </year>
Reference-contexts: The `visual memory prosthetic' is based primarily on a `visual filter' that doesn't (assuming proper calibration) bend any rays of light passing through it, but only delays these light rays. Such a temporal-only `visual filter' might, for example, function like a pair of eyeglasses made of hypothetical slowglass <ref> [31] </ref> and merely delay the lightfield. It might also produce a stroboscopic or freeze-frame effect, as well as more sophisticated temporal visual filters such as for the face-recognition/reminder experiments to be described later. 7. RESULTS AND DISCUSSION 7.0.9.
Reference: [32] <author> Harold E. Edgerton. </author> <title> Electronic flash, </title> <publisher> strobe. MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1979. </year>
Reference-contexts: Rotational periodicity (such as in the blades of a spinning airplane propeller) would appear as objects rotating slowly backwards or forwards, in much the same way as objects do under the stroboscopic lights of Harold Edgerton <ref> [32] </ref>. 7.0.10. FLASHBACKS AND FREEZE-FRAMES Of greater interest than just being able to see that which is invisible to the naked eye, was the fact that sometimes the effect would cause certain things to be rememembered much better.
Reference: [33] <author> T. Starner. </author> <title> The remembrance agent, 1993. Class project for intelligent software agents class of Patti Maes. </title>
Reference-contexts: When this comparison is done automatically, the computer attempts to compare all previous images with the current image (from what the camera is currently pointed at). With some form of visual processing, WearCam can also function as the visual equivalent of Starner's rememberance agent <ref> [33] </ref> (a text interface that constantly watches what the user types and automatically reminds the user of related text files). 7.0.12. `VISUAL CLEW' The `visual clew 2 ' is a computer-assisted way-finding system that runs on WearCam. We've all no doubt been lost at one time or another.
Reference: [34] <author> S. Mann and R. W. </author> <title> Picard. Virtual bellows: constructing high-quality images from video. </title> <booktitle> In Proceedings of the IEEE first international conference on image processing, </booktitle> <address> Austin, Texas, </address> <month> Nov. 13-16 </month> <year> 1994. </year>
Reference: [35] <author> W.W. Bledsoe. </author> <title> Man-machine facial recognition. In Rep. </title> <publisher> PRI:22, Panoramic Research Inc., </publisher> <address> Palo Alto, CA, </address> <month> August </month> <year> 1966. </year>
Reference: [36] <author> S.R. Cannon, G.W. Jones, R. Campbell, and N.W. Morgan. </author> <title> A computer vision system for identification of individuals. </title> <booktitle> In Proceedings of IECON,1, </booktitle> <year> 1986. </year>
Reference: [37] <author> Matthew A. Turk. </author> <title> Interactive-Time Vision: Face Recognition as a Visual Behavior. </title> <type> PhD dissertation, </type> <institution> MIT, Media Arts and Sciences Section. Advisor: A.P. Pentland. </institution>
Reference: [38] <author> A. Pentland, T. Starner, N. Etcoff, A. Masoiu, O. Oliyide, and M. Turk. </author> <title> Experiments with eigenfaces. </title> <booktitle> In Appears: Looking At People Workshop, IJCAI'93, </booktitle> <address> Chamberry, France, </address> <month> August </month> <year> 1993. </year>
Reference: [39] <author> Simon Davies. </author> <title> Privacy international calls for cctv debate, </title> <note> September 8, ABC News 20/20. http://wearcam.org/privacy forum digest on CCTV.html. </note>
Reference-contexts: The FBI-funded FERET project comprises a large database (more than 7000 faces) that can be searched quickly on a workstation-class system. Automatic face recognition has raised extensive privacy concerns <ref> [39] </ref>: "Privacy International is calling on the UK government to prohibit : : : Computerised Face Recognition (CFR) systems that have the capacity to automatically compare faces captured on CCTV, with a database of facial images.
Reference: [40] <author> George M. Stratton. </author> <title> Some preliminary experiements on vision. </title> <journal> Psychological Review, </journal> <volume> 1896. </volume> <booktitle> Reprinted from ACM Multimedia 96, </booktitle> <address> Boston, MA, </address> <month> November </month> <year> 1996, </year> <pages> Pages 163-174, </pages> <note> Copyright, ACM 14 [41] S. Mann. "smart clothing". TR 366, </note> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <month> February 2 </month> <year> 1996. </year>
Reference-contexts: It was found that the best overall configuration was to have the camera rotated 90 degrees (portrait) but with the display still in landscape orientation. Anecdotes on the author's experiences "living in a rot90 world" (reminiscent of George Stratton's upside-down glasses <ref> [40] </ref>) appear in [30].
Reference: [42] <author> Neal Stephenson. </author> <title> Global neighborhood watch, </title> <note> 1995. http://vip.hotwired.com/wired/scenarios/global.html. </note>
Reference-contexts: Neil Stephenson's Global Neighborhood Watch <ref> [42] </ref> comes to mind, but now with free-roaming tetherless connectivity.
Reference: [43] <author> William J. Mitchell. </author> <title> The Reconfigured Eye. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Falsification of video surveillance recordings is a point addressed in the movie Rising Sun, and in William Mitchell's book, The Reconfigured Eye <ref> [43] </ref>. However, if there is a chance that individuals might have their own account of what happened, organizations using surveillance would be much less likely to risk falsifying surveillance data. Even though it is easy to falsify images [43], when accounts of what happened differ, further investigation would be called for. <p> the movie Rising Sun, and in William Mitchell's book, The Reconfigured Eye <ref> [43] </ref>. However, if there is a chance that individuals might have their own account of what happened, organizations using surveillance would be much less likely to risk falsifying surveillance data. Even though it is easy to falsify images [43], when accounts of what happened differ, further investigation would be called for. Careful analysis (e.g. kinematic constraints on moving objects in the scene, the way shadows reflect in shiny surfaces, etc) of two or more differing accounts of what happened would likely uncover falsification that would otherwise remain unnoticed. <p> The proliferation of hidden cameras everywhere has the possibility to threaten our privacy, but suppose the only cameras were the prosthetic elements of other individuals. Then at least one would still have privacy when one was alone. Furthermore, as images become easier and easier to edit <ref> [43] </ref>, WearCam, like other cameras, will begin to be regarded as being more like a visual memory aid and an artist's tool than a source of evidence of fact. 10.
Reference: [44] <author> Richard W. Stevenson. </author> <title> They're capturing suspects on candid camera. New York Times International, </title> <address> Saturday, </address> <month> March 11 </month> <year> 1995. </year> <note> (excerpt appears in http://wearcam.org/trafficcam.html). </note>
Reference-contexts: One can only imagine what would have happened if the only video recording of the Rodney King beating were one that had been made by police, using a network of police surveillance cameras, such as the camera networks used in some cities in the UK <ref> [44] </ref>. Of course, most officials are honest, and would have no reason to be any more paranoid of an average citizen's camera operating together with their own network of cameras.
References-found: 43

