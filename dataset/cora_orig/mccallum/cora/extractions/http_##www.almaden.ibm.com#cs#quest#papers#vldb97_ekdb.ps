URL: http://www.almaden.ibm.com/cs/quest/papers/vldb97_ekdb.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parallel Algorithms for High-dimensional Proximity Joins  
Author: John C. Shafer Rakesh Agrawal 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We consider the problem of parallelizing high-dimensional proximity joins. We present a parallel multidimensional join algorithm based on an the epsilon-kdB tree and compare it with the more common approach of space partitioning. An evaluation of the algorithms on an IBM SP2 shared-nothing multiprocessor is presented using both synthetic and real-life datasets. We also examine the effectiveness of the algorithms in the context of a specific data-mining problem, that of finding similar time-series. The empirical results show that our algorithm exhibits good performance and scalability, as well an ability to handle data skew.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, C. Faloutsos, and A. Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In Proc. of the Fourth Int'l Conference on Foundations of Data Organization and Algorithms, </booktitle> <address> Chicago, </address> <month> October </month> <year> 1993. </year> <booktitle> Also in Lecture Notes in Computer Science 730, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1993, </year> <pages> 69-84. </pages>
Reference: [2] <author> R. Agrawal, K.-I. Lin, H. S. Sawhney, and K. Shim. </author> <title> Fast similarity search in the presence of noise, scaling, and translation in time-series databases. </title> <booktitle> In Proc. of the 21st Int'l Conference on Very Large Databases, </booktitle> <pages> pages 490-501, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 23rd VLDB Conference Athens, Greece, 1997 The work presented in this paper was motivated by the particular data-mining problem of finding similar time-series [1][7]. In <ref> [2] </ref>, an algorithm was proposed that first finds all similar "atomic" subsequences, and then stitches together the atomic subsequence matches to obtain larger similar subsequences. A sliding window of size w is used to create atomic subsequences from each time series. <p> To perform the join efficiently, the number of buckets is chosen such that two buckets will fit entirely in memory. 2.2 Index Based Considerable recent work in multidimensional joins has focused on using indices to aid the join. This includes R-trees as used in [4], [3] and <ref> [2] </ref>, PMR quadtrees in [9], and seeded trees in [13]. Whatever the index used, they follow the same schema whereby two sets of multidimensional objects are joined by doing a synchronized depth-first traversal of their indices. Intersection joins are handled by joining any two index buckets whose extents overlap. <p> Note that it is possible for the original subspace and/or multiple neighboring subspaces to belong to a single processor. For example, the point x1 in Figure 5 resides in data bucket B [0] and is replicated in replica bucket R <ref> [2] </ref>; both of these buckets reside on processor P 0 . Although a data-point is not replicated for each instance, a pointer to the data-point must still be inserted into each required bucket. After the data has been redistributed, processors self join each of their data buckets B [i]. <p> In contrast, the parallel *-kdB algorithm is robust, as it has built-in capability for skew handling. 4.2 Sample Application: Similar Time Series For our last set of experiments, we return to the problem that originally motivated us | discovering similar time-series <ref> [2] </ref>. As discussed in the Section 1, a significant part of this data-mining problem is proximity joining points in w-dimensional space. We can perform this step in parallel by using our *-kdB proximity-join algorithm.
Reference: [3] <author> T. Brinkhoff, H. Kriegel, and B. Seeger. </author> <title> Parallel processing of spatial joins using R-Trees. </title> <booktitle> In Proc. of 12th Int'l Conference on Data Engineering, </booktitle> <address> New Orleans, USA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Finding similar atomic subsequences now corresponds to the problem of finding all pairs of w-dimensional points that lie within *-distance of each other, where * is a user-specified parameter. While parallel algorithms for performing joins on spatial data already exists (e.g. [18], <ref> [3] </ref>, [9]), they have mainly concentrated on joining map data where spaces are typically limited to only two or three dimensions. <p> To perform the join efficiently, the number of buckets is chosen such that two buckets will fit entirely in memory. 2.2 Index Based Considerable recent work in multidimensional joins has focused on using indices to aid the join. This includes R-trees as used in [4], <ref> [3] </ref> and [2], PMR quadtrees in [9], and seeded trees in [13]. Whatever the index used, they follow the same schema whereby two sets of multidimensional objects are joined by doing a synchronized depth-first traversal of their indices. <p> We assume that the data to be joined is distributed equally over the local disks of the multiprocessor. 3.1 Previous Work Virtually all of the existing work on parallelizing multidimensional joins has focused on joining two-dimensional geometric objects. For example, the authors in <ref> [3] </ref> use R-trees to join spatial objects in a hybrid shared-nothing/shared-memory architecture where a single data processor services all I/O requests. The authors in [9] compare data-parallel PMR quadtrees with data-parallel Rand R + -trees for joins and range queries on two-dimensional line segments.
Reference: [4] <author> T. Brinkhoff, H.-P. Kriegel, and B. Seeger. </author> <title> Efficient processing of spatial joins using R-trees. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference on Management of Data, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: To perform the join efficiently, the number of buckets is chosen such that two buckets will fit entirely in memory. 2.2 Index Based Considerable recent work in multidimensional joins has focused on using indices to aid the join. This includes R-trees as used in <ref> [4] </ref>, [3] and [2], PMR quadtrees in [9], and seeded trees in [13]. Whatever the index used, they follow the same schema whereby two sets of multidimensional objects are joined by doing a synchronized depth-first traversal of their indices.
Reference: [5] <author> D. J. DeWitt, S. Ghandeharizadeh, D. A. Schnei-der, A. Bricker, H.-I. Hsiao, and R. Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> In IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We assume a shared-nothing parallel environment where each of N processors has private memory and disks. The processors are connected by a communication network and can communicate only by passing messages. Examples of such parallel machines include GAMMA <ref> [5] </ref> and IBM's SP2 [10]. We assume that the data to be joined is distributed equally over the local disks of the multiprocessor. 3.1 Previous Work Virtually all of the existing work on parallelizing multidimensional joins has focused on joining two-dimensional geometric objects.
Reference: [6] <author> C. Faloutsos. </author> <title> Multiattribute hashing using gray codes. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference on Management of Data, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Standard relational indices and techniques for computing joins can now be used on the tuples' one-dimensional cell values. This approach in [17] uses a space-filling curve known as the "Z curve"; assigned cell-numbers are called "Z values". Other space-filling curves include the Gray code <ref> [6] </ref> and the Hilbert curve [15]. Of these three, the Hilbert curve has been shown to cluster space better [11]. A shortcoming of space-filling curves is that some proximity information is always lost, so nearby objects may have very different Z values. This complicates the join algorithm.
Reference: [7] <author> C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. </author> <title> Fast subsequence matching in time-series databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <month> May </month> <year> 1994. </year>
Reference: [8] <author> M. P. I. Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: This results in data-point x6 being replicated across the boundary, but not x3. 4 Performance Evaluation We have implemented both the parallel *-kdB and space-partitioning proximity join algorithms on an IBM SP2 [10] using the MPI-standard communication primitives <ref> [8] </ref>. The use of MPI allows our implementation to be portable to other shared-nothing parallel architectures, including workstation clusters. Experiments were conducted on a 16-node IBM SP2 Model 302.
Reference: [9] <author> E. G. Hoel and H. Samet. </author> <title> Algorithms for Data-Parallel spatial operations. </title> <type> Technical Report CS-TR-3230, </type> <institution> University of Maryland, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Finding similar atomic subsequences now corresponds to the problem of finding all pairs of w-dimensional points that lie within *-distance of each other, where * is a user-specified parameter. While parallel algorithms for performing joins on spatial data already exists (e.g. [18], [3], <ref> [9] </ref>), they have mainly concentrated on joining map data where spaces are typically limited to only two or three dimensions. Furthermore, these algorithms have been designed primarily to perform intersection joins on geometric objects such as polygons and line segments and are not well optimized for handling high-dimensional point data. <p> This includes R-trees as used in [4], [3] and [2], PMR quadtrees in <ref> [9] </ref>, and seeded trees in [13]. Whatever the index used, they follow the same schema whereby two sets of multidimensional objects are joined by doing a synchronized depth-first traversal of their indices. Intersection joins are handled by joining any two index buckets whose extents overlap. <p> For example, the authors in [3] use R-trees to join spatial objects in a hybrid shared-nothing/shared-memory architecture where a single data processor services all I/O requests. The authors in <ref> [9] </ref> compare data-parallel PMR quadtrees with data-parallel Rand R + -trees for joins and range queries on two-dimensional line segments. Several unusual architectural models were explored in that work in addition to a shared-memory model.
Reference: [10] <institution> International Business Machines. Scalable POW-ERparallel Systems, </institution> <address> GA23-2475-02 edition, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: We assume a shared-nothing parallel environment where each of N processors has private memory and disks. The processors are connected by a communication network and can communicate only by passing messages. Examples of such parallel machines include GAMMA [5] and IBM's SP2 <ref> [10] </ref>. We assume that the data to be joined is distributed equally over the local disks of the multiprocessor. 3.1 Previous Work Virtually all of the existing work on parallelizing multidimensional joins has focused on joining two-dimensional geometric objects. <p> This results in data-point x6 being replicated across the boundary, but not x3. 4 Performance Evaluation We have implemented both the parallel *-kdB and space-partitioning proximity join algorithms on an IBM SP2 <ref> [10] </ref> using the MPI-standard communication primitives [8]. The use of MPI allows our implementation to be portable to other shared-nothing parallel architectures, including workstation clusters. Experiments were conducted on a 16-node IBM SP2 Model 302. <p> Attached to each node is a 1GB disk. The processors run AIX level 4.1 and communicate with each other through the High-Performance Switch with HPS-tb3 adapters. See <ref> [10] </ref> for SP2 hardware details. To study the algorithms' sensitivity to different sized inputs, we generated synthetic datasets with both uniform and Gaussian distributions. Data-points were generated with eight dimensions with the values in each dimension ranging from 1:0 to 1:0.
Reference: [11] <author> H. Jagadish. </author> <title> Linear clustering of objects with multiple attributes. </title> <booktitle> In Proc. of the ACM-SIGMOD Conference on Management of Data, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: This approach in [17] uses a space-filling curve known as the "Z curve"; assigned cell-numbers are called "Z values". Other space-filling curves include the Gray code [6] and the Hilbert curve [15]. Of these three, the Hilbert curve has been shown to cluster space better <ref> [11] </ref>. A shortcoming of space-filling curves is that some proximity information is always lost, so nearby objects may have very different Z values. This complicates the join algorithm. This approach works best when the join condition is that two objects overlap.
Reference: [12] <author> N. Koudas and K. C. Sevcik. </author> <title> Size separation spatial join. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: However, this introduces the additional question of choosing which dimensions to split. Ultimately, the parallel *-kdB algorithm is likely to retain the advantage since partitioning of space in that algorithm is dynamic and automatic. Recently, another serial spatial-join algorithm (the Size Separation Spatial Join) was presented in <ref> [12] </ref>. It is a space-partitioning algorithm but differs in that it uses multiple levels of partitioning with increasing degrees of granularity. The algorithm appears to perform well on two-dimensional point data | even when that dataset is skewed.
Reference: [13] <author> M. Lo and C. V. Ravishankar. </author> <title> Generating seeded trees from data sets. </title> <booktitle> In Proc. of the Fourth International Symposium on Large Spatial Databases, </booktitle> <address> Port-land, ME, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: In the join phase, an algorithm identifies pairs of buckets to be joined (termed join-bucket pairs) and effects each join in turn. An example of this framework was presented in [14] where bootstrap seeding <ref> [13] </ref> and sampling was used to obtain the initial bucket extents. Another space partitioning algorithm (P BSM ) was recently presented in [18] and to some degree, it also fits within the above framework. <p> This includes R-trees as used in [4], [3] and [2], PMR quadtrees in [9], and seeded trees in <ref> [13] </ref>. Whatever the index used, they follow the same schema whereby two sets of multidimensional objects are joined by doing a synchronized depth-first traversal of their indices. Intersection joins are handled by joining any two index buckets whose extents overlap.
Reference: [14] <author> M. Lo and C. V. Ravishankar. </author> <title> Spatial Hash-Joins. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We can then perform a self join by joining each individual data bucket with itself. Non-self joins are handled by using the same partitioning scheme on each dataset and then joining corresponding data buckets. This approach falls within the general framework presented in <ref> [14] </ref>. In that framework, an algorithm defines bucket extents to hold data objects and an assignment function that maps data objects to buckets. Bucket extents may or may not be immutable and the assignment function may be one-to-one or many-to one. <p> Bucket extents may or may not be immutable and the assignment function may be one-to-one or many-to one. In the join phase, an algorithm identifies pairs of buckets to be joined (termed join-bucket pairs) and effects each join in turn. An example of this framework was presented in <ref> [14] </ref> where bootstrap seeding [13] and sampling was used to obtain the initial bucket extents. Another space partitioning algorithm (P BSM ) was recently presented in [18] and to some degree, it also fits within the above framework. <p> A detailed description of these performance considerations and how they impact the implementation can be found in [19]. 3.3 Parallel Space Partitioning For comparison purposes, we have also implemented a parallel space-partitioning algorithm for performing proximity joins. Our implementation fits within the hash-join framework <ref> [14] </ref> in that we divide space into a regular multidimensional grid and join corresponding partitions. Join work is distributed across the multiprocessor by dividing the set of bucket extents equally among the N processors.
Reference: [15] <author> B. Moon, H. Jagadish, C. Faloutsos, and J. H. Saltz. </author> <title> Analysis of the clustering properties of hilbert space-filling curve. </title> <journal> In IEEE Transactions on Knowledge and Data Engineering, </journal> <month> March </month> <year> 1996. </year>
Reference-contexts: Standard relational indices and techniques for computing joins can now be used on the tuples' one-dimensional cell values. This approach in [17] uses a space-filling curve known as the "Z curve"; assigned cell-numbers are called "Z values". Other space-filling curves include the Gray code [6] and the Hilbert curve <ref> [15] </ref>. Of these three, the Hilbert curve has been shown to cluster space better [11]. A shortcoming of space-filling curves is that some proximity information is always lost, so nearby objects may have very different Z values. This complicates the join algorithm. <p> Since workload balancing is performed after the data has been fully examined, we can use a more sophisticated assignment algorithm than round-robin to create the join workloads. A good approach might be to use space-filling curves such as the Hilbert curve <ref> [15] </ref> to create a total ordering of the data buckets. The buckets would then be assigned to different processors by partitioning the ordering into contiguous ranges. This could take advantage of the clustering capabilities of space-filling curves and help minimize the amount of data replication.
Reference: [16] <author> J. Nievergelt, H. Hinterberger, and K. Sevcik. </author> <title> The grid file: an adaptable, symmetric multikey file structure. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(1) </volume> <pages> 38-71, </pages> <year> 1984. </year>
Reference-contexts: Many of these algorithms therefore assume that the required index is already available. This is not well-suited to our motivating application of similar time-series since the data to be joined is typically generated "on-the-fly". Other drawbacks include skew-handling capabilities. In the Grid File <ref> [16] </ref>, skewed data can cause rapid growth in the size of the directory structures. For other indices such as the R tree, skew-handling typically requires maintaining height-balanced trees so that range queries can be efficiently processed.
Reference: [17] <author> J. A. Orenstein and T. Merrett. </author> <title> A class of data structures for associative searching. </title> <booktitle> In Proc. of the ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <year> 1984. </year>
Reference-contexts: Objects to be joined are then examined sequentially and for each cell which an object overlaps, a &lt;cell-number, object-pointer&gt; pair is created. Standard relational indices and techniques for computing joins can now be used on the tuples' one-dimensional cell values. This approach in <ref> [17] </ref> uses a space-filling curve known as the "Z curve"; assigned cell-numbers are called "Z values". Other space-filling curves include the Gray code [6] and the Hilbert curve [15]. Of these three, the Hilbert curve has been shown to cluster space better [11].
Reference: [18] <author> J. M. Patel and D. J. DeWitt. </author> <title> Partition Based Spatial-Merge Join. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Finding similar atomic subsequences now corresponds to the problem of finding all pairs of w-dimensional points that lie within *-distance of each other, where * is a user-specified parameter. While parallel algorithms for performing joins on spatial data already exists (e.g. <ref> [18] </ref>, [3], [9]), they have mainly concentrated on joining map data where spaces are typically limited to only two or three dimensions. <p> An example of this framework was presented in [14] where bootstrap seeding [13] and sampling was used to obtain the initial bucket extents. Another space partitioning algorithm (P BSM ) was recently presented in <ref> [18] </ref> and to some degree, it also fits within the above framework. To address data skew, P BSM partitions the space into many tiles such that there are more tiles than data buckets. These tiles are then grouped together using hashing to produce buckets that are relatively consistent in size. <p> Most indices also have substantial build-times. If an index required for a join does not exist, the cost required to build it can often be more than the cost of the join <ref> [18] </ref>. Many of these algorithms therefore assume that the required index is already available. This is not well-suited to our motivating application of similar time-series since the data to be joined is typically generated "on-the-fly". Other drawbacks include skew-handling capabilities. <p> Space partitioning can be parallelized by regularly dividing the data space into bucket extents as before, and then assigning bucket extents to different processors. The parallelization of P BSM outlined in <ref> [18] </ref> follows this approach. After space is partitioned, data is redistributed accordingly and joins are effected independently. As is pointed out in [18], data skew can by addressed by using tiling to fine-partition multidimensional space. The tiles can then be assigned to proces sors via hashing to balance the load. <p> The parallelization of P BSM outlined in <ref> [18] </ref> follows this approach. After space is partitioned, data is redistributed accordingly and joins are effected independently. As is pointed out in [18], data skew can by addressed by using tiling to fine-partition multidimensional space. The tiles can then be assigned to proces sors via hashing to balance the load. In general, the larger the data skew, the more finely the space must be partitioned. <p> The number of data buckets M is chosen to be fairly large compared to the size of the multiprocessor. This not only ensures smaller and more efficient joins, but also allows us to balance the workload similar to how tiling is used in P BSM <ref> [18] </ref>. 3.3.1 Implementation Details Since we will be partitioning the data space into M subspaces (where M &gt; N ), each processor allocates an array of M=N data buckets for storing data points.
Reference: [19] <author> J. C. Shafer and R. Agrawal. </author> <title> Parallel Algorithms for High-dimensional Proximity Joins. </title> <type> Research Report, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1997. </year> <note> Available from http://www.almaden.ibm.com/cs/quest. </note>
Reference-contexts: Thus, the joins that are finally executed by the parallel algorithm are exactly the joins that would be executed by the serial algorithm working with the same dataset. A detailed description of these performance considerations and how they impact the implementation can be found in <ref> [19] </ref>. 3.3 Parallel Space Partitioning For comparison purposes, we have also implemented a parallel space-partitioning algorithm for performing proximity joins. Our implementation fits within the hash-join framework [14] in that we divide space into a regular multidimensional grid and join corresponding partitions.
Reference: [20] <author> K. Shim, R. Srikant, and R. Agrawal. </author> <title> High-dimensional similarity joins. </title> <booktitle> In Proc. of the 13th Int'l Conference on Data Engineering, </booktitle> <address> Birmingham, U.K., </address> <month> April </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Many emerging applications require efficient processing of proximity joins on high-dimensional points <ref> [20] </ref>. Typical queries in these applications include: * Find all pairs of similar images (often as a prelude to clustering the images). * Retrieve music scores similar to a target music score. * Discover all stocks with similar price movements. fl Also, Department of Computer Science, University of Wis-consin, Madison. <p> Likewise, proximity joins are handled by joining any two index buckets whose boundaries are sufficiently near. Most of these approaches are not well suited to the particular problem of proximity joins on high-dimensional points. The inadequacies include an inability to scale to high-dimensions <ref> [20] </ref>. For example, the R tree and the kdB tree both use a "minimum bounding rectangle" (M BR) to represent the regions covered by each node in the index. As the number of dimensions gets large, the storage and traversal costs associated with using M BRs increases. <p> Height-balancing with required updates and possible reinsertions is a major reason for the high cost required to build these indices. A data structure called the *-kdB tree was recently presented in <ref> [20] </ref> to address the above concerns. 2.3 The *-kdB Tree The *-kdB tree is an attractive base for our parallel algorithm due to it being specifically designed for performing proximity joins on high-dimensional points. <p> This approach works only because building the *-kdB tree dynamically is very fast. Datasets that are too large to fit in memory are handled by partitioning the data and performing an in-memory *-kdB join on each individual partition in turn <ref> [20] </ref>. procedure self-join (x) begin if leaf-node (x) then leaf-self-join (x); else begin for i = 1 to f 1 do begin self-join (x [i], x [i]); join (x [i], x [i+1]); end self-join (x [f], x [f]); end procedure join (x, y) begin if leaf (x) and leaf (y) then <p> Global split ordering minimizes the number of neighboring leaf nodes and therefore minimizes the number of leaves that must later be joined <ref> [20] </ref>. Ideally, the dimension ordering should be chosen to minimize correlations between dimensions, but a random ordering is reasonably effective. Join Algorithm To perform a self join of an *-kdB tree, we begin at the root and recursively call the self-join algorithm on each child.
References-found: 20

