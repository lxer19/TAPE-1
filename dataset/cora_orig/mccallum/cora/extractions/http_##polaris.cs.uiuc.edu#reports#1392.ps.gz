URL: http://polaris.cs.uiuc.edu/reports/1392.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: On the Automatic Parallelization of the Perfect Benchmarks  
Author: R Rudolf Eigenmann Jay Hoeflinger David Padua 
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: We present a set of advanced program parallelization techniques that are able to significantly improve the performance of application programs. We present evidence for this improvement in terms of the overall program speedup that we have achieved on the Perfect Benchmarks R fl programs, and in terms of the performance gains that can be attributed to individual techniques. These numbers were measured on the Cedar multiprocessor at the University of Illinois. This paper extends the findings previously reported in [EHLP91]. The techniques credited most for the performance gains include array privatization, parallelization of reduction operations, and the substitution of generalized induction variables. We have applied these transformations by hand to the given programs, in a mechanical manner, similar to that of parallelizing compiler. Because of our success with these transformations, we believe that it will be possible to implement many of these techniques in a new generation of parallelizing compilers. All these techniques can be considered extensions of transformations that are available in current restructuring compilers. The implementation of these techniques, however, will require the development of more powerful analysis techniques. We have already started a major new project to do this, the early results of which we present here. 
Abstract-found: 1
Intro-found: 1
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year> <month> 31 </month>
Reference-contexts: TRFD-olda/300 showed the same pattern for arrays XIJKS and XKL. In an earlier version of TRFD, where the outer loop was not yet parallelized, parallel reduction transformations proved effective. 2.3 Generalized induction variables (GIV) In Fortran DO loops, array subscripts often use the values of induction variables <ref> [ASU86] </ref>, which are updated in each iteration in the form of V = V op K, where the value K is loop-invariant. Such a recursive assignment causes cross-iteration data dependences.
Reference: [BDE + 96] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jaejin Lee, David Padua, Yunheung Paek, William Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris. </title> <type> Technical Report 1473, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: Figure 4 shows that Polaris delivers, in many cases, substantially better speedups than PFA. For a few of the programs there is little speedup, and in one case there is a slowdown. We have studied and discussed the reasons for this in <ref> [BDE + 96] </ref>. We found that Polaris is significantly more successful in identifying parallel loops. However, in the programs where PFA identifies the important parallel loops equally well, its additional techniques for improving the parallel code make a difference. These transformations include loop interchanging, unrolling, and fusion.
Reference: [BE92] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The short descriptions of the problems being solved in the Perfect Benchmarks are taken from [Poi90]. Detailed optimization reports can be found in individual technical reports avail 20 able from CSRD 8 . A more thorough discussion of the automatic parallelization done by both KAP/Cedar and VAST is in <ref> [BE92] </ref>. Both restructurers were similar in terms of the resulting code and performance. ADM "is a three-dimensional fluid flow code that simulates pollutant concentration and deposition patterns in lakeshore environments by solving the complete system of hydrodynamic equations.
Reference: [BE94a] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Nonlinear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <address> Washington D.C., </address> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We have described GIVs in a preliminary report ([EHLP91]), which has led to the development of several recognition techniques, as mentioned in Section 1. All these techniques introduce non-linear subscript expressions, which cannot be understood by previous data-dependence analysis techniques. In <ref> [BE94a] </ref>, we have described a new analysis technique that can handle such nonlinear subscripts. The effect of the Generalized Induction Variable transformation is shown in Table 5. The table shows the same type of information as Tables 3 and 4. <p> This knowledge is lost when substituting the induction variable. An approach could be to flag this situation in the induction variable substitution pass and enhance the dependence test to understand this flag. Alternatively, a non-linear test could be devised <ref> [BE94a] </ref>. Of further difficulty is the fact that the subscript contains symbolic values whose relation to other terms determines whether the loop is independent. Symbolic analysis techniques are important in this situation. Failing to parallelize this loop in TRFD would result in a program slowdown of a factor of 13.
Reference: [BE94b] <author> William Blume and Rudolf Eigenmann. </author> <title> An Overview of Symbolic Analysis Techniques Needed for the Effective Parallelization of the Perfect Benchmarks. </title> <booktitle> Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <address> pages II233 II238, </address> <month> August, </month> <year> 1994. </year>
Reference-contexts: A similar situation exists in QCD (measur/3). To privatize the arrays in loop actfor/240 in BDNA, one has to recognize subscripted subscript patterns, which is very difficult in general. However, in the given situation, all necessary information can be derived from the program text <ref> [TP93, BE94b] </ref>. 2.2 Parallel Reductions Statements of the type sum = sum + a (i) (where i is the loop index) form a recurrence pattern that usually must be executed serially. <p> We believe that, although the techniques may be complex in some cases, it is possible to develop the necessary symbolic analysis technology. These patterns and possible symbolic analysis techniques are described in more detail in <ref> [BE94b] </ref>. In most of the Perfect Benchmarks programs, we have found this analysis to be necessary across subroutine calls. We need interprocedural analysis for two reasons. <p> In this section, we present early results that demonstrate Po-laris' success and areas for potential improvement in next generation compilers. The specific transformations and their implementations are not discussed in this paper. We refer interested readers to [TP93], [BEH + 94], <ref> [BE94b] </ref>, and [PE95], which describe the details of the algorithms used. The sole purpose of this section is to give some evidence that the automation of the hand transformations outlined in this paper is a feasible goal, and to point out problems that may be hard to solve.
Reference: [BE95] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic Range Propagation. </title> <booktitle> Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 357-363, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: The program initialization routine needs to be investigated to determine the values given to the subscript arrays, which are read-only from then on. A starting point for achieving such compiler capabilities could be the symbolic range propagation techniques described in <ref> [BE95] </ref>. Without recognizing that these loops are independent, only inner loops could be parallelized, resulting in a program slowdown of a factor of 3. 2.5.3 Runtime analysis techniques The described analysis may be complex or even undecidable at compile time.
Reference: [BEH + 94] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen, Lawrence Rauchw-erger, and Peng Tu. </author> <title> Automatic Detection of Parallelism: A Grand Challenge for High-Performance Computing. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 37-47, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: In this section, we present early results that demonstrate Po-laris' success and areas for potential improvement in next generation compilers. The specific transformations and their implementations are not discussed in this paper. We refer interested readers to [TP93], <ref> [BEH + 94] </ref>, [BE94b], and [PE95], which describe the details of the algorithms used.
Reference: [BENP93] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Other related projects have studied the effectiveness of existing parallelizing compilers or their techniques. A summary of these studies is given in <ref> [BENP93] </ref>. A second contribution of this paper is the discussion of new parallelizing compiler techniques. There exists a large body of publications on this subject. A survey can be found in [BENP93]. Several papers are directly related to the techniques discussed here. <p> A summary of these studies is given in <ref> [BENP93] </ref>. A second contribution of this paper is the discussion of new parallelizing compiler techniques. There exists a large body of publications on this subject. A survey can be found in [BENP93]. Several papers are directly related to the techniques discussed here. Early results of the presented work [EHLP91] have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables [Wol92, HP93] and for analyzing privatizable ar 3 rays [Li92, MAL92].
Reference: [EH96] <author> Rudolf Eigenmann and Siamak Hassanzadeh. </author> <title> Benchmarking with real industrial applications: The SPEC High-Performance Group. </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 3(1) </volume> <pages> 18-23, </pages> <month> Spring </month> <year> 1996. </year>
Reference-contexts: In fact, in many cases we have not been able to improve the Polaris-optimized code manually. Nevertheless, our future work will include studies with even more realistic applications, such as programs found among the "Grand Challenge" applications and the SPEChpc96 benchmarks <ref> [EH96] </ref>. Acknowledgment Some of the experiments described in this paper were done by Greg Jaxon and Zhiyuan Li while they were members of our research group at CSRD. Their contributions were essential for the success of this project.
Reference: [EHJ + 93] <author> Rudolf Eigenmann, Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 553-573, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Automatic parallelization had limited effectiveness with the technology used by commercial restructurers such as KAP and VAST 2 , as can be seen in Table 1 (under "Automatically compiled"). "Automatically compiled / Cedar" displays the results we had obtained after retar-geting KAP at the Cedar multiprocessor <ref> [EHJ + 93] </ref>. Without applying further transformations the performance is often less than the one of the eight-processor FX/8 machine. This is because this early compiler technology typically can find only very small parallelizable loops and the overhead of starting such small loops on Cedar is larger. <p> ENDIF 109 CONTINUE 2.4 Techniques that map parallel loops to the actual machine The restructuring compiler that we used as a starting point for our hand-optimized codes (KAP/Cedar <ref> [EHJ + 93] </ref>) often was able to discover parallelism, but then mapped it to the machine poorly. In this section, we will show various methods to improve this. <p> Large loop iteration counts often can be obtained with large input data sets. The Cedar architecture can work efficiently given large data sets, as we have shown in our previous work <ref> [EHJ + 93] </ref>: linear algebra routines working on matrices of size 1000 by 1000 can exploit the 32 Cedar processors well. <p> The example in Figure 3 shows that compiling a structure of multiple small sdoall loops into a single sdoall can result in a significant code improvement on Cedar. The compiler used in our studies (KAP/Cedar <ref> [EHJ + 93] </ref>) often was able to find large concurrent loops or to interchange loops to an outer position. It failed in other cases when too many potential data dependences were detected or when the outer loop was in a calling subroutine.
Reference: [EHLP91] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 589, </volume> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: We have commented on these questions in several reports <ref> [EHLP91, Eig93, EM93] </ref>. According to our main interest, we will emphasize the implications of this project on the design of future compilers. We will discuss program transformations and their automatability in an optimizing compiler. <p> A second contribution of this paper is the discussion of new parallelizing compiler techniques. There exists a large body of publications on this subject. A survey can be found in [BENP93]. Several papers are directly related to the techniques discussed here. Early results of the presented work <ref> [EHLP91] </ref> have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables [Wol92, HP93] and for analyzing privatizable ar 3 rays [Li92, MAL92]. <p> The loop intgrl/140 was parallelized at the outermost level after we determined that the form of initialization of the subscripting array made subscripted-subscript access parallelizable. We described this situation in an earlier paper <ref> [EHLP91] </ref>. 4 Early results of a new generation parallelizing compiler The objective of this paper was to show new transformations that can improve future paral-lelizing compilers. We have already started the design of a new compiler, called Polaris, that incorporates these techniques.
Reference: [Eig93] <author> Rudolf Eigenmann. </author> <title> Toward a Methodology of Optimizing Programs for High-Performance Computers. </title> <booktitle> Conference Proceedings, ICS'93, </booktitle> <address> Tokyo, Japan, </address> <pages> pages 27-36, </pages> <month> July 20-22, </month> <year> 1993. </year>
Reference-contexts: We have commented on these questions in several reports <ref> [EHLP91, Eig93, EM93] </ref>. According to our main interest, we will emphasize the implications of this project on the design of future compilers. We will discuss program transformations and their automatability in an optimizing compiler.
Reference: [EM93] <author> Rudolf Eigenmann and Patrick McClaughry. </author> <title> Practical Tools for Optimizing Parallel Programs. </title> <booktitle> Presented at the 1993 SCS Multiconference, </booktitle> <address> Arlington, VA, March 27 - April 1, </address> <year> 1993. </year>
Reference-contexts: We have commented on these questions in several reports <ref> [EHLP91, Eig93, EM93] </ref>. According to our main interest, we will emphasize the implications of this project on the design of future compilers. We will discuss program transformations and their automatability in an optimizing compiler.
Reference: [EPV96] <author> Rudolf Eigenmann, Insung Park, and Michael J. Voss. </author> <title> Are parallel workstations the right target for parallelizing compilers? In Processdings of the Ninth Workshop on Languages and Compilers for Parallel Computers, </title> <month> August 96. </month>
Reference-contexts: Early results of a new compiler that incorporates the proposed techniques show substantial performance gains over state-of-the art compilers on an SGI Challenge machine. Similar results were reported on a Cray T3D machine [PP96] and a Sun multiprocessor <ref> [EPV96] </ref>. Privatization may make data distribution less important. Much recent work on programming techniques for distributed-memory machines has focused on data-distribution techniques. In our study we have not found such techniques to be of significant importance.
Reference: [For93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical report, </type> <institution> Rice University, Houston Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines. Discussions on complementing techniques for code generation on message passing machines can be found in <ref> [For93, GMS + 95] </ref>. The basics about Cedar Fortran The parallel Fortran language that we used for expressing our explicitly parallel program variants is called Cedar Fortran.
Reference: [GJT + 91] <author> K. Gallivan, W. Jalby, S. Turner, A. Veidenbaum, and H. Wijshoff. </author> <title> Preliminary Basic Performance Analysis of the Cedar Multiprocessor Memory Systems. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, I:71-75, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: The four-cluster version stripmines the iteration space onto the processors in contiguous blocks to produce vector instructions with a stride of one. Stride-one vector references are beneficial in the Cedar architecture because of the way the prefetch unit operates <ref> [GJT + 91] </ref>. When the original loops are doubly-nested they are coalesced into a single loop first, so that the combined iteration space is available to divide among the processors. The stripmin-ing transformation itself divides the iterations among the clusters first, and then among the processors of a cluster.
Reference: [GKT91] <author> Gina Goff, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Practical Dependence Testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations <ref> [MHL91, GKT91] </ref>, and enhancing tests with symbolic analysis capabilities [HP91]. In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines.
Reference: [GMS + 95] <author> M. Gupta, S. Midkiff, E. Schoenberg, B. Seshadri, D. Shields, K.Y. Wang, M.M. Ching, and Ton Ngo. </author> <title> An HPF compiler for the IBM SP-2. </title> <booktitle> In Proc. Supercomputing 95. </booktitle> <publisher> ACM Press, </publisher> <address> New York., San Diego, California, </address> <year> 1995. </year>
Reference-contexts: In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines. Discussions on complementing techniques for code generation on message passing machines can be found in <ref> [For93, GMS + 95] </ref>. The basics about Cedar Fortran The parallel Fortran language that we used for expressing our explicitly parallel program variants is called Cedar Fortran.
Reference: [Hoe92a] <author> Jay Hoeflinger. </author> <title> Coalescing Triangular Loops. </title> <type> Technical Report 1364, </type> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In this situation, it is advantageous to coalesce the two levels and make a single parallel loop out of them. For Cedar, this loop may become an xdoall. This transformation has been described in <ref> [Hoe92a] </ref>. Once these loops were coalesced, the number of iterations became large enough that every invocation of the loop could exploit the Cedar resources. As a result, the speed of the loop doubled. The loop nest accounts for approximately 50% of the serial program execution time. <p> OCEAN ffi (1) Loop coalescing. Improved range analysis QCD Substitution of a random-number algorithm. SPEC77 Substitution of a search algorithm. SPICE (1) Analysis of subscripted subsctripts and irregular control flow. TRACK Parallelization of memory allocation algorithms. TRFD * (1) These issues are being addressed in ongoing Polaris work <ref> [Hoe92a, RAP95] </ref> Table 10: Success and unsolved problems in the automatic parallelization of the Perfect Bench marks achieved by a prototype of Polaris. 29 grams.
Reference: [Hoe92b] <author> Jay Hoeflinger. </author> <title> Run-Time Dependence Testing by Integer Sequence Analysis. </title> <type> Technical Report 1194, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1992. </year> <month> 32 </month>
Reference-contexts: If this is the case, then there are no dependences on the references to those arrays. The result of this experiment is the loop shown in Table 9. A more detailed description of the test can be found in <ref> [Hoe92b] </ref>.
Reference: [HP91] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High--Performance Parallelizing Compilers. </title> <booktitle> Parallel and Distributed Computing: Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 310-330, </pages> <year> 1991. </year>
Reference-contexts: There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities <ref> [HP91] </ref>. In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines.
Reference: [HP93] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Analysis: A Basis for Paral-leliziation, Optimization, and Scheduling of Programs. </title> <booktitle> Proceedings of the Sixth Annual Languages and Compilers for Parallelism Workshop, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: A survey can be found in [BENP93]. Several papers are directly related to the techniques discussed here. Early results of the presented work [EHLP91] have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables <ref> [Wol92, HP93] </ref> and for analyzing privatizable ar 3 rays [Li92, MAL92]. There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities [HP91].
Reference: [Hus82] <author> Christopher Alan Huson. </author> <title> An In-Line Subroutine Expander for Parafrase. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Dec., </month> <year> 1982. </year>
Reference-contexts: Second, there may be variables whose values determine the existence of dependences, and these values may be passed as arguments from other subroutines where they are defined. Again, this prevents parallelization. The compiler we used in our experiments relies on inlining <ref> [Hus82] </ref>, which replaces call statements with the text of the called subroutines. This has worked well in a few programs, but we have found that other programs would need more advanced interprocedural analysis capabilities than we had available.
Reference: [KBC + 74] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Muraoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: The programs in the Perfect Benchmarks suite have not been found to be sensitive to such reorderings. Reordering certain arithmetic operations in order to increase parallelism is a technique known as tree height reduction <ref> [KBC + 74] </ref>. Simple reduction operations are recognized by paral-lelizing compilers and transformed into the appropriate vector or vector-concurrent constructs. We have measured this capability and found it to be one of the most effective ones.
Reference: [KDL + 93] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, C.-Q Zhu, A. Veidenbaum, J. Konicek, P. Yew, K. Gal-livan, W. Jalby, H. Wijshoff, R. Bramley, U.M. Yang, P. Emrath, D. Padua, R. Eigenmann, J. Hoe-flinger, G. Jaxon, Z. Li, T. Murphy, J. Andrews, and S. Turner. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> Proceedings of the 20th Int'l. Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <pages> pages 213-224, </pages> <month> May 16-19, </month> <year> 1993. </year>
Reference-contexts: If we define "speedup" as the ratio of the best performance versus the serial program (i.e., neither vector nor concurrent parallelism, but scalar-optimized), an excellent speedup would be about 16 for the FX/8 and about 50 for the Cedar machine. More details on these architectures can be found in <ref> [KDL + 93] </ref>. Table 1 summarizes our measurements. The column "Automatically compiled" shows the performance gain achieved by an available parallelizing compiler. The column "Manually improved" shows the results of our hand optimizations, which we will discuss in this paper.
Reference: [Li92] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of ICS'92, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: Several papers are directly related to the techniques discussed here. Early results of the presented work [EHLP91] have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables [Wol92, HP93] and for analyzing privatizable ar 3 rays <ref> [Li92, MAL92] </ref>. There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities [HP91].
Reference: [MAL92] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proc. 5rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Several papers are directly related to the techniques discussed here. Early results of the presented work [EHLP91] have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables [Wol92, HP93] and for analyzing privatizable ar 3 rays <ref> [Li92, MAL92] </ref>. There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities [HP91].
Reference: [MHL91] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In SIGPLAN NOTICES: Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June 26-28, </month> <pages> pages 1-14. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations <ref> [MHL91, GKT91] </ref>, and enhancing tests with symbolic analysis capabilities [HP91]. In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines.
Reference: [PE95] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 444-448, 95. </pages>
Reference-contexts: In this section, we present early results that demonstrate Po-laris' success and areas for potential improvement in next generation compilers. The specific transformations and their implementations are not discussed in this paper. We refer interested readers to [TP93], [BEH + 94], [BE94b], and <ref> [PE95] </ref>, which describe the details of the algorithms used. The sole purpose of this section is to give some evidence that the automation of the hand transformations outlined in this paper is a feasible goal, and to point out problems that may be hard to solve.
Reference: [Poi90] <author> Lynn Pointer. </author> <title> Perfect: Performance Evaluation for Cost-Effective Transformations Report 2. </title> <type> Technical Report 964, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Our experiments did not include the program SPICE. SPICE has been discussed in a related project, which found runtime parallelization techniques to be applicable to the irregular structure of this code [RP95]. The short descriptions of the problems being solved in the Perfect Benchmarks are taken from <ref> [Poi90] </ref>. Detailed optimization reports can be found in individual technical reports avail 20 able from CSRD 8 . A more thorough discussion of the automatic parallelization done by both KAP/Cedar and VAST is in [BE92]. Both restructurers were similar in terms of the resulting code and performance. <p> ADM "is a three-dimensional fluid flow code that simulates pollutant concentration and deposition patterns in lakeshore environments by solving the complete system of hydrodynamic equations. The advection-diffusion equation for the transport, diffusion, and deposition of pollutants is also included in the model <ref> [Poi90] </ref>." The program code is 6104 lines long and consists of 97 subroutines. The execution time is spread evenly throughout the program; 90% of the execution time is spent in 23 subroutines. Almost all of these subroutines contain three or fewer loop nests, all of which are important. <p> ARC2D "was developed at NASA/Ames and run on a Cray X-MP. It is a robust, general-purpose, implicit finite-difference code for analyzing fluid flow problems. It solves the Euler equations. Arc2D can be used for steady and unsteady flows, but only for inviscid flows <ref> [Poi90] </ref>." The code contains 4,000 lines of Fortran77 in 74 subroutines. Arc2D shows the best speedup from automatic parallelization on both FX/8 (8.7) and Cedar (13.5). There are about 30 loop nests that need to be parallelized well for good overall performance. <p> This package aims at an understanding of the hydration, structure, and dynamics of nucleic acids and, more broadly, the role of water in the operation of biological systems <ref> [Poi90] </ref>." The code contains 4,000 lines of Fortran77 in 76 subroutines. BDNA gains insignificant speedup from automatic parallelization by KAP (1.9). VAST does somewhat better on this code on the Alliant FX/8 machine because it can parallelize simple reduction operations better than KAP. <p> DYFESM "is a two-dimensional finite element code for the analysis of symmetric anisotropic structures. An explicit leap-frog temporal method with substructuring is used to solve for the displacements and stresses, along with the velocities and accelerations at each time step <ref> [Poi90] </ref>." The code contains 7,600 lines of Fortran77 in 113 subroutines. 10 This is not necessarily a requirement for parallelization, but can simplify the generated code. 22 Automatic parallelization by KAP yielded a speedup of 4 on the Alliant FX/8 over the unoptimized code. <p> In other cases, such as the major loops mxmult/10 and formr0/20, the patterns probably can not be investigated at compile time. FLO52 "This two-dimensional code provides an analysis of the transonic inviscid flow past an airfoil by solving the unsteady Euler equations <ref> [Poi90] </ref>." The code contains 2,000 lines of Fortran77 in 64 subroutines. KAP produced the best speedup numbers of any Perfect Benchmarks program for FLO52 running on the FX/8 (9.0). The speedup of FLO52 on Cedar produced by KAP was second best (5.5, behind the speedup of ARC2D). <p> The code uses the Matsuoka-Clementi-Yoshimine configuration interaction potential for rigid water-water interactions and extends it to include the effects of intra-molecular vibration. MDG can be used to predict a wide variety of static and dynamic properties of liquid water <ref> [Poi90] </ref>." The code contains 1200 lines of Fortran77 in 51 subroutines. MDG has no speedup from automatic parallelization. None of the major loops in this code can be parallelized because KAP reports data dependences. <p> The primary challenge in parallelizing MDG automatically is to detect all privatizable variables. In Section 2.1, we have described these patterns, that require advanced symbolic analysis techniques. MG3D "is a seismic migration code used to investigate the geological structure of the Earth <ref> [Poi90] </ref>." The code contains 2800 lines of Fortran77 in 64 subroutines. KAP parallelized most loop nests not containing subroutine calls or input/output statements, except for those loops within certain routines (cpass, cpassm, rpass, and rpassm) that contain potential dependences. <p> We have not exploited this situation in our manual optimizations. OCEAN "solves the dynamical equations of a two-dimensional Boussinesq fluid layer. The code is needed in order to study the chaotic behavior of free-slip Rayleigh-Benard convection <ref> [Poi90] </ref>." The code contains 3400 lines of Fortran77 in 70 subroutines. KAP was able to parallelize only inner loops and trivial perfectly-nested loops. <p> Since the code ignores the effect of dynamical fermions, it may be considered a pure-gauge model in the quenched approximation <ref> [Poi90] </ref>." The code contains 2,300 lines of Fortran77 in 69 subroutines. KAP was able to parallelize only minor, inner-most loops in QCD, stripmining them to be sdoall/cdoall/vector. <p> A variant without replacing the random number generator yielded a speedup of only 1.8. SPEC77 "is a global spectral model for simulating atmospheric flow. The code was originally developed at the National Meteorological Center (NMC). Only the forecast module of the NMC code is used in the benchmarking <ref> [Poi90] </ref>." The code contains 3,900 lines of Fortran77 in 99 subroutines. KAP was unable to transform most major loops due to the large number of subroutine calls in this program. Typically, KAP could parallelize nothing beyond innermost loops, yielding a speedup of 2.4 on both machines. <p> TRACK "This missile tracking code is used to determine the course of a set of an unknown number of targets, such as rocket boosters, from observations of the targets taken by sensors at regular time intervals. The targets may be launched from a number of different sites <ref> [Poi90] </ref>." The code contains 4300 lines of Fortran77 in 66 subroutines. KAP was unable to speed up this code. From an application viewpoint, the problem consists of a number of independent tasks that track missiles, which would seem highly-parallelizable. <p> TRFD "is a kernel simulating the computational aspects of a two-electron integral transformation and part of the HONDO quantum mechanical package. The evaluation of these types of integral transformations is a necessary first step in computing correlated wave functions and is used in determinations of molecular electronic structure <ref> [Poi90] </ref>." The code contains 500 lines of Fortran77 in 42 subroutines. KAP, again, could parallelize only inner or perfectly-nested loops within TRFD.
Reference: [PP96] <author> Yunheung Paek and David Padua. </author> <title> Automatic parallelization for noncoherent cache multiprocessors. </title> <booktitle> In Processdings of the Ninth Workshop on Languages and Compilers for Parallel Computers, </booktitle> <month> August 96. </month>
Reference-contexts: Early results of a new compiler that incorporates the proposed techniques show substantial performance gains over state-of-the art compilers on an SGI Challenge machine. Similar results were reported on a Cray T3D machine <ref> [PP96] </ref> and a Sun multiprocessor [EPV96]. Privatization may make data distribution less important. Much recent work on programming techniques for distributed-memory machines has focused on data-distribution techniques. In our study we have not found such techniques to be of significant importance.
Reference: [Pug92] <author> William Pugh. </author> <title> A Practical Algorithm for Exact Array Dependence Analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis <ref> [Pug92] </ref>, more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities [HP91]. In our work, we have focused on machines that provide a hardware-supported global address space to the programmer and the compiler. They represent an important and widely-available class of parallel machines.
Reference: [RAP95] <author> Lawrence Rauchwerger, Nancy M. Amato, and David A. Padua. </author> <title> Run-Time Methods for Parallelizing Partially Parallel Loops. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <year> 1995. </year>
Reference-contexts: OCEAN ffi (1) Loop coalescing. Improved range analysis QCD Substitution of a random-number algorithm. SPEC77 Substitution of a search algorithm. SPICE (1) Analysis of subscripted subsctripts and irregular control flow. TRACK Parallelization of memory allocation algorithms. TRFD * (1) These issues are being addressed in ongoing Polaris work <ref> [Hoe92a, RAP95] </ref> Table 10: Success and unsolved problems in the automatic parallelization of the Perfect Bench marks achieved by a prototype of Polaris. 29 grams.
Reference: [RP95] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <booktitle> Proceedings for the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: For each code we will briefly discuss the expected difficulties for automating the transformations. Our experiments did not include the program SPICE. SPICE has been discussed in a related project, which found runtime parallelization techniques to be applicable to the irregular structure of this code <ref> [RP95] </ref>. The short descriptions of the problems being solved in the Perfect Benchmarks are taken from [Poi90]. Detailed optimization reports can be found in individual technical reports avail 20 able from CSRD 8 .
Reference: [SH91] <author> J.P. Singh and J.L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Related Work The primary contribution of this paper is the quantitative analysis of real program patterns and the discussion of their implications for compiler design. There exist very few similar publications. One related program analysis project was presented in <ref> [SH91] </ref>, where the authors reach qualitatively similar conclusions that there is a need for advanced compiler techniques including symbolic analysis and the privatization of data structures. Other related projects have studied the effectiveness of existing parallelizing compilers or their techniques. A summary of these studies is given in [BENP93].
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid Gelern-terAlex NicolauDavid Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: A similar situation exists in QCD (measur/3). To privatize the arrays in loop actfor/240 in BDNA, one has to recognize subscripted subscript patterns, which is very difficult in general. However, in the given situation, all necessary information can be derived from the program text <ref> [TP93, BE94b] </ref>. 2.2 Parallel Reductions Statements of the type sum = sum + a (i) (where i is the loop index) form a recurrence pattern that usually must be executed serially. <p> In this section, we present early results that demonstrate Po-laris' success and areas for potential improvement in next generation compilers. The specific transformations and their implementations are not discussed in this paper. We refer interested readers to <ref> [TP93] </ref>, [BEH + 94], [BE94b], and [PE95], which describe the details of the algorithms used.
Reference: [Wol92] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proc. ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <year> 1992. </year> <month> 33 </month>
Reference-contexts: A survey can be found in [BENP93]. Several papers are directly related to the techniques discussed here. Early results of the presented work [EHLP91] have led to new efforts elsewhere, such as the work on techniques for handling general forms of induction variables <ref> [Wol92, HP93] </ref> and for analyzing privatizable ar 3 rays [Li92, MAL92]. There also has been significant new work in data dependence analysis for parallelizing compilers, including techniques for more exact subscript analysis [Pug92], more efficient analysis in practical situations [MHL91, GKT91], and enhancing tests with symbolic analysis capabilities [HP91].
References-found: 37

