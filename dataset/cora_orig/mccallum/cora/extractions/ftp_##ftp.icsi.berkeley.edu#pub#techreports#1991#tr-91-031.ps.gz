URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-031.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Email: E-mail jfeldman@icsi.berkeley.edu.  E-mail clim@icsi.berkeley.edu.  E-mail mazz@icsi.berkeley.edu.  
Title: pSather monitors: Design, Tutorial, Rationale and Implementation  
Author: Jerome A. Feldman Chu-Cheow Lim Franco Mazzanti 
Address: Berkeley.  Berkeley.  Pisa Italy.  
Affiliation: ICSI and Computer Science Division, U.C.  ICSI and Computer Science Division, U.C.  ICSI and Istituto di Elaborazione dell'Informazione, CNR  
Date: September 1991  
Pubnum: TR-91-031  
Abstract: pSather is a parallel extension of Sather aimed at shared memory parallel architectures. A prototype of the language is currently being implemented on a Sequent Symmetry and on SUN Sparc-Stations. pSather monitors are one of the basic new features introduced in the language to deal with parallelism. The current design is presented and discussed in detail. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha. </author> <title> Concurrent object-oriented programming. </title> <journal> Communications of the ACM, </journal> <volume> 33(9) </volume> <pages> 125-141, </pages> <month> September </month> <year> 1990. </year>
Reference: [2] <author> Gul Agha and Carl Hewitt. </author> <title> Actors: A conceptual foundation for concurrent object-oriented programming. </title> <editor> In Bruce Shriver and Peter Wegner, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference: [3] <author> Pierre America. </author> <title> Issues in the Design of a Parallel Object-Oriented Language. </title> <institution> Philips Research Laboratories, Eindhoven and University of Amsterdam, </institution> <month> March 1 </month> <year> 1989. </year> <title> Part of POOL2/PTC Distribution Package. </title>
Reference: [4] <author> Pierre America. </author> <title> Programmer's Guide for POOL2. </title> <institution> Philips Research Laboratories, Eindhoven and University of Amsterdam, </institution> <month> January 10 </month> <year> 1991. </year> <title> Part of POOL2/PTC Distribution Package. </title>
Reference: [5] <author> Pierre America and Ben Hulshof. </author> <title> Definition of POOL2/PTC, a Parallel Object-Oriented Language. </title> <institution> Philips Research Laboratories, Eindhoven and University of Amsterdam, </institution> <month> March 15 </month> <year> 1991. </year> <title> Part of POOL2/PTC Distribution Package. </title>
Reference: [6] <author> Birger Andersen. </author> <title> Ellie a general, fine-grained first class object based language. </title> <type> Technical report, </type> <institution> University of Copenhagen, </institution> <month> July </month> <year> 1991. </year>
Reference: [7] <author> Birger Andersen. </author> <title> Ellie Language Definition Report. </title> <type> PhD thesis, </type> <institution> University of Copenhagen, Department of Computer Science, University of Copenhagen, </institution> <address> Universitetsparken 1, 2100 Copen-Hagen, Denmark, </address> <month> June </month> <year> 1991. </year> <journal> Second edition of paper in ACM SIGPLAN Notices, </journal> <volume> 25(11) </volume> <pages> 45-64, </pages> <month> November </month> <year> 1990. </year>
Reference: [8] <author> Birger Andersen. </author> <title> Fine-grained Parallelism in Ellie. </title> <type> PhD thesis, </type> <institution> University of Copenhagen, Department of Computer Science, University of Copenhagen, </institution> <address> Universitetsparken 1, 2100 Copen-Hagen, Denmark, </address> <month> June </month> <year> 1991. </year>
Reference: [9] <author> Thomas E. Anderson. </author> <title> Fastthreads user's manual. FastThreads software package manual, </title> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: Since this depends on threads package, the virtual process package should be machine-independent for single-processor machines. The runtime support for pSather has evolved from incorporating the FastThreads <ref> [9] </ref> into the original runtime support for Sather. Figure 3 shows the top-level design of the driver program that creates multiple processes, and schedules the threads. The initial implementation of the runtime was done on the Sequent.
Reference: [10] <author> Colin Atkinson, Stephen Goldsack, Andrea Di Maio, and Rami Bayan. </author> <title> Object-oriented con-currency and distribution in dragoon. </title> <type> Technical Report Research Report DoC 89/3, </type> <institution> Imperial College, </institution> <month> June </month> <year> 1989. </year>
Reference: [11] <author> Henri E. Bal, Andrew S. Tanenbaum, and M. Frans Kaashoek. Orca: </author> <title> A language for distributed programming. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(5) </volume> <pages> 17-24, </pages> <year> 1990. </year>
Reference: [12] <author> Vasanth Balasundarm, Geoffrey Fox, Ken Kennedy, and Ulrich Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <pages> pages 213-223, </pages> <address> Williams-burg, Virginia, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In addition, we plan to examine a sufficiently large application, such as the N-body problem [24], to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], [36], <ref> [12] </ref>, [25], [47], [32]), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine). We are also investigating general algorithms for parallel objects spread across a NUMA architecture, such as parallel sets and graphs.
Reference: [13] <author> H. Boehm and Weiser M. </author> <title> Garbage collection in an uncooperative environment. </title> <journal> Software Software Practice & Experience pp. </journal> <pages> 807-820, </pages> <month> September </month> <year> 1988. </year> <note> REFERENCES 89 </note>
Reference: [14] <author> P. Brinch Hansen. </author> <title> The programming language concurrent pascal. </title> <journal> IEEE Transactions on Software Engineering 1: </journal> <pages> pp. 199-207, </pages> <month> June </month> <year> 1975. </year>
Reference: [15] <author> P. Brinch Hansen. </author> <title> Monitors and concurrent pascal: A personal history, </title> <month> June </month> <year> 1991. </year> <title> Private communication. </title>
Reference: [16] <author> D. Caromel. </author> <title> A general model for concurrent and distributed object-oriented programming. </title> <journal> SIGPLAN Notices, </journal> <volume> 24(4), </volume> <month> April </month> <year> 1989. </year>
Reference: [17] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Cool: A language for parallel programming. </title> <type> Technical Report CSL-TR-89-396, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> October </month> <year> 1989. </year>
Reference: [18] <author> Andrew A. Chien and William J. Dally. </author> <title> Concurrent aggregates (ca). </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on the Principles and Practice of Parallel Programming, </booktitle> <year> 1990. </year>
Reference: [19] <author> Michael Coffin. </author> <title> Par: A language for architecture-independent parallel programming. </title> <type> Technical Report TR 89-18, </type> <institution> Department of Computer Science, The University of Arizona, </institution> <address> Tucson, Arizona 85721, </address> <month> September 28 </month> <year> 1989. </year>
Reference: [20] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April 8-11 </month> <year> 1991. </year>
Reference-contexts: As will be pointed out in the conclusion, we plan to look further in the issues of implementing algorithms and data structures in pSather. There appears to be no inherent problem in applying expression-level parallelism <ref> [20] </ref> to pSather programs and we will explore the possibility that some of the same thread mechanisms can be used in such cases. 7 FUTURE DIRECTIONS 86 7 Future Directions This paper has described the design and implementation of the monitor concept which is the basic parallel construct in pSather.
Reference: [21] <author> Flavio De Paoli and Mehdi Jazayeri. </author> <title> Flame: A language for distributed programming. </title> <institution> Hewlett-Packard Laboratories, </institution> <address> Palo Alto, CA 94304. </address>
Reference: [22] <author> N. H. Gehani and W. D. Roome. </author> <title> Concurrent c. </title> <journal> Software Practice and Experience, </journal> <volume> 16(9) </volume> <pages> 821-844, </pages> <month> September </month> <year> 1986. </year>
Reference: [23] <author> Andrew V. Goldberg and Robert E. Tarjan. </author> <title> A new approach to the maximum-flow problem. </title> <journal> Journal of the ACM, </journal> <volume> 35(4) </volume> <pages> 921-940, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: As a result, we need to understand how to program, so that more than one thread can work simultaneously on the same data structure. Availability of Theoretical Results A number of algorithms (eg connected component [48], maximum flow <ref> [23] </ref>) have well-understood theoretical complexity. The question is whether these algorithms can be implemented efficiently in an actual parallel environment. Using the performance statistics of these implementations, we can further fine-tune the runtime and compiler in the following respects.
Reference: [24] <author> Leslie Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <publisher> ACM Distinguished Dissertations. The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Our strategy is to first try to solve the compiler portability issue in a NUMA machine, and then re-examine the implemented algorithms in that environment. In addition, we plan to examine a sufficiently large application, such as the N-body problem <ref> [24] </ref>, to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], [36], [12], [25], [47], [32]), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine).
Reference: [25] <author> Seema Hiranandani, Joel Saltz, Harry Berryman, and Piyush Mehrotra. </author> <title> A scheme for supporting distributed data structures on multicomputers. </title> <type> Technical Report NASA Contractor Report 181987, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, Hampton, Virginia 23665. </institution>
Reference-contexts: In addition, we plan to examine a sufficiently large application, such as the N-body problem [24], to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], [36], [12], <ref> [25] </ref>, [47], [32]), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine). We are also investigating general algorithms for parallel objects spread across a NUMA architecture, such as parallel sets and graphs.
Reference: [26] <author> C. A. R. Hoare. </author> <title> Monitors: an operating system structuring concept. </title> <journal> Communications of the ACM 17: </journal> <pages> pp. 156-164, </pages> <month> April </month> <year> 1974. </year>
Reference: [27] <author> Waldemar Horwat, Andrew A. Chien, and William J. Dally. </author> <title> Experience with cst: </title> <booktitle> Programming and implementation. In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference: [28] <author> Jin H. Hur and Kilnam Chon. </author> <title> Overview of a parallel object-oriented language clix. </title> <type> Technical Report CS-TR-87-25, </type> <institution> Computer Science Department, Korea Advanced Institute of Science and Technology, Seoul, Republic of Korea, </institution> <year> 1987. </year>
Reference: [29] <author> Thomas W. Doeppner Jr. and Alan J. Gebele. </author> <title> C++ on a parallel machine. </title> <type> Technical Report CS-87-26, </type> <institution> Brown University, Department of Computer Science, Brown University, </institution> <address> Providence, RI 02912, </address> <month> November 17 </month> <year> 1987. </year> <note> REFERENCES 90 </note>
Reference: [30] <author> Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black. </author> <title> Fine-grained mobility in the emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference: [31] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on simd machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference: [32] <author> Charles Koelbel, Piyush Mehrotra, and John Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <type> Technical Report NASA Contractor Report 181981, ICASE Report No. 90-7, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, Hampton, Virginia 23665., </institution> <month> Jan </month> <year> 1990. </year>
Reference-contexts: In addition, we plan to examine a sufficiently large application, such as the N-body problem [24], to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], [36], [12], [25], [47], <ref> [32] </ref>), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine). We are also investigating general algorithms for parallel objects spread across a NUMA architecture, such as parallel sets and graphs.
Reference: [33] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the dash multiprocessor. </title> <type> Technical Report CSL-TR-89-404, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Sather [35] shows that the performance of a Sather program is close to a comparable C program. * It is a relatively clean language, offering certain constructs such as strong typing, storage management and class parameterization which are not available in efficient object-oriented languages such as C++. 5 [34] and <ref> [33] </ref> are examples of work in building NUMA multiprocessors. 6 [44] gives a non-exhaustive list of references of work on parallel debuggers. REFERENCES 88 Acknowledgements Thanks to Krste Asanovic, Joachim Beer, Jeff Bilmes, Steve Omohundro, Abhiram Ranade and Heinz Schmidt, who have contributed much to the language design and implementation.
Reference: [34] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> Design of the stanford dash multiprocessor. </title> <type> Technical Report CSL-TR-89-403, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: evaluation of Sather [35] shows that the performance of a Sather program is close to a comparable C program. * It is a relatively clean language, offering certain constructs such as strong typing, storage management and class parameterization which are not available in efficient object-oriented languages such as C++. 5 <ref> [34] </ref> and [33] are examples of work in building NUMA multiprocessors. 6 [44] gives a non-exhaustive list of references of work on parallel debuggers.
Reference: [35] <author> Chu-Cheow Lim and Andreas Stolcke. </author> <title> Sather Language design and performance evaluation. </title> <type> Technical Report TR-91-034, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca., </address> <month> May </month> <year> 1991. </year>
Reference-contexts: This is where the object-oriented aspects of pSather should come in handy. * Unlike object-oriented languages (such as Smalltalk and Eiffel) which incur high runtime costs, a performance evaluation of Sather <ref> [35] </ref> shows that the performance of a Sather program is close to a comparable C program. * It is a relatively clean language, offering certain constructs such as strong typing, storage management and class parameterization which are not available in efficient object-oriented languages such as C++. 5 [34] and [33] are
Reference: [36] <author> Richard J. Littlefield. </author> <title> Efficient iteration in data-parallel programs with irregular and dynamically distributed data structures. </title> <type> Technical Report 90-02-06, </type> <institution> Department of Computer Science and Engineering, FR-35 University of Washington, </institution> <address> Seattle, Washington 98195, USA., </address> <month> Feb </month> <year> 1990. </year>
Reference-contexts: In addition, we plan to examine a sufficiently large application, such as the N-body problem [24], to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], <ref> [36] </ref>, [12], [25], [47], [32]), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine). We are also investigating general algorithms for parallel objects spread across a NUMA architecture, such as parallel sets and graphs.
Reference: [37] <author> Bertrand Meyer. </author> <title> Object-oriented Software Construction. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [38] <author> Betrand Meyer. </author> <title> Sequential and concurrent object-oriented programming. </title> <booktitle> In TOOLS, </booktitle> <year> 1990. </year>
Reference: [39] <author> Thanasis Mitsolides. </author> <title> The Design and Implementation of ALLOY, a Higher Level Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> June </month> <year> 1991. </year>
Reference: [40] <author> Greg Nelson, </author> <title> editor. Systems Programming in Modula-3. </title> <institution> Digital Equipment Corp., </institution> <month> October 17 </month> <year> 1990. </year>
Reference: [41] <author> Stephen M. Omohundro. </author> <title> The Sather Language. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca., </address> <year> 1991. </year>
Reference: [42] <author> Stephen M. Omohundro, Chu-Cheow Lim, and Jeff Bilmes. </author> <title> The Sather Language compiler/debugger implementation. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, Ca., </institution> <note> 1991 (in preparation). </note>
Reference: [43] <author> Joseph Ira Pallas. </author> <title> Multiprocessor Smalltalk: Implementation, Performance and Analysis. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1990. </year> <note> Also available as technical report CSL-TR-90-429. </note>
Reference: [44] <author> Cherri M. Pancake and Sue Utter. </author> <title> A bibliography of parallel debuggers, 1990 edition. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(1) </volume> <pages> 21-37, </pages> <month> Jan </month> <year> 1991. </year>
Reference-contexts: is close to a comparable C program. * It is a relatively clean language, offering certain constructs such as strong typing, storage management and class parameterization which are not available in efficient object-oriented languages such as C++. 5 [34] and [33] are examples of work in building NUMA multiprocessors. 6 <ref> [44] </ref> gives a non-exhaustive list of references of work on parallel debuggers. REFERENCES 88 Acknowledgements Thanks to Krste Asanovic, Joachim Beer, Jeff Bilmes, Steve Omohundro, Abhiram Ranade and Heinz Schmidt, who have contributed much to the language design and implementation.
Reference: [45] <author> Edward Rothberg and Anoop Gupta. </author> <title> Parallel iccg on a hierarchical memory multiprocessor - addressing the triangular solve bottleneck. </title> <type> Technical report, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> Ca., </address> <month> September </month> <year> 1990. </year>
Reference-contexts: The aim of this exercise is to learn more about the usefulness of the parallel constructs and any limitations. * Substantial theoretical work has been done in designing parallel algorithms using PRAM as the machine model. However, the efficiency of an implementation often falls short of expectations (eg <ref> [45] </ref>). We therefore plan to implement a number of selected graph algorithms. There are a number of reasons for picking graph algorithms over other kinds of algorithms. Symbolic vs Numerical The parallel implementation of symbolic algorithms as a whole are not as well understood as numerical algorithms.
Reference: [46] <author> Heinz W. Schmidt and Jeff Bilmes. </author> <title> Exception handling in psather, </title> <note> 1991. Extended Abstract. REFERENCES 91 </note>
Reference-contexts: We give two examples here. The first glaring language construct that is missing in the current Sather/pSather implementation is the exception mechanism. <ref> [46] </ref> gives an initial proposal for an exception mechanism in Sather. However, since our eventual goal is to integrate both compilers, and eliminate the distinction between Sather and pSather, we would have to review the exception mechanism carefully before extending the language.
Reference: [47] <author> L.R. Scott, J.M. Boyle, and B. Bagheri. </author> <title> Distributed data structures for scientific computation. </title> <type> Technical Report IMA Preprint Series #291, </type> <month> January </month> <year> 1987, </year> <title> Institute for Mathematics and Its Applications, </title> <institution> University of Minnesota, </institution> <type> 514 Vincent Hall, 206 Church Street S.E., </type> <institution> Minneapolis, Minnesota 55455. </institution>
Reference-contexts: In addition, we plan to examine a sufficiently large application, such as the N-body problem [24], to study data placement problems which will arise. There has been some work done on the data allocation problem on distributed memory machines ([31], [36], [12], [25], <ref> [47] </ref>, [32]), but the studies have been limited to the allocation of arrays and/or SIMD machines (such as the Connection Machine). We are also investigating general algorithms for parallel objects spread across a NUMA architecture, such as parallel sets and graphs.
Reference: [48] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> An o(log n) parallel connectivity algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 3 </volume> <pages> 57-67, </pages> <year> 1982. </year>
Reference-contexts: As a result, we need to understand how to program, so that more than one thread can work simultaneously on the same data structure. Availability of Theoretical Results A number of algorithms (eg connected component <ref> [48] </ref>, maximum flow [23]) have well-understood theoretical complexity. The question is whether these algorithms can be implemented efficiently in an actual parallel environment. Using the performance statistics of these implementations, we can further fine-tune the runtime and compiler in the following respects. <p> As discussed, an obvious optimization might be to avoid creating new threads in the 1-processor case. 4 The alternative is static optimizations. The compiler may explicitly re-structure the code to reduce thread-switching and synchronization. For example, an implementation of the connected component algorithm <ref> [48] </ref> is such that for each vertex of the graph, we create a new thread. However, each iteration of the algorithm has several points where all the threads have to synchronize. This requires a large amount of thread-switching, and the performance deteriorates rapidly with large number of vertices.
Reference: [49] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference: [50] <author> Jan van den Bos and Chris Laffra. </author> <title> Procol: A concurrent object-oriented language with protocols delegation and constraints. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Leiden, </institution> <month> December 6 </month> <year> 1990. </year>
Reference: [51] <author> Katherine Anne Yelick. </author> <title> Using Abstraction in Explicitly Parallel Programs. </title> <type> PhD thesis, </type> <institution> MIT, MIT Laboratory for Computer Science, </institution> <address> Cambridge, MA 02139, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: We expect pSather to be a practical language for exploring parallel program implementation because of the following advantages: * As pointed out in <ref> [51] </ref>, abstraction mechanisms are useful in building and debugging large parallel programs.
Reference: [52] <author> Y. Yokote and M. Tokoro. </author> <title> Experience and evolution of concurrentsmalltalk. </title> <booktitle> In Proceedings of OOPSLA, </booktitle> <pages> pages 406-415, </pages> <address> Orlando, Florida, </address> <month> December </month> <year> 1987. </year> <note> ACM. </note>
Reference: [53] <editor> A. Yonezawa and M. Tokoro, editors. </editor> <booktitle> Object-Oriented Concurrent Programming. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
References-found: 53

