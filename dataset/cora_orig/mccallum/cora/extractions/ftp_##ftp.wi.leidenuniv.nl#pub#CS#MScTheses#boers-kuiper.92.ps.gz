URL: ftp://ftp.wi.leidenuniv.nl/pub/CS/MScTheses/boers-kuiper.92.ps.gz
Refering-URL: http://www.wi.LeidenUniv.nl/CS/SEIS/gain.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Biological metaphors and the design of modular artificial neural networks Master's thesis of  
Author: Egbert J.W. Boers and Herman Kuiper 
Affiliation: Departments of Computer Science and Experimental and Theoretical Psychology at Leiden University, the Netherlands  
Abstract-found: 0
Intro-found: 1
Reference: [ALLP80] <author> D.A. </author> <title> Allport; `Pattern and actions'. In: New directions in cognitive psychology, G.L. </title> <editor> Clagton (Ed.), Routledge and Kegan Paul, </editor> <address> London, </address> <year> 1980. </year>
Reference-contexts: The simultaneous execution of similar tasks (like presentation of two auditory or two visual messages), causes much more interference. The difference in performance found in these tasks can be explained by assuming a modular organization of the brain <ref> [ALLP80] </ref>. Some tasks are processed in separate modules and do not interfere. Other tasks require simultaneous processing in single modules and are thus harder to execute in parallel.
Reference: [CREU77] <editor> O.D. </editor> <booktitle> Creutzfeldt; `Generality of the functional structure of the neocortex'. In: </booktitle> <volume> Naturwissenschaften 64, </volume> <pages> 507-517, </pages> <year> 1977. </year>
Reference-contexts: For example, modules containing between 90 and 150 neurons, also known as minicolumns, have been proposed as the basic functional and anatomical modular units of the cerebral cortex (e.g. [MOUN75], [SZEN77], [ECCL81]). These modules are thought to cooperate in the execution of cortical functions <ref> [CREU77] </ref>. The question may now be raised at what moment in the development of the brain this modularity arises. An important argument supporting the view that part of the structure is in some way pre-wired, is the anatomical location of different functional regions in the brain.
Reference: [DAWK86] <author> R. </author> <title> Dawkins; The blind watchmaker, </title> <publisher> Longman, </publisher> <year> 1986. </year> <title> Reprinted with appendix by Penguin, </title> <address> London, </address> <year> 1991. </year>
Reference-contexts: It is as simple as that.' Monty Python on a disease called hooping cough `We animals are the most complicated things in the known universe.' This quote from the biologist Richard Dawkins <ref> [DAWK86] </ref> (p.1) assents the complexity of the problems seen in biology. The brain is perhaps the most complex organ of an animal, particularly in humans. Discovering how the brain works, and how it is able to be intelligent, is probably the most challenging task ever. <p> This genetic information is not a blueprint of that final form, but can be seen as a recipe. For example, in his excellent book "The blind watchmaker", Richard Dawkins <ref> [DAWK86] </ref> describes how this `recipe' is followed not by the organism as a whole, but by each cell individually. The shape and behaviour of a cell depend on the genes from which information is extracted. <p> It suggests that iterative modularity is a very common principle in nature. In the process of evolution it has time and again been profitable for all kinds of species to duplicate that which already has been `invented' once before. An example of this is found in <ref> [DAWK86] </ref>: The middle part of the body of a snake is composed of a number of segments. Each segment consists of a vertebra, a set of nerves, a set of blood vessels, a set of muscles etcetera. <p> In other words: instead of a blueprint the genes contain a kind of recipe <ref> [DAWK86] </ref>. This means that there is no one-to-one correspondence between a part of the DNA and a part of the actual organism. It allows the genetic search to utilize already discovered principles repeatedly. Not the resulting organisms, but the recipes that built them are combined in evolution.
Reference: [DENK87] <author> J.S. Denker, D.B. Schwartz, B.S. Wittner, S.A. Solla, R.E. Howard, L.D. Jackel and J.J. </author> <title> Hopfield; `Large automatic learning, rule extraction and generalization'. In: </title> <journal> Complex systems, </journal> <volume> 1, </volume> <pages> 877-922, </pages> <year> 1987. </year>
Reference-contexts: A useful measure of the diversity of possible mappings f that can be implemented with the chosen architecture is the entropy <ref> [DENK87] </ref> S 0 P f ln P f of the a priory probability distribution.
Reference: [DODD90] <author> N. Dodd, </author> <title> `Optimization of network structure using genetic algorithms'. </title> <booktitle> In: Proceedings of the International Neural Network Conference, INNC-90-Paris, </booktitle> <pages> 693-696, </pages> <editor> B. Widrow and B. Angeniol (Eds.), </editor> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1990. </year>
Reference-contexts: The genes of the genetic algorithm now contain a coding for the topology of the network, specifying which connections are present. The weights of the network have to be trained as usual (see for example <ref> [DODD90] </ref>). Of course also combinations of the two methods are possible: coding the presence of connections as well as their weights in the genes.
Reference: [FREE91] <author> J.A. Freeman and D.M. </author> <title> Skapura; Neural networks: algorithms, applications and programming techniques. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1991. </year>
Reference-contexts: The explanation is restricted to a minimum, the interested reader can find a more extensive treatment in [RUME86], or in <ref> [FREE91] </ref>. The derivation will be given for a simple three-layer network, but can easily be generalized for networks with more layers. Several other artificial neural network classes are treated in [FREE91], which offers a good general introduction into the field of neural networks. <p> The explanation is restricted to a minimum, the interested reader can find a more extensive treatment in [RUME86], or in <ref> [FREE91] </ref>. The derivation will be given for a simple three-layer network, but can easily be generalized for networks with more layers. Several other artificial neural network classes are treated in [FREE91], which offers a good general introduction into the field of neural networks. The error for an output node during training is , with the desired output, andd j y j o j y j o j the actual output for the jth output node. <p> Both have the same form as the delta rule, which is used for training networks without hidden layer (see e.g. <ref> [FREE91] </ref>). 86 A Derivation of backpropagation B Entropy of neural networks All throughout this study we claimed and showed examples of the fact that the initial structure given to a neural network greatly determines its performance. This appendix gives a more mathematical foundation to these claims.
Reference: [GARI90] <author> H. </author> <title> De Garis; `Brain building with GenNets'. </title> <booktitle> In: Proceedings of the International Neural Network Conference, INNC-90-Paris, </booktitle> <pages> 1036-1039, </pages> <editor> B. Widrow and B. Angeniol (Eds.), </editor> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1990. </year>
Reference-contexts: In order to keep the algorithm from becoming a simple random search, mutation rate has to be low, so it doesn't interfere too much with crossover and inversion. There are some applications however, where selection and mutation are enough for the GA to function (e.g. <ref> [GARI90] </ref>). 24 3 Genetic Algorithms Building blocks So far, it may not be clear how and why those simple genetic operators combine into such a powerful and robust search method. Or, as Goldberg [GOLD89] (p.28) describes it: `The operation of genetic algorithms is remarkably straightforward. <p> The genes of the algorithm have a one-to-one correspondence to the weights of the network. A slight variation of this method is to use the genetic algorithm to find a set of reasonably good weights, leaving the fine tuning to a learning algorithm (see for example [WHIT89B] and <ref> [GARI90] </ref>). Use the genetic algorithm to find the structure of a network. With this method the genetic algorithm tries to find the optimal structure of a network, instead of the weights of a given structure.
Reference: [GAZZ89] <author> M.S. </author> <title> Gazzaniga; `Organization of the human brain'. In: </title> <journal> Science, </journal> <volume> 245, </volume> <pages> 947-952. </pages>
Reference-contexts: At a smaller scale the brain is divided in a number of functional areas, for example the visual area, auditory area, and separated sensory and motor areas, and so on. Between these areas too, only a relatively small number of connections exist. Gazzaniga <ref> [GAZZ89] </ref> describes a patient who was not able to name the colour of red fruit, after suffering from a head injury. The patient was able to name the colour of every other object presented to him, including other red objects. But when presented with red fruit, the answers where random.
Reference: [GOLD89] <author> D.E. </author> <title> Goldberg; Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1989. </year>
Reference-contexts: Recently, search methods have been developed which can handle such multiple constraint problems. One of them, genetic algorithms, will be treated in this chapter. Genetic algorithms, introduced by John Holland [HOLL75], are based on the biological metaphor of evolution. In his recent book, David Goldberg <ref> [GOLD89] </ref> (p.1) describes genetic algorithms as `... search algorithms based on the mechanics of natural selection and natural genetics [resulting in] a search algorithm with some of the innovative flair of human search.' Overview Goldberg mentions the following differences between GAs (genetic algorithms) and more traditional search algorithms: 1. <p> Selection is used to choose strings from the population for reproduction. In parallel with the natural selection mechanism, strings (solutions) with a high fitness are more likely to be selected than less fit strings. The two selection methods applied in this research are described respectively by Goldberg <ref> [GOLD89] </ref> and Whitley [WHIT89A]. With roulette wheel selection [GOLD89], strings are selected with a probability proportional to their fitness. Selection 23 Another method is called rank based selection [WHIT89A], where the chance of being selected is defined as a linear function of the rank of an individual in the population. <p> In parallel with the natural selection mechanism, strings (solutions) with a high fitness are more likely to be selected than less fit strings. The two selection methods applied in this research are described respectively by Goldberg <ref> [GOLD89] </ref> and Whitley [WHIT89A]. With roulette wheel selection [GOLD89], strings are selected with a probability proportional to their fitness. Selection 23 Another method is called rank based selection [WHIT89A], where the chance of being selected is defined as a linear function of the rank of an individual in the population. <p> Or, as Goldberg <ref> [GOLD89] </ref> (p.28) describes it: `The operation of genetic algorithms is remarkably straightforward. <p> Especially schemata with high 26 3 Genetic Algorithms fitness and a short defining length are propagated exponentially throughout the population (the Schema Theorem, <ref> [GOLD89] </ref>). Those short schemata are called building blocks. Crossover directs the genetic search towards finding building blocks (or partial solutions) and also combines them into better overall solutions (the building block hypothesis, [GOLD89]). Inversion also facilitates the formation of building blocks. <p> 3 Genetic Algorithms fitness and a short defining length are propagated exponentially throughout the population (the Schema Theorem, <ref> [GOLD89] </ref>). Those short schemata are called building blocks. Crossover directs the genetic search towards finding building blocks (or partial solutions) and also combines them into better overall solutions (the building block hypothesis, [GOLD89]). Inversion also facilitates the formation of building blocks. Complex problems often consist of multiple parameters which are coded by different genes on the chromosome. With these multiple parameter problems however, complex relations may exist between different parameters. <p> Implicit parallelism The exponential propagation of high fit, small size schemata (building blocks) goes on in parallel, without any more special bookkeeping or memory than a population of n strings. Goldberg <ref> [GOLD89] </ref> presents a more precise count of how many schemata are processed usefully in each generation: the number turns out to be roughly n 3 . <p> Applications The genetic algorithms described, and many variations, are still a active topic of research. However, they are used in many applications already, and in this paragraph a few are mentioned. Also one application is described in more detail as an example. Goldberg's book <ref> [GOLD89] </ref> contains a table with an overview of the history of genetic algorithms. Below is an extract from that table, which shows the diversity of problems where genetic algorithms have been applied. The table shown here is far from complete, and new applications are found continuously. <p> The library can also be used with normal C programs, if they are compiled with a C++ compiler. Main program Two variations of the main program were written: one implementing the roulette wheel selection described by Goldberg <ref> [GOLD89] </ref>, and one implementing rank based selection and one-at-a-time replacement, as described by Whitley [WHIT89A]. Both programs first read a simulation file, which contains all the necessary parameters. This file is an ASCII file containing lines starting with the # symbol, followed by a keyword.
Reference: [GUYT86] <author> A.C. </author> <title> Guyton; Textbook of medical physiology. </title> <publisher> Saunders, </publisher> <address> Philadelphia, </address> <year> 1986. </year> <note> 92 References </note>
Reference-contexts: An important argument supporting the view that part of the structure is in some way pre-wired, is the anatomical location of different functional regions in the brain. The location of most functions are situated at the same place for almost all individuals, see e.g. <ref> [GUYT86] </ref>. Another argument to support this view is that most stages in the development of language in children are the same, independent of the language learned. Furthermore all the basic grammar rules are the same all over the world. <p> These proteins will also repress another group of genes and once the positive feedback has started, it will never stop, so the repressed group of genes will never be active again. Embryological experiments show also that certain cells in an embryo control the differentiation of adjacent cells <ref> [GUYT86] </ref>, hereby implementing the idea of context. Most mature cells in humans only produce about 8000 to 10,000 proteins rather than the total amount of 30,000. It is this process of cell differentiation that determines the final shape of the organism in all its detail.
Reference: [HAPP92] <author> B.L.M. </author> <title> Happel; Architecture and function of neural networks: designing modular architectures. </title> <booktitle> In prep., </booktitle> <year> 1992. </year>
Reference-contexts: Within each hemisphere individual functions are again organized anatomically into separate regions. Analysis of behavioral functions indicate that even the most complex functions of the brain can be localized to some extent [KAND85]. For example 1 Parts of this paragraph are adapted from <ref> [HAPP92] </ref>. It should be noted that the ideas of this paragraph are not universally accepted by all researchers. Modularity in the brain 37 studies of localized brain damage reveal that isolated, mental abilities can be lost as a result of local lesions leaving other abilities unimpaired. <p> But because nature does not code for blueprints, a research project trying to construct a method that enables the coding of recipes in genes was started <ref> [HAPP92] </ref>. Using graph grammars as recipes A neural network, when seen as a collection of nodes and edges, is a graph. So what was needed to code neural network structures was a method of graph generation. <p> The method takes genetic, cytological and physiological observations 42 5 The search for modularity into account as well as purely morphological ones [LIND68]. Since the biological metaphor is one of the mainstays of this research, taking L-systems for our recipes was an obvious choice (the original idea coming from <ref> [HAPP92] </ref>). Besides, L-systems offer the possibility of describing highly modular structures. They are very good at describing (giving a recipe of) both iterative and differentiating modularity, and are often used to describe fractals. Other graph grammars were shortly investigated, but they seemed to lack the flexibility offered with L-systems. <p> The network is trained with one half of the stimuli, the other half was used to test network generalization capabilities. The data was originally used as a test for network design principles using genetic algorithms and CALM networks <ref> [HAPP92] </ref> (for a short introduction to CALM, see inset). One of the networks found Short introduction to CALM. (For a more complete description, the reader is referred to [MURR92]). <p> It proved to be too simple a problem for backpropagation because our method found a network without a hidden layer (25 input and 10 output nodes) that classified 97% of the test set correctly 1 For this experiment training and test data from <ref> [HAPP92] </ref> were used. 70 8 Experiments (and 100% of the training set). However, one of the advantages of the CALM network is the ability to learn new patterns without interfering too much with the earlier learned patterns. `Where' and `what' categorization ten digit recognition.
Reference: [HARP89] <author> S.A. Harp, T. Samad and A. </author> <title> Guha; `Toward the genetic synthesis of neural networks'. </title> <booktitle> In: Proceedings of the 3rd International Conference on Genetic Algorithms and their applications (ICGA), </booktitle> <pages> 360-369, </pages> <editor> J.D. Schaffer (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1989. </year>
Reference: [HECH90] <author> R. </author> <title> Hecht-Nielsen; Neurocomputing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1990. </year>
Reference-contexts: Some areas where neural networks are successfully used are (for a more exhaustive overview see e.g. <ref> [HECH90] </ref>): handwritten character recognition, image compression, noise filtering, broomstick balancing, automobile autopilot, nuclear power-plant control, loan application scoring, speech processing, medical diagnosis, but this list is far from complete, and new applications seem to appear every day. <p> A network with just one hidden layer can compute any function that a network with 2, or even more, hidden layers can compute: with an exponential number of hidden nodes, one node could be assigned to every possible input pattern (see e.g. <ref> [HECH90] </ref>). However, learning is sometimes much faster with multiple hidden layers, especially if the input is highly nonlinear, in other words, hard to separate with a series of straight lines.
Reference: [HEEM91] <author> J.N.H. Heemskerk and J.M.J. Murre; `Neurocomputers: </author> <title> parallelle machines voor neurale netwerken'. In: </title> <journal> Informatie, </journal> <pages> 33-6, 365-464, </pages> <year> 1991. </year>
Reference-contexts: It should be clear that it is quite impossible to fully connect each neuron with each other neuron. It has been estimated that a full connectivity would result in a head with a 10 kilometre diameter, because of the huge amount of wiring <ref> [HEEM91] </ref>. In order to reduce the amount of connections, the brain is divided in different modules at several levels. The clearest division of the brain is the division in a right and a left half, which function to a large extent independently.
Reference: [HOFS79] <author> D.R. Hofstadter; Gdel, Escher, Bach: </author> <title> an eternal golden braid. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Gdel's theorem suggests that there may be `ideas' which can not be understood by the brain, assuming the brain can be described as a formal system... (see e.g. <ref> [HOFS79] </ref> and [PENR89]) 6 2 Neural Networks Artificial intelligence The artificial intelligence community has for a long time been trying to imitate intelligent behaviour with computer programs.
Reference: [HOGE74] <author> P. Hogeweg and B. </author> <title> Hesper; `A model study on biomorphological description'. In: </title> <journal> Pattern Recognition, </journal> <volume> 6, </volume> <pages> 165-179, </pages> <year> 1974. </year>
Reference-contexts: During context matching, the geometric symbols (-, + and F) are ignored. During the drawing of the string, the 1s and 0s are ignored. The production rules were constructed by Hogeweg and Hesper <ref> [HOGE74] </ref>, along with 3583 other patterns generated by bracketed 2L-systems. Implementation Przemyslaw Prusinkiewicz and James Hanan [PRUS89] present a small L-system program for the Macintosh (in C). In order to experiment with L-systems, we have ported the source code to work on PC's.
Reference: [HOLL68] <author> J.H. </author> <title> Holland; Hierarchical descriptions of universal spaces and adaptive systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Harbor, </address> <year> 1968. </year>
Reference-contexts: This idea of using similar (small) parts of highly fit strings to create a new string can be explained more precisely using the concepts of schemata and building blocks. A schema (introduced by John Holland, <ref> [HOLL68] </ref> and [HOLL75]) is a template describing a subset of strings with similarities at certain string positions. If we take for example a population of binary strings, schemata for these strings are strings themselves, consisting of 0, 1 and * symbols.
Reference: [HOLL75] <author> J.H. </author> <title> Holland; Adaptation in natural and artificial systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Harbor, </address> <year> 1975. </year>
Reference-contexts: Recently, search methods have been developed which can handle such multiple constraint problems. One of them, genetic algorithms, will be treated in this chapter. Genetic algorithms, introduced by John Holland <ref> [HOLL75] </ref>, are based on the biological metaphor of evolution. <p> This idea of using similar (small) parts of highly fit strings to create a new string can be explained more precisely using the concepts of schemata and building blocks. A schema (introduced by John Holland, [HOLL68] and <ref> [HOLL75] </ref>) is a template describing a subset of strings with similarities at certain string positions. If we take for example a population of binary strings, schemata for these strings are strings themselves, consisting of 0, 1 and * symbols.
Reference: [HOOG91] <author> R.J.W. </author> <title> van Hoogstraten; A neural network for genetic facies recognition. </title> <type> Unpublished student report, </type> <institution> Leiden, </institution> <year> 1991. </year>
Reference-contexts: Mapping problem The last problem tried was more of a problem for standard backpropagation networks with one or no hidden layer. The original problem was one of the experiments done by Van Hoogstraten <ref> [HOOG91] </ref> in order investigate the influence of the structure of the network upon its ability to map functions. In the experiment, he created a two-dimensional classification problem with an input space of [0,1) 2 .
Reference: [HUBE62] <author> D.H. </author> <title> Hubel and T.N. Wiesel; `Receptive fields, binocular interaction and functional architecture in the cat's visual cortex'. In: </title> <journal> Journal of physiology, </journal> <volume> 160, </volume> <pages> 106-154. </pages>
Reference-contexts: Such multi-stage information processing can for example be identified in the primary visual system, where simple features of visual images as lines and arcs are represented in layers of simple neurons which are combined and represented by neurons in subsequent layers that possess increasingly complex representational properties (e.g. <ref> [HUBE62] </ref>). Apart from horizontal structure or a layered structure, there exist at all levels in the primate brain multiple parallel processing pathways that constitute a vertical structuring (e.g. [LIVI88], [ZEKI88]). Vertical structure allows for the separate processing of different kinds of information.
Reference: [JERI85] <author> H.J. </author> <title> Jerison; `Issues in brain evolution'. In: </title> <journal> Oxford surveys in evolutionary biology, </journal> <volume> 2, </volume> <pages> 102-134, </pages> <editor> R. Dawkins and M. Ridley (Eds.), </editor> <year> 1985. </year>
Reference-contexts: This `size' has of course to be corrected for the size of the animal. The brain of an elephant is much larger, for example. (See <ref> [JERI85] </ref> for a measure of brain sizes: the encephalization quotient.) The cortex contains, as mentioned, a high amount of repeating modules. The only thing that was necessary, as was the case with the segments of the snake, were genes saying `more of this, please'.
Reference: [KAND85] <author> E.R. Kandel and J.H. </author> <title> Schwartz; Principles of neuroscience. </title> <publisher> Elsevier, </publisher> <address> New York. </address>
Reference-contexts: Within each hemisphere individual functions are again organized anatomically into separate regions. Analysis of behavioral functions indicate that even the most complex functions of the brain can be localized to some extent <ref> [KAND85] </ref>. For example 1 Parts of this paragraph are adapted from [HAPP92]. It should be noted that the ideas of this paragraph are not universally accepted by all researchers.
Reference: [KITA90] <author> H. </author> <title> Kitano; `Designing neural networks using genetic algorithms with graph generation system'. In: </title> <journal> Complex Systems, </journal> <volume> 4, </volume> <pages> 461-476, </pages> <address> Champaign, IL, </address> <year> 1990. </year>
Reference-contexts: The purpose of training is to confine the configuration space to the region , thus eliminating all ambiguity about thew Q f (w) 1 input-output mapping implemented by the trained output. Addendum At the time this thesis was almost finished, we found a reference to an article by Kitano <ref> [KITA90] </ref> with the title: "Designing neural networks using genetic algorithms with graph generation system". This addendum gives a summary of the method used by Kitano and highlights the main differences between the method he proposed and the one proposed in this thesis. <p> In the second part, which is not changed during the genetic search, for 16 predetermined non-terminals a matrix consisting of 1s and 0s is pre-encoded. For 90 Addendum a more precise description of the production rules used, as well as the rewriting method itself, the reader is referred to <ref> [KITA90] </ref>. With the method proposed by Kitano, the final size of the network is determined directly by the number of rewriting steps. <p> We believe that, since context plays an important role in biological growth, there should be context incorporated in any method that is supposed to be biological plausible (and that is exactly what Kitano tried to do). The results from <ref> [KITA90] </ref> do indicate his method works and that the method has less scaling problems than can be expected with so-called blueprint methods. However, to our knowledge, the method proposed by Kitano has not been used by many others.
Reference: [KOCH05] <author> H. </author> <title> von Koch; `Une mthode gometrique lmentaire pour l'tude de certaines questions de la thorie des courbes planes'. In: </title> <journal> Acta mathematica, </journal> <volume> 30, </volume> <year> 1905. </year>
Reference-contexts: Proposed by Helge von Koch in 1905 <ref> [KOCH05] </ref>, it shocked mathematicians: a curve where each part, however small, had an infinite length! Mandelbrot restates its construction as follows: `One begins with two shapes, an initiator and a generator. a. The initiator b. The generator c. The generator rewritten Koch-graph.
Reference: [LIND68] <author> A. </author> <title> Lindenmayer; `Mathematical models for cellular interaction in development, parts I and II'. In: </title> <journal> Journal of theoretical biology, </journal> <volume> 18, </volume> <pages> 280-315, </pages> <year> 1968. </year>
Reference-contexts: As can be seen in figure 2c, a fractal is very self similar. Simple L-systems A special class of fractals are called L-systems and were introduced in 1968 by Aristid Lindenmayer <ref> [LIND68] </ref> in an attempt to model the biological growth of plants. An L-system is a parallel string rewriting mechanism, a kind of grammar. A grammar consists of a starting string and a set of production rules. <p> The method takes genetic, cytological and physiological observations 42 5 The search for modularity into account as well as purely morphological ones <ref> [LIND68] </ref>. Since the biological metaphor is one of the mainstays of this research, taking L-systems for our recipes was an obvious choice (the original idea coming from [HAPP92]). Besides, L-systems offer the possibility of describing highly modular structures.
Reference: [LIVI88] <author> M. Livingstone and D. </author> <title> Hubel; `Segregation of form, color, movement and depth: anatomy, physiology and perception'. In: </title> <journal> Science, </journal> <volume> 240, </volume> <pages> 740-749, </pages> <year> 1988. </year>
Reference-contexts: Apart from horizontal structure or a layered structure, there exist at all levels in the primate brain multiple parallel processing pathways that constitute a vertical structuring (e.g. <ref> [LIVI88] </ref>, [ZEKI88]). Vertical structure allows for the separate processing of different kinds of information. <p> A good example can, once more, be found in the visual system, where different aspects of visual stimuli like form, colour, motion and place, are processed in parallel by anatomically separate, neural systems, organized in the magno cellular and parvo cellular pathways (e.g. <ref> [LIVI88] </ref>, [HILZ89]). Convergent structures integrate this separately processed visual information at higher hierarchical levels to produce a unitary percept (e.g. [ZEKI88], [POGG88], [YOE88]). The presence of both horizontal and vertical structure leads to a modular organization of the brain [MURR92]. <p> Rueckl et al. conducted these experiments in an attempt to explain why in the natural visual system what and where are processed by separate cortical structures (e.g. <ref> [LIVI88] </ref>). They trained a number of different networks with 25 input and 18 output nodes, and one hidden layer of 18 nodes. The 18 output nodes were separated in two groups of 9: one group for encoding the form, one group for encoding the place.
Reference: [MAND82] <author> B.B. </author> <title> Mandelbrot; The fractal geometry of nature. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: Therefore the development is solely governed by the local interactions between elements that obey the same global rules. Such a principle lies also at the basis of a mathematical system called fractals. Fractals Fractals have been made popular by Benoit Mandelbrot with his book "The fractal geometry of nature" <ref> [MAND82] </ref>. Before describing one of the oldest examples of a fractal, the Koch-graph, let's take a look at an experiment by meteorologist Lewis Richardson.
Reference: [MARI90] <author> B. Maricic and Z. </author> <title> Nikolow; `GENNET Systems for computer aided neural network design using genetic algorithms'. </title> <booktitle> In: Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington DC, </address> <month> 1, </month> <pages> 102-105, </pages> <year> 1990. </year>
Reference: [MARS74] <author> J.C. </author> <title> Marshall and F.J. </title> <journal> Newcombe; Journal of physiology, </journal> <volume> 2, 175, </volume> <year> 1974. </year>
Reference-contexts: Within the field of psychology, for example, word-matching experiments show that the human information processing system uses a number of subsequent levels of encoding in performing lexical tasks [POSN86], <ref> [MARS74] </ref>. In an experiment by Marshall & Newcombe [MARS74], subjects had to decide if two simultaneously presented words did, or did not belong to the same category. Reaction times for visually identical words like TABLE-TABLE were shorter than for visually different words like TABLE-table. <p> Within the field of psychology, for example, word-matching experiments show that the human information processing system uses a number of subsequent levels of encoding in performing lexical tasks [POSN86], <ref> [MARS74] </ref>. In an experiment by Marshall & Newcombe [MARS74], subjects had to decide if two simultaneously presented words did, or did not belong to the same category. Reaction times for visually identical words like TABLE-TABLE were shorter than for visually different words like TABLE-table.
Reference: [MINS69] <author> M. Minsky and S. </author> <title> Papert; Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference-contexts: A simple example: exclusive OR We will now give an example of a small problem: learning the logic exclusive OR function. This problem is of some historical interest because in "Perceptrons" Marvin Minsky and Seymour Papert <ref> [MINS69] </ref> showed that it was not possible to make a network for this problem without a hidden layer (for which no learning rule was known at the time). The trivial proof of this was generalized and expanded by them. <p> Exclusive OR (XOR) The XOR function (see also chapter 2) is a boolean (logical) function of two variables: f (0,0) = 0 f (1,0) = 1 It was proven in 1969 that a network able to solve this problem should have a hidden layer <ref> [MINS69] </ref>. Some of the `standard' solutions of networks able to solve XOR are shown in figure 1.
Reference: [MOUN75] <author> V.B. </author> <title> Mountcastle; `An organizing principle for cerebral function: the unit module and the distributed system'. In: The mindful brain, </title> <editor> G.M. Edelman, V.B. Mountcastle (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1975. </year>
Reference-contexts: For example, modules containing between 90 and 150 neurons, also known as minicolumns, have been proposed as the basic functional and anatomical modular units of the cerebral cortex (e.g. <ref> [MOUN75] </ref>, [SZEN77], [ECCL81]). These modules are thought to cooperate in the execution of cortical functions [CREU77]. The question may now be raised at what moment in the development of the brain this modularity arises.
Reference: [MURR92] <author> J.M.J. </author> <title> Murre; Categorization and learning in neural networks. Modelling and implementation in a modular framework. </title> <type> Dissertation, </type> <institution> Leiden University, </institution> <year> 1992. </year> <month> 93 </month>
Reference-contexts: At the department of Experimental and Theoretical Psychology at the Leiden University, where this research took place, a library was written (in C) to create and manipulate populations of binary strings either using Goldberg or Whitley selection and replacement ([HAPP92] and <ref> [MURR92] </ref>). <p> Convergent structures integrate this separately processed visual information at higher hierarchical levels to produce a unitary percept (e.g. [ZEKI88], [POGG88], [YOE88]). The presence of both horizontal and vertical structure leads to a modular organization of the brain <ref> [MURR92] </ref>. The subdivision of the brain into two hemispheres, as already mentioned in chapter 2, illustrates anatomical modularity at a very large scale. Functionally, this division is paralleled by hemispheric specialization. Whole groups of mental functions are allocated to different halves of the brain. <p> The data was originally used as a test for network design principles using genetic algorithms and CALM networks [HAPP92] (for a short introduction to CALM, see inset). One of the networks found Short introduction to CALM. (For a more complete description, the reader is referred to <ref> [MURR92] </ref>). The CALM model (Categorizing And Learning Module) [MURR92] was proposed as a candidate for the incorporation of modular information processing principles in artificial neural networks. CALM is a modular network algorithm that has been especially developed as a functional building block for large modular neural networks. <p> One of the networks found Short introduction to CALM. (For a more complete description, the reader is referred to <ref> [MURR92] </ref>). The CALM model (Categorizing And Learning Module) [MURR92] was proposed as a candidate for the incorporation of modular information processing principles in artificial neural networks. CALM is a modular network algorithm that has been especially developed as a functional building block for large modular neural networks.
Reference: [ORGE73] <author> L.E. </author> <title> Orgel; The origins of life. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973 </year>
Reference-contexts: This can very well be seen as a local optimum in evolution. This last idea gives rise to a comparison with the `primeval soup' theories (see e.g. <ref> [ORGE73] </ref>). The atmosphere of the earth at the time when there was no life on earth, contained no oxygen but plenty of hydrogen and water, carbon dioxide, and very likely some ammonia, methane and other simple organic gases.
Reference: [PARK85] <author> D.B. </author> <title> Parker; Learning logic. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Backpropagation There are a lot of different neural network paradigms, of which backpropagation probably is the best known. It was formalized first by Werbos [WERB74] and later by Parker <ref> [PARK85] </ref> and 10 2 Neural Networks Rumelhart and McClelland [RUME86]. It is a multi-layer feedforward network that is trained by supervised learning. A standard backpropagation network consists of 3 layers, an input, an output and a hidden layer.
Reference: [PENR89] <author> R. </author> <title> Penrose; The emperor's new mind. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Gdel's theorem suggests that there may be `ideas' which can not be understood by the brain, assuming the brain can be described as a formal system... (see e.g. [HOFS79] and <ref> [PENR89] </ref>) 6 2 Neural Networks Artificial intelligence The artificial intelligence community has for a long time been trying to imitate intelligent behaviour with computer programs. This is not an easy task because a computer program must be able to do many different things in order to be called intelligent.
Reference: [PHAF91] <author> R.H. </author> <title> Phaf; Learning in natural and connectionist systems: experiments and a model. </title> <type> Unpublished dissertation, </type> <institution> Leiden University, Leiden, </institution> <year> 1991. </year>
Reference: [POSN72] <author> M.I. Posner, J. Lewis, C. </author> <title> Conrad; In: Language by ear and by eye, 159-192, </title> <editor> J.F. Kavanaugh and I.G. Mattingly (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1972. </year>
Reference-contexts: Reaction times for visually identical words like TABLE-TABLE were shorter than for visually different words like TABLE-table. Even longer reaction times are found for words that belong to a different category like TABLE-DOG. Posner, Lewis & Conrad <ref> [POSN72] </ref> explain these data as follows: Visually identical words can be compared on the level of the visual encoding of words. Matching of visually different words can take place when also a phonological code has been formed.
Reference: [POSN86] <author> M.I. </author> <title> Posner; Chronometric explorations of mind. </title> <publisher> Oxford University Press, </publisher> <year> 1986. </year>
Reference-contexts: Within the field of psychology, for example, word-matching experiments show that the human information processing system uses a number of subsequent levels of encoding in performing lexical tasks <ref> [POSN86] </ref>, [MARS74]. In an experiment by Marshall & Newcombe [MARS74], subjects had to decide if two simultaneously presented words did, or did not belong to the same category. Reaction times for visually identical words like TABLE-TABLE were shorter than for visually different words like TABLE-table.
Reference: [POSN88] <author> M.I. Posner, S.E. Peterson, P.T. Fox and M.E. </author> <title> Raichle; `Localization of cognitive operations in the human brain'. In: </title> <journal> Science, </journal> <volume> 240, </volume> <pages> 1627-1631, </pages> <year> 1988. </year>
Reference-contexts: These studies indicate that in lexical tasks a separate visual, phonological and semantic encoding analysis of words is involved. PET-scan (positron emission tomography) studies show that these functionally distinguished subprocesses are also separately localized in the brain <ref> [POSN88] </ref>. Local changes in nerve cell metabolism corresponding to changes in neuronal activity, were registered in subjects during the execution of different lexical tasks. These experiments revealed that visual encoding of words takes place mainly in the occipital lobe in the posterior part of the brain.
Reference: [PRUS89] <author> P. Prusinkiewicz and J. </author> <title> Hanan; Lindenmayer systems, fractals and plants. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: During context matching, the geometric symbols (-, + and F) are ignored. During the drawing of the string, the 1s and 0s are ignored. The production rules were constructed by Hogeweg and Hesper [HOGE74], along with 3583 other patterns generated by bracketed 2L-systems. Implementation Przemyslaw Prusinkiewicz and James Hanan <ref> [PRUS89] </ref> present a small L-system program for the Macintosh (in C). In order to experiment with L-systems, we have ported the source code to work on PC's. <p> Two features were added: probabilistic production rules and production rule ranges (both from <ref> [PRUS89] </ref>). With probabilistic production rules, more than one production rule for the same L, P and R can be given, each with a fixed probability. When rewriting a string, one of the rules is selected at random, proportional to its probability.
Reference: [PRUS90] <author> P. Prunsikiewicz and A. </author> <title> Lindenmayer; The algorithmic beauty of plants. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: With these new symbols, more realistic drawings can be obtained, as shown in figures 5 and 6. Both these L-systems, and the others used in this chapter, are from the book "The Algorithmic Beauty of Plants", by Prusinkiewicz and Lindenmayer <ref> [PRUS90] </ref>. For figure 5 the X symbols are ignored during the drawing of the string. Context-sensitive L-systems A final extension of L-systems, called context, is needed to model information exchange between neighbouring cells, as described in the first paragraph. Context also leads to more natural looking 32 4 L-systems plants.
Reference: [RUEC89] <author> J.G. Rueckl, K.R. </author> <title> Cave and S.M. Kosslyn; `Why are `what' and `where' processed by separate cortical visual systems? A computational investigation'. In: </title> <journal> Journal of cognitive neuroscience, </journal> <volume> 1, </volume> <pages> 171-186, </pages> <year> 1989. </year>
Reference-contexts: An example of such interference between more classifications is the recognition of both position and shape of an input pattern (see <ref> [RUEC89] </ref>). Rueckle et al. conducted a number of simulations in which they trained a three layer backpropagation network with 25 input nodes, 18 hidden nodes and 18 output nodes to simultaneously process form and place of the input pattern. <p> However, one of the advantages of the CALM network is the ability to learn new patterns without interfering too much with the earlier learned patterns. `Where' and `what' categorization ten digit recognition. Another problem we tried was proposed by Rueckl et al. <ref> [RUEC89] </ref> where, like the TC problem, a number of 3x3 patterns had to be recognized on a larger grid. With this problem, there are 9 patterns (see figure 7), which are placed on a 5x5 grid.
Reference: [RUME86] <editor> D.E. Rumelhart and J.L McClelland (Eds.); </editor> <booktitle> Parallel distributed processing. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Backpropagation There are a lot of different neural network paradigms, of which backpropagation probably is the best known. It was formalized first by Werbos [WERB74] and later by Parker [PARK85] and 10 2 Neural Networks Rumelhart and McClelland <ref> [RUME86] </ref>. It is a multi-layer feedforward network that is trained by supervised learning. A standard backpropagation network consists of 3 layers, an input, an output and a hidden layer. <p> As an example of how much depends on the structure of XOR network. figure 12 versus a and b. the network, remember the XOR network shown in figure 5. The figure showed one possible solution, but on many trials, the network did not converge. Rumelhart and McClelland <ref> [RUME86] </ref> describe several occasions where the network of figure 5 got stuck in a local minimum. By changing the network topology we found a network (figure 12) that, for as many times as we have tested it, always learned the XOR problem. <p> TC problem With the TC problem, a neural network should be able to recognize the letters T and C in a 4x4 input grid of 4x4. grid (see also <ref> [RUME86] </ref>). Each letter, consisting of 3x3 pixels, can be rotated 0, 90, 180 or 270 and can be anywhere on the 4x4 grid. The total number of input patterns is therefore 32 (there are 4 positions to put the 3x3 grid). <p> The explanation is restricted to a minimum, the interested reader can find a more extensive treatment in <ref> [RUME86] </ref>, or in [FREE91]. The derivation will be given for a simple three-layer network, but can easily be generalized for networks with more layers. Several other artificial neural network classes are treated in [FREE91], which offers a good general introduction into the field of neural networks.
Reference: [SCHW88] <author> J.T. </author> <title> Schwartz; `The new connectionism: developing relationships between neuroscience and artificial intelligence', </title> <year> 1988. </year>
Reference-contexts: Combined with the total number of axons, this results in roughly 10 13 bits per second. This is an estimation of just the data transmission in the brain: the amount of computation is even more staggering. Jacob Schwartz <ref> [SCHW88] </ref> estimates the total amount of arithmetic operations needed to simulate the brain in every detail as high as 10 18 per second, needing 10 16 bytes of memory. This is probably a million times as fast as the fastest supercomputer available in the next decade.
Reference: [SOLL89] <author> S.A. </author> <title> Solla; `Learning and generalization in layered neural networks: the contiguity problem'. In: Neural networks: from models to applications, 168-177, </title> <editor> L. Personnas and G. Dreyfus (Eds.), I.D.S.E.T, </editor> <address> Paris, </address> <year> 1989. </year>
Reference-contexts: This appendix gives a more mathematical foundation to these claims. It is partly based on <ref> [SOLL89] </ref>. Learning as entropy reduction A neural network, when trained, is performing an input-output mapping. The mapping that is implemented by the network depends on the architecture of the network and its weights. When the architecture is fixed, the mapping of network is solely determined by the weights. <p> Given a network architecture and its corresponding weight space W (the set of all possible weight settings w), Sara Solla <ref> [SOLL89] </ref> defines the a priori probability of the network for a mapping f as ,P f W f /W W where equals the total volume of the allowed weight space, andW W W f W W is the volume in the initial weight space that implements the desired mapping with Q
Reference: [SZEN75] <author> J. </author> <title> Szentagothai; `The neural network of the cerebral cortex: a functional interpretation'. In: </title> <journal> Proceedings of the Royal Society of London, B, </journal> <volume> 201, </volume> <pages> 219-248, </pages> <year> 1975. </year>
Reference-contexts: CALM is a modular network algorithm that has been especially developed as a functional building block for large modular neural networks. A number of neurophysiological constraints are implemented in CALM by using the general architecture of the neocortical mini-columns <ref> [SZEN75] </ref> as a basic design principle. These modules contain about 100 neurons, and an important feature of these modules is the division in excitatory pyramidal cells, which form long ranging connections to other cortical regions, and various types of short range, within module, inhibitory interneurons.
Reference: [SZEN77] <author> J. </author> <title> Szentagothai; `The `module-concept' in the cerebral cortex architecture'. In: </title> <journal> Brain Research, </journal> <volume> 95, </volume> <pages> 475-496, </pages> <year> 1977. </year>
Reference-contexts: For example, modules containing between 90 and 150 neurons, also known as minicolumns, have been proposed as the basic functional and anatomical modular units of the cerebral cortex (e.g. [MOUN75], <ref> [SZEN77] </ref>, [ECCL81]). These modules are thought to cooperate in the execution of cortical functions [CREU77]. The question may now be raised at what moment in the development of the brain this modularity arises. <p> How modularity is coded In addition to the already mentioned minicolumns, the cortex apparently contains, at a higher level, yet another form of modular organization. So called macro modules consist of an aggregation of a few hundred minicolumns, forming a larger processing module. According to Szentagothai <ref> [SZEN77] </ref>: `the cerebral cortex has to be envisaged as a mosaic of columnar units of remarkable similar internal structure and surprisingly little variation of diameter'.
Reference: [SZIL79] <author> A.L. Szilard and R.E. </author> <title> Quinton; `An interpretation for D0L-systems by computer graphics'. In: </title> <journal> The Science Terrapin, </journal> <volume> 4, </volume> <pages> 8-13, </pages> <year> 1979. </year>
Reference-contexts: Whereas in other grammars production rules are applied one-by-one sequentially, in an L-system all characters in a string are rewritten in parallel to form a new string. When we attach a specific meaning (based on a LOGO-style turtle, <ref> [SZIL79] </ref>) to the characters in a string, we are able to visualize the string. The Koch-graph from figure 1, for example, can be described with the following L-system: Simple L-systems 31 axiom F and production rule F fi F-F++F-F.
Reference: [TURI63] <author> A. </author> <title> Turing; `Computing machinery and intelligence'. In: Computers and thought, E.A. </title> <editor> Feigenbaum and J. Feldman (Eds.), </editor> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: In a restricted sense, the study of techniques to use computers more effectively by improved programming techniques. Allan Turing has proposed a test that should be satisfied in order to speak of artificial intelligence. In this test, known as the Turing Test <ref> [TURI63] </ref>, a person, say Q, is placed in a room with a terminal connected to two other sites. At one of the two terminals a person is situated and at the other a computer. By asking questions, Q must determine at which of the two terminals the computer is situated.
Reference: [WARR82] <author> E.K. </author> <title> Warrington; The fractionation of arithmetical skills: a single case study. In: </title> <journal> Quarterly journal of experimental psychology: human experimental psychology, </journal> <volume> 34, </volume> <pages> A(1), 31-51, </pages> <year> 1982. </year>
Reference-contexts: Warrington described a subject with a severe impairment of arithmetic skills that was not accompanied by a deficit in other cognitive abilities <ref> [WARR82] </ref>. Also, different types of aphasia (language disorders) indicate that different functions are separately localized in the brain. Patients with Wernicke's aphasia are not able to understand written or spoken language while they can speak and write unintelligibly but fluently. Broca's aphasia shows the reverse symptoms.
Reference: [WERB74] <author> P.J. </author> <title> Werbos; Beyond regression: new tools for prediction and analysis in the behavioral sciences. </title> <type> Unpublished Ph.D. thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1974. </year>
Reference-contexts: Backpropagation There are a lot of different neural network paradigms, of which backpropagation probably is the best known. It was formalized first by Werbos <ref> [WERB74] </ref> and later by Parker [PARK85] and 10 2 Neural Networks Rumelhart and McClelland [RUME86]. It is a multi-layer feedforward network that is trained by supervised learning. A standard backpropagation network consists of 3 layers, an input, an output and a hidden layer.
Reference: [WHIT89A] <author> D. </author> <title> Whitley; `The GENITOR algorithm and selection pressure: why rank-based allocation of reproductive trials is best'. </title> <booktitle> In: Proceedings of the 3rd International Conference on Genetic Algorithms and their applications (ICGA), </booktitle> <pages> 116-121, </pages> <editor> J.D. Schaffer (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1989. </year> <note> 94 References </note>
Reference-contexts: In parallel with the natural selection mechanism, strings (solutions) with a high fitness are more likely to be selected than less fit strings. The two selection methods applied in this research are described respectively by Goldberg [GOLD89] and Whitley <ref> [WHIT89A] </ref>. With roulette wheel selection [GOLD89], strings are selected with a probability proportional to their fitness. Selection 23 Another method is called rank based selection [WHIT89A], where the chance of being selected is defined as a linear function of the rank of an individual in the population. <p> The two selection methods applied in this research are described respectively by Goldberg [GOLD89] and Whitley <ref> [WHIT89A] </ref>. With roulette wheel selection [GOLD89], strings are selected with a probability proportional to their fitness. Selection 23 Another method is called rank based selection [WHIT89A], where the chance of being selected is defined as a linear function of the rank of an individual in the population. The population must remain sorted by fitness for this method to work. <p> Some changes to the standard algorithm proposed by W h i t l e y a r e d e s c r i b e d i n <ref> [WHIT89A] </ref>. His program, called GENITOR, besides using rank based selection, also uses one-at-a-time selection and replacement: a new solution replaces the worst member of the population when the new solution has a higher fitness. <p> E.g. with a pressure of 2.0, the top ranking member in a population of 100 members has a twice as large probability of being selected than number 50. This function is the same as proposed by Whitley <ref> [WHIT89A] </ref> and accepts pressure values between 1.0 and 2.0. For rank based selection to work, the population must be sorted. <p> Main program Two variations of the main program were written: one implementing the roulette wheel selection described by Goldberg [GOLD89], and one implementing rank based selection and one-at-a-time replacement, as described by Whitley <ref> [WHIT89A] </ref>. Both programs first read a simulation file, which contains all the necessary parameters. This file is an ASCII file containing lines starting with the # symbol, followed by a keyword. Parameters are separated by spaces and follow the keyword. <p> Prune extraneous input nodes. With the SuperSnoei pruning method, only extraneous output nodes and chain ares removed. However, extraneous input nodes can also be removed from the network. The genetic algorithm Calculate the fitness of the initial population. While using the algorithm based on Whitley <ref> [WHIT89A] </ref>, we initialized the population with random chromosomes. The fitness of all randomly determined initial members were however not evaluated, and were set to zero. This might have caused a direct loss of useful information that happened to be present in the random initialization.
Reference: [WHIT89B] <author> D. Whitley and T. </author> <title> Hanson; `Towards the genetic synthesis of neural networks'. </title> <booktitle> In: Proceedings of the 3rd International Conference on Genetic Algorithms and their applications (ICGA), </booktitle> <pages> 391-396, </pages> <editor> J.D. Schaffer (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1989. </year>
Reference-contexts: The genes of the algorithm have a one-to-one correspondence to the weights of the network. A slight variation of this method is to use the genetic algorithm to find a set of reasonably good weights, leaving the fine tuning to a learning algorithm (see for example <ref> [WHIT89B] </ref> and [GARI90]). Use the genetic algorithm to find the structure of a network. With this method the genetic algorithm tries to find the optimal structure of a network, instead of the weights of a given structure.
Reference: [ZEKI88] <author> S. Zeki and S. </author> <title> Shipp; `The functional logic of cortical connections'. In: </title> <journal> Nature, </journal> <volume> 335, </volume> <pages> 311-317, </pages> <year> 1988. </year>
Reference-contexts: Apart from horizontal structure or a layered structure, there exist at all levels in the primate brain multiple parallel processing pathways that constitute a vertical structuring (e.g. [LIVI88], <ref> [ZEKI88] </ref>). Vertical structure allows for the separate processing of different kinds of information. <p> Convergent structures integrate this separately processed visual information at higher hierarchical levels to produce a unitary percept (e.g. <ref> [ZEKI88] </ref>, [POGG88], [YOE88]). The presence of both horizontal and vertical structure leads to a modular organization of the brain [MURR92]. The subdivision of the brain into two hemispheres, as already mentioned in chapter 2, illustrates anatomical modularity at a very large scale. Functionally, this division is paralleled by hemispheric specialization.
References-found: 54

