URL: http://www.research.microsoft.com/lampson/52-CoherentMemory\Postscript.ps
Refering-URL: http://www.research.microsoft.com/lampson/Publications.html
Root-URL: http://www.research.microsoft.com
Title: Chapter 1 Implementing Coherent Memory  
Author: Butler W. Lampson 
Abstract: In the design of a shared-memory multiprocessor, there is a conflict between the need for a coherent memory in which every write done by one processor is immediately visible to all the others, and the fact that a memory read or write that uses only a cache local to the processor can be done more quickly than one that communicates with a global memory or another processor. Coherent memory is good because we know how to program with it; the incoherent memory that results from minimizing communication is good because it is fast. In this paper we show how to write precise specifications for coherent and incoherent memory, and how to implement coherent memory in several ways, one of which is on top of incoherent memory. Our technique for showing the correctness of the implementations is the abstraction function introduced by Hoare [8] to handle abstract data types. A decade later, Lamport [1] and Lynch [10] extended Hoare's methods to concurrent systems like the ones we treat. We begin by giving a careful specification for the coherent memory S that we really want; it is just a function from addresses to data values. We also specify an incoherent memory T that has fast implementations. After a brief explanation of what it means to implement a specification and how to prove the correctness of an implementation using abstraction functions, we explore how to change T so that it implements coherent memory with as little communication as possible. Our first step is a simple idealized implementation U derived from T by strengthening the guards. Unfortunately U is extremely non-local and therefore impractical. We describe two ways to make U local enough to be practical. Both are based on the idea of using locks on memory locations. First we show how to use reader/writer locks to get a practical version of U called a coherent cache. We do this in two stages: an ideal cache B and a concrete cache C. The cache changes the guards on internal actions of T as well as on the external read and write actions, so it can't be implemented simply by adding a test before each read or write of T, but instead requires changes to the insides of T. We complete our treatment of caches by sketching several implementations of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abadi, M. and Lamport, L., </author> <title> The existence of refinement mappings, </title> <booktitle> Theoretical Computer Science 82 (2), </booktitle> <year> 1991, </year> <pages> 253-284. </pages>
Reference-contexts: Implementing Coherent Memory 7 1.3 Implementations Having seen the specs for coherent and incoherent memory, we are ready to study some implementations. We begin with a precise statement of what it means for an implementation Y to satisfy a specification X <ref> [1, 10] </ref>. X and Y are state machines. We partition their actions into external and internal actions. <p> Roughly speaking, a safety property is an assertion that nothing bad happens; it is a generalization of the notion of partial correctness for sequential programs. Specifications may also include liveness properties, which roughly assert that something good eventually happens; these generalize the notion of termination for sequential programs <ref> [1] </ref>. Liveness is beyond the scope of this paper. 8 Butler W. Lampson sequence of Y-actions will be the same externally as the corresponding sequence of X-actions.
Reference: [2] <author> Archibald, J. and Baer, J-L., </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model, </title> <journal> ACM Trans. </journal> <note> Computer Systems 4 (4), </note> <month> Nov. </month> <year> 1986, </year> <pages> 273-298. </pages>
Reference-contexts: To establish f ree ^ only p , we need: * a way to test it, and * a way to progress toward it by suitable Release p and Drop p operations. There are three general approaches to solving these problems. All three have been used in commercial multiprocessors <ref> [2, 7] </ref>. 1. Directory, usually in conjunction with a switch-based processor-memory in terconnect [11]: * Keep centrally a set fp : live p _ lock p g for each address or set of addresses or "cache block".
Reference: [3] <institution> Digital Equipment Corporation, Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference-contexts: An external Barrier p action waits until an address is not live; this ensures that the value written by any earlier W rite has been copied to ^m and that any later Read sees a value from ^m. There are commercial machines whose memory systems have essentially this specification <ref> [3] </ref>. Others have explored similar specifications [4, 5]. 1 in ^m and in three processors a, b, and c. A new value is marked with a *, and circles mark values that have changed. <p> The implementation E below shows how to use T to obtain critical sections. 1.2.1 Specifying legal histories directly It's common in the literature to write the specifications S and T explicitly in terms of legal sequences of references at each processor, rather than as state machines <ref> [3, 4] </ref>. We digress briefly to explain this approach. <p> The effect is that once the W riteConditional succeeds, the entire sequence is an atomic read-modify-write from the viewpoint of another processor <ref> [3] </ref>. Of course these operations also incur communication costs, at least if the address a is shared. We have shown that a program that touches shared memory only inside a critical section cannot distinguish memory that satisfies T from memory that satisfies the serial specification S.
Reference: [4] <author> Gharachorloo, K., et al., </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors, </title> <booktitle> Proc. 17th Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: There are commercial machines whose memory systems have essentially this specification [3]. Others have explored similar specifications <ref> [4, 5] </ref>. 1 in ^m and in three processors a, b, and c. A new value is marked with a *, and circles mark values that have changed. <p> The implementation E below shows how to use T to obtain critical sections. 1.2.1 Specifying legal histories directly It's common in the literature to write the specifications S and T explicitly in terms of legal sequences of references at each processor, rather than as state machines <ref> [3, 4] </ref>. We digress briefly to explain this approach.
Reference: [5] <author> Gibbons, P. and Merritt, M., </author> <title> Specifying nonblocking shared memories, </title> <booktitle> Proc. 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> 158-168. </pages>
Reference-contexts: There are commercial machines whose memory systems have essentially this specification [3]. Others have explored similar specifications <ref> [4, 5] </ref>. 1 in ^m and in three processors a, b, and c. A new value is marked with a *, and circles mark values that have changed.
Reference: [6] <author> Goodman, J., </author> <title> Using cache memory to reduce processor-memory traffic. </title> <booktitle> Proc. 10th Symposium on Computer Architecture, </booktitle> <year> 1983, </year> <pages> 124-131. </pages>
Reference-contexts: Usually this directory information is kept with the ^m data. * Ask each p that holds data or a lock to give it up (by doing Drop p or Release p ) in order to ensure progress. 2. Snoopy, usually in conjunction with a bus-based processor-memory intercon nect <ref> [6] </ref>: * If you don't hold the necessary lock, broadcast a request for progress to all processors. * Each processor q responds with the value of lock q ; "or" all the responses, often using a "wired-or" electrical signalling scheme. 3.
Reference: [7] <author> Hennessy, J. and Patterson, D., </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufann, </publisher> <year> 1990. </year>
Reference-contexts: To establish f ree ^ only p , we need: * a way to test it, and * a way to progress toward it by suitable Release p and Drop p operations. There are three general approaches to solving these problems. All three have been used in commercial multiprocessors <ref> [2, 7] </ref>. 1. Directory, usually in conjunction with a switch-based processor-memory in terconnect [11]: * Keep centrally a set fp : live p _ lock p g for each address or set of addresses or "cache block".
Reference: [8] <author> Hoare, C. A. R., </author> <title> Proof of Correctness of Data Representation, </title> <journal> Acta Informatica 4, </journal> <year> 1972, </year> <pages> 271-281. </pages>
Reference: [9] <author> Lamport, L., </author> <title> A simple approach to specifying concurrent systems, </title> <journal> Communications of the ACM, </journal> <volume> 32 (1), </volume> <year> 1989, </year> <pages> 32-47. </pages>
Reference-contexts: 1.0.1 Notation We write our specifications and implementations using state machines <ref> [9] </ref>. As usual, a state machine consists of * a state space, which we represent by the values of a set of named variables, * a set of initial states, and * a set of transitions or actions.
Reference: [10] <author> Lynch, N. and Tuttle, M., </author> <title> Hierarchical correctness proofs for distributed algorithms, </title> <booktitle> Proc. ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> 137-151. </pages>
Reference-contexts: Implementing Coherent Memory 7 1.3 Implementations Having seen the specs for coherent and incoherent memory, we are ready to study some implementations. We begin with a precise statement of what it means for an implementation Y to satisfy a specification X <ref> [1, 10] </ref>. X and Y are state machines. We partition their actions into external and internal actions.
Reference: [11] <author> Tang, C., </author> <title> Cache system design in the tightly coupled multiprocessor system. </title> <booktitle> Proc. AFIPS National Computer Conference, </booktitle> <year> 1976, </year> <pages> 749-753. 16 </pages>
Reference-contexts: There are three general approaches to solving these problems. All three have been used in commercial multiprocessors [2, 7]. 1. Directory, usually in conjunction with a switch-based processor-memory in terconnect <ref> [11] </ref>: * Keep centrally a set fp : live p _ lock p g for each address or set of addresses or "cache block".
References-found: 11

