URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/clearning.uncompressed.ps
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  georg.zimmermann@zfe.siemens.de  
Author: P. Refenes, Y. Abu-Mostafa, J. Moody, and A. S. Weigend, p. Clearning Andreas S. Weigend Hans Georg Zimmermann Ralph Neuneier Otto Hahn 
Address: Boulder, CO 80309-0430, USA  SN 4  D-81739 Munchen, Germany  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Siemens AG, ZFE T  Ring 6  
Web: URL: ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/clearning.ps  
Note: In: Neural Networks in Financial Engineering (1996) (Proceedings of NNCM'95, London). Edited by  511-522. Singapore: World Scientific.  
Abstract: Most connectionist modeling assumes noise-free inputs. This assumption is often violated. This paper introduces the idea of clearning, of simultaneously cleaning the data and learning the underlying structure. The cleaning step can be viewed as top-down processing (where the model modifies the data), and the learning step can be viewed as bottom-up processing (where the data modifies the model). Clearning is used in conjunction with standard pruning. This paper discusses the statistical foundation of clearning, gives an interpretation in terms of a mechanical model, describes how to obtain both point predictions and conditional densities for the output, and shows how the resulting model can be used to discover properties of the data otherwise not accessible (such as the signal-to-noise ratio of the inputs). This paper uses clearning to predict foreign exchange rates, a noisy time series problem with well-known benchmark performances. On the out-of-sample 1993-1994 test period, clearning obtains an annualized return on investment above 30%, significantly better than an otherwise identical network. The final ultra-sparse network with 36 remaining non-zero input-to-hidden weights (of the 1035 initial weights between 69 inputs and 15 hidden units) is very robust against overfitting. This small network also lends itself to interpretation.
Abstract-found: 1
Intro-found: 1
Reference: [Abu-Mostafa, 1995] <author> Abu-Mostafa, Y. </author> <year> (1995). </year> <title> Hints. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 639-671. </pages>
Reference-contexts: The relative weight given to the output units can be adjusted during learning. A principled way of scheduling additional tasks has been developed it the following framework of hints. * Hints. <ref> [Abu-Mostafa, 1995] </ref> gives the example of the symmetry-hint for predicting foreign exchange data. This hint corresponds to viewing exchange rate returns first from one country, then from the other country. The hint suggests that the dynamics should be the same.
Reference: [Buntine and Weigend, 1991] <author> Buntine, W. L. and Weigend, A. S. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 603-643. </pages>
Reference-contexts: the training set, and allows a simpler decision structure to match the cleaned data. 3 Clearning can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables [Seber and Wild, 1989], to methods dealing with missing data <ref> [Buntine and Weigend, 1991, Tresp et al., 1994] </ref>, and to bounded influence models in econometrics. 2 prevents the network from overfitting by moving each input pattern to a more likely location, based on information from the corresponding target value and the model.
Reference: [Cottrell et al., 1995] <author> Cottrell, M., Girard, B., Girard, Y., Mangeas, M., and Muller, C. </author> <year> (1995). </year> <title> Neural modeling for time series: a statistical stepwise method for weight elimination. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 6 </volume> <pages> 1355-1364. </pages>
Reference-contexts: Pruning limits the ability of the network to memorize the training data without introducing a bias towards linear models <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref>. * Add tasks. In order to reduce overfitting and stabilize learning on very noisy data, we bestow supplementary tasks upon the network in the form of additional output units with their own targets [Weigend et al., 1992]. <p> We repeat this procedure several hundred or a few thousand times, and present tomorrow's probability distribution as a histogram of these predictions. 6 This section has introduced clearning. It is used in conjunction with pruning. The next section discusses pruning <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref>. 3 Pruning The previous section discussed how to use the evolving model to modify the input data. In the introductory section, we had already discussed some standard methods of regularization in neural networks.
Reference: [Finnoff et al., 1993] <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving generalization performance by nonconvergent model selection methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: Pruning limits the ability of the network to memorize the training data without introducing a bias towards linear models <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref>. * Add tasks. In order to reduce overfitting and stabilize learning on very noisy data, we bestow supplementary tasks upon the network in the form of additional output units with their own targets [Weigend et al., 1992]. <p> We repeat this procedure several hundred or a few thousand times, and present tomorrow's probability distribution as a histogram of these predictions. 6 This section has introduced clearning. It is used in conjunction with pruning. The next section discusses pruning <ref> [Finnoff et al., 1993, Cottrell et al., 1995] </ref>. 3 Pruning The previous section discussed how to use the evolving model to modify the input data. In the introductory section, we had already discussed some standard methods of regularization in neural networks.
Reference: [Huffel and Vanderwalle, 1991] <author> Huffel, S. V. and Vanderwalle, J. </author> <year> (1991). </year> <title> The total least squares problem: Computational aspects and analysis. </title> <booktitle> In Frontiers in Applied Mathematics, </booktitle> <volume> volume 9. </volume> <publisher> SIAM. </publisher>
Reference-contexts: noise to the inputs at each training iteration in pseudo-data, clearning patterns on the training set closer to their centers, reducing the overlap on the training set, and allows a simpler decision structure to match the cleaned data. 3 Clearning can be related to the method of total least squares <ref> [Huffel and Vanderwalle, 1991] </ref> (in the context of linear models), to error in variables [Seber and Wild, 1989], to methods dealing with missing data [Buntine and Weigend, 1991, Tresp et al., 1994], and to bounded influence models in econometrics. 2 prevents the network from overfitting by moving each input pattern to
Reference: [Nix and Weigend, 1995] <author> Nix, D. A. and Weigend, A. S. </author> <year> (1995). </year> <title> Learning local error bars for nonlinear regression. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*94), </booktitle> <pages> pages 488-496. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: to variables derived from cleaned inputs; e.g., a moving average should be replace by its cleaned value. 5 Examples are (1) estimating uncertainties due to the splitting of the data [Weigend and LeBaron, 1994]-crucial when validation sets are set aside to determine meta-parameters, (2) estimating confidence intervals for unimodal distributions <ref> [Nix and Weigend, 1995] </ref>-a connectionist implementation of ARCH models, and also useful for problems that consider Sharpe ratios, (3) obtaining model-free distributions with the method of fractional binning [Weigend and Srivastava, 1995]-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4)
Reference: [Rumelhart et al., 1995] <author> Rumelhart, D. E., Durbin, R., Golden, R., and Chauvin, Y. </author> <year> (1995). </year> <title> Backpropagation: The basic theory. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, Backpropagation: </editor> <booktitle> Theory, Architectures, and Applications, </booktitle> <pages> pages 1-34, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The ground state is reached when the sum of this input energy and the energy stored in the output spring (drawn next to in the figure) is minimal. A statistical interpretation of the cost function can be given in a maximum likelihood framework <ref> [Rumelhart et al., 1995] </ref>. We assume that each pattern was generated by a "true" input (estimated by x) and a "true" output (estimated by y). We then assume that all input and output components were independently corrupted by additive Gaussian noise.
Reference: [Seber and Wild, 1989] <author> Seber, G. A. F. and Wild, C. J. </author> <year> (1989). </year> <title> Nonlinear Regression. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: training set closer to their centers, reducing the overlap on the training set, and allows a simpler decision structure to match the cleaned data. 3 Clearning can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables <ref> [Seber and Wild, 1989] </ref>, to methods dealing with missing data [Buntine and Weigend, 1991, Tresp et al., 1994], and to bounded influence models in econometrics. 2 prevents the network from overfitting by moving each input pattern to a more likely location, based on information from the corresponding target value and the
Reference: [Tresp et al., 1994] <author> Tresp, V., Ahmad, S., and Neuneier, R. </author> <year> (1994). </year> <title> Training neural networks with deficient data. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6 (NIPS*93), </booktitle> <pages> pages 128-135, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: the training set, and allows a simpler decision structure to match the cleaned data. 3 Clearning can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables [Seber and Wild, 1989], to methods dealing with missing data <ref> [Buntine and Weigend, 1991, Tresp et al., 1994] </ref>, and to bounded influence models in econometrics. 2 prevents the network from overfitting by moving each input pattern to a more likely location, based on information from the corresponding target value and the model.
Reference: [Weigend, 1994] <author> Weigend, A. S. </author> <year> (1994). </year> <title> On overfitting and the effective number of hidden units. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 335-342, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The complexity of the model, expressed as an effective number of hidden units, gradually increases with training time (iterations of backpropagation) <ref> [Weigend, 1994] </ref>. Starting training with small weights and stopping early introduces a preference for linear models, a serious problem when the nonlinear signal is masked by noise. We always monitor the error on a validation set as function of training time.
Reference: [Weigend et al., 1990] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference-contexts: The simplest regularization weight-decay, known as ridge regression in the statistics community, penalizes weights proportionally to their squared value. It also represents a bias towards linear models. Adding a complexity term to the cost function that effectively counts the number of significantly sized weights is known as weight-elimination <ref> [Weigend et al., 1990] </ref>. * Prune weights.
Reference: [Weigend et al., 1992] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1992). </year> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In Casdagli, M. and Eubank, S., editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, </booktitle> <pages> pages 395-432. </pages> <publisher> Addison-Wesley. </publisher>
Reference-contexts: In order to reduce overfitting and stabilize learning on very noisy data, we bestow supplementary tasks upon the network in the form of additional output units with their own targets <ref> [Weigend et al., 1992] </ref>.
Reference: [Weigend and LeBaron, 1994] <author> Weigend, A. S. and LeBaron, B. </author> <year> (1994). </year> <title> Evaluating neural network predictors by bootstrapping. </title> <booktitle> In Proceedings of International Conference on Neural Information Processing (ICONIP'94), </booktitle> <pages> pages 1207-1212. </pages> <note> Technical Report CU-CS-725-94, </note> <institution> Computer Science Department, University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/bootstrap.ps. </institution>
Reference-contexts: v u N t=1 ~ t mean (~) fl 2 rms weight change (fluctuations) : (7) 4 This also applies to variables derived from cleaned inputs; e.g., a moving average should be replace by its cleaned value. 5 Examples are (1) estimating uncertainties due to the splitting of the data <ref> [Weigend and LeBaron, 1994] </ref>-crucial when validation sets are set aside to determine meta-parameters, (2) estimating confidence intervals for unimodal distributions [Nix and Weigend, 1995]-a connectionist implementation of ARCH models, and also useful for problems that consider Sharpe ratios, (3) obtaining model-free distributions with the method of fractional binning [Weigend and Srivastava,
Reference: [Weigend et al., 1995] <author> Weigend, A. S., Mangeas, M., and Srivastava, A. N. </author> <year> (1995). </year> <title> Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 373-399. </pages>
Reference-contexts: with the method of fractional binning [Weigend and Srivastava, 1995]-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4) finding trading days where we can trust our model to a higher than average degree, using the method of gated experts <ref> [Weigend et al., 1995] </ref>-important for very noisy processes where additional information both about the certainty and the noise level of the regime can be crucial. 6 If we want more resamplings than we have input patterns, the individual errors to be added to the inputs can be drawn independently, as opposed
Reference: [Weigend et al., 1991] <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The idea of adding noise to the inputs in order to prevent overfitting has been used in <ref> [Weigend et al., 1991] </ref> and compared to weight-elimination.
Reference: [Weigend and Srivastava, 1995] <author> Weigend, A. S. and Srivastava, A. N. </author> <year> (1995). </year> <title> Predicting conditional probability distributions: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 109-118. 8 </pages>
Reference-contexts: the data [Weigend and LeBaron, 1994]-crucial when validation sets are set aside to determine meta-parameters, (2) estimating confidence intervals for unimodal distributions [Nix and Weigend, 1995]-a connectionist implementation of ARCH models, and also useful for problems that consider Sharpe ratios, (3) obtaining model-free distributions with the method of fractional binning <ref> [Weigend and Srivastava, 1995] </ref>-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4) finding trading days where we can trust our model to a higher than average degree, using the method of gated experts [Weigend et al., 1995]-important for very noisy
References-found: 16

