URL: ftp://synapse.cs.byu.edu/pub/papers/rudolph_95c.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: e-mail: george@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Word Perfect Corp. A TRANSFORMATION FOR IMPLEMENTING EFFICIENT DYNAMIC BACKPROPAGATION NEURAL NETWORKS  
Author: George L. Rudolph and Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: In proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms pp. 41-44, 1995. This research is funded by grants from Novell Inc. and  
Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing distributed feedforward networks that use dynamic topologies (dynamic ANNs) efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient sup port for adding and deleting nodes dynamically during learning. In particular, this paper presents an LIT for standard Backpropagation with two layers of weights, and shows how dynamic extensions to Backpropagation can be supported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Fahlmann, Scott, C. Lebiere. </author> <booktitle> The Cascade-Correlation Learning Architechture. in Advances in Neural Information Processing 2 . pp. </booktitle> <pages> 524-532. </pages> <publisher> Morgan Kaufmann Publishers: </publisher> <address> Los Altos, CA. </address>
Reference-contexts: - comings: 1) sensitivity to usersupplied parameterslearning rate (s), momentum, etc., 2) local error minima during learning, and 3) there is no mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems <ref> [1] </ref>, [3], [4]. Early ANN hardware implementations were modelspe cific, and are intended to support only static topologies. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [2].
Reference: [2] <author> Hammerstrom, D., W. Henry, M. Kuhn. </author> <title> Neurocomputer System for Neural-Network Applications. In Parallel Digital Implementations of Neural Networks. </title> <editor> K. Przytula, V. Prasanna, Eds. </editor> <publisher> Prentice-Hall, Inc. </publisher> <year> 1991. </year>
Reference-contexts: Early ANN hardware implementations were modelspe cific, and are intended to support only static topologies. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs <ref> [2] </ref>. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not. Of course, general parallel machines, like the Connection Machine, can simulate the desired dynam ics in software, but these machines are not optimized for neural computation.
Reference: [3] <author> Odri, </author> <title> S.V., D.P. Petrovacki, G.A. Krstonosic. Evolutional Development of a Multilevel Neural Network. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #4. </volume> <pages> pp. 583-595. </pages> <publisher> Pergamon Press Ltd.: </publisher> <address> New York. </address> <year> 1993. </year>
Reference-contexts: comings: 1) sensitivity to usersupplied parameterslearning rate (s), momentum, etc., 2) local error minima during learning, and 3) there is no mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems [1], <ref> [3] </ref>, [4]. Early ANN hardware implementations were modelspe cific, and are intended to support only static topologies. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [2]. <p> Many dynamic extensions to BP have been proposed in the literature. Although any of these can be sup ported by LIT, one algorithm that allows addition and deletion of both nodes and weights <ref> [3] </ref> is chosen as an example for this paper. Section 2 discusses the mapping from an original (either standard or extended) BP net work to a transformed (LIT BP) network. <p> Weight changes for the first layer of weights are computed, also as in the original. The preceeding discussion shows how the standard BP model is supported. This paragraph describes the dynamic extension to learning. The dynamic BP algorithm of <ref> [3] </ref> allows adding and delet ing hidden nodes, but not input or output nodes (original network). Node addition is accomplished through probabilistic node division. <p> This probability is a function of the global time since the last weight was deleted in the network and how much the weight has contributed correctly to its nodes overall activity in the last epoch. The learning algorithm below includes the steps of the dynamic algorithm based on <ref> [3] </ref>, but the equations and further de tails are not included for reasons of space. More detail is avail able in [3] and [7]. 3. ALGORITHMS The following definitions and equations describe execution mode and learning for the 2 - layer LIT BP. <p> The learning algorithm below includes the steps of the dynamic algorithm based on <ref> [3] </ref>, but the equations and further de tails are not included for reasons of space. More detail is avail able in [3] and [7]. 3. ALGORITHMS The following definitions and equations describe execution mode and learning for the 2 - layer LIT BP. They are essentially the original equations, but altered to show the behavior of the LIT model. The learning algorithm also includes steps for the dynamic extension based on [3]. <p> <ref> [3] </ref> and [7]. 3. ALGORITHMS The following definitions and equations describe execution mode and learning for the 2 - layer LIT BP. They are essentially the original equations, but altered to show the behavior of the LIT model. The learning algorithm also includes steps for the dynamic extension based on [3]. The main steps of the respective algorithms are in italics.
Reference: [4] <author> Reilly, D.L., L.N. Cooper, C. Elbaum. </author> <title> Learning Systems Based on Multiple Neural Networks. (Internal paper). Nestor, </title> <publisher> Inc. </publisher> <year> 1988. </year>
Reference-contexts: 1) sensitivity to usersupplied parameterslearning rate (s), momentum, etc., 2) local error minima during learning, and 3) there is no mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems [1], [3], <ref> [4] </ref>. Early ANN hardware implementations were modelspe cific, and are intended to support only static topologies. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [2].
Reference: [5] <author> Rudolph G., and T.R. Martinez. </author> <title> An Efficient Static Topology for Modeling ASOCS. </title> <booktitle> International Conference on Artificial Neural Networks, </booktitle> <address> Helsinki, Finland. </address> <booktitle> In Artificial Neural Networks, Kohonen et al, </booktitle> <pages> pp. 279-734. </pages> <publisher> North Holland: Elsevier Publishers, </publisher> <year> 1991. </year>
Reference: [6] <author> Rudolph G., and T.R. Martinez. </author> <title> A Transformation for Implementing Localist Neural Networks. </title> <note> Submitted, </note> <year> 1994. </year>
Reference: [7] <author> Rudolph G., Martinez, T. R. </author> <title> An Efficient Transformation for Implementing Two-Layer FeedForward Neural Networks. </title> <note> To appear in the Journal of Artificial Neural Networks, </note> <year> 1995. </year>
Reference-contexts: The learning algorithm below includes the steps of the dynamic algorithm based on [3], but the equations and further de tails are not included for reasons of space. More detail is avail able in [3] and <ref> [7] </ref>. 3. ALGORITHMS The following definitions and equations describe execution mode and learning for the 2 - layer LIT BP. They are essentially the original equations, but altered to show the behavior of the LIT model. The learning algorithm also includes steps for the dynamic extension based on [3]. <p> First, only highly unstable nodes have high probabilities of dividing. Second, as training time increases and the network becomes more stable, it is less likely for any particular node to become unstable, and correspond ingly more remote that even a few nodes will simultaneously divide. 8 <ref> [7] </ref> shows that complexity of both learning and execution algorithms is O (n+p+logm) for a single pattern, where nis the number of inputs, p is the number of outputs, and m is the number of hidden nodes in the original network. 4.
Reference: [8] <author> Stout, M., G. Rudolph, T.R. Martinez, L. Salmon, </author> <title> A VLSI Implementation of a Parallel, Self-Organizing Learning Model, </title> <booktitle> Proceedings of the International Conference on Pattern Recognition (1994) 373-376. </booktitle>
Reference-contexts: Furthermore, because a nodes information is local, adding or deleting nodes from the network can be done without affecting any other nodes. Thus, location-independence allows efficient support for dynamic topologies. A prototype VLSI chip set has been fabricated as proof-of - concept of the overall LIT strategy <ref> [8] </ref>. This paper presents the transformation for standard backpropagation (BP) networks with a single hidden layer, and shows how dynamic extensions to BP can be supportedthe same basic transformation applies to networks with an arbitrary number of layers. Many dynamic extensions to BP have been proposed in the literature.
References-found: 8

