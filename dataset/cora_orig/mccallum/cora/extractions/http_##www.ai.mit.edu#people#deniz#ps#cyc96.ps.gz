URL: http://www.ai.mit.edu/people/deniz/ps/cyc96.ps.gz
Refering-URL: http://www.ai.mit.edu/people/deniz/papers.html
Root-URL: 
Email: e-mail: deniz@mit.edu  
Title: The binding roots of symbolic AI: a brief review of the Cyc project  
Author: Deniz Yuret 
Date: February 13, 1996  
Address: 545 Technology Square, Room 815 Cambridge, MA 02139, USA  
Affiliation: MIT Artificial Intelligence Laboratory  
Abstract: Cyc is a monumental but controversial research effort for codifying the human consensus knowledge initiated by Doug Lenat in 1984. The human consensus knowledge is meant to be the background knowledge that a human is assumed to possess in order to understand, for example, newspaper articles or encyclopedia entries. The methodology chosen is to enter this knowledge explicitly in a large knowledge base. A team of knowledge enterers have been actively engaged in this process since the onset of the project. It is a multi-million dollar, decade-long, two person-century effort. The engineering goal of Cyc is to overcome the brittleness of conventional application programs by letting them fall back on background knowledge. The scientific goal of Cyc is to build a system that would exhibit human level common sense and understanding. I believe that Cyc has a respectable goal but inadequate methodology. I further believe that this inadequacy comes from insisting on three limiting principles: (1) representing knowledge explicitly, (2) representing knowledge in a single uniform framework, and (3) insisting on deduction as the main inference engine. 
Abstract-found: 1
Intro-found: 1
Reference: [Davis, 1990] <author> Davis, E. </author> <year> (1990). </year> <title> Representations of commonsense knowledge. </title> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Other symbols are reserved to represent properties, relations and quantifiers. Syntactic rules are specified for constructing well formed formulae (wff). Assertions express propositions about the real world using wff's. Rules for algorithmic manipulation of these wff's for inference are determined <ref> [Davis, 1990] </ref>. There are two critical properties in such a system: that the inferences made by following the rules of symbol manipulation correspond to the external reality, and that things that are true in the real world can be inferred by symbol manipulation. These correspond to soundness and completeness respectively.
Reference: [Elkan and Greiner, 1993] <author> Elkan, C. and Greiner, R. </author> <year> (1993). </year> <title> Book Review of Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: For the first five years there was an emphasis on frame based representations and almost a reaction to the findings of the formal community (as illustrated by the "neats versus scruffies" discussion [Lenat and Guha, 1990]). As most of the reviewers of the Cyc book point out <ref> [Elkan and Greiner, 1993, Skuce, 1993] </ref>, they did not have much justification for this decision, other than the questionable claims for efficiency. In a sense, they were following history with a ten year lag, as Minsky had published his seminal work on frames in 1974 [Minsky, 1974]. <p> The basic semantics of a frame x having value v under slot p is the same as saying p (x; v) in logic. However there is a lot more that can 8 be expressed in logic that is not so easy with frames. Embedded clauses are the standard example <ref> [Elkan and Greiner, 1993] </ref>. Mayor (Capital (Texas)) cannot be expressed without making Capital (Texas) into a new frame (reifying it). Another problem comes up when handling time. To express the fact that some object had a property over a particular time period, Cyc chose to define subabstractions.
Reference: [Gazzaniga, 1985] <author> Gazzaniga, M. S. </author> <year> (1985). </year> <title> The social brain: discovering the networks of the mind. Basic Books. </title> <type> 25 </type>
Reference-contexts: How else is it possible to represent things and do inference? To illustrate the point, I will give an example from Herbert Simon, commenting on visual imagery to Gazzaniga <ref> [Gazzaniga, 1985] </ref>: "Imagine a rectangle. Draw a line from the top right-hand corner to the bottom left corner. Now draw a line from the middle of the diagonal to the bottom right corner.
Reference: [Godel, 1962] <author> Godel, K. </author> <year> (1992, </year> <title> c1962). On formally undecidable propositions of principia mathematica and related systems. </title> <publisher> Dover. </publisher>
Reference: [Guha and Lenat, 1993] <author> Guha, R. and Lenat, D. </author> <year> (1993). </year> <title> Re: Cycling paper reviews (response). </title> <journal> Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: I thought they were after constructing an infrastructure, not the building itself. Apparently, I was wrong. Here is a typical example of a Cyc axiom <ref> [Guha and Lenat, 1993] </ref> (#%ist #%LargeCorpInternalsMt (#%ForAll x (#%HumanResourcesDepartment #%allInstances) (#%actsInCapacity x #%mediatorInProcesses #%EmployeeHiring #%MainFunction))) Translated into English, it says that the human resources department of a company plays the primary role of mediating the hiring of employees.
Reference: [Guha and Lenat, 1990] <author> Guha, R. V. and Lenat, D. B. </author> <year> (1990). </year> <title> Cyc: A midterm report. </title> <journal> AI Magazine, </journal> <volume> 11(3). </volume>
Reference-contexts: The period of the project getting off the ground during 1984-1986 as depicted in [Lenat et al., 1986], the state of Cyc as it is 6 depicted in the book mostly written before 1990 [Lenat and Guha, 1990], the midterm report <ref> [Guha and Lenat, 1990] </ref> and their AI journal discussion of the methodology [Lenat and Feigenbaum, 1991, Smith, 1991], and the current state of affairs in 1996 [Lenat, 1996]. I would like to have played with the system myself, to get an objective view of the performance. <p> But every once in a while, you can see glimpses of the idea that makes you feel that this is their background hope. A related hope is their anticipation of a kind of crossover <ref> [Guha and Lenat, 1990] </ref> from manual knowledge entry to primarily automatic entry via natural language understanding. It is observed that the role of human knowledge enterers today, manually entering assertion after assertion, is akin to teachers who must instruct by surgically manipulating brains.
Reference: [Hausser, 1989] <author> Hausser, R. </author> <year> (1989). </year> <title> Computation of language: an essay on syntax, semantics and pragmatics in natural man-machine communication. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: In the following sections I will try to build an argument on pragmatic grounds. 6.1.1 A grounded understanding of red To facilitate discussion, I would like to describe a hypothetical color reader machine inspired by Hausser <ref> [Hausser, 1989] </ref>. This machine has a camera and a speaker. It can look at objects and tell what color they are. In that sense, it satisfies the basic premise of understanding given above.
Reference: [Hofstadter and Dennett, 1981] <author> Hofstadter, D. R. and Dennett, D. C. </author> <year> (1981). </year> <title> The mind's I: fantasies and reflections on self and soul. </title> <publisher> Basic Books. </publisher>
Reference-contexts: To cope with this, the field of "non-monotonic reasoning" was born. To handle further problems, frame axioms, closed world assumption, circumscription, default reasoning etc. had to be invented. 11 The immunity of humans to inconsistency is a remarkable fact. In a science fiction story by C. Cherniak <ref> [Hofstadter and Dennett, 1981] </ref>, an artificial intelligence researcher enters a state of trance in front of his computer. His friends finally notice the problem after a few days and try to wake him up. He never comes out of the trance, though, and dies within a couple of days.
Reference: [Johnson, 1987] <author> Johnson, M. </author> <year> (1987). </year> <title> The body in the mind: the bodily basis of meaning, imagination and reason. </title> <publisher> University of Chicago Press. </publisher>
Reference-contexts: The logical tradition concerns itself with the relationship between symbols (5) and objects (1) in the real world <ref> [Lakoff, 1987, Johnson, 1987] </ref>. One starts by linking symbols to objects, sets of objects, and tuples of objects. Other symbols are reserved to represent properties, relations and quantifiers. Syntactic rules are specified for constructing well formed formulae (wff). Assertions express propositions about the real world using wff's.
Reference: [Kurzweil, 1990] <author> Kurzweil, R. </author> <year> (1990). </year> <title> The age of intelligent machines. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Maybe 12 we should think of Leibniz as the founder of AI. Almost two centuries later, inspired by the writings of Leibniz, Boole developed the theory of propositional logic, and modestly called it The Laws of Thought (1854) <ref> [Kurzweil, 1990] </ref>. Frege finally layed the mature foundations of modern logic as we know it (1879). From his work it followed that arithmetic, and pure mathematics generally, is nothing but a prolongation of deductive logic. He remained without recognition until Russell drew attention to him in 1903.
Reference: [Lakoff, 1987] <author> Lakoff, G. </author> <year> (1987). </year> <title> Women, fire, and dangerous things: what categories reveal about the mind. </title> <publisher> University of Chicago Press. </publisher>
Reference-contexts: The logical tradition concerns itself with the relationship between symbols (5) and objects (1) in the real world <ref> [Lakoff, 1987, Johnson, 1987] </ref>. One starts by linking symbols to objects, sets of objects, and tuples of objects. Other symbols are reserved to represent properties, relations and quantifiers. Syntactic rules are specified for constructing well formed formulae (wff). Assertions express propositions about the real world using wff's. <p> So we have to be more careful. The model theoretic definition of meaning is a function which assigns a truth value to that sentence in each possible world. Thus it is equivalent to the set of possible worlds under which this sentence would be true. Putnam proved in 1981 <ref> [Putnam, 1981, Lakoff, 1987] </ref> that such a definition was inadequate to capture the concept of meaning. Putnam's theorem shows that one can always come up with two different sentences consisting of words with different meanings, that would be mapped to the same "meaning" according to the above definition. <p> High level concepts are defined in terms of lower level concepts. Cyc understands sliding and falling in terms of friction. We understood friction in high school in terms of our memories of sliding and falling. Lakoff characterizes this phenomenon by the term basic level category <ref> [Lakoff, 1987] </ref>. We tend to understand more abstract concepts in terms of our bodies. We also understand more primitive concepts (if you agree that friction is more primitive than sliding) in terms of our bodies.
Reference: [Lenat and Feigenbaum, 1991] <author> Lenat, D. and Feigenbaum, E. </author> <year> (1991). </year> <title> On the thresholds of knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 47. </volume>
Reference-contexts: the project getting off the ground during 1984-1986 as depicted in [Lenat et al., 1986], the state of Cyc as it is 6 depicted in the book mostly written before 1990 [Lenat and Guha, 1990], the midterm report [Guha and Lenat, 1990] and their AI journal discussion of the methodology <ref> [Lenat and Feigenbaum, 1991, Smith, 1991] </ref>, and the current state of affairs in 1996 [Lenat, 1996]. I would like to have played with the system myself, to get an objective view of the performance. However the bureaucracy has been holding down my efforts for the last four months.
Reference: [Lenat, 1996] <author> Lenat, D. B. </author> <year> (1996). </year> <title> Cycorp, </title> <publisher> inc. homepage. </publisher> <address> http://www.cyc.com/. </address>
Reference-contexts: Cyc may prove useful in commercial applications like data mining, but the goal of creating a system that would exhibit real common sense failed." Lenat, who recently founded the privately-owned company Cycorp to continue the develop ment of the Cyc project, still seems optimistic. His view is <ref> [Lenat, 1996] </ref> "The development of Cyc was a very long-term, high-risk gamble that has begun to pay off. Begun as a research project in 1984, Cyc is now a working technology with applications to many real-world business problems. <p> 1986], the state of Cyc as it is 6 depicted in the book mostly written before 1990 [Lenat and Guha, 1990], the midterm report [Guha and Lenat, 1990] and their AI journal discussion of the methodology [Lenat and Feigenbaum, 1991, Smith, 1991], and the current state of affairs in 1996 <ref> [Lenat, 1996] </ref>. I would like to have played with the system myself, to get an objective view of the performance. However the bureaucracy has been holding down my efforts for the last four months. All my observations, thus, are based on reading reviews.
Reference: [Lenat and Guha, 1990] <author> Lenat, D. B. and Guha, R. V. </author> <year> (1990). </year> <title> Building large knowledge-based systems: representation and inference in the Cyc project. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The basic idea of typing in a lot of knowledge is interesting but their knowledge representation technology seems poor." R. V. Guha, who was the co-leader of the project with Lenat for years and the co-author of the book Building large knowledge based systems <ref> [Lenat and Guha, 1990] </ref> left the team in 1994. He is quoted saying in an interview [Stipp, 1995] "We were killing ourselves trying to create a pale shadow of what had been promised. <p> I am just stating a simple fact that this small system doesn't have any understanding of the symbols it is using. What would a program be able to do, if it did have any understanding of its symbols? The Cyc book <ref> [Lenat and Guha, 1990] </ref> has many nice examples: 4 Brittleness One can imagine many examples of hypothetical situations where expert sys-tems fall into ridiculous situations like deciding that a man is pregnant, or believing that a 20 year old has been working for 22 years. <p> I will distinguish between four phases in this evolution. The period of the project getting off the ground during 1984-1986 as depicted in [Lenat et al., 1986], the state of Cyc as it is 6 depicted in the book mostly written before 1990 <ref> [Lenat and Guha, 1990] </ref>, the midterm report [Guha and Lenat, 1990] and their AI journal discussion of the methodology [Lenat and Feigenbaum, 1991, Smith, 1991], and the current state of affairs in 1996 [Lenat, 1996]. <p> For the first five years there was an emphasis on frame based representations and almost a reaction to the findings of the formal community (as illustrated by the "neats versus scruffies" discussion <ref> [Lenat and Guha, 1990] </ref>). As most of the reviewers of the Cyc book point out [Elkan and Greiner, 1993, Skuce, 1993], they did not have much justification for this decision, other than the questionable claims for efficiency. <p> the system founded by people whose most important criteria for success was consistency is not consistent with the basic facts of its domain. 6.1.3 The practical problem One thing the objectivist theory of meaning bypasses completely is how the universe gets divided up into concepts (the cosmic cookie cutter problem <ref> [Lenat and Guha, 1990] </ref>). We look at the color spectrum and divide a continuous space into discrete regions, which we call "red", "blue", etc. We look at orientations, and come up with three main concepts, "vertical", "horizontal", and "diagonal".
Reference: [Lenat et al., 1986] <author> Lenat, D. B., Prakash, M., and Shepherd, M. </author> <year> (1986). </year> <title> Cyc: using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks. </title> <journal> AI Magazine, </journal> <volume> 6(4). </volume>
Reference-contexts: This will bring interesting issues about knowledge representation in general. I will distinguish between four phases in this evolution. The period of the project getting off the ground during 1984-1986 as depicted in <ref> [Lenat et al., 1986] </ref>, the state of Cyc as it is 6 depicted in the book mostly written before 1990 [Lenat and Guha, 1990], the midterm report [Guha and Lenat, 1990] and their AI journal discussion of the methodology [Lenat and Feigenbaum, 1991, Smith, 1991], and the current state of affairs
Reference: [McAllester, 1991] <author> McAllester, D. </author> <year> (1991). </year> <title> Observations on cognitive judgements. </title> <publisher> MIT AI Memo 1340. </publisher>
Reference-contexts: We will all be able to answer a question such as "Can a lone king reach every square of an empty chess board?" after given a brief description of the rules. We will all have a hard time answering the same question for a knight <ref> [McAllester, 1991] </ref>. An argument can be made for this kind of common-sense reflecting the infrastructure for intelligence. An argument cannot be made for knowledge about large corporation internals being a necessary prerequisite for intelligence. That is static encyclopedic knowledge.
Reference: [McCarthy and Hayes, 1969] <author> McCarthy, J. and Hayes, P. J. </author> <year> (1969). </year> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In Meltzer, B. and Michie, D., editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 4. </volume> <publisher> Edinburgh University Press. </publisher>
Reference-contexts: As the knowledge base grew, so did the problem of inference efficiency. I will mention two methods they used to fight this problem. 4.3.1 Epistemological level / heuristic level distinction The importance of this concept was first pointed out twenty years earlier by McCarthy and Hayes <ref> [McCarthy and Hayes, 1969] </ref>. There are two properties that knowledge based systems must satisfy. Epistemological adequacy means that we should be able to state whatever we 9 want to state in the domain using the language of the system.
Reference: [McDermott, 1993] <author> McDermott, D. </author> <year> (1993). </year> <title> Book Review of Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: This seems like a step in the right direction. However, as I will argue below, it wasn't a sufficient step. Other reviewers <ref> [McDermott, 1993, Neches, 1993] </ref> are also skeptical about the premise that what is holding AI back is programs' lack of explicitly represented knowledge. <p> So if Mary believes that Fred is a plumber, you create a Mary subabstraction of Fred as a separate frame and assert facts about it. These are special cases of the embedding problem. In a frame language, one is not allowed to say true-over-interval (1980, occupation (Fred, plumber)) <ref> [McDermott, 1993] </ref>. So Cyc started its life having a simple frame language. By the time the book was written (1989), on top of the frames there is a predicate-calculus like constraint language. <p> McDermott, in his review, points out that a computer system cannot effectively receive a representation of a piece of knowledge without an algorithm ready to process it with reasonable efficiency <ref> [McDermott, 1993] </ref>. He also adds in a footnote that if knowledge is not represented, but merely implicit in an efficient algorithm, this requirement will not come up. 19 The imagination-perception loop also illustrates the implicit representation.
Reference: [Minsky, 1974] <author> Minsky, M. L. </author> <year> (1974). </year> <title> A framework for representing knowledge. </title> <publisher> MIT AI Memo 306. </publisher>
Reference-contexts: In a sense, they were following history with a ten year lag, as Minsky had published his seminal work on frames in 1974 <ref> [Minsky, 1974] </ref>. Frames do have their own problems. The basic semantics of a frame x having value v under slot p is the same as saying p (x; v) in logic.
Reference: [Montague, 1974] <author> Montague, R. </author> <year> (1974). </year> <title> Formal Philosophy. </title> <publisher> Yale University Press. </publisher> <pages> 26 </pages>
Reference-contexts: In fact, Frege is the first person who explicitly defended the role of symbolic formalism as a general representation to be applied to arbitrary domains. The definition of truth by Tarski [Tarski, 1935] which later led to the implementation of model-theoretic semantics in linguistics by Montague <ref> [Montague, 1974] </ref> pre-dates the Dartmouth Conference (1956). Now let's trace the thirty years of the AI experience that led to Cyc. The first era of AI focused on isolated problems and search methodologies. Soon the need to attack real world problems as opposed to investigating toy domains arose.
Reference: [Neches, 1993] <author> Neches, R. </author> <year> (1993). </year> <title> Book Review of Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: This seems like a step in the right direction. However, as I will argue below, it wasn't a sufficient step. Other reviewers <ref> [McDermott, 1993, Neches, 1993] </ref> are also skeptical about the premise that what is holding AI back is programs' lack of explicitly represented knowledge.
Reference: [Papert, 1980] <author> Papert, S. </author> <year> (1980). </year> <title> Mindstorms: children, computers, and powerful ideas. </title> <publisher> Basic Books. </publisher>
Reference-contexts: It is only natural that we build the later layers of knowledge, on top of what already exists, irrespective of whether they are higher or lower in some hierarchy. Papert's Mindstorms has wonderful examples of this structure <ref> [Papert, 1980] </ref>. Humans come to life with powerful computers and optimized representations in a number of domains. The visuo-spatial system gives us powerful tools to perceive and think about 23 the forms and positions of objects. The motor system senses and directs our posture, orien-tation and movements.
Reference: [Polya, 1954] <author> Polya, G. </author> <year> (1954). </year> <title> Mathematics and plausible reasoning. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: The standards of plausible reasoning are fluid, and there is no theory of such reasoning that could be compared to demonstrative logic in clarity or would command comparable consensus." <ref> [Polya, 1954] </ref> 6.5 The "high level" reasoning Imagine standing in front of an ice pond, wearing flat-soled shoes and thinking "what would happen if I tried to walk on this pond?". Let's ask this question to Cyc and try to visualize the steps it would take.
Reference: [Putnam, 1981] <author> Putnam, H. </author> <year> (1981). </year> <title> Reason, truth and history. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: So we have to be more careful. The model theoretic definition of meaning is a function which assigns a truth value to that sentence in each possible world. Thus it is equivalent to the set of possible worlds under which this sentence would be true. Putnam proved in 1981 <ref> [Putnam, 1981, Lakoff, 1987] </ref> that such a definition was inadequate to capture the concept of meaning. Putnam's theorem shows that one can always come up with two different sentences consisting of words with different meanings, that would be mapped to the same "meaning" according to the above definition.
Reference: [Rao, 1995a] <author> Rao, S. </author> <year> (1995a). </year> <title> Multiple representational systems. </title> <note> Unpublished paper. </note>
Reference-contexts: What does this tell us about how to organize a system with common sense? One would first have to select some primitive domains as a basis for the system <ref> [Rao, 1995a] </ref>. There are two requirements a basis domain has to satisfy to be useful: (1) It has to be expressive enough, so that other domains can be mapped on it. (2) It has to have efficient inference engines. A visual system satisfies both of the above criteria.
Reference: [Rao, 1995b] <author> Rao, S. </author> <year> (1995b). </year> <title> A visuospatial representational system. </title> <type> MIT PhD proposal. </type>
Reference-contexts: You simply used the reverse wiring in your brain to go from descriptions to sensations, and just ran your already existing perceptual machinery to look at the answer for you <ref> [Rao, 1995b] </ref>. This illustrates a powerful inference engine. To find out whether a flying bird touches ground, you might have used this machinery for a blink of a second to see that it does not. Maybe the picture was drawn for you while you were hearing the sentence.
Reference: [Rao, 1996] <author> Rao, S. </author> <year> (1996). </year> <type> Personal communication. </type>
Reference-contexts: To completely capture what is represented by a particular concept, you need a lot of pictures. This is the power of multiple representational systems. One cannot substitute for the other. What you can say in a few bits in one, require infinite space in the other, and vice versa <ref> [Rao, 1996] </ref>. 6.4 Plausible reasoning In fact, as you might have suspected already, there is a bug with our scheme of inference using imagery. And this bug stems from the realization of the previous section, that we cannot substitute different representations for one another.
Reference: [Russell, 1945] <author> Russell, B. </author> <year> (1945). </year> <title> A history of western philosophy. </title> <publisher> Simon and Schuster. </publisher>
Reference-contexts: If controversies were to arise, there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down to their slates, and to say to each other: Let us calculate." <ref> [Russell, 1945] </ref> With a little imagination we can replace the people with pencils with computers. Maybe 12 we should think of Leibniz as the founder of AI.
Reference: [Searle, 1980] <author> Searle, J. </author> <year> (1980). </year> <title> Minds, brains, and programs. </title> <journal> Behavioral and brain sciences, </journal> <volume> 3. </volume>
Reference-contexts: Am I repeating the Chinese room argument? <ref> [Searle, 1980] </ref> No. I am just stating a simple fact that this small system doesn't have any understanding of the symbols it is using.
Reference: [Skuce, 1993] <author> Skuce, D. </author> <year> (1993). </year> <title> Book Review of Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: For the first five years there was an emphasis on frame based representations and almost a reaction to the findings of the formal community (as illustrated by the "neats versus scruffies" discussion [Lenat and Guha, 1990]). As most of the reviewers of the Cyc book point out <ref> [Elkan and Greiner, 1993, Skuce, 1993] </ref>, they did not have much justification for this decision, other than the questionable claims for efficiency. In a sense, they were following history with a ten year lag, as Minsky had published his seminal work on frames in 1974 [Minsky, 1974].
Reference: [Smith, 1991] <author> Smith, B. C. </author> <year> (1991). </year> <title> The owl and the electric encylopedia. </title> <journal> Artificial Intelligence, </journal> <volume> 47. </volume>
Reference-contexts: the project getting off the ground during 1984-1986 as depicted in [Lenat et al., 1986], the state of Cyc as it is 6 depicted in the book mostly written before 1990 [Lenat and Guha, 1990], the midterm report [Guha and Lenat, 1990] and their AI journal discussion of the methodology <ref> [Lenat and Feigenbaum, 1991, Smith, 1991] </ref>, and the current state of affairs in 1996 [Lenat, 1996]. I would like to have played with the system myself, to get an objective view of the performance. However the bureaucracy has been holding down my efforts for the last four months. <p> At the conclusion it is revealed that the first victim actually had discovered the Godel sentence for humans. Thank God, we are not created as deductive systems. Tuttle and Smith discuss various ways in which human thought can be different from logical automatons <ref> [Tuttle, 1993, Smith, 1991] </ref>.
Reference: [Sowa, 1993] <author> Sowa, J. F. </author> <year> (1993). </year> <title> Book Review of Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: The representations they manipulate are still symbolic representations, albeit expressed in a more convenient data structure. The inference they support is still deductive inference. 4.3.2 Microtheories An orthogonal direction for modularity is using microtheories. A microtheory is an internally consistent collection of facts about a particular domain <ref> [Sowa, 1993] </ref>. For example the system can have a naive theory of physics, in addition to a more formal theory. They would be useful in different contexts. However they talk about the same concepts, and may include incompatible facts.
Reference: [Stevenson, 1996] <author> Stevenson, D. C. </author> <year> (1996). </year> <note> The tech classics archive. http://the-tech.mit.edu/Classics/. </note>
Reference-contexts: Although the rules set down weren't as clearcut as the modern logicians would like them to be, a collection of regularities was discovered for making logical arguments <ref> [Stevenson, 1996] </ref>. Leibniz (1646-1716), is the founder of mathematical logic. Unfortunately he abstained from publishing his results, because he kept on finding evidence that Aristotle's doctrine of syllogism was wrong on some points, and respect for Aristotle made it impossible for him to believe this.
Reference: [Stipp, 1995] <author> Stipp, D. </author> <year> (1995). </year> <title> 2001 is just around the corner. Where's Hal? Fortune. </title>
Reference-contexts: V. Guha, who was the co-leader of the project with Lenat for years and the co-author of the book Building large knowledge based systems [Lenat and Guha, 1990] left the team in 1994. He is quoted saying in an interview <ref> [Stipp, 1995] </ref> "We were killing ourselves trying to create a pale shadow of what had been promised. <p> Large scale work can be cumulative. Even if Cyc is built on a flawed methodology, maybe one day the machine with the right methodology can read its knowledge base like a book, to learn/verify its own knowledge. I happen to agree with Lenat's complaint <ref> [Stipp, 1995] </ref> "... Hal-like computers would fundamentally change the whole nature of existence. But AI is filled with little bump-on-a-log projects that couldn't possibly lead there." The next section is going to describe the goal of Cyc and provide evidence for why it is a useful goal.
Reference: [Tarski, 1935] <author> Tarski, R. </author> <year> (1935). </year> <title> Der wahrheitsbegriff in den formalisierten sprachen. </title> <journal> Studia Philosophica, </journal> <volume> 1. </volume>
Reference-contexts: It should also be noted that this was not an original hypothesis. In fact, Frege is the first person who explicitly defended the role of symbolic formalism as a general representation to be applied to arbitrary domains. The definition of truth by Tarski <ref> [Tarski, 1935] </ref> which later led to the implementation of model-theoretic semantics in linguistics by Montague [Montague, 1974] pre-dates the Dartmouth Conference (1956). Now let's trace the thirty years of the AI experience that led to Cyc. The first era of AI focused on isolated problems and search methodologies.
Reference: [Tuttle, 1993] <author> Tuttle, M. S. </author> <year> (1993). </year> <title> Book Review of Representations of Commonsense Knowledge (E. Davis) and Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (D.B. </title> <journal> Lenat and R.V. Guha). Artificial Intelligence, </journal> <volume> 61. </volume>
Reference-contexts: At the conclusion it is revealed that the first victim actually had discovered the Godel sentence for humans. Thank God, we are not created as deductive systems. Tuttle and Smith discuss various ways in which human thought can be different from logical automatons <ref> [Tuttle, 1993, Smith, 1991] </ref>.
Reference: [Whitten, 1996] <author> Whitten, D. </author> <year> (1996). </year> <note> The unofficial, unauthorized cyc frequently asked questions information sheet. http://www.cais.com/wcmac/cyc/cyc-faq.text. </note>
Reference-contexts: As a rough estimate of the current magnitude of knowledge in Cyc, there are more than 400,000 significant assertions of which less than 30,000 are rules for inference. There are over 500 microtheories defined <ref> [Whitten, 1996] </ref>. The size of the knowledge base has fluctuated over the years, in particular it has decreased in size when axioms have been generalized. In 10 the midterm report (1990) Cyc was reported to have over a million assertions.
Reference: [Winston, 1992] <author> Winston, P. H. </author> <year> (1992). </year> <note> Artificial Intelligence. Addison-Wesley, 3rd edition. 27 </note>
Reference-contexts: The last section gives a short summary and concludes with a list of principles for building the next Cyc. 2 The problem statement of Cyc Consider a toy deduction system, e.g. Zookeeper from <ref> [Winston, 1992] </ref>. It has rules such as the following: If ?x Flies ?x LaysEggs then ?x is a Bird It is a trivial fact that such systems don't know the meaning of the symbols that make up their sentences.
References-found: 38

