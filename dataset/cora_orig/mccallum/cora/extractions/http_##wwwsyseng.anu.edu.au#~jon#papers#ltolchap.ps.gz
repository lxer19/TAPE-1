URL: http://wwwsyseng.anu.edu.au/~jon/papers/ltolchap.ps.gz
Refering-URL: 
Root-URL: 
Title: Theoretical Models of Learning to Learn  Editor:  
Author: JONATHAN BAXTER 
Keyword: Learning to Learn, Bias Learning, Empirical Processes, Hierarchical Bayes  
Address: Canberra 0200 Australia  
Affiliation: Department of Systems Engineering Research School of Information Science and Engineering Australian National University  
Note: 1-24 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: jon@syseng.anu.edu.au  
Date: Received May 1, 1991  
Abstract: A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [4, 6]. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Martin Anthony and Norman Biggs. </author> <title> Computational Learning Theory. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: The conditions ensuring convergence between ^er z (h) and er P (h) are by now well understood; in the case Boolean function learning (Y = f0; 1g), convergence is controlled by VCdim (H)|the VC-dimension of H (see e.g. <ref> [1, 12] </ref>). <p> Suppose the loss function l: Y fi Y ! R has range <ref> [0; 1] </ref> (any bounded loss function can be rescaled so this 11 is true). Let H = fHg be any hypothesis space family. <p> Suppose the loss function l: Y fi Y ! R has range <ref> [0; 1] </ref> (any bounded loss function can be rescaled so this is true). Let H = fHg be any hypothesis space family.
Reference: 2. <author> Andrew Barron and Bertrand Clarke. </author> <title> Jeffreys' Prior is Asymptotically Least Favourable under Entropy Risk. </title> <journal> Journal of Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: The expectation is over all pairs (x; y) drawn according to the true distribution P fl . This quantity is only zero if the posterior is equal to the true distribution. In <ref> [8, 2] </ref> an analysis of D K (P fl kP m ) was given for the limit of large training set size (m).
Reference: 3. <author> Jonathan Baxter. </author> <title> A Model of Bias Learning. </title> <type> Technical Report LSE-MPS-97, </type> <institution> London School of Economics, Centre for Discrete and Applicable Mathematics, </institution> <month> November </month> <year> 1995. </year> <note> Submitted for publication. Copy available from http://keating.anu.edu.au/~jon/papers/lse.ps.gz. </note>
Reference-contexts: Bias learning is a form of learning to learn, and the two expressions will be used interchangeably throughout this document. The purpose of this chapter is to present an overview of two models of supervised bias learning. The first <ref> [4, 3] </ref> is based on Empirical Process theory (henceforth the EP model) and the second [6] is based on Bayesian inference and information theory (henceforth the Bayes model). Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis [13, 12]. <p> l ) ; " 2 (27) then with probability at least 1 ffi (over the (n; m)-sample z), all H 2 H will satisfy er Q (H) ^er z (H) + " (28) For a proof of a similar theorem to this one, see the proof of theorem 7 in <ref> [3] </ref>. Note that the constants in this theorem have not been heavily optimized. There are several important points to note about theorem 2: 1. <p> Feature learning in these domains should be particularly successful. Notes 1. Strictly speaking, in order for Q to be well defined we need to specify a -algebra of subsets of P. However, such considerations are beyond the scope of the present discussion. See <ref> [3] </ref> for more details. 2. is Lipschitz if there exists a constant K such that (x; x 0 ) Kjx x 0 j for all x; x 0 2 R.
Reference: 4. <author> Jonathan Baxter. </author> <title> Learning Internal Representations. </title> <booktitle> In Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <pages> pages 311-320. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: Bias learning is a form of learning to learn, and the two expressions will be used interchangeably throughout this document. The purpose of this chapter is to present an overview of two models of supervised bias learning. The first <ref> [4, 3] </ref> is based on Empirical Process theory (henceforth the EP model) and the second [6] is based on Bayesian inference and information theory (henceforth the Bayes model). Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis [13, 12]. <p> For details see <ref> [4] </ref>. The size of z ensuring that the resulting features will be good for learning novel tasks from the same environment is given by theorem 2.
Reference: 5. <author> Jonathan Baxter. </author> <title> Learning Internal Representations. </title> <type> PhD thesis, </type> <institution> Department of Mathematics and Statistics, The Flinders University of South Australia, </institution> <year> 1995. </year> <note> Copy available from http://keating.anu.edu.au/~jon/papers/thesis.ps.gz. </note>
Reference-contexts: : : : ; ff k are bounded, and the squashing function is Lipschitz 2 , then there exist constants ; 0 (independent of "; W and k) such that for all " &gt; 0, ln C ("; H n ln C ("; H fl ) 2W ln " (see <ref> [5] </ref> for a proof). Plugging these expressions into theorem 2 gives the following theorem. Theorem 4 Let H = fH w g be a hypothesis space family where each hypothesis space H w is a set of squashed linear maps composed with a neural network feature map, as above.
Reference: 6. <author> Jonathan Baxter. </author> <title> A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling. </title> <booktitle> Machine Learning, </booktitle> <year> 1997. </year> <note> To Appear. </note>
Reference-contexts: The purpose of this chapter is to present an overview of two models of supervised bias learning. The first [4, 3] is based on Empirical Process theory (henceforth the EP model) and the second <ref> [6] </ref> is based on Bayesian inference and information theory (henceforth the Bayes model). Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis [13, 12]. <p> Note that the expectation is over all sequences of n tasks n drawn according to P fl and all (n; k)-samples drawn according to p (z n j n ). 4.2. (a; b) models In <ref> [6] </ref>, the asymptotic behaviour of C n;m; fl as a function of m was analysed for general hierarchical models. To illustrate the results, and to show how they apply to the feature learning example of section 3.2, here we will restrict our attention to (a; b)-models. <p> To illustrate the results, and to show how they apply to the feature learning example of section 3.2, here we will restrict our attention to (a; b)-models. The concept of an (a; b)-model was formally defined in <ref> [6] </ref> as follows: Definition 5 An (a; b)-model is a hierarchical model in which = R b , fi = R a fi R b and the following conditions hold: 1. <p> The hyper-prior on , P has continuous density p () and the true prior fl has positive density p ( fl ). 3. The conditional distributions p (zj) are twice continuously differentiable func tions of . The definition of an (a; b)-model in <ref> [6] </ref> contains two technical conditions which have been omitted in the above definition. The interested reader is referred to that 21 paper for a full discussion. <p> Theorem 5 (<ref> [6] </ref>,Theorem 6) In an (a; b)-model, the learner's cumulative risk (50) satisfies C n;m; fl = ;P 2 a + n + o (log m) : (53) If the true prior fl is known then C n;m; fl = 2 Note that equation (54) is an equality. In [6] it was stated as a : = relation but in fact the stronger expression holds. Comparing (53) and (54), we see that as the number of tasks n increases, the effect of lack of knowledge of the true prior can be made arbitrarily small. <p> Unfortunately, the neural network feature model does not satisfy the technical conditions mentioned after definition 5, so a straightforward application of theorem 5 is not possible. However, the difficulties are not insurmountable (see <ref> [6] </ref> for the details) and so we obtain the following theorem: Theorem 6 For the neural-network feature learning model as above, the cumulative risk (50) satisfies C n;m; fl = 2 k + n + o (log m) : (55) If the true prior is known (i.e. the true feature weights
Reference: 7. <author> James O Berger. </author> <title> Multivariate Estimation: Bayes, Empirical Bayes, and Stein Approaches. </title> <publisher> SIAM, </publisher> <year> 1986. </year>
Reference-contexts: The two-tiered structure with a set of possible priors fP : 2 g and a hyper-prior p () on makes this model an example of a hierarchical Bayesian model <ref> [7, 9] </ref>.
Reference: 8. <author> Bertrand Clarke and Andrew Barron. </author> <title> Information-Theoretic Asymptotics of Bayes Methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36 </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: The expectation is over all pairs (x; y) drawn according to the true distribution P fl . This quantity is only zero if the posterior is equal to the true distribution. In <ref> [8, 2] </ref> an analysis of D K (P fl kP m ) was given for the limit of large training set size (m).
Reference: 9. <author> I J Good. </author> <title> Some History of the Hierarchical Bayesian Methodology. </title> <editor> In J M Bernado, M H De Groot, D V Lindley, and A F M Smith, editors, </editor> <title> Bayesian Statistics II. </title> <publisher> University Press, </publisher> <address> Valencia, </address> <year> 1980. </year>
Reference-contexts: The two-tiered structure with a set of possible priors fP : 2 g and a hyper-prior p () on makes this model an example of a hierarchical Bayesian model <ref> [7, 9] </ref>.
Reference: 10. <author> David Hume. </author> <title> A Treatise of Human Nature. </title> <type> 1737. </type>
Reference-contexts: 1. Introduction Hume's analysis <ref> [10] </ref> shows that there is no a priori basis for induction. In a machine learning context, this means that a learner must be biased in some way for it to generalise well [11].
Reference: 11. <author> Tom M Mitchell. </author> <title> The need for biases in learning generalisations. </title> <editor> In Tom G Dietterich and Jude Shavlik, editors, </editor> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1. Introduction Hume's analysis [10] shows that there is no a priori basis for induction. In a machine learning context, this means that a learner must be biased in some way for it to generalise well <ref> [11] </ref>. Typically such bias is introduced by hand through the skill and insights of experts, but despite many notable successes, this process is limited by the experts' abilities. Hence a desirable goal is to find ways of automatically learning the bias.
Reference: 12. <author> V N Vapnik. </author> <title> Estimation of Dependences based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis <ref> [13, 12] </ref>. Note that these are models of supervised bias learning and as such have little to say about learning to learn in a reinforcement learning setting. <p> The conditions ensuring convergence between ^er z (h) and er P (h) are by now well understood; in the case Boolean function learning (Y = f0; 1g), convergence is controlled by VCdim (H)|the VC-dimension of H (see e.g. <ref> [1, 12] </ref>).
Reference: 13. <author> V N Vapnik and A Ya Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory Probab. Appl., </journal> <volume> 16 </volume> <pages> 264-280, </pages> <year> 1971. </year> <note> 24 Received Date Accepted Date Final Manuscript Date </note>
Reference-contexts: Empirical process theory is a general theory that includes the analysis of pattern classification first introduced by Vapnik and Chervonenkis <ref> [13, 12] </ref>. Note that these are models of supervised bias learning and as such have little to say about learning to learn in a reinforcement learning setting.
References-found: 13

