URL: http://www.eecs.umich.edu/~tnm/papers/fastsim.ps
Refering-URL: http://www.eecs.umich.edu/~tnm/papers.html
Root-URL: http://www.eecs.umich.edu
Title: 4 INSTRUMENTATION TOOLS  
Author: Jim Pierce Michael D. Smith Trevor Mudge 
Address: Ann Arbor  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan,  Division of Applied Sciences Harvard University, Massachusetts  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Sites, and M. Horowitz, "ATUM: </author> <title> A new technique for capturing address traces using microcode," </title> <booktitle> Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <address> (Tokyo, Japan), </address> <month> Jun. </month> <year> 1986, </year> <pages> pp. 119-127. </pages>
Reference-contexts: Overall, researchers have proposed the following three distinct mechanisms to invoke the run-time data collection routines: microcode instrumentation, operating system (OS) trapping, and code instrumentation. Agarwal, Sites, and Horowitz <ref> [1] </ref> describe a microcode-instrumentation technique called ATUM (Address Tracing Using Microcode) that supports the capture of application, operating system, interrupt routine, and multiprogramming address activity. <p> Agarwal, Sites, and Horowitz report that the overhead of this approach causes applications to 50 Chapter 4 run about ten times slower than normal when used to collect address traces <ref> [1] </ref>. Of course, microcode instrumentation is only applicable to hardware platforms using microcode and even then the user must have the ability to modify the code. Furthermore, since most processors today have hardwired control, this approach has limited applicability.
Reference: [2] <author> A. Agarwal, </author> <title> Analysis of Cache Performance for Operating Systems and Multiprogramming. </title> <publisher> Kluwer Academic Publishers: Norwell, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: This approach is effective because, typically, only a small number of the microcode routines are responsible for the generation of all memory references. This approach is general because it is independent of the compiler, object code format, and operating system- as Agarwal states <ref> [2] </ref>, ATUM is "tracing below the operating system." In fact, any information visible to the microcode can be instrumented. Agarwal, Sites, and Horowitz report that the overhead of this approach causes applications to 50 Chapter 4 run about ten times slower than normal when used to collect address traces [1]. <p> Similar techniques were employed by Agarwal <ref> [2] </ref> and Mogul and Borg [20]. Mazieres and Smith [19] describe another multitasking tracing tool based on the QPT instrumentation tool [18] that performs late code modification. Unlike Chen [4], their research is interested in the analysis and evaluation of I/O-bound applications such as network applications.
Reference: [3] <author> T. Ball and J. Larus, </author> <title> "Optimally profiling and tracing programs," </title> <booktitle> Proceedings of the 19th Annual Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: This reduces both the execution time and resulting trace size. AE (Abstract Execution) is a tracing system developed by Larus and Ball which is incorporated as part of the Gnu C compiler <ref> [3] </ref>. Its goal is to generate very small traces which can be saved and then reused for multiple simulation runs. The modified compiler actually produces two executable programs. The first is the modified application. <p> Therefore, they organized their multitasking tool to reduce the effects of time dilation. Essentially, Mazieres and Smith attack the problem of time dilation in two ways. First, they chose QPT as their base instrumentation tools since it uses abstract execution <ref> [3] </ref> to minimize the amount of instrumentation overhead that occurs during the execution of an instrumented application. Second, they implemented their tool on a SPARC architecture where they could take advantage of several unused registers that are reserved by the SPARC ABI [27].
Reference: [4] <author> J. Chen, </author> <title> "The Impact of Software Structure and Policy on CPU and Memory System Performance," </title> <type> Technical Report CMU-CS-94-145, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Its advantages over Mahler are that the standard linker can be used and data sections remain fixed so data relocation is not necessary. Epoxie produces address traces and block statistics. An extension of epoxie has been created by Chen which can instrument kernel-level code <ref> [4] </ref>. It is described in Section 4.8. Link-time instrumentation is not automatic like late code modification and requires input from the user. The user must have the application object files and know the application's linking requirements. <p> Furthermore, several portions of the operating system are too delicate to instrument automatically. For example, standard basic block instrumentation techniques will fail to instrument properly an operating system routine which flushes the CPU write buffer. Chen <ref> [4] </ref> describes one such multitasking tracing tool based on the epoxie instrumentation tool [29] that modifies executables prior to linking. Chen's modified epoxie tool instruments code written for the MIPS instruction set architecture and thus, like pixie [23], uses register scavenging to select registers for use by the instrumentation code. <p> Chen's tool illustrates how one can minimize the problems of memory and time dilation. Even though epoxie creates instrumented executables with very little code expansion due to its link-time optimizations, these instrumented executables are approximately 2-times larger and run approximately 15-times slower than the uninstrumented versions of the executables <ref> [4] </ref>. Compensation for memory dilation in epixie is accomplished in two ways. <p> Similar techniques were employed by Agarwal [2] and Mogul and Borg [20]. Mazieres and Smith [19] describe another multitasking tracing tool based on the QPT instrumentation tool [18] that performs late code modification. Unlike Chen <ref> [4] </ref>, their research is interested in the analysis and evaluation of I/O-bound applications such as network applications. Therefore, they organized their multitasking tool to reduce the effects of time dilation. Essentially, Mazieres and Smith attack the problem of time dilation in two ways.
Reference: [5] <author> B. Cmelik and D. Keppel, "Shade: </author> <title> A fast instruction-set simulator for execution profiling," </title> <booktitle> Proceedings of 1994 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> (Nashville, TN), </address> <month> May </month> <year> 1994, </year> <pages> pp. 128-137. </pages>
Reference-contexts: Chapter 1 presented a detailed description of the first approach. Briefly, a code emulation tool is a program that simulates the hardware execution of the test program by fetching, decoding, and emulating the operation of each instruction in the test program. SPIM [12] and Shade <ref> [5] </ref> are examples of tools in this category. One of the major advantages of emulation tools is that they support cross-simulation and the ability to execute code on hardware that may not yet exist. Compared to instrumentation tools though, an emulated binary, even with sophisticated techniques such as dynamic cross-compilation [5], <p> <ref> [5] </ref> are examples of tools in this category. One of the major advantages of emulation tools is that they support cross-simulation and the ability to execute code on hardware that may not yet exist. Compared to instrumentation tools though, an emulated binary, even with sophisticated techniques such as dynamic cross-compilation [5], is noticeably slower than an instrumented binary when capturing the same run-time information. An instrumentation tool works by rewriting the program that is the target of the study so that the desired run-time information is collected during its execution.
Reference: [6] <author> B. Cmelik, </author> <title> "SpixTools Introduction and User's Manual," </title> <type> Technical Report SMLI TR-93-6, </type> <institution> Sun Microsystems Laboratory, Mountain View, </institution> <address> CA, </address> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The usefulness of instrumentation tools is obvious from a quick glance at current research publications in the area, where a significant number of authors use traces generated by two of the most popular instrumentation tools: pixie [23] and spixtools <ref> [6] </ref>. These tools are popular because of their applicability to many architectures and programs, their relatively low overhead, and their simplicity of use. This chapter's focus is the design of instrumentation tools. <p> Finally, since storage of large traces is difficult, there is library which performs on-the-fly basic block statistic calculations so that the whole trace need not be saved. 4.4 SpixTools SpixTools comprises several programs that implement late-code modification of SPARC application binaries to produce instruction-level statistics <ref> [6] </ref>. The two main tools in the SpixTools distribution are spix and spixstats. Spix accepts an executable program and generates an instrumented executable. <p> On the floating-point intensive benchmarks where instrumentation code execution can be overlapped with long latency floating-point operations and the basic block size is larger, the spix-instrumented executables run anywhere from 5% to 50% slower <ref> [6] </ref>. 4.5 QPT Like its predecessor AE [16], the design goal of QPT is to produce compact traces which can be stored for later simulations [18]. The difference between the two tools is that QPT instruments the executable while AE is part of a C compiler.
Reference: [7] <institution> Digital Equipment Corp., Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference-contexts: observe and record bus activity; 2) off-computer logic analyzers, such as the University of Michigan's Monster system [21], that monitor the activity of the system bus; and 3) special on-chip logic, such as the performance monitoring counters on the DEC ALPHA 21064 microprocessor chip, which summarize specific run- time events <ref> [7] </ref>. The two main advantages of a hardware-assisted collection scheme are that one can build hardware to capture almost any type of event and that a hardware monitor can theoretically collect dynamic information without slowing down the application under test.
Reference: [8] <author> S. Eggers, D. Keppel, E. Koldinger, and H. Levy, </author> <title> "Techniques for efficient inline tracing on a shared-memory multiprocessor," </title> <booktitle> Proceedings of 1990 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> (Boulder, CO), </address> <month> May </month> <year> 1990, </year> <pages> pp. 37-47. </pages>
Reference-contexts: The tracing overhead, including the cost of saving the compacted trace to disk, is 1-12 times the unmodified program's execution time [17]. Instrumentation Tools 55 MPtrace is a source-level instrumentation tool developed by Eggers et al. to generate shared-memory multiprocessor traces <ref> [8] </ref>. Their goals were to develop a tool which was highly portable, caused minimal trace dilation, and generated accurate traces, i.e., complete traces which closely resemble those gathered using non-intrusive techniques. Dilation describes the increases in execution time that result from code expansion due to instrumentation. <p> Using a post-processing program and the roadmap file, the full multiprocessor trace can later be generated. MPtrace can achieve a time dilation of less than a factor of 3 but the usual execution time increase is around a factor of 10 <ref> [8] </ref>. Library module code is not traced. In summary, there are three times at which code instrumentation can take place. Late code modification does not require source files, library code is automatically instrumented, and the binary creation details are hidden from the user.
Reference: [9] <author> K. Flanagan, K. Grimsrud, J. Archibald, B. Nelson, "BACH: </author> <title> BYU Address Collection Hardware," </title> <type> Technical Report TR-A150-92.1, </type> <institution> Department of Electrical and Computer Engineering, Brigham Young University, </institution> <address> Provo, UT, </address> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: These monitoring devices are not necessary for the proper functioning of the computer system under test. Many different hardware methods exist for unobtrusively monitoring system-wide events. They include: 1) specially designed hardware boards, such as the BACH system <ref> [9] </ref>, which observe and record bus activity; 2) off-computer logic analyzers, such as the University of Michigan's Monster system [21], that monitor the activity of the system bus; and 3) special on-chip logic, such as the performance monitoring counters on the DEC ALPHA 21064 microprocessor chip, which summarize specific run- time
Reference: [10] <author> G. Gircys, </author> <title> Understanding and Using COFF, </title> <publisher> O'Reilly & Associates, </publisher> <address> Se-bastopol, CA. </address>
Reference: [11] <author> M. Golden, </author> <title> "Issues in Trace Collection Through Program Instrumentation," </title> <type> MS Thesis, </type> <institution> Department of Electrical and Computer Engineering, The University of Illinois, Urbana-Champaign, </institution> <year> 1991. </year> <note> 82 Chapter 4 </note>
Reference-contexts: Overall, ATOM is a powerful tool for building customized analysis programs. 4.7 Spike Spike is an instrumentation tool which, like AE, was built into a compiler (GNU CC) <ref> [11] </ref>. Unlike AE, it is optimized for on-the-fly trace consumption rather than trace storage. This is performed by linking the original program with an instrumentation library. The library contains a procedure that is invoked for every trace event. This procedure can implement any kind of simulator or trace collector.
Reference: [12] <author> J. Hennessy and D. Patterson, </author> <title> Computer Organization and Design: The Hardware/Software Interface, </title> <publisher> Morgan Kaufmann Publishers: </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Chapter 1 presented a detailed description of the first approach. Briefly, a code emulation tool is a program that simulates the hardware execution of the test program by fetching, decoding, and emulating the operation of each instruction in the test program. SPIM <ref> [12] </ref> and Shade [5] are examples of tools in this category. One of the major advantages of emulation tools is that they support cross-simulation and the ability to execute code on hardware that may not yet exist.
Reference: [13] <author> Intel Corp., </author> <title> i486 Microprocessor Programmer's Reference Manual, </title> <year> 1990. </year>
Reference-contexts: The i486 has approximately 180 instructions which can address memory. In addition, many of these instructions can perform both a load and a store and some non-string instructions reference two different addresses <ref> [13] </ref>. In contrast, the MIPS R3000 has only 14 instructions which can reference memory. Each can only perform a single read or write and no instruction can access more than one memory address [15].
Reference: [14] <author> Intel Corp., </author> <title> UNIX System V Rel. 4.0 Programmer's Guide, </title> <type> Order #465800-001, </type> <year> 1990. </year>
Reference: [15] <author> Kane, Gerry, </author> <title> MIPS R2000 RISC Architecture, </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: In contrast, the MIPS R3000 has only 14 instructions which can reference memory. Each can only perform a single read or write and no instruction can access more than one memory address <ref> [15] </ref>. Multi-reference instructions Some processor instruction sets such as the i486 and the RS/6000 include string operations which can perform an indeterminate number of references per instruction.
Reference: [16] <author> J. Larus, </author> <title> "Abstract execution: A technique for efficiently tracing programs," </title> <journal> Software Practice and Experience, </journal> <volume> Volume 20, Number 12, </volume> <month> Dec. </month> <year> 1990, </year> <pages> pp. 1241-1258. </pages>
Reference-contexts: On the floating-point intensive benchmarks where instrumentation code execution can be overlapped with long latency floating-point operations and the basic block size is larger, the spix-instrumented executables run anywhere from 5% to 50% slower [6]. 4.5 QPT Like its predecessor AE <ref> [16] </ref>, the design goal of QPT is to produce compact traces which can be stored for later simulations [18]. The difference between the two tools is that QPT instruments the executable while AE is part of a C compiler.
Reference: [17] <author> J. Larus, </author> <title> "Efficient program tracing," </title> <journal> IEEE Computer, </journal> <volume> Volume 26, Number 5, </volume> <month> May </month> <year> 1993, </year> <pages> pp. 52-60. </pages>
Reference-contexts: The compiler also produces an application specific trace regeneration program. The regeneration program is a post-processing tool which accepts the compacted trace and outputs the full execution trace. The tracing overhead, including the cost of saving the compacted trace to disk, is 1-12 times the unmodified program's execution time <ref> [17] </ref>. Instrumentation Tools 55 MPtrace is a source-level instrumentation tool developed by Eggers et al. to generate shared-memory multiprocessor traces [8]. <p> In both these cases the expanded trace would then be piped to the consumer program. QPT instead creates a regeneration program object file which can be linked into the compiled consumer program. Thus, the consumer program can read the compacted trace directly from disk <ref> [17] </ref>. The performance of the abstract execution instrumentation depends upon the regularity of the program's control flow and memory reference patterns. <p> Numeric programs with sequential access patterns and few conditional branches require less instrumentation and therefore produce a more compact trace than Instrumentation Tools 73 do non-numeric programs with more irregular behavior. Statistics reported by Larus in <ref> [17] </ref> show that the runtime of traced programs ranges from 1.4 to 12.3 times that of the non-traced program. These numbers include the time to store the trace to disk. The compact traces are between 13 and 250 times smaller than the expanded full execution trace.
Reference: [18] <author> J. Larus and T. Ball, </author> <title> "Rewriting executable files to measure program behavior," </title> <journal> Software Practice and Experience, </journal> <volume> Volume 24, Number 2, </volume> <month> Feb. </month> <year> 1994, </year> <pages> pp. 197-218. </pages>
Reference-contexts: Most instrumentation tools can create instrumented binaries that run at less than a ten-times slowdown in execution time when collecting an address trace. Some instrumentation tools such as QPT <ref> [18] </ref> rely on sophisticated analysis routines and post-processing tools to reduce this overhead even more. This approach is generally applicable because it is independent of the operating system and underlying hardware, it has been implemented on systems ranging from Intel architectures [22] to the DEC ALPHA architecture [18][24]. <p> These too can be translated during instrumentation if assumptions about the compiler are utilized. The discussion of nixie in Section 4.2 describes how this can be done. QPT cleverly stores the translation table in the location of the original text section <ref> [18] </ref>. Instead of being an opcode, the word at the original instruction address is the translated address. This allows QPT to load a complete translation table, one which holds the translation for every original instruction 62 Chapter 4 address, without using any additional memory or file space. <p> can be overlapped with long latency floating-point operations and the basic block size is larger, the spix-instrumented executables run anywhere from 5% to 50% slower [6]. 4.5 QPT Like its predecessor AE [16], the design goal of QPT is to produce compact traces which can be stored for later simulations <ref> [18] </ref>. The difference between the two tools is that QPT instruments the executable while AE is part of a C compiler. This allows QPT to be applicable to many applications created by various compilers. <p> Similar techniques were employed by Agarwal [2] and Mogul and Borg [20]. Mazieres and Smith [19] describe another multitasking tracing tool based on the QPT instrumentation tool <ref> [18] </ref> that performs late code modification. Unlike Chen [4], their research is interested in the analysis and evaluation of I/O-bound applications such as network applications. Therefore, they organized their multitasking tool to reduce the effects of time dilation.
Reference: [19] <author> D. Mazieres and M. Smith, </author> <title> "Abstract Execution in a Multitasking Environment," </title> <type> Technical Report 31-94, </type> <institution> Center for Research in Computing Technology, Harvard University, </institution> <address> Cambridge, MA, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Similar techniques were employed by Agarwal [2] and Mogul and Borg [20]. Mazieres and Smith <ref> [19] </ref> describe another multitasking tracing tool based on the QPT instrumentation tool [18] that performs late code modification. Unlike Chen [4], their research is interested in the analysis and evaluation of I/O-bound applications such as network applications. Therefore, they organized their multitasking tool to reduce the effects of time dilation.
Reference: [20] <author> J. C. Mogul and A. Borg, </author> <title> "The effect of context switches on cache performance," </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> (Santa Clara, CA), </address> <year> 1991, </year> <pages> pp. 75-84. </pages>
Reference-contexts: Similar techniques were employed by Agarwal [2] and Mogul and Borg <ref> [20] </ref>. Mazieres and Smith [19] describe another multitasking tracing tool based on the QPT instrumentation tool [18] that performs late code modification. Unlike Chen [4], their research is interested in the analysis and evaluation of I/O-bound applications such as network applications.
Reference: [21] <author> R. Uhlig, D. Nagle, T. Stanley, T. Mudge, S. Sechrest and R. Brown, </author> <title> "Design tradeoff for software-managed TLBs," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Volume 12, Number 3, </volume> <month> Aug. </month> <year> 1995, </year> <pages> pp. 206-235. </pages>
Reference-contexts: Many different hardware methods exist for unobtrusively monitoring system-wide events. They include: 1) specially designed hardware boards, such as the BACH system [9], which observe and record bus activity; 2) off-computer logic analyzers, such as the University of Michigan's Monster system <ref> [21] </ref>, that monitor the activity of the system bus; and 3) special on-chip logic, such as the performance monitoring counters on the DEC ALPHA 21064 microprocessor chip, which summarize specific run- time events [7]. <p> In either case, less than the full amount of information is gathered which could lead to a distorted picture. To minimize the amount of unwanted data collected, researchers have combined hardware-assisted approaches with software instrumentation of applications to signal when the hardware should start and stop monitoring <ref> [21] </ref>- another compelling reason to understand software instrumentation methods. Finally, hardware-assisted collection schemes are costly and highly dependent Instrumentation Tools 49 upon the characteristics of the monitored machine; thus, they are not a practical alternative for many user.
Reference: [22] <author> J. Pierce and T. Mudge, "IDtrace: </author> <title> A Tracing Tool for i486 Simulation," </title> <type> Technical Report CSE-TR-203-94, </type> <institution> Dept. of Electrical Engineering. and Computer Science, University of Michigan, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Some instrumentation tools such as QPT [18] rely on sophisticated analysis routines and post-processing tools to reduce this overhead even more. This approach is generally applicable because it is independent of the operating system and underlying hardware, it has been implemented on systems ranging from Intel architectures <ref> [22] </ref> to the DEC ALPHA architecture [18][24]. Furthermore, most code instrumentation tools require only the executables, not the sources files, so a user can instrument a wide range of programs. There are a number of shortcomings to code instrumentation, however. It is most suited to the instrumentation of application programs. <p> The following is a description of a selection of tools for use on various platforms. 4.1 IDtrace IDtrace is an instrumentation tool for Intel architecture Unix platforms <ref> [22] </ref>. It instruments SysV R4 ELF binaries compiled using Intel/AT&T C, USL CCS C, and gcc compilers. Currently, it cannot automatically process code compiled by Intel's Proton compiler developed for the Pentium. IDtrace can produce a variety of trace types including profile, memory reference, and full execution traces.
Reference: [23] <author> M. Smith, </author> <title> "Tracing with Pixie," </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Center for Integrated Systems, Stanford University, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The architect then feeds that trace to a trace-driven simulation program. The usefulness of instrumentation tools is obvious from a quick glance at current research publications in the area, where a significant number of authors use traces generated by two of the most popular instrumentation tools: pixie <ref> [23] </ref> and spixtools [6]. These tools are popular because of their applicability to many architectures and programs, their relatively low overhead, and their simplicity of use. This chapter's focus is the design of instrumentation tools. <p> Instrumentation Tools 69 4.2 pixie and nixie Pixie was the first binary instrumentation tool which received widespread use. Pixie is a full execution trace generation tool which runs on MIPS R2000, R3000 and R4000 based systems <ref> [23] </ref>. The tool is included in the performance/debugging software package of most systems based upon the MIPS architecture. Versions are available which instrument ECOFF and ELF file formats. <p> Chen [4] describes one such multitasking tracing tool based on the epoxie instrumentation tool [29] that modifies executables prior to linking. Chen's modified epoxie tool instruments code written for the MIPS instruction set architecture and thus, like pixie <ref> [23] </ref>, uses register scavenging to select registers for use by the instrumentation code. Ideally, one would like to share the pointer into the global trace buffer indicating where the last trace item was written among all of the instrumented applications.
Reference: [24] <author> A. Srivastava and A. Eustace. </author> <title> "ATOM: A system for building customized program analysis tools," </title> <booktitle> Proceedings of the SIGPLAN 1994 Conference on Programming Language Design and Implementation, </booktitle> <address> (Orlando, FL), </address> <month> Jun. </month> <year> 1994, </year> <pages> pp. 196-205. </pages> <booktitle> Instrumentation Tools 83 </booktitle>
Reference-contexts: QPT does not currently instrument dynamically-linked shared libraries but could be modified to do so. 4.6 ATOM ATOM <ref> [24] </ref> is a tool that allows the user to build his/her own customized instrumentation and analysis tools. For example, using ATOM, a few small C routines can be written to emulate the functionality of pixie and pixstats on a DEC ALPHA machine. <p> The performance of ATOM is related to the granularity of instrumentation and the complexity of the analysis routines. Srivastava and Eustace <ref> [24] </ref> report performance numbers for several different analysis tools built with ATOM.
Reference: [25] <author> A. Srivastava and D. Wall, </author> <title> "A Practical System for Intermodular Code Optimization at Link-Time," </title> <type> Research Report 92/6, </type> <institution> DEC Western Research Laboratory, </institution> <address> Palo Alto, CA, </address> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: ATOM employs several techniques and urges the user to avoid certain programming constructs to make certain that the behavior of the application is unchanged by the instrumentation and analysis routines. ATOM is implemented on top of a link-time modification system called OM <ref> [25] </ref>. ATOM works by translating an ALPHA executable into OM's RISC-like symbolic intermediate representation. Through some extensions to OM, ATOM inserts instrumentation procedure calls at the appropriate points in the application code, optimizes the instrumentation interface, and translates the symbolic intermediate representation back into an ALPHA executable.
Reference: [26] <author> C. Stephens, B. Cogswell, J. Heinlein, G. Palmer, and J. Shen, </author> <title> "Instruction level profiling and evaluation of the IBM RS/6000," </title> <booktitle> Proceedings of 18th Annual International Symposium on Computer Architecture, </booktitle> <address> (Toronto, Canada), </address> <month> May </month> <year> 1991, </year> <pages> pp. 180-189. </pages>
Reference-contexts: However, results from benchmark tests showed that the runtime of nixie instrumented binaries were up to 30% faster than pixie-instrumented ones [29]. 4.3 Goblin Goblin is a trace generation tool which instruments IBM RS/6000 applications <ref> [26] </ref>. It annotates code on the basic block level, i.e., code is added prior to each basic block to report block execution. Goblin has characteristics of both a late code and link-time modification tool.
Reference: [27] <author> Sun Microsystems, </author> <title> The Sparc Architecture Manual, </title> <year> 1989. </year>
Reference-contexts: Second, they implemented their tool on a SPARC architecture where they could take advantage of several unused registers that are reserved by the SPARC ABI <ref> [27] </ref>. They use one of these reserved registers as the single, global, register-based, trace-buffer pointer that is shared by all instrumented executables. This decision removes the need for the copying of the per-process trace buffers into the global trace buffer as seen in Chen's system.
Reference: [28] <author> R. Uhlig, D. Nagle, T. Mudge, and S. Sechrest, </author> <title> "Trap-driven simulation with Tapeworm II," </title> <booktitle> Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> (San Jose, CA), </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: A disadvantage of using OS traps is that, if many events must be recorded, the cumulative OS overhead of handling all the traps is significant. However, there are a number of exception mechanisms in operating systems that can be utilized to improve the efficiency of this method. Tapeworm II <ref> [28] </ref> is an example of an efficient software-based tool that drives cache and TLB simulations using information from kernel traps. It utilizes low-overhead exceptions and traps of relatively few events. The applicability and efficiency of the OS-trap approach depends upon the accessibility of certain OS primitives.
Reference: [29] <author> D. Wall, </author> <title> "Systems for late code modification," In Code Generation-Concepts, Tools, Techniques, </title> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <pages> pp. 275-293. </pages>
Reference-contexts: Code and data relocation is done as described above. Another tool, epoxie, relies on incremental linking which produces an executable containing a combined relocation dictionary and symbol table <ref> [29] </ref>. Its advantages over Mahler are that the standard linker can be used and data sections remain fixed so data relocation is not necessary. Epoxie produces address traces and block statistics. An extension of epoxie has been created by Chen which can instrument kernel-level code [4]. <p> The original code instructions which referenced these registers are replaced with memory referencing instructions. Pixie then uses the registers exclusively as instrumentation registers holding buffer pointers and effective address calculations. They are used in instrumentation segments throughout the program without having to continually save and restore their values <ref> [29] </ref>. QPT relies on the caller-save procedure register convention to scavenge instrumentation registers. QPT finds registers which were saved by the calling procedure but unused in the current procedure. <p> Hand assembled code could cause errors in separation and lead to inaccurate results. In addition, pixie cannot trace past fork calls and will fail on some special library routines. In an attempt to lower the runtime overhead of pixie, another tool called nixie was created <ref> [29] </ref>. At the cost of becoming compiler-dependent and operating on a smaller set of application binaries, it makes assumptions about the binary code structure in order reduce runtime address translations. <p> Because nixie makes compiler-based assumptions about code structure, it can only instrument a subset of the pixie instrumentable applications. However, results from benchmark tests showed that the runtime of nixie instrumented binaries were up to 30% faster than pixie-instrumented ones <ref> [29] </ref>. 4.3 Goblin Goblin is a trace generation tool which instruments IBM RS/6000 applications [26]. It annotates code on the basic block level, i.e., code is added prior to each basic block to report block execution. Goblin has characteristics of both a late code and link-time modification tool. <p> Furthermore, several portions of the operating system are too delicate to instrument automatically. For example, standard basic block instrumentation techniques will fail to instrument properly an operating system routine which flushes the CPU write buffer. Chen [4] describes one such multitasking tracing tool based on the epoxie instrumentation tool <ref> [29] </ref> that modifies executables prior to linking. Chen's modified epoxie tool instruments code written for the MIPS instruction set architecture and thus, like pixie [23], uses register scavenging to select registers for use by the instrumentation code.
Reference: [30] <author> D. Wall, </author> <title> "Link-Time Code Modification," </title> <type> Research Report 89/17, </type> <institution> DEC Western Research Laboratory, </institution> <address> Palo Alto, CA, </address> <month> Sept. </month> <year> 1989. </year> <note> 84 Chapter 4 </note>
Reference-contexts: Postponing modification until the executable stage when this information is missing makes relocation much more difficult and sometimes impossible. There are several tools which perform link-time modification. Mahler is a back-end code generator and linker for Titan, a DECWRL experimental workstation <ref> [30] </ref>. The module rewrite linker can perform intermodule register allocation, instruction pipeline scheduling, and the insertion of code for basic block counting and address trace generation. Code and data relocation is done as described above.
References-found: 30

