URL: ftp://info.mcs.anl.gov/pub/ADOLC/PAPERS/jaco_Ne_Ra.ps.gz
Refering-URL: http://www.mcs.anl.gov/Projects/autodiff/AD_Tools/adolc.anl/adolc.html
Root-URL: http://www.mcs.anl.gov
Title: Automatic Computation of Sparse Jacobians by Applying the Method of Newsam and Ramsdell  
Author: Uwe Geitner Jean Utke Andreas Griewank 
Abstract: The computation of sparse Jacobians is a common subproblem in iterative numerical algorithms. The sparsity structure is not always known a priori and may sometimes change from point to point. The subject of this paper is the automatic detection of the sparsity structure and its exploitation for an efficient computation of Jacobians using automatic differentiation, graph coloring, and fast solution algorithms for Vandermonde systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Automatic Differentiation of Algorithms: </editor> <booktitle> Theory, Implementation and Application, Proceedings of the First SIAM Workshop on Automatic Differentiation, </booktitle> <editor> A. Griewank, G. F. Corliss eds., </editor> <publisher> SIAM 1991. </publisher>
Reference-contexts: Therefore, information about the dependency relation is available. It is a natural idea to ask for this qualitative dependency information before proceeding to the actual evaluation. (We do not intend to explain the theory of automatic differentiation and its application. One may refer to M. Iri's contribution in <ref> [1] </ref>, L. <p> After some tests, the i for the seed matrices were chosen as the roots of Chebyshev polynomials, for (sfi) and (fdc) and equidistantly in <ref> [1; +1] </ref> for (mdg). An optimization algorithm for the selection of the i that takes account of the actual grouping of Vandermonde rows is currently under development. The relative error of J was evaluated in several cases, see Table 5.
Reference: [2] <author> B. M. Averick, C. H. Bishof, A. Carle, A. Griewank and J. J. </author> <title> More, Computing Large Sparse Jacobian Matrices Using Automatic Differentiation, </title> <journal> SIAM J. Sci. Comp., </journal> <volume> vol. 15(2), </volume> <year> 1994, </year> <pages> pp. 258-294. </pages>
Reference-contexts: This routine requires the argument point x 0 from (1) and the previously defined seed matrix S as input and returns the product J S, which is exact up to machine precision. The temporal complexity for this step is C fl p fl ops (f ) with C 2 <ref> [2; 5] </ref> : A4: To reconstruct the full Jacobian, one has to solve m linear equation systems with Vandermonde matrices using the fast solver that takes only 5 2 p 2 operations. <p> 00 ops (f )) and the previously defined seed matrix W T as input and returns the product W T J , which is exact up to machine precision.The temporal complexity for this step is C 0 fl q fl ops (f ) + C 00 with C 0 2 <ref> [2; 5] </ref>: B4: Analogous to A4. 3 Combining Both Directions In Section 1.1 we considered an arrow-like matrix structure as an example for which the forward and the reverse approach alone cannot be applied with reasonable effort. We use this example to explain how both schemes can be combined.
Reference: [3] <author> B. M. Averick, R. G. Carter, J. J. More and G. Xue, </author> <title> The MINPACK-2 Test Problem Collection, </title> <institution> Argonne National Laboratory, preprint MCS-P153-0692, </institution> <year> 1992. </year>
Reference-contexts: We have used three different heuristics: largest first ordering, smallest last ordering, and incidence degree ordering, where the smallest coloring number was then taken [8]. The test suite consists of five problems from the MINPACK-2 test collection <ref> [3] </ref>, a molecular distance geometry problem (mdg) [15], and a matrix with arrow structure (mas).
Reference: [4] <author> C. H. Bishof, A. Bouaricha, A. Carle and P. M. Khademi, </author> <title> Efficient Computation of Gradients and Jacobians by Transparent Exploitation of Sparsity in Automatic Differentiation, </title> <institution> Argonne National Laboratory, preprint MCS-P519-0595, </institution> <year> 1995. </year>
Reference-contexts: Although we implemented the method using ADOL-C, these variants can be performed by any tool that provides forward and reverse mode. Since the compressed matrices are still sparse, one could refine the implementation by applying dynamically sparse data structures as in <ref> [4] </ref> or the "dirty vectors" introduced by B. Christianson in [6]. The reconstruction of the full Jacobian from the compressed matrix B is completely independent of automatic differentiation. We have already mentioned the general conditioning problem with the Van-dermonde systems. <p> The test suite consists of five problems from the MINPACK-2 test collection [3], a molecular distance geometry problem (mdg) [15], and a matrix with arrow structure (mas). The MINPACK problems <ref> [4] </ref> are flow in a channel (fic), solid fuel ignition (sfi), flow in a driven cavity (fdc), swirling flow between disks (sfd), and incompressible elastic rods (ier). 4.1 The MINPACK Problems All problems are square in the sense that the number of independent and dependent variables are equal.
Reference: [5] <author> C. H. Bishof, A. Bouaricha, P. M. Khademi and J. J. </author> <title> More, Gradients in Large-Scale Optimization using Automatic Differentiation, </title> <institution> Argonne National Laboratory, preprint MCS-P488-0195, </institution> <year> 1995. </year>
Reference-contexts: This routine requires the argument point x 0 from (1) and the previously defined seed matrix S as input and returns the product J S, which is exact up to machine precision. The temporal complexity for this step is C fl p fl ops (f ) with C 2 <ref> [2; 5] </ref> : A4: To reconstruct the full Jacobian, one has to solve m linear equation systems with Vandermonde matrices using the fast solver that takes only 5 2 p 2 operations. <p> 00 ops (f )) and the previously defined seed matrix W T as input and returns the product W T J , which is exact up to machine precision.The temporal complexity for this step is C 0 fl q fl ops (f ) + C 00 with C 0 2 <ref> [2; 5] </ref>: B4: Analogous to A4. 3 Combining Both Directions In Section 1.1 we considered an arrow-like matrix structure as an example for which the forward and the reverse approach alone cannot be applied with reasonable effort. We use this example to explain how both schemes can be combined.
Reference: [6] <author> B. Christianson, </author> <title> Sharing Storage Using Dirty Vectors, </title> <note> in this proceedings </note>
Reference-contexts: Since the compressed matrices are still sparse, one could refine the implementation by applying dynamically sparse data structures as in [4] or the "dirty vectors" introduced by B. Christianson in <ref> [6] </ref>. The reconstruction of the full Jacobian from the compressed matrix B is completely independent of automatic differentiation. We have already mentioned the general conditioning problem with the Van-dermonde systems.
Reference: [7] <author> T. F. Coleman and J. Cai, </author> <title> The Cyclic Coloring Problem and Estimation of Sparse Hessians, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> vol. 7(2), </volume> <year> 1986, </year> <pages> pp. 221-235. </pages>
Reference-contexts: The major advantage of choosing S according to structural orthogonality is that each element of J S represents exactly one element of J ; that is, the reconstruction simplifies to a correct placement (spreading) of the elements. When Coleman and More [8] and Coleman and Cai <ref> [7] </ref> investigated the problem, they discovered a relation between finding structurally orthogonal columns and coloring the column incidence graph. Each color corresponds to a set S r . Thus the number of linear combinations to be computed is the number of colors. <p> For some practical problems the heuristics return increasing coloring numbers with increasing dimension, while nz r remains constant. In contrast to the original CPR algorithm, graph coloring achieves results that are essentially independent of column permutation. Recently Steihaug and Hossain [18] and Coleman and Cai <ref> [7] </ref> proposed a major extension and improvement built upon vector-Jacobian products as in (f3). The principle is to look also at structurally independent rows, perform a coloring of the row incidence graph, and then exploit (f3). <p> For matrices with an arrow-like structure that have only a few (nearly) dense rows and columns, both techniques alone are impracticable. Therefore, the authors of both [18] and <ref> [7] </ref> suggested a combination of the two techniques that allows the treatment of a larger class of sparse matrices. In this paper we extend the method of G. N. Newsam and J. D. Ramsdell (NR), which is discussed in the next section.
Reference: [8] <author> T. F. Coleman and J. J. </author> <title> More, Estimation of Sparse Jacobian Matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> vol. 20(1), </volume> <year> 1983, </year> <pages> pp. 187-209. </pages>
Reference-contexts: The major advantage of choosing S according to structural orthogonality is that each element of J S represents exactly one element of J ; that is, the reconstruction simplifies to a correct placement (spreading) of the elements. When Coleman and More <ref> [8] </ref> and Coleman and Cai [7] investigated the problem, they discovered a relation between finding structurally orthogonal columns and coloring the column incidence graph. Each color corresponds to a set S r . Thus the number of linear combinations to be computed is the number of colors. <p> Each color corresponds to a set S r . Thus the number of linear combinations to be computed is the number of colors. The graph coloring problem is known to be NP-hard, but some reliable heuristics have been developed <ref> [8] </ref>. A lower bound for the number of colors in the column incidence graph is obviously the maximal number of nonzero elements per row nz r . <p> The first tests of the precision of the results showed clearly that the graph coloring to reduce the number of different i cannot be omitted. We have used three different heuristics: largest first ordering, smallest last ordering, and incidence degree ordering, where the smallest coloring number was then taken <ref> [8] </ref>. The test suite consists of five problems from the MINPACK-2 test collection [3], a molecular distance geometry problem (mdg) [15], and a matrix with arrow structure (mas).
Reference: [9] <author> A. R. Curtis, M. J. D. Powell and J. K. Reid, </author> <title> On the Estimation of Sparse Jacobian Matrices, </title> <journal> SIAM J. Inst. Math. Appl., </journal> <volume> vol. 13, </volume> <year> 1974, </year> <pages> pp. 117-119. </pages>
Reference-contexts: are as follows: How one can find a suitable seed matrix S for a given sparsity pattern? How complicated is the reconstruction of the Jacobian in general? Automatic Computation of Sparse Jacobians by Newsam-Ramsdell 3 The first algorithm to construct S was proposed by Curtis, Powell, and Reid (CPR) in <ref> [9] </ref> in the context of divided differences, since automatic differentiation was barely known then. Instead of directional derivatives J s, divided differences f (x + hs) f (x) (4) with respect to a certain direction s were used. Nonetheless, all statements about complexity in [9] still hold because of (f4). <p> Curtis, Powell, and Reid (CPR) in <ref> [9] </ref> in the context of divided differences, since automatic differentiation was barely known then. Instead of directional derivatives J s, divided differences f (x + hs) f (x) (4) with respect to a certain direction s were used. Nonetheless, all statements about complexity in [9] still hold because of (f4). CPR is based on finding structurally orthogonal columns in the Jacobian. Two columns are structurally orthogonal when no row exists in which both have a nonzero entry. This relation implies that the inner product of the two columns is always zero.
Reference: [10] <author> W. Gautschi, </author> <title> Optimally Conditioned Vandermonde Matrices, </title> <journal> Numer. Math., </journal> <volume> vol. 24, </volume> <year> 1975. </year>
Reference-contexts: We have investigated ways to reduce v cond by suitable choices of abscissas. Even optimizing the conditioning of a square Vandermonde matrix is a difficult problem, as was shown in <ref> [10] </ref>. Here we face the additional difficulty that certain square submatrices need to be reasonably conditioned. This idea is currently under investigation. Another concept is the reduction of the number of different i 's from n to a number less than n.
Reference: [11] <author> U. Geitner, </author> <title> Automatische Berechnung von dunnbesetzten Jacobimatrizen nach dem Ansatz von Newsam-Ramsdell, diploma theses, </title> <type> TU Dresden, </type> <institution> Inst. of Scientific Computing, </institution> <year> 1995. </year>
Reference: [12] <author> G. H. Golub and C. F. van Loan, </author> <title> Matrix Computation, </title> <publisher> John Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: submatrix b S 2 IR pfip that is regular? Compared with other sets of matrices with that regularity property, the Vandermonde matrices V = i ; i; j = 1 : : : p; i 6= i 0 ; have the advantage that a very fast solution algorithm is applicable <ref> [12] </ref>. Its complexity is 5 2 p 2 versus p 3 3 for the LU-factorization of a dense arbitrary matrix of dimension p. The major disadvantage is the large condition number v cond for which Vandermonde matrices are well known.
Reference: [13] <author> A. Griewank, D. Juedes and J. Utke, ADOL-C, </author> <title> A Package for the Automatic Differentiation of Algorithms Written in C/C++, </title> <note> to appear in TOMS 1996. </note>
Reference-contexts: It is a natural idea to ask for this qualitative dependency information before proceeding to the actual evaluation. (We do not intend to explain the theory of automatic differentiation and its application. One may refer to M. Iri's contribution in [1], L. Rall's introduction in this volume, or <ref> [13] </ref> for more detailed introductions.) Four facts are essential in our context: (f1) there are two modes of automatic differentiation, the forward and the reverse modes, both of which have predictable temporal and spatial complexity; (f2) the forward mode can compute linear combinations of columns (J s) of the Jacobian 1 <p> The implementation is based on the package ADOL-C <ref> [13] </ref>. For the Fortran test codes we used the Fortran 90 front-end ADOL-F [17]. <p> The number of colors is the number of different i to be chosen. By assigning r to all rows s i ; i 2 S r , we construct the seed matrix S. A3: Using another routine from ADOL-C, we perform a first-order vector forward sweep <ref> [13] </ref>. This routine requires the argument point x 0 from (1) and the previously defined seed matrix S as input and returns the product J S, which is exact up to machine precision. <p> By assigning r to all rows s i ; i 2 S r , we construct the seed matrix W in the same fashion as S. B3: Using another routine from ADOL-C, we perform a first-order vector reverse sweep <ref> [13] </ref>. <p> We start with p = 1, calculate q and p + q, increase p by 1, and calculate q and p + q again, and so forth. 4 This is the so-called Taylor file; see <ref> [13] </ref>. 8 Geitner, Utke, Griewank The optimal combination of (p; q) that has been found in the described process will now be used for the splitting of J . The whole algorithm can be described as follows: C1: Exactly the same as A1.
Reference: [14] <author> W. Gropp, S. Balay, L. C. McInnes, B. Smith, </author> <note> PETSc 2.0 Users Manual, Argonne National Laboratory, draft 11/1995. </note>
Reference-contexts: The implementation is based on the package ADOL-C [13]. For the Fortran test codes we used the Fortran 90 front-end ADOL-F [17]. The visualization of the sparsity patterns were done with routines from PETSc <ref> [14] </ref>. 1.1 The Problem For a given function f : IR n 7! IR m ; f (x) = y one wants to evaluate the Jacobian J = @f i # (1) at a certain argument x 0 .
Reference: [15] <author> J. J. More and Z. Wu, </author> <title> *-Optimal Solutions to Distance Geometry Problems via Global Continuation, </title> <institution> Argonne National Laboratory, preprint MCS-P520-0595, </institution> <year> 1995. </year>
Reference-contexts: We have used three different heuristics: largest first ordering, smallest last ordering, and incidence degree ordering, where the smallest coloring number was then taken [8]. The test suite consists of five problems from the MINPACK-2 test collection [3], a molecular distance geometry problem (mdg) <ref> [15] </ref>, and a matrix with arrow structure (mas).
Reference: [16] <author> G. N. Newsam and J. D. Ramsdell, </author> <title> Estimation of Sparse Jacobian Matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> vol. 4(3), </volume> <year> 1983, </year> <pages> pp. 404-417. </pages>
Reference-contexts: An idea of how this could work was given in the first example. 2 This is the so-called chromatic number O. 4 Geitner, Utke, Griewank 2 The Method of Newsam and Ramsdell In 1983 Newsam and Ramsdell published their paper <ref> [16] </ref> describing a more efficient method to calculate sparse Jacobians. Since they did not know about automatic differentiation, they had to rely on divided differences, with that method's implicit inaccuracy, but the important complexity results still hold.
Reference: [17] <author> D. Shiriaev, A. Griewank and J. </author> <month> Utke, </month>
Reference-contexts: The implementation is based on the package ADOL-C [13]. For the Fortran test codes we used the Fortran 90 front-end ADOL-F <ref> [17] </ref>.
References-found: 17

