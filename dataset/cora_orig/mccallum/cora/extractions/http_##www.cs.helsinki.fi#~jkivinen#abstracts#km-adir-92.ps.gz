URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/km-adir-92.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~jkivinen/abstracts/km-adir-92.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Approximate Inference of Functional Dependencies from Relations  
Author: Jyrki Kivinen Heikki Mannila 
Address: P.O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, Finland  
Affiliation: Department of Computer Science  
Abstract: The functional dependency inference problem is the following. Given a relation r, find a set of functional dependencies that is equivalent with the set of all functional dependencies holding in r. All known algorithms for this task have running times that can be in the worst case exponential in the size of the smallest cover of the dependency set. We consider approximate dependency inference. We define various measures for the error of a dependency in a relation. These error measures have the value 0 if the dependency holds and a value close to 1 if the dependency clearly does not hold. Depending on the measure used, all dependencies with error at least " in r can be detected with high probability by considering only O(1=") or O(jrj 1=2 =") random tuples of r. We also show how a machine learning algorithm due to Angluin, Frazier and Pitt can be applied to give in output-polynomial time an approximately correct cover for the set of dependencies holding in r.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hussein Almuallim and Thomas G. Dietterich, </author> <title> Learning with many irrelevant features, </title> <booktitle> in: Proc. 9th National Conference on Artificial Intelligence (MIT Press, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1991) </year> <month> 547-552. </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization [8, 28] and artificial intelligence <ref> [25, 26, 1] </ref>. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes. <p> We suggest three measures, G 1 , G 2 and G 3 , for measuring the error of a dependency X ! Y in a relation r. Their respective scaled versions g 1 , g 2 and g 3 all range over <ref> [0; 1] </ref>, having the value 0 when X ! Y holds in r and the value 1 (or almost 1) if for all tuples u; v 2 r such that u 6= v, we have u [X] = v [X] but u [Y ] 6= v [Y ].
Reference: [2] <author> Dana Angluin, </author> <title> Queries and concept learning, </title> <journal> Mach. </journal> <note> Learning 2 (1988) 319-242. </note>
Reference-contexts: Obviously, if a dependency does not hold in s, it cannot hold in r. The idea of using random sampling and controlling the quality of the approximation by accuracy and confidence parameters is similar to the approach proposed by Valiant in machine learning <ref> [31, 2] </ref>. We start by looking at one dependency at a time. Consider the following algorithm. Algorithm 3.1 Using a sample to determine whether a dependency X ! Y is "-good with respect to g 2 in r. Input. <p> Assume, however, that we are willing to accept as an answer any set F with d P (F; dep (r)) " for some probability measure P on P (R) and some error parameter ". It is well known <ref> [2] </ref> that we can then answer the jth equivalence query `is the current hypothesis H j equivalent to H fl ' by choosing q j ("; ffi) = d (1=")(ln (1=ffi) + j ln 2)e random subsets X 2 P (R) according to P .
Reference: [3] <author> Dana Angluin, Michael Frazier and Leonard Pitt, </author> <title> Learning conjunctions of Horn clauses, </title> <journal> Mach. </journal> <note> Learning 9 (1992) 147-164. </note>
Reference-contexts: In Section 4 we modify an algorithm for learning conjunctions of Horn clauses, due to Angluin, Frazier and Pitt <ref> [3] </ref>. We get a randomized algorithm for finding, with high probability, a set F of dependencies such that d (F; dep (r)) &lt; " where d is a certain distance measure for dependency sets. <p> The algorithm is based on the insightful algorithm of Angluin, Frazier and Pitt <ref> [3] </ref> for learning Horn sentences. Similar applications of their algorithm have been independently done by Dechter and Pearl [9] and Kautz, Kearns and Selman [13]. <p> A Horn clause is a propositional formula of the form B 1 ^ : : : ^ B k ! A, where B i and A are propositional variables. A Horn sentence is a conjunction of Horn clauses. The algorithm of Angluin et al. <ref> [3] </ref> uses equivalence 14 and membership queries. <p> Compared to previous algorithms, the ingenious idea of Angluin, Frazier and Pitt <ref> [3] </ref> is the intersection technique. In the dependency inference terminology, when a new set X is found that is not dep (r)-closed, one looks at previous examples of this type from the list L and checks if an example can be shortened by computing the intersection. <p> On the other hand, it would be interesting to see what can be achieved by a careful application of the intersection technique of Angluin, Frazier and Pitt <ref> [3] </ref> to the dependency inference problem. There is always the possibility of an exact output-polynomial algorithm for the problem. Beeri et al. [5] describe some preliminary results about the dependencies that hold in a random relation. <p> = q 1 and G 3 (A ! BC; r) = p + q 2. 2 B Proof of Lemma 4.4 and Theorem 4.5 Insert reference num bers! Done The proof of the correctness of Algorithm 4.3 is basically the same as the proof given by Angluin, Frazier and Pitt <ref> [3] </ref> for their algorithm; our some 23 what different setting makes some modifications necessary. We need some preliminary lemmas. Lemma B.1 Let H be a cover of dep (r), and let Z ! B 2 H.
Reference: [4] <author> Dana Angluin and Leslie G. Valiant, </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matching, </title> <institution> J. Comp. Syst. Sci. </institution> <month> 18 </month> <year> (1979) </year> <month> 155-193. </month>
Reference-contexts: Let the random variable L be the number of indices i such that the ball b i is from a new urn. Then L can be considered the number of successes in m independent trials, each with probability at least " of success. Hence, by the Chernoff bounds <ref> [4] </ref> for the binomial distribution, the probability of the event L m"=2 is at most e m"=8 . By substituting into this upper bound the lower bound m (8=") ln (2=ffi) from (1), we see that the probability of the event L m"=2 is at most ffi=2.
Reference: [5] <author> Catriel Beeri, Martin Dowd, Ronald Fagin and Richard Statman, </author> <title> On the structure of Armstrong relations for functional dependencies, </title> <journal> J. Assoc. Comput. Mach. </journal> <month> 31 </month> <year> (1984) </year> <month> 30-46. </month>
Reference-contexts: On the other hand, it would be interesting to see what can be achieved by a careful application of the intersection technique of Angluin, Frazier and Pitt [3] to the dependency inference problem. There is always the possibility of an exact output-polynomial algorithm for the problem. Beeri et al. <ref> [5] </ref> describe some preliminary results about the dependencies that hold in a random relation.
Reference: [6] <author> M. Bouzeghoub, George Gardarin and E. Metais, </author> <title> Database design tools: First names? An expert system approach, </title> <booktitle> in: Proc. 11th International Conference on Very Large Data Bases (1985) 82-95. </booktitle>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design <ref> [29, 17, 6] </ref>, analysis of existing databases [7], query optimization [8, 28] and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances.
Reference: [7] <author> Marco Antonio Casanova and Jose E. Amaral de Sa, </author> <title> Mapping uninter-preted schemes into entity-relationship diagrams: Two applications to conceptual schema design, </title> <institution> IBM J. Res. Devel. </institution> <month> 28 </month> <year> (1984) </year> <month> 82-94. </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases <ref> [7] </ref>, query optimization [8, 28] and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances.
Reference: [8] <author> Rina Dechter, </author> <title> Decomposing a relation into a tree of binary relations, Not checked J. </title> <institution> Comp. Syst. Sci. </institution> <month> 41 </month> <year> (1990) </year> <month> 2-24. </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization <ref> [8, 28] </ref> and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes.
Reference: [9] <author> Rina Dechter and Judea Pearl, </author> <title> Structure identification in relational data, </title> <booktitle> Artif. Intell. </booktitle> <month> 58 </month> <year> (1992) </year> <month> 237-270. </month>
Reference-contexts: Thus, the algorithm achieves the goals above at the expense of allowing a small error. Similar results, on slightly different frameworks, have been obtained independently by Dechter and Pearl <ref> [9] </ref> and Kautz, Kearns and Selman [13]. We assume the reader is familiar with the basic notions of functional dependencies [30, 16]. Here we just mention some notational conventions. If A 2 R is a single attribute, we write for example Z ! A instead of Z ! fAg. <p> The algorithm is based on the insightful algorithm of Angluin, Frazier and Pitt [3] for learning Horn sentences. Similar applications of their algorithm have been independently done by Dechter and Pearl <ref> [9] </ref> and Kautz, Kearns and Selman [13]. A Horn clause is a propositional formula of the form B 1 ^ : : : ^ B k ! A, where B i and A are propositional variables. A Horn sentence is a conjunction of Horn clauses.
Reference: [10] <author> Janos Demetrovics and Vu Duc Thi, </author> <title> Keys, antikeys and prime attributes, </title> <institution> Annales Univ. Sci. Budapest, Sect. Comp. </institution> <month> 8 </month> <year> (1987) </year> <month> 35-52. </month>
Reference-contexts: The following simple but crucial lemma shows how we can answer membership queries and the queries replacing equivalence queries. The lemma is essentially given by Demetrovics and Thi <ref> [11, 10] </ref>. For two rows u; u 0 2 r, their agree set ag (u; u 0 ) is defined by ag (u; u 0 ) = fB 2 R j s [B] = t [B]g.
Reference: [11] <author> Janos Demetrovics and Vu Duc Thi, </author> <title> Some results about functional dependencies, </title> <note> Acta Cybernetica 8 (1988) 273-278. </note>
Reference-contexts: The following simple but crucial lemma shows how we can answer membership queries and the queries replacing equivalence queries. The lemma is essentially given by Demetrovics and Thi <ref> [11, 10] </ref>. For two rows u; u 0 2 r, their agree set ag (u; u 0 ) is defined by ag (u; u 0 ) = fB 2 R j s [B] = t [B]g.
Reference: [12] <author> Thomas Eiter and Georg Gottlob, </author> <title> Identifying the minimal transversals of a hypergraph and related problems, </title> <type> Technical Report CD-TR 91/16, </type> <institution> Technische Universitat Wien, </institution> <year> 1991. </year>
Reference-contexts: Previous algorithms for the dependency inference problem all require in the worst case exponential time with respect to the size of the output. In fact, it has been shown <ref> [12] </ref> that the existence of an output-polynomial algorithm for dependency inference would imply the existence of a similar algorithm for several other open problems. <p> The equivalence queries, however, would in the dependency inference setting correspond to queries of the form `is F equivalent to dep (r),' and no polynomial time algorithm is known for this problem. Indeed, Eiter and Gottlob <ref> [12] </ref> have shown that a special case of this problem is equivalent to several other problems for which no polynomial time algorithm is known. <p> Thus they can be generalized to more general constraints, as long as the constraints can be formulated as Horn clauses. It is an interesting open question whether similar randomized algorithms can be obtained for the problems related to dependency inference described by Eiter and Gottlob <ref> [12] </ref>. On the other hand, it would be interesting to see what can be achieved by a careful application of the intersection technique of Angluin, Frazier and Pitt [3] to the dependency inference problem. There is always the possibility of an exact output-polynomial algorithm for the problem.
Reference: [13] <author> Henry A. Kautz, Michael J. Kearns and Bart Selman, </author> <title> Horn approximations of Empirical Data, </title> <journal> Artif. Intell. </journal> <note> (to appear). </note>
Reference-contexts: Thus, the algorithm achieves the goals above at the expense of allowing a small error. Similar results, on slightly different frameworks, have been obtained independently by Dechter and Pearl [9] and Kautz, Kearns and Selman <ref> [13] </ref>. We assume the reader is familiar with the basic notions of functional dependencies [30, 16]. Here we just mention some notational conventions. If A 2 R is a single attribute, we write for example Z ! A instead of Z ! fAg. <p> The algorithm is based on the insightful algorithm of Angluin, Frazier and Pitt [3] for learning Horn sentences. Similar applications of their algorithm have been independently done by Dechter and Pearl [9] and Kautz, Kearns and Selman <ref> [13] </ref>. A Horn clause is a propositional formula of the form B 1 ^ : : : ^ B k ! A, where B i and A are propositional variables. A Horn sentence is a conjunction of Horn clauses.
Reference: [14] <author> Jyrki Kivinen and Heikki Mannila, </author> <title> The power of sampling in knowledge discovery, </title> <booktitle> in: Proc. 13th Symposium on Principles of Database Systems (ACM, </booktitle> <address> New York, </address> <year> 1994). </year>
Reference-contexts: There are several interesting directions for extending the results about using samples to detect clearly erroneous dependencies. We have recently generalized them to approximately checking not only functional dependencies but all properties of relations that can be expressed by universal sentences in tuple relational calculus <ref> [14] </ref>. Another direction, which we are currently studying, is to consider alternative error measures and try to relate the required sample sizes to other properties of the measures. We have also considered finding all the functional dependencies that are "-good in a relation.
Reference: [15] <author> Richard J. Lipton, Jeffrey F. Naughton and Donovan A. Schneider, </author> <title> Practical selectivity estimation through adaptive sampling, </title> <booktitle> in: Proc. 1990 International Conference on Management of Data (ACM, </booktitle> <address> New York, </address> <year> 1990) </year> <month> 1-11. </month>
Reference-contexts: Algorithm 3.1 is similar in spirit to the method of Lipton et al. <ref> [15] </ref> for estimating the size of a query by sampling. In our special case, a much simpler analysis is sufficient.
Reference: [16] <author> David Maier, </author> <title> The Theory of Relational Databases (Computer Science Press, </title> <address> Rockville, Maryland, </address> <year> 1983). </year>
Reference-contexts: 1 Introduction In database design, integrity constraints are conditions that define what database states are allowed. There exist several classes of dependencies (see, e.g., <ref> [16, 30, 21] </ref>). In practice, the most important class consists of functional 1 dependencies. Given a set of attributes R, a functional dependency over R is an expression X ! Y , where X; Y R. <p> Thus, the algorithm achieves the goals above at the expense of allowing a small error. Similar results, on slightly different frameworks, have been obtained independently by Dechter and Pearl [9] and Kautz, Kearns and Selman [13]. We assume the reader is familiar with the basic notions of functional dependencies <ref> [30, 16] </ref>. Here we just mention some notational conventions. If A 2 R is a single attribute, we write for example Z ! A instead of Z ! fAg.
Reference: [17] <author> Heikki Mannila and Kari-Jouko Raiha, </author> <title> Design by example: An application of Armstrong relations, </title> <institution> J. Comp. Syst. Sci. </institution> <month> 33 </month> <year> (1986) </year> <month> 126-141. </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design <ref> [29, 17, 6] </ref>, analysis of existing databases [7], query optimization [8, 28] and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances.
Reference: [18] <author> Heikki Mannila and Kari-Jouko Raiha, </author> <title> Dependency inference, </title> <booktitle> in: Proc. 13th International Conference on Very Large Data Bases (1987) 155-158. </booktitle>
Reference-contexts: Furthermore, for each n there exists a relation r over a schema of n attributes such that jrj = O (n), but each cover of dep (r) has (2 n=2 ) dependencies <ref> [18, 20] </ref>. Hence there is no hope of achieving a polynomial-time solution for the problem.
Reference: [19] <author> Heikki Mannila and Kari-Jouko Raiha, </author> <title> Algorithms for inferring functional dependencies, </title> <journal> Data Knowl. Eng. </journal> <month> 12 </month> <year> (1994) </year> <month> 83-99. </month>
Reference-contexts: That is, we would like to take a single sample s in such a way that with probability at least 1ffi, no dependency that is "-bad in r holds in s. After drawing the sample s, we infer the dependencies holding in it using any exact dependency inference algorithm <ref> [19] </ref>. Assuming the sample is small enough, the complexity of the exact algorithms does not matter. At the end of this section we discuss how one can base an exact dependency inference algorithm for the original relation on the use of a sample. <p> We have also considered finding all the functional dependencies that are "-good in a relation. This is possible by a straightforward modification of one of the algorithms <ref> [19] </ref> for finding all functional dependencies that hold exactly. The techniques we have used are fairly independent of the actual class of dependencies used. Thus they can be generalized to more general constraints, as long as the constraints can be formulated as Horn clauses.
Reference: [20] <author> Heikki Mannila and Kari-Jouko Raiha, </author> <title> On the complexity of inferring functional dependencies, </title> <journal> Discrete Appl. Math. </journal> <month> 40 </month> <year> (1992) </year> <month> 237-243. </month>
Reference-contexts: Furthermore, for each n there exists a relation r over a schema of n attributes such that jrj = O (n), but each cover of dep (r) has (2 n=2 ) dependencies <ref> [18, 20] </ref>. Hence there is no hope of achieving a polynomial-time solution for the problem.
Reference: [21] <author> Heikki Mannila and Kari-Jouko Raiha, </author> <title> Design of Relational Databases (Addison-Wesley, </title> <address> Wokingham, United Kingdom, </address> <year> 1992). </year>
Reference-contexts: 1 Introduction In database design, integrity constraints are conditions that define what database states are allowed. There exist several classes of dependencies (see, e.g., <ref> [16, 30, 21] </ref>). In practice, the most important class consists of functional 1 dependencies. Given a set of attributes R, a functional dependency over R is an expression X ! Y , where X; Y R. <p> It is known that given X R and a set G of functional dependencies Correct? over R, the time needed to decide whether X is G-closed is linear in the total length of the dependencies in G <ref> [30, 21] </ref>. For our algorithm, we can compress F by combining the dependencies with a common left-hand side.
Reference: [22] <editor> Ilkka Niiniluoto, Truthlikeness (D. </editor> <publisher> Reidel, Dordrecht, Holland, </publisher> <year> 1987). </year>
Reference-contexts: d P by d P (F; G) = P (CL (F ) 4 CL (G)) : It is clear that d P is a metric on equivalence classes of dependency sets (assuming P (A) &gt; 0 for all non-empty A P (R)); d P is also called the Mazurkiewicz metric <ref> [22] </ref>. 5 In dependency inference we are interested in finding a small cover for dep (r). In approximate dependency inference, we merely wish to find a set F that is close to dep (r) as measured by d P .
Reference: [23] <author> Gregory Piatetsky-Shapiro, </author> <title> Probabilistic Data Dependencies, </title> <booktitle> in: Proc. Machine Discovery Workshop (Aberdeen, </booktitle> <address> Scotland, </address> <year> 1992). </year>
Reference-contexts: Removed Appendix 2 Added one paragraph about P-S's measure. An alternative measure for the truth of a data dependency has been proposed by Piatetsky-Shapiro <ref> [23] </ref>. It is not directly equivalent to any of the above measures, but some bounds can be obtained. We omit the details. Next we define a measure for the distance between two sets of functional dependencies over R, without reference to any particular relation over R.
Reference: [24] <editor> Gregory Piatetsky-Shapiro and William J. Frawley (eds.), </editor> <booktitle> Knowledge Discovery in Databases (AAAI Press, </booktitle> <address> Menlo Park, California, </address> <year> 1991). </year>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization [8, 28] and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley <ref> [24] </ref> for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes.
Reference: [25] <author> Stuart J. Russell, </author> <title> The Use of Knowledge in Analogy and Induction (Mor-gan Kaufmann, </title> <address> San Mateo, California, </address> <year> 1989). </year>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization [8, 28] and artificial intelligence <ref> [25, 26, 1] </ref>. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes.
Reference: [26] <author> Jeffrey C. Schlimmer, </author> <title> Learning determinations and checking databases, </title> <booktitle> in: Proc. 1991 AAAI Workshop on Knowledge Discovery in Databases (1991) 64-76. </booktitle>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization [8, 28] and artificial intelligence <ref> [25, 26, 1] </ref>. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes.
Reference: [27] <author> Yehoshua Sagiv, Claude Delobel, David S. Parker and Ronald Fagin, </author> <title> An equivalence between relational database dependencies and a fragment of propositional logic, </title> <journal> J. Assoc. Comput. Mach. </journal> <month> 28 </month> <year> (1981) </year> <month> 435-453. </month>
Reference-contexts: The first step in adapting this algorithm for dependency inference is easy: functional dependencies can be interpreted as Horn clauses <ref> [27] </ref>. Assume that we are given the set R of attributes, which we also take as the names of the propositional variables.
Reference: [28] <author> Michael Siegel, </author> <title> Automatic rule derivation for semantic query optimization, </title> <type> Technical Report # 86-013, </type> <institution> Computer Science Department, Boston University, </institution> <year> 1986. </year> <month> 28 </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design [29, 17, 6], analysis of existing databases [7], query optimization <ref> [8, 28] </ref> and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances. The problem of inferring functional dependencies from a relation r has been shown to require time (jrj log jrj) even for a schema containing two attributes.
Reference: [29] <author> Antonio M. Silva and Michael A. Melkanoff, </author> <title> A method for helping dis-cover the dependencies of a relation, in: Advances in Data Base Theory (Plenum Press, </title> <address> New York, </address> <year> 1981) </year> <month> 115-133. </month>
Reference-contexts: The dependency inference problem is the problem of finding a small cover for dep (r). This problem has applications in database design <ref> [29, 17, 6] </ref>, analysis of existing databases [7], query optimization [8, 28] and artificial intelligence [25, 26, 1]. See Piatetsky-Shapiro and Frawley [24] for some other applications of rule inference from database instances.
Reference: [30] <author> Jeffrey D. Ullman, </author> <title> Principles of Database and Knowledge-Base Systems, </title> <publisher> Volume I (Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1988). </year>
Reference-contexts: 1 Introduction In database design, integrity constraints are conditions that define what database states are allowed. There exist several classes of dependencies (see, e.g., <ref> [16, 30, 21] </ref>). In practice, the most important class consists of functional 1 dependencies. Given a set of attributes R, a functional dependency over R is an expression X ! Y , where X; Y R. <p> Thus, the algorithm achieves the goals above at the expense of allowing a small error. Similar results, on slightly different frameworks, have been obtained independently by Dechter and Pearl [9] and Kautz, Kearns and Selman [13]. We assume the reader is familiar with the basic notions of functional dependencies <ref> [30, 16] </ref>. Here we just mention some notational conventions. If A 2 R is a single attribute, we write for example Z ! A instead of Z ! fAg. <p> It is known that given X R and a set G of functional dependencies Correct? over R, the time needed to decide whether X is G-closed is linear in the total length of the dependencies in G <ref> [30, 21] </ref>. For our algorithm, we can compress F by combining the dependencies with a common left-hand side.
Reference: [31] <author> Leslie G. Valiant, </author> <title> A theory of the learnable, </title> <journal> Comm. </journal> <note> ACM 27 (1984) 1134-1142. </note>
Reference-contexts: Obviously, if a dependency does not hold in s, it cannot hold in r. The idea of using random sampling and controlling the quality of the approximation by accuracy and confidence parameters is similar to the approach proposed by Valiant in machine learning <ref> [31, 2] </ref>. We start by looking at one dependency at a time. Consider the following algorithm. Algorithm 3.1 Using a sample to determine whether a dependency X ! Y is "-good with respect to g 2 in r. Input.
References-found: 31

