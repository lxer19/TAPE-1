URL: http://cs.anu.edu.au/~Lex.Weaver/pub_sem/publications/alife-pa125.ps
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/indexes/other-papers.html
Root-URL: 
Email: Email: lex@cs.anu.edu.au, tbossomaier@csu.edu.au  
Title: Evolution of Neural Networks to Play the Game of Dots-and-Boxes  
Author: Lex Weaver and Terry Bossomaier 
Abstract: Dots-and-Boxes is a child's game which remains analytically unsolved. We implement and evolve artificial neural networks to play this game, evaluating them against simple heuristic players. Our networks do not evaluate or predict the final outcome of the game, but rather recommend moves at each stage. Superior generalisation of play by co-evolved populations is found, and a comparison made with networks trained by back-propagation using simple heuristics as an oracle.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.M. Baldwin. </author> <title> A New Factor in Evolution. </title> <journal> The American Naturalist, </journal> <volume> 30 441-451,536-553, </volume> <month> June 1896. </month>
Reference-contexts: We plan to investigate this and combine it with training of the weights, leading to a comparison of Lamarckian Evolution and the Baldwin Effect <ref> [1] </ref> on this application. A significant problem yet to be tackled is the propagation of strategies learnt on smaller instances of a game to larger instances. Traditionally defined network architectures which are a fixed and immutable whole, are not amenable to spatial and temporal replication of discovered strategies.
Reference: [2] <author> T.R.J. Bossomaier and N. Snoad. </author> <title> Evolution of Mod ularity in Neural Networks. </title> <booktitle> In IEEE NSIP95, </booktitle> <pages> pages 289-292, </pages> <year> 1995. </year>
Reference-contexts: Indeed with these architectures, it is usually necessary for strategies to be discovered several times to be fully applied across the network. In the current application, the box completion strategy would need to be discovered once for each box. Modular network construction <ref> [2] </ref>, with re-usable modules implementing simple strategies being developed and propagated throughout an existing network and/or to larger instances, is a possible solution. 6 Conclusions Dots-and-Boxes is a simple game which is well suited to the research at hand. It has several useful properties, including clearly defined strategies and scalability.
Reference: [3] <institution> IBM Corporation. </institution> <note> The EOS Deep Blue Chess Page. Available from http://www.ibm.com/Stretch/EOS/deepblue.html. </note>
Reference-contexts: Yet the diversity and complexity of behaviour we see in ALife models does little to reassure us that they are correct. Penrose has an illustrative argument [10, p46] where he exhibits a chess problem, rapidly solved by novices yet incorrectly handled by Deep Thought <ref> [3] </ref>, a chess computer of grandmaster rank. The argument is that human analysis is somehow different. Not surprisingly, the best game playing programs build in all sorts of human-developed heuristics, which begs the question of what an artificial player can do by just evolving or learning.
Reference: [4] <author> P. Darwen and X. Yao. </author> <title> Automatic Modularisa tion by Speciation. </title> <booktitle> Accepted by IEEE Third International Conference on Evolutionary Computation (ICEC '96), </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: The other variants use the diversity of opponents within the population to bootstrap themselves out of ignorance. Implicit Fitness Sharing specifically attempts to maintain diversity within the population by reducing the reward for success against mediocre phenotypes <ref> [4] </ref>.
Reference: [5] <author> D.E. Goldberg. </author> <title> Genetic Algorithms in Search, Opti mization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <note> ISBN 0-201-15767-5. </note>
Reference-contexts: Networks are rewarded for winning games in each match, two games yielding one point, one game a half point, and no games no points 1 . The linear scaling modification described by Goldberg <ref> [5, pp.76-79] </ref> is subsequently applied to the raw fitness, such that the best individual accounts for 10% of population after proportional reproduction.
Reference: [6] <author> J.C. Holladay. </author> <title> A Note on the Game of Dots. </title> <journal> Amer ican Mathematical Monthly, </journal> <volume> 73 </volume> <pages> 717-720, </pages> <month> September </month> <year> 1966. </year>
Reference-contexts: This game is sometimes fl L. Weaver and T. Bossomaier. Evolution of Neural Networks to Play the Game of Dots-and-Boxes. In Alife V: Poster Presentations, May 16-18 1996, pages 43-50. referred to as Dots-and-Boxes <ref> [6, 8, 13] </ref>. It is described in detail in section 2. The game belongs to a class, some members of which have been completely solved, others, of which this is one, have not [7]. <p> However, in the present game we can provide precise feedback at every move against a player with a fixed lookahead (which, as discussed below, correlates well with human strategies). 2 The Game of Dots and Boxes There are two forms of this game, one of which has been solved analytically <ref> [6] </ref>. The other, which has one rule fewer, is described below and has been analysed by Nowakowski [8]. We concern ourselves only with this more general and unsolved form. ._. . . ._. . . . . . . . . . . <p> A player who can complete a box is not obliged to do so. The analysis is simplified considerably if players are obliged to complete boxes, and this was the form solved by Holladay <ref> [6] </ref>. The game ends when there are no legal moves available, ie: there are no unjoined vertically or horizontally adjacent dots left. The winner is the player to have completed the most boxes (scored the most points).
Reference: [7] <author> H. Meyniel and J. Roudneff. </author> <title> The Vertex Picking Game and a variation of the game of Dots-and-Boxes. </title> <journal> Discrete Mathematics, </journal> <volume> 70 </volume> <pages> 311-313, </pages> <year> 1988. </year>
Reference-contexts: In Alife V: Poster Presentations, May 16-18 1996, pages 43-50. referred to as Dots-and-Boxes [6, 8, 13]. It is described in detail in section 2. The game belongs to a class, some members of which have been completely solved, others, of which this is one, have not <ref> [7] </ref>. Its interest to us is that it embodies several levels of strategy and can be made arbitrarily small or large.
Reference: [8] <editor> R.J. Nowakowski. Welter's Game, Sylver Coinage, Dots-and-Boxes. </editor> <booktitle> In Proceedings of Symposia in Applied Mathematics, </booktitle> <volume> volume 43, </volume> <pages> pages 155-182. </pages> <publisher> American Mathematical Society, </publisher> <year> 1991. </year>
Reference-contexts: This game is sometimes fl L. Weaver and T. Bossomaier. Evolution of Neural Networks to Play the Game of Dots-and-Boxes. In Alife V: Poster Presentations, May 16-18 1996, pages 43-50. referred to as Dots-and-Boxes <ref> [6, 8, 13] </ref>. It is described in detail in section 2. The game belongs to a class, some members of which have been completely solved, others, of which this is one, have not [7]. <p> The other, which has one rule fewer, is described below and has been analysed by Nowakowski <ref> [8] </ref>. We concern ourselves only with this more general and unsolved form. ._. . . ._. . . . . . . . . . . Player 1 =&gt; Player 2 2.1 Rules The game is played between two players on a rectangular array of dots. <p> This is illustrated in Figure 2, where player 1's only sure win is to move in the top-right corner and concede two boxes to player 2. This contrasts with the greedy approach of the previous strategies. Nowakowski <ref> [8] </ref> refers to this as the "double-dealing endgame." Temporal and spatial limitations for this strategy are again undefined. This strategy is often not even learned by children playing the game, and to our knowledge is the deepest strategy admitted.
Reference: [9] <author> R. Penrose. </author> <title> The Emperor's New Mind. </title> <publisher> Oxford Uni versity Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction The rapid growth in Artificial Life research brings us faster than we might ever have thought to the mystery of consciousness itself. There are still informed opinions from authors such as Searle [11] and Penrose <ref> [9, 10] </ref> who believe that the workings of the human mind are fundamentally different to Turing computation. Yet the diversity and complexity of behaviour we see in ALife models does little to reassure us that they are correct.
Reference: [10] <author> R. Penrose. </author> <title> Shadows of the Mind. </title> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The rapid growth in Artificial Life research brings us faster than we might ever have thought to the mystery of consciousness itself. There are still informed opinions from authors such as Searle [11] and Penrose <ref> [9, 10] </ref> who believe that the workings of the human mind are fundamentally different to Turing computation. Yet the diversity and complexity of behaviour we see in ALife models does little to reassure us that they are correct. <p> Yet the diversity and complexity of behaviour we see in ALife models does little to reassure us that they are correct. Penrose has an illustrative argument <ref> [10, p46] </ref> where he exhibits a chess problem, rapidly solved by novices yet incorrectly handled by Deep Thought [3], a chess computer of grandmaster rank. The argument is that human analysis is somehow different.
Reference: [11] <author> J. R. Searle. </author> <title> Minds, brains and programs. </title> <journal> Behav ioral and Brain Sciences, </journal> <volume> 3 </volume> <pages> 417-424, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction The rapid growth in Artificial Life research brings us faster than we might ever have thought to the mystery of consciousness itself. There are still informed opinions from authors such as Searle <ref> [11] </ref> and Penrose [9, 10] who believe that the workings of the human mind are fundamentally different to Turing computation. Yet the diversity and complexity of behaviour we see in ALife models does little to reassure us that they are correct.
Reference: [12] <author> G. Tesauro. </author> <title> Practical Issues in Temporal Difference Learning. </title> <journal> Machine Learning, </journal> 8(3/4):257-277, May 1992. 
Reference-contexts: The argument is that human analysis is somehow different. Not surprisingly, the best game playing programs build in all sorts of human-developed heuristics, which begs the question of what an artificial player can do by just evolving or learning. This question is the theme of the present paper. Tesauro <ref> [12] </ref> has demonstrated that a neural network can reach the top rank of backgammon players with little more than feedback as to which games it wins and which it loses.
Reference: [13] <author> L. Weaver. </author> <title> Rules of Dots-and-Boxes. </title> <note> Available from http://cs.anu.edu.au/~Lex.Weaver/strategy games/dots-boxes/dandb.rules.html, </note> <month> September </month> <year> 1995. </year>
Reference-contexts: This game is sometimes fl L. Weaver and T. Bossomaier. Evolution of Neural Networks to Play the Game of Dots-and-Boxes. In Alife V: Poster Presentations, May 16-18 1996, pages 43-50. referred to as Dots-and-Boxes <ref> [6, 8, 13] </ref>. It is described in detail in section 2. The game belongs to a class, some members of which have been completely solved, others, of which this is one, have not [7].
Reference: [14] <author> P. H. Winston. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Addison Wesley, 3rd edition, </publisher> <year> 1992. </year> <note> ISBN 0-201-53377-4. </note>
Reference-contexts: Both of these networks could be reached by evolution, but they are considerably sparser than the evolved architecture, and thus present a substantial challenge. 3.7 Supervised Training We can train the network directly by a supervised learning algorithm such as back-propagation <ref> [14, pp.448-458] </ref>. Training sets were constructed by playing games between two players, one moving randomly, and the other using the level 1 box completion strategy implemented via a minimax search. Each time the level 1 player makes a move, the current game board and the move recommendations are recorded.
References-found: 14

