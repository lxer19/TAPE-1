URL: http://www.cs.brandeis.edu/~hugues/papers/ICGI_98.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~hugues/publications.html
Root-URL: http://www.cs.brandeis.edu
Email: hugues,pollack@cs.brandeis.edu  
Title: A Stochastic Search Approach to Grammar Induction  
Author: Hugues Juille and Jordan B. Pollack 
Address: Waltham, Massachusetts 02254-9110, USA  
Affiliation: Computer Science Department, Brandeis University  
Abstract: This paper describes a new sampling-based heuristic for tree search named SAGE and presents an analysis of its performance on the problem of grammar induction. This last work has been inspired by the Abbadingo DFA learning competition [14] which took place between Mars and November 1997. SAGE ended up as one of the two winners in that competition. The second winning algorithm, first proposed by Rod-ney Price, implements a new evidence-driven heuristic for state merging. Our own version of this heuristic is also described in this paper and compared to SAGE.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Carl H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-269, </pages> <month> september </month> <year> 1983. </year>
Reference-contexts: In the field of grammar induction, observations are strings that are labeled "accepted" or "rejected" and the goal is to determine the language that generates those strings. An excellent survey of the field is presented in <ref> [1] </ref>, covering in particular the issue of computational complexity and describing some inference methods for inductive learning. Several representations have been proposed to describe the abstract models used for grammar induction like deterministic finite state automata, boolean formula or propositional logic.
Reference: [2] <editor> Thomas Back, Frank Hoffmeister, and Hans-Paul Schwefel. </editor> <title> A survey of evolution strategies. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 2-9, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, techniques like Genetic Algorithms (GAs) [8], Genetic Programming (GP) [12], Evolutionary Programming (EP) [5] or Evolutionary Strategies (ES) <ref> [2] </ref> have had some recent success to tackle some problems with ill-defined search space. Basically, those algorithms sample the state space in order to gather information about the distribution of solutions. Then, this information is used to control the focus of the search.
Reference: [3] <author> Pang C. Chen. </author> <title> Heuristic sampling: a method for predicting the performance of tree searching programs. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21 </volume> <pages> 295-315, </pages> <month> april </month> <year> 1992. </year>
Reference-contexts: Indeed, problems for which the natural representation of the state space is a tree of a DAG are usually not amenable to evolutionary search. So far, random sampling techniques on search trees have been used essentially to predict the complexity of search algorithms <ref> [11, 3] </ref>, but never as a heuristic to control the search. We believe our algorithm to be the first to exploit that knowledge. The work presented in this paper tests this search algorithm on a grammar induction problem.
Reference: [4] <author> S. Das and M. C. Mozer. </author> <title> A unified gradient-descent/clustering architecture for finite state machine induction. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 19-26, </pages> <year> 1994. </year>
Reference-contexts: More recently, Pollack [16] proposed dynamical recognizers as an interesting alternative to those symbolic approaches, leading to a wide range of Recurrent Neural Network (RNN) architectures <ref> [18, 19, 4, 6] </ref> that have been employed for similar tasks.
Reference: [5] <author> Lawrence J. Fogel. </author> <title> Autonomous automata. </title> <journal> Industrial Research, </journal> <volume> 4 </volume> <pages> 14-19, </pages> <year> 1962. </year>
Reference-contexts: Secondly, they offer a general purpose procedure when little knowledge is available about the intrinsic properties of the problem or when this knowledge is difficult to introduce in a search procedure. In particular, techniques like Genetic Algorithms (GAs) [8], Genetic Programming (GP) [12], Evolutionary Programming (EP) <ref> [5] </ref> or Evolutionary Strategies (ES) [2] have had some recent success to tackle some problems with ill-defined search space. Basically, those algorithms sample the state space in order to gather information about the distribution of solutions. Then, this information is used to control the focus of the search.
Reference: [6] <author> M. L. Forcada and R. C. Carrasco. </author> <title> Learning the initial state of a second-order recurrent neural network during regular-language inference. </title> <journal> Neural Computation, </journal> <volume> 7(5) </volume> <pages> 923-930, </pages> <year> 1995. </year>
Reference-contexts: More recently, Pollack [16] proposed dynamical recognizers as an interesting alternative to those symbolic approaches, leading to a wide range of Recurrent Neural Network (RNN) architectures <ref> [18, 19, 4, 6] </ref> that have been employed for similar tasks.
Reference: [7] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: Those problems are supposed to be just beyond the current state of the art for today's DFA learning algorithms and their difficulty increases along two dimensions: the size of the underlying DFA and the sparsity of the training data. Gold <ref> [7] </ref> has shown that inferring a minimum finite state automaton compatible with given data consisting of a finite number of labeled strings is NP-complete. However, Lang [13] empirically found out that the average case is tractable.
Reference: [8] <author> David E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Secondly, they offer a general purpose procedure when little knowledge is available about the intrinsic properties of the problem or when this knowledge is difficult to introduce in a search procedure. In particular, techniques like Genetic Algorithms (GAs) <ref> [8] </ref>, Genetic Programming (GP) [12], Evolutionary Programming (EP) [5] or Evolutionary Strategies (ES) [2] have had some recent success to tackle some problems with ill-defined search space. Basically, those algorithms sample the state space in order to gather information about the distribution of solutions.
Reference: [9] <author> Hugues Juille. </author> <title> Evolution of non-deterministic incremental algorithms as a new approach for search in state spaces. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To search such a large state space, the introduction of problem-specific heuristics becomes necessary. DFA induction is not the only field of application for SAGE. In a previous work, we applied SAGE to the construction of sorting networks with a minimum number of comparators <ref> [9] </ref>.
Reference: [10] <author> Hugues Juille and Jordan B. Pollack. Sage: </author> <title> a sampling-based heuristic for tree search. </title> <note> 1998. Submitted to Machine Learning. </note>
Reference-contexts: In the SAGE model, the level of search in the tree is called the commitment degree since it corresponds to a commitment to the first choices of the incremental construction of the current best solution. A complete description of the search algorithm can be found in <ref> [10] </ref>. 3 Induction of DFAs 3.1 Presentation The aim of inductive inference is to discover an abstract model which captures the underlying rules of a system from the observation of its behavior and thus to become able to give some prediction for the future behavior of that system.
Reference: [11] <author> Donald E. Knuth. </author> <title> Estimating the efficiency of backtracking programs. </title> <journal> Math. Comp., </journal> <volume> 29 </volume> <pages> 121-136, </pages> <year> 1975. </year>
Reference-contexts: Indeed, problems for which the natural representation of the state space is a tree of a DAG are usually not amenable to evolutionary search. So far, random sampling techniques on search trees have been used essentially to predict the complexity of search algorithms <ref> [11, 3] </ref>, but never as a heuristic to control the search. We believe our algorithm to be the first to exploit that knowledge. The work presented in this paper tests this search algorithm on a grammar induction problem.
Reference: [12] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Secondly, they offer a general purpose procedure when little knowledge is available about the intrinsic properties of the problem or when this knowledge is difficult to introduce in a search procedure. In particular, techniques like Genetic Algorithms (GAs) [8], Genetic Programming (GP) <ref> [12] </ref>, Evolutionary Programming (EP) [5] or Evolutionary Strategies (ES) [2] have had some recent success to tackle some problems with ill-defined search space. Basically, those algorithms sample the state space in order to gather information about the distribution of solutions.
Reference: [13] <author> Kevin J. Lang. </author> <title> Random dfa's can be approximately learned from sparse uniform examples. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 45-52, </pages> <year> 1992. </year>
Reference-contexts: Gold [7] has shown that inferring a minimum finite state automaton compatible with given data consisting of a finite number of labeled strings is NP-complete. However, Lang <ref> [13] </ref> empirically found out that the average case is tractable. That is, randomly generated target DFAs are approximately learnable even from sparse data when this training data is also generated at random.
Reference: [14] <author> Kevin J. Lang and Barak A. Pearlmutter. </author> <title> Abbadingo one: Dfa learning competition. </title> <note> http://abba-dingo.cs.unm.edu, 1997. </note>
Reference-contexts: We believe our algorithm to be the first to exploit that knowledge. The work presented in this paper tests this search algorithm on a grammar induction problem. This appli-cation originated from the Abbadingo DFA learning competition <ref> [14] </ref> which took place between March and November 1997. This competition proposed a set of difficult instances for the problem of DFA learning as a challenge to the machine learning community and to encourage the development of new algorithms for grammar induction. <p> However, none of them could compete in the Abbadingo competition because of the proposed problems size. 3.2 The Abbadingo Competition The Abbadingo competition (organized by Lang and Pearlmutter <ref> [14] </ref>) is a challenge proposed to the machine learning community in which a set of increasingly difficult DFA induction problems have been designed. <p> Since the labeling for the test sets has not been released, the validity of a model can be tested only by submitting a candidate labeling to an "Oracle" implemented on a server at the University of New Mexico <ref> [14] </ref> which returns a "pass/fail" answer. Table 1 presents the different problems that compose this competition. The size and the depth of the target DFA are provided as a piece of information to estimate how close a DFA hypothesis is from the target.
Reference: [15] <author> Kevin J. Lang, Barak A. Pearlmutter, and Rodney Price. </author> <title> Results of the abbadingo one dfa learning competition and a new evidence driven state merging algorithm. </title> <note> 1998. Submitted to Machine Learning. </note>
Reference-contexts: DFA Target DFA Training set name size depth size 1 63 10 3478 3 260 14 28413 4 68 10 2499 6 262 14 19834 7 65 10 1521 9 267 14 11255 Results of the Competition The description of the development of the competition is presented in details in <ref> [15] </ref>. The two-dimensional ranking of problems with respect to target size and training data density allowed multiple winners. In fact, two algorithms ended up as co-winners in the competition. The first one used an evidence driven heuristic discovered by Rodney Price.
Reference: [16] <author> Jordan B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252, </pages> <year> 1991. </year>
Reference-contexts: Several representations have been proposed to describe the abstract models used for grammar induction like deterministic finite state automata, boolean formula or propositional logic. More recently, Pollack <ref> [16] </ref> proposed dynamical recognizers as an interesting alternative to those symbolic approaches, leading to a wide range of Recurrent Neural Network (RNN) architectures [18, 19, 4, 6] that have been employed for similar tasks.
Reference: [17] <author> B. A. Trakhtenbrot and Ya M. Barzdin. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North Holland Publishing Company, </publisher> <year> 1973. </year>
Reference-contexts: However, SAGE has later been able to solve problem 7 (the smallest of the problems with sparse training data) and ended up as the second co-winner in the competition. 4 Implementation 4.1 Construction Procedure for SAGE The construction procedure makes use of the state merging method described in <ref> [17] </ref>. It takes as input the prefix tree acceptor constructed from the training data. Then, a finite state automaton is iteratively constructed, one transition at a time until a valid DFA is generated (i.e., until every state has a "0" and a "1" outgoing transition). <p> Randomized construction procedure for DFA learning. 4.2 The Evidence-Driven Heuristic The state merging method implemented in <ref> [17] </ref> considers a breadth-first order for merging nodes, with the idea that a valid merge involving the largest sub-trees in the prefix tree has a higher probability of being correct than other merges.
Reference: [18] <author> R. L. Watrous and G. M. Kuhn. </author> <title> Induction of finite state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 406-414, </pages> <year> 1992. </year>
Reference-contexts: More recently, Pollack [16] proposed dynamical recognizers as an interesting alternative to those symbolic approaches, leading to a wide range of Recurrent Neural Network (RNN) architectures <ref> [18, 19, 4, 6] </ref> that have been employed for similar tasks.
Reference: [19] <author> Z. Zeng, R. M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990, </pages> <year> 1994. </year>
Reference-contexts: More recently, Pollack [16] proposed dynamical recognizers as an interesting alternative to those symbolic approaches, leading to a wide range of Recurrent Neural Network (RNN) architectures <ref> [18, 19, 4, 6] </ref> that have been employed for similar tasks.
References-found: 19

