URL: http://www.isle.org/~langley/papers/oblivion.cbr94.ps
Refering-URL: http://www.isle.org/publications.html
Root-URL: 
Email: (Langley@flamingo.stanford.edu)  (Sage@flamingo.stanford.edu)  
Title: Oblivious Decision Trees and Abstract Cases  
Author: Pat Langley Stephanie Sage 
Address: 2451 High Street, Palo Alto, CA 94301  
Affiliation: Institute for the Study of Learning and Expertise  
Note: From Working Notes of the AAAI-94 Workshop on Case-Based Reasoning (1994). Seattle, WA: AAAI Press.  
Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1990). </year> <title> A study of instance-based algorithms for supervised learning tasks: Mathematical, empirical, and psychological evaluations. </title> <type> Doctoral dissertation, </type> <institution> Department of Information & Computer Science, University of California, Irvine. </institution>
Reference-contexts: Cover and Hart (1967) have proven that a simple nearest neighbor algorithm, probably the simplest case-based method, has excellent asymptotic accuracy. However, more recent theoretical analyses (Langley & Iba, 1993) and experimental studies <ref> (Aha, 1990) </ref> suggest that the empirical sample complexity of nearest neighbor methods is exponential in the number of irrelevant features. This means that the presence of irrelevant attributes can slow the rate of case-based learning drastically.
Reference: <author> Almuallim, H., & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 547-552). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Barletta, R., & Mark, W. </author> <year> (1988). </year> <title> Explanation-based indexing of cases. </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 541-546). </pages> <address> St. Paul, MN: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgements. </title> <booktitle> Proceedings of the DARPA Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 191-199). </pages> <address> Washington, DC: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> 13 , 21-27. </pages>
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used domains. </title> <booktitle> Machine Learning, </booktitle> <pages> 11 , 63-91. </pages>
Reference: <author> Kira, K., & Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., & Iba, W. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Chambery, France. </address>
Reference-contexts: Rather than selecting features, one might employ all available features during case retrieval, giving them equal weight in this process. Cover and Hart (1967) have proven that a simple nearest neighbor algorithm, probably the simplest case-based method, has excellent asymptotic accuracy. However, more recent theoretical analyses <ref> (Langley & Iba, 1993) </ref> and experimental studies (Aha, 1990) suggest that the empirical sample complexity of nearest neighbor methods is exponential in the number of irrelevant features. This means that the presence of irrelevant attributes can slow the rate of case-based learning drastically.
Reference: <author> Moore, A. W., Hill, D. J., & Johnson, P. </author> <title> (in press). An empirical investigation of brute force to choose features, smoothers, and function approximators. </title> <editor> In S. Hanson, S. Judd, & T. Petsche (Eds.), </editor> <booktitle> Computational learning theory and natural learning systems (Vol. 3). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> C4.5: Programs for machine learning . San Mateo, </title> <address> CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Efficiently inducing determinations: A complete and efficient search algorithm that uses optimal pruning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 284-290). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 12

