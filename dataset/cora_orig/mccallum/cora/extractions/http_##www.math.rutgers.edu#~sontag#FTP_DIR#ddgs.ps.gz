URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/ddgs.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: Constr. Approx. CONSTRUCTIVE APPROXIMATION  Rates of Convex Approximation in Non-Hilbert Spaces  
Author: Michael J. Donahue, Leonid Gurvits, Christian Darken, and Eduardo Sontag 
Note: c 1994 Springer-Verlag NewYork Inc.  
Abstract: This paper deals with sparse approximations by means of convex combinations of elements from a predetermined "basis" subset S of a function space. Specifically, the focus is on the rate at which the lowest achievable error can be reduced as larger subsets of S are allowed when constructing an approximant. The new results extend those given for Hilbert spaces by Jones and Barron, including in particular a computationally attractive incremental approximation scheme. Bounds are derived for broad classes of Banach spaces; in particular, for L p spaces with 1 &lt; p &lt; 1, the O(n 1=2 ) bounds of Barron and Jones are recovered when p = 2. One motivation for the questions studied here arises from the area of "artificial neural networks," where the problem can be stated in terms of the growth in the number of "neurons" (the elements of S) needed in order to achieve a desired error rate. The focus on non-Hilbert spaces is due to the desire to understand approximation in the more "robust" (resistant to exemplar noise) L p , 1 p &lt; 2 norms. The techniques used borrow from results regarding moduli of smoothness in functional analysis as well as from the theory of stochastic processes on function spaces. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Banach, S., and S. </author> <title> Saks (1930) Sur la convergence forte dans les champs L P , Studia Math., </title> <booktitle> 2 </booktitle> <pages> 51-57. </pages>
Reference: <author> Barron, A. R. </author> <title> (1991) Approximation and estimation bounds for artificial neural networks, </title> <booktitle> in Proc. Fourth Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 243-249. </pages>
Reference: <author> Barron, A. R. </author> <title> (1992) Neural net approximation, </title> <booktitle> in Proc. of the Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> 69-72. </pages>
Reference-contexts: However, by restricting S to classes of indicator functions, we can recover L 2 -like convergence in L 1 . This approach, common in the artificial neural network community, is especially interesting in pattern clas sification applications, and the connections with Vapnik-Chervonenkis (VC) dimension|first discovered by Barron in <ref> (Barron 1992) </ref>|are especially intriguing. Definition 4.1. Let F be a class of indicator functions on a set X. Let W be the set of all Y X such that for all Z Y there exists an f 2 F such that f " Y = Z. <p> The "VC (Vapnik-Chervonenkis) dimension of F " is sup Y 2W jY j. Our bounds (Theorem 4.5) are slightly weaker than those given by <ref> (Barron 1992) </ref> (ours are "big O" as compared to "little O"). However, the proof method here Rates of Convex Approximation 25 seems worthy of note, especially as it makes use of more basic results than the central limit theorem relied upon in (Barron 1992). <p> 4.5) are slightly weaker than those given by <ref> (Barron 1992) </ref> (ours are "big O" as compared to "little O"). However, the proof method here Rates of Convex Approximation 25 seems worthy of note, especially as it makes use of more basic results than the central limit theorem relied upon in (Barron 1992). We also explore the implications of good convergence rate bounds on the VC dimension of the corresponding set S. Good convergence rate bounds for S do not imply that S has a finite V C dimension (Theorem 4.6), i.e., the converse of Theorem 4.5 is false. <p> In the multivariable case, that is, f : K ! IR with K a compact subset of IR n , the situation is far less clear. If f has "bounded variation with respect to half-spaces" (i.e., is in the convex hull of the set of all half spaces <ref> (Barron 1992) </ref>), and in particular if f admits a Fourier representation f (x) = IR n with Z k!kj ~ f (!)jd! &lt; 1; then by (Barron 1992) there are approximations with rate O (1= p n) (since f is in the convex hull of the Heavisides). <p> If f has "bounded variation with respect to half-spaces" (i.e., is in the convex hull of the set of all half spaces <ref> (Barron 1992) </ref>), and in particular if f admits a Fourier representation f (x) = IR n with Z k!kj ~ f (!)jd! &lt; 1; then by (Barron 1992) there are approximations with rate O (1= p n) (since f is in the convex hull of the Heavisides). But the precise analog of regulated functions is less obvious.
Reference: <author> Bessaga, C., and A. </author> <title> Pelczynski (1958) A generalization of results of R. C. James concerning absolute bases in Banach spaces, </title> <journal> Studia Math., </journal> <volume> 17 </volume> <pages> 151-164. </pages>
Reference-contexts: Rates of Convex Approximation 9 This theorem implies that examples of arbitrarily slow convergence exist in L 1 [0; 1] and C [0; 1], although in these cases it is also easy to construct the embed-dings directly (Darken et al. 1993). Related results can be found in <ref> (Bessaga and Pelczynski 1958) </ref>, (Rosenthal 1974), (Rosenthal 1994), and (Gowers preprint). In Section 3 we will show that a condition somewhat stronger than reflexivity (uniform smoothness) guarantees convergence of incremental approximates.
Reference: <author> Clarkson, J. A. </author> <title> (1936) Uniformly convex spaces, </title> <journal> Trans. Amer. Math. Soc., </journal> <volume> 40 </volume> <pages> 396-414. </pages>
Reference: <author> Darken, C., M. Donahue, L. Gurvits, and E. </author> <title> Sontag (1993) Rate of approximation results motivated by robust neural network learning, </title> <booktitle> in Proc. of the Sixth Annual ACM Conference on Computational Learning Theory, The Association for Computing Machinery, </booktitle> <address> New York, N.Y. </address> <pages> 303-309. </pages>
Reference-contexts: In the particular context of regression with neural networks, Hanson (1988) presents experimental results showing the superiority of L p (p t 2) to L 2 . 1 A preliminary version of some of the results presented in this paper appeared as <ref> (Darken, Donahue, Gurvits and Sontag 1993) </ref>. Rates of Convex Approximation 5 1.4. Connections to Learning Theory Of course, neural networks are closely associated with learning theory. <p> Rates of Convex Approximation 9 This theorem implies that examples of arbitrarily slow convergence exist in L 1 [0; 1] and C [0; 1], although in these cases it is also easy to construct the embed-dings directly <ref> (Darken et al. 1993) </ref>. Related results can be found in (Bessaga and Pelczynski 1958), (Rosenthal 1974), (Rosenthal 1994), and (Gowers preprint). In Section 3 we will show that a condition somewhat stronger than reflexivity (uniform smoothness) guarantees convergence of incremental approximates.
Reference: <author> Deville, R., G. Godefroy, and V. </author> <title> Zizler (1993) Smoothness and Renormings in Banach Spaces. </title>
Reference: <author> Wiley, </author> <month> N.Y.. </month>
Reference: <author> Dieudonne, J. </author> <title> (1960) Foundations of Modern Analysis. </title> <publisher> Academic Press, N.Y.. </publisher>
Reference-contexts: This is a classical question; see for instance <ref> (Dieudonne 1960, VII.6) </ref>: the closure is the set of all regulated functions, that is, the set of functions f : [a; b] ! R for which lim x!x 0 0 exist for all x 0 2 [a; b) and x 0 2 (a; b], respectively.
Reference: <author> Figiel, T. and G. </author> <title> Pisier (1974) Series aleatoires dans les espaces uniformement convexes ou uniformement lisses, </title> <editor> C. R. </editor> <publisher> Acad. Sci. </publisher> <address> Paris, 279, Serie A:611-614. </address>
Reference-contexts: See also <ref> (Figiel and Pisier 1974) </ref>.) The upper bounds obtained for incremental approximation error in the power type spaces agree with the bounds for optimal approximation error obtained in Sect. 2.2 (albeit with a slightly larger constant of proportionality), which were shown to be the best possible in Sect. 2.3.
Reference: <author> Gowers, W. T., </author> <title> A Banach space not containing c 0 , l 1 , or a reflexive subspace. </title> <type> Preprint. </type>
Reference: <author> Haagerup, U. </author> <title> (1982) The best constants in the Khintchine inequality, </title> <journal> Studia Math., </journal> <volume> 70 </volume> <pages> 231-283. </pages>
Reference-contexts: These spaces are of type t = minfp; 2g. From <ref> (Haagerup 1982) </ref> we find that the best value for C in (2.5) is 1 if 1 p 2 and 2 [ ((p + 1)=2)= ] if 2 &lt; p &lt; 1. One may use Stirling's formula to get an asymptotic formula for the latter expression. Corollary 2.6.
Reference: <author> Hanner, O. </author> <title> (1956) On the uniform convexity of L p and ` p , Arkiv for Matematik, </title> <booktitle> 3 </booktitle> <pages> 239-244. </pages>
Reference-contexts: It is possible to use the methods in <ref> (Hanner 1956) </ref> to show that the above bounds on (u) are tight. 30 Donahue, Gurvits, Darken, and Sontag For 1 &lt; p 2, (B.30) now follows immediately.
Reference: <author> Hanson, S. J. and D. J. </author> <title> Burr (1988) Minkowski-r back-propagation: learning in connectionist models with non-Euclidean error signals, </title> <booktitle> in Neural Information Processing Systems, American Institute of Physics, </booktitle> <address> New York, </address> <month> 348. </month>
Reference: <author> James, R. C. </author> <title> (1978) Nonreflexive spaces of type 2, </title> <journal> Israel J. Math., </journal> <volume> 30 :1-13. </volume>
Reference-contexts: and each sequence (f i ) it holds that E fl X fl fl C kf i k t :(2.4) It is interesting to note that t can never be greater than 2, that Hilbert spaces are of type 2, and that there exists nonreflexive Banach spaces of type 2 <ref> (James 1978) </ref>. Theorem 2.5. Let X be a Banach space of type t, where 1 t 2. Pick S X, f 2 co (S), and K &gt; 0 such that 8g 2 S, kg f k K. <p> &lt; 1, are examples of spaces with modulus of smoothness of power type t = min (p; 2) (see Appendix B), which themselves sit inside the more general class of spaces of type t. (See, for example, (Lindenstrauss and Tzafriri 1979, p. 78; Deville et al. 1993, p. 166), while <ref> (James 1978) </ref> shows that the containment is strict.
Reference: <author> Jones, L. K. </author> <title> (1992) A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training, </title> <journal> Annals of Statistics, </journal> <volume> 20 </volume> <pages> 608-613. </pages>
Reference-contexts: Also, the convex combination of the elements included in the approximant is not optimized. Instead a simple average is used. (This is an example of a fixed convexity schedule, as defined in Sect. 1.2.) Thus, our incremental approximants are the simplest yet studied, simpler even than those of <ref> (Jones 1992) </ref>. Nonetheless, the same worst-case order is obtained for these approximants on L p , 1 &lt; p &lt; 1, as for the optimal approximant. In more general spaces, the incremental approximants may not even converge (Sect. 3.1).
Reference: <author> Kakutani, S. </author> <title> (1938) Weak convergence in uniformly convex spaces, </title> <journal> Tohoku Math. J., </journal> <volume> 45 </volume> <pages> 188-193. </pages>
Reference: <author> Khintchine, J. </author> <title> (1923) Uber die diadischen Bruche. </title> <journal> Math. Z., </journal> <volume> 18 </volume> <pages> 109-116. </pages>
Reference: <author> Ledoux, M. and M. </author> <title> Talagrand (1991) Probability in Banach Space. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: This class includes infinite dimensional L 1 and L 1 (or C (X)) spaces. In Theorem 2.5 we then study general bounds valid for spaces of (Rademacher) type t. It is well-known that L p spaces with 1 p &lt; 1 are of type minfp; 2g <ref> (Ledoux and Talagrand 1991) </ref>; on this basis an explicit specialization to L p is given in Corollary 2.6. We then close this section with explicit examples showing that the obtained bounds are tight. 2.1. <p> Since the range of ~ j has finitely many values and the space is type t, by <ref> (Ledoux and Talagrand 1991, Prop. 9.11, p. 248) </ref> it follows that: E fl fl fl j=1 fl fl fl t n X E kf ~ j k :(2.7) On the other hand, we have: Ekf ~ 1 k t = i=1 i=1 t n * X ff i (K + *) <p> basic property of type t spaces: If a Banach space X is of type t, then for any independent mean zero random variables ~ i 2 X taking finitely many values, E (k~ 1 + : : : + ~ n k t ) C i=1 Ek~ i k t <ref> (Ledoux and Talagrand 1991, p. 248) </ref>. We also need the following result from Ledoux and Talagrand (1991, Theorem 6.20, p. 171). Theorem 3.9. Suppose that ~ i are independent mean zero random variables in X, Ek~ i k N 1, 1 i n, N &gt; 1, N 2 IN. <p> Let V C be the operator on classes of indicator functions that measures the Vapnik-Chervonenkis (VC) dimension. If F is a class of indicator functions, we define the co-VC dimension by coV C (F ) := V C (F 0 ):(4.25) The next lemma is a consequence of <ref> (Ledoux and Talagrand 1991, Theorem 14.15, p. 418) </ref>. Lemma 4.4. Denote by M (; ) the Banach space of all bounded signed measures on (; ) equipped with the norm kk := jj () + () + (). <p> Note that j is a linear operator. By the triangle inequality, E ~ fl fl j i=1 ! fl fl fl fl fl fl l ! fl fl fl By a simple inequality <ref> (Ledoux and Talagrand, 1991, Lemma 6.3, p. 152) </ref>, E ~ fl fl X j (~ l k ) fl fl 2E ~ E fl fl fl X fl l j (~ l k ) fl fl ; where fl l are i.i.d. random variables taking values +1 and -1 with equal
Reference: <author> Leshno, M., V. Lin, A. Pinkus, and S. </author> <title> Schocken (1992) Multilayer feedforward networks with a non-polynomial activation function can approximate any function. </title> <type> Preprint. </type> <institution> Hebrew University. </institution>
Reference-contexts: This density result holds under extremely weak conditions on ; being locally Riemann integrable and non-polynomial is enough. See for instance <ref> (Leshno et al., 1992) </ref>. Spaces L p with p equal to or slightly greater than one are particularly important because of their usefulness for robust estimation (e.g., (Rey 1983)).
Reference: <author> Lindenstrauss, J. </author> <title> (1963) On the modulus of smoothness and divergent series in Banach spaces, </title> <journal> The Michigan Math. J., </journal> <volume> 10 </volume> <pages> 241-252. </pages>
Reference-contexts: reverse sense if 2 p &lt; 1. (The second inequality above is actually due to Clarkson (1936).) Combining this with (B.31) yields (u) &lt; (1 + u p ) 1=p 1 if 1 p 2 (1 + u) p + j1 uj p 1=p (B.33) This result is cited in <ref> (Lindenstrauss 1963) </ref>. It is possible to use the methods in (Hanner 1956) to show that the above bounds on (u) are tight. 30 Donahue, Gurvits, Darken, and Sontag For 1 &lt; p 2, (B.30) now follows immediately.
Reference: <author> Lindenstrauss, J. and L. </author> <title> Tzafriri (1979) Classical Banach Spaces II: Function Spaces. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Powell, M. J. D. </author> <title> (1981) Approximation theory and methods. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: These distances as a function of n represent the convergence rates of the best approximants to the target function f . The study of such rates is standard in approximation theory (e.g., <ref> (Powell 1981) </ref>), but the questions addressed here are not among those classically considered. Let be a positive function on the integers.
Reference: <author> Rey, W. J. </author> <title> (1983) Introduction to Robust and Quasi-Robust Statistical Methods. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: This density result holds under extremely weak conditions on ; being locally Riemann integrable and non-polynomial is enough. See for instance (Leshno et al., 1992). Spaces L p with p equal to or slightly greater than one are particularly important because of their usefulness for robust estimation (e.g., <ref> (Rey 1983) </ref>). In the particular context of regression with neural networks, Hanson (1988) presents experimental results showing the superiority of L p (p t 2) to L 2 . 1 A preliminary version of some of the results presented in this paper appeared as (Darken, Donahue, Gurvits and Sontag 1993).
Reference: <author> Rosenthal, H. </author> <title> (1974) A characterization of Banach spaces containing l 1 , Proc. </title> <institution> Nat. Acad. Sci. (USA), </institution> <month> 71 </month> <pages> 2411-2413. </pages>
Reference-contexts: Related results can be found in (Bessaga and Pelczynski 1958), <ref> (Rosenthal 1974) </ref>, (Rosenthal 1994), and (Gowers preprint). In Section 3 we will show that a condition somewhat stronger than reflexivity (uniform smoothness) guarantees convergence of incremental approximates.
Reference: <author> Rosenthal, H. </author> <title> (1994) A subsequence principle characterizing Banach spaces containing c 0 , Bull. </title> <journal> Amer. Math. Soc., </journal> <volume> 30 </volume> <pages> 227-233. </pages>
Reference-contexts: Related results can be found in (Bessaga and Pelczynski 1958), (Rosenthal 1974), <ref> (Rosenthal 1994) </ref>, and (Gowers preprint). In Section 3 we will show that a condition somewhat stronger than reflexivity (uniform smoothness) guarantees convergence of incremental approximates.
Reference: <author> Sontag, E. D. </author> <title> (1992) Feedback stabilization using two-hidden-layer nets, </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3 </volume> <pages> 981-990. </pages>
Reference-contexts: Moreover, everywhere the values of g are in (1=8; 1=8) (7=8; +1). Now, the function 4g (1=2) is again in the same span, and it now has values in (1; 0) and (3; +1) in the same regions. This contradicts <ref> (Sontag 1992, Prop. 3.8) </ref>. (Of course, one can also prove this directly.) B. Properties of the Modulus of Smoothness In this section we collect some inequalities related to power type estimates of the modulus of smoothness.
Reference: <author> Stromberg, K. R. </author> <title> (1981) An Introduction to Classical Real Analysis. </title> <publisher> Wadsworth, </publisher> <address> New York. </address>
Reference-contexts: If (a n ) is not convex, then replace it with a strictly convex sequence (a n ) converging to zero such that a n a n for all n. This is a well-known construction, for example <ref> (Stromberg 1981, p. 515) </ref>. The result then follows from Lemmas 2.1 and 2.2. ut If a Banach space contains a subspace isomorphic to l 1 or c 0 , then it is not reflexive.
Reference: <author> Woo, J. Y. T. </author> <title> (1973) On modular sequence spaces, </title> <journal> Studia Math., </journal> <volume> 48 </volume> <pages> 271-289. </pages> <note> 36 Donahue, Gurvits, Darken, and Sontag Michael J. </note> <institution> Donahue Institute for Mathematics and its Applications University of Minnesota Minneapolis, MN 55455 Leonid Gurvits Learning Systems Department Siemens Corporate Research, Inc. 755 College Road East Princeton, NJ 08540 Christian Darken Learning Systems Department Siemens Corporate Research, Inc. 755 College Road East Princeton, NJ 08540 Eduardo Sontag Department of Mathematics Rutgers University New Brunswick, </institution> <note> NJ 08903 This article was processed using the LATEX macro package with CAP style </note>
Reference-contexts: a 2 IR IN j sup F n (a) &lt; 1 ;(D.48) kak := F (a) := lim F n (a):(D.49) The reader may verify that X (p n ) equipped with the norm (D.49) is a Banach space. (This space is similar to the modular sequence spaces studied in <ref> (Woo 1973) </ref>.) If (p n ) is bounded and p n &gt; 1 for all n, then it can be shown that X (p n ) is smooth.
References-found: 29

