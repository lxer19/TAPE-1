URL: http://www.cs.umd.edu/~acha/papers/preprint-iopads96.ps.gz
Refering-URL: http://www.cs.umd.edu/~acha/papers/iopads96.html
Root-URL: 
Title: Tuning the Performance of I/O-Intensive Parallel Applications  
Author: Anurag Acharya zy Mustafa Uysal Robert Bennett Assaf Mendelson Michael Beynon Jeff Hollingsworth Joel Saltz zy Alan Sussman zy 
Note: Center for Excellence in Space Data and  
Address: College Park MD 20742  Greenbelt MD 20771  
Affiliation: Dept of Computer Science University of Maryland,  Information Sciences Goddard Space Flight Center,  
Abstract: Getting good I/O performance from parallel programs is a critical problem for many application domains. In this paper, we report our experience tuning the I/O performance of four application programs from the areas of satellite-data processing and linear algebra. After tuning, three of the four applications achieve application-level I/O rates of over 100 MB/s on 16 processors. The total volume of I/O required by the programs ranged from about 75 MB to over 200 GB. We report the lessons learned in achieving high I/O performance from these applications, including the need for code restructuring, local disks on every node and knowledge of future I/O requests. We also report our experience on achieving high performance on peer-to-peer configurations. Finally, we comment on the necessity of complex I/O interfaces like collective I/O and strided requests to achieve high performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Our current implementation assumes that a standard Unix file system with asynchronous I/O calls is available on individual nodes. On the SP-2, our implementation uses the user-space communication primitives provided by IBM's Message Passing Library (MPL). Jovian-2 is significantly different from the original Jovian I/O library <ref> [1] </ref>. These changes were prompted by our experience with real I/O-intensive applications and by changes in our experimental platform. There are four main differences. <p> In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 14] </ref> on both client-server and peer-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time. Diskful machines are important: Diskful machines (machines with local disks) allow problems to be partitioned such that most of the I/O requests are satisfied by local disks. <p> Relatively straightforward loop restructuring, including loop splitting, interchanging the order of nested loops [18] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 16] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth-science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [2] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 1-10. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 8. </pages>
Reference-contexts: To date, most researchers have focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems available on parallel machines to execute these applications <ref> [2, 3, 4, 7, 9] </ref>. We take a different approach. Instead of assuming that the applications are fixed and that the I/O system alone is open to modification, we believe that both the applications and the I/O system have to be tuned to achieve good performance. <p> In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 14] </ref> on both client-server and peer-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time. Diskful machines are important: Diskful machines (machines with local disks) allow problems to be partitioned such that most of the I/O requests are satisfied by local disks.
Reference: [3] <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krish-naiyer, R. Ponnusamy, T. Singh, and Rajeev Thakur. </author> <title> PASSION: Parallel and scalable software for Input-Output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, </institution> <note> Septem-ber 1994. Also available as CRPC Report CRPC-TR94483. </note>
Reference-contexts: To date, most researchers have focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems available on parallel machines to execute these applications <ref> [2, 3, 4, 7, 9] </ref>. We take a different approach. Instead of assuming that the applications are fixed and that the I/O system alone is open to modification, we believe that both the applications and the I/O system have to be tuned to achieve good performance. <p> Relatively straightforward loop restructuring, including loop splitting, interchanging the order of nested loops [18] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 16] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth-science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [4] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings Supercomputing'95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: To date, most researchers have focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems available on parallel machines to execute these applications <ref> [2, 3, 4, 7, 9] </ref>. We take a different approach. Instead of assuming that the applications are fixed and that the I/O system alone is open to modification, we believe that both the applications and the I/O system have to be tuned to achieve good performance. <p> This belief is based on our examination of other NASA satellite-data processing programs and on the experiences reported by Pat-terson et al [15]. The characterization study by Crandall et al <ref> [4] </ref> provides another example. It describes a significantly different program running on a machine with much lower I/O bandwidth (the JPL terrain rendering application running on an Intel Paragon) that is able to achieve relatively good I/O performance with just asynchronous I/O requests.
Reference: [5] <author> J.J. Dongarra, J. DuCroz, I.S.Duff, and S. Hammar-ling. </author> <title> A set of level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Each supernode is a set of contiguous columns such that every adjacent column in the set has an identical sparsity structure below the diagonal. Using supernodes enables the use of efficient dense linear-algebra kernels <ref> [5] </ref>, as well as large transfers between secondary storage and primary memory. These applications assume a peer-peer con figuration and directly use Unix I/O calls.
Reference: [6] <author> M. King. </author> <title> Earth Observation System Project Science homepage. </title> <address> http://spso.gsfc.nasa.gov/spso homepage.html, </address> <year> 1995. </year>
Reference-contexts: Furthermore, the structure of these applications is similar to the large set of programs currently being developed to process data from the Earth Observation System <ref> [6] </ref> satellites. Pathfinder: This program is the first in the processing chain and processes AVHRR global area coverage data.
Reference: [7] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: To date, most researchers have focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems available on parallel machines to execute these applications <ref> [2, 3, 4, 7, 9] </ref>. We take a different approach. Instead of assuming that the applications are fixed and that the I/O system alone is open to modification, we believe that both the applications and the I/O system have to be tuned to achieve good performance.
Reference: [8] <author> D. Kotz and T. Cai. </author> <title> Exploring the use of I/O nodes for computation in a MIMD processor. </title> <booktitle> In Proceedings of the IPPS'95 Third Annual Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 78-89, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: On one hand, the performance of pathfinder on peer-peer configurations was poor; on the other hand, factor achieved excellent performance. The problem of achieving good computation performance on processors that are serving data to others has been previously noted by Kotz and Cai <ref> [8] </ref>. In their experiments on a cluster of RS6000s, they found that serving off-processor I/O requests can slow a relatively simple parallel program by between 17% and 98%.
Reference: [9] <author> D. Kotz and N. Nieuwejaar. </author> <title> File-system workload on a scientific multiprocessor. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3(1) </volume> <pages> 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: To date, most researchers have focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems available on parallel machines to execute these applications <ref> [2, 3, 4, 7, 9] </ref>. We take a different approach. Instead of assuming that the applications are fixed and that the I/O system alone is open to modification, we believe that both the applications and the I/O system have to be tuned to achieve good performance.
Reference: [10] <author> J. W. H. Liu. </author> <title> The Role of Elimination Trees in Sparse Factorization. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> (11):134-172, 1990. 
Reference-contexts: We have not attempted to improve the communication balance for the factor. Our relatively simple technique provided acceptable performance for moderately unstructured matrices but did not perform well on sara-2, which is very sparse. A key data structure in factor is the elimination tree <ref> [10] </ref> generated during symbolic factorization using the structure of the sparse matrix. This structure contains dependency information between different supernodes and does not change over the course of the computation.
Reference: [11] <author> E. G. Ng and B. W. Peyton. </author> <title> Block Sparse Cholesky Algorithms on Advanced Uniprocessor Computers. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14(5) </volume> <pages> 1034-1056, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The Cholesky factor of a symmetric positive definite matrix A is a lower-triangular matrix L with positive diagonal, such that A = LL T . Our parallel out-of-core sparse Cholesky factorization is a parallelization of a left-looking supernodal Cholesky factorization algorithm <ref> [11] </ref>. This par ticular formulation of Cholesky factorization is based on su 3 We arrive at this number by calculating the number of double-precision complex values that a 50 GB memory will hold and by using this number and the sparsity to compute the corresponding number of equations.
Reference: [12] <author> N. Nieuwejaar and D. Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In Proceedings of the IPPS'95 Third Annual Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Complex I/O interfaces are not required: * After code restructuring, most requests in the studied applications were large. For large requests, the inter face is usually less important. * Small strided requests were a recurrent pattern in the original versions of pathfinder and climate. Nested-strided requests <ref> [12] </ref> have been proposed for just such patterns. However we found that these patterns were caused by the embedding of small I/O requests in the innermost loops.
Reference: [13] <author> W. Norcutt. </author> <title> IOZONE benchmark program. </title> <note> Available at ftp://ftp.cs.umn.edu/packages/FreeBSD/2.0.5-RELEASE/ports/utils/iozone, </note> <year> 1991. </year>
Reference-contexts: To measure single node JFS performance, we used a modified version of the widely used iozone benchmark <ref> [13] </ref>. iozone determines the maximum application-level I/O bandwidth by making a sequence of contiguous write requests followed by a sequence of contiguous read requests. Our version of this program supports multi-disk configurations and can generate multiple simultaneous requests per disk.
Reference: [14] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on parallel machines. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 14] </ref> on both client-server and peer-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time. Diskful machines are important: Diskful machines (machines with local disks) allow problems to be partitioned such that most of the I/O requests are satisfied by local disks.
Reference: [15] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 79-95, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: For factor and partitioner, the sequence of requests is available from the elimination-tree structure generated by symbolic factorization. Similar experiences have been reported by Patterson et al <ref> [15] </ref>. They report that after relatively simple loop-splitting transformations, significant knowledge about future I/O requests is available in all five I/O-intensive programs they studied. <p> However, we believe that a substantial class of I/O-intensive programs will be able to achieve good I/O performance with simple I/O interfaces. This belief is based on our examination of other NASA satellite-data processing programs and on the experiences reported by Pat-terson et al <ref> [15] </ref>. The characterization study by Crandall et al [4] provides another example.
Reference: [16] <author> J. Rosario and A. Choudhary. </author> <title> High-performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Relatively straightforward loop restructuring, including loop splitting, interchanging the order of nested loops [18] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 16] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth-science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [17] <author> Edward Rothberg and Anoop Gupta. </author> <title> An Efficient Block-Oriented Approach to Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 503-512, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The Cholesky factor is partitioned using a 2-D strategy originally developed in <ref> [17] </ref>. The processors are organized in a k fi m grid. Let P r;q denote the processor number at the rth row and qth column of the processor grid. Supernode i of matrix A is mapped to processors in the (i mod m) th column of the processor grid.
Reference: [18] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year> <month> 13 </month>
Reference-contexts: Nested-strided requests [12] have been proposed for just such patterns. However we found that these patterns were caused by the embedding of small I/O requests in the innermost loops. Relatively straightforward loop restructuring, including loop splitting, interchanging the order of nested loops <ref> [18] </ref> and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O [1, 3, 16]. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion.
References-found: 18

