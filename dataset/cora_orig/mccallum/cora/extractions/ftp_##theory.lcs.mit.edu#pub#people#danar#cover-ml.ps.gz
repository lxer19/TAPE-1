URL: ftp://theory.lcs.mit.edu/pub/people/danar/cover-ml.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~danar/papers.html
Root-URL: 
Email: danar@theory.lcs.mit.edu  ronitt@cs.cornell.edu  
Title: Exactly Learning Automata of Small Cover Time  
Author: Dana Ron Ronitt Rubinfeld 
Note: Supported by a National Science Foundation Postdoctoral Research Fellowship, Grant No. DMS-9508963 Supported by ONR Young Investigator Award N00014-93-1-0590 and grant No. 92-00226 from the United States Israel Binational Science Foundation (BSF), Jerusalem, Israel.  
Address: Cambridge, MA 02139  Ithaca, NY 14853  
Affiliation: Laboratory of Computer Science MIT  Computer Science Department Cornell University  
Abstract: We present algorithms for exactly learning unknown environments that can be described by deterministic finite automata. The learner performs a walk on the target automaton, where at each step it observes the output of the state it is at, and chooses a labeled edge to traverse to the next state. The learner has no means of a reset, and does not have access to a teacher that answers equivalence queries and gives the learner counterexamples to its hypotheses. We present two algorithms: The first is for the case in which the outputs observed by the learner are always correct, and the second is for the case in which the outputs might be corrupted by random noise. The running times of both algorithms are polynomial in the cover time of the underlying graph of the target automaton. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Romas Aleliunas, Richard M. Karp, Richard J. Lipton, Laszlo Lovasz, and Charles Rack-off. </author> <title> Random walks, universal traversal sequences, and the complexity of maze problems. </title> <booktitle> In Proceedings of the Twentieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 218-223, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: Several natural classes of directed graphs are known to have this property. One important such class is the class of graphs in which the indegree of each node is equal to the outdegree <ref> [1] </ref>.
Reference: [2] <author> D. Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference-contexts: We also study the case in which the environment is noisy, in the sense that there is some fixed probability that the learner observes an incorrect output of the state it is at. Angluin <ref> [2] </ref> has shown that the general problem of exactly learning finite automata by performing a walk on the target automaton, but without access to an equivalence oracle, is hard in the information theoretic sense (even when the learner has means of a reset). <p> Note that having a teacher which answers membership queries is equivalent to having the means of a reset. We use as a subroutine of our algorithm a variant of Angluin's algorithm which is similar to the one described in <ref> [2] </ref>. In this procedure (for learning with means of a reset) lies the first key to overcoming the need for a teacher which answers equivalence queries. <p> It then proceeds by performing additional walks (starting from the start state) which are determined by the initial random walk. Using a simple argument (similar to an argument used in <ref> [2] </ref>), we show that all that is needed for the procedure to terminate (in polynomial time) with a hypothesis automaton which is equivalent to the target automaton, is that each state of the target automaton is passed in the initial walk. <p> However, if M is not known to the player, then Fortnow and Whang [11] show, using the same combination-lock automata argument of Angluin <ref> [2] </ref>, that it is hard to find an optimal strategy in the case of a general game 5 . Clearly, if a class of automata can be learned exactly and efficiently without reset, then an optimal cycle strategy can be found efficiently. <p> We refer the reader to a survey by Angluin and Smith [6]. Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs. In addition to the work of Angluin <ref> [2, 3] </ref> and Rivest and Schapire [22] that were discussed previously, the following is also known: Rivest and Schapire [23] show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. <p> The algorithm works in the setting where the learner has means of a reset. The analysis is similar to that in <ref> [2] </ref> and shows that if the target automaton M has cover time C (M ) then with high probability, the algorithm exactly learns the target automaton by performing O (nC (M )) walks, each of length O (C (M )).
Reference: [3] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Our results are easily extendible to larger alphabets. In our algorithms we apply ideas from the no-reset learning algorithm of Rivest and Schapire [22], which in turn uses Angluin's algorithm <ref> [3] </ref> as a subroutine. Angluin's algorithm is an algorithm for exactly learning automata from a teacher that can answer both membership queries and equivalence queries. Note that having a teacher which answers membership queries is equivalent to having the means of a reset. <p> This model is equivalent to PAC learning with membership queries. Since Angluin's algorithm <ref> [3] </ref> can be modified to a PAC learning algorithm with membership queries, DFAs are efficiently learnable in this model. However, when the learner does not have means of a reset, and thus performs a single walk on M , we know of no natural notion of approximately correct learning. <p> We refer the reader to a survey by Angluin and Smith [6]. Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs. In addition to the work of Angluin <ref> [2, 3] </ref> and Rivest and Schapire [22] that were discussed previously, the following is also known: Rivest and Schapire [23] show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. <p> Since the task of learning becomes harder as approaches 1=2, and ff approaches 0, we allow the running algorithm to depend polynomially on 1=ff, as well as on n and log (1=ffi). 3 Exact Learning with Reset In this section we describe a simple variant of Angluin's algorithm <ref> [3] </ref> for learning deterministic finite automata. The algorithm works in the setting where the learner has means of a reset. <p> T def = frow T (r i ) j r i 2 R; 8 2 f0; 1g; r i 2 Rg; * t T (row T (r i ); ) = row T (r i ); 0 = row T (); def 7 It is not hard to verify (see <ref> [3] </ref>) that M T is consistent with T in the sense that for every r i 2 R, and for every s j 2 S, M T (r i s j ) = T (r i ; s j ).
Reference: [4] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Since permutation automata have the property that the indegree and outdegree of each node is equal, the underlying automata has small cover time and thus our result can be viewed as a generalization. Angluin <ref> [4] </ref> proves that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang [17] show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries.
Reference: [5] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: number of walks, each of polynomial length, it output a hypothesis c M , which is equivalent to M , i.e., for every string s, c M (s) = M (s). 2.2.2 The noisy model Our assumptions on the noise follow the classification noise model introduced by Angluin and Laird <ref> [5] </ref>. We assume that for some fixed noise rate &lt; 1=2, at each step, with probability 1 the algorithm observes the (correct) output of the state it has reached, and with probability it observes an incorrect output.
Reference: [6] <author> D. Angluin and C. H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: We refer the reader to a survey by Angluin and Smith <ref> [6] </ref>. Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs.
Reference: [7] <author> M. Bender and D. </author> <title> Slonim. The power of team exploration: Two robots can learn unlabeled directed graphs. </title> <booktitle> In Proceedings of the Thirty Fifth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 75-85, </pages> <year> 1994. </year>
Reference-contexts: Angluin [4] proves that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang [17] show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries. Bender and Slonim <ref> [7] </ref> study the related problem of exactly learning directed graphs (which do no have any outputs associated with their nodes). They show that this task can be performed efficiently by two cooperating robots where each robot performs a single walk on the target graph.
Reference: [8] <author> F. Bergando and S. Varricchio. </author> <title> Learning behaviors of automata from multiplicity and equiv-alnece queries. In Algorithms and complexity, </title> <booktitle> Proceedings of the 2nd Italian conference, </booktitle> <pages> pages 54-62, </pages> <year> 1994. </year> <note> To appear in Siam J. of Computing. </note>
Reference-contexts: They also show how their algorithm can be modified and made more efficient if the graph has high conductance [28], where conductance is a measure of the expansion properties of the graph. Bergando and Varricchio <ref> [8] </ref> show that automata with multiplicity can be exactly learned from multiplicity and equivalence queries.
Reference: [9] <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Kaelbling, E. Kokkevis, and O. Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: One major difficulty is that it is not clear how the learner can orient itself since when executing a homing sequence, with high probability it does not observe the correct output sequence. In order to overcome this difficulty, we adapt a "looping" idea presented by Dean et al. <ref> [9] </ref>. Dean et al. study a similar setting in which the noise rate is not fixed but is a function of the current state, and present a learning algorithm for this problem. <p> These are finite automata whose transition function is deterministic, but whose output function is probabilistic. Namely, for any given string, whenever performing the walk corresponding to the string from a certain state, we reach the same state. However, similarly to the model studied by Dean et al. <ref> [9] </ref>, the output observed each time is determined by the probabilistic process of flipping a coin with a bias that depends on the state reached. <p> In addition to the work of Dean et al. <ref> [9] </ref> which was previously mentioned, the following works consider the case when the labels of the examples are assumed to be noisy. In [25], an algorithm is given for PAC-learning DFAs with membership queries in the presence of persistent noise. <p> Next we assume that the algorithm has no means of a reset, but instead has a homing sequence h. Clearly, in a single execution of h, with high probability the output sequence will be erroneous. We thus adapt a technique that was used in <ref> [9] </ref>.
Reference: [10] <author> F. Ergun, S. Ravikumar, and R. Rubinfeld. </author> <title> On learning bounded-width branching programs. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 361-368, </pages> <year> 1995. </year>
Reference-contexts: Learning algorithms for several special classes of automata have been studied in this setting: Li and Vazirani [20] give several examples of regular languages that can be learned efficiently, including 1-letter languages. In <ref> [10] </ref> a learning algorithm is given for languages accepted by width-2 branching programs that are read-once and leveled (a special case of DFAs). Schapire and Warmuth [27] have shown (see also [10]) that the problem of learning width-3 (read-once and leveled) branching programs is as hard as learning DNF, and they <p> In <ref> [10] </ref> a learning algorithm is given for languages accepted by width-2 branching programs that are read-once and leveled (a special case of DFAs). Schapire and Warmuth [27] have shown (see also [10]) that the problem of learning width-3 (read-once and leveled) branching programs is as hard as learning DNF, and they also observe that learning width-5 (read-once and leveled) branching programs is hard under certain number theoretical assumptions.
Reference: [11] <author> L. Fortnow and D. Whang. </author> <title> Optimality and domination in repeated games with bounded players. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 741-749, </pages> <year> 1994. </year>
Reference-contexts: If M is known to the player, then it is not hard to prove that the player can find an optimal cycle strategy efficiently using dynamic programming. However, if M is not known to the player, then Fortnow and Whang <ref> [11] </ref> show, using the same combination-lock automata argument of Angluin [2], that it is hard to find an optimal strategy in the case of a general game 5 . <p> When the underlying game is penny matching, Fortnow and Whang <ref> [11] </ref> describe an algorithm that finds an optimal strategy efficiently, using ideas from Rivest and Schapire's [22] learning algorithm (but without actually learning the automaton). 4 hard when only given access to random examples.
Reference: [12] <author> M. Frazier, S. Goldman, N. Mishra, and L. Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 328-339, </pages> <year> 1994. </year> <note> To appear in Journal of Computer Systems Science. </note>
Reference-contexts: In [25], an algorithm is given for PAC-learning DFAs with membership queries in the presence of persistent noise. In <ref> [12] </ref>, an algorithm is given for learning DFAs by blurry concepts. 2 Preliminaries 2.1 Basic Definitions Let M be the deterministic finite state automaton (DFA) we would like to learn.
Reference: [13] <author> Y. Freund, M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, and R. Schapire. </author> <title> Efficient algorithms for learning to play repeated games against computationally bounded adversaries. </title> <booktitle> In Proceedings of the Thirty Seventh Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 332-341, </pages> <year> 1996. </year>
Reference-contexts: However, when the learner does not have means of a reset, and thus performs a single walk on M , we know of no natural notion of approximately correct learning. In recent work of Freund et al. <ref> [13] </ref> our results have been improved as follows. Freund et al. consider the problem of learning probabilistic output automata. These are finite automata whose transition function is deterministic, but whose output function is probabilistic. <p> In the case when the biases at each state are either or 1 for some 0 &lt; 1=2, this is essentially the problem of learning deterministic automata in the presence of noise, for which we give an algorithm in this paper. In <ref> [13] </ref>, a learning algorithm is given that runs in time polynomial in the cover time of the target automaton, with no restrictions on the biases at each state.
Reference: [14] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Schapire and Warmuth [27] have shown (see also [10]) that the problem of learning width-3 (read-once and leveled) branching programs is as hard as learning DNF, and they also observe that learning width-5 (read-once and leveled) branching programs is hard under certain number theoretical assumptions. In <ref> [14] </ref> it is shown how to learn typical automata (automata in which the underlying graph is arbitrary, but the accept/reject labels on the states are chosen randomly) by passive learning (the edge traversed by the robot is chosen randomly) in a type of mistake bound model.
Reference: [15] <author> I. Gilboa and D. Samet. </author> <title> Bounded versus unbounded rationality: The tyranny of the weak. </title> <journal> Games and Economic Behavior, </journal> <volume> 1(3) </volume> <pages> 213-221, </pages> <year> 1989. </year>
Reference-contexts: However, our algorithms can easily be 3 It is known <ref> [15] </ref> that there exist optimal strategies in which the player simply forces the opponent DFA M to follow a cycle along the nodes of M 's underlying graph.
Reference: [16] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Since we have less than n 2 pairs, if L = ((1=) 2 (1=ff) 2 log (n=ffi 0 )), then by Hoeffding's inequality <ref> [16] </ref>, with probability at least 1 ffi 0 , for every pair i; j, jd ij E [d ij ]j &lt; ff, and hence jd min 2 (1 )j &lt; 2ff. It directly follows (see [25]) that j^ j &lt; .
Reference: [17] <author> O. Ibarra and T. Jiang. </author> <title> Learning regular languaages from counterexamples. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-385, </pages> <year> 1988. </year>
Reference-contexts: Angluin [4] proves that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang <ref> [17] </ref> show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries. Bender and Slonim [7] study the related problem of exactly learning directed graphs (which do no have any outputs associated with their nodes).
Reference: [18] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year> <note> An extended abstract of this paper appeared in STOC89. </note>
Reference-contexts: These automata can be exactly learned when given access to an equivalence oracle and an oracle which for any given string returns the probability that this string reaches an accepting state. As for non-exact (approximate) learning, without the aid of queries, Kearns and Valiant <ref> [18] </ref> show that under certain number theoretical assumptions, the problem of PAC learning DFAs is modified to adapt to this difference. 5 For certain games, such as penny matching (where the player gets positive payoff if and only if it matches the opponent's action), the combination-lock argument cannot be applied.
Reference: [19] <author> Z. Kohavi. </author> <title> Switching and Finite Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: It is not hard to verify (cf. <ref> [19] </ref>) that every DFA has a homing sequence of length at most quadratic in its size.
Reference: [20] <author> M. Li and U. Vazirani. </author> <title> On the learnability of finite automata. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 359-370, </pages> <year> 1988. </year> <month> 22 </month>
Reference-contexts: Learning algorithms for several special classes of automata have been studied in this setting: Li and Vazirani <ref> [20] </ref> give several examples of regular languages that can be learned efficiently, including 1-letter languages. In [10] a learning algorithm is given for languages accepted by width-2 branching programs that are read-once and leveled (a special case of DFAs).
Reference: [21] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <address> first edition, </address> <year> 1995. </year>
Reference-contexts: It is known that a graph has polynomial cover time exactly when the probability assigned by the stationary distribution to each edge 2 is at least an inverse polynomial in the size of the graph (this can be inferred from the results described in <ref> [21] </ref>). Several natural classes of directed graphs are known to have this property. One important such class is the class of graphs in which the indegree of each node is equal to the outdegree [1].
Reference: [22] <author> R. Rivest and R. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: Our results are easily extendible to larger alphabets. In our algorithms we apply ideas from the no-reset learning algorithm of Rivest and Schapire <ref> [22] </ref>, which in turn uses Angluin's algorithm [3] as a subroutine. Angluin's algorithm is an algorithm for exactly learning automata from a teacher that can answer both membership queries and equivalence queries. Note that having a teacher which answers membership queries is equivalent to having the means of a reset. <p> As in <ref> [22] </ref>, we use a homing sequence to overcome the absence of a reset. Informally, a homing sequence is a sequence such that whenever it is executed, the corresponding output sequence observed uniquely determines the final state reached. As was shown in [22], if a homing sequence is known, learning algorithms that <p> As in <ref> [22] </ref>, we use a homing sequence to overcome the absence of a reset. Informally, a homing sequence is a sequence such that whenever it is executed, the corresponding output sequence observed uniquely determines the final state reached. As was shown in [22], if a homing sequence is known, learning algorithms that use a reset can be easily converted into learning algorithms that do not use a reset. <p> We refer the reader to a survey by Angluin and Smith [6]. Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs. In addition to the work of Angluin [2, 3] and Rivest and Schapire <ref> [22] </ref> that were discussed previously, the following is also known: Rivest and Schapire [23] show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. <p> When the underlying game is penny matching, Fortnow and Whang [11] describe an algorithm that finds an optimal strategy efficiently, using ideas from Rivest and Schapire's <ref> [22] </ref> learning algorithm (but without actually learning the automaton). 4 hard when only given access to random examples. Learning algorithms for several special classes of automata have been studied in this setting: Li and Vazirani [20] give several examples of regular languages that can be learned efficiently, including 1-letter languages. <p> This algorithm closely follows Rivest and Schapire's learning algorithm <ref> [22] </ref>. However, we use new techniques that exploit the small cover time of the automaton in place of relying on a teacher who supplies us with counterexamples to incorrect hypotheses. We name the algorithm Exact-Learn, and its pseudo-code appears in Figure 3. <p> The main problem encountered when the learner does not have means of a reset is that it cannot simply orient itself whenever needed by returning to the starting state. We thus need an alternative way by which the learner can orient itself. As in <ref> [22] </ref>, we overcome the absence of a reset by the use of a homing sequence. A homing sequence is a sequence such that whenever it is executed, the corresponding output sequence observed uniquely determines the final state reached. <p> (h) if T is consistent but not closed, then discard ELRR ; Exact-Learn-with-Reset in which given an integer N , each walk to fill in an entry in the table is repeated N times and only if a single output is observed, then this output is entered. 6 As in <ref> [22] </ref>, we actually need not discard all copies and restart the algorithm, but we may only discard the copy in which the disagreement was found, and construct an adaptive homing sequence which results in a more efficient algorithm. <p> The total running time is hence O 3 3 We have thus proven that: Theorem 2 Algorithm Exact-Learn is an exact learning algorithm for DFAs, and its running time is O n 5 (C (M )) log 4 (n=ffi) . As mentioned previously, Rivest and Schapire <ref> [22] </ref> give an exact learning algorithm that runs in time polynomial in n and log (1=ffi) and does not depend on any other parameter related to 7 Note that each such entry is uniquely determined by the current h, the initial random walks which label the rows of the corresponding tables,
Reference: [23] <author> R. Rivest and R. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 43(3) </volume> <pages> 555-589, </pages> <year> 1994. </year>
Reference-contexts: Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs. In addition to the work of Angluin [2, 3] and Rivest and Schapire [22] that were discussed previously, the following is also known: Rivest and Schapire <ref> [23] </ref> show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries.
Reference: [24] <author> R. Rivest and D. Zuckermann. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: Rivest and Zuckerman <ref> [24] </ref> construct a pair of automata which both have small cover time, but for which the probability of randomly guessing a sequence which distinguishes between the automata is exponentially small.
Reference: [25] <author> D. Ron and R. Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 149-185, </pages> <year> 1995. </year>
Reference-contexts: In addition to the work of Dean et al. [9] which was previously mentioned, the following works consider the case when the labels of the examples are assumed to be noisy. In <ref> [25] </ref>, an algorithm is given for PAC-learning DFAs with membership queries in the presence of persistent noise. In [12], an algorithm is given for learning DFAs by blurry concepts. 2 Preliminaries 2.1 Basic Definitions Let M be the deterministic finite state automaton (DFA) we would like to learn. <p> This is done by running Procedure Estimate-Noise-Rate whose pseudo-code appears in Figure 4, and which is analyzed in the following lemma. A very similar procedure was described in <ref> [25] </ref>. Lemma 5.1 For any given ffi 0 &gt; 0, and &gt; 0, after time polynomial in log (1=ffi 0 ), 1=, n, and 1=ff, Procedure Estimate-Noise-Rate outputs an approximation ^ of , such that with probability at least 1 ffi 0 , j^ j &lt; . <p> It directly follows (see <ref> [25] </ref>) that j^ j &lt; . We thus assume from here on that we have a good approximation, ^, of .
Reference: [26] <author> Y. Sakakibara. </author> <title> On learning from queries and couterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <volume> 37 </volume> <pages> 279-284, </pages> <year> 1991. </year>
Reference-contexts: In the noisy setting the learning problem becomes harder since the outputs observed may be erroneous. If the learner has means of a reset then the problem can easily be solved <ref> [26] </ref> by running the noise-free algorithm and repeating each walk a large enough number of times so that the majority output observed is the correct output. However, when the learner does not have means 2 of a reset then we encounter several difficulties.
Reference: [27] <author> R. Schapire and M. Warmuth. </author> <note> Presented at COLT90 rump session, </note> <year> 1990. </year>
Reference-contexts: In [10] a learning algorithm is given for languages accepted by width-2 branching programs that are read-once and leveled (a special case of DFAs). Schapire and Warmuth <ref> [27] </ref> have shown (see also [10]) that the problem of learning width-3 (read-once and leveled) branching programs is as hard as learning DNF, and they also observe that learning width-5 (read-once and leveled) branching programs is hard under certain number theoretical assumptions.
Reference: [28] <author> A. Sinclair and M. Jerrum. </author> <title> Approximate counting, uniform generation, and rapidly mixing Markov chains. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 93-13, </pages> <year> 1989. </year>
Reference-contexts: They also show how their algorithm can be modified and made more efficient if the graph has high conductance <ref> [28] </ref>, where conductance is a measure of the expansion properties of the graph. Bergando and Varricchio [8] show that automata with multiplicity can be exactly learned from multiplicity and equivalence queries.
References-found: 28

