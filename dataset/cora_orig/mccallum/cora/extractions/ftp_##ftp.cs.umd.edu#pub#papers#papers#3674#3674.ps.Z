URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3674/3674.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Final Iterations in Interior Point Methods Preconditioned Conjugate Gradients and Modified Search Directions  
Author: Weichung Wang 
Date: August 4, 1996  
Abstract: In this article we consider modified search directions in the endgame of interior point methods for linear programming. In this stage, the normal equations determining the search directions become ill-conditioned. The modified search directions are computed by solving perturbed systems in which the systems may be solved efficiently by the preconditioned conjugate gradient solver. We prove the convergence of the interior point methods using the modified search directions and show that each barrier problem is solved with a superlinear convergence rate. A variation of Cholesky factorization is presented for computing a better preconditioner when the normal equations are ill-conditioned. These ideas have been implemented successfully and the numerical results show that the algorithms enhance the performance of the preconditioned conjugate gradients-based interior point methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Carolan, J. Hill, J. Kennington, S. Niemi, and S. Wichmann. </author> <title> An empirical evaluation of the KORBX algorithms for military airlift applications. </title> <journal> Operations Research, </journal> <volume> 38(2) </volume> <pages> 240-248, </pages> <year> 1990. </year>
Reference-contexts: Without doing so, neither method terminates successfully, for the matrix AfiA T is very ill-conditioned. 5.2. The "Kennington" problems Another large problem set found in the NETLIB site is the "Kennington" problems used by Carolan, Hill, Kennington, Niemi, and Wichmann <ref> [1] </ref>. We present the problems from the set containing 25; 000 to 370; 000 nonzero elements in the matrix A. Table 4 and Table 5 give the statistics and results of these problems, respectively.
Reference: [2] <author> Joseph Czyzyk, Sanjay Mehrotra, and Stephen J. Wright. </author> <title> PCx user guide. </title> <type> Technical Report OTC 96/01, </type> <institution> Optimization Technology Center, Argonne National Laboratory and Northwestern University, </institution> <year> 1996. </year>
Reference-contexts: We survey some other related works. Many papers (i.e. [11], [25], and [15]) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) <ref> [2] </ref>, Zhang (LIPSOL) [26], and other researchers. Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method.
Reference: [3] <author> J. E. Dennis and Jorge J. </author> <title> More. A characterization of superlinear convergence and its application to quasi-newton methods. </title> <journal> Mathematics of Computation, </journal> <volume> 28(126) </volume> <pages> 549-560, </pages> <year> 1974. </year>
Reference-contexts: This completes the proof. Lemma 8 and 9 imply that full Newton step may be taken when fx j g approaches x fl . A classical result by Dennis and More <ref> [3] </ref> shown in Lemma 10 leads to Theorem 11 establishing the convergence rate. Lemma 10 (Dennis and More [3]). <p> This completes the proof. Lemma 8 and 9 imply that full Newton step may be taken when fx j g approaches x fl . A classical result by Dennis and More <ref> [3] </ref> shown in Lemma 10 leads to Theorem 11 establishing the convergence rate. Lemma 10 (Dennis and More [3]). Let F : R p ! R p be differentiable in the open convex set D in R p , and assume that for some s fl in D, F 0 is continuous at s fl and F 0 (s fl ) is nonsingular.
Reference: [4] <author> J. E. Dennis and Jorge J. </author> <title> More. Quasi-newton methods, motivation and theory. </title> <journal> SIAM Review, </journal> <volume> 19(1) </volume> <pages> 46-89, </pages> <year> 1977. </year>
Reference-contexts: We first show that the full "Newton" step may be taken when x j is close to the optima x fl by showing that the following lemma is applicable. Lemma 8 (Dennis and More <ref> [4] </ref>).
Reference: [5] <author> A. V. Fiacco and G. P. McCormick. </author> <title> Nonlinear Programming : Sequential Unconstrained Minimization Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year> <booktitle> Reprint : Volume 4 of SIAM Classics in Applied Mathematics, </booktitle> <publisher> SIAM Publications, </publisher> <address> Philadel-phia, PA 19104-2688, USA, </address> <year> 1990. </year>
Reference-contexts: Once we prove that an iterative method converges to the solution of (2) for a fixed , the global convergence of the interior point method, Algorithm 1, follows from the classical results by Fiacco and McCormick <ref> [5] </ref>. We first consider two lemmas to be used for proving the main convergence theorem. Lemma 2. Let B (x) = c T x P n i=1 ln i and M (x; ) = B (x) + kAx bk 1 , where is a positive number.
Reference: [6] <author> Anders Forsgren, Philip E. Gill, and Joseph R. Shinnerl. </author> <title> Stability of symmetric ill-conditioned systems arising in interior methods for constrained optimization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 17 </volume> <pages> 187-211, </pages> <year> 1996. </year>
Reference-contexts: Hough and Vavasis [12] considered weighted least-squares problems with a highly ill-conditioned weight matrix. They proposed a complete orthogonal decomposition algorithm which is stable in the sense that its forward error bound is independent of the matrix fi. In <ref> [6] </ref>, Forsgren, Gill, and Shinnerl presented a perturbation analysis of a class of symmetric diagonally ill-conditioned systems and gave a rounding-error analysis for symmetric indefinite matrix factorization. 3 In the next section, we discuss ideas for perturbing the normal equations to obtain modified search directions and then propose an algorithm based
Reference: [7] <author> Roland W. Freund and Florian Jarre. </author> <title> A QMR-based interior-point algorithm for solving linear programs. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories and Institut fur Angewandte Mathematik und Statistik, </institution> <year> 1995. </year>
Reference-contexts: Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method. See, for example, Freund and Jarre <ref> [7] </ref>, Portugal, Resende, Veiga, and Judice [18], and Mehrotra and Wang [16]. Mizuno and Jarre [17] proposed and further analyzed an infeasible-interior-point algorithm using inexact solutions of the reduced KKT system as a search directions. <p> The NETLIB problems We first present our numerical results on the NETLIB problem collection [8], a standard linear programming test problem set. Small problems have a relatively small cost for forming and factoring the coefficient matrix in the normal equations, as mentioned in <ref> [7] </ref> and [23], so we do not expect that a interior point algorithm based on iterative solvers may prevail over a direct solver based algorithm.
Reference: [8] <author> D. M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> Mathematical Programming Soc. </journal> <note> COAL Newsletter, </note> <year> 1985. </year>
Reference-contexts: Table 1 illustrates the performance of the preconditioned conjugate gradient solver in the last values. The original and the perturbed normal equations are solved for the problems pilot and pilot87 from the NETLIB collection <ref> [8] </ref>. The number of preconditioned conjugate gradient iterations and the time for forming and solving the normal equations (in seconds) are compared for both approaches. The preconditioned conjugate gradient solvers use the same stopping criterion. The artificial variables are kept in pilot87. <p> The timings reported are CPU time in seconds. Since all three codes use the same preprocessor HPREP, we omit the preprocessing time. All the statistical tables in the section are extracted from [23]. 5.1. The NETLIB problems We first present our numerical results on the NETLIB problem collection <ref> [8] </ref>, a standard linear programming test problem set. <p> Table 2 shows the characteristics of the tested NETLIB problems. The numbers of rows, columns, and nonzeros of coefficient matrix A are reported. The numbers are obtained from output of the preprocessor HPREP and may not be identical with the data in <ref> [8] </ref>. Only the nonzero elements in the lower sub-diagonal part of AA T and L are counted and tabulated.
Reference: [9] <author> Philip E. Gill and Walter Murray. </author> <title> Newton-type methods for unconstrained and linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 7 </volume> <pages> 311-350, </pages> <year> 1974. </year>
Reference-contexts: Initialize pii tiny a positive tiny number. do (i = 1 : m) Determine P ii by the modified Cholesky factorization in <ref> [9] </ref>. if (P ii pii tiny) Set P ii = 0. <p> In contrast, Adap1 uses only the OB1-R Cholesky factorization. * Adap2 allows zero in the diagonal Cholesky factor P while Adap1 can not handle the situation. Adap2 uses a portion of the modified Cholesky factor by Gill and Murray <ref> [9] </ref>. See [22, Chap. 5] for details. Table 6 compares Adap1 and Adap2 in the costly problems that take Adap1 more than 1; 500 seconds to solve. Both algorithms perform similarly for other cheaper problems not listed.
Reference: [10] <author> Philip E. Gill, Walter Murray, Dulce B. Ponceleon, and Michael A. Saunders. </author> <title> Primal-dual methods for linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 70 </volume> <pages> 251-277, </pages> <year> 1995. </year>
Reference-contexts: We also modify the y so that i 2 S Y . Other parts of the algorithm are similar to standard interior point methods. We mention that Algorithm 1 is similar to an algorithm recently proposed by Gill, Murray, Ponceleon, and Saunders <ref> [10] </ref>. Their algorithm allows the variables y and z to be chosen arbitrarily within two bounded sets. Our algorithm, in contrast, explicitly states the way we choose y and z and allows the matrix Z and the vector z to differ. <p> Choose a positive k &lt; k1 . end if 2.2. Convergence analysis We prove the global convergence of Algorithm 1 and establish the rate of convergence in the (inner) Newton iterations. To prove the convergence of Algorithm 1, we adopt the procedure described by Gill et al. <ref> [10] </ref>. Lemmas and theorems are similar to the ones in [10]; some proofs are different [22]. <p> Convergence analysis We prove the global convergence of Algorithm 1 and establish the rate of convergence in the (inner) Newton iterations. To prove the convergence of Algorithm 1, we adopt the procedure described by Gill et al. <ref> [10] </ref>. Lemmas and theorems are similar to the ones in [10]; some proofs are different [22]. Once we prove that an iterative method converges to the solution of (2) for a fixed , the global convergence of the interior point method, Algorithm 1, follows from the classical results by Fiacco and McCormick [5].
Reference: [11] <author> Clovis C. Gonzaga. </author> <title> Path-following methods for linear programming. </title> <journal> SIAM Review, </journal> <volume> 34(2) </volume> <pages> 167-224, </pages> <month> June </month> <year> 1992. </year> <month> 36 </month>
Reference-contexts: Numerical experiments demonstrate that the timing may be improved by using preconditioned conjugate gradients through the whole interior point method process and using the modified search directions in the endgame. We survey some other related works. Many papers (i.e. <ref> [11] </ref>, [25], and [15]) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) [2], Zhang (LIPSOL) [26], and other researchers.
Reference: [12] <author> Patricia D. Hough and Stephen A. Vavasis. </author> <title> Complete orthogonal decomposition for weighted least squares. </title> <institution> Center of Applied Mathematics and Department of Computer Science, Cornell University, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: On the other hand, many recent studies concentrated on the stability of the highly ill-conditioned systems which may be found in the endgame of interior point methods. Hough and Vavasis <ref> [12] </ref> considered weighted least-squares problems with a highly ill-conditioned weight matrix. They proposed a complete orthogonal decomposition algorithm which is stable in the sense that its forward error bound is independent of the matrix fi.
Reference: [13] <author> Jr. J. E. Dennis and Robert B.Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Lemma 5. If x is defined as in Lemma 4, then there exist positive numbers ff and fl X such that the sufficient Goldstein-Armijo conditions <ref> [13] </ref> are satisfied. with x + ffx &gt; fl S e. Proof. For simplicity, we adopt the notation M (x) instead of M (x; ) in this proof.
Reference: [14] <author> N. K. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Com-binatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: The modification is closely akin to that proposed by Karmarkar <ref> [14] </ref> in order to reduce the complexity of his interior point method to O (n 2:5 ) by updating a matrix rather than recomputing it.
Reference: [15] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> Computational experience with a primal-dual interior point method for linear programming. Linear Algebra and Its Application, </title> <booktitle> 152 </booktitle> <pages> 191-222, </pages> <year> 1991. </year>
Reference-contexts: A superlinear convergence rate of the iterations is also proved. All the ideas are implemented by modifying a well-coded direct method based interior point method program, OB1-R <ref> [15] </ref>. Numerical experiments demonstrate that the timing may be improved by using preconditioned conjugate gradients through the whole interior point method process and using the modified search directions in the endgame. We survey some other related works. Many papers (i.e. [11], [25], and [15]) address theoretical and implementation aspects of interior <p> method based interior point method program, OB1-R <ref> [15] </ref>. Numerical experiments demonstrate that the timing may be improved by using preconditioned conjugate gradients through the whole interior point method process and using the modified search directions in the endgame. We survey some other related works. Many papers (i.e. [11], [25], and [15]) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) [2], Zhang (LIPSOL) [26], and other researchers. <p> We survey some other related works. Many papers (i.e. [11], [25], and <ref> [15] </ref>) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) [2], Zhang (LIPSOL) [26], and other researchers. Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method. <p> Restart the PCG iteration. end if end until if [(UseRefact = True) and (pcg itn 5)] then UseStdChol False else if [(UseRefact = True) and (pcg itn 50)] then UseDirect True end if 5. Numerical results We modify OB1-R, by Lustig, Marsten, and Shanno <ref> [15] </ref> and dated December 1989, to implement Algorithm 17 and 18 (Adap2). We choose some problems tested in [23] to make comparisons with the computational results of OB1-R and the adaptive algorithm (Adap1) reported in [23]. All the algorithms are coded in FORTRAN using double precision arithmetic.
Reference: [16] <author> Sanjay Mehrotra and Jen-Shan Wang. </author> <title> Conjugate gradient based implementation of interior point methods for network flow problems. </title> <type> Technical Report 95-70.1, </type> <institution> Department of Industrial Engineering and Management Sciences, Northwestern University, </institution> <address> Evanston, IL 60208-3119, U.S.A., </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method. See, for example, Freund and Jarre [7], Portugal, Resende, Veiga, and Judice [18], and Mehrotra and Wang <ref> [16] </ref>. Mizuno and Jarre [17] proposed and further analyzed an infeasible-interior-point algorithm using inexact solutions of the reduced KKT system as a search directions.
Reference: [17] <author> Shinji Mizuno and Florian Jarre. </author> <title> Global and polynomial-time convergence of an infeasible-interior-point algorithm using inexact computation. May be found in the IPM archive, </title> <month> April </month> <year> 1996. </year>
Reference-contexts: See, for example, Freund and Jarre [7], Portugal, Resende, Veiga, and Judice [18], and Mehrotra and Wang [16]. Mizuno and Jarre <ref> [17] </ref> proposed and further analyzed an infeasible-interior-point algorithm using inexact solutions of the reduced KKT system as a search directions. On the other hand, many recent studies concentrated on the stability of the highly ill-conditioned systems which may be found in the endgame of interior point methods.
Reference: [18] <author> L. F. Portugal, M. G. C. Resende, G. Veiga, and J. J. Judice. </author> <title> A truncated primal-infeasible dual-feasible network interior point method. </title> <month> November </month> <year> 1994. </year>
Reference-contexts: Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method. See, for example, Freund and Jarre [7], Portugal, Resende, Veiga, and Judice <ref> [18] </ref>, and Mehrotra and Wang [16]. Mizuno and Jarre [17] proposed and further analyzed an infeasible-interior-point algorithm using inexact solutions of the reduced KKT system as a search directions.
Reference: [19] <author> G. W. Stewart. </author> <title> On scaled projections and pseudo-inverses. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 112 </volume> <pages> 189-194, </pages> <year> 1989. </year>
Reference-contexts: We present a theorem showing that the difference between the original search direction y and the modified search direction y approaches zero if the difference between Z and Z goes to zero. First, we introduce a lemma, by Stewart <ref> [19] </ref> and Todd [20] independently, that will be used in the proof of the theorem. Lemma 13. (Stewart [19]) Let D + be the set of all n fi n diagonal matrices with positive diagonal elements. Let A be of full row rank. <p> First, we introduce a lemma, by Stewart <ref> [19] </ref> and Todd [20] independently, that will be used in the proof of the theorem. Lemma 13. (Stewart [19]) Let D + be the set of all n fi n diagonal matrices with positive diagonal elements. Let A be of full row rank.
Reference: [20] <author> Michael J. Todd. </author> <title> A Dantzig-Wolfe-like variant of Karmarkar's interior-point linear programming algorithm. </title> <journal> Operation Research, </journal> <volume> 38 </volume> <pages> 1006-1018, </pages> <year> 1990. </year>
Reference-contexts: We present a theorem showing that the difference between the original search direction y and the modified search direction y approaches zero if the difference between Z and Z goes to zero. First, we introduce a lemma, by Stewart [19] and Todd <ref> [20] </ref> independently, that will be used in the proof of the theorem. Lemma 13. (Stewart [19]) Let D + be the set of all n fi n diagonal matrices with positive diagonal elements. Let A be of full row rank.
Reference: [21] <author> Robert J. Vanderbei. </author> <title> LOQO : An interior point code for quadratic programming. </title> <institution> Program in Statistics and Operations Research, Princeton University. rvdb@princeton.edu, </institution> <year> 1995. </year>
Reference-contexts: Adap2 takes one addition value in pilot87, achieves a slightly smaller relative duality gap, and uses less time. In the problem greenbea, both algorithms stop unsuccessfully since they fail to converge with a small duality gaps. The problem, as mentioned in <ref> [21] </ref>, is difficult to solve by interior point methods. On the problem d6cube, our algorithm attains a duality gap two orders smaller and is a little quicker. Performance of the two algorithms is similar for the problems taking four minutes or less.
Reference: [22] <author> Weichung Wang. </author> <title> Iterative methods in interior point algorithms for linear programming. </title> <type> Ph.d. dissertation, </type> <institution> Applied Mathematics Program, University of Maryland, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: To prove the convergence of Algorithm 1, we adopt the procedure described by Gill et al. [10]. Lemmas and theorems are similar to the ones in [10]; some proofs are different <ref> [22] </ref>. Once we prove that an iterative method converges to the solution of (2) for a fixed , the global convergence of the interior point method, Algorithm 1, follows from the classical results by Fiacco and McCormick [5]. <p> In contrast, Adap1 uses only the OB1-R Cholesky factorization. * Adap2 allows zero in the diagonal Cholesky factor P while Adap1 can not handle the situation. Adap2 uses a portion of the modified Cholesky factor by Gill and Murray [9]. See <ref> [22, Chap. 5] </ref> for details. Table 6 compares Adap1 and Adap2 in the costly problems that take Adap1 more than 1; 500 seconds to solve. Both algorithms perform similarly for other cheaper problems not listed. Adap2 outperforms in all the problems except the problems cre-b and cre-d without artificial variables.
Reference: [23] <author> Weichung Wang and Dianne P. O'Leary. </author> <title> Adaptive use of iterative methods in interior point methods for linear programming. </title> <type> Technical Report CS-TR-3560, </type> <institution> Computer Science Department Report, University of Maryland, </institution> <month> November </month> <year> 1995. </year> <month> 37 </month>
Reference-contexts: To overcome these difficulties, Wang and O'Leary <ref> [23] </ref> recently proposed an algorithm that adaptively chooses either a using direct method or preconditioned conjugate gradients. <p> The adaptive algorithm switches to a direct method whenever P contains a zero element in its main diagonal. This situation is due to ill-conditioning in fi and may be found in the endgame of many linear programming problems. Consequently, though the computational results reported in <ref> [23] </ref> are promising, there is room for improvement. We improve the algorithm in [23] by considering modified search directions in the endgame. <p> This situation is due to ill-conditioning in fi and may be found in the endgame of many linear programming problems. Consequently, though the computational results reported in <ref> [23] </ref> are promising, there is room for improvement. We improve the algorithm in [23] by considering modified search directions in the endgame. When the iterates are close to optimal solutions, we perturb small entries in the slack variables z in the left hand side of equation (3), so that preconditioned conjugate gradients converges rapidly. <p> Numerical results We modify OB1-R, by Lustig, Marsten, and Shanno [15] and dated December 1989, to implement Algorithm 17 and 18 (Adap2). We choose some problems tested in <ref> [23] </ref> to make comparisons with the computational results of OB1-R and the adaptive algorithm (Adap1) reported in [23]. All the algorithms are coded in FORTRAN using double precision arithmetic. The codes are compiled on a SUN SPARCstation 20 containing 64 megabytes main memory and running SunOS Release 4.1.3. <p> Numerical results We modify OB1-R, by Lustig, Marsten, and Shanno [15] and dated December 1989, to implement Algorithm 17 and 18 (Adap2). We choose some problems tested in <ref> [23] </ref> to make comparisons with the computational results of OB1-R and the adaptive algorithm (Adap1) reported in [23]. All the algorithms are coded in FORTRAN using double precision arithmetic. The codes are compiled on a SUN SPARCstation 20 containing 64 megabytes main memory and running SunOS Release 4.1.3. Optimization level -O3 is turned on for compiling the programs. Numerical experiments are performed on the same platform. <p> Numerical experiments are performed on the same platform. The timings reported are CPU time in seconds. Since all three codes use the same preprocessor HPREP, we omit the preprocessing time. All the statistical tables in the section are extracted from <ref> [23] </ref>. 5.1. The NETLIB problems We first present our numerical results on the NETLIB problem collection [8], a standard linear programming test problem set. Small problems have a relatively small cost for forming and factoring the coefficient matrix in the normal equations, as mentioned in [7] and [23], so we do <p> are extracted from <ref> [23] </ref>. 5.1. The NETLIB problems We first present our numerical results on the NETLIB problem collection [8], a standard linear programming test problem set. Small problems have a relatively small cost for forming and factoring the coefficient matrix in the normal equations, as mentioned in [7] and [23], so we do not expect that a interior point algorithm based on iterative solvers may prevail over a direct solver based algorithm. <p> Comparison with the adaptive algorithm We compare the numerical performance of our algorithm with the adaptive algorithm of <ref> [23] </ref>. <p> The resulting coefficient matrix AfiA T is thus more closely related to the preconditioner. We discuss the motivation of the modified search directions and prove the convergence of the interior point method. Numerical results show that the algorithms enhance the performance of OB1-R and the adaptive algorithm in <ref> [23] </ref>. Acknowledgment I thank Dianne P. O'Leary for many helpful discussions and useful suggestions during the preparation of this article. The idea used to prove the superlinear convergence rate in x 2.3 is also attributed to her. 35
Reference: [24] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Claredon Press, Oxford, </publisher> <address> England, </address> <year> 1965. </year>
Reference-contexts: For the first term, we use the fact that, if H 1 N is symmetric, the magnitude of g T N H 1 N N T g is bounded by the largest and smallest eigenvalue of H 1 N times kN T gk 2 2 <ref> [24] </ref>. We therefore obtain g T N H 1 2 ; where c 1 = 1 max , max is the largest eigenvalue of H N , and max &gt; 0 since H N is positive definite.
Reference: [25] <author> M. H. Wright. </author> <title> Interior methods for constrained optimization. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica 1992, </booktitle> <pages> pages 341-407. </pages> <publisher> Cambridge University Press, </publisher> <address> New York, USA, </address> <year> 1992. </year>
Reference-contexts: Numerical experiments demonstrate that the timing may be improved by using preconditioned conjugate gradients through the whole interior point method process and using the modified search directions in the endgame. We survey some other related works. Many papers (i.e. [11], <ref> [25] </ref>, and [15]) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) [2], Zhang (LIPSOL) [26], and other researchers.
Reference: [26] <author> Yin Zhang. </author> <title> Solving large-scale linear programs by interior-point methods under the MATLAB environment. </title> <type> Technical Report TR 96-01, </type> <institution> Department of Mathematics and Statistics, University of Maryland Baltimore County, </institution> <month> February </month> <year> 1996. </year> <month> 38 </month>
Reference-contexts: Many papers (i.e. [11], [25], and [15]) address theoretical and implementation aspects of interior point methods. Direct methods relying on sparse Cholesky factorization were used to solve the normal equations by Lustig, Marsten, and Shanno (OB1-R) [15], Czyzyk, Mehrotra, and Wright (PCx) [2], Zhang (LIPSOL) <ref> [26] </ref>, and other researchers. Iterative methods, in contrast, were also considered, since iterative methods may take advantage the fact that approximate solutions are allowed in the early stage of an interior point method. <p> The strict complementarity implies that, for each i, either i or i is close to zero in the relative interior of a non-singleton solution 4 set. See, for example, <ref> [26] </ref>. The resulting diagonal matrix fi, in which the i-th diagonal entry is fi ii = i consequently contains some very small positive entries and some irregularly distributed large entries corresponding to small i 's.
References-found: 26

