URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/fault.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Phone: 460  
Title: Floating Point Fault Tolerance with Backward Error Assertions  
Author: Daniel Boley Gene H. Golub Samy Makar Nirmal Saxena Edward J. McCluskey 
Address: 4-192 EE/CSci Building Building  Minneapolis, MN 55455 Stanford, CA 94305-2140 Stanford, CA 94305-4055 U.S.A. U.S.A. U.S.A.  
Affiliation: Computer Science Dept. Computer Science Dept. Center for Reliable Computing  Computer Systems Laboratory University of Minnesota Stanford University Stanford University  
Abstract: This paper introduces an assertion scheme based on the backward error analysis for error detection in algorithms that solve dense systems of linear equations, Ax = b. Unlike previous methods, this Backward Error Assertion Model is specifically designed to operate in an environment of floating point arithmetic subject to round-off errors, and can be easily instrumented in a Watchdog processor environment. The complexity of verifying assertions is O(n 2 ) compared to the O(n 3 ) complexity of algorithms solving A x = b. Unlike other proposed error detection methods, this assertion model does not require any encoding of the matrix A . Experimental results under various error models are presented to validate the effectiveness of this assertion scheme. * The work of this author was supported in part by the National Science Foundation under Grant NSF CCR-8821078. The work of this author was supported in part by the Innovative Science and Technology Office of the Strategic Defense Initiative and administered through the Office of Naval Research under Contract No. N0001492-J-1782, and in part by the National Science Foundation under Grant No. MIP-9107760. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Arioli, J. Demmel, I. S. Duff, </author> <title> Solving Sparse Linear Systems with Sparse Backward Error, </title> <journal> SIAM J. Matr. Anal. </journal> <volume> 10, </volume> <pages> pp. 165-190, </pages> <year> 1989. </year>
Reference-contexts: Early work [9] [19] was based on norm-wise bounds, along the lines of the bounds in the previous section. But recent work <ref> [1] </ref> [7] has adopted a component-wise analysis that in many cases yields much tighter bounds on the sizes of the individual components of the residual vector. In this section, we sketch one of these results that is of particular interest as a backward Error Assertion.
Reference: [2] <author> D. L. Boley, G. H. Golub, S. Makar, N. Saxena, E. J. McCluskey, </author> <title> Backward Error Assertions for Checking Solutions to Systems of Linear Equations, Stanford Univ. Numerical Analysis Project, </title> <type> report NA-89-12, </type> <month> November </month> <year> 1989. </year>
Reference-contexts: We check that the computed solution satisfies an a priori error bound for the particular method used. Suitable a priori bounds are given in the Appendix. A preliminary version of this paper appeared as <ref> [2] </ref>. The rest of this paper is organized as follows. We illustrate the checksum scheme with a simple example, which we then use to show a major weakness of this scheme, as it is commonly defined.
Reference: [3] <author> G. Forsythe, C. Moler, </author> <title> Computer Solution of Linear Algebraic Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1967. </year>
Reference-contexts: Suppose we want to solve the system A x = b in 3 decimal digit rounded arithmetic, where A = Q 1.00e-3 1.00 P ; b = Q 1.00 P . The method we use is GE with Partial Pivoting <ref> [3] </ref> to factor the matrix A as the product of a permutation matrix P, a unit lower triangular matrix of multipliers L and an upper triangular matrix U, to get A = P T LU. <p> It is well known <ref> [3] </ref> that the relative error in the solution is bounded by K (A ) . c c E c c /c c A + E c c , where K (A) c c A c c . c c A -1 c c is the condition number of A . <p> The accuracy of solutions computed in floating point arithmetic is guaranteed only indirectly through this relationship <ref> [3] </ref>. Our approach is to use the guarantee as a Backward Error Assertion by checking whether the computed solution meets this guarantee. <p> For this method, the basic steps are as follows: - 7 - Algorithm 1. Solve A x = b for x with GE with partial pivoting and validation of the result. 1. Use GE with Partial Pivoting <ref> [3] </ref> to factor A into PA ~ ~ L c U c , where L c , U c are lower and upper triangular, and P is a permutation. (cost: O (n 3 )) 2. <p> If a slight modification to the software in steps 1 and 2 is allowed, one can monitor this growth and if necessary abort if this growth factor is exceeded. In particular, for Partial Pivoting, it is known <ref> [3] </ref> that at the k -th stage, the maximum possible growth is g = 2 k-1 c c A c c , so that a further check can be had by monitoring this growth during the elimination. <p> Iterative Refinement Iterative refinement is a technique that can be applied to any solution method for systems of linear equations. It can reduce the residual to the minimum possible <ref> [3] </ref> [5]. It yields finer error detection through a tighter backward error bound, and at the same time yields partial error correction through its iterative convergence property, even with just one iteration.
Reference: [4] <author> W. M. Gentleman, H. T. Kung, </author> <title> Matrix Triangularization by Systolic Arrays, </title> <booktitle> in Proc. SPIE 298, Real-Time Signal Processing IV, </booktitle> <pages> pp. 298-303, </pages> <year> 1981. </year>
Reference-contexts: To take a specific example, the systolic array is a parallel processing paradigm which was pioneered by Kung [11], which is particularly well suited for implementation in VLSI, and which has been particularly successful in signal processing applications (see e.g. <ref> [4] </ref> [6] [15] [16]). However, individual processors in a processor array can suffer a hard failure or a transient error (an error that may occur only occasionally or irregularly), giving rise to erroneous results which may be difficult to detect. <p> Specifically, we examine algorithms to solve dense systems of linear equations and linear least squares problems. These algorithms can be easily implemented on a systolic array <ref> [4] </ref> [11] and are fundamental to many methods in signal processing and parameter identification [6].
Reference: [5] <author> G. H. Golub, C. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins, </publisher> <year> 1989. </year>
Reference-contexts: A description of this scheme can be found in many places (cf. [10]). It is well known (e.g. <ref> [5, sec 2.4] </ref>) that floating point arithmetic presents distinctive problems in that computations are no longer exact. Almost all computations suffer from contamination arising out of the round-off error. <p> In what follows, we use the subscript c to denote numerically computed quantities, possibly with errors. The vector and matrix norms we use are defined as follows (cf. <ref> [5, pp53, pp56-7] </ref>) - 6 - i L i O i c c A c c 1 max S c a ij c , c c A c c max S c a ij c , c c A c c F I S c a ij c 2 M 1 <p> Note that the norm of E in (2) can be computed directly in terms of the norms of r c and x c (the left-most equality) without explicitly forming E <ref> [5, p60] </ref>. Also, in the above formula the growth factor g represents the maximum possible value that can occur in a matrix entry during the elimination process. <p> Likewise, we can use the method of Orthogonal Triangularization, also known as the QR Decomposition (cf. <ref> [5] </ref>, sec 5.2.1). In this method, steps 1 and 2 are replaced by Second Update to Algorithm 1. <p> and R c is an upper triangular matrix. (cost: O (n 3 )) 2 Solve triangular system R c x c = Q c T b for x c . (cost: O (n 2 )) Normally Q c is left as a list of Elementary Reflectors known as Householder Transformations <ref> [5, pp195-9] </ref> whose product is Q c . These Householder Transformations are exactly those generated by the method itself. Multiplication by Q c T is accomplished by applying the individual House holder Transformations in reverse order. <p> (2) becomes (A9): x c c c r c c c 2 ec c A c c F (1.18n 2 + 30n). (6) Note that also in this case we can bound the norm of E directly from the norms of r c and x c without explicitly forming E <ref> [5, p60] </ref>. 4. Iterative Refinement Iterative refinement is a technique that can be applied to any solution method for systems of linear equations. It can reduce the residual to the minimum possible [3] [5]. <p> Iterative Refinement Iterative refinement is a technique that can be applied to any solution method for systems of linear equations. It can reduce the residual to the minimum possible [3] <ref> [5] </ref>. It yields finer error detection through a tighter backward error bound, and at the same time yields partial error correction through its iterative convergence property, even with just one iteration. <p> In Figure 3, we show the maximum relative error in any accepted solution (solid line). This is compared with the theoretical bound on the relative error in the solution (dashed line in Figure 3), given by the formula <ref> [5, p82] </ref>, assuming d . K (A )&lt;1: c c Dxc c 1 - d . <p> Since we can model residuals either as errors in the matrix using (1) or in the right hand side, we have chosen to simplify the analysis of <ref> [5] </ref> by choosing the matrix error model. If no transient errors occur, then d . c c A c c will be bounded by (2), (6), (9) as appropriate, in any case O (e). <p> The main computation would proceed without any degradation from the Watchdog processor, unless an error is signaled. The basic tasks in the Assertion processor are Matrix Vector Products and Back-substitutions, which are both parallelizable in their own right <ref> [5] </ref>. In addition, the use of Backward Error Assertions does not preclude the use of any other error detection or correction scheme, such as further use of iterative refinement, replication of the backward error computation, or a checksum based scheme.
Reference: [6] <author> S. Haykin, </author> <title> Adaptive Filter Theory (2nd ed.), </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: To take a specific example, the systolic array is a parallel processing paradigm which was pioneered by Kung [11], which is particularly well suited for implementation in VLSI, and which has been particularly successful in signal processing applications (see e.g. [4] <ref> [6] </ref> [15] [16]). However, individual processors in a processor array can suffer a hard failure or a transient error (an error that may occur only occasionally or irregularly), giving rise to erroneous results which may be difficult to detect. <p> Specifically, we examine algorithms to solve dense systems of linear equations and linear least squares problems. These algorithms can be easily implemented on a systolic array [4] [11] and are fundamental to many methods in signal processing and parameter identification <ref> [6] </ref>.
Reference: [7] <author> N. J. Higham, </author> <title> Iterative Refinement Enhances the Stability of QR Factorization Methods for Solving Linear Equations, </title> <journal> BIT 31, </journal> <pages> pp. 447-468, </pages> <year> 1991. </year>
Reference-contexts: Early work [9] [19] was based on norm-wise bounds, along the lines of the bounds in the previous section. But recent work [1] <ref> [7] </ref> has adopted a component-wise analysis that in many cases yields much tighter bounds on the sizes of the individual components of the residual vector. In this section, we sketch one of these results that is of particular interest as a backward Error Assertion. <p> Form refined solution x I x c - e c . 4. Form new residual r I A x I - b. When QR is used, the bound on the residual r I is given in <ref> [7] </ref> and is as follows. Assume A is a dense, nonsingular matrix, and x c is obtained using the QR decomposition using Householder transformations or Givens rotations. <p> These bounds lead to effective error detection schemes developed in [18] which are applicable to the computations in steps 3-7. The robustness of Backward Error Assertions against undetected errors and false alarms depends on the mathematical theory in <ref> [7] </ref>. The computed solution to the exact set of equations A x = b must satisfy exactly the approximate set of equations (A + E )x = b, where c c E c c is bounded by the backward error bounds (8) or (9).
Reference: [8] <author> K. H. Huang, J. A. Abraham, </author> <title> Algorithm-based fault tolerance for matrix operations, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-33 #6, </volume> <pages> pp. 518-528, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This is discussed further below. Most previous approaches for error detection and correction of any computation have been based on the use of checksums. Many papers <ref> [8] </ref>, [10], [13] have been devoted to the study of checksum schemes for algorithms in floating point arithmetic such as Gaussian Elimination on a matrix A .
Reference: [9] <author> M. Jankowski, H. Wozniakowski, </author> <title> Iterative Refinement Implies Numerical Stability, </title> <journal> BIT 17, </journal> <pages> pp. 303-311, </pages> <year> 1977. </year>
Reference-contexts: Early work <ref> [9] </ref> [19] was based on norm-wise bounds, along the lines of the bounds in the previous section. But recent work [1] [7] has adopted a component-wise analysis that in many cases yields much tighter bounds on the sizes of the individual components of the residual vector.
Reference: [10] <author> J. Y. Jou, J. A. Abraham, </author> <title> Fault-tolerant matrix arithmetic and signal processing on highly concurrent computing structures, </title> <booktitle> Proc. IEEE 74 #5, Special Issue on Fault Tolerance, </booktitle> <pages> pp. 732-741, </pages> <month> May </month> <year> 1986. </year> <month> - 15 </month> - 
Reference-contexts: This is discussed further below. Most previous approaches for error detection and correction of any computation have been based on the use of checksums. Many papers [8], <ref> [10] </ref>, [13] have been devoted to the study of checksum schemes for algorithms in floating point arithmetic such as Gaussian Elimination on a matrix A . <p> A description of this scheme can be found in many places (cf. <ref> [10] </ref>). It is well known (e.g. [5, sec 2.4]) that floating point arithmetic presents distinctive problems in that computations are no longer exact. Almost all computations suffer from contamination arising out of the round-off error. <p> We construct a Checksum matrix H (I c H c ) Q 0 1 c 1 1 P , where the checksum coefficients H c are chosen so that any pair of rows are independent, theoretically allowing the detection of up to 2 errors ([8] <ref> [10] </ref> [13]). (In this case, this trivially reduced to nonsingular.) The row operations are carried out on the extended matrix A w = AH = (A c A c ) = Q 1.00e-3 1.00 c 1.00 1.00 P .
Reference: [11] <author> H. T. Kung, C. E. Leiserson, </author> <title> Systolic Arrays (for VLSI), in Sparse Matrix Proceedings 1978 (I. </title> <editor> S. Duff and G. W. Stewart ed.), </editor> <booktitle> pp. </booktitle> <pages> 256-282, </pages> <publisher> SIAM, </publisher> <address> Philadelphia, 1979,. </address>
Reference-contexts: To take a specific example, the systolic array is a parallel processing paradigm which was pioneered by Kung <ref> [11] </ref>, which is particularly well suited for implementation in VLSI, and which has been particularly successful in signal processing applications (see e.g. [4] [6] [15] [16]). <p> Specifically, we examine algorithms to solve dense systems of linear equations and linear least squares problems. These algorithms can be easily implemented on a systolic array [4] <ref> [11] </ref> and are fundamental to many methods in signal processing and parameter identification [6].
Reference: [12] <author> P. A. Lee, T. Anderson, </author> <title> Fault Tolerance, </title> <booktitle> Principles and Practice (2nd ed.), </booktitle> <publisher> Springer Ver-lag, </publisher> <year> 1990. </year>
Reference-contexts: A Watchdog process is a small process running concurrently with the main computation process that detects errors by monitoring the behavior of the main computation process [14]. If it were possible to carry out the computations in exact arithmetic, then a reversal check <ref> [12] </ref> could be used to verify that the computed solution does indeed exactly satisfy the original set of equations. But floating point computations always involve some error, normally on the order of the unit round-off of the machine. <p> Hence, to detect the presence of errors from hardware faults, it is necessary to distinguish such errors from those that could be caused just by normal round-off errors. Indeed, floating point arithmetic generally precludes the use of a simple reversal check <ref> [12, p106] </ref>. By using a technique (based on iterative refinement) designed to reduce round-off level errors to a minimum, we can automatically correct many errors, whether from round-off or from hardware faults, that are "small" enough.
Reference: [13] <author> F. T. Luk, H. Park, </author> <title> An analysis of algorithm-based fault tolerance, </title> <journal> J. Parallel Distr. Comput. </journal> <volume> 5, </volume> <pages> pp. 172-84, </pages> <year> 1988. </year>
Reference-contexts: This is discussed further below. Most previous approaches for error detection and correction of any computation have been based on the use of checksums. Many papers [8], [10], <ref> [13] </ref> have been devoted to the study of checksum schemes for algorithms in floating point arithmetic such as Gaussian Elimination on a matrix A . <p> We construct a Checksum matrix H (I c H c ) Q 0 1 c 1 1 P , where the checksum coefficients H c are chosen so that any pair of rows are independent, theoretically allowing the detection of up to 2 errors ([8] [10] <ref> [13] </ref>). (In this case, this trivially reduced to nonsingular.) The row operations are carried out on the extended matrix A w = AH = (A c A c ) = Q 1.00e-3 1.00 c 1.00 1.00 P .
Reference: [14] <author> A. Mahmood, E. J. McCluskey, </author> <title> Concurrent Error Detection using Watchdog Processors - a Survey, </title> <journal> IEEE Trans. Comput. </journal> <volume> 37 #2, </volume> <pages> pp. 160-174, </pages> <year> 1988. </year>
Reference-contexts: A Watchdog process is a small process running concurrently with the main computation process that detects errors by monitoring the behavior of the main computation process <ref> [14] </ref>. If it were possible to carry out the computations in exact arithmetic, then a reversal check [12] could be used to verify that the computed solution does indeed exactly satisfy the original set of equations.
Reference: [15] <author> J. G. McWhirter, </author> <title> Recursive Least-Squares Minimization using a Systolic Array, </title> <booktitle> in Proc. SPIE 431, Real-Time Signal Processing VI, </booktitle> <pages> pp. 105-112, </pages> <year> 1983. </year>
Reference-contexts: To take a specific example, the systolic array is a parallel processing paradigm which was pioneered by Kung [11], which is particularly well suited for implementation in VLSI, and which has been particularly successful in signal processing applications (see e.g. [4] [6] <ref> [15] </ref> [16]). However, individual processors in a processor array can suffer a hard failure or a transient error (an error that may occur only occasionally or irregularly), giving rise to erroneous results which may be difficult to detect.
Reference: [16] <author> J. G. McWhirter, </author> <title> Algorithmic Engineering -- an Emerging Discipline, </title> <booktitle> in Proc. SPIE 1152, Advanced Algorithms and Architectures for Signal Processing IV (F. T. Luk ed.), </booktitle> <pages> pp. 2-15, </pages> <year> 1989. </year>
Reference-contexts: To take a specific example, the systolic array is a parallel processing paradigm which was pioneered by Kung [11], which is particularly well suited for implementation in VLSI, and which has been particularly successful in signal processing applications (see e.g. [4] [6] [15] <ref> [16] </ref>). However, individual processors in a processor array can suffer a hard failure or a transient error (an error that may occur only occasionally or irregularly), giving rise to erroneous results which may be difficult to detect.
Reference: [17] <author> H. Park, </author> <title> On multiple error correction in matrix triangularizations using checksum schemes, </title> <journal> J. Parallel Distr. Comput. </journal> <volume> 14, </volume> <pages> pp. 90-97, </pages> <year> 1992. </year>
Reference-contexts: Almost all computations suffer from contamination arising out of the round-off error. Hence any method that attempts to detect or correct errors must account for the fact that some error occurs in normal processing in floating point arithmetic. In <ref> [17] </ref> there is an extensive discussion of the behavior of checksum schemes for detecting or correcting multiple errors when floating point arithmetic is used. <p> However, temporary errors could affect intermediate results that are not stored either in A or in L. Such intermediate results are used to determine - 5 - the order of the rows in pivoting. It has been shown in <ref> [17] </ref> that errors to the matrix entries may also affect the row order, but such errors can be detected by a checksum scheme, even if correction is precluded by the catastrophic cancellation resulting from the incorrect row ordering.
Reference: [18] <author> A. Roy-Chowdhury, P. Banerjee, </author> <title> Tolerance Determination for Algorithm Based Checks using Simple Error Analysis Techniques, </title> <booktitle> in Fault Tolerant Computing Symp. FTCS-23, </booktitle> <pages> pp. 290-298, </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: However, a more efficient approach can be designed using forward error bounds for matrix-matrix and matrix-vector multiplications. These bounds lead to effective error detection schemes developed in <ref> [18] </ref> which are applicable to the computations in steps 3-7. The robustness of Backward Error Assertions against undetected errors and false alarms depends on the mathematical theory in [7]. <p> But some catastrophic errors will never be correctable, so there is always the chance the whole computation will need to be repeated. The Backward Error Assertion Model can easily be combined with the techniques of <ref> [18] </ref> to achieve error detection capability on the entire computation including the Watchdog process. We contrast this to the traditional approach where errors are corrected by first detecting and locating them, and then applying an explicit correction.
Reference: [19] <author> R. D. Skeel, </author> <title> Iterative Refinement Implies Numerical Stability for Gaussian Elimination, </title> <journal> Math. Comp. </journal> <volume> 35, </volume> <pages> pp. 817-832, </pages> <year> 1980. </year>
Reference-contexts: Early work [9] <ref> [19] </ref> was based on norm-wise bounds, along the lines of the bounds in the previous section. But recent work [1] [7] has adopted a component-wise analysis that in many cases yields much tighter bounds on the sizes of the individual components of the residual vector.
Reference: [20] <author> D. C. Sorensen, </author> <title> Analysis of pairwise pivoting in Gaussian elimination, </title> <journal> IEEE Trans. Com-put. </journal> <volume> C-34, </volume> <pages> pp. 274-278, </pages> <year> 1985. </year>
Reference-contexts: The Pivot row is swapped with the k -th row, and then multiples of it are added to rows k +1, . . . , n to annihilate all the entries in column k below the diagonal. We remark that Pairwise Pivoting <ref> [20] </ref> suffers in the same way as Partial Pivoting in this example.
Reference: [21] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: We examine three methods: Gaussian Elimination (GE) with Partial Pivoting, GE with Complete Pivoting, and Orthogonal Triangularization using Householder Transformations (QR Factorization). We also examine the addition of iterative refinement. It is well known from the landmark work of J. H. Wilkinson (e.g. <ref> [21, pp157-160, pp209-215, p236, pp247-252] </ref>) that these methods are all backward stable.
Reference: [22] <author> J. H. Wilkinson, </author> <title> Error analysis of direct methods of matrix inversion, </title> <editor> J. A. C. M. </editor> <volume> 8, </volume> <pages> pp. 281-330, </pages> <year> 1961. </year>
Reference-contexts: Also, in the above formula the growth factor g represents the maximum possible value that can occur in a matrix entry during the elimination process. For Partial Pivoting, the maximum growth is g = 2 n-1 c c A c c , (3) though Wilkinson <ref> [22] </ref> points out that it is extremely rare to encounter a matrix with growth greater that g = 8c c A c c . (4) In those rare cases where this last heuristic bound is exceeded, those cases are exactly the ones in which great improvement in accuracy could be achieved <p> c = Pb for y c and U c z c = y c for z c , then form solution x c Q T z c . (cost: O (n 2 )) Then the bound in step 4 will again be (2), but with a different growth factor g <ref> [22] </ref>: g = 1.8n (1/4)logn . (5) We remark again that for both pivoting strategies, g is a bound on the growth in the matrix elements that can occur during the elimination process.
References-found: 22

