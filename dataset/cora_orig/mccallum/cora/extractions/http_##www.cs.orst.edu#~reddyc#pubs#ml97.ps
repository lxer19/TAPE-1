URL: http://www.cs.orst.edu/~reddyc/pubs/ml97.ps
Refering-URL: http://www.cs.orst.edu/~reddyc/pubs.html
Root-URL: 
Email: freddyc,tadepallig@cs.orst.edu  
Title: Learning Goal-Decomposition Rules using Exercises  
Author: Chandra Reddy Prasad Tadepalli 
Address: Corvallis, OR-97331. USA  
Affiliation: Department of Computer Science Oregon State University,  
Abstract: Exercises are problems ordered in increasing order of difficulty. Teaching problem-solving through exercises is a widely used pedagogic technique. A computational reason for this is that the knowledge gained by solving simple problems is useful in efficiently solving more difficult problems. We adopt this approach of learning from exercises to acquire search-control knowledge in the form of goal-decomposition rules (d-rules). D-rules are first order, and are learned using a new "generalize-and-test" algorithm which is based on inductive logic programming techniques. We demonstrate the feasibility of the approach by applying it in two planning do mains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackerman, P., & Kanfer, R. </author> <year> (1993). </year> <title> Kanfer-Ackerman Air Traffic Control Task c fl CD-ROM Database, Data-Collection Program, and Playback Program. </title> <institution> Dept. of Psychology, Univ. of Minn., Minneapolis, MN. </institution>
Reference-contexts: These d-rules are such that they are applicable to any-sized grid. These results demonstrate that learning from exercises is a feasible approach to learn recursive d-rules. ATC. This domain is a simplified version of the Kanfer-Ackerman air-traffic control task <ref> (Ackerman & Kanfer, 1993) </ref>. The task here is to land a plane that is in a holding position at some holding level onto an appropriate runway. New planes come in from a queue of incoming planes. There are 3 holding levels, each with 4 holding positions.
Reference: <author> Cohen, W. </author> <year> (1995a). </year> <title> Pac-learning non-recursive prolog clauses. </title> <journal> Artificial Intelligence, </journal> <volume> 79 (1), </volume> <pages> 1-38. </pages>
Reference-contexts: Learning d-rules for a single goal is equivalent to learning Horn clauses for a single head literal|i.e., Horn definitions (Reddy & Tadepalli, 1997). Horn programs|each of which is a set of Horn definitions| are shown to be not PAC-learnable from examples alone <ref> (Cohen, 1995a) </ref>. The ordering-the-learning idea of the exercises approach, however, enables the Horn-definition learning algorithms to learn Horn programs also. X-Learn, in fact, demonstrates it in learning d-rules for multiple goals, which are equivalent to Horn programs.
Reference: <author> Cohen, W. </author> <year> (1995b). </year> <title> Pac-learning recursive logic programs: efficient algorithms. </title> <journal> Jl. of AI Research, </journal> <volume> 2, </volume> <pages> 500-539. </pages>
Reference: <author> Cohen, W. </author> <year> (1995c). </year> <title> Pac-learning recursive logic programs: negative results. </title> <journal> Jl. of AI Research, </journal> <volume> 2, </volume> <pages> 541-573. </pages>
Reference-contexts: In any case, in the process, it may squander computing time and available training problems. A set of d-rules could be looked at as first-order function-free Horn programs. In this respect, learning d-rules is an ILP problem. From <ref> (Cohen, 1995c) </ref>, one can infer that this concept class is hard to learn without external help. Our system uses testing to learn this class of concepts.
Reference: <author> DeJong, G., & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 145-176. </pages>
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pp. 128-135. </pages>
Reference: <author> Erol, K., Hendler, J., & Nau, D. </author> <year> (1994). </year> <title> HTN planning: complexity and expressivity. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: For example, going from the ?rfr room to the neighboring ?rg room, the robot needs to open the connecting door and enter, which are primitive operations. D-rules are a specialization of hierarchical transition networks of <ref> (Erol, Hendler, & Nau, 1994) </ref>, the only difference being that the subgoals are totally ordered in d-rules. D-rule representation compares favorably against macro operators and control rules, in terms of computational complexity of planning when there is a loosely coupled set of goals and subgoals. D-rule planner.
Reference: <author> Estlin, T., & Mooney, R. </author> <year> (1996). </year> <title> Multi-strategy learning of search control for partial-order planning. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pp. 843-848. </pages> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: X-Learn, in fact, demonstrates it in learning d-rules for multiple goals, which are equivalent to Horn programs. FOIL is another ILP system (Quinlan, 1990) that is employed for learning control knowledge. DOLPHIN (Zelle & Mooney, 1993), Grasshopper (Leckie & Zuk-erman, 1993) and SCOPE <ref> (Estlin & Mooney, 1996) </ref> are three such systems that embed FOIL for learning control rules. FOIL, as well as GOLEM, are batch-learning algorithms. Therefore, the incremental nature of the exercises approach precludes using such systems for inductive learning in the exercises setting.
Reference: <author> Fikes, R., Hart, P., & Nilsson, N. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3, </volume> <pages> 251-288. </pages>
Reference-contexts: The exercise solver, X-Solver, employs fixed-depth depth-first search to solve the exercises. It uses, along with primitive actions of a domain, previously learned d-rules as operators for the search. Primitive actions of a domain are specified as STRIPS-style operators, and their execution is straightforward <ref> (Fikes, Hart, & Nilsson, 1972) </ref>. <p> To expedite the experiments, we have supplied the solvers with the goal-subgoal hierarchies in both domains, so that X-Solve can focus its search in solving the exercises. SW. This domain is a slight variation of STRIPS world <ref> (Fikes et al., 1972) </ref>. The room configuration we use is a grid, unlike in (Fikes et al., 1972) where it is a random configuration. The rooms are laid out as a grid with doors between neighboring rooms. Each room can have zero or more boxes. <p> SW. This domain is a slight variation of STRIPS world <ref> (Fikes et al., 1972) </ref>. The room configuration we use is a grid, unlike in (Fikes et al., 1972) where it is a random configuration. The rooms are laid out as a grid with doors between neighboring rooms. Each room can have zero or more boxes. There is a robot that can open or close doors of the room that it is in.
Reference: <author> Gil, Y. </author> <year> (1993). </year> <title> Efficient domain-independent experimentation. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 128-134. </pages>
Reference-contexts: In X-Learn, since the learner has control over which examples are generated, it can generate suitable near-miss examples. The way the testing examples are generated and used is somewhat similar to the way Gil in <ref> (Gil, 1993, 1994) </ref> generates and uses experiments in learning operator models. Gil uses them to specialize overgeneral conditions, whereas we use test examples to generalize overspecific conditions. In our case, to eliminate a literal for generalizing a condition, we choose a literal that is already there in the condition.
Reference: <author> Gil, Y. </author> <year> (1994). </year> <title> Learning by experimentation: Incremental refinement of incomplete planning domains. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 87-95. </pages>
Reference: <author> Korf, R. </author> <year> (1987). </year> <title> Depth-first iterative deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 (1), </volume> <pages> 97-109. </pages>
Reference-contexts: Therefore, X-Learn uses iterative-deepening depth-first search (IDS)|i.e., starting with a depth limit and doing DFS until that depth limit is reached, and if no solution is found, increasing the depth limit and re-doing DFS with the new depth limit, and so on <ref> (Korf, 1987) </ref>. IDS also guarantees that the solution, if any, is reached with the fewest number of operator applications. The solver returns the solution plan that solves the exercise problem along with the subgoal sequence used to solve the exercise.
Reference: <author> Laird, J., Rosenbloom, P., & Newell, A. </author> <year> (1986). </year> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 11-46. </pages>
Reference: <author> Leckie, C., & Zukerman, I. </author> <year> (1993). </year> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pp. 1100-1105. </pages>
Reference-contexts: X-Learn, in fact, demonstrates it in learning d-rules for multiple goals, which are equivalent to Horn programs. FOIL is another ILP system (Quinlan, 1990) that is employed for learning control knowledge. DOLPHIN (Zelle & Mooney, 1993), Grasshopper <ref> (Leckie & Zuk-erman, 1993) </ref> and SCOPE (Estlin & Mooney, 1996) are three such systems that embed FOIL for learning control rules. FOIL, as well as GOLEM, are batch-learning algorithms. Therefore, the incremental nature of the exercises approach precludes using such systems for inductive learning in the exercises setting.
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Search Control Knowledge. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA. </address>
Reference-contexts: Section 3 discusses the learning approach and the related algorithms. Section 4 discusses the results of experiments. Section 5 concludes with a discussion of related issues and future work. 2 D-rule Representation Macro operators and control rules are two prominent methods of representing control knowledge <ref> (Minton, 1988) </ref>. D-rules is another method for this purpose. Representation. A d-rule is a 3-tuple hg; c; sgi that decomposes a goal g into a sequence of subgoals sg, provided the condition c holds. The goal component g is a single literal.
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <pages> pp. 368-381. </pages> <month> Ohmsha/Springer-Verlag. </month>
Reference-contexts: Our system uses testing to learn this class of concepts. GOLEM, an ILP system, instead restricts the concept class to determinate clauses, which means in a clause there is a unique binding to all the free variables of a literal, given the bindings of all previous literals <ref> (Muggleton & Feng, 1990) </ref>. In fact, this concept class with the further restriction of keeping the depth of the dependency chain of the variables in a clause constant, is shown to be PAC-learnable from examples (Cohen, 1995b; Dzeroski, Muggleton, & Rus-sell, 1992).
Reference: <author> Natarajan, B. </author> <year> (1989). </year> <title> On learning from exercises. </title> <booktitle> In Proceedings of the Second Workshop on Computational Learning Theory, </booktitle> <pages> pp. 72-87. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The learner has to solve the exercise problems using the bootstrapping method akin to the above-described method followed by a human student. The exercises approach for speedup learning has been used for learning control rules <ref> (Natarajan, 1989) </ref>. In this work, we use exercises for learning goal-decomposition rules (d-rules). The learning of control rules in (Natarajan, 1989) is essentially propositional, whereas learning d-rules is first-order or relational. <p> The exercises approach for speedup learning has been used for learning control rules <ref> (Natarajan, 1989) </ref>. In this work, we use exercises for learning goal-decomposition rules (d-rules). The learning of control rules in (Natarajan, 1989) is essentially propositional, whereas learning d-rules is first-order or relational. Moreover, d-rules can be recursive, which complicates solving of exercise problems: to solve a particular goal, the learner should already know something about solving that very goal. <p> However, in the exercises framework, the burden of solving the input problems is on the learner, unlike in the examples framework where that burden is on the teacher. We have not yet defined what a problem's level of difficulty means. In <ref> (Natarajan, 1989) </ref>, the solution length of a problem is chosen as the level of difficulty|i.e., the problems with higher solution length are more difficult. This, however, does not work for d-rules. A d-rule, depending on the problem it is applied to, may produce varying solution lengths. <p> Combining such examples to form a d-rule enables X-Learn to eliminate unnecessary literals more easily, thus arriving at more concise and general rules. In the future, we would like to establish PAC-learnability of d-rules using exercises, similar to the results for control rules in <ref> (Natarajan, 1989) </ref>. Also, we would like to extend this work to enable learning in the context of real-time planning. Acknowledgments. We gratefully acknowledge the support of ONR under grant# N00014-95-1-0557. We thank the Machine Learning Reading Group at OSU and the anonymous reviewers for their many helpful comments.
Reference: <author> Plotkin, G. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 153-163. </pages> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions of from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: The ordering-the-learning idea of the exercises approach, however, enables the Horn-definition learning algorithms to learn Horn programs also. X-Learn, in fact, demonstrates it in learning d-rules for multiple goals, which are equivalent to Horn programs. FOIL is another ILP system <ref> (Quinlan, 1990) </ref> that is employed for learning control knowledge. DOLPHIN (Zelle & Mooney, 1993), Grasshopper (Leckie & Zuk-erman, 1993) and SCOPE (Estlin & Mooney, 1996) are three such systems that embed FOIL for learning control rules. FOIL, as well as GOLEM, are batch-learning algorithms.
Reference: <author> Reddy, C., & Tadepalli, P. </author> <year> (1997). </year> <title> Learning Horn definitions using equivalence and membership queries. </title> <type> Tech. rep., </type> <institution> Dept. of Comp. Sci., Oregon State Univ. </institution> <note> Submitted to ILP-97. </note>
Reference-contexts: The use of lgg guarantees that a d-rule of the learner and an example are specializations of a target d-rule if and only if their lgg is also a specialization of the same target d-rule <ref> (Reddy & Tadepalli, 1997) </ref>. Hence, we find the lgg of each hypothesis d-rule and the new example, and test it to see if it, indeed, is a specialization of one of the target d-rules. <p> Unfortunately, these restrictions are too limiting on the expressiveness of d-rules to enable efficient planning. Therefore, we circumvent this problem by employing exercises and self-testing. It is shown in <ref> (Reddy & Tadepalli, 1997) </ref> that d-rules for a single goal are PAC-learnable using the algorithm in Figure 4 with a modification|a membership query replacing Test. Learning d-rules for a single goal is equivalent to learning Horn clauses for a single head literal|i.e., Horn definitions (Reddy & Tadepalli, 1997). <p> It is shown in <ref> (Reddy & Tadepalli, 1997) </ref> that d-rules for a single goal are PAC-learnable using the algorithm in Figure 4 with a modification|a membership query replacing Test. Learning d-rules for a single goal is equivalent to learning Horn clauses for a single head literal|i.e., Horn definitions (Reddy & Tadepalli, 1997). Horn programs|each of which is a set of Horn definitions| are shown to be not PAC-learnable from examples alone (Cohen, 1995a). The ordering-the-learning idea of the exercises approach, however, enables the Horn-definition learning algorithms to learn Horn programs also.
Reference: <author> Reddy, C., Tadepalli, P., & Roncagliolo, S. </author> <year> (1996). </year> <title> Theory-guided empirical speedup learning of goal-decomposition rules. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pp. 409-417. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, d-rules can be recursive, which complicates solving of exercise problems: to solve a particular goal, the learner should already know something about solving that very goal. Previously, learnability of d-rules in supervised setting|using solved examples|is demon strated in <ref> (Reddy, Tadepalli, & Roncagliolo, 1996) </ref>. In this work, we instead use exercises to learn d-rules. <p> In the following, we position our work in the context of the related work in those two areas. The speedup learning from examples system that is closely related to our work is ExEL <ref> (Reddy et al., 1996) </ref>. This system learns goal-decomposition rules using examples|problems and their solutions. Unlike X-Learn's generalization algorithm, ExEL's does not use lgg, but uses an ad-hoc method for generalizing a hypothesis d-rule with an example d-rule. Therefore, ExEL's generalization method lacks theoretical justification.
Reference: <author> Ruby, D., & Kibler, D. </author> <year> (1991). </year> <title> Learning subgoal sequences for planning. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: To combat the explosive nature of this process, Gil employs heuristics to choose a literal to add. Similar to our approach in learning goal decompositions, the SteppingStones system also learns subgoal sequences <ref> (Ruby & Kibler, 1991) </ref>. However, Stepping-Stones learns from single examples using an EBL-like method, unlike our system, X-Learn, which utilizes multiple examples to learn a goal decomposition.
Reference: <author> Shavlik, J. </author> <year> (1990). </year> <title> Acquiring recursive and iterative concepts with explanation-based learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 39-70. </pages>
Reference: <author> Zelle, J., & Mooney, R. </author> <year> (1993). </year> <title> Combining FOIL and EBG to speedup logic programs. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pp. 1106-1111. </pages>
Reference-contexts: X-Learn, in fact, demonstrates it in learning d-rules for multiple goals, which are equivalent to Horn programs. FOIL is another ILP system (Quinlan, 1990) that is employed for learning control knowledge. DOLPHIN <ref> (Zelle & Mooney, 1993) </ref>, Grasshopper (Leckie & Zuk-erman, 1993) and SCOPE (Estlin & Mooney, 1996) are three such systems that embed FOIL for learning control rules. FOIL, as well as GOLEM, are batch-learning algorithms.
References-found: 25

