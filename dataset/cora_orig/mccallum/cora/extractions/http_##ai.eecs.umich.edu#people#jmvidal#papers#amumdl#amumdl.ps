URL: http://ai.eecs.umich.edu/people/jmvidal/papers/amumdl/amumdl.ps
Refering-URL: http://ai.eecs.umich.edu/people/jmvidal/papers/amumdl/
Root-URL: http://www.cs.umich.edu
Email: jmvidal@umich.edu  
Title: The Impact of Nested Agent Models in an Information Economy  
Author: Jose M. Vidal and Edmund H. Durfee 
Keyword: Topic Areas: Agent Modeling/Learning, Economic Societies of Agents.  
Address: 1101 Beal Avenue, Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory, University of Michigan  
Abstract: We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. model the other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents. These agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using agent models. Our results show, among other lessons, how savvy buyers can avoid being "cheated" by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akerlof, G. A. </author> <year> 1970. </year> <title> The market for `lemons': Quality uncertainty and the market mechanism. </title> <journal> The Quaterly Journal of Economics 488-500. </journal>
Reference: <author> Atkins, D. E.; Birmingham, W. P.; Durfee, E. H.; Glover, E. J.; Mullen, T.; Rundensteiner, E. A.; Soloway, E.; Vidal, J. M.; Wallace, R.; and Well-man, M. P. </author> <year> 1996. </year> <title> Toward inquiry-based education through interacting software agents. </title> <journal> IEEE Computer. </journal> <note> http://www.computer.org/pubs/computer/- dli/r50069/r50069.htm </note> . 
Reference-contexts: Unfortunately, this seems to be possible only in very restrictive domains (Rosenschein & Zlotkin 1994). Specifically, it is not possible if we situate our agents in an economic society of agents, such as the University of Michigan Digital Library 1 <ref> (Atkins et al. 1996) </ref>(UMDL). Here the agents will be responsible for making their own decisions about when to buy/sell and who to do business with. Market systems (e.g. auctions) will be implemented as part of the UMDL in order to facilitate the transactions. <p> We are also encouraged by the fact that increasing the agents' capabilities changes the system in ways that we can recognize from our everyday economic experience. Some of the agent capabilities shown in this paper are already being implemented into the UMDL <ref> (Atkins et al. 1996) </ref> MAS. Our results showed how sellers with deeper models fare better, in general, even when they produce less valuable goods. This means that we should expect those type of agents to, eventually, be added into the UMDL 7 .
Reference: <author> Axelrod, R. </author> <year> 1996. </year> <title> The evolution of strategies in the iterated prisoner's dilemma. </title> <publisher> Cambridge University Press. forthcoming. </publisher>
Reference: <author> Durfee, E. H.; Gmytrasiewicz, P. J.; and Rosenschein, J. S. </author> <year> 1994. </year> <title> The utility of embedded communications and the emergence of protocols. </title> <booktitle> In Proceedings of the 13th International Distributed Artificial Intelligence Workshop. </booktitle>
Reference: <author> Epstein, J. M., and Axtell, R. L. </author> <year> 1996. </year> <title> Growing Artifical Societies: Social Science from the Bottom Up. </title> <journal> Brookings Institution. </journal> <note> Description at http://- www.brook.edu/pub/books/ARTIFSOC.HTM </note> . 
Reference: <author> Gmytrasiewicz, P. J. </author> <year> 1996. </year> <title> On reasoning about other agents. </title> <editor> In Wooldridge, M.; Muller, J. P.; and Tambe, M., eds., </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We divide the agents into classes that correspond to their modeling capabilities. The hierarchy we present is inspired by RMM <ref> (Gmytrasiewicz 1996) </ref>, but is function-based rather than matrix-based, and includes learning. We start with agents with no models (also referred to as 0-level agents), who must base their actions purely on their inputs and the rewards they receive. They are not aware that there are other agents out there.
Reference: <author> Mullen, T., and Wellman, M. P. </author> <year> 1996. </year> <title> Some issues in the design of market-oriented agents. </title> <editor> In Wooldridge, M.; Muller, J. P.; and Tambe, M., eds., </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Rosenschein, J. S., and Zlotkin, G. </author> <year> 1994. </year> <title> Rules of Encounter. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Unfortunately, this seems to be possible only in very restrictive domains <ref> (Rosenschein & Zlotkin 1994) </ref>. Specifically, it is not possible if we situate our agents in an economic society of agents, such as the University of Michigan Digital Library 1 (Atkins et al. 1996)(UMDL). <p> The rewards, we are finding, start to diminish as the other agents become "smarter". It would be very useful to characterize the type of environments and agent populations that, combined, foster such antisocial behavior (see <ref> (Rosenschein & Zlotkin 1994) </ref>), especially as interest in multi-agent systems grows.
Reference: <author> Russell, S., and Wefald, E. </author> <year> 1991. </year> <title> Do The Right Thing. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Russell, S. </author> <year> 1995. </year> <title> Rationality and intelligence. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 950-957. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: In general, agents get some input, take an action, then receive some reward. This is the same basic framework under which most learning mechanism are presented. We decided to use a form of reinforcement learning <ref> (Sutton 1988) </ref> (Watkins & Dayan 1992) for implementing this kind of learning in our agents, since it is a simple method and the domain is simple enough for it to do a reasonable job.
Reference: <author> Tambe, M., and Rosenbloom, P. S. </author> <year> 1996. </year> <title> Agent tracking in real-time dynamic environments. </title> <editor> In Wooldridge, M.; Muller, J. P.; and Tambe, M., eds., </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Vidal, J. M., and Durfee, E. H. </author> <year> 1995. </year> <title> Task planning agents in the UMDL. </title> <booktitle> In Proceedings of the 1995 Intelligent Agents Workshop. </booktitle> <address> http://ai.eecs.umich.edu/- people/jmvidal/papers/tpa/tpa.html </address> . 
Reference: <author> Vidal, J. M., and Durfee, E. H. </author> <year> 1996. </year> <title> Using recursive agent models effectively. </title> <editor> In Wooldridge, M.; Muller, J. P.; and Tambe, M., eds., </editor> <booktitle> Intelligent Agents Volume II | Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher> <address> http://ai.eecs.umich.edu/- people/jmvidal/papers/lr-rmm2/lr-rmm2.html </address> . 
Reference: <author> Watkins, C. J., and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 279-292. </pages>
Reference-contexts: In general, agents get some input, take an action, then receive some reward. This is the same basic framework under which most learning mechanism are presented. We decided to use a form of reinforcement learning (Sutton 1988) <ref> (Watkins & Dayan 1992) </ref> for implementing this kind of learning in our agents, since it is a simple method and the domain is simple enough for it to do a reasonable job. Both buyers and sellers will use the equations in the next sections for determining what actions to take.
References-found: 15

