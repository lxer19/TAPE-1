URL: http://sdg.lcs.mit.edu/~jchapin/papers/chapin-hotos97.ps.gz
Refering-URL: http://sdg.lcs.mit.edu/~jchapin/publications.html
Root-URL: 
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> E. Bugnion, J. M. Anderson, T. C. Mowry, M. Rosenblum, and M. S. Lam. </author> <title> Compiler-directed page coloring for multiprocessors. ASPLOS VII, available as Operating Systems Review (Dec. </title> <booktitle> 1996), </booktitle> <volume> vol. 30, no. 5, </volume> <editor> p. </editor> <volume> 244255. </volume>
Reference-contexts: The operating system can allocate pages in a way that reduces conict misses in direct-mapped caches [13] or that improves cache utilization <ref> [1] </ref>. In a multiprocessor, the operating system can use affinity scheduling to increase the reuse of cache lines by the same process [15].
Reference: 2. <author> J. B. Chen, Y. Endo, K. Chan, D. Mazieres, A. Dias, M. Seltzer, and M. D. Smith, </author> <title> The measured performance of personal computer operating systems. Fifteenth ACM SOSP, available as Operating Systems Review (Dec. </title> <booktitle> 1995), </booktitle> <volume> vol. 29, no. 5, </volume> <editor> p. </editor> <volume> 299313. </volume>
Reference-contexts: The system MCPI of a process is straightforward to measure for research or evaluation purposes. One can use hardware performance counters <ref> [2] </ref>, simulation techniques [14], or software tracing combined with simulation [11], in conjunction with a small amount of support in the operating system to measure the latencies of the appropriate system calls and page faults.
Reference: 3. <author> H. Custer. </author> <title> Inside Windows NT. </title> <address> Redmond, WA: </address> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: For example, the global CLOCK algorithms used in UNIX variants seek to maximize the probability that the next reference issued by the processor will not cause a page fault. The working set trimmer in Windows NT appears to seek to equalize the page fault rates of different processes <ref> [3] </ref>. It is time to reevaluate whether overall throughput should be the fundamental goal of memory management. Given current system usage patterns as described in Section 1, the goal of memory management should be to maximize the performance of primary applications rather than to maximize the machines processor utilization.
Reference: 4. <author> P. J. Denning. </author> <title> Thrashing: its causes and prevention. </title> <booktitle> AFIPS Conference Proceedings, </booktitle> <volume> Vol 33, </volume> <booktitle> 1968 Fall Joint Computer Conference. </booktitle>
Reference: 5. <author> P. J. Denning. </author> <title> Working sets past and present. </title> <journal> IEEE Transactions on Software Engineering (Jan. 1980), </journal> <volume> vol. </volume> <editor> SE-6, no.1, p. </editor> <volume> 6484. </volume>
Reference-contexts: Windows NT). Another cause of prioritization problems is that current page replacement algorithms are designed to emphasize overall system throughput rather than process prioritization. Overall throughput has been the primary design goal of memory management algorithms since the early timesharing systems <ref> [5] </ref>. For example, the global CLOCK algorithms used in UNIX variants seek to maximize the probability that the next reference issued by the processor will not cause a page fault. The working set trimmer in Windows NT appears to seek to equalize the page fault rates of different processes [3].
Reference: 6. <author> P. Krueger and R. Chawla. </author> <title> The Stealth distributed scheduler. </title> <booktitle> Eleventh International Conference on Distributed Computing Systems. </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991, </year> <note> p. 336343. </note>
Reference-contexts: The other effects will only become important if page faults are sufficiently low. There are various ways to modify global page replacement algorithms such as used in UNIX so they do a better job of prioritization. Perhaps the most complete solution is that developed for the Stealth distributed scheduler <ref> [6] </ref>. Stealth is designed to remotely execute processes to take advantage of idle cycles in a network of workstations, with the explicit goal of not interfering with local work done on any of the workstations.
Reference: 7. <author> H. M. Levy and P. H. Lipman. </author> <title> Virtual memory management in the VAX/VMS operating system. </title> <journal> IEEE Computer (March 1982), </journal> <volume> vol. 15, no. 3, </volume> <editor> p. </editor> <volume> 3541. </volume>
Reference-contexts: This issue has been studied deeply in some of the classic work on page replacement algorithms [4][5]<ref> [7] </ref>. Another well-studied area is prefetching, whether achieved by clustering a few pages together on disk [7], processing hints provided by the application [9], or storing working set information about swapped processes to speed restart after a process is swapped in [12].
Reference: 8. <author> D. Marr. </author> <title> SMP performance on Pentium Pro systems. </title> <booktitle> Presentation at Sixth Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <address> Cambridge, MA, </address> <month> October 5, </month> <year> 1996. </year>
Reference-contexts: Latency of processor cache misses Modern memory systems do not respond to all cache misses in the same amount of time. Even small-scale bus-based multiprocessors have significantly non-uniform memory access times due to DRAM page-mode hits or misses <ref> [8] </ref>. On machines with scalable interconnect networks, memory on remote nodes can be several times slower to access than memory local to the node where the cache miss is issued.
Reference: 9. <author> R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. Fifteenth ACM SOSP, available as Operating Systems Review (Dec. </title> <booktitle> 1995), </booktitle> <volume> vol. 29, no. 5, </volume> <editor> p. </editor> <volume> 7995. </volume>
Reference-contexts: This issue has been studied deeply in some of the classic work on page replacement algorithms [4][5][7]. Another well-studied area is prefetching, whether achieved by clustering a few pages together on disk [7], processing hints provided by the application <ref> [9] </ref>, or storing working set information about swapped processes to speed restart after a process is swapped in [12]. The primary way to reduce the number of page faults experienced by high-priority applications is to modify the page replacement policy to focus on this goal.
Reference: 10. <author> D. A. Patterson and J. L. Hennessy. </author> <title> Computer architecture: a quantitative approach (2nd ed). </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: System MCPI is a variant of MCPI (Memory Cycles Per Instruction) which is a metric frequently used by hardware architects to quantify the performance of the memory hierarchy of a machine <ref> [10] </ref>. To compute the hardware MCPI, the total processor stall time caused by operations that access memory is divided by the total number of instructions executed, giving the overhead added by the memory system to the raw instruction execution costs of the processor.
Reference: 11. <author> S. E. Perl and R. L. </author> <title> Sites. Studies of Windows NT Performance using Dynamic Execution Traces. </title> <journal> OSDI96, </journal> <note> available as Operating Systems Review (Oct. 1996), vol. 30, special issue, p. 169183. </note>
Reference-contexts: The system MCPI of a process is straightforward to measure for research or evaluation purposes. One can use hardware performance counters [2], simulation techniques [14], or software tracing combined with simulation <ref> [11] </ref>, in conjunction with a small amount of support in the operating system to measure the latencies of the appropriate system calls and page faults.
Reference: 12. <author> D. Potier. </author> <title> Analysis of demand paging policies with swapped working sets. </title> <booktitle> Sixth ACM SOSP, available as Proceedings of Sixth ACM Symposium on Operating Systems Principles (Nov. </booktitle> <year> 1977), </year> <note> p. 125131. </note>
Reference-contexts: Another well-studied area is prefetching, whether achieved by clustering a few pages together on disk [7], processing hints provided by the application [9], or storing working set information about swapped processes to speed restart after a process is swapped in <ref> [12] </ref>. The primary way to reduce the number of page faults experienced by high-priority applications is to modify the page replacement policy to focus on this goal. This can be done directly in a system with local page replacement like Windows NT by modifying the working set trimmer.
Reference: 13. <author> T. H. Romer, D. Lee, B. N. Bershad, and J. B. Chen. </author> <title> Dynamic page mapping policies for cache conict resolution on standard hardware. </title> <booktitle> OSDI94, available as Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation. Berkeley: USENIX Association, </booktitle> <year> 1994. </year>
Reference-contexts: Number of processor cache misses Although the processor cache is managed by hardware on most systems, operating system decisions can have a significant effect on the number of cache misses experienced by applications. The operating system can allocate pages in a way that reduces conict misses in direct-mapped caches <ref> [13] </ref> or that improves cache utilization [1]. In a multiprocessor, the operating system can use affinity scheduling to increase the reuse of cache lines by the same process [15].
Reference: 14. <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The impact of architectural trends on operating system performance. Fifteenth ACM SOSP, available as Operating Systems Review (Dec. </title> <booktitle> 1995), </booktitle> <volume> vol. 29, no. 5, </volume> <editor> p. </editor> <volume> 285298. </volume>
Reference-contexts: The system MCPI of a process is straightforward to measure for research or evaluation purposes. One can use hardware performance counters [2], simulation techniques <ref> [14] </ref>, or software tracing combined with simulation [11], in conjunction with a small amount of support in the operating system to measure the latencies of the appropriate system calls and page faults.
Reference: 15. <author> M. S. Squillante and E. D. Lazowska. </author> <title> Using processor-cache affinity in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems (Feb. 1993), </journal> <volume> vol. 4, no. 2, </volume> <editor> p. </editor> <volume> 131143. </volume>
Reference-contexts: The operating system can allocate pages in a way that reduces conict misses in direct-mapped caches [13] or that improves cache utilization [1]. In a multiprocessor, the operating system can use affinity scheduling to increase the reuse of cache lines by the same process <ref> [15] </ref>. A particularly strong form of affinity scheduling that should be investigated is to leave a processor idle even if a low-priority job is ready to run, so that the processors cache will preserve the working set of a temporarily stalled high-priority process. 5.2.
Reference: 16. <author> B. Verghese, </author> <title> private communication, </title> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: The operating system may need to consider swapping or descheduling low-priority processes if their paging bandwidth is high and high-priority processes begin issuing disk requests <ref> [16] </ref>. 5.6. Determining importance of OS modifications This section has discussed a number of levels of the storage hierarchy at which operating systems could be modified to improve memory prioritization.
Reference: 17. <author> B. Verghese, S. Devine, A. Gupta, and M. Rosenblum. </author> <title> Operating system support for improving data locality on CC-NUMA compute servers. ASPLOS VII, available as Operating Systems Review (Dec. </title> <booktitle> 1996), </booktitle> <volume> vol. 30, no. 5, </volume> <editor> p. </editor> <volume> 279289. </volume>
Reference-contexts: The operating system therefore has a significant effect on processor cache miss latency through its decisions about where and when to allocate, replicate, or migrate pages <ref> [17] </ref>. In addition to making these decisions in favor of high-priority processes, the operating system should consider monitoring interconnect bandwidth usage and memory controller contention, in order to swap out or avoid scheduling low-priority processes that are causing unacceptable increases in the cache miss latency of high-priority processes. 5.3.
References-found: 17

