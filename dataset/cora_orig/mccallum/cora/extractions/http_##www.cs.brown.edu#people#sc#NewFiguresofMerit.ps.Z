URL: http://www.cs.brown.edu/people/sc/NewFiguresofMerit.ps.Z
Refering-URL: http://www.cs.brown.edu/people/sc/
Root-URL: 
Email: fsc,ecg@cs.brown.edu  
Title: New Figures of Merit for Best-First Probabilistic Chart Parsing  
Author: Sharon A. Caraballo and Eugene Charniak 
Date: October 24, 1997  
Affiliation: Brown University  
Abstract: Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily-computable figure of merit which provides excellent performance on various measures and two different grammars.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert J. Bobrow. </author> <title> Statistical agenda parsing. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 222-224, </pages> <year> 1990. </year>
Reference-contexts: The treebank grammar was introduced in [6], and the parser in that paper is a best-first parser using the boundary trigram figure of merit. The literature shows many implementations of best-first parsing, but none of the previous work shares our goal of explicitly comparing figures of merit. Bobrow <ref> [1] </ref> and Chitrao and Grishman [8] introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word.
Reference: [2] <author> Sharon Caraballo and Eugene Charniak. </author> <title> Figures of merit for best-first probabilistic chart parsing. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 127-132, </pages> <year> 1996. </year>
Reference-contexts: Although the x-axis covers a much wider range than in Figure 13, the relationship between the two estimates is quite similar. 22 23 8 Previous work In an earlier version of this paper <ref> [2] </ref>, we presented the results for several of these models using our original grammar. The treebank grammar was introduced in [6], and the parser in that paper is a best-first parser using the boundary trigram figure of merit.
Reference: [3] <author> Glenn Carroll and Eugene Charniak. </author> <title> Learning probabilistic dependency grammars from labeled text. </title> <booktitle> In Working Notes, Fall Symposium Series, </booktitle> <pages> pages 25-32. </pages> <publisher> AAAI, </publisher> <year> 1992. </year>
Reference-contexts: the parsing accuracy obtained by each of our parsers as the number of edges they create increases. 2 Comparing figures of merit 2.1 The Experiment We used as our first grammar a probabilistic context-free grammar learned from the Brown corpus (see [9] for a description of the Brown Corpus, and <ref> [3] </ref>, [4], and [7] for grammar and training details). This grammar contains about 5,000 3 rules using 32 terminal and nonterminal symbols.
Reference: [4] <author> Glenn Carroll and Eugene Charniak. </author> <title> Two experiments on learning probabilistic dependency grammars from corpora. </title> <booktitle> In Workshop Notes, Statistically-Based NLP Techniques, </booktitle> <pages> pages 1-13. </pages> <publisher> AAAI, </publisher> <year> 1992. </year>
Reference-contexts: parsing accuracy obtained by each of our parsers as the number of edges they create increases. 2 Comparing figures of merit 2.1 The Experiment We used as our first grammar a probabilistic context-free grammar learned from the Brown corpus (see [9] for a description of the Brown Corpus, and [3], <ref> [4] </ref>, and [7] for grammar and training details). This grammar contains about 5,000 3 rules using 32 terminal and nonterminal symbols.
Reference: [5] <author> Eugene Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Traditionally, the agenda is represented as a stack, so that the last item added to the agenda is the next one removed. Chart parsing is described extensively in the literature; for one such discussion see Section 1.4 of <ref> [5] </ref>. Best-first probabilistic chart parsing is a variation of chart parsing which attempts to find the most likely parses first, by adding constituents to the chart in order of the likelihood that they will appear in a correct parse, rather than simply popping constituents off of a stack.
Reference: [6] <author> Eugene Charniak. </author> <title> Tree-bank grammars. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1031-1036. </pages> <publisher> AAAI, </publisher> <year> 1996. </year>
Reference-contexts: trigram estimate 22.1 18.4 39.6 1700 boundary trigram estimate 18.2 13.9 31.2 1111 7 Comparing figures of merit using a treebank grammar 7.1 Background To verify that our results are not an artifact of the particular grammar we chose for testing, we also tested using a treebank grammar introduced in <ref> [6] </ref>. This grammar was trained in a straightforward way by reading the grammar directly (with minor modifications) from a portion of the Penn Treebank Wall Street Journal data comprised of about 300,000 words. The boundary statistics were counted directly from the training data as well. <p> The treebank grammar was introduced in <ref> [6] </ref>, and the parser in that paper is a best-first parser using the boundary trigram figure of merit. The literature shows many implementations of best-first parsing, but none of the previous work shares our goal of explicitly comparing figures of merit.
Reference: [7] <author> Eugene Charniak and Glenn Carroll. </author> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 728-733, </pages> <year> 1994. </year>
Reference-contexts: obtained by each of our parsers as the number of edges they create increases. 2 Comparing figures of merit 2.1 The Experiment We used as our first grammar a probabilistic context-free grammar learned from the Brown corpus (see [9] for a description of the Brown Corpus, and [3], [4], and <ref> [7] </ref> for grammar and training details). This grammar contains about 5,000 3 rules using 32 terminal and nonterminal symbols.
Reference: [8] <author> Mahesh V. Chitrao and Ralph Grishman. </author> <title> Statistical parsing of messages. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 263-266, </pages> <year> 1990. </year>
Reference-contexts: This can result in a "thrashing" effect as noted in <ref> [8] </ref>, where the system parses short constituents, even very low probability ones, while avoiding combining them into longer constituents. To avoid thrashing, some technique is used to normalize the inside probability for use as a figure of merit. <p> The literature shows many implementations of best-first parsing, but none of the previous work shares our goal of explicitly comparing figures of merit. Bobrow [1] and Chitrao and Grishman <ref> [8] </ref> introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word.
Reference: [9] <author> W. Nelson Francis and Henry Kucera. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <year> 1982. </year>
Reference-contexts: Appendix C presents data comparing the parsing accuracy obtained by each of our parsers as the number of edges they create increases. 2 Comparing figures of merit 2.1 The Experiment We used as our first grammar a probabilistic context-free grammar learned from the Brown corpus (see <ref> [9] </ref> for a description of the Brown Corpus, and [3], [4], and [7] for grammar and training details). This grammar contains about 5,000 3 rules using 32 terminal and nonterminal symbols.
Reference: [10] <author> Frederick Jelinek and John D. Lafferty. </author> <title> Computation of the probability of initial substring generation by stochastic context-free grammars. </title> <journal> Computational Linguistics, </journal> <volume> 17 </volume> <pages> 315-323, </pages> <year> 1991. </year>
Reference-contexts: This formula can be infinitely recursive, depending on the properties of the grammar. A method for calculating ff L more efficiently can be derived from the calculations given in <ref> [10] </ref>. A simple extension to the normalized fi model allows us to estimate the per-word probability of all tags in the sentence through the end of the constituent 11 under consideration. This allows us to take advantage of information already obtained in a left-right parse.
Reference: [11] <author> Fred Kochman and Joseph Kupin. </author> <title> Calculating the probability of a partial parse of a sentence. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 237-240, </pages> <year> 1991. </year>
Reference-contexts: Miller and Fox [15] compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar. Kochman and Kupin <ref> [11] </ref> propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus [12] use the geometric mean to compute a figure of merit that is independent of constituent length.
Reference: [12] <author> David M. Magerman and Mitchell P. Marcus. </author> <title> Parsing the Voyager domain using Pearl. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <pages> pages 231-236, </pages> <year> 1991. </year>
Reference-contexts: Kochman and Kupin [11] propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus <ref> [12] </ref> use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman and Weir [13] use a similar model with a different parsing algorithm. 9 Conclusions We have presented and evaluated several figures of merit for best-first parsing.
Reference: [13] <author> David M. Magerman and Carl Weir. </author> <title> Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th ACL Conference, </booktitle> <pages> pages 40-47, </pages> <year> 1992. </year>
Reference-contexts: Kochman and Kupin [11] propose a figure of merit closely related to our prefix estimate. They do not actually incorporate this figure into a best-first parser. Magerman and Marcus [12] use the geometric mean to compute a figure of merit that is independent of constituent length. Magerman and Weir <ref> [13] </ref> use a similar model with a different parsing algorithm. 9 Conclusions We have presented and evaluated several figures of merit for best-first parsing.
Reference: [14] <author> Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: This grammar contains about 5,000 3 rules using 32 terminal and nonterminal symbols. We parsed 500 sentences of length 3 to 30 (including punctuation) from the Penn Treebank Wall Street Journal corpus <ref> [14] </ref> using a best-first parsing method and various estimates for p (N i j;k jt 0;n ) as the figure of merit. For each figure of merit, we compared the performance of best-first parsing using that figure of merit to exhaustive parsing.
Reference: [15] <author> Scott Miller and Heidi Fox. </author> <title> Automatic grammar acquisition. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 268-271, </pages> <year> 1994. </year>
Reference-contexts: Bobrow [1] and Chitrao and Grishman [8] introduced statistical agenda-based parsing techniques. Chitrao and Grishman implemented a best-first probabilistic parser and noted the parser's tendency to prefer shorter constituents. They proposed a heuristic solution of penalizing shorter constituents by a fixed amount per word. Miller and Fox <ref> [15] </ref> compare the performance of parsers using three different types of grammars, and show that a probabilistic context-free grammar using inside probability (unnormalized) as a figure of merit outperforms both a context-free grammar and a context-dependent grammar.
References-found: 15

