URL: http://www.daimi.aau.dk/Courses/dAlg/parallel.ps
Refering-URL: http://www.daimi.aau.dk/Courses/dAlg/praktiske-oplysninger.html
Root-URL: http://www.daimi.aau.dk
Title: Part III Introduction to parallel algorithms 4 Models asynchronous computations there is no global clock,
Note: In  computing while synchronous computation is called parallel computing. This distinction is not sharp though. 36  
Abstract: The basic machine model behind the sequential algorithms presented up to now has been the RAM (Random Access Machine) in which each operation such as LOAD, STORE, JUMP, ADD, MULT, etc. is assumed to take one unit of time. In this note we present algorithms to be executed on a multi-processor machine. In contrast to the sequential case there are numerous architectures for multi-processor machines and therefore many difierent models for parallel machines. In multi-processor machines each processor p has a local memory which can only be accessed by the processor p. The difierence in the architectures lies in the way communication between processors is handled. Communication (and computation) can be either synchronous or asynchronous. In synchronous computations we have a global clock, such that each processor executes the instructions synchronously. We will consider synchronous computations in this note and refer to synchronous multi-processor machines as parallel machines. There are two groups of parallel machines. One is connection machines such as vector- or array 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.E. Ladner, M.J. Fischer: </author> <title> Parallel preflx computations, </title> <journal> J.ACM 27 (1980) pp. </journal> <pages> 831-838. </pages>
Reference-contexts: a positive power of two): function smax1 (A; n); m := 1; m := maxfm; A [i]g od; smax1 := m end; function smax2 (A; n); for i := 1 to n=2 do B [i] := maxfA [2i 1]; A [2i]g od; if n = 2 then smax2 := B <ref> [1] </ref> else smax2 := smax2 (B; n=2) fl Both algorithms take time O (n). While the flrst is highly sequential in nature and di-cult to parallelise the other is an example of the compress and iterate paradigm which leads to natural parallelisations. <p> algorithm as follows: 39 function pmax (A; n)[p 1 ; p 2 ; : : : ; p n=2 ]; for i := 1 to n=2 pardo p i : B [i] := maxfA [2i 1]; A [2i]g od; if n = 2 then p 1 : pmax := B <ref> [1] </ref> else pmax := pmax (B; n=2)[p 1 ; p 2 ; : : : ; p n=4 ] fl The processors in brackets are the processors that are used to solve the problem at hand. <p> If R [j] 2 6= R [j 1] 2 , then processor j assigns the content of cell R [j] 2 to C [j], otherwise it assigns undeflned (?) to C [j]. If R [j] 2 6= R [j 1] 2 (R <ref> [1] </ref> 2 6= R [0] 2 ) then C [j] contains the value to be read by processors R [j] 1 ; R [j + 1] 1 ; : : : ; R [m 1] 1 , where m is the smallest index greater than j such that R [m] 2
Reference: [2] <author> C.P. Kruskal, L. Rudolph, M. Snir: </author> <title> A complexity theory of e-cient parallel algorithms. </title> <note> TCS 71 (1990) pp. 95-132. </note>
Reference: [3] <author> L.G. Valiant: </author> <title> General Purpose Parallel Architectures, </title> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <publisher> Elsevier Science Publishers B.V. </publisher> <pages> (1990) pp. 945-972. </pages>
References-found: 3

