URL: ftp://theory.lcs.mit.edu/pub/cilk/joerg-phd-thesis.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~cilk/abstracts/joerg-phd-thesis.html
Root-URL: 
Title: The Cilk System for Parallel Multithreaded Computing  
Author: by Christopher F. Joerg 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by Charles E. Leiserson Professor Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: January 1996  January, 1996  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1996.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [AAC + 92] <author> Gail Alverson, Robert Alverson, David Callahan, Brian Koblenz, Allan Porter-field, and Burton Smith. </author> <title> Exploiting heterogeneous parallelism on a multi-threaded multiprocessor. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 188-197, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Executing efficiently under these conditions requires a platform with cheap thread creation and scheduling, as well as a high-bandwidth, low-overhead communication infrastructure. There are several machines which have been designed with these characteristics in mind, such as HEP [Smi78], Tera <ref> [AAC + 92] </ref>, and dataflow machines such as Monsoon [PC90] and the EM-4 [SKY91]. Most existing machines do not have these characteristics, however. As analysis techniques improve, compilers are becoming better able to exploit locality in these programs and to increase the thread lengths. <p> Several research machines, such as HEP [Smi78], the Monsoon dataflow system [PC90], and the forthcoming Tera machine <ref> [AAC + 92] </ref>, have been designed expressly to support mul-tithreaded computations. These machines provide highly integrated, low overhead, message interfaces as well as hardware support for scheduling and synchronization.
Reference: [ABLL91] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference: [ACP95] <author> Thomas E. Anderson, David E. Culler, and David A. Patterson. </author> <title> A case for NOW (networks of workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: As with any other commodity, as parallel machines drop in price, they become cost-effective in new areas, leading to parallel machines being installed at more and more sites. If this trend wasn't enough, high-speed networks and lower-overhead software are threatening to turn every LAN into a potential parallel machine <ref> [ACP95] </ref>. We may finally be witnessing the move of parallel machines into the mainstream. Although building parallel computers has become easier, programming parallel computers can still be quite difficult.
Reference: [BB94] <author> Eric A. Brewer and Robert Blumofe. Strata: </author> <title> A multi-layer communications library. </title> <booktitle> In Proceedings of the 1994 MIT Student Workshop on Scalable Computing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: The CM-5 is a massively parallel computer consisting of 32MHz SPARC processors wired together by a fat-tree interconnection network [LAD + 92]. All communication mechanisms required to implement the work stealer and the inter-thread communication have been built using a version of Active Messages <ref> [BB94] </ref>. As Figure 2-5 illustrates, we equate useful computation with what a sequential program would have to do and classify everything else, such as communication, synchronization, and dynamic scheduling as additional overhead. <p> In this chapter, we focus on the Connection Machine CM-5 implementation of Cilk-1. The Cilk-1 scheduler on the CM-5 is written in about 40 pages of C, and it performs communication among processors using the Strata <ref> [BB94] </ref> active-message library. The remainder of this chapter is organized as follows. Section 3.2 describes Cilk-1's runtime data structures and the C language extensions that are used for programming. Section 3.3 describes the work-stealing scheduler. Section 3.4 documents the performance of several Cilk-1 applications. <p> To avoid the complexity involved in such a modification, we chose to implement a blocking transposition table. Since there is no way to implement this blocking mechanism using Cilk primitives, we dropped to a lower level and used the Strata active-message library <ref> [BB94] </ref>. We designed the transposition table such that all accesses are atomic. For example, when a value is to be put into the table, the information about the position is sent to the processor where the entry resides, and that processor updates the entry as required.
Reference: [BBB + 94] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, et al. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Naturally, much of the early research in parallel programming was directed towards programming these applications. Even today, the suites commonly used to benchmark parallel machines (e.g. Perfect [BCK + 89], NAS <ref> [BBB + 94] </ref>, and Linpack [DMBS79]) are representative of such programs. Less progress has been made for easing the task of writing parallel programs for dynamic applications.
Reference: [BBZ88] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The control mechanism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year> <month> 187 </month>
Reference-contexts: The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE [MR87] language incorporated a memory semantics similar to that of dag-consistency into a PASCAL-like language. The Myrias <ref> [BBZ88] </ref> computer was designed to support a relaxed memory semantics similar to dag-consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory [LRV94] allows for a range of consistency protocols and uses compiler support to direct their use.
Reference: [BCK + 89] <author> M. Berry, D. Chen, P. Koss, D. Kuck, et al. </author> <title> The Perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: Naturally, much of the early research in parallel programming was directed towards programming these applications. Even today, the suites commonly used to benchmark parallel machines (e.g. Perfect <ref> [BCK + 89] </ref>, NAS [BBB + 94], and Linpack [DMBS79]) are representative of such programs. Less progress has been made for easing the task of writing parallel programs for dynamic applications.
Reference: [BE89] <author> Hans Berliner and Carl Ebeling. </author> <title> Pattern knowledge and search: The SUPREM architecture. </title> <journal> Artificial Intelligence, </journal> <volume> 38(2) </volume> <pages> 161-198, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: StarTech was based on Hans Berliner's serial Hitech <ref> [BE89] </ref> program. Although ?Socrates and StarTech are based on different serial programs and do not share any code, ?Socrates borrowed techniques originally developed for StarTech, such as the basic search algorithm.
Reference: [Bea95] <author> D. </author> <title> Beal. </title> <journal> Round-by-round. ICCA Journal, </journal> <volume> 18(2), </volume> <year> 1995. </year>
Reference-contexts: After it was apparent to both programs that the pawn could not be stopped, we resigned. Although we lost the playoff, we did finish a respectable second. Our program ran 4 According to D. Beal in a description of the tournament in <ref> [Bea95] </ref>. 110 reliably throughout the tournament, with the only crashes being due to memory ECC errors. One area for improvement to ?Socrates that this tournament pointed out is our opening, as we fell behind early in several of our games.
Reference: [BFJ + 95] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Bradley C. Kusz-maul, Charles E. Leiserson, Rob Miller, Keith H. Randall, and Yuli Zhou. </author> <title> Cilk 2.0 Reference Manual. </title> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, Massachusetts 02139, </address> <month> June </month> <year> 1995. </year> <note> Available via ftp://theory.lcs.mit.edu/pub/cilk/manual2.0.ps.Z. </note>
Reference-contexts: In this thesis we focus on the key ideas of each version of Cilk and we do not attempt to describe all the details needed to write an application in Cilk. Those interested in using the system are referred to the Cilk Reference Manual <ref> [BFJ + 95] </ref>. 22 System Novel Features PCM Basic multithreaded system Cilk-1 Provably good scheduler Cilk-2 Call/return semantics Cilk-3 Shared memory Cilk-4 Inlets + aborts Table 1.1: Evolution of the Cilk System. This work began with a basic multithreaded programming system called the Parallel Continuation Machine, or PCM for short.
Reference: [BFJ + 96] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: is a lock-free consistency model which, rather than forcing a total order on global memory operations, instead ensures only that the constraints specified by Some of the work described in this chapter will be reported on in a paper by Robert Blumofe, Matteo Frigo, Charles Leiserson, Keith Randall and myself <ref> [BFJ + 96] </ref>. 127 the computation dag are enforced. Because dag consistency is a relaxed consistency model, we have been able to implement coherence efficiently in software for Cilk. <p> The Dagger algorithm is simpler to implement, however. In this section we have proven that the Dagger algorithm maintains dag consistency. See <ref> [BFJ + 96] </ref> for another algorithm we have implemented which also maintains dag consistency. 6.4 Implementation This section describes our implementation of dag-consistent shared memory for the Cilk multithreaded runtime system running on the Connection Machine CM-5 parallel supercomputer [LAD + 92].
Reference: [BH86] <author> J. E. Barnes and P. Hut. </author> <title> A hierarchical O(N log N ) force calculation algorithm. </title> <booktitle> Nature, </booktitle> <address> 324:446, </address> <year> 1986. </year>
Reference-contexts: The lower curve is for the matrixmul code in Figure 6-2 and the upper two curves are for an optimized version that uses no temporary storage. We have implemented irregular applications that employ Cilk's dag-consistent shared memory, including a port of a Barnes-Hut N -body simulation <ref> [BH86] </ref> and an implementation of Strassen's algorithm [Str69] for matrix multiplication. These irregular applications provide a good test of Cilk's ability to schedule computations dynamically.
Reference: [BJK + 95] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: To address this problem we incorporated a provably good scheduler into the PCM system and renamed the system Cilk-1. We also added additional structure to the language, and The work described in this chapter was previously reported on in a paper <ref> [BJK + 95] </ref> by Robert Blumofe, Bradley Kuszmaul, Charles Leiserson, Keith Randall, Yuli Zhou, and myself. 57 cleaned up the language a bit. With these changes Cilk-1's work-stealing scheduler achieves space, time, and communication bounds all within a constant factor of optimal. <p> This program could be written in many ways, but we have chosen to present the code as it was originally written for the performance tests shown in Table 3.1 of Chapter 3, and first presented in <ref> [BJK + 95] </ref>. As such, this code does not necessarily represent the "best" way of implementing knary, but shows how an experienced Cilk programmer quickly coded up this application. were used in this version of knary.
Reference: [BL94] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: A closure is a self-contained unit containing all the information needed to execute an instance of a user's thread, and therefore the computation described by a closure is free to be executed on any processor. The runtime system uses a randomized work-stealing scheduler <ref> [BS81, Hal84, BL94] </ref> to schedule and load balance the computation. A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor. <p> The Cilk-1 system meets these goals. The Cilk-1 system is an enhanced version of PCM which gives the user predictable and provably good performance. The theoretical work of Blumofe and Leiserson <ref> [BL94] </ref> presented a work-stealing scheduling algorithm which, for a class of well-structured programs, is provably efficient. By adding some structure to our programs and making a change to our scheduler, we were able to extend the proofs in [Blu95] to cover our scheduler as well. <p> With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy for selecting the victim processor is to have the thief choose the victim at random <ref> [BL94, KZ93, RSAU91] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each 64 (d 2 ; t 2 )(d 1 ; t 1 ) fifi infinite-processor schedule can be computed by time-stamping the tokens. <p> This bound is existentially optimal to within a constant factor <ref> [BL94] </ref>. Time With P processors, the expected execution time, including scheduling overhead, is bounded by T P = O (T 1 =P + T 1 ). <p> This bound is existentially optimal to within a constant factor [WK91]. The expected-time bound and the expected-communication bound can be converted into high-probability bounds at the cost of only a small additive term in both cases. Full proofs of these bounds, using generalizations of the techniques developed in <ref> [BL94] </ref>, can be found in [Blu95]. We defer complete proofs and give outlines here. The space bound can be obtained from a "busy-leaves" property that characterizes the allocated closures at all times during the execution. In order to state this property simply, we first define some terms. <p> We are now ready to analyze execution time. Our strategy is to mimic the theorems of <ref> [BL94] </ref> for a more restricted model of multithreaded computation. As in [BL94], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [LAB93]. <p> We are now ready to analyze execution time. Our strategy is to mimic the theorems of <ref> [BL94] </ref> for a more restricted model of multithreaded computation. As in [BL94], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [LAB93]. <p> Proof: The computation contains a total of T 1 instructions. Lemma 4 When the execution of a fully strict Cilk computation ends, the expected number of dollars in the Wait bucket is less than the number of dollars in the Steal bucket. Proof: Lemma 5 of <ref> [BL94] </ref> shows that if P processors make M random steal requests during the course of a computation, where requests with the same destination are serially queued at the destination, then the expected total delay is less than M . <p> Proof sketch: The proof follows the delay-sequence argument of <ref> [BL94] </ref>, but with some differences that we shall point out. Full details can be found in [Blu95]. At any given time during the execution, we say that a thread is critical if it has not yet been executed but all of its predecessors in the dag have been executed. <p> That is, there must be some path P in the dag such that each of the s steal attempts occurs while some thread of P is critical. We do not give the construction here, but rather refer the reader to <ref> [Blu95, BL94] </ref> for directly analogous arguments. The last step of the proof is to show that a delay sequence with s = (P T 1 ) is unlikely to occur. The key to this step is a lemma, which describes the structure of threads the processors' ready pools. <p> Thus, the sum of the dollars is T 1 + O (P T 1 ), and the bound on execution time is obtained by dividing by P . In fact, it can be shown using the techniques of <ref> [BL94] </ref> that for any * &gt; 0, with probability at least 1 *, the execution time on P processors is O (T 1 =P + T 1 + lg P + lg (1=*)).
Reference: [Ble92] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <booktitle> In Proceedings of the 1992 Dartmouth Institute for Advanced Graduate Studies (DAGS) Symposium on Parallel Computation, </booktitle> <pages> pages 11-18, </pages> <address> Hanover, New Hampshire, </address> <month> June </month> <year> 1992. </year> <month> 188 </month>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing [Sun90] and data parallel <ref> [Ble92, HS86] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for "fully strict" (well-structured) programs, Cilk-1's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal. <p> Cilk-1 provides a programming model in which work and critical path are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical path have been used in the theory community for years to analyze parallel algorithms [KR90]. Blelloch <ref> [Ble92] </ref> has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages of such a model over machine-based models. Cilk-1 provides a similar performance model for the domain of asynchronous, multithreaded computation. Although Cilk-1 offers performance guarantees, its capabilities are somewhat limited.
Reference: [Ble93] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language. </title> <institution> Technical Re--port CMU-CS-93-129, School of Computer Science, Carnegie-Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: A careful programmer can therefore write a program and be confident that the program's performance will scale as the machine size grows. Blelloch has taken this a step further in NESL <ref> [Ble93] </ref>, where every built in function has two complexity measures, which a programmer can use to derive the asymptotic running time of his program. Although the data-parallel paradigm is quite popular, it has two significant drawbacks.
Reference: [Blu95] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Because dag consistency is a relaxed consistency model, we were able to implement coherence efficiently in software. Currently, versions of Cilk run on the Thinking Machines CM-5 [Thi92], the Intel Paragon [Int94], various SMPs, and on networks of workstations <ref> [Blu95] </ref>. The same Cilk program will run on all of these platforms with few, if any, modifications. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won second prize in the 1995 World Computer Chess Championship. <p> The theoretical work of Blumofe and Leiserson [BL94] presented a work-stealing scheduling algorithm which, for a class of well-structured programs, is provably efficient. By adding some structure to our programs and making a change to our scheduler, we were able to extend the proofs in <ref> [Blu95] </ref> to cover our scheduler as well. With these changes Cilk's work-stealing scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The two parameters which predict how well a program will perform are the "work" and "critical path" of the program. <p> A Cilk-1 program is preprocessed to C and then linked with a runtime library to run on the Connection Machine CM-5 MPP, the Intel Paragon MPP, or the Silicon Graphics Power Challenge SMP. In addition, Blumofe has designed a fault tolerant version of Cilk, called Cilk-NOW <ref> [Blu95, BP94] </ref>, which runs on a network of workstations. In this chapter, we focus on the Connection Machine CM-5 implementation of Cilk-1. The Cilk-1 scheduler on the CM-5 is written in about 40 pages of C, and it performs communication among processors using the Strata [BB94] active-message library. <p> The expected-time bound and the expected-communication bound can be converted into high-probability bounds at the cost of only a small additive term in both cases. Full proofs of these bounds, using generalizations of the techniques developed in [BL94], can be found in <ref> [Blu95] </ref>. We defer complete proofs and give outlines here. The space bound can be obtained from a "busy-leaves" property that characterizes the allocated closures at all times during the execution. In order to state this property simply, we first define some terms. <p> Proof sketch: The proof follows the delay-sequence argument of [BL94], but with some differences that we shall point out. Full details can be found in <ref> [Blu95] </ref>. At any given time during the execution, we say that a thread is critical if it has not yet been executed but all of its predecessors in the dag have been executed. <p> That is, there must be some path P in the dag such that each of the s steal attempts occurs while some thread of P is critical. We do not give the construction here, but rather refer the reader to <ref> [Blu95, BL94] </ref> for directly analogous arguments. The last step of the proof is to show that a delay sequence with s = (P T 1 ) is unlikely to occur. The key to this step is a lemma, which describes the structure of threads the processors' ready pools. <p> The analysis and bounds we have derived apply to fully strict programs in the case when each thread spawns at most one successor. In <ref> [Blu95] </ref>, the theorems above are generalized to handle situations where a thread can spawn more than one successor. 79 3.7 Conclusion To produce high-performance parallel applications, programmers often focus on communication costs and execution time, quantities that are dependent on specific machine configurations. <p> We are currently investigating how to incorporate file I/O in our system. We are also currently working on porting dag-consistent shared memory to our Cilk-NOW <ref> [Blu95] </ref> adaptively parallel, fault-tolerant, network-of-workstations system. We are using operating system hooks to make the use of shared memory be transparent to the user. <p> It runs on various serial machines (under Unix and Linux), Symmetric MultiProcessors (e.g. Sun, SGI), and Massively Parallel Processors (e.g. CM-5, Paragon). In addition, Blumofe has implemented a version of Cilk which runs on Networks of Workstations <ref> [Blu95] </ref>. * Leverage existing codes: Since Cilk can call standard C functions, much of the existing serial code can often be used when porting an application to Cilk. This was especially important in porting the Socrates chess program and the POV-Ray ray tracer to Cilk.
Reference: [BP94] <author> Robert D. Blumofe and David S. Park. </author> <title> Scheduling large-scale parallel computations on networks of workstations. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 96-105, </pages> <address> San Francisco, California, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A Cilk-1 program is preprocessed to C and then linked with a runtime library to run on the Connection Machine CM-5 MPP, the Intel Paragon MPP, or the Silicon Graphics Power Challenge SMP. In addition, Blumofe has designed a fault tolerant version of Cilk, called Cilk-NOW <ref> [Blu95, BP94] </ref>, which runs on a network of workstations. In this chapter, we focus on the Connection Machine CM-5 implementation of Cilk-1. The Cilk-1 scheduler on the CM-5 is written in about 40 pages of C, and it performs communication among processors using the Strata [BB94] active-message library.
Reference: [Bre74] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: The Cilk-1 scheduler uses "work stealing" [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [Bre74, Gra66, Gra69] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. In this chapter we demonstrate the efficiency of the Cilk-1 scheduler both empirically and analytically.
Reference: [BS81] <author> F. Warren Burton and M. Ronan Sleep. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Proceedings of the 1981 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 187-194, </pages> <address> Portsmouth, New Hampshire, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: A closure is a self-contained unit containing all the information needed to execute an instance of a user's thread, and therefore the computation described by a closure is free to be executed on any processor. The runtime system uses a randomized work-stealing scheduler <ref> [BS81, Hal84, BL94] </ref> to schedule and load balance the computation. A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor. <p> With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [BZS93] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528-537, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Likewise, we felt that the strategy of using binary rewriting to detect writes in software <ref> [BZS93] </ref> would entail too much effort. We finally settled on the diff mechanism for its simplicity. Some means of allocating memory must be provided in any useful implementation of shared memory.
Reference: [CA94] <author> David Chaiken and Anant Agarwal. </author> <title> Software-extended coherent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others [CBZ91, FLA94, KCDZ94], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [CA94, KOH + 94, RLW94] </ref> require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines.
Reference: [CAL + 89] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year> <month> 189 </month>
Reference-contexts: In contrast, systems that use cache lines [CA94, KOH + 94, RLW94] require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines. As another alternative, systems that use arbitrary--sized objects or regions <ref> [CAL + 89, JKW95, TBK93] </ref> require either an object-oriented programming model or explicit user management of objects. As we have gained experience programming with dag consistency, we have encountered some deficiencies of dag consistency that tend to make certain programming idioms inefficient.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, California, </address> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others <ref> [CBZ91, FLA94, KCDZ94] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects.
Reference: [CD88] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference: [CDG + 93] <author> Daved E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishna-murthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Ore-gon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In fact, we use Active Messages in just this way. The Cilk system is implemented on top of Active Messages. The Split-C <ref> [CDG + 93] </ref> parallel programming language is an attempt to merge some of the features of the data-parallel and message-passing paradigms. As in the message-passing paradigm, Split-C exposes to the user one thread of control for each processor. <p> The dag-consistent shared-memory code performs at 5 megaflops per processor as long as the work per processor is large. This performance compares reasonably well with the other matrix multiplication codes on the CM-5. For example, an implementation coded in Split-C <ref> [CDG + 93] </ref> attains just over 6 megaflops per processor on 64 processors using a static data layout, a static thread schedule, and an optimized assembly language inner loop.
Reference: [CGH94] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference: [CRRH93] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early experiences with Olden. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference: [CSS + 91] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: There are two extensions that are needed. The first extension is to allow the user to specify a restricted piece of code, called an inlet <ref> [CSS + 91] </ref>, that is to be executed as soon as a spawned child returns. In chess, an inlet can be used to check the result of a test of a position and perform some action based on that result.
Reference: [DMBS79] <author> J. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart. </author> <title> LINPACK Users' Guide. </title> <publisher> Siam, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Naturally, much of the early research in parallel programming was directed towards programming these applications. Even today, the suites commonly used to benchmark parallel machines (e.g. Perfect [BCK + 89], NAS [BBB + 94], and Linpack <ref> [DMBS79] </ref>) are representative of such programs. Less progress has been made for easing the task of writing parallel programs for dynamic applications. These programs are ones where the execution of the program is heavily influenced by the data input to the program and by the data computed by the program.
Reference: [DSB86] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Unfortunately, designers have found Lamport's model difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [DSB86, GS93, GLL + 90] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions performed by physical processors.
Reference: [EAL93] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. Filaments: </author> <title> Efficient support for fine-grain parallelism. </title> <type> Technical Report TR 93-13a, </type> <institution> The University of Arizona, </institution> <year> 1993. </year> <month> 190 </month>
Reference: [EL94] <author> Natalie Engler and David Linthicum. </author> <title> Not just a PC on steroids. </title> <booktitle> Open Com--puting, </booktitle> <pages> pages 43-47, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This usage pattern is hard to document quantitatively, but as an example, "Open Computing" estimates that of all the high end, 2-8 processor PCs sold, 70% of them are used as file and print servers <ref> [EL94] </ref>. It seems that most small scale SMPs are destined to spend their lives as "throughput machines," never to run a single parallel job. We should not be too 13 negative, however, since clearly some progress has been made on the software front.
Reference: [FF82] <author> Raphael A. Finkel and John P. Fishburn. </author> <title> Parallelism in alpha-beta search. </title> <journal> Artificial Intellgence, </journal> <volume> 19(1) </volume> <pages> 89-106, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: For integer scores one uses the values (ff 1) and (ff) as the parameters of the recursive 1 R. Finkel and J. Fishburn showed that if the serialization implied by ff-fi pruning is ignored by a parallel program, then it will achieve only p P speedup on P processors <ref> [FF82] </ref>. 87 (S1) Define scout (n; ff; fi) as (S2) If n is a leaf then return static eval (n). (S3) Let ~c the children of n, and (S4) b scout (c 0 ; fi; ff): (S5) ;; The first child's valuation may cause this node to fail high. (S6) If
Reference: [FLA94] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others <ref> [CBZ91, FLA94, KCDZ94] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects.
Reference: [FM87] <author> Raphael Finkel and Udi Manber. </author> <title> DIB|a distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [FMM93] <author> R. Feldmann, P. Mysliwietz, and B. Monien. </author> <title> Game tree search on a massively parallel system. </title> <booktitle> In Advances in Computer Chess 7, </booktitle> <pages> pages 203-219, </pages> <year> 1993. </year>
Reference-contexts: Without such a methodology it can be very difficult to do algorithm design. For example, Feldmann, Monien, and Mysliwietz find themselves changing their Zugzwang chess program to increase the parallelism without really having a good way to measure their changes <ref> [FMM93] </ref>. They express concern that by serially searching the first child before starting the other children, they may have reduced the available parallelism. Our technique allows us to state that there is sufficient parallelism to keep thousands of processors busy without changing the algorithm.
Reference: [FMM94] <author> Rainer Feldmann, Peter Mysliwietz, and Burkhard Monien. </author> <title> Studying overheads in massively parallel min/max-tree evaluation. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 94-103, </pages> <address> Cape May, New Jersey, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability. W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: To efficiently execute such an application, it is necessary to have efficient runtime thread placement and scheduling techniques. Although finding the optimal thread placement is known to be an NP-hard problem <ref> [GJ79] </ref>, it is possible to implement schedulers based on simple heuristics that achieve good machine utilizations at reasonable cost.
Reference: [GLL + 90] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Unfortunately, designers have found Lamport's model difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [DSB86, GS93, GLL + 90] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions performed by physical processors.
Reference: [Gra66] <author> R. L. Graham. </author> <title> Bounds for certain multiprocessing anomalies. </title> <journal> The Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <month> November </month> <year> 1966. </year>
Reference-contexts: The Cilk-1 scheduler uses "work stealing" [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [Bre74, Gra66, Gra69] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. In this chapter we demonstrate the efficiency of the Cilk-1 scheduler both empirically and analytically.
Reference: [Gra69] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year> <month> 191 </month>
Reference-contexts: The Cilk-1 scheduler uses "work stealing" [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [Bre74, Gra66, Gra69] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. In this chapter we demonstrate the efficiency of the Cilk-1 scheduler both empirically and analytically.
Reference: [GS93] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond the barriers of memory coherence and serializability. </title> <type> Technical Report 78, </type> <institution> McGill University, School of Computer Science, Advanced Compilers, Architectures, and Parallel Systems (ACAPS) Laboratory, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Unfortunately, designers have found Lamport's model difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [DSB86, GS93, GLL + 90] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions performed by physical processors. <p> Dag consistency is similar to location consistency <ref> [GS93] </ref>, but it is defined in terms of the dag of a user's multithreaded computation rather than in terms of processors and synchronization points. We shall present a formal model of dag consistency in Section 6.2.
Reference: [Gwe94] <author> Linley Gwennap. </author> <title> Intel extends 486, Pentium families. </title> <type> Microprocessor Report, 8(3) </type> <pages> 1-11, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: This trend towards including multiprocessor support in standard microprocessors occurred first with processors used in workstations (e.g. MIPS R4000 [MWV92], Sparc [Sun89], PowerPC 601 [Mot93]) and more recently with processors for PCs (e.g. In-tel's Pentium P54C <ref> [Gwe94] </ref>). As with any other commodity, as parallel machines drop in price, they become cost-effective in new areas, leading to parallel machines being installed at more and more sites.
Reference: [Hal84] <author> Robert H. Halstead, Jr. </author> <title> Implementation of Multilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 9-17, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: A closure is a self-contained unit containing all the information needed to execute an instance of a user's thread, and therefore the computation described by a closure is free to be executed on any processor. The runtime system uses a randomized work-stealing scheduler <ref> [BS81, Hal84, BL94] </ref> to schedule and load balance the computation. A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor.
Reference: [Hal85] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk-1 thread leaves the C runtime stack empty when it completes. Thus, Cilk-1 can run on top of a vanilla C runtime system. A common alternative <ref> [Hal85, KC93, MKH91, Nik94] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [HD68] <author> E. A. Hauck and B. A. Dent. </author> <title> Burroughs' B6500/B7500 stack mechanism. </title> <booktitle> Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 245-251, </pages> <year> 1968. </year>
Reference-contexts: Cilk also supports stack allocation of distributed shared memory. The declaration of tmp in line 8 of matrixmul causes the shared-memory stack pointer to be incremented by nb*nb blocks. When running in parallel, however, a simple serial stack is insufficient. Instead, Cilk provides a distributed "cactus stack" <ref> [HD68, Mos70, Ste88] </ref> that mimics a serial stack in such a way that during execution, every thread can access all the variables allocated by its parents and can address a variable directly by its depth in the stack. <p> We first describe the Cilk language extensions for supporting shared-memory objects and the "diff" mechanism [KCDZ94] for managing dirty bits. We then describe the distributed "cactus-stack" <ref> [HD68, Mos70, Ste88] </ref> memory allocator which the system uses to allocate shared-memory objects. Finally, we describe the mechanisms used 151 by the runtime system to maintain dag-consistency. The Cilk system on the CM-5 supports concrete shared-memory objects of 32-bit words. All consistency operations are logically performed on a per-word basis. <p> Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [HD68, Mos70, Ste88] </ref> allocator. From the point of view of a single Cilk procedure, a cactus-stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decrementing a stack pointer.
Reference: [HKT93] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 338-349, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: This deficiency is particularly acute for languages such as CM Fortran, where the code generated uses "virtual processors". A virtual processor mechanism allows the same 17 code to run on a machine of any size, but it adds significant inefficiencies <ref> [HKT93] </ref>. Message Passing Another common paradigm for writing parallel programs is message passing. Message-passing models present the programmer with one thread of control in each processor, and these processors communicate by sending messages. This model is a good representation of the actual implementation of current parallel machines.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: a Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: If we were to allow every word to be fetched and reconciled independently, however, the system would be terribly inefficient. Since extra fetches and reconciles do not adversely affect the consistency algorithm, we implemented the familiar strategy of grouping objects into pages <ref> [HP90, Section 8.2] </ref>, each of which is fetched or reconciled as a unit. Assuming that spatial locality exists when objects are accessed, grouping objects helps amortize the fetch/reconcile overhead.
Reference: [HS86] <author> W. Hillis and G. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: In short, we wanted the best of both worlds. Before describing the system we designed, we will first take a look at what other parallel programming paradigms were available. Data Parallel One of the most successful parallel programming models is the data-parallel programming paradigm <ref> [HS86] </ref>. This model is useful for taking advantage of the large amounts of data parallelism that is available in many scientific/numeric applications. This data parallelism is exploited by performing the same operation on a large amount of data, distributed across the processors of the machine. <p> The data-parallel programming model has two main virtues that have led to its success. The first virtue of this model is that data-parallel codes are fairly easy to write and debug <ref> [HS86] </ref>. Just as in a serial program, the programmer sees a sequential flow of control. The values making up a parallel value are automatically spread across the machine, although typically the programmer does have the option of influencing how data is placed. <p> Many of these applications pose problems for more traditional parallel environments, such as message passing [Sun90] and data parallel <ref> [Ble92, HS86] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for "fully strict" (well-structured) programs, Cilk-1's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [HWW93] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference: [HZJ94] <author> Michael Halbherr, Yuli Zhou, and Chris F. Joerg. </author> <title> MIMD-style parallel programming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd Inter 192 national Workshop on Massive Parallelism: Hardware, Software, and Appli--cations, </booktitle> <address> Capri, Italy, </address> <month> September </month> <year> 1994. </year> <note> A longer version appeared as : MIT Laboratory for Computer Science, Computation Structures Group Memo 355. </note>
Reference-contexts: We believe that existing programming models, such as data parallel programming and explicit message passing, have been successful in addressing the needs of programs with simple static Much of the work described in this section was reported on by Michael Halbherr, Yuli Zhou and myself in an earlier paper <ref> [HZJ94] </ref>. 31 communication patterns. For these programs it is usually possible to carefully orchestrate communication and computation to statically optimize the overall performance.
Reference: [Int94] <institution> Intel Supercomputer Systems Division, Beaverton, Oregon. </institution> <note> Paragon User's Guide, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Because dag consistency is a relaxed consistency model, we were able to implement coherence efficiently in software. Currently, versions of Cilk run on the Thinking Machines CM-5 [Thi92], the Intel Paragon <ref> [Int94] </ref>, various SMPs, and on networks of workstations [Blu95]. The same Cilk program will run on all of these platforms with few, if any, modifications. <p> We had our work cut out for us. For this tournament we were able to get access to the 1824 node Intel Paragon <ref> [Int94] </ref> at Sandia National Labs. We began by porting Cilk to the Paragon, with the help of Rolf Riesen of Sandia. The port of Cilk was fairly easy, although in the process we exposed several bugs in the Paragon's SUNMOS operating system.
Reference: [JD73] <author> Edward G. Coffman Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Indeed, at the point when one execution has just taken its Cth page fault, each cache contains exactly the last C distinct pages referenced <ref> [JD73] </ref>. We now use this property of LRU to count the number of page faults of the execution E. The fault behavior of E is the same as the fault behavior of E 0 except for the subcomputation T and its parent, call it U , in the kernel tree.
Reference: [JK94] <author> Chris Joerg and Bradley C. Kuszmaul. </author> <title> Massively parallel chess. </title> <booktitle> In Proceedings of the Third DIMACS Parallel Implementation Challenge, </booktitle> <institution> Rutgers University, </institution> <address> New Jersey, </address> <month> October </month> <year> 1994. </year> <note> Available as ftp://theory.lcs.mit.edu/ pub/cilk/dimacs94.ps.Z. </note>
Reference-contexts: At each node of the tree, the program runs an empty "for" loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [JK94, Kus94] </ref> to parallelize a minmax tree search. We give performance numbers for the search of a position to depth 10. The work of the algorithm varies with the number of processors, because it does speculative work that may be aborted during runtime. <p> In order to obtain good performance during this search, we use several mecha Part of this work was reported on by Kuszmaul and myself in an earlier article <ref> [JK94] </ref>. 81 nisms not directly provided by Cilk, such as aborting computations and directly accessing the active message layer to implement a global transposition table distributed across the processors.
Reference: [JKW95] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: These irregular applications provide a good test of Cilk's ability to schedule computations dynamically. We achieve a speedup of 9 on an 8192-particle N -body simulation using 32 processors, which is competitive with other software implementations of distributed shared memory <ref> [JKW95] </ref>. Strassen's algorithm runs as fast as matrixmul for 2048 fi 2048 matrices, and we coded it in Cilk in a few hours. The remainder of this chapter is organized as follows. <p> In contrast, systems that use cache lines [CA94, KOH + 94, RLW94] require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines. As another alternative, systems that use arbitrary--sized objects or regions <ref> [CAL + 89, JKW95, TBK93] </ref> require either an object-oriented programming model or explicit user management of objects. As we have gained experience programming with dag consistency, we have encountered some deficiencies of dag consistency that tend to make certain programming idioms inefficient.
Reference: [JP92] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A customizable substrate for concurrent languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 55-67, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference: [Kal90] <author> L. V. Kale. </author> <title> The Chare kernel parallel programming system. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, Volume II: Software, </booktitle> <pages> pages 17-25, </pages> <month> August </month> <year> 1990. </year>
Reference: [KC93] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert|efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 598-607, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Each Cilk-1 thread leaves the C runtime stack empty when it completes. Thus, Cilk-1 can run on top of a vanilla C runtime system. A common alternative <ref> [Hal85, KC93, MKH91, Nik94] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24.10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [KC93, MKH91] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a C function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [KCDZ94] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. </author> <title> Tread-Marks: Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year> <month> 193 </month>
Reference-contexts: We first describe the Cilk language extensions for supporting shared-memory objects and the "diff" mechanism <ref> [KCDZ94] </ref> for managing dirty bits. We then describe the distributed "cactus-stack" [HD68, Mos70, Ste88] memory allocator which the system uses to allocate shared-memory objects. Finally, we describe the mechanisms used 151 by the runtime system to maintain dag-consistency. <p> Rather than using dirty bits explicitly, as the Dagger algorithm from Section 6.3 would suggest, Cilk uses a diff mechanism as is used in the Treadmarks system <ref> [KCDZ94] </ref>. The diff mechanism computes the dirty bit for an object by comparing that object's value with its value in a copy made at fetch time. Our implementation makes this copy only for pages loaded in read/write mode, thereby avoiding the overhead of copying for read-only pages. <p> Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others <ref> [CBZ91, FLA94, KCDZ94] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects.
Reference: [KEW + 85] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon. </author> <title> Implementing a cache consistency protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <year> 1985. </year>
Reference-contexts: Introduction Researchers have long worked to bring parallel hardware and software into widespread use. Recently there has been progress on the hardware front. Serial microprocessors have been used as cost-effective building blocks for medium and large scale parallel machines. Now many high-volume serial processors contain hooks, such as snoopy buses <ref> [KEW + 85] </ref>, for implementing multiprocessor systems. These hooks make it quite simple and cheap for commercial computer manufacturers to build inexpensive, entry-level, multiprocessor machines. This trend towards including multiprocessor support in standard microprocessors occurred first with processors used in workstations (e.g.
Reference: [KHM89] <author> David A. Kranz, Robert H. Halstead, Jr., and Eric Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [KM75] <author> Donald E. Knuth and Ronald W. Moore. </author> <title> An analysis of alpha-beta pruning. </title> <journal> Artificial Intelligence, </journal> <volume> 6(4) </volume> <pages> 293-326, </pages> <month> Winter </month> <year> 1975. </year>
Reference-contexts: Any one of a number of possibilities suffices. Thus, White can stop thinking about the move without having exhaustively searched all of Black's options. The idea of pruning subtrees that do not need to be searched is embodied in the serial ff-fi search algorithm <ref> [KM75] </ref>, which computes the negamax score for a node without actually looking at the entire search tree. The algorithm is expressed as a recursive subroutine with two new parameters ff and fi. <p> The ff-fi algorithm can substantially reduce the size of the tree searched. The ff-fi algorithm works best if the best moves are considered first, because if any move can make the position fail high, then certainly the best move can make the position fail high. Knuth and Moore <ref> [KM75] </ref> show that for searches of a uniform best-ordered tree of height H and degree D, the ff-fi algorithm searches only O ( p D H ) leaves instead of D H leaves.
Reference: [KOH + 94] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stan-ford Flash multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others [CBZ91, FLA94, KCDZ94], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [CA94, KOH + 94, RLW94] </ref> require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines.
Reference: [KR90] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science|Volume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Cilk-1 provides a programming model in which work and critical path are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical path have been used in the theory community for years to analyze parallel algorithms <ref> [KR90] </ref>. Blelloch [Ble92] has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages of such a model over machine-based models. Cilk-1 provides a similar performance model for the domain of asynchronous, multithreaded computation.
Reference: [Kus94] <author> Bradley C. Kuszmaul. </author> <title> Synchronized MIMD Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-645 or ftp://theory.lcs.mit.edu/ pub/bradley/phd.ps.Z. </note>
Reference-contexts: Computer chess is such an application. Our chess program uses large global data structures, is nondeterministic, and performs speculative computations, some of which are aborted. This work was in part a natural follow on of StarTech <ref> [Kus94] </ref>, a parallel chess program designed by Bradley Kuszmaul which had the scheduler and search algorithm intertwined. We wanted to show that the scheduler and search algorithm could be separated, thereby greatly simplifying the programmer's job, without sacrificing performance. <p> With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> At each node of the tree, the program runs an empty "for" loop for 400 iterations. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [JK94, Kus94] </ref> to parallelize a minmax tree search. We give performance numbers for the search of a position to depth 10. The work of the algorithm varies with the number of processors, because it does speculative work that may be aborted during runtime. <p> On the other hand, the fib program has low efficiency, because the threads are so short: fib does almost nothing besides spawn and send argument. 69 Despite it's long threads, the ?Socrates program shows low efficiency, because its parallel Jamboree search algorithm <ref> [Kus94] </ref> is based on speculatively searching subtrees that are not searched by a serial algorithm. Consequently, as we increase the number of processors, the program executes more threads and, hence, does more work. <p> Also, since we would test the program in actual competitions against both humans and computers, we would be forced to implement the best algorithms, not just whatever happened to be easiest to implement in our system. Our chess program, ?Socrates, uses the Jamboree <ref> [Kus94] </ref> algorithm to perform a parallel game-tree search. This search algorithm has a complex control structure which is nondeterministic and performs speculative computations, some of which need to be killed off before completing. <p> Cilk and ?Socrates were later ported to the Intel Paragon in March 1995, and running on Sandia National Laboratories' 1824-node 82 Paragon, ?Socrates finished second in the 1995 World Computer Chess Championship. ?Socrates is, in part, a continuation of earlier work performed here on the StarTech <ref> [Kus94] </ref> chess program. StarTech was based on Hans Berliner's serial Hitech [BE89] program. Although ?Socrates and StarTech are based on different serial programs and do not share any code, ?Socrates borrowed techniques originally developed for StarTech, such as the basic search algorithm. <p> We make some concluding remarks in Section 4.6. 4.2 Parallel Game Tree Search The ?Socrates chess program uses an efficient parallel game-tree search algorithm called "Jamboree" search <ref> [Kus94] </ref>. In this section we explain Jamboree search, starting with the basics of negamax search and serial ff-fi search, and present some analytical performance results for the algorithm.
Reference: [KZ93] <author> Richard M. Karp and Yanjun Zhang. </author> <title> Randomized parallel algorithms for backtrack search and branch-and-bound computation. </title> <journal> Journal of the ACM, </journal> <volume> 40(3) </volume> <pages> 765-789, </pages> <month> July </month> <year> 1993. </year> <month> 194 </month>
Reference-contexts: A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor. This work-stealing scheduling strategy tends to provide good load balancing without requiring excessive communication <ref> [KZ93, RSAU91, ZO94] </ref>. We wrote several applications in PCM, including a ray tracer based on the serial POVRAY program [POV93], and a protein-folding code [PJGT94], which is still being used 23 to investigate various models of protein formation. <p> With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy for selecting the victim processor is to have the thief choose the victim at random <ref> [BL94, KZ93, RSAU91] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each 64 (d 2 ; t 2 )(d 1 ; t 1 ) fifi infinite-processor schedule can be computed by time-stamping the tokens.
Reference: [LAB93] <author> Pangfeng Liu, William Aiello, and Sandeep Bhatt. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 154-163, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: As in [BL94], the bounds assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered <ref> [LAB93] </ref>. For technical reasons in our analysis of execution time, the critical path is calculated assuming that all threads spawned by a parent thread are spawned at the end of the parent thread. In our analysis of execution time, we use an accounting argument.
Reference: [LAD + 92] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The CM-5 is a massively parallel computer consisting of 32MHz SPARC processors wired together by a fat-tree interconnection network <ref> [LAD + 92] </ref>. All communication mechanisms required to implement the work stealer and the inter-thread communication have been built using a version of Active Messages [BB94]. <p> We also present empirical evidence from experiments run on a CM-5 to document the efficiency of our work-stealing scheduler. The CM-5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network <ref> [LAD + 92] </ref>. The applications are described below: * fib (n) is the same as was presented in Section 3.2, except that the second recursive spawn is replaced by a "tail call" that avoids the scheduler. <p> See [BFJ + 96] for another algorithm we have implemented which also maintains dag consistency. 6.4 Implementation This section describes our implementation of dag-consistent shared memory for the Cilk multithreaded runtime system running on the Connection Machine CM-5 parallel supercomputer <ref> [LAD + 92] </ref>. We first describe the Cilk language extensions for supporting shared-memory objects and the "diff" mechanism [KCDZ94] for managing dirty bits. We then describe the distributed "cactus-stack" [HD68, Mos70, Ste88] memory allocator which the system uses to allocate shared-memory objects.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Robert Blumofe is implementing a version of this system for networks of workstations. Charles Leiserson and I devised the correctness proof given in Section 6.3. 6.1 Introduction Architects of shared memory for parallel computers have traditionally attempted to support Lamport's model of sequential consistency <ref> [Lam79] </ref> which states: A system is sequentially consistent if the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. <p> This extra copying can be very expensive when it occurs in the inner loop of a program. A big advantage of direct hardware support for Lamport's model of sequential consistency <ref> [Lam79] </ref> is that no copying of temporaries need occur. We are currently investigating how this kind of problem can be solved efficiently in Cilk without direct hardware support.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy <ref> [LH89] </ref> and others [CBZ91, FLA94, KCDZ94], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects.
Reference: [LRV94] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 208-218, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The BLAZE [MR87] language incorporated a memory semantics similar to that of dag-consistency into a PASCAL-like language. The Myrias [BBZ88] computer was designed to support a relaxed memory semantics similar to dag-consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory <ref> [LRV94] </ref> allows for a range of consistency protocols and uses compiler support to direct their use. Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory.
Reference: [Mil95] <author> Robert C. Miller. </author> <title> A type-checking preprocessor for Cilk 2, a multithreaded C language. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In Cilk-2, our next major release of the system, we focused on making the system easier to program. As a stepping stone towards this goal we first introduced a type-checking preprocessor <ref> [Mil95] </ref>. Previously, Cilk programs were converted to C via a standard, but simple, macro preprocessor. This preprocessor limited the constructs we could use in the language, occasionally forcing us to expose to the programmer details we would rather have kept hidden. <p> In order to perform such a translation, we needed a more sophisticated preprocessor than the simple macro preprocessor originally used with Cilk-1. What we implemented was a new type-checking preprocessor for Cilk. This preprocessor was implemented by Rob Miller and is described in more detail in <ref> [Mil95] </ref>. This preprocessor is based on C-to-C, a tool which parses a C program and turns it into an abstract syntax tree (AST). C-to-C then performs type checking and dataflow analysis on this AST, and then uses the AST to regenerate a C program. C-to-C was extended to create Cilk-to-C. <p> Simplicity of implementation is not why we chose this path, but it is a nice feature anyway. Before concluding this section, we give a brief overview of the implementation of Cilk-2. We focus on what constructs Cilk-2 code is translated into. See <ref> [Mil95] </ref> for details on how this transformation is performed. 121 The idea for transforming Cilk-2 style code into threaded code is fairly straightforward. Since the runtime system deals with threads, not procedures, a Cilk-2 procedure must be broken up into a set of threads. <p> Specifically, in our CM-5 implementation, shared memory is kept separate from the other user memory, and special operations are required to operate on it. Most painfully, testing for page faults occurs explicitly in software, rather than implicitly in hardware. Our Cilk-to-C type-checking preprocessor <ref> [Mil95] </ref> alleviates some of the discomfort, but a transparent solution that uses hardware support for paging would be much preferable. A minor advantage to the software approach we use, however, is that we can support full 64-bit addressing of shared memory on the 32-bit Sparc processors of the CM-5 system.
Reference: [MKH91] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Each Cilk-1 thread leaves the C runtime stack empty when it completes. Thus, Cilk-1 can run on top of a vanilla C runtime system. A common alternative <ref> [Hal85, KC93, MKH91, Nik94] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24.10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [KC93, MKH91] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a C function call. We hope to incorporate such techniques into future implementations of Cilk. <p> A final improvement that we are considering is to implement a stack-based execution model using lazy task creation <ref> [MKH91] </ref>. This modification is one we think we understand, and we expect to implement it in the near future. Switching Cilk to a stack-based execution model would provide two benefits. First, it would lower the overhead of spawning new tasks.
Reference: [Mos70] <author> Joel Moses. </author> <title> The function of FUNCTION in LISP or why the FUNARG problem should be called the envronment problem. </title> <type> Technical Report memo AI-199, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> June </month> <year> 1970. </year>
Reference-contexts: Cilk also supports stack allocation of distributed shared memory. The declaration of tmp in line 8 of matrixmul causes the shared-memory stack pointer to be incremented by nb*nb blocks. When running in parallel, however, a simple serial stack is insufficient. Instead, Cilk provides a distributed "cactus stack" <ref> [HD68, Mos70, Ste88] </ref> that mimics a serial stack in such a way that during execution, every thread can access all the variables allocated by its parents and can address a variable directly by its depth in the stack. <p> We first describe the Cilk language extensions for supporting shared-memory objects and the "diff" mechanism [KCDZ94] for managing dirty bits. We then describe the distributed "cactus-stack" <ref> [HD68, Mos70, Ste88] </ref> memory allocator which the system uses to allocate shared-memory objects. Finally, we describe the mechanisms used 151 by the runtime system to maintain dag-consistency. The Cilk system on the CM-5 supports concrete shared-memory objects of 32-bit words. All consistency operations are logically performed on a per-word basis. <p> Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [HD68, Mos70, Ste88] </ref> allocator. From the point of view of a single Cilk procedure, a cactus-stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decrementing a stack pointer. <p> Cactus stacks have many of the same limitations as ordinary procedure stacks <ref> [Mos70] </ref>. For instance, a child thread cannot return to its parent a pointer to an object that it has created. Similarly, sibling procedures cannot share storage that they create on the stack.
Reference: [Mot93] <author> Motorola. </author> <note> PowerPc 601 User's Manual, 1993. 195 </note>
Reference-contexts: These hooks make it quite simple and cheap for commercial computer manufacturers to build inexpensive, entry-level, multiprocessor machines. This trend towards including multiprocessor support in standard microprocessors occurred first with processors used in workstations (e.g. MIPS R4000 [MWV92], Sparc [Sun89], PowerPC 601 <ref> [Mot93] </ref>) and more recently with processors for PCs (e.g. In-tel's Pentium P54C [Gwe94]). As with any other commodity, as parallel machines drop in price, they become cost-effective in new areas, leading to parallel machines being installed at more and more sites.
Reference: [MR87] <author> Piyush Mehrotra and Jon Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: To conclude, we briefly discuss related work and offer some ideas for the future. The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE <ref> [MR87] </ref> language incorporated a memory semantics similar to that of dag-consistency into a PASCAL-like language. The Myrias [BBZ88] computer was designed to support a relaxed memory semantics similar to dag-consistency, with many of the mechanisms implemented in hardware.
Reference: [MSA + 85] <author> J.R. McGraw, S.K. Skedzielewski, S.J. Allan, R.R. Odledhoeft, , J. Glauert, C. Kirkham, W. Noyce, and R. Thomas. </author> <title> Sisal: Streams and iteration in a single assignment language: Reference manual version 1.2. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratories, Livermore CA, </institution> <month> March </month> <year> 1985. </year>
Reference-contexts: The static set of sequential threads making up the multithreaded program can either be generated implicitly by a sophisticated compiler, or explicitly by the programmer. Programming languages advocating the implicit style, such as Id [Nik91] and Sisal <ref> [MSA + 85] </ref>, usually take a high-level, functional, description of the actual problem, extract the available parallelism from the declaration and partition it into sequential threads. While implicit programming languages simplify the programming task for some problems, other problems are difficult to efficiently express in a functional style.
Reference: [MWV92] <author> Sunil Mirapuri, Michael Woodacre, and Mader Vasseghi. </author> <title> The Mips R4000 processor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 10-22, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: These hooks make it quite simple and cheap for commercial computer manufacturers to build inexpensive, entry-level, multiprocessor machines. This trend towards including multiprocessor support in standard microprocessors occurred first with processors used in workstations (e.g. MIPS R4000 <ref> [MWV92] </ref>, Sparc [Sun89], PowerPC 601 [Mot93]) and more recently with processors for PCs (e.g. In-tel's Pentium P54C [Gwe94]). As with any other commodity, as parallel machines drop in price, they become cost-effective in new areas, leading to parallel machines being installed at more and more sites.
Reference: [Nik91] <author> R.S. Nikhil. </author> <title> ID language reference manual. Computation Structure Group Memo 284-2, </title> <institution> Massachusetts Institute of Technology, 545 Technology Square, </institution> <address> Cambridge, Massachusetts 02139, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: When using such a multithreaded programming model, the runtime system must schedule these tasks and dynamically spread them across the machine in order to load balance the computation. The most ambitious of the multithreaded languages are the implicitly parallel languages, 20 such as Id <ref> [Nik91] </ref>. In these languages the programmer expresses his algorithm at a high level without any mention of parallelism. Then, a sophisticated compiler automatically breaks the program up into a fine-grained multithreaded program. <p> The static set of sequential threads making up the multithreaded program can either be generated implicitly by a sophisticated compiler, or explicitly by the programmer. Programming languages advocating the implicit style, such as Id <ref> [Nik91] </ref> and Sisal [MSA + 85], usually take a high-level, functional, description of the actual problem, extract the available parallelism from the declaration and partition it into sequential threads.
Reference: [Nik93] <author> Rishiyur S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pages 390-405, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference: [Nik94] <author> Rishiyur S. Nikhil. Cid: </author> <title> A parallel, shared-memory C for distributed-memory machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk-1 thread leaves the C runtime stack empty when it completes. Thus, Cilk-1 can run on top of a vanilla C runtime system. A common alternative <ref> [Hal85, KC93, MKH91, Nik94] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> We considered several choices for how to express synchronization in Cilk-2 before deciding on this one. The other options we considered incorporated mechanisms which gave the user more control over the synchronization process. In particular we considered a mechanism essentially the same as join variables in Cid <ref> [Nik94] </ref>. In this proposal each spawn would 120 cilk int fib (int n) if (n&lt;2) return (n); else - x = spawn fib (n-1); y = spawn fib (n-2); sync; return (x+y); - have a join counter associated with it.
Reference: [PC90] <author> Gregory M. Papadopoulos and David E. Culler. Monsoon: </author> <title> An explicit token-store architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 306. </note>
Reference-contexts: Executing efficiently under these conditions requires a platform with cheap thread creation and scheduling, as well as a high-bandwidth, low-overhead communication infrastructure. There are several machines which have been designed with these characteristics in mind, such as HEP [Smi78], Tera [AAC + 92], and dataflow machines such as Monsoon <ref> [PC90] </ref> and the EM-4 [SKY91]. Most existing machines do not have these characteristics, however. As analysis techniques improve, compilers are becoming better able to exploit locality in these programs and to increase the thread lengths. <p> These heuristics usually work well for a broad class of applications, making it possible to implement the scheduling and placement task as a fairly generic service that resides at the core of the runtime system. Several research machines, such as HEP [Smi78], the Monsoon dataflow system <ref> [PC90] </ref>, and the forthcoming Tera machine [AAC + 92], have been designed expressly to support mul-tithreaded computations. These machines provide highly integrated, low overhead, message interfaces as well as hardware support for scheduling and synchronization.
Reference: [Pea80] <author> Judea Pearl. </author> <title> Asymptotic properties of minimax trees and game-searching procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 14(2) </volume> <pages> 113-138, </pages> <month> September </month> <year> 1980. </year>
Reference-contexts: then abort-and-return s. (J16) If s &gt; ff then set ff s. (J17) If s &gt; b then set b s. (J18) Note the completion of the ith iteration of the parallel loop. (J19) enddo (J20) return b. cost of any nodes that must be searched twice, and empirical evidence <ref> [Pea80] </ref> justify its dominance as the search algorithm of choice in modern serial chess-playing programs. 4.2.4 Jamboree Search The Jamboree algorithm, shown in Figure 4-5, is a parallelized version of the Scout search algorithm.
Reference: [PJGT94] <author> Vijay S. Pande, Christopher F. Joerg, Alexander Yu Grosberg, and Toyoichi Tanaka. </author> <title> Enumerations of the hamiltonian walks on a cubic sublattice. </title> <journal> Journal of Physics A, </journal> <volume> 27, </volume> <year> 1994. </year> <month> 196 </month>
Reference-contexts: This work-stealing scheduling strategy tends to provide good load balancing without requiring excessive communication [KZ93, RSAU91, ZO94]. We wrote several applications in PCM, including a ray tracer based on the serial POVRAY program [POV93], and a protein-folding code <ref> [PJGT94] </ref>, which is still being used 23 to investigate various models of protein formation. The PCM system performed well on these applications, achieving nearly linear speedup without adding significant overhead. This initial system showed us that we could easily build a powerful multithreaded system. <p> For the rest of this section we will be concerned mainly with the implementation of this problem using PCM, focusing on the routine that enumerates all possible paths. More details on the algorithms used and the results obtained with this program are given in <ref> [PJGT94] </ref>. At its heart, this program is a search program that finds all possible unique paths that visit each node of the cube exactly once. This algorithm works by incrementally building up paths through the cube until complete paths are reached. <p> The Cilk-1 program is based on serial code by R. Sargent of the MIT Media Laboratory. Thread length was enhanced by serializing the bottom 7 levels of the search tree. * pfold (x,y,z) is a protein-folding program <ref> [PJGT94] </ref> written in conjunction with V. Pande of MIT's Center for Material Sciences and Engineering. This program was described in more detail in Section 2.4. This program finds hamiltonian paths in a three-dimensional grid of size x fi y fi z.
Reference: [POV93] <author> POV-Ray Team. </author> <title> Persistence of Vision Ray Tracer (POV-Ray) User's Docu--mentation, </title> <year> 1993. </year>
Reference-contexts: This work-stealing scheduling strategy tends to provide good load balancing without requiring excessive communication [KZ93, RSAU91, ZO94]. We wrote several applications in PCM, including a ray tracer based on the serial POVRAY program <ref> [POV93] </ref>, and a protein-folding code [PJGT94], which is still being used 23 to investigate various models of protein formation. The PCM system performed well on these applications, achieving nearly linear speedup without adding significant overhead. This initial system showed us that we could easily build a powerful multithreaded system. <p> Ray Tracer Parallelization To show the power of our thread package as a tool to retarget existing sequential programs for parallel processors, we took the serial POV-Ray package <ref> [POV93] </ref> that implements the optimized ray-tracing method described above and rewrote its kernel with our thread language. The serial POV-Ray program is quite large, the C source files consist of over 20,000 lines.
Reference: [PYGT94] <author> Vijay Pande, Alexander Yu, Grosberg, and Toyoichi Tanaka. </author> <title> Thermodynamic procedure to construct heteropolymers that can be renatured to recognize a given target molecule. </title> <booktitle> Proceeding of the National Academy of Science, </booktitle> <address> U.S.A, 91(12976), </address> <year> 1994. </year>
Reference-contexts: An implementation using PCM avoids this problem. The work on this problem was done in conjunction with Pande, Yu, Grosberg, and Tanaka of the Center for Material Sciences and Engineering at MIT. In their work <ref> [PYGT94] </ref> Pande, Yu, Grosberg, and Tanaka use the lattice model [SG90] to model protein folding. In this model a protein is described as a chain of monomers, and it is assumed that in a folded protein each monomer will sit on a point on a 3-dimensional lattice.
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag-consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [LH89] and others [CBZ91, FLA94, KCDZ94], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [CA94, KOH + 94, RLW94] </ref> require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines.
Reference: [RSAU91] <author> Larry Rudolph, Miriam Slivkin-Allalouf, and Eli Upfal. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor. This work-stealing scheduling strategy tends to provide good load balancing without requiring excessive communication <ref> [KZ93, RSAU91, ZO94] </ref>. We wrote several applications in PCM, including a ray tracer based on the serial POVRAY program [POV93], and a protein-folding code [PJGT94], which is still being used 23 to investigate various models of protein formation. <p> Cilk's strategy for selecting the victim processor is to have the thief choose the victim at random <ref> [BL94, KZ93, RSAU91] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each 64 (d 2 ; t 2 )(d 1 ; t 1 ) fifi infinite-processor schedule can be computed by time-stamping the tokens.
Reference: [RSL93] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference: [SFL + 94] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Like Ivy [LH89] and others [CBZ91, FLA94, KCDZ94], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [CA94, KOH + 94, RLW94] require some degree of hardware support <ref> [SFL + 94] </ref> to manage shared memory effi 160 ciently at the granularity of cache lines. As another alternative, systems that use arbitrary--sized objects or regions [CAL + 89, JKW95, TBK93] require either an object-oriented programming model or explicit user management of objects.
Reference: [SG90] <author> E. Shakhnovich and A. Gutin. </author> <title> J Chem. </title> <journal> Phys., </journal> <volume> 93, 5967, </volume> <year> 1990. </year>
Reference-contexts: An implementation using PCM avoids this problem. The work on this problem was done in conjunction with Pande, Yu, Grosberg, and Tanaka of the Center for Material Sciences and Engineering at MIT. In their work [PYGT94] Pande, Yu, Grosberg, and Tanaka use the lattice model <ref> [SG90] </ref> to model protein folding. In this model a protein is described as a chain of monomers, and it is assumed that in a folded protein each monomer will sit on a point on a 3-dimensional lattice.
Reference: [SKY91] <author> S. Sakai, Y. Kodama, and Y. Yamaguchi. </author> <title> Prototype implementation of a highly parallel dataflow machine EM-4. </title> <booktitle> In Proceedings of the 5th International Parallel Processing Symposium, </booktitle> <pages> pages 278-286, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: There are several machines which have been designed with these characteristics in mind, such as HEP [Smi78], Tera [AAC + 92], and dataflow machines such as Monsoon [PC90] and the EM-4 <ref> [SKY91] </ref>. Most existing machines do not have these characteristics, however. As analysis techniques improve, compilers are becoming better able to exploit locality in these programs and to increase the thread lengths.
Reference: [Smi78] <author> Burton J. Smith. </author> <title> A pipelined, shared resource MIMD computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year> <month> 197 </month>
Reference-contexts: Executing efficiently under these conditions requires a platform with cheap thread creation and scheduling, as well as a high-bandwidth, low-overhead communication infrastructure. There are several machines which have been designed with these characteristics in mind, such as HEP <ref> [Smi78] </ref>, Tera [AAC + 92], and dataflow machines such as Monsoon [PC90] and the EM-4 [SKY91]. Most existing machines do not have these characteristics, however. As analysis techniques improve, compilers are becoming better able to exploit locality in these programs and to increase the thread lengths. <p> These heuristics usually work well for a broad class of applications, making it possible to implement the scheduling and placement task as a fairly generic service that resides at the core of the runtime system. Several research machines, such as HEP <ref> [Smi78] </ref>, the Monsoon dataflow system [PC90], and the forthcoming Tera machine [AAC + 92], have been designed expressly to support mul-tithreaded computations. These machines provide highly integrated, low overhead, message interfaces as well as hardware support for scheduling and synchronization.
Reference: [Ste88] <author> Per Stenstrom. </author> <title> VLSI support for a cactus stack oriented memory organization. </title> <booktitle> Proceedings of the Twenty-First Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 211-220, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Cilk also supports stack allocation of distributed shared memory. The declaration of tmp in line 8 of matrixmul causes the shared-memory stack pointer to be incremented by nb*nb blocks. When running in parallel, however, a simple serial stack is insufficient. Instead, Cilk provides a distributed "cactus stack" <ref> [HD68, Mos70, Ste88] </ref> that mimics a serial stack in such a way that during execution, every thread can access all the variables allocated by its parents and can address a variable directly by its depth in the stack. <p> We first describe the Cilk language extensions for supporting shared-memory objects and the "diff" mechanism [KCDZ94] for managing dirty bits. We then describe the distributed "cactus-stack" <ref> [HD68, Mos70, Ste88] </ref> memory allocator which the system uses to allocate shared-memory objects. Finally, we describe the mechanisms used 151 by the runtime system to maintain dag-consistency. The Cilk system on the CM-5 supports concrete shared-memory objects of 32-bit words. All consistency operations are logically performed on a per-word basis. <p> Since Cilk procedures operate in a parallel tree-like fashion, however, we needed some kind of parallel stack. We settled on implementing a cactus-stack <ref> [HD68, Mos70, Ste88] </ref> allocator. From the point of view of a single Cilk procedure, a cactus-stack behaves much like an ordinary stack. The procedure can allocate and free memory by incrementing and decrementing a stack pointer.
Reference: [Str69] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: We have implemented irregular applications that employ Cilk's dag-consistent shared memory, including a port of a Barnes-Hut N -body simulation [BH86] and an implementation of Strassen's algorithm <ref> [Str69] </ref> for matrix multiplication. These irregular applications provide a good test of Cilk's ability to schedule computations dynamically. We achieve a speedup of 9 on an 8192-particle N -body simulation using 32 processors, which is competitive with other software implementations of distributed shared memory [JKW95]. <p> To measure the warm-up overhead, we counted the number of page faults taken by several applications|including matrixmul, a parallel version of Strassen's algorithm <ref> [Str69] </ref>, and a parallel version of a Barnes-Hut N -body code [BH86]|for various choices of cache, processor, and problem size. For each run we measured the cache warm-up fraction (F P F 1 )=2Cs, which represents the fraction of the cache that needs to be warmed up on each steal.
Reference: [Sun89] <author> Sun Microsystems, Inc. </author> <title> Sparc Architecture Manual, </title> <type> Version 8, </type> <month> January </month> <year> 1989. </year>
Reference-contexts: These hooks make it quite simple and cheap for commercial computer manufacturers to build inexpensive, entry-level, multiprocessor machines. This trend towards including multiprocessor support in standard microprocessors occurred first with processors used in workstations (e.g. MIPS R4000 [MWV92], Sparc <ref> [Sun89] </ref>, PowerPC 601 [Mot93]) and more recently with processors for PCs (e.g. In-tel's Pentium P54C [Gwe94]). As with any other commodity, as parallel machines drop in price, they become cost-effective in new areas, leading to parallel machines being installed at more and more sites.
Reference: [Sun90] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: To date, the applications we have programmed include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won second prize in the 1995 World Computer Chess 59 Championship. Many of these applications pose problems for more traditional parallel environments, such as message passing <ref> [Sun90] </ref> and data parallel [Ble92, HS86], because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for "fully strict" (well-structured) programs, Cilk-1's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [TBK93] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaashoek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In contrast, systems that use cache lines [CA94, KOH + 94, RLW94] require some degree of hardware support [SFL + 94] to manage shared memory effi 160 ciently at the granularity of cache lines. As another alternative, systems that use arbitrary--sized objects or regions <ref> [CAL + 89, JKW95, TBK93] </ref> require either an object-oriented programming model or explicit user management of objects. As we have gained experience programming with dag consistency, we have encountered some deficiencies of dag consistency that tend to make certain programming idioms inefficient.
Reference: [Thi91a] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. Getting Started in CM Fortran, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: This data parallelism is exploited by performing the same operation on a large amount of data, distributed across the processors of the machine. Data-parallel languages, such as CM Fortran <ref> [Thi91a] </ref>, C* [Thi93], and *Lisp [Thi91b], all of which were available on the CM-5, are similar to sequential languages. The main difference is that certain data types are defined to be parallel. Parallel data values consist of a collection of standard, scalar data values.
Reference: [Thi91b] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, Massachusetts. </address> <booktitle> Getting Started in *Lisp, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: This data parallelism is exploited by performing the same operation on a large amount of data, distributed across the processors of the machine. Data-parallel languages, such as CM Fortran [Thi91a], C* [Thi93], and *Lisp <ref> [Thi91b] </ref>, all of which were available on the CM-5, are similar to sequential languages. The main difference is that certain data types are defined to be parallel. Parallel data values consist of a collection of standard, scalar data values.
Reference: [Thi92] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <type> CM5 Technical Summary, </type> <month> January </month> <year> 1992. </year>
Reference-contexts: Because dag consistency is a relaxed consistency model, we were able to implement coherence efficiently in software. Currently, versions of Cilk run on the Thinking Machines CM-5 <ref> [Thi92] </ref>, the Intel Paragon [Int94], various SMPs, and on networks of workstations [Blu95]. The same Cilk program will run on all of these platforms with few, if any, modifications.
Reference: [Thi93] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> Getting Started in C*, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: This data parallelism is exploited by performing the same operation on a large amount of data, distributed across the processors of the machine. Data-parallel languages, such as CM Fortran [Thi91a], C* <ref> [Thi93] </ref>, and *Lisp [Thi91b], all of which were available on the CM-5, are similar to sequential languages. The main difference is that certain data types are defined to be parallel. Parallel data values consist of a collection of standard, scalar data values.
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Asynchronous message passing eases the programmer's job, but adds significant overhead to each communication due to the copying and buffering that the system invisibly 18 performs. Active Messages <ref> [vECGS92] </ref>, the third strategy for message passing, reduces this overhead by providing asynchronous message passing without the automatic buffering. An active message contains a header which points to a handler, which is a piece of user code that specifies what to do with the data in the message. <p> The PCM model presented in this chapter is aimed at solving the aforementioned problems. The intended target architectures are simple message-passing machines which support the implementation of low-overhead communication layers such as Active Messages <ref> [vECGS92] </ref>. We do not assume any additional hardware support. We have provided C language extensions where threads can be specified along with conventional sequential C code.
Reference: [VR88] <author> Mark T. Vandevoorde and Eric S. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year> <month> 198 </month>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk-1 scheduler uses "work stealing" <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> to achieve execution time very near to the sum of these two measures. <p> As we will see later, these values are useful to a programmer trying to understand the performance of his program. The computation of the critical path is done by a system of time-stamping, as shown in Figure 3-4. 3.3 Cilk's Work-Stealing Scheduler Cilk's scheduler uses the technique of work-stealing <ref> [BL94, BS81, FMM94, FM87, FLA94, Hal85, KZ93, KHM89, Kus94, Nik94, VR88] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [WK91] <author> I-Chen Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide--and-conquer. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 151-162, </pages> <address> San Juan, Puerto Rico, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Communication The expected number of bytes communicated during a P -processor execution is O (T 1 P S max ), where S max denotes the largest size of any closure. This bound is existentially optimal to within a constant factor <ref> [WK91] </ref>. The expected-time bound and the expected-communication bound can be converted into high-probability bounds at the cost of only a small additive term in both cases. Full proofs of these bounds, using generalizations of the techniques developed in [BL94], can be found in [Blu95].
Reference: [ZO94] <author> Y. Zhang and A. Ortynski. </author> <title> The efficiency of randomized parallel backtrack search. </title> <booktitle> In Proceedings of the 6th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, Texas, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: A processor typically works locally, mimicking the serial execution order. When a processor runs out of work, it chooses a processor at random and steals a ready closure from the chosen processor. This work-stealing scheduling strategy tends to provide good load balancing without requiring excessive communication <ref> [KZ93, RSAU91, ZO94] </ref>. We wrote several applications in PCM, including a ray tracer based on the serial POVRAY program [POV93], and a protein-folding code [PJGT94], which is still being used 23 to investigate various models of protein formation.
Reference: [ZSB94] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Bershad. </author> <title> Software write detection for a distributed shared memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year> <month> 199 </month>
Reference-contexts: The diff mechanism imposes extra overhead on each reconcile, but it allows the user to manipulate a page using an ordinary C pointer that incurs no run-time system overhead <ref> [ZSB94] </ref>. We needed to support the detection of writes in software, because the CM-5 provides no direct hardware support to maintain dirty bits explicitly at the granularity of words. We rejected out of hand the unpleasant alternative of requiring the user to maintain his own dirty bits.
References-found: 108

