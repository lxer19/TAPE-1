URL: http://www.cam.sri.com/tr/crc031/paper.ps.Z
Refering-URL: http://www.cam.sri.com/tr/ABSTRACTS.html
Root-URL: 
Phone: (2)  
Title: A SPEECH TO SPEECH TRANSLATION SYSTEM BUILT FROM STANDARD COMPONENTS  
Author: Manny Rayner Hiyan Alshawi Ivan Bretan David Carter Vassilios Digalakis Bjorn Gamback Jaan Kaja Jussi Karlgren Bertil Lyberg Steve Pulman Patti Price and Christer Samuelsson 
Address: Cambridge, UK  Menlo Park, CA (3) SICS, Stockholm, Sweden (4) Telia Research AB, Haninge, Sweden  
Affiliation: (1) SRI International,  SRI International,  
Date: 1993  
Note: HLT meeting, Princeton,  
Web: URL: http://www.cam.sri.com/tr/crc031/paper.ps.ZARPA  
Abstract: This paper 1 describes a speech to speech translation system using standard components and a suite of generalizable customization techniques. The system currently translates air travel planning queries from English to Swedish. The modular architecture is designed to be easy to port to new domains and languages, and consists of a pipelined series of processing phases. The output of each phase consists of multiple hypotheses; statistical preference mechanisms, the data for which is derived from automatic processing of domain corpora, are used between each pair of phases to filter hypotheses. Linguistic knowledge is represented throughout the system in declarative form. We summarize the architectures of the component systems and the interfaces between them, and present initial performance results. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Alshawi, H. (ed.), </author> <title> The Core Language Engine, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: It outputs to the source language processor a small lattice of word hypotheses generated using acoustic and language model scores. The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) <ref> [1] </ref>, a unification-based, broad coverage natural language system for analysis and generation. Transfer occurs at the level of quasi logical form (QLF); transfer rules are defined in a simple declarative formalism [2]. Speech synthesis is performed by the Swedish Telecom PROPHON system [8], based on stored polyphones. <p> Language Analysis and Generation Language analysis and generation are performed by the SRI Core Language Engine (CLE), a general natural-language processing system developed at SRI Cambridge <ref> [1] </ref>; two copies of the CLE are used, equipped with English and Swedish grammars respectively. The English grammar is a large, domain-independent unification-based phrase-structure grammar, augmented by a small number of domain-specific rules (Section 3.1). The Swedish grammar is a fairly direct adaptation of the English one (Section 3.3). <p> Here, the input is a QLF form, and the output is the set of possible surface strings which realize the form. Early forms of the analysis and generation algorithms used are described in <ref> [1] </ref>. 2.3. Speech/Language Interface The interface between speech recognition and source language analysis can be either a 1-best or an N-best interface. In 1-best mode, the recognizer simply passes the CLE a string representing the single best hypothesis.
Reference: 2. <author> Alshawi, H., Carter, D., Rayner, M. and Gamback, B., </author> <title> "Transfer through Quasi Logical Form", Proc. 29th Table 2: Component error rates (1-best recognition) (37.4%) 5-best recognition 21.8% Speech/language interface 8.7% Source linguistic analysis 11.8% Source analysis preferences 13.4% Transfer and generation 22.7% ACL, </title> <address> Berkeley, </address> <year> 1991. </year>
Reference-contexts: The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) [1], a unification-based, broad coverage natural language system for analysis and generation. Transfer occurs at the level of quasi logical form (QLF); transfer rules are defined in a simple declarative formalism <ref> [2] </ref>. Speech synthesis is performed by the Swedish Telecom PROPHON system [8], based on stored polyphones. This section describes in more detail these components and their in-terfaces. 2.1. Speech Recognition The first component is a fast version of SRI's DECIPHER (TM) speaker-independent continuous speech recognition system [12]. <p> Our experimental findings to date indicate that N=5 gives a good tradeoff between speed and accuracy, performance surprisingly being fairly insensitive to the setting of the relative weights given to acoustic and linguistic scoring information. Some performance results are presented in Section 5. 2.4. Transfer Unification-based QLF transfer <ref> [2] </ref>, compositionally translates a QLF of the source language to a QLF of the target language. QLF is the transfer level of choice in the system, since it is a contextually unresolved semantic representation reflecting both predicate-argument relations and linguistic features such as tense, aspect, and modality.
Reference: 3. <author> Alshawi, H. and Crouch, R., </author> <title> "Monotonic Semantic Interpretation", </title> <booktitle> Proc. 30th ACL, </booktitle> <address> Newark, </address> <year> 1992. </year>
Reference-contexts: QLF includes predicate-argument structure and some surface features, but also allows a semantic analysis to be only partially specified <ref> [3] </ref>. The set of QLF analyses is then ranked in order of a priori plausibility using a set of heuristic preferences, which are partially trainable from example corpus data (Section 3.2).
Reference: 4. <author> Alshawi, H., and Carter, D., </author> <title> "Optimal Scaling of Preference Metrics", </title> <institution> SRI Cambridge Research Report, </institution> <year> 1992. </year>
Reference-contexts: Initially, we chose scaling factors by hand, but this became an increasingly skilled and difficult task as more metrics were added, and it was clear that the choice would have to be repeated for other domains. The following semi-automatic optimization procedure <ref> [4] </ref> was therefore developed. QLFs were derived for about 4600 context-independent and context-dependent ATIS sentences of 1 to 15 words. It is easy to derive from a QLF the set of segments of the input sentence which it analyses as being either predications or arguments.
Reference: 5. <author> Black, E., et al., </author> <title> "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars," </title> <booktitle> Proc. Third DARPA Speech and Language Workshop, </booktitle> <editor> P. Price (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: It is easy to derive from a QLF the set of segments of the input sentence which it analyses as being either predications or arguments. These segments, taken together, effectively define a tree of roughly the form used by the Treebank project <ref> [5] </ref>. A user presented with all strings derived from any QLF for a sentence selected the correct tree (if present). A skilled judge was then able to assign trees to hundreds of sentences per hour.
Reference: 6. <author> Carter, </author> <title> D.M., "Lattice-based Word Identification in CLARE", </title> <booktitle> Proc 30th ACL, </booktitle> <address> Newark, </address> <year> 1992. </year>
Reference-contexts: Since the word lattice generated during the first two recognition passes significantly constrains the search space of the third pass, we can have a large number of hypotheses without a significant increase in computation. As the CLE is capable of using lattice input directly <ref> [6] </ref>, the N-best hypotheses are combined into a new lattice before being passed to linguistic processing; in cases where divergences occur near the end of the utterance, this yields a substantial speed improvement.
Reference: 7. <author> Carter, </author> <title> D.M., "Lexical Acquisition in the Core Language Engine", </title> <booktitle> Proc. 4th European ACL, </booktitle> <address> Manchester, </address> <year> 1989. </year>
Reference-contexts: First, about 500 lexical entries needed to be added. Of these, about 450 were regular content words (airfare, Boston, seven forty seven, etc.), all of which were added by a graduate student 3 using the interactive VEX lexicon acquisition tool <ref> [7] </ref>. About 55 other entries, not of a regular form, were also added.
Reference: 8. <author> Ceder, K. and Lyberg, B., </author> <title> "Yet Another Rule Compiler for Text-to-Speech Conversion?", </title> <booktitle> Proc. ICSLP, </booktitle> <address> Banff, </address> <year> 1993. </year>
Reference-contexts: Transfer occurs at the level of quasi logical form (QLF); transfer rules are defined in a simple declarative formalism [2]. Speech synthesis is performed by the Swedish Telecom PROPHON system <ref> [8] </ref>, based on stored polyphones. This section describes in more detail these components and their in-terfaces. 2.1. Speech Recognition The first component is a fast version of SRI's DECIPHER (TM) speaker-independent continuous speech recognition system [12].
Reference: 9. <author> Church, K.W. and Hanks, P., </author> <title> "Word Association Norms, Mutual Information, and Lexicography", </title> <booktitle> Computational Linguistics 16 </booktitle> <pages> 22-30, </pages> <year> 1990. </year>
Reference-contexts: We have found that a simple metric, original to us, that scores triples according to the average treebank score of QLFs in which they occur, performs about as well as a chi-squared metric, and better than one based on mutual information (cf <ref> [9] </ref>). 3.3.
Reference: 10. <author> Gamback, B. and Rayner, M., </author> <title> "The Swedish Core Language Engine", </title> <booktitle> Proc. 3rd NOTEX, </booktitle> <address> Linkoping, </address> <year> 1992. </year>
Reference-contexts: CLE Language Adaptation The Swedish-language customization of the CLE (S-CLE) has been developed at SICS from the English-language version by replacing English-specific mod ules with corresponding Swedish-language versions. 4 Swedish is a Germanic language, linguistically about as 4 The S-CLE and the adaptation process is described in detail in <ref> [10] </ref>. "far" from English as German is. Our experience sug-gests that adapting the English system to close languages is fairly easy and straight-forward.
Reference: 11. <author> Moulines, E. and Charpentier, F., </author> <title> "Pitch-Synchronous Waveform Processing Techniques for Text-to-Speech Synthesis Using Diphones", </title> <journal> Speech Communication Vol. </journal> <volume> 9, </volume> <year> 1990. </year>
Reference-contexts: In the latter case, the synthesizer accesses the database of polyphone speech waveforms according to the allophonic specification derived from the lexicon and/or phonetic transcription rules. The poly phones are concatenated and the prosody of the utterance is imposed via the PSOLA (pitch synchronous overlap add) signal processing technique <ref> [11] </ref>. The Prophon system has access to information other than the text string, in particular the parse tree, which can be used to provide a better, more natural prosodic structure than normally is possible. 3.
Reference: 12. <author> Murveit, H., Butzberger, J. and Weintraub, M., </author> <title> "Speech Recognition in SRI's Resource Management and ATIS Systems", </title> <booktitle> Proc. DARPA Workshop on Speech and Natural Language, </booktitle> <year> 1991. </year>
Reference-contexts: Components and Interfaces The speech translation process begins with SRI's DECIPHER (TM) system, based on hidden Markov modeling and a progressive search <ref> [12, 13] </ref>. It outputs to the source language processor a small lattice of word hypotheses generated using acoustic and language model scores. The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) [1], a unification-based, broad coverage natural language system for analysis and generation. <p> Speech synthesis is performed by the Swedish Telecom PROPHON system [8], based on stored polyphones. This section describes in more detail these components and their in-terfaces. 2.1. Speech Recognition The first component is a fast version of SRI's DECIPHER (TM) speaker-independent continuous speech recognition system <ref> [12] </ref>. It uses context-dependent phonetic-based hidden Markov models with discrete observation distributions for 4 features: cepstrum, delta-cepstrum, energy and delta-energy. The models are gender-independent and the system is trained on 19,000 sentences and has a 1381-word vocabulary.
Reference: 13. <author> Murveit, H., et al., </author> <title> "Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques", </title> <booktitle> Proc. ICASSP, </booktitle> <year> 1993. </year>
Reference-contexts: Components and Interfaces The speech translation process begins with SRI's DECIPHER (TM) system, based on hidden Markov modeling and a progressive search <ref> [12, 13] </ref>. It outputs to the source language processor a small lattice of word hypotheses generated using acoustic and language model scores. The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) [1], a unification-based, broad coverage natural language system for analysis and generation. <p> It uses context-dependent phonetic-based hidden Markov models with discrete observation distributions for 4 features: cepstrum, delta-cepstrum, energy and delta-energy. The models are gender-independent and the system is trained on 19,000 sentences and has a 1381-word vocabulary. The progressive recognition search <ref> [13] </ref> is a three-pass scheme that produces a word lattice and an N-best list for use by the language analysis component. Two recognition passes are used to create a word lattice.
Reference: 14. <author> Roe, D.B., Pereira, F.C.N., Sproat, R.W., Riley, M.D. and Moreno, P.J., </author> <title> "Towards a Spoken-Language Translator for Restricted-Domain Context-Free Languages", </title> <booktitle> Proc. Eurospeech, </booktitle> <year> 1991. </year>
Reference-contexts: There is so far little consensus on how to evaluate spoken language translation systems; for instance, no evaluation figures on unseen material are cited for the systems described in [17] and <ref> [14] </ref>. We present the results below partly in an attempt to stimulate discussion on this topic.
Reference: 15. <author> Samuelsson, C. and Rayner, M., </author> <title> "Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System", </title> <booktitle> Proc. 12th IJCAI, </booktitle> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: What is needed is a sub-corpus that contains all the commonly occurring types of construction, together with an indication of how many sentences each example in the sub-corpus represents. We have developed a systematic method for constructing representative sub-corpora, using "Explanation Based Learning" (EBL) <ref> [15] </ref>. The original corpus is parsed, and the resulting analysis trees are grouped into equivalence classes; then one member is chosen from each class, and stored with the number of examples it represents. In the simplest version, trees are equivalent if their leaves are of the same lexical types.
Reference: 16. <author> Shieber, S. M., van Noord, G., Pereira, F.C.N and Moore, </author> <title> R.C., </title> <journal> "Semantic-Head-Driven Generation", Computational Linguistics, </journal> <volume> 16 </volume> <pages> 30-43, </pages> <year> 1990. </year>
Reference-contexts: In generation mode, the linguistic information is compiled into another set of tables, which control a version of the Semantic Head-Driven Generation algorithm <ref> [16] </ref>. Here, the input is a QLF form, and the output is the set of possible surface strings which realize the form. Early forms of the analysis and generation algorithms used are described in [1]. 2.3.
Reference: 17. <author> Woszczyna, M. et al., </author> <title> "Recent advances in JANUS: A Speech Translation System", </title> <booktitle> ARPA Workshop on Human Language Technology, </booktitle> <address> Plainsboro, NJ, </address> <year> 1993. </year>
Reference-contexts: There is so far little consensus on how to evaluate spoken language translation systems; for instance, no evaluation figures on unseen material are cited for the systems described in <ref> [17] </ref> and [14]. We present the results below partly in an attempt to stimulate discussion on this topic.
References-found: 17

