URL: http://www.cs.umd.edu/~kalvis/thesis.ps
Refering-URL: http://www.cs.umd.edu/~kalvis/
Root-URL: 
Title: HIERARCHIES OF PROBABILISTIC AND TEAM LEARNING  
Author: Carl H. Smith 
Degree: Kalvis Apsitis, Doctor of Philosophy, 1998  
Affiliation: Department of Computer Science  
Note: ABSTRACT Title of Dissertation:  Dissertation directed by: Professor  
Abstract: An inductive inference machine M receives successive values of a recursive function f and tries to learn a program for f. This model has two modifications: (1) A probabilistic machine M flips fair coins and learns a function with probability at least p, (2) A [k; n] team: a collection of n machines receiving values of the same function f so that at least k of these machines learn f . For each particular learning type there is a hierarchy of probabilistic and team learning powers as parameters p; k; n vary. For one-shot learning FIN , this hierarchy was not completely known [DKV92, DK96]. This thesis shows that for FIN and for given k i ; n i the problem to determine whether any [k 1 ; n 1 ] team can be simulated by an appropriate [k 2 ; n 2 ] team is decidable. Using team matrices we can also compare the learning power of intersections, team compositions, pairwise unions and other derivations of the usual teams [k; n]. 
Abstract-found: 1
Intro-found: 1
Reference: [Abr74] <author> M. H. Abrams, </author> <title> editor. The Norton anthology of English literature. </title> <publisher> Norton, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: That low man goes on adding one to one, his hundred's soon hit; This high man, aiming at million, misses an unit. Robert Browning. A Grammarian's Funeral. See <ref> [Abr74] </ref>. 1.2 Models of Learning in Inductive Inference 1.2.1 Limit Inference Accordingly to [Rog67], an algorithm terminates on every successful computation. In contrast, Inductive inference machines (IIMs) are algorithms which never terminate their computation.
Reference: [AFK + 92] <author> K. Apstis, R. Freivalds, M. Krik~is, R. Simanovskis, and J. Smotrovs. </author> <title> Unions of identifiable classes of total recursive functions. </title> <editor> In K. Jantke, editor, </editor> <booktitle> Analogical and Inductive Inference, </booktitle> <pages> pages 99-107. </pages> <publisher> Springer, </publisher> <year> 1992. </year> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> No. </volume> <pages> 642. </pages>
Reference-contexts: Is it true that U 1 [ U 2 [ U 3 2 I? An affirmative answer means that [2; 3]I = I. This approach was used in <ref> [AFK + 92] </ref>. The inclusion [k; k + 1]I I has many combinatorial consequences. Since nearly all learning types I have this inclusion for some k, the results obtained in this chapter can be used in many situations. <p> The same is true for the language learning type TxtEx (see [JS95]). For learning type EX 1 , i.e. limit learning with at most one mindchange, we have [k; n]EX 1 = EX 1 iff k n &gt; 6 7 (see <ref> [AFK + 92] </ref>). We notice that all constants: 1 3 ; 6 7 are in form 1 1 m . We show that it is not a coincidence. The result is combinatorial and uses only the simplest set-theoretic properties of classes learnable in the sense of some inference type I.
Reference: [AFS96] <author> K. Apstis, R. Freivalds, and C. Smith. </author> <title> On duality in learning and the selection of learning teams. </title> <journal> Information and Computation, </journal> <volume> 129 </volume> <pages> 53-62, </pages> <year> 1996. </year>
Reference-contexts: More often we are interested in a class of functions learned by some machine than by a family of machines which can learn some function (the latter approach is pursued in Chapter 7 of this thesis; see also <ref> [AFS96] </ref>).
Reference: [AFS97] <author> K. Apstis, R. Freivalds, and C. Smith. </author> <title> Asymmetric team learning. </title> <booktitle> Proceedings of COLT'97, </booktitle> <pages> pages 90-95, </pages> <year> 1997. </year>
Reference-contexts: Reader may choose to skip it and instead look up unusual notation in the notation list (see page 76). Nevertheless, Section 2.5 is a key to understanding of nearly all subsequent results. Section 3.1 introduces a generalized notion of team learning which was first published in <ref> [AFS97] </ref>. While not part of traditional inductive inference, asymmetric teams are used throughout this thesis. The entire Chapter 3 motivates the usefulness of team matrices describing both symmetric and asymmetric teams. <p> Clearly, the notation makes sense only if 0 k n. Traditionally a team was defined as a set of learning machines, but since the introduction of asymmetric teams in <ref> [AFS97] </ref> where the order of machines in the team matters, the team is rather an ordered list of machines, thus justifying the vector notation: (M 1 ; : : : ; M n ). <p> Here describe their properties which connect to the use of DNFs and matrices to describe teams. We illustrate these operations for the learning type FIN . 3.1 Asymmetric Teams We generalize the notion of team learning accordingly to <ref> [AFS97] </ref>. In the symmetric teams [k; n] machines play symmetric roles, i.e. it does not matter which k of them succeed. In the thesis we interpret any matrix of 0's and 1's as a learning type . Consider the matrix A in Figure 1.7.
Reference: [Amb96] <author> A. Ambainis. </author> <title> Probabilistic pfin-type learning: general properties. </title> <booktitle> In Proceedings of the 9th Conference on Computational Learning Theory. ACM, </booktitle> <year> 1996. </year>
Reference-contexts: All entries which are left blank (i.e. everything except the 1st row, the last column and one side diagonal) are assumed to be 0. From Example 5.6 one can obtain an easy recursive algorithm to compute game-values for trees. In <ref> [Amb96] </ref> they are shown to be cut-points in probabilistic PFIN hierarchy. 57 A = 6 6 6 6 1 1 : : : 1 1 0 ff 2 ff 2 . . . ff k ff k 7 7 7 7 5.2 Relations between Probabilities and Teams Lemma 5.7 Let T
Reference: [Bar74] <author> J. Barzdins. </author> <title> Two theorems on the limiting synthesis of functions. </title> <editor> In J. Barzdi~ns, editor, </editor> <booktitle> Theory of Algorithms and Programs, </booktitle> <volume> volume 1, </volume> <pages> pages 82-88. </pages> <institution> Latvian State University, Riga, </institution> <year> 1974. </year> <note> In Russian. </note>
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning [Pit89], teams [OSW86b], mind-changes and anomalies <ref> [BB75, CS83] </ref>, oracles [PGJS90] and queries [GS92].
Reference: [CS78] <author> J. Case and C. Smith. </author> <title> Anomaly hierarchies of mechanized inductive inference. </title> <booktitle> In Proceedings of the 10th Symposium on the Theory of Computing, </booktitle> <pages> pages 314-319, </pages> <address> San Diego, CA, </address> <year> 1978. </year>
Reference-contexts: BC fl in <ref> [CS78] </ref>). Property (2) is trivially satisfied when the goal of learning is identification, i.e. finding full description of the target function. It could be violated when the goal is classification or prediction.
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25(2) </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning [Pit89], teams [OSW86b], mind-changes and anomalies <ref> [BB75, CS83] </ref>, oracles [PGJS90] and queries [GS92]. <p> Often the only relations which hold between the different types of machine learning are those which trivially follow from their definitions. E.g. in the mindchange-anomaly hierarchy of limit learning EX a b EX c d iff a c and b d <ref> [CS83] </ref>. It was hard to develop a mathematical theory in a situation where almost every time two different definitions lead to two different concepts. The initial stage in the research of inductive inference was mostly descriptive: there were more and more new learning types and demonstrations of their differences.
Reference: [DK93] <author> R. Daley and B. Kalyanasundaram. </author> <title> Use of reduction arguments in determining popperian fin-type learning capabilities. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 744 </volume> <pages> 173-186, </pages> <year> 1993. </year> <month> 79 </month>
Reference: [DK95] <author> R. Daley and B. Kalyanasundaram. </author> <title> Towards reduction arguments for finite learning. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 961 </volume> <pages> 63-75, </pages> <year> 1995. </year>
Reference-contexts: By Example 4.8 [2; 5] can label it, but other matrices cannot. Apply Theorem 4.12 to diagonalize. In this example U 1 and U 4 are trivial; U 2 and U 6 can be proved directly by tree methods which were systematically introduced in <ref> [DK95] </ref>. On the other hand, U 3 and U 5 can be constructed using asymmetric team meet and the general algorithm. Example 4.20 Let A 1 ; : : : ; A m and B 1 ; : : : ; B n be two collections of team matrices.
Reference: [DK96] <author> R. Daley and B Kalyanasundaram. </author> <title> Finite learning capabilities and their limits. </title> <note> Available from http://www.cs.pitt.edu/~daley/fin/fin.html, 1996. </note>
Reference-contexts: Theorem 2.44 ([DPVW91]) Let k; n 2 N and k+1 n+2 &lt; p 1. Then FIN hpi [k; n]FIN . Corollary 2.45 ([DPVW91]) k+1 n+2 &lt; a b implies [a; b]FIN [k; n]FIN . Accordingly to <ref> [DPVW91, DK96] </ref> we have H FIN = 1; 3 3 ; 7 5 ; 11 1 ; 49 20 ; 37 17 ; : : : 25 10 ; : : : : Below 10=21 our knowledge about cut-points is limited; in general, we cannot tell whether a given number p <p> E.g. the biggest cut-point in [0; 1=2) is 24=49. Previous work in this area <ref> [DPVW91, DKV92, DKV93, DK96] </ref> used ad-hoc methods to come up with these constants. Even though [DK96] develops some general methods of finding cut-points, they only work above the probability 10 21 . <p> E.g. the biggest cut-point in [0; 1=2) is 24=49. Previous work in this area [DPVW91, DKV92, DKV93, DK96] used ad-hoc methods to come up with these constants. Even though <ref> [DK96] </ref> develops some general methods of finding cut-points, they only work above the probability 10 21 . In this work we reduce the problem of comparing FIN teams to combinatorial optimization on unusual objects we call metatrees.
Reference: [DKV92] <author> R. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <editor> In L. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 203-217. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Soon after that the following result was proved: probabilistic FIN hpi-learning with p 2 ( 24 49 ; 1 2 ] can be simulated by team [2; 4]FIN ; on the other hand, the team [24; 49]FIN can learn more than [2; 4]FIN . The authors of <ref> [DKV92] </ref> used "trial and error" to come up with the ratio 24=49, therefore one can ask an interesting, informal question: Where does the 24/49 come from? Figure 2.3 shows all the known FIN cut-point hierarchy in the interval [10=21; 1]. <p> E.g. the biggest cut-point in [0; 1=2) is 24=49. Previous work in this area <ref> [DPVW91, DKV92, DKV93, DK96] </ref> used ad-hoc methods to come up with these constants. Even though [DK96] develops some general methods of finding cut-points, they only work above the probability 10 21 .
Reference: [DKV93] <author> R. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Capabilities of fallible finite learning. </title> <booktitle> In The 1993 Workshop on Computational Learning Theory. The Association for Computing, </booktitle> <year> 1993. </year>
Reference-contexts: E.g. the biggest cut-point in [0; 1=2) is 24=49. Previous work in this area <ref> [DPVW91, DKV92, DKV93, DK96] </ref> used ad-hoc methods to come up with these constants. Even though [DK96] develops some general methods of finding cut-points, they only work above the probability 10 21 .
Reference: [DPVW91] <author> R. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In M. Warmuth and L. Valiant, editors, </editor> <booktitle> Proceedings of the 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 228-239, </pages> <address> Palo Alto, CA., 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Theorem 2.44 ([DPVW91]) Let k; n 2 N and k+1 n+2 &lt; p 1. Then FIN hpi [k; n]FIN . Corollary 2.45 ([DPVW91]) k+1 n+2 &lt; a b implies [a; b]FIN [k; n]FIN . Accordingly to <ref> [DPVW91, DK96] </ref> we have H FIN = 1; 3 3 ; 7 5 ; 11 1 ; 49 20 ; 37 17 ; : : : 25 10 ; : : : : Below 10=21 our knowledge about cut-points is limited; in general, we cannot tell whether a given number p <p> A corollary of this is the inclusion [ma; mb]EX = [a; b]EX for any positive a; b; m. The initial results about the type FIN were just as simple. All probabilistic types FIN hpi, p &gt; 1=2, are equivalent to certain team types <ref> [DPVW91] </ref>. The first 30 surprise was the proper inclusion [1; 2]FIN [2; 4]FIN in [Vel89]. This differs sharply from the behavior of type EX . <p> Similarly [B]I [A _ B]I. Example 3.35 Accordingly to <ref> [DPVW91] </ref>, [1; 3]FIN and [2; 5]FIN are incomparable, i.e. [1; 3]FIN [2; 5]FIN 6= ; and [2; 5]FIN [1; 3]FIN 6= ;. (See also Examples 4.8 and 4.9, and Theorem 4.12.) The join [1; 3] _ [2; 5] is shown in By Theorem 3.34, [[1; 3] _ [2; 5]]FIN [1; 3]FIN <p> E.g. the biggest cut-point in [0; 1=2) is 24=49. Previous work in this area <ref> [DPVW91, DKV92, DKV93, DK96] </ref> used ad-hoc methods to come up with these constants. Even though [DK96] develops some general methods of finding cut-points, they only work above the probability 10 21 .
Reference: [Fau77] <author> V. Fausboll, </author> <title> editor. Buddhist birth stories or Jataka tales. </title> <publisher> Arno Press Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: Such an outside authority for success in learning is necessary, since inductive inference typically studies claims which cannot be verified by the learners themselves. Many learners can learn more than one A Buddhist tale <ref> [Fau77] </ref> describes six blind men learning an elephant. They give conflicting descriptions of it as a pillar, a rope, a trunk, a fan, a wall and a solid pipe. Along comes a wise man who tells that they all are right.
Reference: [Fre91] <author> R. Freivalds. </author> <title> Inductive inference of recursive functions: Qualitative theory. </title> <editor> In J. Barzdi~ns and D. Bjtrner, editors, </editor> <booktitle> Baltic Computer Science, volume 502 of Lecture Notes in Computer Science, </booktitle> <pages> pages 77-110. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: If M has no meaningful result to output, it outputs the empty conjecture ?. Although it is possible to encode ? by a natural number, we will assume that ? is a special symbol, ? 62 N. See Example 2.31 for a particular case of a folklore result (see <ref> [Fre91] </ref>) claiming that requiring M to be total usually does not affect learning power of M . <p> This property is shared also by "behaviorally correct" and "consistent" learning types BC ; CONS (they are defined in <ref> [Fre91] </ref>). In contrast, learning type FIN has a weaker property: [k; n]FIN = FIN whenever k 3 (see Theorem 2.43). The same is true for the language learning type TxtEx (see [JS95]).
Reference: [FS93] <author> R. Freivalds and C. Smith. </author> <title> On the power of procrastination for machine learning. </title> <journal> Information and Computation, </journal> <volume> 107 </volume> <pages> 237-271, </pages> <year> 1993. </year>
Reference-contexts: Property (3) is satisfied for learning types like EX and FIN , i.e. learners cannot compute in the limit whether they are learning their target function. Violating (4) causes collapse of team learning hierarchy. This is possible, e.g., for learning types with recursive ordinal bounds on mindchanges <ref> [FS93] </ref>. Property (5) holds for most learning types; it is violated for FIN 2 : one-shot learning with 2 anomalies and some kinds of learning "almost" minimal indices. The absence of (5) makes team learning degenerate to trivial inclusions (see Example 2.32 and Theorem 3.6).
Reference: [GO64] <author> B.R. Gelbaum and J.M.H. Olmsted. </author> <title> Counterexamples in analysis. </title> <publisher> Holden-Day, </publisher> <address> San Francisco, </address> <year> 1964. </year>
Reference-contexts: It is done by mathematical, not philosophical, means. There is one good aspect about FIN -learning, namely the methods for building unusual function classes separating different FIN -teams. We elaborate on this point below. Among the books which made me excited when attending college was Counterexamples in Analysis <ref> [GO64] </ref>. It provided a vast collection of unusual mathematical objects like functions, series, integrals, etc. which demonstrate the differences between close concepts in real analysis. Sometimes these "counterexamples" (e.g. unmeasurable sets of reals; continuous functions without derivatives etc.) are regarded as mere pathologies which do not deserve much attention.
Reference: [Gol67] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: However, IIMs have a specific way to interpret their output. Namely, the output is regarded as a result of learning the object described by the input data. Let us consider the oldest model of learning, TxtEx <ref> [Gol67] </ref>. Here an IIM receives natural numbers from some recursively enumerable set in arbitrary order. It tries to come up with an index for that recursively enumerable set. It is ggiven, that any number in the set eventually appears in the input.
Reference: [GS92] <author> W. Gasarch and C. Smith. </author> <title> Learning via queries. </title> <journal> Journal of the ACM, </journal> <pages> pages 649-674, </pages> <year> 1992. </year>
Reference-contexts: There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning [Pit89], teams [OSW86b], mind-changes and anomalies [BB75, CS83], oracles [PGJS90] and queries <ref> [GS92] </ref>. For each learning type inductive inference we have to find problems which are algorithmically unsolvable, otherwise the study of that type does not develop in a recursion-theoretic fashion. 1.2.2 Mindchanges In this thesis we consider just three modifications of EX : mindchanges, probabilities and teams.
Reference: [JS95] <author> S. Jain and A. Sharma. </author> <title> On aggregating teams of learning machines. </title> <journal> Theoretical Computer Science, </journal> <volume> 137(1) </volume> <pages> 85-108, </pages> <year> 1995. </year> <month> 80 </month>
Reference-contexts: In contrast, learning type FIN has a weaker property: [k; n]FIN = FIN whenever k 3 (see Theorem 2.43). The same is true for the language learning type TxtEx (see <ref> [JS95] </ref>). For learning type EX 1 , i.e. limit learning with at most one mindchange, we have [k; n]EX 1 = EX 1 iff k n &gt; 6 7 (see [AFK + 92]). We notice that all constants: 1 3 ; 6 7 are in form 1 1 m . <p> The most "natural" limit learning type TxtEx for recursively enumerable sets has team inclusions similar to the type FIN (see <ref> [JS95] </ref>).
Reference: [Kur58] <author> C. </author> <title> Kuratowski. </title> <booktitle> Topologie, 4 th edition, volume 20-21 of Monografie Matematyczne. </booktitle> <address> Panstwowe Wydawnictwo Naukowe, </address> <year> 1958. </year>
Reference: [NW63] <author> C. Nash-Williams. </author> <title> On well-quasi-ordering finite trees. </title> <journal> Proc. Cam-bridge Phil. Soc., </journal> <volume> 59 </volume> <pages> 833-853, </pages> <year> 1963. </year>
Reference: [OSW86a] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: On the other hand, we might simply choose as the child's "official" conjecture at a given moment the grammar assigned highest subjective probability at that moment. <ref> [OSW86a] </ref>, Chapter 3. This illustration does not take into consideration the massive nonverbal communication which accompanies the "linguistic input." By just listening to grammatically correct sentences, e.g. on radio, children cannot learn any language.
Reference: [OSW86b] <author> D. N. Osherson, M. Stob, and S. Weinstein. </author> <title> Aggregating inductive expertise. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 69-95, </pages> <year> 1986. </year>
Reference-contexts: There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning [Pit89], teams <ref> [OSW86b] </ref>, mind-changes and anomalies [BB75, CS83], oracles [PGJS90] and queries [GS92]. <p> We denote these learners by FIN hpi. For other identification types, e.g. EX , EX m , EX a m , the probabilistic derivatives are defined similarly. 1.2.4 Teams Finally, we introduce team learning <ref> [Smi82, OSW86b] </ref>. Consider a list of n inductive inference machines (M 1 ; : : : ; M n ) receiving the same function in the input. Some of them eventually output conjectures. <p> For example, [1; n]FIN means that we can have n FIN -learners from which only one has to be correct [Smi82]. It is easy to see that FIN -teams can simulate any fixed number of mindchanges, i.e. EX n [1; n]FIN . Later in <ref> [OSW86b] </ref> teams were generalized to [k; n]FIN , where k machines from n have to be correct. We can also require that FIN -learner is successful on the target function f with some probability p. This derived learning type is denoted FIN hpi.
Reference: [PGJS90] <author> M. Pleszkoch, W. Gasarch, S. Jain, and R. Solovay. </author> <title> Learning via queries to an oracle, </title> <year> 1990. </year>
Reference-contexts: There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning [Pit89], teams [OSW86b], mind-changes and anomalies [BB75, CS83], oracles <ref> [PGJS90] </ref> and queries [GS92]. For each learning type inductive inference we have to find problems which are algorithmically unsolvable, otherwise the study of that type does not develop in a recursion-theoretic fashion. 1.2.2 Mindchanges In this thesis we consider just three modifications of EX : mindchanges, probabilities and teams.
Reference: [Pit89] <author> L. Pitt. </author> <title> Probabilistic inductive inference. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 383-433, </pages> <year> 1989. </year>
Reference-contexts: And even for values it has read, there is no obvious way how to check their consistence with the current conjecture h (see Principle (3) in Subsection 1.1.2). There are many ways to embellish the basic learning types like TxtEx and EX . Most typical are probabilistic learning <ref> [Pit89] </ref>, teams [OSW86b], mind-changes and anomalies [BB75, CS83], oracles [PGJS90] and queries [GS92]. <p> We can also require that FIN -learner is successful on the target function f with some probability p. This derived learning type is denoted FIN hpi. As proven in [Vel89], FIN h1=2i is not the same as [1; 2]FIN in contrast to EX h1=2i = [1; 2]EX proved in <ref> [Pit89] </ref>. Probabilities and teams are two closely related yet different variations on the basic type FIN , they are hard to express each in other's terms (see Theorems 2.44 and 5.9). <p> For the Popperian learning type PFIN there are similar results. For example: Theorem 2.46 ([DK93]) Let k; n 2 N and k+1 n+2 &lt; p 1. Then PFIN hpi [k; n]PFIN . In <ref> [Pit89] </ref> it was shown that not all probabilistic limit learning types EX hpi are different. E.g. if the probability of success p 2 ( 1 n+1 ; 1 n ), then EX hpi can be improved to EX h1=ni.
Reference: [Pla92] <author> Plato. </author> <title> Republic. </title> <publisher> Hackett Publishing Co., </publisher> <address> Indianapolis, </address> <year> 1992. </year>
Reference-contexts: Here we want to discuss the reverse question: can the theorems of inductive inference say something about how humans learn. Plato was perhaps the first author who tried to study the process of knowledge acquisition (see "allegory of cave", <ref> [Pla92] </ref>, Chapter 25). Learners there are represented by chained prisoners who since childhood are exclusively watching shadow projections from some puppet-show, i.e. they are placed in an ancient equivalent of a movie theater. Plato asks how this helps to understand the sunlit world outside the cave.
Reference: [Pol45] <author> G. Polya. </author> <title> How to Solve It. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1945. </year>
Reference-contexts: Let us justify the selection of topics. Extending the traditional notion of symmetric teams to arbitrary (symmetric or asymmetric) teams described by matrices illustrates the idea advocated by Polya <ref> [Pol45] </ref> that a generalized statement can be easier and more fun to prove than a restricted one. Polya calls this "balancing the inductive step".
Reference: [Pop83] <author> K. </author> <title> Popper. Pocket Popper. </title> <publisher> University Press, Oxford, </publisher> <year> 1983. </year>
Reference-contexts: This state of learned ignorance might be a help in many of our troubles. It might be well for all of us to remember that, while differing widely in the various little bits we know, in our infinite ignorance we are all equal. <ref> [Pop83] </ref>, Knowledge without Authority. Inductive inference can measure this ignorance. Assume that the set R of all total recursive functions represents all things we might want to learn. <p> The climber may not merely have difficulties in getting there | he may not know when he gets there, because he may be unable to distinguish, in the clouds, between the main summit and some subsidiary peak. Yet this does not affect the objective existence of the summit... <ref> [Pop83] </ref>, Truth and Approximation to Truth. We claim that truth is even beyond the authority of learning machines. Once we get a conjecture from an inductive inference machine, no one can guarantee its correctness. <p> If the degree of falsifiability is increased, then introducing the hypothesis has actually strengthened the theory: the system now rules out more than it did previously: it prohibits more. <ref> [Pop83] </ref>, Falsificationism versus Conventionalism. Thus Popper advocates adding extra conjectures to the theory, if they would make the theory easier to falsify. In other cases more conjectures are better not because they are easier to falsify, but because voting can eliminate errors without detecting the wrong conjecture. <p> Nevertheless that simplicity is also a disadvantage. K. Popper claims <ref> [Pop83] </ref> that changing conjectures may be necessary when acquiring knowledge. Learning type FIN is so weak that even easy infinite classes of functions (see Example 2.41) are not FIN -learnable. We avoid that difficulty by introducing teams.
Reference: [PS88] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: The absence of (5) makes team learning degenerate to trivial inclusions (see Example 2.32 and Theorem 3.6). This thesis raises the same questions about FIN which were answered for EX a long time ago in <ref> [Smi82, PS88] </ref>. The answers we find are not nearly that satisfying. The reason why we might want to study FIN could be to show that it is not a good model of learning. It is done by mathematical, not philosophical, means.
Reference: [PZ96] <author> L.A. Petrosian and N.A. Zenkevich. </author> <title> Game theory. </title> <address> River Edge, Sin-gapore, </address> <year> 1996. </year>
Reference-contexts: We assume that reader is familiar with standard notions of sets and functions, 1 st order logic, elementary algebraic operations on strings, vectors, matrices. We also use recursion theory [Soa87] and linear programming <ref> [PZ96] </ref>, although we try to introduce the necessary notation and results in these fields. Section 2.1 introduces sets and logic notation, as well as recursive functions and their indices. Section 2.2 defines partially ordered sets and lattices. <p> Each player seeks to maximize his/her payoff. In most textbooks on game theory, Player 1 (who tries to maximize a ij ) picks a row, but Player 2 (who tries to minimize a ij ) picks a column <ref> [PZ96] </ref>. In our thesis all matrix games are defined the other way, as in Definition 2.13. Definition 2.14 Let A be an m fi n matrix. <p> (y 1 ; : : : ; y n ) and x = (x 1 ; : : : ; x m ) are some solutions of these problems, we have fi = P n P m i=1 x i , due to the strong duality theorem in linear programming <ref> [PZ96] </ref>. The mixed strategies q fl = (1=fi)y, p fl = (1=fi)x make a saddle point, and (p fl ) T A (q fl ) = 1=fi is the matrix game value.
Reference: [Rog67] <author> H. Rogers Jr. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: That low man goes on adding one to one, his hundred's soon hit; This high man, aiming at million, misses an unit. Robert Browning. A Grammarian's Funeral. See [Abr74]. 1.2 Models of Learning in Inductive Inference 1.2.1 Limit Inference Accordingly to <ref> [Rog67] </ref>, an algorithm terminates on every successful computation. In contrast, Inductive inference machines (IIMs) are algorithms which never terminate their computation. They receive growing information about some object in stages, and at each stage they output a conjecture about that object, see Figure 1.1. <p> They receive growing information about some object in stages, and at each stage they output a conjecture about that object, see Figure 1.1. Some authors allow IIMs to be partially defined, i.e. to produce no output at some stages. <ref> [Rog67] </ref> discusses enumeration operators which behave similarly to inductive inference machines. However, IIMs have a specific way to interpret their output. Namely, the output is regarded as a result of learning the object described by the input data. Let us consider the oldest model of learning, TxtEx [Gol67]. <p> Threshold functions t k n are important, since they describe success of symmetric teams [k; n]I. We fix some appropriate programming system ' 0 ; ' 1 ; : : : be <ref> [Rog67] </ref>, which allows indexing all computable partial functions by natural numbers. Intuitively, h is a program for the function ' h . The goal of inductive inference is to restore programs h from data describing ' h .
Reference: [Ros82] <author> J.G. Rosenstein. </author> <title> Linear Orderings. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference: [Smi82] <author> C. Smith. </author> <title> The power of pluralism for automatic program synthesis. </title> <journal> Journal of the ACM, </journal> <volume> 29(4) </volume> <pages> 1144-1165, </pages> <year> 1982. </year>
Reference-contexts: A class U R is EX -learnable, if some learning algorithm can eventually find programs for all functions f 2 U by observing their behavior. We can prove that the whole set R cannot be expressed as a finite union of EX -learnable classes (see <ref> [Smi82] </ref> and also Theorem 2.37). <p> The absence of (5) makes team learning degenerate to trivial inclusions (see Example 2.32 and Theorem 3.6). This thesis raises the same questions about FIN which were answered for EX a long time ago in <ref> [Smi82, PS88] </ref>. The answers we find are not nearly that satisfying. The reason why we might want to study FIN could be to show that it is not a good model of learning. It is done by mathematical, not philosophical, means. <p> We denote these learners by FIN hpi. For other identification types, e.g. EX , EX m , EX a m , the probabilistic derivatives are defined similarly. 1.2.4 Teams Finally, we introduce team learning <ref> [Smi82, OSW86b] </ref>. Consider a list of n inductive inference machines (M 1 ; : : : ; M n ) receiving the same function in the input. Some of them eventually output conjectures. <p> Learning type FIN is so weak that even easy infinite classes of functions (see Example 2.41) are not FIN -learnable. We avoid that difficulty by introducing teams. For example, [1; n]FIN means that we can have n FIN -learners from which only one has to be correct <ref> [Smi82] </ref>. It is easy to see that FIN -teams can simulate any fixed number of mindchanges, i.e. EX n [1; n]FIN . Later in [OSW86b] teams were generalized to [k; n]FIN , where k machines from n have to be correct.
Reference: [Smo97] <author> J. Smotrovs. </author> <title> Closedness properties in team learning of recursive functions. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 1208 </volume> <pages> 79-93, </pages> <year> 1997. </year>
Reference-contexts: The smallest number m 2 N such that [m; m + 1]I = I, if it exists, is called closedness degree of I. Write cdeg (I) = m. If such m does not exist, we say that I has infinite closedness degree. 63 This notion first appeared in <ref> [Smo97] </ref>, although there all closedness degrees are defined bigger by 1 unit, i.e. it studies the inclusions [m 1; m]I = I instead of [m; m + 1]I = I. Lemma 6.7 Let I be a learning type with cdeg (I) = m.
Reference: [Soa87] <author> R. I. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: We assume that reader is familiar with standard notions of sets and functions, 1 st order logic, elementary algebraic operations on strings, vectors, matrices. We also use recursion theory <ref> [Soa87] </ref> and linear programming [PZ96], although we try to introduce the necessary notation and results in these fields. Section 2.1 introduces sets and logic notation, as well as recursive functions and their indices. Section 2.2 defines partially ordered sets and lattices. <p> Intuitively, h is a program for the function ' h . The goal of inductive inference is to restore programs h from data describing ' h . R is the set of all total recursive functions <ref> [Soa87] </ref>. ' h denotes the partial recursive function with index h [Soa87]. <p> Intuitively, h is a program for the function ' h . The goal of inductive inference is to restore programs h from data describing ' h . R is the set of all total recursive functions <ref> [Soa87] </ref>. ' h denotes the partial recursive function with index h [Soa87].
Reference: [Tar89] <author> R.E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1989. </year> <month> 81 </month>
Reference-contexts: Later we study mostly the metatrees themselves. Therefore the present thesis is not just about inductive inference. It is more about metatrees, asymmetric team matrices and their partial order relations. Tarjan <ref> [Tar89] </ref> sometimes isolated the study of data structures from the algorithms which use them.
Reference: [Vel89] <author> M. Velauthapillai. </author> <title> Inductive inference with a bounded number of mind changes. </title> <editor> In R. Rivest, D. Haussler, and M. Warmuth, editors, </editor> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <pages> pages 200-213, </pages> <address> Palo Alto, CA., 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Later in [OSW86b] teams were generalized to [k; n]FIN , where k machines from n have to be correct. We can also require that FIN -learner is successful on the target function f with some probability p. This derived learning type is denoted FIN hpi. As proven in <ref> [Vel89] </ref>, FIN h1=2i is not the same as [1; 2]FIN in contrast to EX h1=2i = [1; 2]EX proved in [Pit89]. <p> The initial results about the type FIN were just as simple. All probabilistic types FIN hpi, p &gt; 1=2, are equivalent to certain team types [DPVW91]. The first 30 surprise was the proper inclusion [1; 2]FIN [2; 4]FIN in <ref> [Vel89] </ref>. This differs sharply from the behavior of type EX .
Reference: [Weg87] <author> I. Wegener. </author> <title> The complexity of Boolean functions. </title> <address> Willey-Teubner, New York, </address> <year> 1987. </year>
Reference-contexts: The greatest cut-point p 2 [0; 1) in the probabilistic hierarchy H I is p = 1 1 m for some m 2 N. Complexity of threshold functions (i.e. "voting machines") when constructed by logical elements such as conjunctions and negations has been extensively stud ied <ref> [Weg87] </ref>. Here we try to express threshold function t r s in terms of another threshold function t k n . The construction is possible only if both ratios r s and k are in the same interval (1 1 m ; 1].
Reference: [Wie74] <author> R. Wiehagen. </author> <title> Inductive inference of recursive functions. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 32 </volume> <pages> 462-464, </pages> <year> 1974. </year>
Reference: [Wie76] <author> R. Wiehagen. </author> <title> Limes-erkennung rekursiver funktionen durch spezielle strategien. </title> <journal> Elektronische Informationsverarbeitung und Kybernetik, </journal> <volume> 12 </volume> <pages> 93-99, </pages> <year> 1976. </year>
References-found: 43

