URL: http://www.cs.princeton.edu/prism/papers-ps/svm-irregular-computer.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: liv,jps,li@cs.princeton.edu  
Author: Iftode, Jaswinder Pal Singh and Kai Li 
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science Princeton University  
Note: Liviu  
Abstract: Technical Report TR-514-96. Irregular Applications under Software Shared Memory Abstract Shared Virtual Memory (SVM) provides an inexpensive way to support the popular shared address space programming model on networks of workstations or personal computers. Despite recent advances in SVM systems, their performance for all but coarse-grained or regular applications is not well understood. Nor is there an understanding of whether and how fine-grained, irregular programs should be written differently for SVM, with its large granularities of communication and coherence, than for the more familiar hardware coherent at cache line granularity. In this paper we try to understand the performance and programming issues for emerging, irregular applications on SVM systems. We examine performance on both an aggressive all-software system as well as one with a little hardware support in the network interface. We also present approaches to improve the performance of irregular applications at both the programming and the system level. As a result of our experiences, we identify a set of guidelines and techniques that pertain specifically to programming SVM systems, beyond the guidelines commonly used for programming hardware-coherent systems as well. We also present a further relaxation of the memory consistency model, called scope consistency, which is particularly effective for such applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <address> Seattle, Washington, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: All Software Release Consistency Both Release Consistency (RC) [] and Lazy Release Consistency (LRC) [] have been implemented as software multiple-writer protocols for SVM. Munin <ref> [1] </ref> and TreadMarks [8] are examples of RC and LRC, respectively. In these systems, a processor locally records the changes it makes to each shared page between two synchronization events. The record of changes for a given page is called a diff.
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Saw-don. The Midway distributed shared memory system. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Another way to reduce false sharing effects is to relax the consistency model beyond lazy release consistency. The well-known Entry Consistency (EC) <ref> [2] </ref> model accomplishes this by requiring the program to explicity bind data to synchronzation variables (e.g. locks), and at a synchronization event enforce coherence only for the data protected by that synchronization variable. <p> While Scope Consistency can provide similar benefits to Entry Consistency <ref> [2] </ref> and exploits the same intuition, it does not require explicit binding between data and synchronizations and hence does not change the programming interface. Scope Consistency can be supported on the same kind of hardware as AURC [13].
Reference: [3] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the SHRIMP multi-computer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: While cache-line based approaches are trying to reduce hardware support, some SVM systems have explored integrating a small amount of hardware support into the network interfaces of commodity workstations or PCs. An example is the network interface of the SHRIMP multicomputer <ref> [3] </ref>, a network of commodity PCs being built at Princeton University. The network interface supports a fine-grained automatic update mechanism.
Reference: [4] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Performance evaluations so far have focused on applications that are quite regular or have coarse-grained data sharing patterns (such as traditional array-based scientific applications) <ref> [5, 4, 6] </ref>. They indicate that SVM can work reasonably when data sharing is either small or coarse-grained.
Reference: [5] <author> S. Dwarkadas, P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Performance evaluations so far have focused on applications that are quite regular or have coarse-grained data sharing patterns (such as traditional array-based scientific applications) <ref> [5, 4, 6] </ref>. They indicate that SVM can work reasonably when data sharing is either small or coarse-grained.
Reference: [6] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Performance evaluations so far have focused on applications that are quite regular or have coarse-grained data sharing patterns (such as traditional array-based scientific applications) <ref> [5, 4, 6] </ref>. They indicate that SVM can work reasonably when data sharing is either small or coarse-grained. <p> For example if local page A at processor P0 is mapped to page B at processor P1, then all writes which are performed locally on page A are also automatically propagated to page B at node P1. This simple hardware support is enough to build a multiple writer protocol <ref> [6, 10] </ref>, as long as coherence is still supported in software as before.
Reference: [7] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding the performance of shared virtual memory from an applications perspective. </title> <booktitle> In Proceedings of the 24th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: As we shall see, a software-based SVM coherence protocol can leverage this mechanism to reduce the overhead of communication Despite all these developments, the performance of SVM or software shared memory systems is very sensitive to the data referencing patterns of applications, much more so than that of hardware-coherent systems <ref> [7] </ref>. Performance evaluations so far have focused on applications that are quite regular or have coarse-grained data sharing patterns (such as traditional array-based scientific applications) [5, 4, 6]. They indicate that SVM can work reasonably when data sharing is either small or coarse-grained.
Reference: [8] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: All Software Release Consistency Both Release Consistency (RC) [] and Lazy Release Consistency (LRC) [] have been implemented as software multiple-writer protocols for SVM. Munin [1] and TreadMarks <ref> [8] </ref> are examples of RC and LRC, respectively. In these systems, a processor locally records the changes it makes to each shared page between two synchronization events. The record of changes for a given page is called a diff.
Reference: [9] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Eventually P0 accesses the page and incurs a page fault. The page fault handler requests the diffs for that page from the other processors that may have more recent copies (a time proceeds from left to right virtual timestamp mechanism <ref> [9] </ref> that we shall not describe here is used to determine this) and causes the relevant diffs to be fetched from those processors (P1 in our case). These diffs are then applied to the local copy in the right order.
Reference: [10] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using memory-mapped network interfaces to improve the performance of distributed shared memory. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: For example if local page A at processor P0 is mapped to page B at processor P1, then all writes which are performed locally on page A are also automatically propagated to page B at node P1. This simple hardware support is enough to build a multiple writer protocol <ref> [6, 10] </ref>, as long as coherence is still supported in software as before.
Reference: [11] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Hein-lein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The stanford flash multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The space of proposals is diverse, with various efforts trying to integrate the communication controller that provides the necessary sup port further and further away from the processor-cache subsystem. These efforts include using a flexible customized controller integrated on the memory bus <ref> [11] </ref>, a commodity controller on the memory bus [14], and a decoupled controller partially on the memory bus and partially on the I/O bus.
Reference: [12] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: This approach, called shared virtual memory (SVM), uses the operating system's support for virtual memory management to provide naming, replication and coherence at the granularity of pages instead of cache lines <ref> [12] </ref>. However, in addition to the problem of software overhead for communication it introduces, this approach suffers from problems related to the large granularity of communication and coherence (an entire page). The two major problems are false sharing and fragmentation. <p> In handling the fault, the SVM system fetches a copy of the page into the local memory from the remote node and establishes the mapping. SVM systems usually maintain coherence among multiple page copies with an invalidation-based protocol. Early approaches <ref> [12] </ref> implement the sequential consistency model, which expects updates to shared memory to become visible in the same order at all nodes, just as if they had been produced by concurrent processes on a uniprocessor system.
Reference: [13] <author> Jaswinger Pal Singh Liviu Iftode and Kai Li. </author> <title> Scope consistency: a bridge between release consistency and entry consistency. </title> <type> Technical Report TR-509-96, </type> <institution> Princeton, NJ, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: This can reduce false sharing invalidations, but it has a high programming cost for making these explicit associations. Scope consistency <ref> [13] </ref> is new consistency model which achieves this effect by assuming an implicit association between data and synchronization such as locks, relying on the association naturally assumed by the programmer in most cases. <p> This scheme doesn't require any annotations or explicit associations, but a programming precaution must be taken: We must ensure that critical sections are sized to cover all the updates which are intended to be visible to the processor that next acquires the lock. Details can be found in <ref> [13] </ref>. through an example which actually corresponds to the sharing patterns encountered in Barnes or Raytrace. Lock L defines a consistency scope in which page B was modified by processor P0. <p> While Scope Consistency can provide similar benefits to Entry Consistency [2] and exploits the same intuition, it does not require explicit binding between data and synchronizations and hence does not change the programming interface. Scope Consistency can be supported on the same kind of hardware as AURC <ref> [13] </ref>.
Reference: [14] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These efforts include using a flexible customized controller integrated on the memory bus [11], a commodity controller on the memory bus <ref> [14] </ref>, and a decoupled controller partially on the memory bus and partially on the I/O bus. One extreme in the spectrum that predates these efforts is to support the coherent shared address space entirely in software, using networks of commodity workstations or personal computers with no additional hardware support.
References-found: 14

