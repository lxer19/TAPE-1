URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/SchKroBabKri96.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: g.schram@et.tudelft.nl  Email: krose@fwi.uva.nl  
Title: NEUROCONTROL BY REINFORCEMENT LEARNING  
Author: G. Schram B.J.A. Krose flfl R. Babuska A.J. Krijgsman 
Address: P.O.Box 5031, 2600 GA Delft, The Netherlands.  Amsterdam, Kruislaan 403, 1098 SJ Amsterdam, The Netherlands.  
Affiliation: Department of Electrical Engineering, Delft University of Technology,  flfl Department of Computer Systems, University of  
Abstract: Reinforcement learning (RL) is a model-free tuning and adaptation method for control of dynamic systems. Contrary to supervised learning, based usually on gradient descent techniques, RL does not require any model or sensitivity function of the process. Hence, RL can be applied to systems that are poorly understood, uncertain, nonlinear or for other reasons untractable with conventional methods. In reinforcement learning, the overall controller performance is evaluated by a scalar measure, called reinforcement. Depending on the type of the control task, reinforcement may represent an evaluation of the most recent control action or, more often, of an entire sequence of past control moves. In the latter case, the RL system learns how to predict the outcome of each individual control action. This prediction is then used to adjust the parameters of the controller. The mathematical background of RL is closely related to optimal control and dynamic programming. This paper gives a comprehensive overview of the RL methods and presents an application to the attitude control of a satellite. Some well known applications from the literature are reviewed as well. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson C.W. </author> <year> (1987), </year> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> Proceedings 4th International Workshop on Machine Learning, </booktitle> <address> Irvine, USA, </address> <pages> pp 103-114. </pages>
Reference: <author> Barto A., Sutton R., Anderson C.W. </author> <year> (1983), </year> <title> Neuron-like adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol 13, No 3, </volume> <pages> pp 834-846. </pages>
Reference-contexts: Moreover, modeling errors and time-varying parameters of the process can degrade the controller performance considerably. Therefore, techniques for tuning the controller without an explicit model of the process are desirable. An interesting approach to tune nonlinear controllers without a model is reinforcement learning (RL) <ref> (Barto et al., 1983) </ref>. The idea of RL originates from human and animal learning, based on repeated trials followed by a reward or punishment. Each trial may consist of a sequence of actions while the feedback (reinforcement), related to the achieved performance, is received at the end. <p> Section 5 concludes the paper. 2. REINFORCEMENT LEARNING 2.1 Basic concepts Reinforcement learning refers to a class of adaptation mechanisms where only a scalar evaluation 'r' (reinforcement) of the system performance is available <ref> (Barto et al., 1983) </ref>. This method differs from other (supervised) learning methods where explicit input-output examples of the controller behavior must be reconstructed using a model of the plant. Instead, RL aims at maximizing the expected performance in terms of the received reinforcement. <p> The parameters are then updated by: v k+1 = v k + v k : (8) 2.5 Implementation of critic and controller In the early applications of RL, the critic and the controller were implemented as look-up tables <ref> (Barto et al., 1983) </ref>. More recently, the use of Artificial Neural Networks (ANN) has been studied for this purpose (An-derson, 1989; Krose, 1995). However, it may be difficult to achieve convergence, since two learning systems are active simultanuously and the neural networks are trained by nonlinear optimization methods like backpropagation. <p> In this way, the `improved' estimate of the cost function is more accurate, and the effect of the modified control action is more clear (known as the credit assignment problem <ref> (Barto et al., 1983) </ref>). In the satellite control problem discussed in section 4, good results are achieved by adapting the critic and the controller after each sixth time step. 3. RELATION TO OPTIMAL CONTROL Control systems based on RL aim at optimizing performance by iteratively adapting the controller.
Reference: <author> Bellman R.E., Dreyfus S.E. </author> <year> (1962), </year> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press. </publisher> <address> Prin-ceton, New Jersey. </address>
Reference-contexts: The cost function (1) can be written as: V k = r k + fl i=k+1 = r k + flV k+1 : (2) Maximizing the cost function V k is equivalent to maximizing r k and V k+1 . This principle is known as the Bellman's optimality principle <ref> (Bellman and Dreyfus, 1962) </ref>, see also section 3. The reinforcement r k at the current time instant k is known, but the cost function V k+1 cannot be computed, since it depends on the future reinforcements. <p> Reinforcement learning is also referred to as heuristic dynamic programming (Sutton et al., 1992). Dynamic programming is a tool for solving optimal control problems with general performance criteria and nonlinear systems, based on the Bellman optimality principle <ref> (Bellman and Dreyfus, 1962) </ref>. The main difference is that dynamic programming is applied off-line, while RL tries to learn the optimal policy on-line, concurrently with the system's operation. RL also does not require an model of the system and can be applied to continuous control commands u.
Reference: <author> Buijtenen W.M., Schram G., Babuska R., Verbruggen H.B. </author> <year> (1996), </year> <title> Adaptive fuzzy control of satellite attitude by reinforcement learning. </title> <note> Submitted to IEEE transactions on Fuzzy Systems. </note>
Reference-contexts: Since the nonlinear characteristics vary for weak and strong stars, adaptation features are also desirable in order to maintain satisfactory performance under all conditions. With respect to the above attitude control problem, RL shows some interesting properties <ref> (Buijtenen et al., 1996) </ref>. First of all, a nonlinear controller could cope with the nonlinear discontinuities of the sensor output. Secondly, no model of the satellite is required, which can save a lot of modeling effort. Finally, RL provides automatic adaptation of the controller to specific situations. <p> The value of fl in the cost function is fixed at 0.5. This appeared to be a good value to determine the (delayed) effects of new control commands. The critic is implemented by a neuro-fuzzy network with 25 adjustable parameters <ref> (Buijtenen et al., 1996) </ref>. The inputs of the network are the filtered attitude error and attitude error rate e; _e. From these signals the critic estimates the cost function ^ V . Notice that the control command u is not used as an input signal for the critic (Figure 2).
Reference: <author> Crites R.H., Barto A.G. </author> <year> (1995), </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> Proceedings of 8th Neural Information Processing Systems Conference, </booktitle> <address> Denver, Colorado. </address>
Reference-contexts: Successful applications have also been reported in other fields. A manufacturing process for thermoplastic composite structures was controlled by a RL system (Sofge and White, 1992), capable of on-line tuning. Another interesting application is the control of a set of elevators <ref> (Crites and Barto, 1995) </ref>, where the expected waiting time of all waiting passengers serves as a cost function. The remaining part of this paper is organised as follows. Section 2 presents the basic concepts of RL and describes the learning algorithms as well as some im-plementational aspects.
Reference: <author> Gullapalli V. </author> <year> (1995), </year> <title> Skillful control under uncertainty via direct reinforcement learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> Vol 15, </volume> <pages> pp 237-246. </pages>
Reference-contexts: RL has been applied in the field of robot learning, enabling robots to improve their performance through interaction with the environment (Krose, 1995). An illustrative example is a RL controller for a robot manipulator in the peg-in-hole insertion task <ref> (Gullapalli, 1995) </ref>. In this application, the performance is evaluated on the basis of the position and the forces acting from the environment on the peg. <p> Instead, RL aims at maximizing the expected performance in terms of the received reinforcement. As an example, consider a robot manipulator whose task is to insert a peg into a hole without using a model of the robot dynamics and of the environment <ref> (Gullapalli, 1995) </ref>. The reinforcement signal r indicates the distance to the hole and the forces acting between the environment and the robot arm at the end of the manipulator movement. The reinforcement signal is usually normalized between 0 and 1, where 1 indicates the desired performance.
Reference: <author> Ilg W., Berns K. </author> <year> (1995), </year> <title> A learning architecture based on reinforcement learning for adaptive control of the walking machine LAURON. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> Vol 15, </volume> <pages> pp 321-334. </pages>
Reference-contexts: In this application, the performance is evaluated on the basis of the position and the forces acting from the environment on the peg. Another example is a RL controller for a walking machine <ref> (Ilg and Berns, 1995) </ref>, where the changes in the positions of the legs are used as the performance measure. Successful applications have also been reported in other fields. A manufacturing process for thermoplastic composite structures was controlled by a RL system (Sofge and White, 1992), capable of on-line tuning.
Reference: <author> Jang J.S.R., Sun C.T. </author> <year> (1995). </year> <title> Neuro-fuzzy modeling and control. </title> <booktitle> Prodeedings of IEEE, </booktitle> <month> March </month> <year> 1995. </year>
Reference: <author> Krose B.J.A. </author> <year> (1995), </year> <title> Learning from delayed rewards. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> Vol 15, No 4, </volume> <pages> pp 233-235. </pages> <note> (Guest editor of special issue on RL). </note>
Reference-contexts: This prediction is then used to tune the parameters of the controller. RL has been applied in the field of robot learning, enabling robots to improve their performance through interaction with the environment <ref> (Krose, 1995) </ref>. An illustrative example is a RL controller for a robot manipulator in the peg-in-hole insertion task (Gullapalli, 1995). In this application, the performance is evaluated on the basis of the position and the forces acting from the environment on the peg.
Reference: <author> Schram G., Karsten L., Krose B.J.A., Groen F.C.A. </author> <year> (1994), </year> <title> Optimal attitude control of satellites by artificial neural networks: a pilot study. </title> <booktitle> Proceedings IFAC Symposium on Artificial Intelligence in Real-Time Control, Valencia, Spain, </booktitle> <pages> pp 185-190. </pages>
Reference-contexts: Hence, it is beneficial to help the search procedure by proper initialization, as illustrated in the following section. 4. SATELLITE ATTITUDE CONTROL The considered satellite is designed for observations of infrared light emitting celestial objects. For details, the reader is referred to <ref> (Schram et al., 1994) </ref>. The attitude control scheme of the satellite is depicted in Figure 2. The attitude and attitude rate are measured by a startracker and a gyroscope, respectively. A filter is emplyed to reconstruct the attitude error and attitude error rate from the noisy measurements. <p> Secondly, no model of the satellite is required, which can save a lot of modeling effort. Finally, RL provides automatic adaptation of the controller to specific situations. This was already shown in a previous study on tuning of a linear PD controller <ref> (Schram et al., 1994) </ref>. 4.1 Reinforcement learning set up The control goal is to reduce the limit cycle, i.e. to keep the attitude error low and simultaneously keep the attitude rate as close as possible to zero.
Reference: <author> Sofge D.A., White D.A. </author> <year> (1992). </year> <title> Handbook of Intelligent Control, Neural, Fuzzy, and Adaptive Approaches. Van Nostrand Reinhold. </title> <journal> Special Issue on Fuzzy Control (1995), Journal-A, </journal> <volume> Vol 36, No 3. </volume>
Reference-contexts: Successful applications have also been reported in other fields. A manufacturing process for thermoplastic composite structures was controlled by a RL system <ref> (Sofge and White, 1992) </ref>, capable of on-line tuning. Another interesting application is the control of a set of elevators (Crites and Barto, 1995), where the expected waiting time of all waiting passengers serves as a cost function. The remaining part of this paper is organised as follows.
Reference: <author> Sutton R.S. </author> <year> (1988), </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> Vol 3, </volume> <pages> pp 9-44. </pages>
Reference-contexts: In order to learn this prediction, the parameters w k are adjusted, using the so-called temporal difference (TD) method <ref> (Sutton, 1988) </ref> described below. At the current time instant k, the control command u k is applied to the system, which moves from the current state x k to the new state x k+1 . Reinforcement r k is received from the environment.
Reference: <author> Sutton R.S., Barto A.G., Williams R.J. </author> <year> (1992), </year> <title> Reinforcement learning is direct adaptive optimal control. </title> <journal> IEEE Control Systems, </journal> <month> April, </month> <pages> pp 19-22. </pages>
Reference-contexts: For instance, in the example in section 4, the reinforcements are computed using a fuzzy performance measure. Reinforcement learning is also referred to as heuristic dynamic programming <ref> (Sutton et al., 1992) </ref>. Dynamic programming is a tool for solving optimal control problems with general performance criteria and nonlinear systems, based on the Bellman optimality principle (Bellman and Dreyfus, 1962).
References-found: 13

