URL: http://ciir.cs.umass.edu/info/psfiles/tepubs/soderland_ijcai95.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/te.html
Root-URL: 
Email: fsoderlan dfisher aseltine lehnertg@cs.umass.edu  
Title: CRYSTAL: Inducing a Conceptual Dictionary  
Author: Stephen Soderland, David Fisher, Jonathan Aseltine, Wendy Lehnert 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts,  
Note: In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95)  
Abstract: One of the central knowledge sources of an information extraction (IE) system is a dictionary of linguistic patterns that can be used to identify references to relevant information in a text. Automatic creation of conceptual dictionaries is important for portability and scalability of an IE system. This paper describes CRYSTAL, a system which automatically induces a dictionary of "concept-node definitions" sufficient to identify relevant information from a training corpus. Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances. Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules.
Abstract-found: 1
Intro-found: 1
Reference: [ Granger, 1977 ] <author> R. Granger. FOUL-UP: </author> <title> A program that figures out meanings of words from context. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 172-178. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1977. </year>
Reference-contexts: It's remarkable that we have gotten good performance from a lexicon that lacks words such as "lesions", "rate", "rhythm", "tenderness", and "distention". 5 Related Work Previous research on inductive learning in natural language processing has concentrated on the semantics of isolated words <ref> [ Granger, 1977; Mooney, 1987; Zernik, 1991 ] </ref> . CRYSTAL is one of the first systems to automatically induce a dictionary of information extraction rules.
Reference: [ Lehnert et al., 1993 ] <author> W. Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peterson, F. Feng, C. Dolan, and S. Goldman. </author> <title> University of Massachusetts/Hughes: Description of the CIRCUS system as used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 277-290, </pages> <year> 1993. </year>
Reference-contexts: Information extracted from the text is represented as case frames, called "concept nodes" (CN's) in the University of Massachusetts BADGER sentence analyzer, which performs selective concept extraction similar to that of the previous CIRCUS system <ref> [ Lehnert, 1991; Lehnert et al., 1993 ] </ref> . The domain knowledge needed to identify relevant references is stored in a dictionary of "CN definitions" that describe the local syntactic and semantic context in which relevant information is likely to be found.
Reference: [ Lehnert, 1991 ] <author> W. Lehnert. </author> <title> Symbolic/subsymbolic sentence analysis: Exploiting the best of two worlds. </title> <editor> In J. Barnden and J. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 1, </volume> <pages> pages 135-164. </pages> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year>
Reference-contexts: Information extracted from the text is represented as case frames, called "concept nodes" (CN's) in the University of Massachusetts BADGER sentence analyzer, which performs selective concept extraction similar to that of the previous CIRCUS system <ref> [ Lehnert, 1991; Lehnert et al., 1993 ] </ref> . The domain knowledge needed to identify relevant references is stored in a dictionary of "CN definitions" that describe the local syntactic and semantic context in which relevant information is likely to be found.
Reference: [ Lindberg et al., 1993 ] <author> D. Lindberg, B. Humphreys, and A. McCray. </author> <title> Unified medical language systems. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 32(4) </volume> <pages> 281-291, </pages> <year> 1993. </year>
Reference-contexts: For the hospital discharge report domain, we are using a semantic lexicon and semantic hierarchy derived from the Unified Medical Language Systems (UMLS) medical MetaThesaurus and Semantic Network <ref> [ Lindberg et al., 1993 ] </ref> , which is currently under development by the National Library of Medicine. A dictionary of CN definitions for this domain is specific to the semantics and writing style of hospital discharge records and could not be transferred to other applications.
Reference: [ Michalski, 1983 ] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference: [ Mitchell, 1982 ] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference: [ Moldovan and Kim, 1992 ] <author> D. Moldovan and J. Kim. PALKA: </author> <title> A system for linguistic knowledge acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> USC Department of Electrical Engineering Systems, </institution> <year> 1992. </year>
Reference-contexts: Each proposed definition had to be reviewed by a human who retained the definitions that looked reasonable, and discarded those that were more dubious (roughly 70% of AutoSlog's pro posed definitions had to be manually discarded). The PALKA system <ref> [ Moldovan and Kim, 1992; Moldovan et al., 1993 ] </ref> used by USC includes an induction step similar to CRYSTAL. PALKA constructs an initial "Frame-Phrasal pattern structure" (FP-structure) from each example clause that has been marked as having information to be extracted.
Reference: [ Moldovan et al., 1993 ] <author> D. Moldovan, S. Cha, M. Chung, T. Gallippi, K. Hendrickson, J. Kim, C. Lin, and C. Lin. </author> <title> USC: Description of the SNAP system used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <pages> pages 305-319, </pages> <year> 1993. </year>
Reference-contexts: Each proposed definition had to be reviewed by a human who retained the definitions that looked reasonable, and discarded those that were more dubious (roughly 70% of AutoSlog's pro posed definitions had to be manually discarded). The PALKA system <ref> [ Moldovan and Kim, 1992; Moldovan et al., 1993 ] </ref> used by USC includes an induction step similar to CRYSTAL. PALKA constructs an initial "Frame-Phrasal pattern structure" (FP-structure) from each example clause that has been marked as having information to be extracted.
Reference: [ Mooney, 1987 ] <author> R. Mooney. </author> <title> Integrated learning of words and their underlying concepts. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 974-978, </pages> <year> 1987. </year>
Reference-contexts: It's remarkable that we have gotten good performance from a lexicon that lacks words such as "lesions", "rate", "rhythm", "tenderness", and "distention". 5 Related Work Previous research on inductive learning in natural language processing has concentrated on the semantics of isolated words <ref> [ Granger, 1977; Mooney, 1987; Zernik, 1991 ] </ref> . CRYSTAL is one of the first systems to automatically induce a dictionary of information extraction rules.
Reference: [ MUC-5, 1993 ] <institution> Proceedings of the Fifth Message Understanding Conference (MUC-5), </institution> <address> San Mateo, CA, August 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: CRYSTAL is one of the first systems to automatically induce a dictionary of information extraction rules. Only two of the seventeen research sites participating in the ARPA-sponsored Fifth Message Understanding Conference <ref> [ MUC-5, 1993 ] </ref> described automatically generated dictionaries, the University of Massachusetts and the University of Southern California.
Reference: [ Riloff, 1993 ] <author> E. Riloff. </author> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 811-816, </pages> <address> Washington, DC, July 1993. </address> <publisher> AAAI Press / MIT Press. </publisher>
Reference-contexts: Only two of the seventeen research sites participating in the ARPA-sponsored Fifth Message Understanding Conference [ MUC-5, 1993 ] described automatically generated dictionaries, the University of Massachusetts and the University of Southern California. The UMass system used the Autoslog dictionary construction tool <ref> [ Riloff, 1993 ] </ref> , which generates a proposed CN definition for each phrase to be extracted from a motivating instance in the text. AutoSlog uses heuristics to select certain exact words from the instance as "trig-ger words", often selecting the head verb.
Reference: [ Zernik, 1991 ] <author> U. Zernik. </author> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: It's remarkable that we have gotten good performance from a lexicon that lacks words such as "lesions", "rate", "rhythm", "tenderness", and "distention". 5 Related Work Previous research on inductive learning in natural language processing has concentrated on the semantics of isolated words <ref> [ Granger, 1977; Mooney, 1987; Zernik, 1991 ] </ref> . CRYSTAL is one of the first systems to automatically induce a dictionary of information extraction rules.
References-found: 12

