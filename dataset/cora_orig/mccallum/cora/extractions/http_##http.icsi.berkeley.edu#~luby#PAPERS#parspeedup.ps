URL: http://http.icsi.berkeley.edu/~luby/PAPERS/parspeedup.ps
Refering-URL: http://http.icsi.berkeley.edu/~luby/optsim.html
Root-URL: http://http.icsi.berkeley.edu
Title: Optimal Parallelization of Las Vegas Algorithms  
Author: Michael Luby Wolfgang Ertel 
Address: 1947 Center Street, Berkeley CA 94704,  
Affiliation: Computer Science Institute,  
Note: Current address: International  Research supported by an ICSI Postdoctoral Fellowship.  
Pubnum: TR-93-041  
Email: er-tel@icsi.berkeley.edu.  
Date: September 1993  
Abstract: Let A be a Las Vegas algorithm, i.e., A is a randomized algorithm that always produces the correct answer when it stops but whose running time is a random variable. In [1] a method was developed for minimizing the expected time required to obtain an answer from A using sequential strategies which simulate A as follows: run A for a fixed amount of time t 1 , then run A independently for a fixed amount of time t 2 , etc. The simulation stops if A completes its execution during any of the runs. In this paper, we consider parallel simulation strategies for this same problem, i.e., strategies where many sequential strategies are executed independently in parallel using a large number of processors. We present a close to optimal parallel strategy for the case when the distribution of A is known. If the number of processors is below a certain threshold, we show that this parallel strategy achieves almost linear speedup over the optimal sequential strategy. For the more realistic case where the distribution of A is not known, we describe a universal parallel strategy whose expected running time is only a logarithmic factor worse than that of an optimal parallel strategy. Finally, the application of the described parallel strategies to a randomized automated theorem prover confirms the theoretical results and shows that in most cases good speedup can be achieved up to hundreds of processors, even on networks of workstations. y Current address: International Computer Science Institute, 1947 Center Street, Berkeley CA 94704, luby@icsi.berkeley.edu. Research supported in part by NSF Grant CCR-9016468 and grant No. 89-00312 from the United States-Israel Binational Science Foundation (BSF), Jerusalem, Israel. ICSI and UC Berkeley 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Luby, A. Sinclair, and D. Zuckerman. </author> <title> Optimal speedup of las vegas algorithms. </title> <note> to appear in: Proceedings of the Second Israeli Symposium on Theory of Computing and Systems, 1993 Technical report TR-93-010, </note> <institution> International Computer Science Institute, Berkeley, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: However, the exact behavior of A (x), and in particular its running time, T A (x), depends on the setting of the seed. For example, for some settings of the seed A (x) could halt in one step whereas for other settings A (x) could run forever. In <ref> [1] </ref>, the following problem was studied in the sequential setting: Find a sequential simulation strategy for A (x) that minimizes the expected time required to get an answer. <p> This paper proves results about parallel simulations that are analogous to the sequential simulation results shown in <ref> [1] </ref>. <p> Let ` k A (x) be the expected running time of this strategy. Similar to the sequential setting <ref> [1] </ref>, ` k A (x) is a natural and easily characterized quantity associated with the distribution of T A (x) . <p> Thus, the problem we address is that of designing an efficient universal strategy, i.e., one that is to be used for all distributions on running times. In <ref> [1] </ref>, a simple universal sequential strategy was introduced. In Section 3 we consider the simple universal parallel strategy PAR k ffi , which we hereafter call . <p> Suppose that we have full knowledge of the distribution p ; is there some strategy S k that is optimal for p , in the sense that E (S k ) = inf S k E (S k ) ? In contrast to the sequential case <ref> [1] </ref>, where the optimal sequential strategy is the repeating sequence S fl = (t fl ; t fl ; t fl ; : : :) for a carefully chosen value of t fl , the optimal parallel strategy is not easy to describe. <p> The uniform repeating strategy S k t can be viewed as a sequential repeating strategy, where each run consists of k parallel independent runs of A (x) with the time bound t . Thus, we can apply the sequential results from <ref> [1] </ref> for computing the expected value of any uniform repeating strategy. <p> In <ref> [1] </ref> the expected value `(t) of a sequential strategy S t = (t; t; t; : : :) was shown to be `(t) j E (S t ) = q (t) t t 0 =0 j P t1 q (t) This can easily be verified as follows. <p> Let t fl be any finite value of t for which `(t) = `, if such a value exists, and t fl = 1 otherwise. From <ref> [1] </ref> we know that for any distribution p , the sequential strategy S fl j S t fl = (t fl ; t fl ; t fl ; : : :) is an optimal sequential strategy for p , and E (S fl ) = `. <p> parallel case we define t k fl as that value of t for which ` k (t) is minimal, in the same way as above, where we now have ` k = inf ` k (t) and ` k (t) = E (S k P t1 q k (t) From <ref> [1] </ref>, it follows that the strategy S k t k fl is the optimal uniform strategy 4 , which we will prove to be close to optimal. <p> Remark: Theorem 1 remains true even for more general strategies, in which the run lengths t i are themselves random variables and runs may be suspended and then restarted at a later time. A proof for the sequential case can be found in <ref> [1] </ref>. 2.2 The best uniform repeating strategy is not optimal Unlike in the sequential case, the optimal uniform strategy is not optimal for all distributions q (t). <p> We define the Speedup Sp opt (k) of the strategy S k fl by relating its expected running time to the optimal sequential strategy S fl Sp opt (k) j E (S k = ` k : Since the sequential strategy S fl has optimal expected running time <ref> [1] </ref>, the speedup Sp opt (k) can at most be k . Unfortunately, it is not true that the speedup is almost linear for all distributions p. <p> Moreover, this performance is achieved by a uniform strategy of a very simple form that is easy to implement in practice. In <ref> [1] </ref> the universal sequential strategy = (1; 1; 2; 1; 1; 2; 4; 1; 1; 2; 1; 1; 2; 4; 8; 1; : : :) was described and proven to be within a logarithmic factor of any optimal sequential strategy. <p> it follows that S k mod fails to stop with probability at most (1=4) j by time 2 j+r , and from this it follows that E () j0 It is not hard to see (and it follows from the proof of the analogous sequential version of this theorem in <ref> [1] </ref>) that the 2 j+r+1 (j + r + 1)-prefix of simulates the 2 j+r prefix of any sequential strategy. <p> Remarks: (a) This proof is similar to the proof of the corresponding theorem for k = 1 given in <ref> [1] </ref>, but here we don't need to refer to an optimal strategy. Moreover, the constant factor derived here is slightly less than in [1]. (b) It might be possible to find a slightly (at most a constant factor) faster parallel strategy by partitioning the runs performed in strategy 1 evenly onto <p> Remarks: (a) This proof is similar to the proof of the corresponding theorem for k = 1 given in <ref> [1] </ref>, but here we don't need to refer to an optimal strategy. Moreover, the constant factor derived here is slightly less than in [1]. (b) It might be possible to find a slightly (at most a constant factor) faster parallel strategy by partitioning the runs performed in strategy 1 evenly onto k processors. Finally we show that the universal strategy is optimal (within a constant factor) among universal strategies.
Reference: [2] <author> W. Ertel. </author> <title> OR-Parallel theorem proving with random competition. </title> <booktitle> Proceedings of Logic Programming and Automated Reasoning, </booktitle> <address> St. Petersburg, </address> <month> July </month> <year> 1992, </year> <booktitle> Springer Lecture Notes in AI Vol. </booktitle> <volume> 624, </volume> <pages> pp. 226-237. </pages>
Reference-contexts: An important application area for this kind of parallelization of randomized algorithms is combinatorial search. Here, the algorithm A (x) consists of random search of a (often highly unbalanced) tree. A straightforward way to parallelize such a randomized search algorithm is to use PAR k (A (x)) . In <ref> [2] </ref> this strategy was called random competition and applied to randomized combinatorial search algorithms with various examples from automated theorem proving, some of which will be used here again. <p> Even if the distribution is not known, for most examples the speedup of is close to linear. One of the benefits of over the straightforward parallelization described in <ref> [2] </ref> is that there is a guarantee that has close to optimal speedup, whereas the speedup obtained on many examples using straightforward parallelization was found to be erratic. <p> If a pure broadcast on the Ethernet is used, the communication time for starting and stopping is constant (i.e. it does not depend on the number of workstations used) and therefore, as many workstations as available can be used. For more details on the implementation see <ref> [2] </ref> and [3]. With this implementation of S k 1 it was possible to prove new theorems which could not be solved by SETHEO. Another significant step forward is expected with a parallel implementation of , since this solves the problem of possible infinite loops as already mentioned.
Reference: [3] <editor> W. Ertel. Parallele Suche mit randomisiertem Wettbewerb in Inferenzsystemen. Infix-Verlag, </editor> <address> St. Augustin, Germany, DISKI-series, </address> <note> vol. 25, 1993 PhD-Thesis, </note> <institution> Technische Univer-sitat Munchen. </institution>
Reference-contexts: If a pure broadcast on the Ethernet is used, the communication time for starting and stopping is constant (i.e. it does not depend on the number of workstations used) and therefore, as many workstations as available can be used. For more details on the implementation see [2] and <ref> [3] </ref>. With this implementation of S k 1 it was possible to prove new theorems which could not be solved by SETHEO. Another significant step forward is expected with a parallel implementation of , since this solves the problem of possible infinite loops as already mentioned.
Reference: [4] <author> V. Kumar, V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> in Vipin Ku-mar, P.S. Gopalakrishnan and Laveen N. Kanal (Hrsg.), </editor> <booktitle> Parallel algorithmus for maschine intelligence and vision Springer Verlag, </booktitle> <address> New York 1990, p. </address> <pages> 1-41. </pages>

References-found: 4

