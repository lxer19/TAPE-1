URL: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/expdes.ps
Refering-URL: http://www.iro.umontreal.ca/~lecuyer/papers.html
Root-URL: http://www.iro.umontreal.ca
Title: Discrete Event Dynamic Systems: Theory and Applications,  Combining the Stochastic Counterpart and Stochastic Approximation Methods  
Author: JEAN-PIERRE DUSSAULT DONALD LABRECQUE AND PIERRE L'ECUYER REUVEN Y. RUBINSTEIN 
Keyword: Score function, sensitivity analysis, optimization, stochastic counterpart, stochastic approximation.  
Address: Sherbrooke J1K 2R1, Canada  C.P. 6128, Succ. Centre-Ville, Montreal H3C 3J7, Canada  32000, Israel  CH-1015 Lausanne, Switzerland  
Affiliation: Departement de mathematiques et informatique, Universite de Sherbrooke,  Departement d'IRO, Universite de Montreal,  Faculty of Industrial Engineering and Management Technion|Israel Institute of Technology, Haifa  and Department of Mathematics, EPFL-Ecublens  
Note: 1-24 (Draft:  c Draft: April 18, 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: dussault@dmi.usherb.ca  lecuyer@iro.umontreal.ca  ierrr01@ie.technion.ac.il  
Date: April 18, 1996)  
Abstract: In this work, we examine how to combine the score function method with the standard crude Monte Carlo and experimental design approaches, in order to evaluate the expected performance of a discrete event system and its associated gradient simultaneously for different scenarios (combinations of parameter values), as well as to optimize the expected performance with respect to two parameter sets, which represent parameters of the underlying probability law (for the system's evolution) and parameters of the sample performance measure, respectively. We explore how the stochastic approximation and stochastic counterpart methods can be combined to perform optimization with respect to both sets of parameters at the same time. We outline three combined algorithms of that form, one sequential and two parallel, and give a convergence proof for one of them. We discuss a number of issues related to the implementation and convergence of those algorithms, introduce averaging variants, and give numerical illustrations. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Asmussen, S. and R. Y. </author> <title> Rubinstein (1992a). The efficiency and heavy traffic properties of the score function method for sensitivity analysis of queueing models. </title> <booktitle> Advances in Applied Probability, </booktitle> <volume> 24, </volume> <pages> 172-201. </pages>
Reference-contexts: The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in <ref> [1] </ref>, [2], [3], [9], [10], [17], [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1. <p> A finite-horizon model can be viewed as a special case of this: just replace ` 2 (v; ) by 1. Under standard regularity conditions allowing interchangeability of expectation and differentiation (e.g., uniform integrability), one has <ref> [1] </ref>, [9], [17], [32]: ` 1 (v; ) = E g o X L t W t ; (3) " o X L t r v W t ; (4) where L t = L t (Z 1 ; : : : ; Z t ; ), W t = Q
Reference: 2. <author> Asmussen, S. and R. Y. </author> <title> Rubinstein (1992b). Performance Evaluation for the Score Function Method in Sensitivity Analysis and Stochastic Optimization. </title> <booktitle> International Workshop on Computer-Intensive Methods in Discrete Event Systems, </booktitle> <address> Vienna, </address> <year> 1990, </year> <editor> (G. Pflug ed.). </editor> <publisher> Springer-Verlag, </publisher> <pages> 1-12. </pages>
Reference-contexts: The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], <ref> [2] </ref>, [3], [9], [10], [17], [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1. <p> Henceforth, we shall assume that g () = f (; v 0 ), where v 0 is a fixed value of v called the reference parameter value. Now the question is how to select v 0 . This has been studied, e.g., in <ref> [2] </ref>, [19], [32]. A good choice of v 0 turns out to be extremely important, because for a given v, the variance of the estimators (6-8) may blow up to a very large value or even become infinite for certain choices of v 0 . <p> In our context, we are also interested in a value of v 0 that does well for all values of v in a certain region. Asmussen and Rubinstein <ref> [2] </ref> and Rubinstein and Shapiro [32] have studied the problem (9) in the context where `(v; ) is the average sojourn time per customer in a single queue. In that context, let ae (v) denote the traffic intensity (which is assumed to depend on v). <p> Similar results were obtained for more complex queueing models for which the performance measure is the average sojourn or waiting time. A general recommendation from <ref> [2] </ref>, [32] is: in order to be on the safe side one should choose v 0 such that ae (v 0 ) is moderately larger than the nominal value ae = ae (v). <p> Example 1 Suppose that the performance measure of interest is the average sojourn time in an M=M=1 queue with traffic intensity ae = ae (v). In this case, ~ae can be found analytically as a function of ae <ref> [2] </ref>. For example, if ae = 0:5, then ~ae 0:8, so that the variance is reduced for ae 0 2 [ae; ~ae] [0:5; 0:8], which is a rather broad interval. <p> In the latter case, the variance increase is typically moderate when ^ae ae is not too large and the cycle length o tends to be small (see <ref> [2] </ref>, [19] for illustrations). If that is not the case, then the set fv 1 ; : : : ; v r 1 g should be partitioned into smaller subsets and a different v 0 chosen over each subset. 2.3.
Reference: 3. <author> Asmussen, S., R. Y. Rubinstein, and C. </author> <title> Wang (1994). Estimating Rare Events via Likelihood Ratios: From M/M/1 Queues to Bottleneck Networks, </title> <journal> Journal of Applied Probability, </journal> <volume> 31, </volume> <pages> 797-815. </pages>
Reference-contexts: The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], <ref> [3] </ref>, [9], [10], [17], [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1.
Reference: 4. <author> Basar, T. </author> <year> (1987). </year> <title> Relaxation techniques and asynchronous algorithms for on-line computation of non-cooperative equilibria, </title> <journal> Journal of Economic Dynamics and Control, </journal> <volume> 11, </volume> <pages> 531-549. </pages>
Reference-contexts: The second algorithm is similar to the algorithm with Relaxation used in games theory (see, e.g., <ref> [4] </ref>). 11 Algorithm 1 : Sequential algorithm 1.
Reference: 5. <author> Bertsekas, D. P. and J. N. </author> <title> Tsitsiklis (1989). Parallel and distributed computation: Numerical methods, </title> <publisher> Prentice-Hall. </publisher>
Reference: 6. <author> Ermoliev, Y. M. </author> <year> (1983). </year> <title> Stochastic Quasigradient Methods and their Application to System Optimization, </title> <journal> Stochastics, </journal> <volume> 9, </volume> <pages> 1-36. </pages>
Reference-contexts: In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy [14]. Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., <ref> [6] </ref>, [7], [15], [21], [24], [25], [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> Under a few additional conditions, the SA algorithm can be shown to converge to the optimizer w.p.1, and convergence rates can also be obtained in several cases; see <ref> [6] </ref>, [7], [15], [23], [27] and other numerous references given there.
Reference: 7. <author> Ermoliev, Y. M. and Gaivoronski, A. A. </author> <year> (1992). </year> <title> Stochastic Quasigradient Methods for Optimization of Discrete Event Systems, </title> <journal> Annals of Operations Research, </journal> <volume> 39, </volume> <pages> 1-39. </pages>
Reference-contexts: In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy [14]. Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], <ref> [7] </ref>, [15], [21], [24], [25], [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> Under a few additional conditions, the SA algorithm can be shown to converge to the optimizer w.p.1, and convergence rates can also be obtained in several cases; see [6], <ref> [7] </ref>, [15], [23], [27] and other numerous references given there. The use of SA and other similar stochastic iterative methods which use gradient or subgradient estimates in the context of on-line or simulated discrete-event dynamic systems has attracted much attention recently; see, e.g., [7], [21], [22], [26] and the several other <p> be obtained in several cases; see [6], <ref> [7] </ref>, [15], [23], [27] and other numerous references given there. The use of SA and other similar stochastic iterative methods which use gradient or subgradient estimates in the context of on-line or simulated discrete-event dynamic systems has attracted much attention recently; see, e.g., [7], [21], [22], [26] and the several other references given there. Remark 5 Here, to simplify the discussion, we have assumed that fl n is a scalar. However, it can also be a matrix of the same dimension as . <p> We will then do a similar reasoning for i . We draw some ideas from the proofs of Lemmas 7 and 8 of Ermoliev and Gaivoronski <ref> [7] </ref>.
Reference: 8. <author> Glasserman, P. </author> <year> (1991). </year> <title> Gradient Estimation via Perturbation Analysis, </title> <publisher> Kluwer Academic Press. </publisher>
Reference-contexts: Since it requires multiple runs (at least a separate run for each point (v; )), it is typically time-consuming. Note that a modification of the SF method, the so-called "push out " method [32], as well as the perturbation analysis (PA) method <ref> [8] </ref>, also called "push in" method in [32], combined with the use of a likelihood ratio, permit (in many cases) the solution of the "what-if" problem simultaneously from a single simulation with respect to both v and ; see [19] for examples. <p> If we further assume that r L t () is available from the simulation, that r o = 0 (which is typical, since o is usually piecewise constant as a function of ), and under a few additional conditions (see <ref> [8] </ref>, [13]), then one also has r ` 1 (v; ) ` 1 (v; )r ` 2 (v; ) = E g o X W t r L t : (5) (Note that the latter bracketted expression is typically not an unbiased estimator of ` 1 (v; ) when o depends <p> For that second derivative, we will use here perturbation analysis (IPA) <ref> [8] </ref>, [17].
Reference: 9. <author> Glynn, P. W. and D. L. </author> <month> Iglehart </month> <year> (1989). </year> <title> Importance Sampling for Stochastic Simulations, </title> <journal> Management Science, </journal> <volume> 35, 11, </volume> <pages> 1367-1392. </pages>
Reference-contexts: The latter "likelihood ratio" or "change of measure" technique is in fact exactly the same as that used in importance sampling for variance reduction <ref> [9] </ref>. Unlike SF, the CMC method permits the solution of the "what-if" problem with respect to both v and , simply by performing separate simulations at each parameter values of interest. Since it requires multiple runs (at least a separate run for each point (v; )), it is typically time-consuming. <p> The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], <ref> [9] </ref>, [10], [17], [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1. <p> A finite-horizon model can be viewed as a special case of this: just replace ` 2 (v; ) by 1. Under standard regularity conditions allowing interchangeability of expectation and differentiation (e.g., uniform integrability), one has [1], <ref> [9] </ref>, [17], [32]: ` 1 (v; ) = E g o X L t W t ; (3) " o X L t r v W t ; (4) where L t = L t (Z 1 ; : : : ; Z t ; ), W t = Q t
Reference: 10. <author> Glynn, P. W. </author> <year> (1990). </year> <title> Likelihood Ratio Gradient Estimation for Stochastic Systems, </title> <journal> Communications of the ACM , 33, </journal> <volume> 10, </volume> <pages> 75-84. </pages>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method <ref> [10] </ref>, [28], [29], [30], [31], [32] permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed. <p> The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], [9], <ref> [10] </ref>, [17], [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1.
Reference: 11. <author> Glynn, P. W., L'Ecuyer, P., and Ades, M. </author> <year> (1991). </year> <title> Gradient Estimation for Ratios, </title> <booktitle> Proceedings of the 1991 Winter Simulation Conference, </booktitle> <publisher> IEEE Press, </publisher> <pages> 986-993. </pages>
Reference-contexts: These estimators then permit one to estimate the function and its gradient in functional form w.r.t. v, from the simulation of N regenerative cycles based on density g. Confidence intervals at any fixed value of v can be computed as explained in <ref> [11] </ref>. In a similar way, again under the appropriate conditions, a consistent estimator of r `(v; ) is given by r ` N (v; ) = ` 2N (v; ) 2.2. Selecting the Reference Parameter Value An important question in this context is how to select g.
Reference: 12. <author> Goldsman, D., Nelson, B., and Schmeiser, B. </author> <year> (1991). </year> <title> Methods for Selecting the best System, </title> <booktitle> Proceedings of the 1991 Winter Simulation Conference, </booktitle> <publisher> IEEE Press, </publisher> <pages> 177-186. </pages>
Reference-contexts: A statistical analysis of such an approach can be performed along the lines of the statistical ranking and selection and multiple comparison methods <ref> [12] </ref>, [16], [37]. In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy [14]. Suppose now that both parameters are continuous. <p> For finite sample sizes N , there exists "ranking and selection" procedures for selecting the best system among a finite number of candidates (here r 1 fi r 2 candidates), but these procedures usually assume independence between the performance estimators for the different candidates (see <ref> [12] </ref>, [16]). Such procedures will return one of the candidates, which will be the the best system, i.e., the minimizer of (17), with probability at least p fl , where p fl depends on the difference in performance between the best and second best systems.
Reference: 13. <author> Heidelberger, P., X.-R. Cao, M. A. Zazanis, and R. </author> <title> Suri (1988). "Convergence Properties of Infinitesimal Perturbation Analysis Estimates", </title> <journal> Management Science, </journal> <volume> 34, 11, </volume> <pages> 1281-1302. </pages>
Reference-contexts: If we further assume that r L t () is available from the simulation, that r o = 0 (which is typical, since o is usually piecewise constant as a function of ), and under a few additional conditions (see [8], <ref> [13] </ref>), then one also has r ` 1 (v; ) ` 1 (v; )r ` 2 (v; ) = E g o X W t r L t : (5) (Note that the latter bracketted expression is typically not an unbiased estimator of ` 1 (v; ) when o depends on
Reference: 14. <author> Kleijnen, J. P. C. and Van Groenendaal, W. </author> <year> (1992). </year> <title> Simulation: A Statistical Perspective, </title> <publisher> Wiley, </publisher> <address> Chichester. </address>
Reference-contexts: In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy <ref> [14] </ref>. Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], [21], [24], [25], [27], [30] and several further references given there).
Reference: 15. <author> Kushner, H. J. and D. S. </author> <title> Clark (1978). Stochastic Approximation Methods for Constrained and Unconstrained Systems, </title> <journal> Springer-Verlag, Applied Math. Sciences, </journal> <volume> Vol. </volume> <pages> 26. </pages>
Reference-contexts: Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], <ref> [15] </ref>, [21], [24], [25], [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> Under a few additional conditions, the SA algorithm can be shown to converge to the optimizer w.p.1, and convergence rates can also be obtained in several cases; see [6], [7], <ref> [15] </ref>, [23], [27] and other numerous references given there. <p> However, it can also be a matrix of the same dimension as . Indeed, fl n = fl 0 =n, where fl 0 is the inverse of the Hessian at the optimum, is asymptotically optimal under broad conditions <ref> [15] </ref>. That inverse is of course unknown in practice, but adaptive algorithms have been designed which modify both n and fl n (adaptively) between iterations. Other techniques (e.g., averaging) can also improve the performance of SA. For further details, see [15], [22], [27], [35] and the references given there. <p> Hessian at the optimum, is asymptotically optimal under broad conditions <ref> [15] </ref>. That inverse is of course unknown in practice, but adaptive algorithms have been designed which modify both n and fl n (adaptively) between iterations. Other techniques (e.g., averaging) can also improve the performance of SA. For further details, see [15], [22], [27], [35] and the references given there. Let us now turn back to the problem (18).
Reference: 16. <author> Law, A. M. and Kelton, W. D. </author> <year> (1991). </year> <title> Simulation Modeling and Analysis, second edition, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: A statistical analysis of such an approach can be performed along the lines of the statistical ranking and selection and multiple comparison methods [12], <ref> [16] </ref>, [37]. In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy [14]. Suppose now that both parameters are continuous. <p> On the other hand, using the same v 0 for all of interest is not really necessary. Remark 3 It is common practice in simulation to use the same stream of random numbers while running different scenarios (see, e.g., <ref> [16] </ref>, [18], [30], [37]), in order to reduce the variance of the differences across scenarios. <p> For finite sample sizes N , there exists "ranking and selection" procedures for selecting the best system among a finite number of candidates (here r 1 fi r 2 candidates), but these procedures usually assume independence between the performance estimators for the different candidates (see [12], <ref> [16] </ref>). Such procedures will return one of the candidates, which will be the the best system, i.e., the minimizer of (17), with probability at least p fl , where p fl depends on the difference in performance between the best and second best systems.
Reference: 17. <author> L'Ecuyer, P. </author> <year> (1990). </year> <title> A unified view of the IPA, SF, and LR gradient estimation techniques. </title> <journal> Management Science, </journal> <volume> 36, </volume> <pages> 1364-1384. </pages>
Reference-contexts: The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], [9], [10], <ref> [17] </ref>, [28], [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1. <p> A finite-horizon model can be viewed as a special case of this: just replace ` 2 (v; ) by 1. Under standard regularity conditions allowing interchangeability of expectation and differentiation (e.g., uniform integrability), one has [1], [9], <ref> [17] </ref>, [32]: ` 1 (v; ) = E g o X L t W t ; (3) " o X L t r v W t ; (4) where L t = L t (Z 1 ; : : : ; Z t ; ), W t = Q t sity <p> We call W t , rW t , L t W t , and L t rW t the likelihood ratio, score function, sample performance, and sensitivity processes, respectively. This could also be generalized to larger values of k <ref> [17] </ref>, [32]. <p> For that second derivative, we will use here perturbation analysis (IPA) [8], <ref> [17] </ref>.
Reference: 18. <author> L'Ecuyer, P. </author> <year> (1992). </year> <title> Convergence rates for steady-state derivative estimator. </title> <journal> Annals of Operations Research, </journal> <volume> 39, </volume> <pages> 121-136. </pages>
Reference-contexts: On the other hand, using the same v 0 for all of interest is not really necessary. Remark 3 It is common practice in simulation to use the same stream of random numbers while running different scenarios (see, e.g., [16], <ref> [18] </ref>, [30], [37]), in order to reduce the variance of the differences across scenarios.
Reference: 19. <author> L'Ecuyer, P. </author> <year> (1993). </year> <title> Two Approaches for Estimating the Gradient in Functional Form, </title> <booktitle> Proceedings of the 1993 Winter Simulation Conference, </booktitle> <publisher> IEEE Press, </publisher> <pages> 338-346. </pages>
Reference-contexts: " method [32], as well as the perturbation analysis (PA) method [8], also called "push in" method in [32], combined with the use of a likelihood ratio, permit (in many cases) the solution of the "what-if" problem simultaneously from a single simulation with respect to both v and ; see <ref> [19] </ref> for examples. Here, we shall not deal with the latter approaches. We should mention that the SF method sometimes suffers from a variance explosion problem (the variance of the estimator may become huge at some values), especially when the values of v of interest span a large area (see [19], <p> <ref> [19] </ref> for examples. Here, we shall not deal with the latter approaches. We should mention that the SF method sometimes suffers from a variance explosion problem (the variance of the estimator may become huge at some values), especially when the values of v of interest span a large area (see [19], [32] for details). But there are ways of dealing with such problems (e.g., break the area of interest into smaller subareas), at least for certain classes of applications [32]. Suppose now that we want to minimize `(v; ) with respect to v and . <p> Henceforth, we shall assume that g () = f (; v 0 ), where v 0 is a fixed value of v called the reference parameter value. Now the question is how to select v 0 . This has been studied, e.g., in [2], <ref> [19] </ref>, [32]. A good choice of v 0 turns out to be extremely important, because for a given v, the variance of the estimators (6-8) may blow up to a very large value or even become infinite for certain choices of v 0 . <p> In the latter case, the variance increase is typically moderate when ^ae ae is not too large and the cycle length o tends to be small (see [2], <ref> [19] </ref> for illustrations). If that is not the case, then the set fv 1 ; : : : ; v r 1 g should be partitioned into smaller subsets and a different v 0 chosen over each subset. 2.3.
Reference: 20. <author> L'Ecuyer, P. and G. </author> <title> Perron (1994). On the Convergence Rates of IPA and FDC Derivative Estimators. </title> <journal> Operations Research 42, </journal> <pages> 643-656. 24 </pages>
Reference-contexts: (Note that the latter bracketted expression is typically not an unbiased estimator of ` 1 (v; ) when o depends on .) When these conditions are not satisfied, one can still rely to finite differences to estimate the gradients on the left-hand-side of (5), preferably with common random numbers (see <ref> [20] </ref>). Remark 1 In this setup, we have implicitly assumed that both v and are con-tinous parameters and that the derivatives exist.
Reference: 21. <author> L'Ecuyer, P. and P. W. </author> <title> Glynn (1994). Stochastic Optimization by Simulation: Convergence Proofs for the GI/G/1 Queue in Steady-State. </title> <booktitle> Management Science 40, </booktitle> <pages> 1562-1578. </pages>
Reference-contexts: Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], <ref> [21] </ref>, [24], [25], [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> The use of SA and other similar stochastic iterative methods which use gradient or subgradient estimates in the context of on-line or simulated discrete-event dynamic systems has attracted much attention recently; see, e.g., [7], <ref> [21] </ref>, [22], [26] and the several other references given there. Remark 5 Here, to simplify the discussion, we have assumed that fl n is a scalar. However, it can also be a matrix of the same dimension as . <p> or, equivalently, to solving the equations: ` 2 d ff (v; ) = ` 2 (v; ) d which can also be written as ` 2 (v; ) dv d ` 2 (v; ) ` 2 d ` 1 (v; ) `(v; ) d As explained in L'Ecuyer and Glynn <ref> [21] </ref>, one can obtain an unbiased estimator of the left-hand-side of (44) from two independent regenerative cycles and the score function method, and an unbiased estimator of the left-hand-side of (45) from one regenerative cycle with IPA. The numerical results we present here are for Algorithms 1'-3'. <p> The function ff here satisfies Assumption 1 (i), while (iv) is satisfied since i i = 0 and one can show (much as in L'Ecuyer and Glynn <ref> [21] </ref>) that sup (v;)2V fifi E [k n k 2 j v; ] &lt; 1. Note that for fi i constant, (iii) does not hold, but that nevertheless gave us the best results empirically.
Reference: 22. <author> L'Ecuyer, P., N. Giroux, and P. W. </author> <title> Glynn (1994). Stochastic Optimization by Simulation: Numerical Experiments for the M/M/1 Queue in Steady-State. </title> <booktitle> Management Science 40, </booktitle> <pages> 1245-1261. </pages>
Reference-contexts: The use of SA and other similar stochastic iterative methods which use gradient or subgradient estimates in the context of on-line or simulated discrete-event dynamic systems has attracted much attention recently; see, e.g., [7], [21], <ref> [22] </ref>, [26] and the several other references given there. Remark 5 Here, to simplify the discussion, we have assumed that fl n is a scalar. However, it can also be a matrix of the same dimension as . <p> That inverse is of course unknown in practice, but adaptive algorithms have been designed which modify both n and fl n (adaptively) between iterations. Other techniques (e.g., averaging) can also improve the performance of SA. For further details, see [15], <ref> [22] </ref>, [27], [35] and the references given there. Let us now turn back to the problem (18). <p> ; ffi i g; fl 0 ; T ), each algorithm was repeated 10 times and we computed the empirical mean, the standard deviation s d , and the standard error s e of the 10 final values of v i and i , as in L'Ecuyer, Giroux, and Glynn <ref> [22] </ref>.
Reference: 23. <author> L'Ecuyer, P. and G. </author> <title> Yin (1994). Budget-Dependent Convergence Rate of Stochastic Approximation. </title> <note> Submitted. </note>
Reference-contexts: Under a few additional conditions, the SA algorithm can be shown to converge to the optimizer w.p.1, and convergence rates can also be obtained in several cases; see [6], [7], [15], <ref> [23] </ref>, [27] and other numerous references given there.
Reference: 24. <author> Pflug, G. Ch. </author> <year> (1990). </year> <title> On-Line Optimization of Simulated Markov Processes. </title> <journal> Mathematics of Operations Research 15, </journal> <pages> 381-395. </pages>
Reference-contexts: Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], [21], <ref> [24] </ref>, [25], [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction.
Reference: 25. <author> Pflug, G. Ch. </author> <year> (1992). </year> <title> Gradient Estimates for the Performance of Markov Chains and Discrete Event Processes. </title> <journal> Annals of Operations Research 39, </journal> <pages> 173-194. </pages>
Reference-contexts: Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], [21], [24], <ref> [25] </ref>, [27], [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction.
Reference: 26. <author> Plambeck, E. L., Fu, B.-R., Robinson, S. M., and Suri, R. </author> <year> (1993). </year> <title> Optimizing Performance Functions in Stochastic Systems. </title> <note> Submitted. </note>
Reference-contexts: The use of SA and other similar stochastic iterative methods which use gradient or subgradient estimates in the context of on-line or simulated discrete-event dynamic systems has attracted much attention recently; see, e.g., [7], [21], [22], <ref> [26] </ref> and the several other references given there. Remark 5 Here, to simplify the discussion, we have assumed that fl n is a scalar. However, it can also be a matrix of the same dimension as .
Reference: 27. <author> Polyak, B. T. and Juditsky, A. B. </author> <year> (1992). </year> <title> Acceleration of Stochastic Approximation by Averaging, </title> <journal> SIAM J. on Control and Optimization 30, </journal> <volume> 4, </volume> <pages> 838-855. </pages>
Reference-contexts: Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], [21], [24], [25], <ref> [27] </ref>, [30] and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> Under a few additional conditions, the SA algorithm can be shown to converge to the optimizer w.p.1, and convergence rates can also be obtained in several cases; see [6], [7], [15], [23], <ref> [27] </ref> and other numerous references given there. <p> That inverse is of course unknown in practice, but adaptive algorithms have been designed which modify both n and fl n (adaptively) between iterations. Other techniques (e.g., averaging) can also improve the performance of SA. For further details, see [15], [22], <ref> [27] </ref>, [35] and the references given there. Let us now turn back to the problem (18).
Reference: 28. <author> Reiman, M. I. and A. </author> <title> Weiss (1989). Sensitivity analysis for simulations via likelihood ratios, </title> <journal> Operations Research 37, </journal> <pages> 830-844. </pages>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method [10], <ref> [28] </ref>, [29], [30], [31], [32] permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed. <p> The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], [9], [10], [17], <ref> [28] </ref>, [31], [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1.
Reference: 29. <author> Rubinstein, R. Y. </author> <year> (1976). </year> <title> A Monte Carlo method for estimating the gradient in a stochastic network. </title> <type> Unpublished manuscript, </type> <institution> Technion, Haifa, Israel. </institution>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method [10], [28], <ref> [29] </ref>, [30], [31], [32] permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed.
Reference: 30. <author> Rubinstein, R. Y. </author> <year> (1986). </year> <title> Monte Carlo Optimization Simulation and Sensitivity of Queueing Network, </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York. </address>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method [10], [28], [29], <ref> [30] </ref>, [31], [32] permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed. <p> Suppose now that both parameters are continuous. To optimize w.r.t. for fixed v, one can use a stochastic approximation (SA) algorithm (see, e.g., [6], [7], [15], [21], [24], [25], [27], <ref> [30] </ref> and several further references given there). SA is an iterative procedure which at each step estimates the gradient of the objective function and makes a small step in its opposite direction. <p> On the other hand, using the same v 0 for all of interest is not really necessary. Remark 3 It is common practice in simulation to use the same stream of random numbers while running different scenarios (see, e.g., [16], [18], <ref> [30] </ref>, [37]), in order to reduce the variance of the differences across scenarios.
Reference: 31. <author> Rubinstein, R. Y. </author> <year> (1992). </year> <title> Monte Carlo Methods for performance evaluation, sensitivity analysis and optimization of stochastic systems, </title> <journal> Encyclopedia of Computer Science and Technology (Kent ed.), Marcel Deker, Inc., </journal> <volume> Vol. 25, </volume> <pages> 211-233. </pages>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method [10], [28], [29], [30], <ref> [31] </ref>, [32] permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed. <p> The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], [9], [10], [17], [28], <ref> [31] </ref>, [32]. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1.
Reference: 32. <author> Rubinstein, R. Y. and A. </author> <title> Shapiro (1993). Discrete Event Systems: Sensitivity Analysis and Stochastic Optimization via the Score Function Method, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In its original form, the likelihood ratio (LR) or score function (SF) method [10], [28], [29], [30], [31], <ref> [32] </ref> permits the solution of the "what-if" problem from a single simulation run (single sample path) with respect to v alone, that is, when is fixed. <p> Since it requires multiple runs (at least a separate run for each point (v; )), it is typically time-consuming. Note that a modification of the SF method, the so-called "push out " method <ref> [32] </ref>, as well as the perturbation analysis (PA) method [8], also called "push in" method in [32], combined with the use of a likelihood ratio, permit (in many cases) the solution of the "what-if" problem simultaneously from a single simulation with respect to both v and ; see [19] for examples. <p> Since it requires multiple runs (at least a separate run for each point (v; )), it is typically time-consuming. Note that a modification of the SF method, the so-called "push out " method <ref> [32] </ref>, as well as the perturbation analysis (PA) method [8], also called "push in" method in [32], combined with the use of a likelihood ratio, permit (in many cases) the solution of the "what-if" problem simultaneously from a single simulation with respect to both v and ; see [19] for examples. Here, we shall not deal with the latter approaches. <p> Here, we shall not deal with the latter approaches. We should mention that the SF method sometimes suffers from a variance explosion problem (the variance of the estimator may become huge at some values), especially when the values of v of interest span a large area (see [19], <ref> [32] </ref> for details). But there are ways of dealing with such problems (e.g., break the area of interest into smaller subareas), at least for certain classes of applications [32]. Suppose now that we want to minimize `(v; ) with respect to v and . <p> the estimator may become huge at some values), especially when the values of v of interest span a large area (see [19], <ref> [32] </ref> for details). But there are ways of dealing with such problems (e.g., break the area of interest into smaller subareas), at least for certain classes of applications [32]. Suppose now that we want to minimize `(v; ) with respect to v and . <p> The latter minimization problem is called the stochastic counterpart (SC). That SC optimization approach is studied in much detail in Rubinstein and Shapiro <ref> [32] </ref>, where it is shown that the sample optimizer converges to the true optimizer with probability one (w.p. 1), and obeys a central-limit theorem, as the sample size N goes to 1. <p> The "What-if" Problem In this section, we recall some background material on the SF method and on how a functional estimator w.r.t. v can be obtained. Further details on this are given in [1], [2], [3], [9], [10], [17], [28], [31], <ref> [32] </ref>. We then explain how to combine SF and CMC in order to estimate `(v; ) and its gradient simultaneously for several values of v and . We distinguish the following two cases: (a) is fixed; (b) is not fixed. 4 2.1. <p> A finite-horizon model can be viewed as a special case of this: just replace ` 2 (v; ) by 1. Under standard regularity conditions allowing interchangeability of expectation and differentiation (e.g., uniform integrability), one has [1], [9], [17], <ref> [32] </ref>: ` 1 (v; ) = E g o X L t W t ; (3) " o X L t r v W t ; (4) where L t = L t (Z 1 ; : : : ; Z t ; ), W t = Q t sity Q <p> We call W t , rW t , L t W t , and L t rW t the likelihood ratio, score function, sample performance, and sensitivity processes, respectively. This could also be generalized to larger values of k [17], <ref> [32] </ref>. <p> Henceforth, we shall assume that g () = f (; v 0 ), where v 0 is a fixed value of v called the reference parameter value. Now the question is how to select v 0 . This has been studied, e.g., in [2], [19], <ref> [32] </ref>. A good choice of v 0 turns out to be extremely important, because for a given v, the variance of the estimators (6-8) may blow up to a very large value or even become infinite for certain choices of v 0 . <p> In our context, we are also interested in a value of v 0 that does well for all values of v in a certain region. Asmussen and Rubinstein [2] and Rubinstein and Shapiro <ref> [32] </ref> have studied the problem (9) in the context where `(v; ) is the average sojourn time per customer in a single queue. In that context, let ae (v) denote the traffic intensity (which is assumed to depend on v). Under conditions given in [32], Var v 0 [` N (v; <p> Rubinstein [2] and Rubinstein and Shapiro <ref> [32] </ref> have studied the problem (9) in the context where `(v; ) is the average sojourn time per customer in a single queue. In that context, let ae (v) denote the traffic intensity (which is assumed to depend on v). Under conditions given in [32], Var v 0 [` N (v; )] is strictly convex w.r.t. v 0 and one has ae (v fl 0 ) &gt; ae (v), where v fl 0 is the optimal solution of (9). <p> Similar results were obtained for more complex queueing models for which the performance measure is the average sojourn or waiting time. A general recommendation from [2], <ref> [32] </ref> is: in order to be on the safe side one should choose v 0 such that ae (v 0 ) is moderately larger than the nominal value ae = ae (v). <p> This follows from the strong law of large numbers (see also <ref> [32] </ref>). Note however that this does not tell us about the probability of making the correct decision for a specific N . <p> In particular, we can estimate the optimal solution of (19), say v fl (), by solving its stochastic counterpart (SC) (see <ref> [32] </ref>): min ` N (v; ); (21) using a conventional mathematical programming method. The statistical properties of the minimizer of (21), which is taken as an estimator of v fl (), are studied in [32]. <p> the optimal solution of (19), say v fl (), by solving its stochastic counterpart (SC) (see <ref> [32] </ref>): min ` N (v; ); (21) using a conventional mathematical programming method. The statistical properties of the minimizer of (21), which is taken as an estimator of v fl (), are studied in [32]. Under reasonable conditions, the function ` N (; ) is twice continuously differentiable, and the minimizer in (21) obeys a central-limit theorem and converges to v fl () as O (N 1=2 ). <p> Remark 6 Under appropriate assumptions, if we suppose that v fl i converges to some value as i ! 1, then it is not hard to show by standard SA arguments that n must converge w.p.1. Conversely, if n converges to some value, then the arguments of <ref> [32] </ref> can be used to show that v i must also converges w.p.1 under appropriate conditions. In both cases, if the function is convex and the optimizer is in the interior of fi, then the convergence point must be the optimum. <p> Observe that the solution v fl of the SC (21) is usually not an unbiased estimator of the optimal solution of the original minimization problem (18), but it is a consistent estimator under broad conditions (see <ref> [32] </ref>). This is why we need to take N i ! 1. <p> Here, we can use the score function method to estimate the derivative with respect to v, but not the derivative with respect to , because the likelihood ratio does not exist (although one could perhaps apply the "push-out" method as in <ref> [32] </ref>, p.229, but we will not do it here). For that second derivative, we will use here perturbation analysis (IPA) [8], [17].
Reference: 33. <author> Rubinstein, R. Y. and S. </author> <month> Uryas'ev </month> <year> (1994). </year> <title> On Relaxation Algorithms in Computation of NonCooperative Equilibria, </title> <journal> IEEE Transactions on Automatic Control, AC-39, </journal> <volume> 6, </volume> <pages> 1263-1268. </pages>
Reference: 34. <author> Suri, R. and M. A. </author> <month> Zazanis </month> <year> (1988). </year> <title> Perturbation analysis gives strongly consistent sensitivity estimates for the M/G/1 queue, </title> <journal> Management Science, </journal> <volume> 34, 1, </volume> <pages> 39-64. </pages>
Reference-contexts: Example 2 Let L t () be the expected waiting time for the t-th customer in a GI=D=1 queue and assume that we want to estimate the gradient of the steady-state waiting time, r `(v; ), where is the (deterministic) service time. To do so, recall first (see <ref> [34] </ref>) that for a GI/G/1 queue, for t o , one has L t () = j=1 7 where o = minft : L t () 0g; Y j and A j are the service time of customer j and the interarrival time between customers j 1 and j, respectively.
Reference: 35. <author> Uryas'ev, S. P. </author> <year> (1992). </year> <title> A Stochastic Quasigradient Algorithm with Variable Metric, </title> <journal> Annals of Operations Research, </journal> <volume> 39, </volume> <pages> 251-267. </pages>
Reference-contexts: That inverse is of course unknown in practice, but adaptive algorithms have been designed which modify both n and fl n (adaptively) between iterations. Other techniques (e.g., averaging) can also improve the performance of SA. For further details, see [15], [22], [27], <ref> [35] </ref> and the references given there. Let us now turn back to the problem (18).
Reference: 36. <author> Wolff, R. </author> <year> (1989). </year> <title> Stochastic Modeling and the Theory of Queues, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Assume that fL t g is regenerative with cycle length o = o (Y 1 ; Y 2 ; : : : ; ). It is well known <ref> [36] </ref> that the expected steady-state (average) of fL t g can be written as `(v; ) = P o E v [o ] ` 1 (v; ) ; (2) provided that E v [o ] &gt; 0 and E v [j P o t=1 L t j] &lt; 1, and similarly <p> Indeed, the traffic intensity is bounded as follows: 0:01 v 0:91. Minimizing (42) is clearly a rather simple and easy to solve example, but it can nevertheless illustrate quite well our algorithms. For the present example, one has `(v; ) = + v 2 =(2 (1 v)); see Wolff <ref> [36] </ref>, p.385. Using this in a deterministic optimization algorithm, one finds that (42) is minimized by taking (v fl ; fl ) (1:0824; 0:5412).
Reference: 37. <author> Yang, W.-N. and Nelson, B. L. </author> <year> (1991). </year> <title> Using Common Random Numbers and Control Vari-ates in Multiple-Comparison Procedures, </title> <journal> Operations Research, </journal> <volume> 39, 4, </volume> <pages> 583-591. </pages>
Reference-contexts: A statistical analysis of such an approach can be performed along the lines of the statistical ranking and selection and multiple comparison methods [12], [16], <ref> [37] </ref>. In particular, those values of of interest may have been chosen among a much larger (perhaps infinite) set by some experimental design (ED) strategy [14]. Suppose now that both parameters are continuous. <p> On the other hand, using the same v 0 for all of interest is not really necessary. Remark 3 It is common practice in simulation to use the same stream of random numbers while running different scenarios (see, e.g., [16], [18], [30], <ref> [37] </ref>), in order to reduce the variance of the differences across scenarios. <p> Similar selection procedures using control variates and common random numbers have been proposed and analyzed recently <ref> [37] </ref>, but the set of assumptions made for the analysis typically do not hold in the context of the methodology outlines in Section 2.3. Developing ranking and selection procedures for that context is a topic for further research. 3.2.
References-found: 37

