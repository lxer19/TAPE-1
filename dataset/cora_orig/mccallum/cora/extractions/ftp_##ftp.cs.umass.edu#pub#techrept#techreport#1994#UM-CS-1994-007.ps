URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1994/UM-CS-1994-007.ps
Refering-URL: http://www-ml.cs.umass.edu/
Root-URL: 
Email: utgoff@cs.umass.edu  
Title: An Improved Algorithm for Incremental Induction of Decision Trees  
Author: Paul E. Utgoff 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Technical Report 94-07 February 7, 1994 (updated April 25, 1994) This paper will appear in Proceedings of the Eleventh International Conference on Machine Learning. Abstract This paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables. In order to handle numeric variables, a new tree revision operator called `slewing' is introduced. Finally, a non-incremental method is given for finding a decision tree based on a direct metric of a candidate tree. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages> <editor> de Mantaras, R. L. </editor> <year> (1991). </year> <title> A distance-based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 81-92. </pages>
Reference: <author> Fayyad, U. M. </author> <year> (1991). </year> <title> On the induction of decision trees for multiple concept learning. </title> <type> Doctoral dissertation, </type> <institution> Computer Science and Engineering, University of Michigan. </institution>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 87-102. </pages>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference: <author> Laird, J. E., Rosenbloom, P. S., & Newell, A. </author> <year> (1986). </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 11-46. </pages>
Reference: <author> Michalski, R. S., & Chilausky, R. L. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 125-160. </pages>
Reference: <author> Mitchell, T. M. </author> <year> (1978). </year> <title> Version spaces: An approach to concept learning. </title> <type> Doctoral dissertation, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA. </address>
Reference: <author> Pagallo, G. M. </author> <year> (1990). </year> <title> Adaptive decision tree algorithms for learning from examples. </title> <type> Doctoral dissertation, </type> <institution> University of California at Santa Cruz. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Finally, to improve the fit of the tree to the training data, one should group those values of a variable that have similar distributions of instances. This avoids excessive partitioning, generally reducing size and improving accuracy <ref> (Quinlan, 1986) </ref>. Fayyad (1991) has devised a greedy algorithm for grouping the values of a symbolic variable. Neil Berkman is currently investigating several approaches to grouping values for ITI. 10 Summary The paper has presented the algorithm ITI for incremental induction of decision trees.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. Morgan Kaufmann. Improved Algorithm for Incremental Induction 12 Ragavan, </title> <editor> H., & Rendell, L. </editor> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Machine Learning: Proceedings of the Tenth International Conference (pp. </booktitle> <pages> 252-259). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Samuel, A. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of Checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> 211-229. </pages>
Reference: <author> Schlimmer, J. C., & Fisher, D. </author> <year> (1986). </year> <title> A case study of incremental concept induction. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 496-501). </pages> <address> Philadelpha, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1989a). </year> <title> Improved training via incremental learning. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Such a tree is often smaller and more Improved Algorithm for Incremental Induction 8 accurate than a tree based on all the instances presented <ref> (Utgoff, 1989a) </ref>, though the reason for this phenomenon is still unknown. For example, compare the results on the Wisconsin breast cancer task using a ten-fold cross-validation with a 90/10 split for training and test data.
Reference: <author> Utgoff, P. E. </author> <year> (1989b). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: Secondly, knowledge revision is typically much less expensive than knowledge creation. For example, upon receiving a new training instance, it is much less expensive to revise a decision tree than it is to build a new tree from scratch based on the now-augmented set of accumulated training instances <ref> (Utgoff, 1989b) </ref>. Finally, the ability to revise knowledge in an efficient manner opens new possibilities for algorithms that otherwise would remain prohibitively expensive. For example, a non-incremental algorithm is presented that searches the space of decision trees more deliberately than the typical greedy approach. <p> The algorithm should avoid overfitting noise in the instances. 11. The algorithm should have the ability to find compound tests, similar to Pagallo's FRINGE. 12. The algorithm should be capable of grouping the values of a variable. The earlier ID5R algorithm <ref> (Utgoff, 1989b) </ref> meets only the first three of these goals. Regarding the fourth goal, ID5R accepts only symbolic variables. ID5R does not meet the remaining eight of these goals. 3 An Improved Algorithm This section presents the algorithm ITI, which aims to meet the design goals described above. <p> Although it is the case that when only symbolic variables are involved one can show analytically that the incremental cost of tree revision is entirely independent of the number of training instances seen <ref> (Utgoff, 1989b) </ref>, this is not the case for numeric variables. For each numeric variable at each node, a sorted list of the values observed in the instances is maintained. More training instances means a greater cost to maintain each such list.
Reference: <author> Winston, P. H. </author> <year> (1975). </year> <title> Learning structural descriptions from examples. </title> <editor> In Winston (Ed.), </editor> <booktitle> The psychology of computer vision. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
References-found: 16

