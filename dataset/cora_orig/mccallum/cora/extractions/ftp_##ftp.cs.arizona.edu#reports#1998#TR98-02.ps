URL: ftp://ftp.cs.arizona.edu/reports/1998/TR98-02.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Design and Implementation of the Swarm Storage Server  
Author: Rajesh Sundaram 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Date: March 11, 1998  
Pubnum: TR 98-02  
Abstract: The Swarm storage system uses log-based striping to achieve high performance. Clients collect application writes in a log and stripe the log across multiple storage servers to aggregate server bandwidth. The Swarm storage server has been designed to meet various requirements of the Swarm storage system. These include high performance for data-intensive operations, rapid crash recovery, security support and atomic interface routines. The design of the Swarm storage server is simple enough to allow its implementation as a network appliance. 
Abstract-found: 1
Intro-found: 1
Reference: [Baker92] <author> Mary Baker, Satoshi Asami, Etienne Deprit, and John Ousterhout. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 1022, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: One is that Swarm allows clients to access the server using protocols that are not log-based. These protocols, such as RAID, may not perform large writes. The other reason for small writes is application flushes. While flushes do not happen frequently with typical non-transaction processing workloads <ref> [Baker92] </ref>, Swarm needs to take special care to handle them efficiently. This is because Swarm supports other storage abstractions such as parallel file systems, which may exhibit different behavior.
Reference: [Bershad94] <author> B. Bershad, D. Black, D. DeWitt, G. Gibson, K. Li, L. Peterson, and M. Snir. </author> <title> Operating System Support for High-Performance Parallel I/O Systems. Scalable I/O Initiative Working Paper No. </title> <type> 4, </type> <year> 1994. </year>
Reference-contexts: For example, the figure shows how an NFS server can be configured directly over the log layer. The figure also shows how standard NFS can be used without modifications, by constructing a logical disk between NFS and the log layer. As another example, the Scalable I/O Initiative (SIO) API <ref> [Bershad94] </ref> can be used to access the logs.
Reference: [deJonge93] <author> Wiebren deJonge, M. Frans Kaashoek, and Wilson C. Hsieh. </author> <title> The Logical Disk: A New Approach to Improving File Systems. </title> <booktitle> In Proceedings of the 14th Symposium on Operating System Principles, </booktitle> <pages> pages 1528, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The prototype has a two-layered architecture. The upper layer implements the Swarm server functionality. The lower layer, a log-structured implementation of the Logical Disk (LD) <ref> [deJonge93] </ref> manages the disk subsystem. The Swarm server is constructed as a part of the Scout operating system [Montz94]. 6.1 Logical Disk Logical Disk (LD) is a software layer over a raw disk that provides an abstract interface to the disk.
Reference: [Gibson96] <author> Garth A. Gibson, David F. Nagle, Khalil Amiri, Fay W. Chang, Eugene Fienberg, Howard Gobioff, Chen Lee, Berend Ozceri, Erik Riedel, and David Rochberg. </author> <title> A case for network-attached secure disks. </title> <type> Technical Report CMU-CS-96-142, </type> <institution> Carnegie Mellon University, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: Another difference is that Petal's servers map data blocks to fixed locations on disk. For writes we expect that the log-structured approach of the Swarm server will enable it to use 9 the disk bandwidth more efficiently. Network Attached Secure Disks (NASD) <ref> [Gibson96] </ref> are network attached disk drives that have built-in capability to allow clients to directly access them. The goal is to off-load much of the file manager's work onto the disk drive. Data-intensive operations like file reads and writes are sent directly to the disk.
Reference: [Hartman95] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <journal> ACM Transactions of Computer Systems, </journal> <volume> 13(3):274310, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: We disabled disk read caching while measuring performance to reduce variance among different measurements. 7 Related Work This section compares the Swarm storage server with some existing storage servers. 8 Zebra <ref> [Hartman95] </ref> is a log-based, striped network file system. The storage servers in Zebra are very simple. All fragments are of the same size. Zebra stores fragments by mapping them to fixed locations on disk. Zebra allows writing of partial fragments and appending to existing fragments.
Reference: [Lee96] <author> Edward K. Lee and Chandramohan A. Thekkath. </author> <title> Petal: Distributed Virtual Disks. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 1528, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: This simple architecture performs well for typical workstation workloads, but performance will degrade for transaction processing workloads which cause frequent writes of small partial fragments and parity updates. Zebra's storage server design makes it unsuitable for non log-based access protocols. Petal <ref> [Lee96] </ref> is a block level storage system. Storage servers in Petal cooperatively manage a pool of physical disks, providing clients with globally visible virtual disks. A Petal server is more complex than the Swarm server.
Reference: [Long94] <author> Darrell D. E. Long, Bruce R. Montague, and Luis-Filipe Cabrera. Swift/RAID: </author> <title> A Distributed RAID System. </title> <booktitle> Computing Systems, </booktitle> <address> 7(3):333359, </address> <year> 1994. </year>
Reference-contexts: Swarm achieves high performance by using data striping, similar to RAID. Unlike RAID, the striping is done across the network. Application writes are broken up and written to multiple storage servers simultaneously to aggregate bandwidth of the servers. While some systems stripe individual files to achieve high performance <ref> [Long94] </ref>, Swarm takes a different approach. To avoid problems with handling small files and modifications to existing file blocks, Swarm instead uses log-based striping. enough to fill an entire stripe, while files B and C together fill a stripe.
Reference: [Montz94] <author> A. B. Montz, D. Mosberger, S. W. O'Malley, L. L. peterson, T. A. Proebsting, and J. H. Hartman. </author> <title> Scout: A communications-oriented operating system. </title> <type> Technical Report 94-20, </type> <institution> University of Arizona, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The prototype has a two-layered architecture. The upper layer implements the Swarm server functionality. The lower layer, a log-structured implementation of the Logical Disk (LD) [deJonge93] manages the disk subsystem. The Swarm server is constructed as a part of the Scout operating system <ref> [Montz94] </ref>. 6.1 Logical Disk Logical Disk (LD) is a software layer over a raw disk that provides an abstract interface to the disk. This interface is based on logical block numbers and block lists.
Reference: [Rosenblum91] <author> Mendel Rosenblum and John K Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 115, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: File server. Another possible design for the Swarm server is to use a standard file system like UFS. Using this approach we could store fragments as files. This approach also has some problems. One of the problems is that most file systems don't achieve very good write performance <ref> [Rosenblum91] </ref>. We could use a write optimized file system like the Sprite LFS, but there is a loss of efficiency and added complexity due to the maintenance of file-system related metadata for file structures, permissions, directories, etc.
Reference: [Sandberg85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and Implementation of the Sun Network File System. </title> <booktitle> In Proceedings of the Summer 1985 Usenix Conference, </booktitle> <pages> pages 119130, </pages> <month> June </month> <year> 1985. </year> <month> 10 </month>
Reference-contexts: 1 Introduction Today's network file systems suffer from performance problems that limit their scalability. File servers often become a performance bottleneck when a system has to scale beyond a few dozens of nodes. Network file systems like NFS <ref> [Sandberg85] </ref> try to deal with scalability by having multiple servers handling different parts of the directory hierarchy. This mapping of files to servers is static and suffers from the problem of balancing load among the servers.
References-found: 10

