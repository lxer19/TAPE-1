URL: http://www.cs.rice.edu/~adve/Papers/sc95.DPablo.ps.gz
Refering-URL: http://www.cs.rice.edu/~adve/Papers/
Root-URL: 
Title: An Integrated Compilation and Performance Analysis Environment for Data Parallel Programs  
Author: Vikram S. Adve John Mellor-Crummey Mark Anderson Ken Kennedy Jhy-Chun Wang Daniel A. Reed 
Address: Houston, Texas 77251-1892  Urbana, Illinois 61801  
Affiliation: Center for Research on Parallel Computation Rice University  Department of Computer Science University of Illinois  
Abstract: Supporting source-level performance analysis of programs written in data-parallel languages requires a unique degree of integration between compilers and performance analysis tools. Compilers for languages such as High Performance Fortran infer parallelism and communication from data distribution directives; thus, performance tools cannot meaningfully relate measurements about these key aspects of execution performance to source-level constructs without substantial compiler support. This paper describes an integrated system for performance analysis of data-parallel programs based on the Rice Fortran 77D compiler and the Illinois Pablo performance analysis toolkit. During code generation, the Fortran D compiler records mapping information and semantic analysis results describing the relationship between performance instrumentation and the original source program. An integrated performance analysis system based on the Pablo toolkit uses this information to correlate the program's dynamic behavior with the data parallel source code. The integrated system provides detailed source-level performance feedback to programmers via a pair of graphical interfaces. Our strategy serves as a model for integration of data-parallel compilers and performance tools.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Applied Parallel Research. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 ed. </note> <institution> Placerville, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Notable exceptions include Prism [13] and NV [4] for CM-Fortran, Forge90 <ref> [1] </ref> for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [14].
Reference: [2] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The Pablo performance analysis toolkit serves as the basis for performance analysis in our integrated system. Principal components of the toolkit include an extensible data capture library [11] for monitoring program executions, a Self-Defining Data Format (SDDF) <ref> [2] </ref> that describes the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [9]. SDDF provides a flexible medium for information interchange between components of the integrated system.
Reference: [3] <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. </booktitle> <year> 1993), </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: 1 Introduction High-level, data-parallel languages such as High Performance Fortran [5] (HPF) and Fortran D <ref> [3] </ref> have attracted considerable attention because they offer a simple and portable programming model for parallel, scientific programs. In such languages, programmers specify parallelism abstractly using data layout directives, and a compiler uses these directives as the basis for synthesizing a program with explicit parallelism and interprocessor communication and synchronization.
Reference: [4] <author> Irvin, R. B., and Miller, B. P. </author> <title> A Performance Tool for High-Level Parallel Programming Languages. In Programming Environments for Massively Parallel Distributed Systems (Basel, </title> <address> Switzerland, 1994), </address> <publisher> Birkhauser Verlag. </publisher>
Reference-contexts: For high-level parallel languages, such tools can only capture and present dynamic performance data in terms of primitive operations (e.g. communication library calls) in the compiler-generated code. A few tools (Prism [13], NV <ref> [4] </ref>, and MPP-Apprentice [14]) provide source-level support for performance analysis of high-level parallel languages. However, none of these tools provide source-level performance support for the combination of data-parallel languages and optimizing compilers necessary for Fortran D or HPF. <p> Notable exceptions include Prism [13] and NV <ref> [4] </ref> for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [14].
Reference: [5] <author> Koelbel, C., Loveman, D., Schreiber, R., Steele, Jr., G., and Zosel, M. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction High-level, data-parallel languages such as High Performance Fortran <ref> [5] </ref> (HPF) and Fortran D [3] have attracted considerable attention because they offer a simple and portable programming model for parallel, scientific programs.
Reference: [6] <author> Mellor-Crummey, J. M., Adve, V. S., and Koelbel, C. </author> <title> The Compiler's Role in Analysis and Tuning of Data-Parallel Programs. </title> <booktitle> In Proceedings of The Second Workshop on Environments and Tools for Parallel Scientific Computing (Townsend, </booktitle> <address> TN, </address> <month> May </month> <year> 1994), </year> <pages> pp. 211-220. </pages> <note> Also available via anonymous ftp from softlib.cs.rice.edu in pub/CRPC-TRs/reports/CRPC-TR94405.ps. </note>
Reference-contexts: Finally, the compiler can exploit dynamic performance data to generate more efficient code, as described elsewhere <ref> [6] </ref>. 3 Integrated Compilation and Performance Analysis As illustrated in x2, a simple composition of compiler and performance tools is insufficient to support high-level performance analysis and software tuning of Fortran D programs.
Reference: [7] <author> Miller, B. P., Clark, M., Hollingsworth, J., Kierstead, S., Lim, S.-S., and Torzewski, T. IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System. </title> <journal> IEEE Transactions on Computers 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 206-217. </pages>
Reference-contexts: Because the goal of data parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. With the exception of MPP Apprentice [14], existing performance tools (e.g. <ref> [10, 9, 7, 12] </ref>) lack the ability to relate performance information from the compiler-generated message-passing code back to the source in the presence of substantial code restructuring by the compiler.
Reference: [8] <author> Pase, D. </author> <type> Personal communication, </type> <month> Aug. </month> <year> 1995. </year> <month> 17 </month>
Reference-contexts: This basic-block mapping enables the Apprentice to relate measured runtime costs back to source code sections (as small as basic blocks and procedure call sites) in the presence of arbitrary scalar or parallel optimizations <ref> [8] </ref>. Our system is currently limited to source-to-source parallelizing transformations supported by the Fortran D compiler.
Reference: [9] <author> Reed, D. A. </author> <title> Performance Instrumentation Techniques for Parallel Systems. In Models and Techniques for Performance Evaluation of Computer and Communications Systems, </title> <editor> L. Donatiello and R. Nelson, Eds. </editor> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <year> 1993, </year> <pages> pp. 463-490. </pages>
Reference-contexts: Principal components of the toolkit include an extensible data capture library [11] for monitoring program executions, a Self-Defining Data Format (SDDF) [2] that describes the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit <ref> [9] </ref>. SDDF provides a flexible medium for information interchange between components of the integrated system. Both dynamic performance measurement data collected by the data capture library and static analysis results recorded by the Fortran 77D compiler are represented using families of SDDF records designed for these tasks. <p> Because the goal of data parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. With the exception of MPP Apprentice [14], existing performance tools (e.g. <ref> [10, 9, 7, 12] </ref>) lack the ability to relate performance information from the compiler-generated message-passing code back to the source in the presence of substantial code restructuring by the compiler.
Reference: [10] <author> Reed, D. A. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation (May 1994), </booktitle> <pages> pp. 25-51. </pages>
Reference-contexts: Because the goal of data parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. With the exception of MPP Apprentice [14], existing performance tools (e.g. <ref> [10, 9, 7, 12] </ref>) lack the ability to relate performance information from the compiler-generated message-passing code back to the source in the presence of substantial code restructuring by the compiler.
Reference: [11] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: Relevant symbolic values such as message sizes are recorded at compile-time if known, and otherwise instrumented and recorded at runtime. The Pablo performance analysis toolkit serves as the basis for performance analysis in our integrated system. Principal components of the toolkit include an extensible data capture library <ref> [11] </ref> for monitoring program executions, a Self-Defining Data Format (SDDF) [2] that describes the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [9]. SDDF provides a flexible medium for information interchange between components of the integrated system. <p> The compiler also adds instrumentation to record values of relevant symbolics not known at compile time. Pablo's instrumentation library <ref> [11] </ref> supports counting or tracing dynamic events and time intervals as well as specialized instrumentation support for tracing input/output operations, procedure calls, loops, and message passing library calls. The library has been augmented, via its extension interfaces, to support integration with the Fortran D compiler.
Reference: [12] <author> Ries, B., Anderson, R., Auld, W., Breazeal, D., Callaghan, K., Richards, E., and Smith, W. </author> <title> The Paragon Performance Monitoring Environment. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. 1993), Association for Computing Machinery, </booktitle> <pages> pp. 850-859. </pages>
Reference-contexts: Because the goal of data parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. With the exception of MPP Apprentice [14], existing performance tools (e.g. <ref> [10, 9, 7, 12] </ref>) lack the ability to relate performance information from the compiler-generated message-passing code back to the source in the presence of substantial code restructuring by the compiler.
Reference: [13] <author> TMC. </author> <title> Prism User's Guide, </title> <institution> V1.2. Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: For high-level parallel languages, such tools can only capture and present dynamic performance data in terms of primitive operations (e.g. communication library calls) in the compiler-generated code. A few tools (Prism <ref> [13] </ref>, NV [4], and MPP-Apprentice [14]) provide source-level support for performance analysis of high-level parallel languages. However, none of these tools provide source-level performance support for the combination of data-parallel languages and optimizing compilers necessary for Fortran D or HPF. <p> Notable exceptions include Prism <ref> [13] </ref> and NV [4] for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [14].
Reference: [14] <author> Williams, W., Hoel, T., and Pase, D. </author> <title> The MPP Apprentice Performance Tool: Delivering the Performance of the Cray T3D. In Programming Environments for Massively Parallel Distributed Systems (Basel, </title> <address> Switzerland, 1994), </address> <publisher> Birkhauser Verlag. </publisher> <pages> 18 </pages>
Reference-contexts: For high-level parallel languages, such tools can only capture and present dynamic performance data in terms of primitive operations (e.g. communication library calls) in the compiler-generated code. A few tools (Prism [13], NV [4], and MPP-Apprentice <ref> [14] </ref>) provide source-level support for performance analysis of high-level parallel languages. However, none of these tools provide source-level performance support for the combination of data-parallel languages and optimizing compilers necessary for Fortran D or HPF. <p> Because the goal of data parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. With the exception of MPP Apprentice <ref> [14] </ref>, existing performance tools (e.g. [10, 9, 7, 12]) lack the ability to relate performance information from the compiler-generated message-passing code back to the source in the presence of substantial code restructuring by the compiler. <p> Notable exceptions include Prism [13] and NV [4] for CM-Fortran, Forge90 [1] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D <ref> [14] </ref>. Our system differs from these tools in providing comprehensive support for data-parallel programs by mapping runtime costs to data objects as well as to code sections and by including extensive compiler analysis information to explain the performance characteristics of the source program.
References-found: 14

