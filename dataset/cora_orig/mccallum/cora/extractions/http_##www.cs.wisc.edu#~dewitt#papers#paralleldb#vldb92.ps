URL: http://www.cs.wisc.edu/~dewitt/papers/paralleldb/vldb92.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/paralleldb.html
Root-URL: 
Title: Practical Skew Handling in Parallel Joins  
Author: David J. DeWitt Jeffrey F. Naughton Donovan A. Schneider S. Seshadri 
Date: July 14, 1992  
Abstract: We present an approach to dealing with skew in parallel joins in database systems. Our approach is easily implementable within current parallel DBMS, and performs well on skewed data without degrading the performance of the system on non-skewed data. The main idea is to use multiple algorithms, each specialized for a different degree of skew, and to use a small sample of the relations being joined to determine which algorithm is appropriate. We developed, implemented, and experimented with four new skew-handling parallel join algorithms; one, which we call virtual processor range partitioning, was the clear winner in high skew cases, while traditional hybrid hash join was the clear winner in lower skew or no skew cases. We present experimental results from an implementation of all four algorithms on the Gamma parallel database machine. To our knowledge, these are the first reported skew-handling numbers from an actual implementation.
Abstract-found: 1
Intro-found: 1
Reference: [BFKS87] <author> C. Baru, O. Frieder, D. Kandlur, and M. Segal. </author> <title> Join on a cube: Analysis, simulation, and implementation. </title> <editor> In M. Kitsuregawa and H. Tanaka, editors, </editor> <title> Database Machines and Knowledge Base Machines. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, <ref> [BFKS87, Bra87, DG85, DGS88, KTMo83] </ref>.) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., [LY90, SD89]).
Reference: [BGMP79] <author> M. W. Blasgen, J. Gray, M. Mitoma, and T. Price. </author> <title> The convoy phenomenon. </title> <journal> Operating System Review, </journal> <volume> 13(2), </volume> <year> 1979. </year>
Reference-contexts: Each module supports eight full-duplex, serial, reliable communication channels operating at 2.8 megabytes/sec. Gamma is built on top of an operating system designed specifically for supporting database management systems. NOSE provides multiple, lightweight processes with shared memory. A non-preemptive scheduling policy is used to help prevent convoys <ref> [BGMP79] </ref> from occurring. NOSE provides communications between NOSE processes using the reliable message passing hardware of the Intel iPSC/2 hypercube. File services in NOSE are based on the Wisconsin Storage System (WiSS) [CDKK85].
Reference: [Bra87] <author> Kjell Bratbergsengen. </author> <title> Algebra operations on a parallel computer performance evaluation. </title> <editor> In M. Kitsuregawa and H. Tanaka, editors, </editor> <title> Database Machines and Knowledge Base Machines. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, <ref> [BFKS87, Bra87, DG85, DGS88, KTMo83] </ref>.) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., [LY90, SD89]).
Reference: [CDKK85] <author> H-T. Chou, David J. Dewitt, Randy H. Katz, and Anthony C. Klug. </author> <title> Design and implementation of the Wisconsin Storage System. </title> <journal> Software|Practice and Experience, </journal> <volume> 15(10) </volume> <pages> 943-962, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A non-preemptive scheduling policy is used to help prevent convoys [BGMP79] from occurring. NOSE provides communications between NOSE processes using the reliable message passing hardware of the Intel iPSC/2 hypercube. File services in NOSE are based on the Wisconsin Storage System (WiSS) <ref> [CDKK85] </ref>. The services provided by WiSS include sequential files, byte-stream files as in UNIX, B + tree indices, long data items, an external sort utility, and a scan mechanism.
Reference: [Coc77] <author> William G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, New York, 3 edition, </address> <year> 1977. </year>
Reference-contexts: But first we need to discuss how the approximate splitting vectors are computed. For each algorithm except hybrid hash, we first used sampling to compute a statistical profile of the join attribute values of the two relations to be joined. We obtained this sample by using 7 stratified sampling <ref> [Coc77] </ref> with each stratum consisting of the set of tuples initially residing at a processor. Within each processor, the sampling was performed using page-level extent map sampling. Extent map sampling is described in Section 3. Issues involving stratified sampling and page level sampling are discussed in [SN92].
Reference: [CW79] <author> J. Lawrence Carter and Mark N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year> <month> 22 </month>
Reference-contexts: However, load imbalance due to a poor hash function can be removed by choosing a better hash function; the theoretical literature on hashing gives a number of techniques designed to find a hash function that with high probability performs well <ref> [CW79] </ref>. A more fundamental problem arises from repeated values in the join attribute. By definition, any hash function must map tuples with equal join attribute values to the same processor, so there is no way a clever hash function can avoid load imbalances that result from these repeated values.
Reference: [DG85] <author> David M. DeWitt and Robert Gerber. </author> <title> Multiprocessor hash-based join algorithms. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Databases, </booktitle> <pages> pages 151-164, </pages> <address> Stockholm, Sweden, </address> <year> 1985. </year>
Reference-contexts: Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, <ref> [BFKS87, Bra87, DG85, DGS88, KTMo83] </ref>.) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., [LY90, SD89]).
Reference: [DG92] <author> D. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of high performance database processing. </title> <journal> Communications of the ACM, </journal> <note> 1992. To appear. </note>
Reference-contexts: It is clear from the success of these systems that parallelism is an effective means of meeting the performance requirements of large database applications. However, the basic technique that these systems use for exploiting intra-query parallelism (hash-based redistribution of relations on their joining attribute) <ref> [DG92] </ref> is vulnerable to the presence of skew in the underlying data. Simply put, if the underlying data is sufficiently skewed, load imbalances in the resulting parallel join execution will swamp any of the gains due to parallelism and unacceptable performance will result. <p> Items 1 and 2 above were straightforward. We now discuss the changes to the redistribution code in more detail. Basic parallel hybrid hashing in Gamma makes use of a data structure called a split table <ref> [DGS + 90, DG92] </ref>. This data structure contains entries that are (hash bucket, processor number) pairs. If k processors are being used to execute a relational operation, then the split tables have k entries.
Reference: [DGG + 86] <author> David J. Dewitt, Robert H. Gerber, Goetz Graefe, Michael L. Heytens, Krishna B. Kumar, and M. Muralikrishna. </author> <title> GAMMA | a high performance dataflow database machine. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Databases, </booktitle> <pages> pages 228-237, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Thus, each relation occupies approximately 50 megabytes of disk space. 12 All experiments were conducted using 30 processors with disks. Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join <ref> [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] </ref> and sorting [DNS91b] tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations.
Reference: [DGS88] <author> David J. DeWitt, Shahram Ghandeharizadeh, and Donovan Schneider. </author> <title> A performance analysis of the GAMMA database machine. </title> <booktitle> In Proceedings of the SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 350-360, </pages> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Thus, each relation occupies approximately 50 megabytes of disk space. 12 All experiments were conducted using 30 processors with disks. Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join <ref> [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] </ref> and sorting [DNS91b] tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations. <p> Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, <ref> [BFKS87, Bra87, DG85, DGS88, KTMo83] </ref>.) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., [LY90, SD89]).
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I Hsiao, and R. Ras-mussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: This technique is most efficient if the correlation on the join attribute within a page is low. Implementation in Gamma In order to investigate the performance of our skew handling algorithms, we implemented the algorithms using Gamma <ref> [DGS + 90] </ref> as our experimental vehicle. Gamma falls into the class of shared-nothing [Sto86] architectures. The hardware consists of a 32 processor Intel iPSC/2 hypercube. Each processor is configured with a 80386 CPU, 8 megabytes of memory, and a 330 megabyte MAXTOR 4380 (5 1/4 in.) disk drive. <p> Items 1 and 2 above were straightforward. We now discuss the changes to the redistribution code in more detail. Basic parallel hybrid hashing in Gamma makes use of a data structure called a split table <ref> [DGS + 90, DG92] </ref>. This data structure contains entries that are (hash bucket, processor number) pairs. If k processors are being used to execute a relational operation, then the split tables have k entries. <p> Thus, each relation occupies approximately 50 megabytes of disk space. 12 All experiments were conducted using 30 processors with disks. Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join <ref> [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] </ref> and sorting [DNS91b] tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations. <p> Join Product Skew (JPS): The join selectivity on individual nodes may differ, leading to an imbalance in the number of output tuples produced. Walton et al. use an analytical model in order to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [SD89, DGS + 90] </ref>. The main result is that scheduling hash effectively handles RS while hybrid hash degrades and eventually becomes worse than scheduling hash as RS increases.
Reference: [DNS91a] <author> David J. DeWitt, Jeffrey F. Naughton, and Donovan A. Schneider. </author> <title> A comparison of non-equijoin algorithms. </title> <booktitle> In Proceedings of the Eighteenth International Conference on Very Large Databases, </booktitle> <address> Barcelona, Spain, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This technique of sampling for approximate splitting vectors has been used previously in DBMS algorithms for evaluating non-equijoins <ref> [DNS91a] </ref> and for parallel external sorting [DNS91b]. A theoretical investigation of 4 R (K1,A) S (K2,B) (2,3) (2,2) (4,3) (4,4) Table 1: Example relations R and S. the performance of sampling-based range splitting appears in [SN92]. <p> Stratified sampling requires that each processor take some specified number of samples from its partition of the database. A number of techniques have been proposed for this problem, notably sampling from B + trees [OR89], sampling from hash tables [ORX90], and using a dense index on a primary key <ref> [DNS91a] </ref>. In this section we describe a new technique that we call extent map sampling. Extent-based sampling requires neither an index on a dense primary key nor an index on any other attribute. <p> Thus, each relation occupies approximately 50 megabytes of disk space. 12 All experiments were conducted using 30 processors with disks. Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join <ref> [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] </ref> and sorting [DNS91b] tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations.
Reference: [DNS91b] <author> David J. DeWitt, Jeffrey F. Naughton, and Donovan A. Schneider. </author> <title> Parallel external sorting using probabilistic splitting. </title> <booktitle> In PDIS, </booktitle> <address> Miami Beach, Florida, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: This technique of sampling for approximate splitting vectors has been used previously in DBMS algorithms for evaluating non-equijoins [DNS91a] and for parallel external sorting <ref> [DNS91b] </ref>. A theoretical investigation of 4 R (K1,A) S (K2,B) (2,3) (2,2) (4,3) (4,4) Table 1: Example relations R and S. the performance of sampling-based range splitting appears in [SN92]. <p> Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] and sorting <ref> [DNS91b] </ref> tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations. Single Skew Experiments In the first set of experiments we ran the building relation was skewed and the probing relation was uniform.
Reference: [ESW78] <author> Robert Epstein, Michael Stonebraker, and Eugene Wong. </author> <title> Distributed query processing in a relational database system. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1978. </year>
Reference-contexts: We call this technique subset-replicate. (Subset-replicate is similar to the fragment-replicate technique proposed for distributed relational query processing by Epstein et al. <ref> [ESW78] </ref>.) As an example, suppose we are joining R and S with the join predicate R:A = S:B. Furthermore, suppose that the relations R and S contain tuples as shown in Table 1. Suppose we wish to join R and S on two processors.
Reference: [Gra69] <author> R. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17:416 - 429, </volume> <year> 1969. </year>
Reference-contexts: We used the heuristic scheduling algorithm known as "LPT" <ref> [Gra69] </ref>.
Reference: [HL91] <author> Kien A. Hua and Chiang Lee. </author> <title> Handling data skew in multiprocessor database computers using partition tuning. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Databases, </booktitle> <pages> pages 525-535, </pages> <address> Barcelona, Spain, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: For example, most of the previously proposed skew handling algorithms require that the relations to be joined are completely fl Department of Computer Sciences, University of Wisconsin-Madison. y HP-Labs, Palo Alto. 1 scanned before the join begins <ref> [HL91, WDYT90, KO90] </ref>. Since the time to perform a parallel hash join is a small multiple of the time required to scan the two relations being joined, this can represent a substantial overhead, which is unacceptable for anything but extremely skewed data. <p> However, as they point out, this algorithm is very susceptible to RS. 5.4 Hua and Lee Hua and Lee <ref> [HL91] </ref> proposed three algorithms for processing parallel joins in the presence of AVS. The first algorithm, tuple interleaving parallel hash join, is based on the bucket-spreading hash join algorithm of Kitsuregawa and Ogawa [KO90].
Reference: [KO90] <author> Masaru Kitsuregawa and Yasushi Ogawa. </author> <title> Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the Super Database Computer (SDC). </title> <booktitle> In Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <address> Brisbane, England, </address> <month> August </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: For example, most of the previously proposed skew handling algorithms require that the relations to be joined are completely fl Department of Computer Sciences, University of Wisconsin-Madison. y HP-Labs, Palo Alto. 1 scanned before the join begins <ref> [HL91, WDYT90, KO90] </ref>. Since the time to perform a parallel hash join is a small multiple of the time required to scan the two relations being joined, this can represent a substantial overhead, which is unacceptable for anything but extremely skewed data. <p> Experiments with "double-skew" (which lead to JPS) were not run but we extrapolated that the problems would be worse because this case is a superset of the RS for the building relation. 5.3 Kitsuregawa and Ogawa Kitsuregawa and Ogawa <ref> [KO90] </ref> describe two algorithms, bucket-converging parallel hash-join and bucket-spreading parallel hash join. The bucket-converging hash join is a basic paralleliza-tion of the GRACE join algorithm [KTMo83]. Relation R is read from disk in parallel and partitioned into p buckets (where p is much larger than k, the number of nodes). <p> The first algorithm, tuple interleaving parallel hash join, is based on the bucket-spreading hash join algorithm of Kitsuregawa and Ogawa <ref> [KO90] </ref>. The major difference is that instead of relying on a specially designed intelligent network for mapping buckets to nodes, this decision is handled in software by a coordinator node. <p> The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa <ref> [KO90] </ref>. It differs in that it doesn't rely on special-purpose hardware, it assigns buckets to processor (s) using a first-fit decreasing heuristic, and it has other optimizations for the shared-memory environment.
Reference: [KTMo83] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1), </volume> <year> 1983. </year>
Reference-contexts: The solution to this problem is to choose many more partitions than there are processors. This idea has appeared many times before in the skew join literature with respect to hash bucket partitioning; the first reference to the technique is probably in <ref> [KTMo83] </ref>. We refer to the technique of using multiple range partitions per node as virtual processor partitioning. In the previous example, if we chose 100 buckets per processor, for a total of 1000 buckets, 6 we would have a fine enough granularity to resolve this problem. <p> Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, <ref> [BFKS87, Bra87, DG85, DGS88, KTMo83] </ref>.) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., [LY90, SD89]). <p> The bucket-converging hash join is a basic paralleliza-tion of the GRACE join algorithm <ref> [KTMo83] </ref>. Relation R is read from disk in parallel and partitioned into p buckets (where p is much larger than k, the number of nodes). Since each bucket is statically assigned to a particular node, all of R is redistributed during this phase of the algorithm.
Reference: [LY90] <author> M. Seetha Lakshmi and Philip S. Yu. </author> <title> Effectiveness of parallel joins. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, [BFKS87, Bra87, DG85, DGS88, KTMo83].) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., <ref> [LY90, SD89] </ref>). In this section, we examine a number of previously proposed algorithms for dealing with data skew and compare these algorithms with our own. 5.1 Walton, Dale, and Jenevein Walton et al. [WDJ91] present a taxonomy of skew in parallel databases.
Reference: [Omi91] <author> Edward Omiecinski. </author> <title> Performance analysis of a load balancing hash-join algorithm for a shared memory multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The term "scalar skew" is due to Walton et al. [WDJ91]. This is also the model of skew used by Omiecinski <ref> [Omi91] </ref>. The exact description of the attributes are as follows. In each case, we are assuming a relation of N tuples, and that N 100; 000. The attributes relevant to our experiments are x1, x10, x100, x1000, x10000, x20000, x30000, x40000, and x50000. <p> Without implementing both algorithms on the same hardware and software base it is probably impossible to determine precisely which algorithm provides the best overall performance. 5.6 Omiecinski Omiecinski <ref> [Omi91] </ref> proposed a load balancing hash-join algorithm for a shared memory multiprocessor. The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa [KO90].
Reference: [OR89] <author> Frank Olken and Doron Rotem. </author> <title> Random sampling from B + -trees. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Databases, </booktitle> <pages> pages 269-278, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Stratified sampling requires that each processor take some specified number of samples from its partition of the database. A number of techniques have been proposed for this problem, notably sampling from B + trees <ref> [OR89] </ref>, sampling from hash tables [ORX90], and using a dense index on a primary key [DNS91a]. In this section we describe a new technique that we call extent map sampling. Extent-based sampling requires neither an index on a dense primary key nor an index on any other attribute.
Reference: [ORX90] <author> Frank Olken, Doron Rotem, and Ping Xu. </author> <title> Random sampling from hash files. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 375-386, </pages> <address> Atlantic City, New Jersey, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Stratified sampling requires that each processor take some specified number of samples from its partition of the database. A number of techniques have been proposed for this problem, notably sampling from B + trees [OR89], sampling from hash tables <ref> [ORX90] </ref>, and using a dense index on a primary key [DNS91a]. In this section we describe a new technique that we call extent map sampling. Extent-based sampling requires neither an index on a dense primary key nor an index on any other attribute.
Reference: [SD89] <author> Donovan A. Schneider and David J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 110-121, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The basic idea in our approach is that we have multiple algorithms, each optimized for differing degrees of skew. We found in our experiments that two algorithms are sufficient: the usual parallel hybrid hash join algorithm <ref> [SD89] </ref>, and a new algorithm that we call virtual processor range partitioning, performs well on moderately skewed data at a cost slightly higher than that of the parallel hybrid hash join. <p> This observation is validated by results that we reported in <ref> [SD89] </ref> and by our experimental results in Section 4. Subset-Replicate One complication arises with join processing via range partitioning in the presence of highly skewed data: for equal sized partitions, it might be necessary to map a single data value to multiple partitions. <p> Issues involving stratified sampling and page level sampling are discussed in [SN92]. We now describe the skew handling algorithms. 1. Hybrid hash. This is just the basic parallel hybrid hash algorithm (with no modifications for skew handling.) A description of this algorithm and some alternatives appears in <ref> [SD89] </ref>. 2. Simple range partitioning. At the top level, this algorithm works as follows: (a) Sample the building (inner) relation. (b) Use the samples to compute an approximate partitioning vector. <p> Overflow tuples are partitioned into buckets sized so that each such bucket will fit in main memory <ref> [SD89] </ref>. (e) Redistribute the probing (outer) relation using the same approximate partitioning vector as in step 3. (f) For each tuple of the probing relation probe the in-memory hash table, outputting a join result tuple for each match. <p> Thus, each relation occupies approximately 50 megabytes of disk space. 12 All experiments were conducted using 30 processors with disks. Speedup or scaleup exper-iments were not performed as we were more interested in focusing on the relative performance of the different algorithms. Furthermore, previous join <ref> [DGG + 86, DGS + 90, DGS88, DNS91a, SD89] </ref> and sorting [DNS91b] tests demonstrated that the Gamma provides linear speedup and scaleup over a wide range of different hardware and software configurations. <p> Originally, join attribute values were assumed to be uniformly distributed and hence skew was not a problem (see, for example, [BFKS87, Bra87, DG85, DGS88, KTMo83].) As parallel join algorithms have matured, this uniformity assumption has been challenged (see, eg., <ref> [LY90, SD89] </ref>). In this section, we examine a number of previously proposed algorithms for dealing with data skew and compare these algorithms with our own. 5.1 Walton, Dale, and Jenevein Walton et al. [WDJ91] present a taxonomy of skew in parallel databases. <p> Join Product Skew (JPS): The join selectivity on individual nodes may differ, leading to an imbalance in the number of output tuples produced. Walton et al. use an analytical model in order to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [SD89, DGS + 90] </ref>. The main result is that scheduling hash effectively handles RS while hybrid hash degrades and eventually becomes worse than scheduling hash as RS increases. <p> However, unless the join is significantly skewed, the absolute performance of hybrid hash is significantly better than that of scheduling hash. 17 5.2 Schneider and DeWitt In <ref> [SD89] </ref>, we explored the effect of skewed data distributions on four parallel join algorithms in an 8 processor version of the Gamma database machine. The experiments were designed such that TPS and SS were absent.
Reference: [SN92] <author> S. Seshadri and Jeffrey F. Naughton. </author> <title> Sampling issues in parallel database systems. </title> <booktitle> In Proceedings of the EDBT Conference, </booktitle> <address> Vienna, Austria, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: A theoretical investigation of 4 R (K1,A) S (K2,B) (2,3) (2,2) (4,3) (4,4) Table 1: Example relations R and S. the performance of sampling-based range splitting appears in <ref> [SN92] </ref>. In a two relation join, say R 1 S, the question arises whether an algorithm should attempt to balance the number of R tuples per node, or the number of S tuples per node, or the sum of the R and S tuples per node. <p> Within each processor, the sampling was performed using page-level extent map sampling. Extent map sampling is described in Section 3. Issues involving stratified sampling and page level sampling are discussed in <ref> [SN92] </ref>. We now describe the skew handling algorithms. 1. Hybrid hash. This is just the basic parallel hybrid hash algorithm (with no modifications for skew handling.) A description of this algorithm and some alternatives appears in [SD89]. 2. Simple range partitioning.
Reference: [Sto86] <author> M. Stonebraker. </author> <title> The case for shared nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1), </volume> <year> 1986. </year>
Reference-contexts: Implementation in Gamma In order to investigate the performance of our skew handling algorithms, we implemented the algorithms using Gamma [DGS + 90] as our experimental vehicle. Gamma falls into the class of shared-nothing <ref> [Sto86] </ref> architectures. The hardware consists of a 32 processor Intel iPSC/2 hypercube. Each processor is configured with a 80386 CPU, 8 megabytes of memory, and a 330 megabyte MAXTOR 4380 (5 1/4 in.) disk drive.
Reference: [WDJ91] <author> Christopher B. Walton, Alfred G. Dale, and Roy M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: From the preceding description it should be clear that for good parallelization the number of tuples mapped to each processor should be approximately equal, or else load imbalances will result (this form of imbalance is what Walton <ref> [WDJ91] </ref> terms redistribution skew). These load imbalances could be the result of a poorly designed hash function. <p> This form of load imbalance results if the join selectivity for R i 1 S i differs from the join selectivity for R j 1 S j . This type of load imbalance is called join product skew by Walton et al. <ref> [WDJ91] </ref>. 2.2 Skew Avoidance Fundamentals In the next five subsections we describe the techniques we apply to resolving both types of skew. Range Partitioning A basic approach to avoiding redistribution skew is to replace hash partitioning with range partitioning. <p> Finally, it captures the essence of the Zipfian distribution (a small number of highly skewed values with the bulk of the values appearing very infrequently) without suffering its drawbacks. The term "scalar skew" is due to Walton et al. <ref> [WDJ91] </ref>. This is also the model of skew used by Omiecinski [Omi91]. The exact description of the attributes are as follows. In each case, we are assuming a relation of N tuples, and that N 100; 000. <p> In this section, we examine a number of previously proposed algorithms for dealing with data skew and compare these algorithms with our own. 5.1 Walton, Dale, and Jenevein Walton et al. <ref> [WDJ91] </ref> present a taxonomy of skew in parallel databases. First, they distinguish between attribute value skew (AVS) which is skew inherent in the dataset, and partition skew which occurs in parallel machines when the load is not balanced between the nodes.
Reference: [WDYT90] <author> Joel L. Wolf, Daniel M. Dias, Philip S. Yu, and John J. Turek. </author> <title> An effective algorithm for parallelizing hash joins in the presence of data skew. </title> <institution> IBM T. J. Watson Research Center Tech Report RC 15510, </institution> <year> 1990. </year> <month> 24 </month>
Reference-contexts: For example, most of the previously proposed skew handling algorithms require that the relations to be joined are completely fl Department of Computer Sciences, University of Wisconsin-Madison. y HP-Labs, Palo Alto. 1 scanned before the join begins <ref> [HL91, WDYT90, KO90] </ref>. Since the time to perform a parallel hash join is a small multiple of the time required to scan the two relations being joined, this can represent a substantial overhead, which is unacceptable for anything but extremely skewed data. <p> We used the heuristic scheduling algorithm known as "LPT" [Gra69]. This approach is similar to that used by Wolf et al. <ref> [WDYT90] </ref> in scheduling hash partitions, although in that paper the statistics used to schedule these partitions are gained by a complete scan of both relations rather than by sampling, and hash partitioning is used instead of range partitioning. 2.3 Algorithm Description The algorithms that we implemented can be described in terms <p> Join Product Skew (JPS): The join selectivity on individual nodes may differ, leading to an imbalance in the number of output tuples produced. Walton et al. use an analytical model in order to compare the scheduling hash-join algorithm of <ref> [WDYT90] </ref> and the hybrid hash-join algorithm of Gamma [SD89, DGS + 90]. The main result is that scheduling hash effectively handles RS while hybrid hash degrades and eventually becomes worse than scheduling hash as RS increases. <p> The allocation decision is broadcast to all the nodes and all the buckets are redistributed across the network. Local joins of respective buckets are then performed on each node. The basic form of this algorithm is identical to that of Wolf et al. <ref> [WDYT90] </ref>. The algorithms differ in the computation of the allocation strategy. The three algorithms are compared using an analytical model. <p> The cost of this step is certainly higher than the cost we incur sampling one or both relations (about 1/2 second each in our implementation). 19 5.5 Wolf, Dias and Yu Wolf et al. <ref> [WDYT90] </ref>, propose an algorithm for parallelizing hash joins in the presence of severe data skew. The scheduling hash algorithm is as follows. Relations R and S are read, local selections or projections are applied, and the results are written back locally as a set of coarse hash buckets.
References-found: 27

