URL: http://www.cs.wisc.edu/~vernon/cs747/97/papers/gms.96asplos.ps
Refering-URL: http://www.cs.wisc.edu/~vernon/cs747/97/projects.html
Root-URL: 
Title: Reducing Network Latency Using Subpages in a Global Memory Environment  
Author: Herve A. Jamrozik, Michael J. Feeley, Geoffrey M. Voelker, James Evans II Anna R. Karlin, Henry M. Levy, and Mary K. Vernon 
Date: October 1996.  
Note: First published in the Proceedings of the Seventh ACM Conference on Architectural Support for Programming Languages and Operating Systems,  Also available as Technical Report UW-CSE-96-07-03.  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks, small transfers are needed to provide low latency. This trend in page size is thus at odds with the use of network memory on high-speed networks. This paper studies the use of subpages as a means of reducing transfer size and latency in a remote-memory environment. Using trace-driven simulation, we show how and why subpages reduce latency and improve performance of programs using network memory. Our results show that memory-intensive applications execute up to 1.8 times faster when executing with 1K-byte subpages, when compared to the same applications using full 8K-byte pages in the global memory system. Those same applications using 1K-byte subpages execute up to 4 times faster than they would using the disk for backing store. Using a prototype implementation on the DEC Alpha and AN2 network, we demonstrate how subpages can reduce remote-memory fault time; e.g., our prototype is able to satisfy a fault on a 1K subpage stored in remote memory in 0.5 milliseconds, one third the time of a full page. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Susan S. Owicki, James B. Saxe, and Charles P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The prototype and simulator are described below. 3.1 System Prototype Our prototype is implemented as an extension to GMS, a full global memory management system described in [7]. The system is integrated with Digital Unix V3.2; nodes are connected by a DEC AN2 155 Mb/sec ATM network <ref> [1] </ref>. Subpage protection is implemented in software by modifying the PALcode [17] on the DEC Alpha 250. Our system keeps 32 subpage valid bits for each page, one bit for each 256-byte block; the valid bits indicate which subpages are currently valid (subpages are a multiple of 256 bytes).
Reference: [2] <author> Bradford Chamberlain, Tony DeRose, Dani Lischinski, David Salesin, and John Snyder. </author> <title> Fast rendering of complex environments using a spatial hierarchy. </title> <booktitle> In Proc. of Graphics Interface `96, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Atom [18] is the instrumentation software we used to generate the traces. To obtain this trace, we instrumented Atom itself (using Atom), then traced it while processing the gzip binary. The trace includes 73 million memory accesses and from 1175 to 5275 page faults. Render <ref> [2] </ref> is a graphics rendering program that displays a computer-generated scene from a large (over 100MB) pre-computed database. The trace includes 245 million memory references and from 1433 to 6145 page faults. gdb is the GNU debugger. We traced the initialization phase of the debugger, run without loading a program.
Reference: [3] <author> Albert Chang and Mark F. </author> <title> Merge. 801 storage: Architecture and programming. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Similar techniques have been used for trace production as well [22]. 2 The IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line <ref> [3] </ref>. 3 We have optimized the performance of global memory operations along the lines described in [21], hence our latencies are slightly better than those reported in [7]. 3 Performance Operation Cycles Time fast load 52 195 ns slow load 95 361 ns fast store 64 241 ns slow store 102
Reference: [4] <author> Douglas W. Clark, Butler W. Lampson, and Kenneth A. Pier. </author> <title> The memory system of a high-performance personal computer. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(10), </volume> <month> October </month> <year> 1981. </year>
Reference-contexts: As we will see, the first subpages to arrive following arrival of the faulted subpage take relatively little additional latency. This scheme is similar in some ways to the pipelining of cache data into cache lines over small buses <ref> [4] </ref>. It is interesting to compare our subpage schemes with an architecture that uses small pages (i.e., the size of one of our subpages).
Reference: [5] <author> Douglas Comer and James Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging <ref> [5, 9, 12] </ref> or global management and use of network-wide memory [10, 6, 7].
Reference: [6] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and David A. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging [5, 9, 12] or global management and use of network-wide memory <ref> [10, 6, 7] </ref>. That is, network nodes with memory-intensive This research was supported in part by grants from the National Science Foundation (CCR-9200832, CDA-9123308, CCR-9632769, CCR-9024144, GER-9550429, and GER-9450075) and from Digital Equipment Corporation. applications can use the primary memory of lightly-loaded nodes as temporary backing store.
Reference: [7] <author> Michael J. Feeley, William E. Morgan, Frederic H. Pighin, Anna R. Karlin, Henry M. Levy, and Chandramohan A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of the 15th ACM Sym-poisum on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging [5, 9, 12] or global management and use of network-wide memory <ref> [10, 6, 7] </ref>. That is, network nodes with memory-intensive This research was supported in part by grants from the National Science Foundation (CCR-9200832, CDA-9123308, CCR-9632769, CCR-9024144, GER-9550429, and GER-9450075) and from Digital Equipment Corporation. applications can use the primary memory of lightly-loaded nodes as temporary backing store. <p> How important are these measurements to a system using global page caching? In previous work, we showed how network memory could be managed globally by implementing a global memory management system on the Digital Unix operating system on DEC Alphas <ref> [7] </ref>. For our system, the complete latency for faulting an 8K Alpha page into memory from a remote node, including software overhead and management messages, was about 1.6 ms. <p> We used the prototype to provide measured software management and network overheads for the simulator, and to validate simulated results where possible. The prototype and simulator are described below. 3.1 System Prototype Our prototype is implemented as an extension to GMS, a full global memory management system described in <ref> [7] </ref>. The system is integrated with Digital Unix V3.2; nodes are connected by a DEC AN2 155 Mb/sec ATM network [1]. Subpage protection is implemented in software by modifying the PALcode [17] on the DEC Alpha 250. <p> IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line [3]. 3 We have optimized the performance of global memory operations along the lines described in [21], hence our latencies are slightly better than those reported in <ref> [7] </ref>. 3 Performance Operation Cycles Time fast load 52 195 ns slow load 95 361 ns fast store 64 241 ns slow store 102 383 ns null PAL call 15 56 ns L1 cache hit 3 11 ns L2 cache hit 8 30 ns L2 miss 84 315 ns Table 1: <p> The 5 next bar, marked p 8192, shows the performance of global memory management using full 8K pages. As previous studies have shown, the performance improvement of global memory relative to disk is significant in this case, the speedups range from 1.7 to 2.2, depending on memory availability <ref> [7] </ref>. The remaining bars show the performance for subpage sizes ranging from 4096 to 256 bytes. Subpages offer improvement over full pages ranging from about 8% (for 256-byte subpages in full-mem) to 40% (for 2K subpages in 1/4-mem).
Reference: [8] <author> Edward W. Felten, Richard D. Alpert, Angelos Bilas, Matthias A. Blumrich, Douglas W. Clark, Stefanos N. Dami-anakis, Cezary Dubnick, Liviu Iftode, and Kai Li. </author> <title> Early experience with message-passing on the Shrimp multicomputer. </title> <booktitle> In Proc. of the 23rd International Symposium of Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers [23, 21], and newer controllers reduce latency even further <ref> [8] </ref>. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead. For remote memory access, then, smaller transfers may ultimately be needed to reduce latency and improve program performance.
Reference: [9] <author> Edward W. Felten and John Zahorjan. </author> <title> Issues in the implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging <ref> [5, 9, 12] </ref> or global management and use of network-wide memory [10, 6, 7].
Reference: [10] <author> Michael J. Franklin, Michael J. Carey, and Miron Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging [5, 9, 12] or global management and use of network-wide memory <ref> [10, 6, 7] </ref>. That is, network nodes with memory-intensive This research was supported in part by grants from the National Science Foundation (CCR-9200832, CDA-9123308, CCR-9632769, CCR-9024144, GER-9550429, and GER-9450075) and from Digital Equipment Corporation. applications can use the primary memory of lightly-loaded nodes as temporary backing store.
Reference: [11] <author> Samuel P. Harbison. </author> <title> Modula-3. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NY, </address> <year> 1992. </year>
Reference-contexts: We traced and analyzed several applications executing in the simulated subpage environment. To see the effect of heavy paging activity, we ran the applications in different memory configurations. The applications we used were the following: Modula-3 is the DEC SRC compiler for the Modula-3 <ref> [11] </ref> programming language. We traced a Modula-3 compilation of smalldb, a library that transparently maintains a copy of in-memory data structures on secondary storage. The trace in cludes 87 million memory references, and from 773 to 5655 faults, depending on the memory configuration. ld is the unix object file linker.
Reference: [12] <author> Liviu Iftode, Karin Petersen, and Kai Li. </author> <title> Memory servers for multicomputers. </title> <booktitle> In Proceedings of the IEEE Spring COMP-CON '93, </booktitle> <pages> pages 538-547, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction New-generation networks, such as ATM, have now surpassed disks in their ability to transfer data rapidly into processor memory. As research has shown, such performance greatly encourages remote paging <ref> [5, 9, 12] </ref> or global management and use of network-wide memory [10, 6, 7].
Reference: [13] <author> Edward D. Lazowska, John Zahorjan, David R. Cheriton, and Willy Zwaenepoel. </author> <title> File access performance of diskless workstations. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 4(3), </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: A major disadvantage of the small page scheme, relative to sub-pages, is the reduced TLB coverage and therefore higher TLB miss rate that small pages would incur. Furthermore, previous work <ref> [13] </ref> has shown that although smaller transfers offer the potential for increased locality, this advantage is outweighed by the increased overhead of the multiple requests required. We performed experiments to confirm that this is true for our environment as well.
Reference: [14] <author> S. Reinhardt, M. Hill, J. Larus, A. Lebeck, J. Lewis, and D.Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proc. of the ACM Sigmetrics Conf. on Measurement and Modelling of Computer Systems, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: components: (1) a fixed cost of about 0.27 milliseconds for handling the fault, locating the page in the network, sending a request message to the node storing the page, processing the request message, and resuming the 1 An alternative technique would be that used by the Wisconsin Wind Tunnel project <ref> [15, 14] </ref> to implement additional state bits for SVM on the CM5; they used ECC bits to cause faults, however, this would still require emulating writes, and the Alpha 250 has imprecise exceptions on data parity errors, making use of parity difficult or impossible.
Reference: [15] <author> Steve K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Ker--nel support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proc. of the 2nd USENIX Symp. on Micorkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: components: (1) a fixed cost of about 0.27 milliseconds for handling the fault, locating the page in the network, sending a request message to the node storing the page, processing the request message, and resuming the 1 An alternative technique would be that used by the Wisconsin Wind Tunnel project <ref> [15, 14] </ref> to implement additional state bits for SVM on the CM5; they used ECC bits to cause faults, however, this would still require emulating writes, and the Alpha 250 has imprecise exceptions on data parity errors, making use of parity difficult or impossible.
Reference: [16] <author> Ted Romer, Wayne Ohlrich, Anna Karlin, and Brian Bershad. </author> <title> Reducing TLB and memory overhead using online superpage promotion. </title> <booktitle> In Proc. of the 22nd Annual Int. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [20, 19, 16] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [17] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> One Burlington Woods Drive, Burlington, MA 01803, </address> <year> 1992. </year>
Reference-contexts: The system is integrated with Digital Unix V3.2; nodes are connected by a DEC AN2 155 Mb/sec ATM network [1]. Subpage protection is implemented in software by modifying the PALcode <ref> [17] </ref> on the DEC Alpha 250. Our system keeps 32 subpage valid bits for each page, one bit for each 256-byte block; the valid bits indicate which subpages are currently valid (subpages are a multiple of 256 bytes).
Reference: [18] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <type> Technical Report 94/2, </type> <institution> DEC Western Research Lab, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: It takes as input memory reference traces generated from applications instrumented by Atom <ref> [18] </ref>. It then simulates these memory references and models the effect of paging to both local disk and to remote memory. Paging policy is determined by a configurable memory management module; an LRU policy is used by default. <p> We traced a link of Digital Unix V3.2 to generate our ld data. The trace includes 102 million memory references, and from 6807 to 10629 page faults. Atom <ref> [18] </ref> is the instrumentation software we used to generate the traces. To obtain this trace, we instrumented Atom itself (using Atom), then traced it while processing the gzip binary. The trace includes 73 million memory accesses and from 1175 to 5275 page faults.
Reference: [19] <author> Madhusudhan Talluri and Mark D. Hill. </author> <title> Surpassing the TLB performance of superpages with less operating system support. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [20, 19, 16] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [20] <author> Madhusudhan Talluri, Shing Kong, Mark D. Hill, and David A. Patterson. </author> <title> Tradeoffs in supporting two page sizes. </title> <booktitle> In Proc. of the 19th Annual Int. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Second, the combination of relatively small TLBs and large physical memories on current machines causes performance degradation due to insufficient TLB coverage. TLB coverage is increased by large page sizes or superpage mechanisms <ref> [20, 19, 16] </ref>; e.g., the DEC Alpha supports page sizes from 8KB to 1MB, the SUN UltraSPARC supports page sizes from 8KB to 4MB, and the MIPS R10000 supports page sizes from 4KB to 16MB. Unfortunately, the latency characteristics of high-speed networks are at odds with this trend.
Reference: [21] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating data and control transfer in distributed systems. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Prog. Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers <ref> [23, 21] </ref>, and newer controllers reduce latency even further [8]. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead. <p> have been used for trace production as well [22]. 2 The IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line [3]. 3 We have optimized the performance of global memory operations along the lines described in <ref> [21] </ref>, hence our latencies are slightly better than those reported in [7]. 3 Performance Operation Cycles Time fast load 52 195 ns slow load 95 361 ns fast store 64 241 ns slow store 102 383 ns null PAL call 15 56 ns L1 cache hit 3 11 ns L2 cache
Reference: [22] <author> Richard Uhlig, David Nagle, Trevor Mudge, and Stuart Sechrest. </author> <title> Trap-driven simulation with Tapeworm II. </title> <booktitle> In Proc. of the 6th Int. Conf. on Arch. Support for Prog. Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Similar techniques have been used for trace production as well <ref> [22] </ref>. 2 The IBM 801 used a similar scheme to manage transactions on units of less than a page in their case, for each 128-byte line [3]. 3 We have optimized the performance of global memory operations along the lines described in [21], hence our latencies are slightly better than those
Reference: [23] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedingsof the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: Unfortunately, the latency characteristics of high-speed networks are at odds with this trend. Recent research has succeeded in substantially minimizing the latency caused by operating system software for network transfers <ref> [23, 21] </ref>, and newer controllers reduce latency even further [8]. Therefore, the total latency for remote memory transfers is dictated to an increasing extent by the size of the transfer, rather than by the controller and software overhead.
References-found: 23

