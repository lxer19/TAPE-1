URL: http://www.research.att.com/~singer/papers/mmap.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Email: singer@research.att.com  
Title: Adaptive Mixtures of Probabilistic Transducers  
Author: Yoram Singer 
Address: 600 Mountain Ave., Room 2A-407 Murray Hill, NJ 07974  
Affiliation: AT&T Laboratories  
Abstract: We describe and analyze a mixture model for supervised learning of probabilistic transducers. We devise an on-line learning algorithm that efficiently infers the structure and estimates the parameters of each probabilistic transducer in the mixture. Theoretical analysis and comparative simulations indicate that the learning algorithm tracks the best transducer from an arbitrarily large (possibly infinite) pool of models. We also present an application of the model for inducing a noun phrase recognizer.
Abstract-found: 1
Intro-found: 1
Reference: <author> Y. Bengio and P. Fransconi. </author> <title> An input/output HMM architecture. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: <author> N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M. K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year>
Reference: <author> K. W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year>
Reference-contexts: Then, noun phrases are identified by manually defined regular expression patterns that are matched against the part-of-speech sequences. An alternative approach is to learn a mapping from the part-of-speech sequence to an output sequence that identifies the words that belong to a noun phrase <ref> ( Church, 1988 ) </ref> . We followed the second approach by building a suffix tree transducer based on a labeled data set from the Penn Tree Bank corpus.
Reference: <author> T.M. Cover and J.A. Thomas. </author> <title> Elements of information theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year> <title> 14 A. DeSantis, </title> <editor> G. Markowski, </editor> <title> and M.N. Wegman. Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the First Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <year> 1988. </year>
Reference-contexts: log 2 (n s ) : (41) X (m s 1) log 2 (n s ) = s X (m s 1)log 2 (m s 1) X (m s 1) log 2 (n s =(m s 1)) + m (T 0 ) log 2 (K) ; Using the log-sum inequality <ref> ( Cover and Thomas, 1991 ) </ref> we get, X (m s 1) log 2 (n s =(m s 1)) X m s log 2 s n s s (m s 1) = m (T 0 ) log 2 n Therefore, Loss mix n Loss n (T 0 ) log 2 (P
Reference: <author> C.L. Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference: <author> D. Haussler and A. Barron. </author> <title> How well do Bayes methods work for on-line prediction of f+1; 1g values ? In The 3rd NEC Symposium on Computation and Cognition, </title> <year> 1993. </year>
Reference: <author> D.P. Helmbold and R.E. Schapire. </author> <title> Predicting nearly as well as the best pruning of a decision tree. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 61-68, </pages> <year> 1995. </year>
Reference: <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, and G.E. Hinton. </author> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: <author> R.E. Krichevsky and V.K. Trofimov. </author> <title> The performance of universal encoding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27 </volume> <pages> 199-207, </pages> <year> 1981. </year>
Reference: <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference: <author> M.D. Riley. </author> <title> A statistical model for generating pronounication networks. </title> <booktitle> In Proc. of IEEE Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 737-740, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, in many real problems only a small subset of the full output alphabet is observed in a given context (a node in the tree). For example, when mapping phonemes to phones <ref> ( Riley, 1991 ) </ref> , for a given sequence of input phonemes the possible phones that can be pronounced is limited to a few possibilities (usually about two to four).
Reference: <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia: Learning probabilistic automata with variable memory length. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> (to appear). </note>
Reference: <author> F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. </author> <title> The context tree weighting method: Basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <year> 1995. </year> <month> 15 </month>
Reference-contexts: Direct calculation of the mixture probability is infeasible since there might be exponentially many such sub-trees. However, the technique introduced in <ref> ( Willems et al., 1995 ) </ref> can be generalized and applied to our setting. Let T 0 be a sub-tree of T.
References-found: 13

