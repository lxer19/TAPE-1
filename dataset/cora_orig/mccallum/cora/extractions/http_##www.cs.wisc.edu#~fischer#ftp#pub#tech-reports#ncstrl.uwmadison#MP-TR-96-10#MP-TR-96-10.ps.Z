URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-10/MP-TR-96-10.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-10/
Root-URL: http://www.cs.wisc.edu
Title: Solution of General Linear Complementarity Problems via Nondifferentiable Concave Minimization  
Author: O. L. Mangasarian 
Note: Dedicated to Professor Hoang Tuy on the Occasion of His Seventieth Birthday  
Abstract: Finite termination, at point satisfying the minimum principle necessary optimality condition, is established for a stepless (no line search) successive linearization algorithm (SLA) for minimizing a nondifferentiable concave function on a polyhedral set. The SLA is then applied to the general linear complementarity problem (LCP), formulated as minimizing a piecewise-linear concave error function on the usual polyhedral feasible region defining the LCP. When the feasible region is nonempty, the concave error function always has a global minimum at a vertex, and the minimum is zero if and only if the LCP is solvable. The SLA terminates at a solution or stationary point of the problem in a finite number of steps. A special case of the proposed algorithm [8] solved without failure 80 consecutive cases of the LCP formulation of the knapsack feasibilty problem, ranging in size between 10 and 3000. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <type> Technical Report 95-21, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> December </month> <year> 1995. </year> <note> INFORMS Journal on Computing, submitted. Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: We now state and establish finite termination of stepless successive linearization algorithm, which is an extension of an algorithm of [9] to nondifferentiable concave functions that is also very effective for the solution of machine learning problems <ref> [1, 2] </ref>. <p> The encouraging computational results of special cases of this algorithm applied to a knapsack LCP, as well to machine learning problems such as misclassification minimization [9], feature selection <ref> [1] </ref> and clustering [2], lead us to suggest that the proposed SLA 1 is a potential tool for solving important classes of difficult problems that are appropriately formulated as concave minimzation problems on polyhedral sets.
Reference: [2] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 368-374, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </publisher>
Reference-contexts: We now state and establish finite termination of stepless successive linearization algorithm, which is an extension of an algorithm of [9] to nondifferentiable concave functions that is also very effective for the solution of machine learning problems <ref> [1, 2] </ref>. <p> The encouraging computational results of special cases of this algorithm applied to a knapsack LCP, as well to machine learning problems such as misclassification minimization [9], feature selection [1] and clustering <ref> [2] </ref>, lead us to suggest that the proposed SLA 1 is a potential tool for solving important classes of difficult problems that are appropriately formulated as concave minimzation problems on polyhedral sets.
Reference: [3] <author> S.-J. Chung. </author> <title> NP-completeness of the linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 60 </volume> <pages> 393-399, </pages> <year> 1989. </year>
Reference-contexts: This NP-complete problem <ref> [3] </ref>, which may not have a solution, is easily shown to be equivalent to the following minimization of a piecewise-linear concave function on the polyhedral set defining the LCP [7, Lemma 1] 0 = min fe 0 (x (x M x q) + ) fi fi fi fi where e is
Reference: [4] <author> R. W. Cottle and G. Dantzig. </author> <title> Complementary pivot theory of mathematical programming. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 1 </volume> <pages> 103-125, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction We consider the classical linear complementarity problem (LCP) <ref> [4, 12, 5] </ref> 0 x ? M x + q 0; (1) where ? denotes orthogonality, and no assumptions are made on the n fi n real matrix M or the n fi 1 real vector q defining the problem.
Reference: [5] <author> R. W. Cottle, J.-S. Pang, and R. E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction We consider the classical linear complementarity problem (LCP) <ref> [4, 12, 5] </ref> 0 x ? M x + q 0; (1) where ? denotes orthogonality, and no assumptions are made on the n fi n real matrix M or the n fi 1 real vector q defining the problem.
Reference: [6] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 43-54, </pages> <year> 1992. </year>
Reference-contexts: material is based on research supported by National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu 1 and a global error bound for an LCP with positive definite M , or more generally for M 2 R 0 <ref> [15, 6, 13, 10] </ref>. (The class R 0 is the class of matrices M for which 0 is the unique solution to the homogeneous LCP: M x 0; x 0; x 0 M x = 0.) It seems natural, then, to base an algorithm on attempting to drive this residual to
Reference: [7] <author> O. L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: This NP-complete problem [3], which may not have a solution, is easily shown to be equivalent to the following minimization of a piecewise-linear concave function on the polyhedral set defining the LCP <ref> [7, Lemma 1] </ref> 0 = min fe 0 (x (x M x q) + ) fi fi fi fi where e is a column vector of ones and z + denotes the component-wise maximum of z i and 0 for a vector z.
Reference: [8] <author> O. L. Mangasarian. </author> <title> The linear complementarity problem as a separable bilinear program. </title> <journal> Journal of Global Optimization, </journal> <volume> 6 </volume> <pages> 153-161, </pages> <year> 1995. </year>
Reference-contexts: In Section 3 we apply the algorithm to the general LCP via the formulation (3) and indicate its computational effectiveness by citing a specific instance <ref> [8] </ref> of successfully solving the knapsack feasibility problem as an LCP. Section 3 concludes the paper. A word about our notation and background material. <p> We note that the bilinear algorithm of <ref> [8] </ref> for solving the knapsack feasibility problem as an LCP can be interpreted as a special case of Algorithm 4 with a fixed = 0. That bilinear algorithm solved 80 consecutive instances of the knapsack LCP ranging in size between 10 and 3000 without failure.
Reference: [9] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Hei-delberg, </address> <year> 1996. </year> <note> Available by ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: For such a problem, if f is bounded below on X, problem (5) has a vertex solution [16, Corollary 32.3.4]. We now state and establish finite termination of stepless successive linearization algorithm, which is an extension of an algorithm of <ref> [9] </ref> to nondifferentiable concave functions that is also very effective for the solution of machine learning problems [1, 2]. <p> The encouraging computational results of special cases of this algorithm applied to a knapsack LCP, as well to machine learning problems such as misclassification minimization <ref> [9] </ref>, feature selection [1] and clustering [2], lead us to suggest that the proposed SLA 1 is a potential tool for solving important classes of difficult problems that are appropriately formulated as concave minimzation problems on polyhedral sets.
Reference: [10] <author> O. L. Mangasarian and J. Ren. </author> <title> New improved error bounds for the linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 241-255, </pages> <year> 1994. </year>
Reference-contexts: material is based on research supported by National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu 1 and a global error bound for an LCP with positive definite M , or more generally for M 2 R 0 <ref> [15, 6, 13, 10] </ref>. (The class R 0 is the class of matrices M for which 0 is the unique solution to the homogeneous LCP: M x 0; x 0; x 0 M x = 0.) It seems natural, then, to base an algorithm on attempting to drive this residual to
Reference: [11] <author> MathWorks, Inc. </author> <title> PRO-MATLAB for UNIX Computers. The MathWorks, </title> <publisher> Inc., </publisher> <address> South Natick, MA 01760, </address> <year> 1991. </year>
Reference-contexts: The feasible region of the LCP (1) is the set fxjM x + q 0; x 0g: The scalar product of two vectors x and y in the n-dimensional real space will be denoted by x 0 y in conformity with MATLAB <ref> [11] </ref> notation . For a linear program min c 0 x with a vertex solution, the notation arg vertex min x2X will denote the set of vertex solutions of the linear program.
Reference: [12] <author> K. G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman--Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction We consider the classical linear complementarity problem (LCP) <ref> [4, 12, 5] </ref> 0 x ? M x + q 0; (1) where ? denotes orthogonality, and no assumptions are made on the n fi n real matrix M or the n fi 1 real vector q defining the problem.
Reference: [13] <author> J.-S. Pang. </author> <title> Inexact Newton methods for the nonlinear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 36(1) </volume> <pages> 54-71, </pages> <year> 1986. </year>
Reference-contexts: material is based on research supported by National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu 1 and a global error bound for an LCP with positive definite M , or more generally for M 2 R 0 <ref> [15, 6, 13, 10] </ref>. (The class R 0 is the class of matrices M for which 0 is the unique solution to the homogeneous LCP: M x 0; x 0; x 0 M x = 0.) It seems natural, then, to base an algorithm on attempting to drive this residual to
Reference: [14] <author> B. T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonemepty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [14, 16] </ref>. 2 The Concave Minimization Algorithm We consider in this section the following problem: min f (x); (5) where f : R n ! R is a concave function on R n and X is a polyhedral set in R n that does not contain lines going to infinity in <p> Noting that &gt; 0, we immediately have the desired minimum principle (7). We note that the minimum principle is usually given for convex minimization problems <ref> [14, Theorem 3, p. 203] </ref>, [16, Theorem 27.4], and not for a concave minimization problem like the one under consideration here. Also, the proofs are completely different for the convex case, with the above proof being much simpler.
Reference: [15] <author> S. M. Robinson. </author> <title> Some continuity properties of polyhedral multifunctions. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 206-214, </pages> <year> 1981. </year>
Reference-contexts: material is based on research supported by National Science Foundation Grant CCR-9322479. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu 1 and a global error bound for an LCP with positive definite M , or more generally for M 2 R 0 <ref> [15, 6, 13, 10] </ref>. (The class R 0 is the class of matrices M for which 0 is the unique solution to the homogeneous LCP: M x 0; x 0; x 0 M x = 0.) It seems natural, then, to base an algorithm on attempting to drive this residual to
Reference: [16] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year> <month> 6 </month>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonemepty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [14, 16] </ref>. 2 The Concave Minimization Algorithm We consider in this section the following problem: min f (x); (5) where f : R n ! R is a concave function on R n and X is a polyhedral set in R n that does not contain lines going to infinity in <p> For such a problem, if f is bounded below on X, problem (5) has a vertex solution <ref> [16, Corollary 32.3.4] </ref>. We now state and establish finite termination of stepless successive linearization algorithm, which is an extension of an algorithm of [9] to nondifferentiable concave functions that is also very effective for the solution of machine learning problems [1, 2]. <p> Noting that &gt; 0, we immediately have the desired minimum principle (7). We note that the minimum principle is usually given for convex minimization problems [14, Theorem 3, p. 203], <ref> [16, Theorem 27.4] </ref>, and not for a concave minimization problem like the one under consideration here. Also, the proofs are completely different for the convex case, with the above proof being much simpler. We are ready now to derive our finite termination result for the SLA 1.
References-found: 16

