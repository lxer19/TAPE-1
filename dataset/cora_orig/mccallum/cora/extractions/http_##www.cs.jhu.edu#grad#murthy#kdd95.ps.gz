URL: http://www.cs.jhu.edu/grad/murthy/kdd95.ps.gz
Refering-URL: http://fas.sfu.ca/cs/people/GradStudents/melli/SCDS/intro.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lastname@cs.jhu.edu  
Title: Decision Tree Induction: How Effective is the Greedy Heuristic?  
Author: Sreerama Murthy and Steven Salzberg 
Address: Baltimore, Maryland 21218  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract: Most existing decision tree systems use a greedy approach to induce trees | locally optimal splits are induced at every node of the tree. Although the greedy approach is suboptimal, it is believed to produce reasonably good trees. In the current work, we attempt to verify this belief. We quantify the goodness of greedy tree induction empirically, using the popular decision tree algorithms, C4.5 and CART. We induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees, which in turn are found using a novel map coloring idea. We measure the effect on greedy induction of variables such as the underlying concept complexity, training set size, noise and dimensionality. Our experiments show, among other things, that the expected classification cost of a greedily induced tree is consistently very close to that of the optimal tree. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: we expect to perform? An alternative way of asking the same question is, what is the penalty that decision tree algorithms pay in return for the speed gained by the greedy heuristic? Setting up the Experiments Our experimental framework is quite simple | we use C4.5 (Quinlan 1993) and CART <ref> (Breiman et al. 1984) </ref> to induce decision trees on a large number of random data sets, and in each case we compare the greedily induced tree to the optimal tree. The implementation of this framework raises some interesting issues. Optimal Decision Tree for a Training Set. <p> Tree Induction Methods Used. The tree induction methods we use are C4.5 (Quinlan 1993) and CART <ref> (Breiman et al. 1984) </ref>. One main difference between C4.5 and CART is the goodness criterion, the criterion used to choose the best split at each node. C4.5 uses the information gain 1 criterion, whereas CART uses either the Gini index of diversity or the twoing rule. <p> In the experiments in which the training data is noise-free, no pruning was used with either method. In the experiments using noisy training sets, we augment both methods with cost complexity pruning <ref> (Breiman et al. 1984) </ref>, reserving 10% of the training data for pruning. Experiments This section describes five experiments, each of which is intended to measure the effectiveness of greedy induction as a function of one or more control variables described earlier. <p> Goodman and Smyth (1988) argued, by establishing the equivalence of decision tree induction and a form of Shannon-Fano prefix coding, that the average depth of trees induced by greedy one-pass (i.e., no pruning) algorithms is nearly optimal. * Cost complexity pruning <ref> (Breiman et al. 1984) </ref> dealt effectively with both attribute and class noise. However, the accuracies on the training set were overly optimistic in the presence of attribute noise. * Greedily induced trees became less accurate as the concepts became harder, i.e., as the optimal tree size increased.
Reference: <author> Cox, L. A.; Qiu, Y.; and Kuehner, W. </author> <year> 1989. </year> <title> Heuristic least-cost computation of discrete classification functions with uncertain argument values. </title> <journal> Annals of Operations Research 21(1) </journal> <pages> 1-30. </pages>
Reference-contexts: The problem of computing the shallowest or smallest decision tree for a given data set is NP-complete (Hyafil & Rivest 1976; Murphy & McCraw 1991), meaning that it is highly unlikely that a polynomial solution will be found. Previous studies that attempted comparisons to optimal trees (e.g., <ref> (Cox, Qiu, & Kuehner 1989) </ref>) used approaches like dynamic programming to generate the optimal trees. Because it is slow, this option is impractical for our study, in which we use hundreds of thousands of artificial data sets.
Reference: <author> Dietterich, T. G., and Kong, E. B. </author> <year> 1995. </year> <title> Machine learning bias, statistical bias and statistical variance of decision tree algorithms. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference. </booktitle> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <note> to appear. </note>
Reference: <author> Fukanaga, K., and Hayes, R. A. </author> <year> 1989. </year> <title> Effect of sample size in classifier design. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 11 </journal> <pages> 873-885. </pages>
Reference-contexts: Several authors (e.g., <ref> (Fukanaga & Hayes 1989) </ref>) have argued that for a finite sized data with no a priori probabilistic information, the ratio of training sample size to the dimensionality must be as large as possible. Our results are consistent with these studies.
Reference: <author> Garey, M. R., and Graham, R. L. </author> <year> 1974. </year> <title> Performance bounds on the splitting algorithm for binary testing. </title> <journal> Acta Informatica 3(Fasc. </journal> 4):347-355. 
Reference: <author> Goodman, R. M., and Smyth, P. J. </author> <year> 1988. </year> <title> Decision tree design from a communication theory standpoint. </title> <journal> IEEE Transactions on Information Theory 34(5) </journal> <pages> 979-994. </pages>
Reference-contexts: The choice of a "best" test is what makes this algorithm greedy. The best test at a given internal node of the tree is only a locally optimal choice; and a strategy choosing locally optimal splits necessarily produces suboptimal trees <ref> (Goodman & Smyth 1988) </ref>. Optimality of a decision tree may be measured in terms of prediction accuracy, size or depth. It should be clear that it is desirable to build optimal trees in terms of one or more of these criteria.
Reference: <author> Hyafil, L., and Rivest, R. L. </author> <year> 1976. </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters 5(1) </journal> <pages> 15-17. </pages>
Reference: <author> Mingers, J. </author> <year> 1989. </year> <title> An empirical comparison of selection measures for decision tree induction. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 319-342. </pages>
Reference: <author> Murphy, O. J., and McCraw, R. L. </author> <year> 1991. </year> <title> Designing storage efficient decision trees. </title> <journal> IEEE Transactions on Computers 40(3) </journal> <pages> 315-319. </pages>
Reference: <author> Murthy, S. K.; Kasif, S.; and Salzberg, S. </author> <year> 1994. </year> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research 2 </journal> <pages> 1-33. </pages>
Reference-contexts: For brevity, we report only the results with information gain (i.e., C4.5) in the rest of this paper. We implemented all the goodness measures using the OC1 system <ref> (Murthy, Kasif, & Salzberg 1994) </ref>. Although C4.5 and CART differ in respects other than the goodness measures, we have not implemented these differences. In the experiments in which the training data is noise-free, no pruning was used with either method.
Reference: <author> Murthy, S. K. </author> <year> 1995. </year> <title> Data exploration using decision trees: A survey. </title> <note> In preparation. http://www.cs.jhu.edu/grad/murthy. </note>
Reference-contexts: The quantity that does increase with increasing dimensionality is the variance. Both prediction accuracy and tree size fluctuate significantly more in higher dimensions than in the plane. This result suggests that methods that help decrease variance, such as combining the classifications of multiple decision trees (see <ref> (Murthy 1995) </ref> for a survey), may be useful in higher dimensions. Discussion and Conclusions In this paper, we presented five experiments for evaluating the effectiveness of the greedy heuristic for decision tree induction. <p> Many researchers have studied ways to improve upon greedy induction, by using techniques such as limited lookahead search and more elaborate classifier representations (e.g., decision graphs instead of trees). (See <ref> (Murthy 1995) </ref> for a survey.) The results in the current paper throw light on why it might be difficult to improve upon the simple greedy algorithm for decision tree induction.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: how much better should we expect to perform? An alternative way of asking the same question is, what is the penalty that decision tree algorithms pay in return for the speed gained by the greedy heuristic? Setting up the Experiments Our experimental framework is quite simple | we use C4.5 <ref> (Quinlan 1993) </ref> and CART (Breiman et al. 1984) to induce decision trees on a large number of random data sets, and in each case we compare the greedily induced tree to the optimal tree. The implementation of this framework raises some interesting issues. Optimal Decision Tree for a Training Set. <p> Tree Induction Methods Used. The tree induction methods we use are C4.5 <ref> (Quinlan 1993) </ref> and CART (Breiman et al. 1984). One main difference between C4.5 and CART is the goodness criterion, the criterion used to choose the best split at each node. C4.5 uses the information gain 1 criterion, whereas CART uses either the Gini index of diversity or the twoing rule.
References-found: 12

