URL: http://www.cse.ogi.edu/~denni/Publications/energy.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Email: mattias@thep.lu.se  carsten@thep.lu.se  pihong@thep.lu.se  denni@thep.lu.se  bs@thep.lu.se  
Phone: 2  3  4  5  
Title: LU TP 93-24 Predicting System Loads with Artificial Neural  
Author: Ohlsson Carsten Peterson Hong Pi Thorsteinn Rognvaldsson and Bo Soderberg 
Note: Mattias  To appear in the 1994 Annual Proceedings of the American Society of Heating, Refrigerating and Air-Conditioning Engineers, Inc. 1  
Date: September 1993  
Abstract: Networks Methods and Results from Abstract: We devise a feed-forward Artificial Neural Network (ANN) procedure for predicting utility loads and present the resulting predictions for two test problems given by "The Great Energy Predictor Shootout The First Building Data Analysis and Prediction Competition" [1]. Key ingredients in our approach are a method (ffi -test) for determining relevant inputs and the Multilayer Perceptron. These methods are briefly reviewed together with comments on alternative schemes like fitting to polynomials and the use of recurrent networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.F. Kreider and J.S. Haberl, </author> <title> "The Great Energy Predictor Shootout The First Building Data Analysis and Prediction Competition", this volume. </title>
Reference-contexts: 1 Introduction This paper concerns methods and results in making predictions for two test problems (A and B) given by "The Great Energy Predictor Shootout The First Building Data Analysis and Prediction Competition" <ref> [1] </ref>. The present approach scored # 1 on set B and # 2 on set A. Both problems were given to the contestants in terms of tables of historic data (dependent and independent variables). In approaching the two data sets a few different strategies were explored. <p> The last 1282 time steps make up the test set, in which the independent variables ~x (t) are given whereas the energy consumptions ~y (t) were withheld by the organizers <ref> [1] </ref>. We thus have at our disposal the data patterns [1,2926] that can be used to train the networks. The accuracy in predicting the unknowns in the test set [2927,4208] is used by the organizers to score the generalization performances. <p> All input values are normalized to the interval <ref> [0; 1] </ref> and the output value is scaled by a factor of 1300. Initial weights are randomly distributed in the interval [0:3; 0:3]. During learning weights are updated after presentation of every 10 randomly selected patterns. All the calculations have been done using the F77 package JETNET 2.0 [6]. <p> We hence conclude that our prediction is good enough. 6 Summary We have approached the two data sets provided in "The Great Energy Predictor Shootout The First Building Data Analysis and Prediction Competition" <ref> [1] </ref> with an almost "black-box" procedure based upon * The ffi-test for establishing dependencies and gauging network performance. * A Multilayer Perceptron (MLP) [3] for modeling historic data. When selecting the appropriate ANN architecture and learning algorithm the choices are a standard MLP and/or a recurrent network [5].
Reference: [2] <author> H. Pi and C. Peterson, </author> <title> "Finding the Embedding Dimension and Variable Dependencies in Time Series", </title> <note> LU TP 93-4 (to appear in Neural Computation). 17 </note>
Reference-contexts: In approaching the two data sets a few different strategies were explored. The most powerful approach for these applications turned out to be based on the following two key ingredients: * Prior determination of dependencies in the data by using the ffi-test <ref> [2] </ref>. * Feedforward Artificial Neural Networks (ANN) for model building (system iden tification). This approach is of "black box" nature. However, it is stressed that prior "expert" knowledge about holidays etc. is essential for good performance. This paper is organized as follows. <p> Existing methods for doing this are based either on linear regression, which limits the analysis to linear dependencies, or on trial-and-error procedures. The ffi-test <ref> [2] </ref>, to be briefly described below, aims at determining any dependency, be it linear or nonlinear, assuming an underlying continuous function. Assume that we have sequences of measurements on a dependent variable z 0 and a set of independent variables z 1 , z 2 ,...z m . <p> How does the probability structure of the dependent variable behave with respect to the independent variables? In ref. <ref> [2] </ref> the following important observations were made: 1. <p> Results from applying the ffi-test <ref> [2] </ref> on this data set are shown in fig. 8. The y variable has dependencies on the four angled measures of solar flux as well as on the hour, as seen in fig. 8a.
Reference: [3] <editor> D. E. Rumelhart and J. L. McClelland (eds.) </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. </booktitle> <volume> 1), </volume> <publisher> MIT Press (1986). </publisher>
Reference-contexts: Recently also system identification problems have been approached with these nonlinear techniques. The aim is to realize a function mapping F i from the input values x k to the output values y i . For the so-called Multilayer Perceptron (MLP) <ref> [3] </ref> a particular form of the function F i is chosen y i = F i (x 1 ; x 2 ; :::) = g @ j ij g ( k 1 which corresponds to the feed-forward architecture of fig. 2. <p> is good enough. 6 Summary We have approached the two data sets provided in "The Great Energy Predictor Shootout The First Building Data Analysis and Prediction Competition" [1] with an almost "black-box" procedure based upon * The ffi-test for establishing dependencies and gauging network performance. * A Multilayer Perceptron (MLP) <ref> [3] </ref> for modeling historic data. When selecting the appropriate ANN architecture and learning algorithm the choices are a standard MLP and/or a recurrent network [5]. The former requires preprocessing in terms of choosing appropriately time-lagged inputs whereas the latter approach is supposed to select the relevant time-lags dynamically.
Reference: [4] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, Ca. </address> <year> (1991). </year>
Reference-contexts: This option cuts down the loss of statistics due to unnecessarily restrictive conditions. 3 Feedforward ANN for System Identification Feedforward ANN have turned out to be a very powerful approach for classification problems. A general introduction to the subject can be found in ref. <ref> [4] </ref>. Recently also system identification problems have been approached with these nonlinear techniques. The aim is to realize a function mapping F i from the input values x k to the output values y i .
Reference: [5] <author> R.J. Williams and D. Zipser, </author> <title> "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks", </title> <booktitle> Neural Computation 1, </booktitle> <month> 270 </month> <year> (1989). </year>
Reference-contexts: This is most easily done by monitoring the error on a validation set (a subset of the training data which is not used in the training) and stop the training when this error stops decreasing. An alternative method is to use a recurrent network <ref> [5] </ref> that is capable of building an internal memory of time lagged states by using feedback structures. However, the exact nature of these time lagged states is difficult to analyze and there is no evidence that those states always provide optimum time lags for solving the problems at hand. <p> When selecting the appropriate ANN architecture and learning algorithm the choices are a standard MLP and/or a recurrent network <ref> [5] </ref>. The former requires preprocessing in terms of choosing appropriately time-lagged inputs whereas the latter approach is supposed to select the relevant time-lags dynamically. With the ffi-test in our hands the appropriate time-lags can be efficiently selected for MLP processing.
Reference: [6] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Pattern Recognition in High Energy Physics with Artificial Neural Networks - JETNET 2.0", </title> <journal> Computer Physics Communications 70, </journal> <month> 167 </month> <year> (1992). </year>
Reference-contexts: Standard backpropagation, as implemented in JETNET 2.0 <ref> [6] </ref>, is used with initial weights randomly distributed in the interval [-0.1,0.1]. The learning rate and momentum term are set to 0.1 and 0.0 respectively. <p> Initial weights are randomly distributed in the interval [0:3; 0:3]. During learning weights are updated after presentation of every 10 randomly selected patterns. All the calculations have been done using the F77 package JETNET 2.0 <ref> [6] </ref>.
Reference: [7] <author> T. Rognvaldsson, </author> <title> "On Langevin Updating in Multilayer Perceptrons", </title> <note> LU TP 93-13 (to appear in Neural Computation). 18 </note>
Reference-contexts: Initial weights are randomly distributed in the interval [0:3; 0:3]. During learning weights are updated after presentation of every 10 randomly selected patterns. All the calculations have been done using the F77 package JETNET 2.0 [6]. A Langevin updating scheme (see e.g. ref <ref> [7] </ref>) was used as it often performs better than backpropagation. 5.4 Results In fig. (9) a scatter plot is shown of true and predicted beam insolation for the training data, with a variation coefficient CV = 0:019 and a mean bias error MBE = 0:00032.
References-found: 7

