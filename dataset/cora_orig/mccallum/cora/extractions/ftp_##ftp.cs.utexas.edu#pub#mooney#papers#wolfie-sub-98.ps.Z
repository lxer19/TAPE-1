URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/wolfie-sub-98.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: cthomp@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Semantic Lexicon Acquisition for Learning Natural Language Interfaces  
Author: Cynthia A. Thompson and Raymond J. Mooney 
Keyword: Content areas: corpus-based methods, inductive learning, machine learning, natural language processing  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Abstract: This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with representations of their meaning. The lexicon learned consists of words paired with meaning representations. Wolfie is part of an integrated system that learns to parse novel sentences into semantic representations, such as logical database queries. Experimental results are presented demonstrating Wolfie's ability to learn useful lexicons for a database interface in four different natural languages. The lexicons learned by Wolfie are compared to those acquired by a competing system developed by Siskind (1996). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Borland International. </author> <year> 1988. </year> <title> Turbo Prolog 2.0 Reference Guide. </title> <address> Scotts Valley, CA: </address> <publisher> Borland International. </publisher>
Reference-contexts: The corpus contains 250 questions about U.S. geography paired with logical representations. This domain was chosen due to the availability of an existing hand-build natural language interface to a simple geography database containing about 800 facts. The original interface, Geobase, was supplied with Turbo Prolog 2:0 <ref> (Borland International 1988) </ref>. The questions were collected from uninformed undergraduates and mapped into logical form by an expert. Examples from the corpus were given in the previous sections. To broaden the test, we had the same 250 sentences translated into Spanish, Turkish, and Japanese.
Reference: <author> Brent, M. </author> <year> 1991. </year> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 209-214. </pages>
Reference: <author> Catizone, R.; Russell, G.; and Warwick, S. </author> <year> 1993. </year> <title> Deriving translation data from bilingual texts. </title> <booktitle> In Pro ceedings of the First International Lexical Acquisition Workshop. </booktitle>
Reference: <author> Cohn, D.; Atlas, L.; and Ladner, R. </author> <year> 1994. </year> <title> Improving generalization with active learning. </title> <booktitle> Machine Learning 15(2) </booktitle> <pages> 201-221. </pages>
Reference-contexts: A more important issue is lessening the burden of building a large annotated training corpus. We are exploring two options in this regard. One is to use active learning <ref> (Cohn, Atlas, & Ladner 1994) </ref> in which the system chooses which examples are most usefully anno tated from a larger corpus of unannotated data. This approach can dramatically reduce the amount of annotated data required to achieve a desired accuracy (En-gelson & Dagan 1996).
Reference: <author> Copestake, A.; Briscoe, T.; Vossen, P.; Ageno, A.; Castellon, I.; Ribas, F.; Rigan, G.; Rodrguez, H.; and Samiotou, A. </author> <year> 1995. </year> <title> Acquisition of lexical translation relations from MRDS. </title> <booktitle> Machine Translation 9. </booktitle>
Reference: <author> Engelson, S., and Dagan, I. </author> <year> 1996. </year> <title> Minimizing manual annotation cost in supervised training from corpora. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference: <author> Fukumoto, F., and Tsujii, J. </author> <year> 1995. </year> <title> Representation and acquisition of verbal polysemy. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> 39-44. </pages>
Reference: <author> Gale, W., and Churck, K. </author> <year> 1991. </year> <title> Identifying word correspondences in parallel texts. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop. </booktitle>
Reference: <author> Haruno, M. </author> <year> 1995. </year> <title> A case frame learning method for Japanese polysemous verbs. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> 45-50. </pages>
Reference: <author> Hastings, P., and Lytinen, S. </author> <year> 1994. </year> <title> The ups and downs of lexical acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 754-759. </pages>
Reference: <author> Johnston, M.; Boguraev, B.; and Pustejovsky, J. </author> <year> 1995. </year> <title> The acquisition and interpretation of complex nomi-nals. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> 69-74. </pages>
Reference: <author> Knight, K. </author> <year> 1996. </year> <title> Learning word meanings by instruction. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 447-454. </pages>
Reference: <author> Kohavi, R., and John, G. </author> <year> 1995. </year> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 304-312. </pages>
Reference-contexts: The first four components have smaller values than the last, so they have higher weights. Results are not overly-sensitive to the heuristic weights. Automatically setting the weights using cross-validation on the training set <ref> (Kohavi & John 1995) </ref> had little effect on overall performance. In all of the experiments, the weights were fixed at 50 for the first four parameters, and 8 for the last. To break ties, we choose less "ambiguous" phrases first and learn short phrases before longer ones.
Reference: <author> Kumano, A., and H, H. </author> <year> 1994. </year> <title> Building an MT dictionary from parallel texts based on linguistic and statistical information. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Computational Linguistics. </booktitle>
Reference: <author> Lavrac, N., and Dzeroski, S. </author> <year> 1994. </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Hor-wood. </publisher>
Reference: <author> Manning, C. D. </author> <year> 1993. </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 235-242. </pages>
Reference: <author> Melamed, I. </author> <year> 1995. </year> <title> Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora. </booktitle>
Reference: <author> Melamed, I. </author> <year> 1996. </year> <title> Automatic construction of clean broad-coverage translation lexicons. </title> <booktitle> In Second Conference of the Association for Machine Translation in the Americas. </booktitle>
Reference: <editor> Muggleton, S. H., ed. </editor> <booktitle> 1992. Inductive Logic Programming. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Pedersen, T., and Chen, W. </author> <year> 1995. </year> <title> Lexical acquisition via constraint solving. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <pages> 118-122. </pages>
Reference: <author> Plotkin, G. D. </author> <year> 1970. </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., and Michie, D., eds., </editor> <booktitle> Machine Intelligence (Vol. 5). </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: The second representation handled is the logical query one illustrated earlier, and is the focus of the current paper. To find the common substructure between pairs of query representations, we use a method similar to finding the Least General Generalization (LGG) of first-order clauses <ref> (Plotkin 1970) </ref>. However, instead of using subsumption to guide generalization, we find the set of largest common substructures that two representations share.
Reference: <author> Riloff, E., and Sheperd, K. </author> <year> 1997. </year> <title> A corpus-based approach for building semantic lexicons. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 117-124. </pages>
Reference-contexts: Ideally, we would like to minimize the size and ambiguity of the learned lexicon, since this should ease the parser acquisition task. Note that this notion of semantic lexicon acquisition is distinct from work on learning selectional restrictions (Manning 1993; Brent 1991) and learning clusters of semantically similar words <ref> (Riloff & Sheperd 1997) </ref>. Note that we allow phrases to have multiple meanings (homonymy) and for multiple phrases to have the same meaning (synonymy). Also, some phrases in the sentences may have a null meaning. We make only a few fairly straightforward assumptions about the input.
Reference: <author> Russell, D. </author> <year> 1993. </year> <title> Language Acquisition in a Unification-Based Grammar Processing System Using a Real World Knowledge Base. </title> <type> Ph.D. Dissertation, </type> <institution> University of Illinois, Urbana, IL. </institution>
Reference: <author> Schank, R. C. </author> <year> 1975. </year> <title> Conceptual Information Processing. </title> <publisher> Oxford: North-Holland. </publisher>
Reference-contexts: In the current implementation, phrases are limited to at most two words. The Wolfie algorithm, outlined in Figure 2, has been implemented to handle two kinds of semantic representations. One is a case-role meaning representation based on conceptual dependency <ref> (Schank 1975) </ref>. For example, the sentence "The man ate the cheese" is represented by: [ingest, agent:[person, sex:male, age:adult], patient:[food, type:cheese]]. The second representation handled is the logical query one illustrated earlier, and is the focus of the current paper.
Reference: <author> Siskind, J. M. </author> <year> 1992. </year> <title> Naive Physics, Event Perception, Lexical Semantics and Language Acquisition. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: The Semantic Lexicon Acquisition Problem Given a set of sentences, each consisting of an ordered list of words and annotated with a single semantic representation, we assume that each representation can be fractured into all of its components <ref> (Siskind 1992) </ref>. The fracturing method depends upon the given representation and must be explicitly provided or implicit in the algorithm that forms hypotheses for word meanings. Given a valid set of components, they can be constructed into a valid sentence meaning using a relation we will call compose.
Reference: <author> Siskind, J. M. </author> <year> 1994. </year> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 760-766. </pages>
Reference: <author> Siskind, J. M. </author> <year> 1996. </year> <title> A computational study of cross-situational techniques for learning word-to-meaning mappings. </title> <journal> Cognition 61(1) </journal> <pages> 39-91. </pages>
Reference: <author> Tishby, N., and Gorin, A. </author> <year> 1994. </year> <title> Algebraic learning of statistical associations for language acquisition. </title> <booktitle> Computer Speech and Language 8 </booktitle> <pages> 51-78. </pages>
Reference-contexts: Manning (1993) and Brent (1991) acquire subcategorization information for verbs. Finally, several systems (Knight 1996; Hastings & Lytinen 1994; Russell 1993) learn new words from context, assuming that a large initial lexicon and parsing system are available. Tishby and Gorin <ref> (Tishby & Gorin 1994) </ref> learn associations between words and actions (as meanings of those words). Their system was tested on a corpus of sentences paired with representations but they do not demonstrate the integration of learning a semantic parser using the learned lexicon.
Reference: <author> Walker, D., and Amsler, R. </author> <year> 1986. </year> <title> The use of machine-readable dictionaries in sublanguage analysis. </title> <editor> In Gr-ishman, R., and Kittredge, R., eds., </editor> <booktitle> Analyzing Language in Restricted Domains. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 69-83. </pages>
Reference: <author> Webster, M., and Marcus, M. </author> <year> 1995. </year> <title> Automatic acquisition of the lexical semantics of verbs from sentence frames. </title> <booktitle> In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference: <author> Wu, D., and Xia, X. </author> <year> 1995. </year> <title> Large-scale automatic extraction of an English-Chinese translation lexicon. </title> <journal> Machine Translation 9(3-4):285-313. </journal>
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1996. </year> <title> Learning to parse database queries using inductive logic programming. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Given a correct lexicon, the overly-general parser should be able to parse all of the training examples. In this paper, we limit our discussion of Chill to its ability to learn parsers that map natural-language questions directly into Prolog queries that can be executed to produce an answer <ref> (Zelle & Mooney 1996) </ref>. Following are two sample queries for a database on U.S. Geography paired with their corresponding Prolog query: What is the capital of the state with the biggest population? answer (C, (capital (S,C), largest (P, (state (S), population (S,P))))).
Reference: <author> Zelle, J. M. </author> <year> 1995. </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. Dissertation, </type> <institution> University of Texas, Austin, TX. </institution> <note> Also appears as Artificial Intelligence Laboratory Technical Report AI 96-249. </note>
Reference-contexts: Although a few others (Siskind 1996; Hastings & Lytinen 1994; Brent 1991) have presented systems for semantic lexical acquisition, this work is unique in combining several features. First, interaction with a system, Chill <ref> (Zelle 1995) </ref>, that learns to parse sentences into their semantic representations, is demonstrated. Second, it uses a fairly simple batch, greedy algorithm that is quite fast and accurate. Third, it is easily extendible to new representation formalisms. Finally, it is able to bootstrap from an existing lexicon.
References-found: 33

