URL: ftp://ilk.kub.nl/pub/antalb/clin-95.ps.gz
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: 
Email: fantal,weijtersg@cs.rulimburg.nl (ii) Walter.Daelemans@kub.nl  
Title: An Inductive-Learning Approach to Morphological Analysis  
Author: Antal van den Bosch (i) Walter Daelemans (ii) Ton Weijters (i) (i) 
Address: Netherlands  
Affiliation: Dept. of Computer Science matriks Computational Linguistics University of Limburg, The Netherlands Tilburg University, The  
Abstract: Morphological analysis is an important subtask in text-to-speech conversion, hyphenation, and other language engineering tasks. The traditional approach to performing morphological analysis is to combine a morpheme lexicon, sets of (linguistic) rules, and heuristics to find a most probable analysis. In contrast, we present an inductive learning approach in which morphological analysis is reformulated as a segmentation task. We report on a number of experiments in which five inductive learning algorithms are applied to three variations of the task of morphological analysis. Results show (i) that the gener-alisation performance of the algorithms is good, and (ii) that the lazy learning algorithm ib1-ig performs best on all three tasks. We conclude that lazy learning of morphological analysis as a classification task is indeed a viable approach; moreover, it has the strong advantages of avoiding the knowledge-acquisition bottleneck, being fast and deterministic in learning and processing, and being language-independent.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., & Albert, M. </author> <year> 1991. </year> <title> Instance-based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages>
Reference-contexts: First, we provide a brief summary of the inductive-learning algorithms used in the experiments reported in this paper. 1.2 Algorithms and methods for inductive learning Inductive learning in its most straightforward form is exhibited by memory-based lazy learning algorithms such as ib1 <ref> (Aha et al., 1991) </ref> and variations, e.g., ib1-ig (Daelemans & Van den Bosch, 1992; Daelemans et al., 1996), in which all instances are fully stored in memory, and in which classification involves a pass along all stored instances. <p> Examples of such algorithms are the decision-tree algorithms igtree (Daelemans et al., 1996) and c4.5 (Quinlan, 1993). Another popular in 3 ductive algorithm is the connectionist Back-propagation (bp) (Rumelhart et al., 1986) learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 <ref> (Aha et al., 1991) </ref> constructs a data base of instances (the instance base) during learning. An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification (s) of that particular feature-value vector.
Reference: <author> Allen, J., Hunnicutt, S., & Klatt, D. </author> <year> 1987. </year> <title> From Text to Speech: The mitalk System. </title> <address> Cam-bridge, UK: </address> <publisher> Cambidge University Press. </publisher>
Reference-contexts: 1 Introduction Morphological analysis is often deemed to be an important, if not essential subtask in linguistic modular systems for text-to-speech processing <ref> (Allen et al., 1987) </ref> and hyphenation (Daele-mans, 1989). In text-to-speech processing, it serves to prevent the incorrect application of grapheme-phoneme conversion rules across morpheme boundaries (e.g., preventing carelessly from being pronounced as /k'r~lslai/). <p> We briefly illustrate the functioning of this type of analysis by taking decomp's processing as an example approach, and the word scarcity as the example word <ref> (Allen et al., 1987) </ref>: 1. In a morpheme lexicon covering the English language, a first analysis divides scarcity into scar and city. 2. <p> A second characteristic of our representation of morphological boundaries, is that it is non-hierarchic. Although morpheme hierarchy may be important in determining the part-of-speech of a word <ref> (Allen et al., 1987) </ref>, it is not necessary to have a full hierarchical analysis when the morphological analysis is used as input to a text-to-speech system. 3 Experiments 3.1 Data collection and algorithmic parameters The source for the morphological data used in our experiments is celex (Burnage, 1990), a large lexical <p> Future work on inductive learning of morphological analysis should include a thorough performance comparison with existing traditional systems for morphological analysis, based on linguistic theory and heuristics such as decomp <ref> (Allen et al., 1987) </ref> as well as with probabilistic systems (Heemskerk, 1993).
Reference: <author> Burnage, G. </author> <year> 1990. </year> <title> celex: A Guide for Users. Centre for Lexical Information, </title> <address> Nijmegen. </address>
Reference-contexts: part-of-speech of a word (Allen et al., 1987), it is not necessary to have a full hierarchical analysis when the morphological analysis is used as input to a text-to-speech system. 3 Experiments 3.1 Data collection and algorithmic parameters The source for the morphological data used in our experiments is celex <ref> (Burnage, 1990) </ref>, a large lexical data base of English, Dutch, and German. <p> On the basis of the word token frequency information contained in celex, we computed that this lexicon covers approximately 65% of the word tokens in the 16,6 million word cobuild corpus of written text <ref> (Burnage, 1990) </ref>; 26% of the words in our corpus do not occur in the cobuild corpus. This 65,558-word lexicon was used to create instance bases for the m1, m2, and m3 tasks, each containing 573,544 instances.
Reference: <author> Daelemans, W. </author> <year> 1989. </year> <title> Automatic hyphenation: Linguistics versus engineering. Pages 347-364 of: </title> <editor> Heyvaert, F. J., & Steurs, F. (eds), </editor> <title> Worlds behind Words. Leuven: </title> <publisher> Leuven University Press. </publisher>
Reference: <author> Daelemans, W. </author> <year> 1995. </year> <title> Memory-based lexical acquisition and processing. Pages 85-98 of: </title> <editor> Stef-fens, P. (ed), </editor> <booktitle> Machine Translation and the Lexicon. Springer Lecture Notes in Artificial Intelligence, </booktitle> <volume> no. </volume> <pages> 898. </pages> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Most linguistic tasks can be described as classification tasks, i.e., given a description of an input in terms of a number of feature-values, a classification of the input is performed. Two types of classification tasks can be discerned <ref> (Daelemans, 1995) </ref>: * Identification: given a set of possible classifications and an input of feature values, determine the correct classification for this input.
Reference: <author> Daelemans, W., & Van den Bosch, A. </author> <year> 1992. </year> <title> Generalisation Performance of Backpropagation Learning on a Syllabification Task. Pages 27-37 of: </title> <editor> Drossaers, M. F. J., & Nijholt, A. (eds), twlt3: </editor> <booktitle> Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University. </institution> <note> 14 Daelemans, </note> <author> W., & Van den Bosch, A. </author> <year> 1994. </year> <title> A language-independent, data-oriented architec-ture for grapheme-to-phoneme conversion. </title> <booktitle> Pages 199-203 of: Proceedings of the Second esca/ieee Workshop on Speech Synthesis, </booktitle> <address> New York. esca/ieee. </address>
Reference-contexts: This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 (Quinlan, 1986) and c4.5 (Quinlan, 1993) <ref> (for more details, cf. Daelemans and Van den Bosch, 1992) </ref>. In short, the information gain of a feature expresses its relative relevance compared to the other features in performing the mapping from input to classification.
Reference: <author> Daelemans, W., Gillis, S., & Durieux, G. </author> <year> 1994. </year> <title> The acquisition of stress, a data-oriented approach. </title> <journal> Computational Linguistics, </journal> <volume> 20, </volume> <pages> 421-451. </pages>
Reference-contexts: memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification (Daelemans & Van den Bosch, 1992; Van den Bosch et al., 1995), and identification as in grapheme-phoneme conversion (Weijters, 1991; Van den Bosch & Daelemans, 1993; Daele-mans & Van den Bosch, 1994), and stress assignment <ref> (Daelemans et al., 1994) </ref>. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms.
Reference: <author> Daelemans, W., Van den Bosch, A., & Weijters, A. </author> <year> 1996. </year> <title> igtree: using trees for classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review. </journal> <note> To appear. </note>
Reference-contexts: To optimise memory lookup and minimise memory usage, more eager learning algorithms are available that compress the memory in such a way that most relevant knowledge is retained and stored in a quickly accessible form, and redundant knowledge is removed. Examples of such algorithms are the decision-tree algorithms igtree <ref> (Daelemans et al., 1996) </ref> and c4.5 (Quinlan, 1993). Another popular in 3 ductive algorithm is the connectionist Back-propagation (bp) (Rumelhart et al., 1986) learning algorithm. <p> When information gain is used as 4 the weighting function in the distance function (equation 1), instances that match on an important feature are regarded as less distant (more alike) than instances that match on an unimportant feature. 3. igtree <ref> (Daelemans et al., 1996) </ref> compresses an instance base into a decision tree. Instances are stored in the tree as paths of connected nodes and leaves contain classification information. Nodes are connected via arcs denoting feature values. <p> Interesting is the fact that igtree performs well on m1, but performs relatively badly on m2 and m3. igtree is known to perform worse when the information gain of the input features displays a low variance <ref> (Daelemans et al., 1996) </ref>, i.e., when there is little difference between the relative relevance of the input features. This suggests that the information-gain values of the features with tasks m2 and m3 have less outspoken differences than with m1, which is indeed the case, as is displayed in Figure 3.
Reference: <author> Heemskerk, J. S. </author> <year> 1993. </year> <title> A Probabilistic Context-free Grammar for Disambiguation in Morphological Parsing. </title> <booktitle> Pages 183-192 of: Proceedings of the 6th Conference of the eacl. </booktitle>
Reference-contexts: Morphological analysis on a probabilistic basis, using only a morpheme lexicon, an analyses generator, and a probabilistic function to determine the analysis with the highest probability <ref> (Heemskerk, 1993) </ref> does not suffer from the disadvantageous knowledge acquisition and fine-tuning phase, but is nevertheless also confronted with an explosion of the number of generated analyses. 2.2 Inductive-learning approach In contrast to this decomposition into three components, we reformulate the task of morphological analysis as a one-pass segmentation task, in <p> Future work on inductive learning of morphological analysis should include a thorough performance comparison with existing traditional systems for morphological analysis, based on linguistic theory and heuristics such as decomp (Allen et al., 1987) as well as with probabilistic systems <ref> (Heemskerk, 1993) </ref>. Secondly, we aim at integrating trained learning models of morphological analysis into larger systems, to investigate whether the enrichment of spelling input with morphological boundary information improves the generalisation performance of other learning systems trained on, e.g., stress assignment, grapheme-phoneme conversion, and part-of-speech prediction of unknown words.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-206. </pages>
Reference-contexts: This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 <ref> (Quinlan, 1986) </ref> and c4.5 (Quinlan, 1993) (for more details, cf. Daelemans and Van den Bosch, 1992). In short, the information gain of a feature expresses its relative relevance compared to the other features in performing the mapping from input to classification.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> c4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The memory-based learning approach therefore does not distinguish between regularities and individual exceptions; rule-like behavior automatically emerges from the interaction between the memory contents and the similarity metric used. In eager learning approaches (such as c4.5 <ref> (Quinlan, 1993) </ref> or connectionist learning), abstract data structures (matrices of connection weights in connectionist networks, decision trees in c4.5) are extracted from the learning material during learning. <p> Examples of such algorithms are the decision-tree algorithms igtree (Daelemans et al., 1996) and c4.5 <ref> (Quinlan, 1993) </ref>. Another popular in 3 ductive algorithm is the connectionist Back-propagation (bp) (Rumelhart et al., 1986) learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 (Aha et al., 1991) constructs a data base of instances (the instance base) during learning. <p> This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 (Quinlan, 1986) and c4.5 <ref> (Quinlan, 1993) </ref> (for more details, cf. Daelemans and Van den Bosch, 1992). In short, the information gain of a feature expresses its relative relevance compared to the other features in performing the mapping from input to classification. <p> For more details on igtree, see Daelemans et al. (1996). 4. c4.5 <ref> (Quinlan, 1993) </ref> is a well-known decision-tree algorithm which basically uses the same type of strategy as igtree to compress an instance base into a compact tree. To this purpose, standard c4.5 also uses information gain, or gain ratio (Quinlan, 1993) to select the most important feature in tree building; however, in <p> For more details on igtree, see Daelemans et al. (1996). 4. c4.5 <ref> (Quinlan, 1993) </ref> is a well-known decision-tree algorithm which basically uses the same type of strategy as igtree to compress an instance base into a compact tree. To this purpose, standard c4.5 also uses information gain, or gain ratio (Quinlan, 1993) to select the most important feature in tree building; however, in contrast to igtree, c4.5 recomputes this function for each node in the tree.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. Pages 318-362 of: </title> <editor> Rumelhart, D. E., & McClelland, J. L. (eds), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Examples of such algorithms are the decision-tree algorithms igtree (Daelemans et al., 1996) and c4.5 (Quinlan, 1993). Another popular in 3 ductive algorithm is the connectionist Back-propagation (bp) <ref> (Rumelhart et al., 1986) </ref> learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 (Aha et al., 1991) constructs a data base of instances (the instance base) during learning. <p> Another difference with igtree is that c4.5 implements a pruning stage, in which parts of the tree are removed as they are estimated to contribute to instance classification below a certain threshold. 5. bp <ref> (Rumelhart et al., 1986) </ref> is an artificial-neural-network learning rule, which operates on multi-layer feed-forward networks (mfns). In these networks, feature-values of instances are encoded as activation patterns in the input layer, and the network is trained to 5 produce an activation pattern at the output layer representing the desired classification.
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the acm, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference: <author> Van den Bosch, A., & Daelemans, W. </author> <year> 1993. </year> <title> Data-Oriented Methods for Grapheme-to-Phoneme Conversion. </title> <booktitle> Pages 45-53 of: Proceedings of the 6th Conference of the eacl. </booktitle>
Reference: <author> Van den Bosch, A., Weijters, A., Van den Herik, H. J., & Daelemans, W. </author> <year> 1995. </year> <title> The profit of learning exceptions. </title> <booktitle> Pages 118-126 of: Proceedings of the 5th Belgian-Dutch Conference on Machine Learning, </booktitle> <address> benelearn'95. </address>
Reference: <author> Van den Bosch, A., Daelemans, W., & Weijters, A. </author> <year> 1996. </year> <title> Morphological analysis as classification: an inductive-learning approach. </title> <booktitle> In: Proceedings of NeMLaP-2, New Methods in Language Processing, </booktitle> <institution> Ankara, Turkey. </institution> <note> To appear. </note>
Reference: <author> Weijters, A. </author> <year> 1991. </year> <title> A simple look-up procedure superior to nettalk? In: </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks - icann-91, </booktitle> <address> Espoo, Finland. </address>
Reference: <author> Weiss, S., & Kulikowski, C. </author> <year> 1991. </year> <title> Computer Systems That Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 15 </pages>
Reference-contexts: A method that gives a good estimate of the generalisation performance of an algorithm on a given instance base, is 10-fold cross-validation <ref> (Weiss & Kulikowski, 1991) </ref>. Using this method, 10 partitionings into a training set (90%) and a test set (10%) are generated on the basis of an instance base, leading to 10 experiments and 10 results per learning algorithm and instance base. <p> output units (classes are locally coded), a learning rate of 0.1, a momentum of 0.4, and an update tolerance of 0.2. igtree's functioning is not governed by parameters. 3.2 Results We applied the five algorithms to the three tasks, performing with each algorithm and each task a 10-fold cross-validation experiment <ref> (Weiss & Kulikowski, 1991) </ref>. We computed for each 10-fold cross-validation experiment the average percentage of incorrectly processed test words. A word is incorrectly processed when one or more instance classifications associated with the instances derived from the word are incorrect (i.e., when one or more of the segmentations is incorrect).
References-found: 18

