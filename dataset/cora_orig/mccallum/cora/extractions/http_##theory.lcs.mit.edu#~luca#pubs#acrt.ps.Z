URL: http://theory.lcs.mit.edu/~luca/pubs/acrt.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~luca/papers.html
Root-URL: 
Title: Weak Random Sources, Hitting Sets, and BPP Simulations time simulation of RP [SSZ95] and a
Author: Alexander E. Andreev Andrea E. F. Clementi Jose D. P. Rolim Luca Trevisan 
Note: nomial  
Abstract: We show how to simulate any BPP algorithm in polynomial time using a weak random source of min-entropy r fl for any fl &gt; 0. This follows from a more general result about sampling with weak random sources. Our result matches an information-theoretic lower bound and solves a question that has been open for some years. The previous best results were a poly Departing significantly from previous related works, we do not use extractors; instead, we use the OR-disperser of [SSZ95] in combination with a tricky use of hitting sets borrowed from [ACR96]. Of independent interest is our new (simplified) proof of the main result of [ACR96]. Our proof also gives some new hardness/randomness trade-offs for parallel classes. 
Abstract-found: 1
Intro-found: 1
Reference: [ACR96] <author> A.E. Andreev, A.E.F. Clementi, and J.D.P. </author> <title> Rolim. Hitting sets derandomize BPP. </title> <booktitle> In Proceedings of the 23rd International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 357-368. </pages> <publisher> LNCS 1099, Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Informally speaking, in the context of derandomization, pseudorandom generators play the same role of extractors and hitting sets generators play that of dispersers. A recent result of Andreev et al. <ref> [ACR96] </ref> shows how to deterministically simulate BPP algorithms using hitting set generators. This suggests that perhaps, dispersers could somewhat surprisingly be used to simulate BPP with weak random sources. <p> The probability is here computed with respect to the uniform distribution in f0; 1g n . The main result of <ref> [ACR96] </ref> can be formalized as follows. Theorem 1 ([ACR96]) For any choice of constants *; fl &gt; 0, there is a deterministic algorithm that, given access to a quick fl-HSG, and given in input any circuit C of size n returns in poly (n) time a value A such that jPr <p> Thus, proving the theorem amounts to find a set S that passes the test. This task is solved in <ref> [ACR96] </ref> by means of a rather involved (and inherently sequential) algorithm that goes over O (n log n) local change phases of an initial set (that is the output of the hitting set generator) that is stored in a compressed form. <p> The definition below is a slight variant of the definition of quick *-HSG of price O (log n) given in <ref> [ACR96] </ref>. Definition 10 (Hitting Set Generator) A quick *-HSG is a polynomial-time algorithm H that, given in input a number n in unary, returns a multiset H (n) f0; 1g that is *-hitting for the set ff : f0; 1g ! f0; 1g : L (f) ng. <p> S f0; 1g , define p (~a; f; S) = Pr ~x2S [f (~x ~a) = 1] : For any two subsets S; H f0; 1g n , constant * &gt; 0, and function f : f0; 1g n ! f0; 1g, we define the dis crepancy test introduced in <ref> [ACR96] </ref>. disc-test (f; S; H; *) begin p min := minfp (~a; f; S) : ~a 2 H [ f ~ 0gg; p max := maxfp (~a; f; S) : ~a 2 H [ f ~ 0gg; if p max p min * then return (1) else return (0) end Theorem <p> *) begin p min := minfp (~a; f; S) : ~a 2 H [ f ~ 0gg; p max := maxfp (~a; f; S) : ~a 2 H [ f ~ 0gg; if p max p min * then return (1) else return (0) end Theorem 12 (Soundness of disc-test <ref> [ACR96] </ref>) Constants c 0 and c 1 exist such that, for any * &gt; 0, integer n, function f : f0; 1g n ! f0; 1g, sets S; H f0; 1g , if disc-test (f; S; H; *) = 1 and one of the following conditions holds 1. <p> The sufficiency of condition (1) is used in this section to prove Theorem 1, while condition (2) is exploited in next section in order to prove Theorem 2. Theorem 12 is the core of the results of <ref> [ACR96] </ref>. Note that it says that a set H with a certain one-sided pseudorandom property (the hitting property) can be used to test S for a two-sided pseudorandom property (the discrepancy property).
Reference: [ACR97] <author> A.E. Andreev, A.E.F. Clementi, and J.D.P. </author> <title> Rolim. Worst-case hardness suffices for de-randomization: A new method for hardness vs randomness trade-offs. </title> <booktitle> In Proceedings of the 24th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 177-187. </pages> <publisher> LNCS 1256, Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: The algorithm is of independent interest, since it proves a somewhat stronger result than 1 In the next section we will give a seemingly weaker (but in fact equivalent) formal definition. Theorem 1 and has been also used in <ref> [ACR97] </ref>. For the sake of proving Theorem 1 it might however be over-kill. Our Results We show how to use dispersers and weak random sources to simulate BPP in polynomial time and to even solve a more general sampling problem. <p> Using this proof of Theorem 1 we note that the derandomized algorithm works in log-space (indeed, NC 1 ) when having oracle access to the fl-HSG and to an evaluation procedure for C. This has been used in <ref> [ACR97] </ref> to prove hardness/randomness trade-offs for parallel complexity classes. <p> We also emphasize that our simulation runs in NC and, furthermore, it is possible to give an NC construction of the SSZ-dispersers [SSZ97]. This implies that our method also provides an efficient simulation of BPNC algorithms using weak random sources. Andreev et al. <ref> [ACR97] </ref> have recently used our NC proof of Theorem 1 in order to provide sufficient conditions (in terms of worst-case circuit complexity) for NC = BPNC. Without our result they were only able to provide sufficient conditions for ZPNC = BPNC.
Reference: [BR94] <author> M. Bellare and J. Rompel. </author> <title> Randomness-efficient oblivious sampling. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 276-287, </pages> <year> 1994. </year>
Reference-contexts: Extractors were the only known tool to achieve results of this kind. We remark that extractors yield oblivious sampling algorithms (see <ref> [BR94, Zuc96a] </ref>). We show that dispersers are sufficient. <p> The source of this non-obliviousness is the selection of a good set S j among the candidates S 1 ; : : : ; S k . As a result, our sampling algorithm is not oblivious according to the definition of Bellare and Rompel <ref> [BR94] </ref>, however it is non-adaptive.
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Since it is even questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88] </ref>. <p> Several definitions of weak random source have been proposed in the literature, the most general being the following <ref> [CG88] </ref>: a source has min-entropy r fl for fl &gt; 0 if it outputs a string in f0; 1g r and no string has probabil ity of being output larger than 2 r fl .
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: A polynomial-time simulation of BPP using weak random sources of min-entropy r fl for any fixed fl &gt; 0 was one of the major open ques tions in the field. It is not difficult to show that to simulate RP by means of a weak random source, OR dispersers <ref> [CW89] </ref> (from now on, we will simply call them dispersers) are sufficient.
Reference: [LV90] <author> M. Li and P. Vitany. </author> <title> Kolmogorov complexity and its applications. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 187-254. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: For example, K U (f jf ) = O (1). As usual, if we fix another universal Turing machine U 0 it holds K U 0 (gjf ) = K U (gjf ) + fi (1). We will usually omit the subscript. See e.g. <ref> [LV90] </ref> for an introduction to Kolmogorov complexity. In this paper we will only need to use the obvious fact that, for any fixed f, the number of functions g such that K (gjf) k is at most 2 k .
Reference: [MR95] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Randomized algorithms are often the simpler ones to solve a given problem, or the most efficient, or both (see <ref> [MR95] </ref>). For some problems, including primality testing and approximation of # P-complete counting problems, only randomized solutions are known. The practical applicability of such randomized methods depends on the effective possibility for an algorithm to access truly random bits.
Reference: [Nis90] <author> N. Nisan. </author> <title> Using Hard Problems to Create Pseudorandom Generators. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ACM Distinguished Dissertations. </note>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Nis90, NW94] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and we use a derandom-ization method to take full advantage from a weak random source.
Reference: [Nis96] <author> N. Nisan. </author> <title> Extracting randomness: How and why. </title> <booktitle> In Proceedings of the 11th IEEE Conference on Computational Complexity, </booktitle> <pages> pages 44-58, </pages> <year> 1996. </year>
Reference-contexts: This construction is somewhat eas ier to obtain, and Saks et al. [SSZ95] give indeed a disperser with d = poly (n), for any constant fl &gt; 0, allowing for a polynomial time simulation of RP. See <ref> [Nis96] </ref> for a very complete and updated survey on extractors, dispersers, and weak random sources. Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. <p> The main novelty in our result has been the use of dispersers in a context where extractors seemed to be necessary. Extractors have other applications besides the use of weak random sources (see e.g. <ref> [Nis96] </ref>). It could be case that techniques similar to ours can give stronger results or simplified proofs in these other applications as well. We also emphasize that our simulation runs in NC and, furthermore, it is possible to give an NC construction of the SSZ-dispersers [SSZ97].
Reference: [NW94] <author> N. Nisan and A. Wigderson. </author> <title> Hardness vs randomness. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49 </volume> <pages> 149-167, </pages> <year> 1994. </year> <note> Preliminary version in Proc. of FOCS'88. </note>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Nis90, NW94] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and we use a derandom-ization method to take full advantage from a weak random source.
Reference: [NZ96] <author> N. Nisan and D. Zuckerman. </author> <title> Randomness is linear in space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(1) </volume> <pages> 43-52, </pages> <year> 1996. </year> <note> Preliminary version in Proc. of STOC'93. </note>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible [Nis90, NW94]. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization <ref> [NZ96] </ref>. Here we revert this connection, and we use a derandom-ization method to take full advantage from a weak random source.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 27th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 479-488, </pages> <year> 1995. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> This construction is somewhat eas ier to obtain, and Saks et al. <ref> [SSZ95] </ref> give indeed a disperser with d = poly (n), for any constant fl &gt; 0, allowing for a polynomial time simulation of RP. See [Nis96] for a very complete and updated survey on extractors, dispersers, and weak random sources. <p> ~y2S [C (~y) = 1]. 2 4 Proof of Theorem 2 The starting point of our proof is the following easy observation: if we have a set I f0; 1g N such that Pr ~x [~x 2 I] &gt; 1=2, then using a weak random source and the dispersers of <ref> [SSZ95] </ref>, we can generate a polynomial-sized (in N ) set of vectors ~x 1 ; : : : ; ~x k such that, with high probability, f~x 1 ; : : : ; ~x k g " I 6= ;. This is formalized in Lemma 17 below. <p> As a consequence, the set S (with high probability) the hitting property required by Theorem 12. We start quoting the disperser construction of Saks, Srinivasan, and Zhou <ref> [SSZ95] </ref>.
Reference: [SSZ97] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <type> Personal communication, </type> <month> March </month> <year> 1997. </year>
Reference-contexts: It could be case that techniques similar to ours can give stronger results or simplified proofs in these other applications as well. We also emphasize that our simulation runs in NC and, furthermore, it is possible to give an NC construction of the SSZ-dispersers <ref> [SSZ97] </ref>. This implies that our method also provides an efficient simulation of BPNC algorithms using weak random sources. Andreev et al. [ACR97] have recently used our NC proof of Theorem 1 in order to provide sufficient conditions (in terms of worst-case circuit complexity) for NC = BPNC.
Reference: [SV86] <author> M. Santha and U. Vazirani. </author> <title> Generating quasi-random sequences from slightly random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: Since it is even questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88] </ref>.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-275, </pages> <year> 1994. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
Reference: [TS96] <author> A. Ta-Shma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In Proceedings of the 28th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 276-285, </pages> <year> 1996. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> However, the best present construc tion of extractors for fixed fl &gt; 0 and r = poly (m) has d = n log (k) n <ref> [TS96] </ref>. This implies a quasi-polynomial time simulation of BPP. A polynomial-time simulation of BPP using weak random sources of min-entropy r fl for any fixed fl &gt; 0 was one of the major open ques tions in the field.
Reference: [Vaz86] <author> U. Vazirani. </author> <title> Randomness, Adversaries and Computation. </title> <type> PhD thesis, </type> <institution> University of Cal-ifornia, Berkeley, </institution> <year> 1986. </year>
Reference-contexts: Since it is even questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88] </ref>.
Reference: [Vaz87] <author> U. Vazirani. </author> <title> Efficiency considerations in using semi-random sources. </title> <booktitle> In Proceedings of the 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 160-168, </pages> <year> 1987. </year>
Reference-contexts: Since it is even questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88] </ref>.
Reference: [VV85] <author> U. Vazirani and V. Vazirani. </author> <title> Random polynomial time is equal to slightly random polynomial time. </title> <booktitle> In Proceedings of the 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 417-428, </pages> <year> 1985. </year>
Reference-contexts: Since it is even questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88] </ref>.
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
Reference: [Zuc96a] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors and constructive leader election. </title> <booktitle> In Proceedings of the 28th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 286-295, </pages> <year> 1996. </year>
Reference-contexts: Extractors were the only known tool to achieve results of this kind. We remark that extractors yield oblivious sampling algorithms (see <ref> [BR94, Zuc96a] </ref>). We show that dispersers are sufficient.
Reference: [Zuc96b] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <journal> Algorithmica, </journal> 16(4/5):367-391, 1996. Preliminary version in Proc. of FOCS'91. function bad(a) constants x[1] := ~s <volume> 1 </volume> ; : : : begin count := 0; for i := 1 to m do count := count + f (a x[i]); if count &gt; mp max or count &lt; mp min then return (1) else return (0) end. 
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
References-found: 22

