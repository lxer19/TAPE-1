URL: http://www.speech.cs.cmu.edu/rspeech-1/air/papers/speechwear.ps
Refering-URL: http://www.speech.cs.cmu.edu/rspeech-1/air/papers/
Root-URL: 
Email: email: air@cs.cmu.edu  
Phone: Telephone: +1 412 268 2622  
Title: SpeechWear: A mobile speech system  
Author: Alexander I. Rudnicky, Stephen D. Reed, Eric H. Thayer 
Address: 5000 Forbes Avenue, Pittsburgh, PA 15213-3890 USA  
Affiliation: School of Computer Science, Carnegie Mellon University  
Abstract: We describe a system that allows ambulating users to perform data entry and retrieval using a speech interface to a wearable computer. The interface is a speech-enabled Web browser that allows the user to access both locally stored documents as well as remote ones through a wireless link. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Charles T. Hemphill and Philip R. Thrift. </author> <title> Surfing the Web by voice. </title> <booktitle> In Proceedings of ACM Multimedia'95, </booktitle> <address> San Franscisco, CA. </address> <publisher> ACM, </publisher> <month> November </month> <year> 1995. </year>
Reference-contexts: HYPERSPEECH To provide speech understanding services, we developed a backward-compatible extension to html which facilitates the incorporation of language-specific information into hypertext documents. This approach is somewhat different from that commonly chosen by others <ref> [1, 6, 10, 5] </ref> which is to store such information in data structures that are parallel to the browser's internal representation of the information on a hypertext page. This is a workable approach in cases where speech is meant to support primarily navigation (i. e., "fol-lowing links").
Reference: 2. <author> Sunil Issar and Wayne Ward. </author> <title> Flexible parsing: CMU's approach to spoken language understanding. </title> <booktitle> In Proceedings of the Spoken Language Technology Workshop, </booktitle> <pages> pages 53-58, </pages> <address> San Franscisco, CA, </address> <month> March </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Communications is is by means of a WaveLAN transmitter. Recognition services are provided by a real-time implementation of the Sphinx-II recognition system [7], a continuous-speech speaker-independent system based on hidden Markov modeling. Spoken language interpretation made use of the Phoenix <ref> [2] </ref>. The system implements a "continuous listening" protocol [12] that allows the task to be performed hands-free. A modified mouse is provided to turn the system on and off. Figure 1 shows a diagram of the system. The NCSA Mosaic browser [3] provides the interface to the task hypertext document.
Reference: 3. <institution> Mosaic Home Page. </institution> <address> http://www.ncsa.uiuc.edu/SDG/ Software/Mosaic/. </address>
Reference-contexts: Spoken language interpretation made use of the Phoenix [2]. The system implements a "continuous listening" protocol [12] that allows the task to be performed hands-free. A modified mouse is provided to turn the system on and off. Figure 1 shows a diagram of the system. The NCSA Mosaic browser <ref> [3] </ref> provides the interface to the task hypertext document. It was modified by merging the spoken language code into it to create a single multi-threaded application. Inspection data was recorded through the use of FORMs embedded in the task document.
Reference: 4. <author> NCSA. </author> <title> The common gateway interface. </title> <address> http:// hoohoo.ncsa.uiuc.edu/cgi/. </address>
Reference-contexts: Inspection data was recorded through the use of FORMs embedded in the task document. As the interface is a speech-enhanced version of the Mosaic browser, communication is through the standard http protocol and makes use of servers and CGI <ref> [4] </ref> scripts to implement the inspection system. 4. HYPERSPEECH To provide speech understanding services, we developed a backward-compatible extension to html which facilitates the incorporation of language-specific information into hypertext documents.
Reference: 5. <author> Bill Noon. </author> <title> ListenUp! Speech recognition plugin for Netscape. </title> <address> http://snow.cit.cornell.edu/noon/ ListenUp.html. </address>
Reference-contexts: HYPERSPEECH To provide speech understanding services, we developed a backward-compatible extension to html which facilitates the incorporation of language-specific information into hypertext documents. This approach is somewhat different from that commonly chosen by others <ref> [1, 6, 10, 5] </ref> which is to store such information in data structures that are parallel to the browser's internal representation of the information on a hypertext page. This is a workable approach in cases where speech is meant to support primarily navigation (i. e., "fol-lowing links").
Reference: 6. <author> David G. Novick and David House. </author> <title> Spoken-language access to multimedia (SLAM): A multimodal interface to the World Wide Web. </title> <address> http://www.cse.ogi.edu/ SLAM/slam-paper.html, </address> <year> 1995. </year>
Reference-contexts: HYPERSPEECH To provide speech understanding services, we developed a backward-compatible extension to html which facilitates the incorporation of language-specific information into hypertext documents. This approach is somewhat different from that commonly chosen by others <ref> [1, 6, 10, 5] </ref> which is to store such information in data structures that are parallel to the browser's internal representation of the information on a hypertext page. This is a workable approach in cases where speech is meant to support primarily navigation (i. e., "fol-lowing links").
Reference: 7. <author> Mosur K. Ravishankar. </author> <title> Efficient algorithms for speech recognition. </title> <type> Phd, </type> <institution> Carnegie Mellon University, </institution> <address> Pitts-burgh, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Input is through a head-mounted microphone and output through a small head-mounted (grey-scale) VGA display with a speaker attached to its frame. Communications is is by means of a WaveLAN transmitter. Recognition services are provided by a real-time implementation of the Sphinx-II recognition system <ref> [7] </ref>, a continuous-speech speaker-independent system based on hidden Markov modeling. Spoken language interpretation made use of the Phoenix [2]. The system implements a "continuous listening" protocol [12] that allows the task to be performed hands-free. A modified mouse is provided to turn the system on and off.
Reference: 8. <author> A. I. Rudnicky and A. G. Hauptmann. </author> <title> Models for evalu ating interaction protocols in speech recognition. </title> <booktitle> In Proceedings of CHI, </booktitle> <pages> pages 285-291, </pages> <address> New York, </address> <month> April </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: For example, the form of the task as designed followed quite closely that used in the original VuMan implementation and was implicitly constrained by the characteristics of the rotary mouse interface. Analysis of the task structure, for example, suggests that a different protocol (implicit confirmation <ref> [8] </ref>) could eliminate approximately half the steps in the original task, by implicitly channeling the dialog along the most likely path and relying on the user to indicate deviations.
Reference: 9. <author> Alexander I. Rudnicky. </author> <title> Mode preference in a simple data-retrieval task. </title> <booktitle> In Proceedings of the Arpa Workshop on Human Language Technology, </booktitle> <pages> pages 364-369, </pages> <address> San Mateo, CA, March 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There is reason to believe that some of this impression may be based on a simple lack of experience with the system (users will typically experience long-term improvement in task completion time while using a speech system, e.g. <ref> [9] </ref>). It also became apparent that an interface that is capable of actively guiding users when they exhibit difficulties would also be of value.
Reference: 10. <author> Shocktalk. </author> <note> http://www.emf.net/~dreams/shocktalk/. </note>
Reference-contexts: HYPERSPEECH To provide speech understanding services, we developed a backward-compatible extension to html which facilitates the incorporation of language-specific information into hypertext documents. This approach is somewhat different from that commonly chosen by others <ref> [1, 6, 10, 5] </ref> which is to store such information in data structures that are parallel to the browser's internal representation of the information on a hypertext page. This is a workable approach in cases where speech is meant to support primarily navigation (i. e., "fol-lowing links").
Reference: 11. <author> Asim Smailagic and Daniel P. Sieworek. </author> <title> Modalities of interaction with CMU wearable computers. </title> <journal> IEEE Personal Communications, </journal> <volume> 3(1) </volume> <pages> 14-25, </pages> <month> Feb </month> <year> 1996. </year>
Reference-contexts: An obvious alternative is speech, both for input and for output. The present paper describes an initial attempt to build such an interface in the context of a system for mobile inspection. The task we chose was initially developed as part of the VuMan <ref> [11] </ref> project at Carnegie Mellon University. The Vu-Man has been used for a limited technical inspection (LTI) of an amphibious assault vehicle for the USMC at Camp Pendleton, as a replacement for a clipboard and pencil procedure.
Reference: 12. <author> Reed Stephen D. </author> <title> Utterance end-pointing in the presence of noise using Gaussian classification of cepstral coefficients. </title> <type> Technical Report TR 1995-8, </type> <institution> Information Networking Institute (Carnegie Mellon University), </institution> <address> Pitts-burgh, PA, </address> <year> 1995. </year> <type> Masters' Thesis. </type>
Reference-contexts: Communications is is by means of a WaveLAN transmitter. Recognition services are provided by a real-time implementation of the Sphinx-II recognition system [7], a continuous-speech speaker-independent system based on hidden Markov modeling. Spoken language interpretation made use of the Phoenix [2]. The system implements a "continuous listening" protocol <ref> [12] </ref> that allows the task to be performed hands-free. A modified mouse is provided to turn the system on and off. Figure 1 shows a diagram of the system. The NCSA Mosaic browser [3] provides the interface to the task hypertext document.
References-found: 12

