URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/papers/PWW/pwwECML98.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/index.html
Root-URL: 
Email: E-mail: Dunja.Mladenic@ijs.si  
Phone: Phone: (+38)(61) 1773 272, Fax: (+38)(61) 1258-158  
Title: Feature subset selection in text-learning  
Author: Dunja Mladenic 
Note: Odds ratio that is known from information retrieval.  
Web: http://www-ai.ijs.si/DunjaMladenic  
Address: Jamova 39, 1100 Ljubljana, Slovenia  
Affiliation: Department of Intelligent Systems, J.Stefan Institute,  
Abstract: This paper describes several known and some new methods for feature subset selection on large text data. Experimental comparison given on real-world data collected from Web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected. Our problem domain consists of hyperlinks given in a form of small-documents represented with word vectors. In our learning experiments naive Bayesian classifier was used on text data. The best performance was achieved by the feature selection methods based on the feature scoring measure called 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Apte, C., Damerau, F., Weiss, </author> <title> S.M., Toward Language Independent Automated Learning of Text Categorization Models, </title> <booktitle> Proc. of the 7th Annual Int. ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1994. </year>
Reference-contexts: The usual way of learning on text defines a feature for each word that oc-curred in training documents. This can easily result with several tens of thousands of features. Most methods for feature subset selection that are used information retrieval and text-learning (eg. <ref> [1] </ref>, [3], [11]) are very simple compared to the methods developed in machine learning.
Reference: 2. <author> Kraut, R., Scherlis, W., Mukhopadhyay, T., Manning, J., Kiesler, S., </author> <title> The HomeNet Field Trial of Residential Internet Services, </title> <journal> Communications of the ACM Vol. </journal> <volume> 39, No. 12, pp.55|63, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: For each word position in the document, a feature is defined having a word as its value [9]. Our experiments are performed on data collected for users participating in the HomeNet project <ref> [2] </ref>. Results for two users are described in Section 4 with the data characteristics are given in Table 1. For each user approximately 4000 different words occurred in documents resulting here with 4000 features.
Reference: 3. <author> Joachims, T., </author> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 143|151, </pages> <year> 1997. </year>
Reference-contexts: The usual way of learning on text defines a feature for each word that oc-curred in training documents. This can easily result with several tens of thousands of features. Most methods for feature subset selection that are used information retrieval and text-learning (eg. [1], <ref> [3] </ref>, [11]) are very simple compared to the methods developed in machine learning. <p> Our documents are represented as word vectors (using so called bag-of-words representation commonly used in information retrieval) and learning was performed using Naive (simple) Bayesian classifier the same way as in <ref> [3] </ref> or [10]. For each word position in the document, a feature is defined having a word as its value [9]. Our experiments are performed on data collected for users participating in the HomeNet project [2]. <p> Subset of classification results plotted in Figure 1 for two domains. 5 Discussion We experimentally compared seven attribute scoring measures on our data: Information gain used in most machine learning experiments on text data (eg. <ref> [3] </ref>, [11], [14]), four variants of Odds ratio as the most promising for our problem, Frequency and Random that we used as a baseline measure. The results of experiments suggest that in feature subset selection for text-learning the best 2 % to 5 % of features should be selected.
Reference: 4. <author> John, G.H., Kohavi, R., Pfleger, K., </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> Proc. of the 11th International Conference on Machine Learning ICML94, </booktitle> <pages> pp. 121|129, </pages> <year> 1994. </year>
Reference-contexts: Discussion is given in Section 5. 2 Feature subset selection approaches Different methods have been developed and used for feature subset selection in statistics, pattern recognition and machine learning, using different search strategies and evaluation functions. John et al. <ref> [4] </ref> pointed out the difference between the two main approaches used in machine learning to feature subset selection: filtering approach where the feature subset is selected independent of the learning method and wrapper approach where the feature subset is selected using the same learning algorithm that will be used for learning
Reference: 5. <author> Kindo, T., Yoshida, H., Morimoto, T., Watanabe, T., </author> <title> Adaptive Personal Information Filtering System that Organizes Personal Profiles Automatically, </title> <booktitle> Proc. of the 15th Int. Joint Conference on Artificial Intelligence IJCAI-97, </booktitle> <pages> pp. 716|721, </pages> <year> 1997. </year>
Reference-contexts: As a baseline measure we used Random score assignment. In some text-classification experiments some other measures similar to Information gain are used like expected Cross entropy, Keyword checker and M utual inf ormation used in [6], <ref> [5] </ref>, [14] respectively.
Reference: 6. <author> Koller, D., Sahami, M., </author> <title> Hierarchically classifying documents using very few words, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 170|178, </pages> <year> 1997. </year>
Reference-contexts: As a baseline measure we used Random score assignment. In some text-classification experiments some other measures similar to Information gain are used like expected Cross entropy, Keyword checker and M utual inf ormation used in <ref> [6] </ref>, [5], [14] respectively.
Reference: 7. <author> Kononenko, I. and Bratko, I., </author> <title> Information-Based Evaluation Criterion for Classifier's Performance, Machine Learning 6, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: For each data set we observed the influence of the number of the best features selected for learning (vector size) to the system performance. Since we have unbalanced class distribution (see Table 1), Classification accuracy can give misleading results. For such domains more appropriate measure is Information score <ref> [7] </ref> or Geometric mean of accuracy [8]. In the experimental results presented in Figure 1 Classification accuracy and Information score are used to estimate model quality.
Reference: 8. <author> Kubat, M., Holte, R., Matwing, S., </author> <title> Learning When Negative Examples Abound, </title> <booktitle> 9th European Conference on Machine Learning ECML97, </booktitle> <pages> pp. 146|153, </pages> <year> 1997. </year>
Reference-contexts: Since we have unbalanced class distribution (see Table 1), Classification accuracy can give misleading results. For such domains more appropriate measure is Information score [7] or Geometric mean of accuracy <ref> [8] </ref>. In the experimental results presented in Figure 1 Classification accuracy and Information score are used to estimate model quality.
Reference: 9. <author> Mitchell, </author> <title> T.M., Machine Learning, </title> <publisher> The McGraw-Hill Companies, Inc., </publisher> <year> 1997. </year>
Reference-contexts: For each word position in the document, a feature is defined having a word as its value <ref> [9] </ref>. Our experiments are performed on data collected for users participating in the HomeNet project [2]. Results for two users are described in Section 4 with the data characteristics are given in Table 1. For each user approximately 4000 different words occurred in documents resulting here with 4000 features.
Reference: 10. <author> Mladenic, D., </author> <title> Personal WebWatcher: Implementation and Design, </title> <type> Technical Report IJS-DP-7472, </type> <month> October, </month> <year> 1996. </year> <note> http://www-ai.ijs.si/DunjaMladenic/papers/PWW/ </note>
Reference-contexts: This is performed on-line while user is sitting behind some Web browser and waiting for the requested document. Our prototype system called Personal WebWatcher <ref> [10] </ref> uses text-learning on this problem, learning separate model for each user and highlighting hyperlinks on the requested Web documents. All hyperlinks from the visited documents are used as machine learning examples. Each is assigned one of the two class values: positive (user clicked on the hyperlink) or negative. <p> Our documents are represented as word vectors (using so called bag-of-words representation commonly used in information retrieval) and learning was performed using Naive (simple) Bayesian classifier the same way as in [3] or <ref> [10] </ref>. For each word position in the document, a feature is defined having a word as its value [9]. Our experiments are performed on data collected for users participating in the HomeNet project [2].
Reference: 11. <author> Pazzani, M., Billsus, D., </author> <title> Learning and Revising User Profiles: The Identification of Interesting Web Sites, Machine Learning 27, </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 313|331, </pages> <year> 1997. </year> <note> 12. </note> <editor> van Rijsbergen, C.J,. Harper, D.J., Porter, M.F., </editor> <title> The selection of good search terms, </title> <booktitle> Information Processing & Management, 17, </booktitle> <address> pp.77|91, </address> <year> 1981. </year>
Reference-contexts: The usual way of learning on text defines a feature for each word that oc-curred in training documents. This can easily result with several tens of thousands of features. Most methods for feature subset selection that are used information retrieval and text-learning (eg. [1], [3], <ref> [11] </ref>) are very simple compared to the methods developed in machine learning. <p> Subset of classification results plotted in Figure 1 for two domains. 5 Discussion We experimentally compared seven attribute scoring measures on our data: Information gain used in most machine learning experiments on text data (eg. [3], <ref> [11] </ref>, [14]), four variants of Odds ratio as the most promising for our problem, Frequency and Random that we used as a baseline measure. The results of experiments suggest that in feature subset selection for text-learning the best 2 % to 5 % of features should be selected. <p> The results of experiments suggest that in feature subset selection for text-learning the best 2 % to 5 % of features should be selected. This finding is not in contradiction with the results reported on other text-learning problem domains eg. <ref> [11] </ref>, [14]. The experimental results further suggest that for our problem domain, where one class value is the target class value, features should be scored using some measure based on Odds ratio.
Reference: 13. <author> Shaw Jr, W.M., </author> <title> Term-relevance computations and perfect retrieval performance, </title> <booktitle> Information Processing & Management, 31(4), </booktitle> <address> pp.491|498, </address> <year> 1995. </year>
Reference-contexts: We handle singularities as proposed in <ref> [13] </ref>. Notice that Odds ratio is not averaged over all feature values, like Information gain, but only the value recording that word occurred is considered.
Reference: 14. <author> Yang, Y., Pedersen, J.O., </author> <title> A Comparative Study on Feature Selection in Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 412|420, </pages> <year> 1997. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In our experimental comparison, we also include a very simple measure reported to work well in text classification domains <ref> [14] </ref> that scores each word by F req (W ) frequency of word W . As a baseline measure we used Random score assignment. <p> As a baseline measure we used Random score assignment. In some text-classification experiments some other measures similar to Information gain are used like expected Cross entropy, Keyword checker and M utual inf ormation used in [6], [5], <ref> [14] </ref> respectively. <p> For these measures the best vector size is approximately between 60 and 200 best features. This means that the selected feature subset includes just 2% - 5% of all features. The similar reduction (up to 90%) in the number of features used in text-learning was observed in <ref> [14] </ref>. The other three measures (Inf Gain; F req; F reqOddsRatio) for most vector sizes achieved worst results than Random. Closer look to words sorted by Information gain showed that most best words are characteristic for negative class value (their probability for positive documents is 0). <p> Subset of classification results plotted in Figure 1 for two domains. 5 Discussion We experimentally compared seven attribute scoring measures on our data: Information gain used in most machine learning experiments on text data (eg. [3], [11], <ref> [14] </ref>), four variants of Odds ratio as the most promising for our problem, Frequency and Random that we used as a baseline measure. The results of experiments suggest that in feature subset selection for text-learning the best 2 % to 5 % of features should be selected. <p> The results of experiments suggest that in feature subset selection for text-learning the best 2 % to 5 % of features should be selected. This finding is not in contradiction with the results reported on other text-learning problem domains eg. [11], <ref> [14] </ref>. The experimental results further suggest that for our problem domain, where one class value is the target class value, features should be scored using some measure based on Odds ratio. <p> In further experiments we plan to include more datasets, removal of infre-quent features and common words (using `stop-list'). With this last modification we would like to test on our data the hypothesis about good behavior of a simple scoring by Frequency set by Yang and Pedersen <ref> [14] </ref>. Acknowledgements This work was financially supported by the Slovenian Ministry for Science and Technology. Part of this work was performed during the authors stay at Carnegie Mellon University. Author is grateful to Tom Mitchell and his machine learning group at Carnegie Mellon University for generous support, suggestions and fruitful discussions.
References-found: 13

