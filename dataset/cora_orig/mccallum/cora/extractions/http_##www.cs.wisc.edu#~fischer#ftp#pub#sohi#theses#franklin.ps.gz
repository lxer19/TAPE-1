URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/franklin.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/sohi/theses/
Root-URL: http://www.cs.wisc.edu
Title: THE MULTISCALAR ARCHITECTURE  
Author: by MANOJ FRANKLIN 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY (Computer Sciences) at the  
Date: 1993  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 0
Reference: [1] <institution> CDC Cyber 200 Model 205 Computer System Hardware Reference Manual. Arden Hills, MN: Control Data Corporation, </institution> <year> 1981. </year>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines [2, 3, 130, 131], CDC Cyber-205 <ref> [1] </ref>, and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131]. Vector machines perform remarkably well in executing codes that fit the vector paradigm [157].
Reference: [2] <author> Cray Computer Systems: </author> <title> Cray X-MP Model 48 Mainframe Reference Manual. </title> <institution> Mendota Heights, MN: Cray Research, Inc., HR-0097, </institution> <year> 1984. </year>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines <ref> [2, 3, 130, 131] </ref>, CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131].
Reference: [3] <institution> Cray Computer Systems: </institution> <note> Cray-2 Hardware Reference Manual. </note> <institution> Mendota Heights, MN: Cray Research, Inc., HR-2000, </institution> <year> 1985. </year>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines <ref> [2, 3, 130, 131] </ref>, CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131].
Reference: [4] <author> R. D. Acosta, J. Kjelstrup, and H. C. Torng, </author> <title> ``An Instruction Issuing Approach to Enhancing Performance in Multiple Functional Unit Processors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-35, </volume> <pages> pp. 815-828, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Results are forwarded to the register file in sequential order, as and when instructions are committed from the RUU. Dispatch Stack: The dispatch stack is a technique proposed by Acosta et al <ref> [4, 154] </ref>. The hardware features for carrying out this scheme are: two dependency count fields associated with each register, and a dispatch stack. <p> There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [5] <author> S. V. Adve and M. Hill, </author> <title> ``Weak Ordering ANew Definition,'' </title> <booktitle> Proceedings of 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-14, </pages> <year> 1990. </year>
Reference-contexts: Memory renaming and the potential offered by the memory renaming capability of the ARB are also research areas that need further investigation. Finally, we would like to extend the ARB so as to enforce different memory consistency models such as weak consistency [41], data-race-free-0 (DRF0) <ref> [5] </ref>, data-race-free-1 (DRF1) [6], processor consistency [65], and sequential consistency [91] in a multiprocessor system. 9.2.4.
Reference: [6] <author> S. V. Adve and M. Hill, </author> <title> ``A Unified Formalization of Four Shared-Memory Models,'' </title> <type> Technical Report #1051a, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> Sep-tember </month> <year> 1992. </year>
Reference-contexts: Memory renaming and the potential offered by the memory renaming capability of the ARB are also research areas that need further investigation. Finally, we would like to extend the ARB so as to enforce different memory consistency models such as weak consistency [41], data-race-free-0 (DRF0) [5], data-race-free-1 (DRF1) <ref> [6] </ref>, processor consistency [65], and sequential consistency [91] in a multiprocessor system. 9.2.4. Multi-Version Data Cache A major challenge that we face with the ARB connected as described in chapter 6 is the latency of the interconnection network connecting the execution units to the ARB and the data cache.
Reference: [7] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Figure 2.1 (ii) shows the CDG of the CFG shown in Figure 2.1 (i). Notice that control parallelism exists between basic blocks 1 and 4, i.e., these two blocks are control independent. Data dependencies are derived by performing a data flow analysis of the program <ref> [7] </ref>. 2.2. Program Specification and Execution Models ILP processing involves a sequence of steps, some of which need to be done by software/programmer and some of which need to be done by hardware. <p> All optimizing compilers invariably do dataflow analysis <ref> [7, 52] </ref>; the 97 create mask is similar to the def variables computed by these compilers for each basic block, except that the former represents architectural registers and the latter represent variables of the source program.
Reference: [8] <author> A. Aiken and A. Nicolau, </author> <title> ``Perfect Pipelining: A New Loop Parallelization Technique,'' </title> <booktitle> in ESOP '88: 2nd European Symosium on Programming (Nancy, France), Lecture Notes in Computer Science, </booktitle> <volume> No. 300, </volume> <pages> pp. 221-235. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining <ref> [8, 9] </ref>, boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [9] <author> A. Aiken and A. Nicolau, </author> <title> ``A Development Environment for Horizontal Microcode,'' </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pp. 584-594, </pages> <month> May </month> <year> 1988. </year> <pages> 183 184 </pages>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining <ref> [8, 9] </ref>, boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [10] <author> A. Aiken and A. Nicolau, </author> <title> ``Optimal Loop Parallelization Technique,'' </title> <booktitle> Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 308-317, </pages> <year> 1988. </year>
Reference-contexts: Figure 2.3 (i) shows an example loop; Figure 2.3 (ii) shows the loop unrolled three times. Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining <ref> [10] </ref>, enhanced pipeline scheduling [47], GURPR* [149], modulo scheduling [48, 124], and polycyclic scheduling [125]. Boosting: Boosting is a technique for statically specifying speculative execution [141-143].
Reference: [11] <author> J. R. Allen, </author> <title> ``Dependence Analysis for Subscripted Variables and Its Application to Program Transformation,'' </title> <type> PhD thesis, </type> <institution> Department of Mathematical Science, Rice University, </institution> <year> 1983. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time.
Reference: [12] <author> J. R. Allen and K. Kennedy, </author> <title> ``Automatic Loop Interchange,'' </title> <booktitle> Proceedings of the ACM SIG-PLAN 84 Symposium on Compiler Construction, </booktitle> <pages> pp. 233-246, </pages> <year> 1984. </year>
Reference-contexts: In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies. Some of these techniques are if-conversion, loop unrolling, loop peeling, loop conditioning, loop exchange <ref> [12] </ref>, function inlining [28, 85], replacing a set of IF-THEN statements by a jump table [129], and even changing data structures. All these techniques modify the CFG of the program, mostly by reducing the number of control decision points in the CFG.
Reference: [13] <author> R. Allen, K. Kennedy, C. Porterfield, and J. Warren, </author> <title> ``Conversion of Control Dependence to Data Dependence,'' </title> <booktitle> Proceedings of 10th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1983. </year>
Reference-contexts: Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency.
Reference: [14] <author> R. Allen and K. Kennedy, </author> <title> ``Automatic Translation for FORTRAN Programs to Vector Form,'' </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency. <p> For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays [62, 97]. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler <ref> [14, 49, 54, 86, 94] </ref>. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis. Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers.
Reference: [15] <author> D. W. Anderson, F. J. Sparacio, and R. M. Tomasulo, </author> <title> ``The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling,'' IBM Journal of Research and Development, </journal> <pages> pp. 8-24, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Chapter 6 further addresses the issues involved in dynamic disambiguation. Over the years, different techniques have been proposed for performing dynamic disambiguation <ref> [15, 115, 139] </ref>; these are also described in detail in chapter 6. Another important aspect in connection with dynamic disambiguation is that existing schemes execute memory references after performing disambiguation. <p> Tomasulo's Algorithm: Tomasulo's out-of-order issue algorithm was first presented for the floating point unit of the IBM 360/91 <ref> [15] </ref>, which also issued instructions one at a time. The hardware features for carrying out Tomasulo's algorithm are: a busy bit and a tag field associated with each register, a set of reservation stations associated with each functional unit, and a common data bus (CDB). <p> Thus, to handle the worst case, the latter requires hardware for i=1 i=S 2 S (2W - S - 1) hhhhhhhhhhhh compares, where S is the maximum number of stores execut ed per cycle. 117 91 <ref> [15, 22] </ref> and Astronautics ZS-1 [139]. In this scheme, the stores are allowed to wait in a queue until their store values are ready.
Reference: [16] <author> M. Annartone, E. Arnould, T. Gross, H. T. Kung, M. Lam, O. Menzilcioglu, and J. A. Webb, </author> <title> ``The Warp Computer: Architecture, Implementation and Performance,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1523-1538, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The noted one among them is the systolic paradigm [88], which is excellent in exploiting ILP in numeric applications such as signal processing and computer vision <ref> [16, 23] </ref>. Some specialized computers even hard-wire FFTs and other important algorithms to 42 give tremendous performance for signal processing applications. The applicability of such special purpose processing paradigms to non-numeric applications, having complex control and data flow patterns, is not clear.
Reference: [17] <author> Arvind, K. P. Gostelow, and W. E. Plouffe, </author> <title> ``An Asynchronous Programming Language and Computing Machine,'' </title> <type> Technical Report TR 114a, </type> <institution> Department of Information and Computation Science, University of California, Irvine, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: This is particularly useful for expressing the control parallelism present in an application. For a given application, some algorithms have more inherent parallelism than others. Similarly, some of the control dependencies in a program are an artifact of the programming paradigm used; dataflow languages such as Id <ref> [17] </ref> are tailored to express the parallelism present in an application, and do not introduce artificial control dependencies. One example where data structures help in expressing parallelism is the use of arrays instead of linked lists.
Reference: [18] <author> T. M. Austin and G. S. Sohi, </author> <title> ``Dynamic Dependency Analysis of Ordinary Programs,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 342-351, </pages> <year> 1992. </year> <month> 185 </month>
Reference-contexts: Exploitation of parallelism at the instruction level is very important to speed up such programs. Recent studies have confirmed that there exists a large amount of instruction-level parallelism in ordinary programs <ref> [18, 25, 36, 106, 159] </ref>. Even in other applications, no matter how much parallelism is exploited by coarse-grain parallel processors such as the multiprocessors, a substantial amount of parallelism will still remain to be exploited at the instruction level. <p> Second, studies have found little ILP within a small sequential block of instructions, but significant amounts in large blocks <ref> [18, 25, 90, 159] </ref>. There are several inter-related factors that contribute to this. <p> Memory renaming is analogous to register renaming; providing more physical storage allows more parallelism to be exploited <ref> [18] </ref>. The advantage of memory renaming is especially felt in the renaming of the stack. Consider the piece of assembly code in Figure 6.6, in which a parameter for a procedure is passed through the stack. Both instantiations of PROCEDURE1 use the same stack location for passing the parameter. <p> The multiscalar processor appears to have all the features desirable/essential in an ILP processor as we understand them today; abstract machines with these capabilities appear to be capable of sustaining ILP in C programs far beyond 10+ IPC <ref> [18, 90] </ref>. The preliminary performance results are very optimistic in this regard. With no multiscalar-specific optimizations, a multiscalar processor configuration is able to sustain 1.73 to 3.74 useful instructions per cycle for our non-numeric benchmark programs, and 1.38 to 6.06 useful instructions per cycle for our numeric benchmark programs.
Reference: [19] <author> U. Banerjee, </author> <title> ``Speedup of ordinary Programs,'' </title> <type> Tech. Report UIUCDSR-79-989, </type> <institution> Dept. of Computer Science, Univ. of Illinois, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time.
Reference: [20] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time.
Reference: [21] <author> J. S. Birnbaum and W. S. Worley, </author> <title> ``Beyond RISC: High-Precision Architecture,'' </title> <booktitle> Proceedings of COMPCON86, </booktitle> <pages> pp. 40-47, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency.
Reference: [22] <author> L. J. Boland, G. D. Granito, A. U. Marcotte, B. U. Messina, and J. W. Smith, </author> <title> ``The IBM System/360 Model 91: Storage System,'' </title> <journal> IBM Journal, </journal> <pages> pp. 54-68, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Thus, to handle the worst case, the latter requires hardware for i=1 i=S 2 S (2W - S - 1) hhhhhhhhhhhh compares, where S is the maximum number of stores execut ed per cycle. 117 91 <ref> [15, 22] </ref> and Astronautics ZS-1 [139]. In this scheme, the stores are allowed to wait in a queue until their store values are ready.
Reference: [23] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb, </author> <title> ``Supporting Systolic and Memory Communication in iWarp,'' </title> <booktitle> The 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 70-81, </pages> <year> 1990. </year>
Reference-contexts: The noted one among them is the systolic paradigm [88], which is excellent in exploiting ILP in numeric applications such as signal processing and computer vision <ref> [16, 23] </ref>. Some specialized computers even hard-wire FFTs and other important algorithms to 42 give tremendous performance for signal processing applications. The applicability of such special purpose processing paradigms to non-numeric applications, having complex control and data flow patterns, is not clear. <p> The applicability of such special purpose processing paradigms to non-numeric applications, having complex control and data flow patterns, is not clear. This is evidenced by the recent introduction of the iWarp machine <ref> [23, 117] </ref>, where support is provided for both systolic communication and coarse-grain communication. 2.4.3. The Vector Paradigm The earliest paradigm for ILP processing was the vector paradigm. In fact, much of the compiler work to extract parallelism was driven by vector machines.
Reference: [24] <author> R. Buehrer and K. Ekanadham, </author> <title> ``Incorporating Dataflow Ideas into von Neumann Processors for Parallel Execution,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1515-1522, </pages> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines <ref> [24, 77] </ref>. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [25] <author> M. Butler, T. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow, </author> <title> ``Single Instruction Stream Parallelism Is Greater than Two,'' </title> <booktitle> Proceedings of 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-286, </pages> <year> 1991. </year>
Reference-contexts: Exploitation of parallelism at the instruction level is very important to speed up such programs. Recent studies have confirmed that there exists a large amount of instruction-level parallelism in ordinary programs <ref> [18, 25, 36, 106, 159] </ref>. Even in other applications, no matter how much parallelism is exploited by coarse-grain parallel processors such as the multiprocessors, a substantial amount of parallelism will still remain to be exploited at the instruction level. <p> Second, studies have found little ILP within a small sequential block of instructions, but significant amounts in large blocks <ref> [18, 25, 90, 159] </ref>. There are several inter-related factors that contribute to this.
Reference: [26] <author> R. G. G. Cattell and Anderson, </author> <title> ``Object Oriented Performance Measurement,'' </title> <booktitle> in Proceedings of the 2nd International Workshop on OODBMS. </booktitle> <month> September </month> <year> 1988. </year>
Reference-contexts: Benchmarks and Performance Metrics For benchmarks, we used the SPEC '92 benchmark suite [104] and some other programs, namely Tycho (a sophisticated cache simulator) [69] and Sunbench (an Object Oriented Database benchmark written in C++) <ref> [26] </ref>. The benchmarks and their features are listed in Table 7.1. Although our emphasis is on non-numeric programs, we have included some numeric benchmark programs for the sake of comparison. The benchmark programs were compiled for a DECstation 3100 using the MIPS C and FORTRAN compilers.
Reference: [27] <author> P. P. Chang and W. W. Hwu, </author> <title> ``Trace Selection for Compiling Large C Application Programs to Microcode,'' </title> <booktitle> in Proceedings of 21st Annual Workshop on Microprogramming and Mi-croarchitecture, </booktitle> <address> San Diego, CA, </address> <pages> pp. 21-29, </pages> <month> November </month> <year> 1988. </year> <month> 186 </month>
Reference-contexts: Ebcioglu and Nakatani describe an enhanced implementation of percolation scheduling for VLIW machines having conditional evaluation capabilities [47]. In their implementation of percolation scheduling, a code motion across a large number of basic blocks is allowed only if each of the pair-wise transformations is beneficial. Superblock Scheduling: Superblock scheduling <ref> [27, 28] </ref> is a variant of trace scheduling. A super-block is a trace with a unique entry point and one or more exit points, and is the operation window used by the compiler to extract parallelism.
Reference: [28] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu, </author> <title> ``IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors,'' </title> <booktitle> Proceedings of 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <year> 1991. </year>
Reference-contexts: Initial static 20 prediction schemes were based on branch opcodes, and were not accurate. Now, static prediction schemes are much more sophisticated, and use profile information or heuristics to take decisions <ref> [28, 75, 98, 118, 160] </ref>. In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies. <p> In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies. Some of these techniques are if-conversion, loop unrolling, loop peeling, loop conditioning, loop exchange [12], function inlining <ref> [28, 85] </ref>, replacing a set of IF-THEN statements by a jump table [129], and even changing data structures. All these techniques modify the CFG of the program, mostly by reducing the number of control decision points in the CFG. <p> Global scheduling is more useful, as they consider large operation windows. Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling <ref> [28] </ref>, software pipelining [45, 89, 102, 103, 161], perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths. <p> Ebcioglu and Nakatani describe an enhanced implementation of percolation scheduling for VLIW machines having conditional evaluation capabilities [47]. In their implementation of percolation scheduling, a code motion across a large number of basic blocks is allowed only if each of the pair-wise transformations is beneficial. Superblock Scheduling: Superblock scheduling <ref> [27, 28] </ref> is a variant of trace scheduling. A super-block is a trace with a unique entry point and one or more exit points, and is the operation window used by the compiler to extract parallelism.
Reference: [29] <author> A. E. Charlesworth, </author> <title> ``An Approach to Scientific Array Processing: The Architectural Design of the AP-120B/FPS-164 Family,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 14, </volume> <month> September </month> <year> 1981. </year>
Reference-contexts: Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [30] <author> D. R. Chase, M. Wegman, and F. K. Zadeck, </author> <title> ``Analysis of Pointers and Structures,'' </title> <booktitle> Proceedings of ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 296-310, </pages> <year> 1990. </year>
Reference-contexts: Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers <ref> [30, 67, 70, 92] </ref>.
Reference: [31] <author> W. Y. Chen, S. A. Mahlke, W. W. Hwu, and T. </author> <title> Kiyohara, </title> <type> ``Personal Communication,'' </type> <year> 1992. </year>
Reference-contexts: Realizing this, researchers have proposed schemes that allow ambiguous references to be statically reordered, with checks made at run time to determine if any dependencies are violated by the static code motions <ref> [31, 108, 132] </ref>. Ambiguous references that are statically reordered are called statically unresolved references. A limitation of this scheme, however, is that the run-time checks need extra code and in some schemes associative compare of store addresses with preceding load addresses in the active window. <p> Memory Conflict Buffer (MCB): The MCB scheme proposed by Chen, et al <ref> [31] </ref> is similar to the scheme discussed above. The difference is that it has no non-architected registers, and instead an address tag is associated with every general purpose register. Statically unresolved loads are specified by a special opcode called preload, instead of by the type of target register used.
Reference: [32] <author> J. R. Coffman, </author> <title> Computer and Job-Shop Scheduling Theory. </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year>
Reference-contexts: Optimal scheduling (under finite resource constraints) is an NP-complete problem <ref> [32] </ref>, necessitating the use of heuristics to take decisions. Although programmers can ease scheduling by expressing some of the parallelism present in programs by using a non-standard high-level language (HLL), 17 the major scheduling decisions have to be taken by the compiler, by the hardware, or by both of them.
Reference: [33] <author> E. U. Cohler and J. E. Storer, </author> <title> ``Functionally Parallel Architectures for Array Processors,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 14, </volume> <pages> pp. 28-36, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Several decoupled architectures have been proposed, and some have been built. The noted ones are the MAP-200 <ref> [33] </ref>, the DAE [136], the PIPE [63], the ZS-1 [139], and the WM architecture [165]. A drawback in decou-pled architectures is AP-EP code unbalance. When there is unbalance, the sustained instruction issue rate is low because of poor utilization. <p> There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [34] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> ``A VLIW Architecture for a Trace Scheduling Compiler,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 967-979, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths. Originally developed for microcode compaction [53], trace scheduling later found application in ILP processing <ref> [34, 49] </ref>. Its methodology in the context of ILP processing is as follows: The compiler forms the operation window by selecting from an acyclic part of the CFG the most likely path, called trace, that will be taken at run time. <p> Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126]. <p> The pros and cons of the VLIW approach have been discussed in detail in the literature [29, 34, 49, 53, 54, 56, 125, 146]. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series <ref> [34] </ref> and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126]. <p> Although techniques such as split register files (separate integer and floating point register files) can provide some additional bandwidth, the inter-instruction communication bandwidth requirement of the multiscalar processor is much higher. Similarly, techniques such as multiple architectural register files, proposed for the TRACE /300 VLIW processor <ref> [34] </ref> are inappropriate for the multiscalar processor, which has the notion of a single architectural register file. Therefore, we need to provide a decentralized realization of a single architectural register file.
Reference: [35] <author> R. Comerford, </author> <title> ``How DEC Developed Alpha,'' </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, </volume> <pages> pp. 43-47, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency. <p> These advantages have led to the adoption of small-issue superscalar processors as the paradigm of choice for several modern microprocessors <ref> [35, 40, 123] </ref>.
Reference: [36] <author> D. E. Culler and Arvind, </author> <title> ``Resource Requirements of Dataflow Programs,'' </title> <booktitle> Proceedings of 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 141-150, </pages> <year> 1988. </year>
Reference-contexts: Exploitation of parallelism at the instruction level is very important to speed up such programs. Recent studies have confirmed that there exists a large amount of instruction-level parallelism in ordinary programs <ref> [18, 25, 36, 106, 159] </ref>. Even in other applications, no matter how much parallelism is exploited by coarse-grain parallel processors such as the multiprocessors, a substantial amount of parallelism will still remain to be exploited at the instruction level. <p> Lack of a static ordering has other ramifications as well. Data-driven specification suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management <ref> [36, 114] </ref>. Furthermore, in data-driven specification, data tokens have to be routed through control-decision points such as switch nodes and merge nodes, and often get collected at these control-decision points, waiting for the appropriate control decisions to be taken.
Reference: [37] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> ``Fine-Grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 164-175, </pages> <year> 1991. </year> <month> 187 </month>
Reference-contexts: A large performance penalty is incurred due to the ``stretching'' of the communication arcs of the dataflow graph, which manifests as associative searches through the entire program using matching hardware. The recent advent of more restricted forms of dataflow architectures <ref> [37, 77, 109, 113, 114] </ref> bear testimony to this fact. <p> The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) <ref> [37] </ref>, and *T [110], and other hybrid machines [24, 77]. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [38] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck, </author> <title> ``An Efficient Method of Computing Static Single Assignment Form,'' </title> <booktitle> Conference Record of the 16th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 25-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: A more complex window, which contains instructions from multiple threads of control might be needed; analysis of the control dependence graph <ref> [38, 51] </ref> of a program can aid in the selection of the threads of control. Another major challenge in designing the ILP hardware is to decentralize the critical resources in the system. <p> Many of these artificial dependencies can be eliminated with software register renaming. The idea behind software register renaming is to use a unique architectural register for each assignment in the window, in similar spirit to static single assignment <ref> [38] </ref>. In the above code, by replacing the assignment to register R1 by an assignment to register R5, both the anti- and output dependencies are eliminated, allowing I 3 to be scheduled before I 2 and I 1 as shown below.
Reference: [39] <author> J. Dennis, </author> <title> ``Data Flow Supercomputers,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 48-56, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: The recent advent of more restricted forms of dataflow architectures [37, 77, 109, 113, 114] bear testimony to this fact. The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures <ref> [39, 158] </ref>, to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines [24, 77].
Reference: [40] <author> K. Diefendorff and M. Allen, </author> <title> ``Organization of the Motorola 88110 Superscalar RISC Microprocessor,'' </title> <booktitle> IEEE Micro, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: These advantages have led to the adoption of small-issue superscalar processors as the paradigm of choice for several modern microprocessors <ref> [35, 40, 123] </ref>.
Reference: [41] <author> M. Dubois, C. Scheurich, and F. Briggs, </author> <title> ``Memory Access Buffering in Multiprocessors,'' </title> <booktitle> Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Memory renaming and the potential offered by the memory renaming capability of the ARB are also research areas that need further investigation. Finally, we would like to extend the ARB so as to enforce different memory consistency models such as weak consistency <ref> [41] </ref>, data-race-free-0 (DRF0) [5], data-race-free-1 (DRF1) [6], processor consistency [65], and sequential consistency [91] in a multiprocessor system. 9.2.4.
Reference: [42] <author> M. Dubois and C. Scheurich, </author> <title> ``Memory Access Dependencies in Shared-Memory Multiprocessor,'' </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-16, </volume> <pages> pp. 660-673, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We shall discuss them separately. A.3.1. Detection of Data Races When the individual processors in a multiprocessor system perform dynamic reordering of memory references, sequential consistency may be violated in the multiprocessor unless special care is taken <ref> [42] </ref>. The ARB can be used to detect potential violations to sequential consistency (by detecting data races as and when they occur) in multiprocessors consisting of dynamically scheduled processor nodes. We shall briefly describe how this can be done.
Reference: [43] <author> H. Dwyer, </author> <title> ``A Multiple, Out-of-Order, Instruction Issuing System for Superscalar Processors,'' </title> <type> Ph.D. Thesis, </type> <institution> School of Electrical Engineering, Cornell University, </institution> <year> 1991. </year>
Reference-contexts: This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm [153], decoupled execution [134], register update unit (RUU) [147], dispatch stack <ref> [43, 44] </ref>, deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. Scoreboard: Scoreboard is a hardware technique for dynamic scheduling, originally used in the CDC 6600 [151], which issued instructions one at a time.
Reference: [44] <author> H. Dwyer and H. C. Torng, </author> <title> ``An Out-of-Order Superscalar Processor with Speculative Execution and Fast, Precise Interrupts,'' </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture (MICR-25), </booktitle> <pages> pp. 272-281, </pages> <year> 1992. </year>
Reference-contexts: This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm [153], decoupled execution [134], register update unit (RUU) [147], dispatch stack <ref> [43, 44] </ref>, deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. Scoreboard: Scoreboard is a hardware technique for dynamic scheduling, originally used in the CDC 6600 [151], which issued instructions one at a time.
Reference: [45] <author> K. Ebcioglu, </author> <title> ``A Compilation Technique for Software Pipelining of Loops with Conditional Jumps,'' </title> <booktitle> Proceedings of the 20th Annual Workshop on Microprogramming (Micro 20), </booktitle> <pages> pp. 69-79, </pages> <year> 1987. </year>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining <ref> [45, 89, 102, 103, 161] </ref>, perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [46] <author> K. Ebcioglu, </author> <title> ``Some Design Ideas for a VLIW Architecture for Sequential-natured Software,'' </title> <booktitle> Parallel Processing (Proceedings of IFIP WG 10.3 Working Conference on Parallel Processing), </booktitle> <pages> pp. 3-21, </pages> <year> 1988. </year>
Reference-contexts: Serious thought is being given to expanding the abilities of the VLIW model to allow the execution of multiple branches per cycle; several proposals have been made <ref> [46, 83, 100, 162] </ref>. The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams [162]. <p> The processor coupling approach is similar in spirit to XIMD, but it allows dynamic interleaving of multiple threads to tolerate long memory latencies [83]. In the IBM VLIW research project <ref> [46] </ref>, the form of a VLIW instruction is expanded to that of a directed binary tree, with provision to express conditional operations that depend on multiple condition codes. The effectiveness of these approaches for applications that are not amenable to static analysis techniques (to extract parallelism) is not clear. 2.4.5.
Reference: [47] <author> K. Ebcioglu and T. Nakatani, </author> <title> ``A new Compilation Technique for Parallelizing Loops with Unpredictable Branches on a VLIW Architecture,'' </title> <booktitle> Proceedings of Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 213-229, </pages> <year> 1989. </year> <month> 188 </month>
Reference-contexts: A complete set of semantics-preserving transformations for moving operations between adjacent blocks is described in [107]. Ebcioglu and Nakatani describe an enhanced implementation of percolation scheduling for VLIW machines having conditional evaluation capabilities <ref> [47] </ref>. In their implementation of percolation scheduling, a code motion across a large number of basic blocks is allowed only if each of the pair-wise transformations is beneficial. Superblock Scheduling: Superblock scheduling [27, 28] is a variant of trace scheduling. <p> Figure 2.3 (i) shows an example loop; Figure 2.3 (ii) shows the loop unrolled three times. Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining [10], enhanced pipeline scheduling <ref> [47] </ref>, GURPR* [149], modulo scheduling [48, 124], and polycyclic scheduling [125]. Boosting: Boosting is a technique for statically specifying speculative execution [141-143].
Reference: [48] <author> C. Eisenbeis, </author> <title> ``Optimization of Horizontal Microcode Generation for Loop Structures,'' </title> <booktitle> International Conference on Supercomputing, </booktitle> <pages> pp. 453-465, </pages> <year> 1988. </year>
Reference-contexts: Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining [10], enhanced pipeline scheduling [47], GURPR* [149], modulo scheduling <ref> [48, 124] </ref>, and polycyclic scheduling [125]. Boosting: Boosting is a technique for statically specifying speculative execution [141-143].
Reference: [49] <author> J. R. Ellis, Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time. <p> For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays [62, 97]. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler <ref> [14, 49, 54, 86, 94] </ref>. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis. Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. <p> Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers [30, 67, 70, 92]. Although recent work has suggested that some compile-time analysis could be done with suitable annotations from the programmer <ref> [49, 68, 72] </ref>, it appears that accurate static disambiguation for arbitrary programs written in arbitrary languages with pointers is unlikely without considerable advances in compiler technology. 23 Once the dependencies in the window are determined, the dependencies can be minimized by techniques such as software register renaming (if register allocation has <p> Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths. Originally developed for microcode compaction [53], trace scheduling later found application in ILP processing <ref> [34, 49] </ref>. Its methodology in the context of ILP processing is as follows: The compiler forms the operation window by selecting from an acyclic part of the CFG the most likely path, called trace, that will be taken at run time. <p> Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [50] <author> P. G. Emma, J. W. Knight, J. H. Pomerene, R. N. Rechtschaffen, and F. J. Shapiro, ``Posting Out-of-Sequence Fetches,'' United States Patent 4,991,090, </author> <month> February 5, </month> <year> 1991. </year>
Reference-contexts: the memory system have their addresses compared with the addresses in the store queue, and if there is an address match the load is not allowed to proceed. (This compare can be done after fetching the data from memory too, as in the IBM System/370 Model 168.) Emma, et al <ref> [50] </ref> proposed a similar mechanism, which in addition to executing loads before preceding stores, supports sequential consistency in a multiprocessor system by allowing each processor to monitor the stores made by the remaining processors.
Reference: [51] <author> J. Ferrante, K. Ottenstein, and J. Warren, </author> <title> ``The Program Dependence Graph and Its Use in Optimization,'' </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pp. 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: A more complex window, which contains instructions from multiple threads of control might be needed; analysis of the control dependence graph <ref> [38, 51] </ref> of a program can aid in the selection of the threads of control. Another major challenge in designing the ILP hardware is to decentralize the critical resources in the system. <p> In a control flow graph, a node X is said to be post-dominated by a node Y if every directed path from X to STOP (not including X) contains Y <ref> [51] </ref>. A third type of representation is the program dependence graph (PDG), which explicitly represents the control dependencies and data dependencies for every operation in a program [51, 76]. <p> A third type of representation is the program dependence graph (PDG), which explicitly represents the control dependencies and data dependencies for every operation in a program <ref> [51, 76] </ref>. The control dependencies are represented by a subgraph called control dependence graph (CDG), and the data dependencies are represented by another subgraph called data dependence graph (DDG). Control dependencies are derived from the control flow graph by constructing its post-dominator tree [51]. <p> The control dependencies are represented by a subgraph called control dependence graph (CDG), and the data dependencies are represented by another subgraph called data dependence graph (DDG). Control dependencies are derived from the control flow graph by constructing its post-dominator tree <ref> [51] </ref>. Figure 2.1 (ii) shows the CDG of the CFG shown in Figure 2.1 (i). Notice that control parallelism exists between basic blocks 1 and 4, i.e., these two blocks are control independent. Data dependencies are derived by performing a data flow analysis of the program [7]. 2.2.
Reference: [52] <author> C. N. Fischer and R. J. LeBlanc, Jr., </author> <title> Crafting A Compiler. </title> <address> Menlo Park, CA: </address> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1988. </year>
Reference-contexts: All optimizing compilers invariably do dataflow analysis <ref> [7, 52] </ref>; the 97 create mask is similar to the def variables computed by these compilers for each basic block, except that the former represents architectural registers and the latter represent variables of the source program.
Reference: [53] <author> J. A. Fisher, </author> <title> ``Trace Scheduling: A Technique for Global Microcode Compaction,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Global scheduling is more useful, as they consider large operation windows. Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling <ref> [53] </ref>, percolation scheduling [57, 107], superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. <p> Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths. Originally developed for microcode compaction <ref> [53] </ref>, trace scheduling later found application in ILP processing [34, 49]. Its methodology in the context of ILP processing is as follows: The compiler forms the operation window by selecting from an acyclic part of the CFG the most likely path, called trace, that will be taken at run time. <p> Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [54] <author> J. A. Fisher, </author> <title> ``Very Long Instruction Word Architectures and the ELI-512,'' </title> <booktitle> Proceedings of 10th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 140-150, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays [62, 97]. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler <ref> [14, 49, 54, 86, 94] </ref>. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis. Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. <p> Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [55] <author> J. A. Fisher, </author> <title> ``The VLIW Machine: A Multiprocessor for Compiling Scientific Code,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 45-53, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: We feel that numeric programs have received substantial attention in the past, whereas non-numerical programs have received only a passing attention. The ILP machines developed so far vector machines, the primal VLIW machines <ref> [55] </ref>, etc. have all specifically targeted numeric applications.
Reference: [56] <author> J. A. Fisher, </author> <title> ``A New Architecture for Supercomputing,'' </title> <booktitle> Digest of Papers, COMPCON Spring 1987, </booktitle> <pages> pp. 177-180, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [57] <author> C. C. Foster and E. M. Riseman, </author> <title> ``Percolation of Code to Enhance Parallel Dispatching and Execution,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-21, </volume> <pages> pp. 1411-1415, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: Global scheduling is more useful, as they consider large operation windows. Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling <ref> [57, 107] </ref>, superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95].
Reference: [58] <author> M. Franklin and G. S. Sohi, </author> <title> ``The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 58-67, </pages> <year> 1992. </year> <month> 189 </month>
Reference-contexts: The name multiscalar is derived from the fact that the overall computing engine is a collection of scalar processors that cooperate in the execution of a sequential program. In the initial phases of this research, the multiscalar paradigm was called the Expandable Split Window (ESW) paradigm <ref> [58] </ref>. This chapter is organized in five sections. The first section describes our view of an ideal processing paradigm. Naturally, the attributes mentioned in section 3.1 had a significant impact on the development of the multiscalar paradigm and later became the driving force behind an implementation of the paradigm.
Reference: [59] <author> M. Franklin and G. S. Sohi, </author> <title> ``Register Traffic Analysis for Streamlining Inter-Operation Communication in Fine-Grain Parallel Processors,'' </title> <booktitle> Proceedings of The 25th Annual International Symposium on Microarchitecture (MICRO-25), </booktitle> <pages> pp. 236-145, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: The inter-unit part may or may not be decentralized depending on whether the communication it handles is localized or global in nature. For instance, the inter-unit register communication is easily decentralized (c.f. section 5.3) because much of the register communication in the multiscalar processor is localized <ref> [59] </ref>. The building blocks of the multiscalar processor are described in great detail in the following three chapters. Chapter 4 describes some of the major parts such as the execution units, the control mechanism, and the instruction supply mechanism. <p> Since most of the register instances are used up either in the same task in which they are created or in the subsequent task, we can expect a significant reduction in the forwarding traffic because of the create mask, as shown in the empirical results of <ref> [59] </ref>. Notice also that register values from several units can simultaneously be traveling to their subsequent units in a pipelined fashion. <p> Exploitation of Communication Localities Not only does the multi-version register file decentralize the inter-instruction communication, but also does it exploit localities of communication. Because a significant number of register instances are used up soon after they are created <ref> [59] </ref>, most of the inter-instruction communication occurring in the multiscalar processor falls under intra-task communication, which can be done in 107 parallel in multiple units. Empirical results presented in [59] show that most of the register operands are generated either in the same task in which they are used or in <p> Because a significant number of register instances are used up soon after they are created <ref> [59] </ref>, most of the inter-instruction communication occurring in the multiscalar processor falls under intra-task communication, which can be done in 107 parallel in multiple units. Empirical results presented in [59] show that most of the register operands are generated either in the same task in which they are used or in the preceding task. <p> The performance penalty would decrease further if the compiler takes into consideration the inter-task register dependencies when generating the tasks. It is worthwhile to refer to some results that we had published earlier <ref> [59] </ref>, showing the efficacy of the multi-version register file. It was shown that a large fraction of the operands were generated in the same task or came from at most 3 tasks in the past.
Reference: [60] <author> P. P. Gelsinger, P. A. Gargini, G. H. Parker, and A. Y. C. Yu, </author> <title> ``Microprocessors circa 2000,'' </title> <journal> IEEE Spectrum, </journal> <volume> vol. 26, </volume> <pages> pp. 43-47, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Technology projections even suggest the integration of 100 million transistors on a chip by the year 2000 <ref> [60] </ref>, in place of the 2-4 million transistors that are integrated today. This has prompted computer architects to consider new ways of utilizing the additional resources for doing parallel processing. The execution of a computer program involves computation operations and communication of values, constrained by control structures in the program.
Reference: [61] <author> K. Gharachorloo and P. B. Gibbons, </author> <title> ``Detecting Violations of Sequential Consistency,'' </title> <booktitle> 3rd Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA'91), </booktitle> <pages> pp. 316-326, </pages> <year> 1991. </year>
Reference-contexts: The use of ARBs for the detection of data races is similar to the use of detection buffers proposed by Gharachorloo and Gibbons <ref> [61] </ref>. 179 Example A.3: Consider the two pseudo-assembly program segments given in Table A.2 to be executed concurrently by two processors P1 and P2 in a shared-memory multiprocessor. Initially, location 100 contains 30 and location 120 contains 20.
Reference: [62] <author> G. Goff, K. Kennedy, and C.-W. Tseng, </author> <title> ``Practical Dependence Testing,'' </title> <booktitle> Proceedings of 1991 SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pp. 15-29, </pages> <year> 1991. </year>
Reference-contexts: Otherwise, the two references are guaranteed not to conflict. For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays <ref> [62, 97] </ref>. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler [14, 49, 54, 86, 94]. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis.
Reference: [63] <author> J. R. Goodman, J. T. Hsieh, K. Liou, A. R. Pleszkun, P. B. Schecter, and H. C. Young, </author> <title> ``PIPE: a Decoupled Architecture for VLSI,'' </title> <booktitle> Proceedings of 12th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 20-27, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Several decoupled architectures have been proposed, and some have been built. The noted ones are the MAP-200 [33], the DAE [136], the PIPE <ref> [63] </ref>, the ZS-1 [139], and the WM architecture [165]. A drawback in decou-pled architectures is AP-EP code unbalance. When there is unbalance, the sustained instruction issue rate is low because of poor utilization.
Reference: [64] <author> J. R. Goodman and P. J. Woest, </author> <title> ``The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor,'' </title> <booktitle> Proceedings of 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: If the transferred task is the body of a loop, the local caches of the subsequent units can also grab a copy of the task in parallel, much like the snarfing (read broadcast) scheme pro posed for multiprocessor caches <ref> [64] </ref>. Notice that several IE units can simultaneously be fetching instructions from their correspond ing local instruction caches, and several local caches can simultaneously be receiving a task (if the task is the body of a loop) from the global cache, in any given cycle.
Reference: [65] <author> J. R. Goodman, </author> <title> ``Cache Consistency and Sequential Consistency,'' </title> <type> Technical Report No. 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Finally, we would like to extend the ARB so as to enforce different memory consistency models such as weak consistency [41], data-race-free-0 (DRF0) [5], data-race-free-1 (DRF1) [6], processor consistency <ref> [65] </ref>, and sequential consistency [91] in a multiprocessor system. 9.2.4. Multi-Version Data Cache A major challenge that we face with the ARB connected as described in chapter 6 is the latency of the interconnection network connecting the execution units to the ARB and the data cache.
Reference: [66] <author> G. F. Grohoski, </author> <title> ``Machine Organization of the IBM RISC System/6000 processor,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 37-58, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming <ref> [66, 73, 116, 147, 153] </ref>. 2.3.3.3. Scheduling Instructions In parallel to establishing a window and enforcing the register and memory dependencies, the hardware performs scheduling of ready-to-execute instructions. Instructions that are speculatively fetched from beyond unresolved branches are executed speculatively, i.e., before determining that their execution is needed. <p> There have been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 <ref> [66, 111] </ref>, decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116]. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism.
Reference: [67] <author> L. J. Hendren and G. R. Gao, </author> <title> ``Designing Programming Languages for Analyzability: A Fresh Look at Pointer Data Structures,'' </title> <booktitle> Proceedings of 4th IEEE International Conference on Computer Languages, </booktitle> <pages> pp. 242-251, </pages> <year> 1992. </year> <month> 190 </month>
Reference-contexts: Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers <ref> [30, 67, 70, 92] </ref>.
Reference: [68] <author> L. J. Hendren, J. Hummel, and A. Nicolau, </author> <title> ``Abstractions for Recursive Pointer Data Structure: Improving the Analysis and Transformatio ns of Imperative Programs,'' </title> <booktitle> Proceedings of SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 249-260, </pages> <year> 1992. </year>
Reference-contexts: Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers [30, 67, 70, 92]. Although recent work has suggested that some compile-time analysis could be done with suitable annotations from the programmer <ref> [49, 68, 72] </ref>, it appears that accurate static disambiguation for arbitrary programs written in arbitrary languages with pointers is unlikely without considerable advances in compiler technology. 23 Once the dependencies in the window are determined, the dependencies can be minimized by techniques such as software register renaming (if register allocation has
Reference: [69] <author> M. D. Hill and A. J. Smith, </author> <title> ``Evaluating Associativity in CPU Caches,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: For instance, if the number of units is 12, the interleaving factor used is 32. 7.1.3. Benchmarks and Performance Metrics For benchmarks, we used the SPEC '92 benchmark suite [104] and some other programs, namely Tycho (a sophisticated cache simulator) <ref> [69] </ref> and Sunbench (an Object Oriented Database benchmark written in C++) [26]. The benchmarks and their features are listed in Table 7.1. Although our emphasis is on non-numeric programs, we have included some numeric benchmark programs for the sake of comparison.
Reference: [70] <author> S. Horwitz, P. Pfeiffer, and T. Reps, </author> <title> ``Dependence Analysis for Pointer Variables,'' </title> <booktitle> Proceedings of SIGPLAN'89 Symposium on Compiler Construction, </booktitle> <pages> pp. 28-40, </pages> <month> July </month> <year> 1989. </year> <note> Published as SIGPLAN Notices Vol. 24, Num. 7. </note>
Reference-contexts: Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers <ref> [30, 67, 70, 92] </ref>.
Reference: [71] <author> P. Y. T. Hsu and E. S. Davidson, </author> <title> ``Highly Concurrent Scalar Processing,'' </title> <booktitle> Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 386-395, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency.
Reference: [72] <author> J. Hummel, L. J. Hendren, and A. Nicolau, </author> <title> ``Applying an Abstract Data Structure Description Approach to Parallelizing Scientific Pointer Programs,'' </title> <booktitle> Proceedings of 1992 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 100-104, </pages> <year> 1992. </year>
Reference-contexts: Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers [30, 67, 70, 92]. Although recent work has suggested that some compile-time analysis could be done with suitable annotations from the programmer <ref> [49, 68, 72] </ref>, it appears that accurate static disambiguation for arbitrary programs written in arbitrary languages with pointers is unlikely without considerable advances in compiler technology. 23 Once the dependencies in the window are determined, the dependencies can be minimized by techniques such as software register renaming (if register allocation has
Reference: [73] <author> W. W. Hwu and Y. N Patt, ``HPSm, </author> <title> a High Performance Restricted Data Flow Architecture Having Minimal Functionality,'' </title> <booktitle> Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 297-307, </pages> <year> 1986. </year>
Reference-contexts: With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming <ref> [66, 73, 116, 147, 153] </ref>. 2.3.3.3. Scheduling Instructions In parallel to establishing a window and enforcing the register and memory dependencies, the hardware performs scheduling of ready-to-execute instructions. Instructions that are speculatively fetched from beyond unresolved branches are executed speculatively, i.e., before determining that their execution is needed. <p> If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. <p> been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS <ref> [73, 116] </ref>. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism.
Reference: [74] <author> W. W. Hwu and Y. N. Patt, </author> <title> ``Checkpoint Repair for High-Performance Out-of-Order Execution Machines,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1496-1514, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity.
Reference: [75] <author> W. W. Hwu, T. M. Conte, and P. P. Chang, </author> <title> ``Comparing Software and Hardware Schemes for Reducing the Cost of Branches,'' </title> <booktitle> Proceedings of 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 224-233, </pages> <year> 1989. </year> <month> 191 </month>
Reference-contexts: Initial static 20 prediction schemes were based on branch opcodes, and were not accurate. Now, static prediction schemes are much more sophisticated, and use profile information or heuristics to take decisions <ref> [28, 75, 98, 118, 160] </ref>. In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies.
Reference: [76] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery, </author> <title> ``The Super-block: An Effective Technique for VLIW and Superscalar Compilation,'' </title> <note> (To appear in) Journal of Supercomputing, </note> <year> 1993. </year>
Reference-contexts: A third type of representation is the program dependence graph (PDG), which explicitly represents the control dependencies and data dependencies for every operation in a program <ref> [51, 76] </ref>. The control dependencies are represented by a subgraph called control dependence graph (CDG), and the data dependencies are represented by another subgraph called data dependence graph (DDG). Control dependencies are derived from the control flow graph by constructing its post-dominator tree [51].
Reference: [77] <author> R. A. </author> <title> Iannucci, ``Toward a Dataflow / von Neumann Hybrid Architecture,'' </title> <booktitle> Proceedings of 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 131-140, </pages> <year> 1988. </year>
Reference-contexts: Data-Driven Specification with Control-Driven Tasks: In an attempt to capture the benefits of control-driven specification, researchers have proposed using control-driven specification at the lower level with data-driven specification at the higher level <ref> [77, 113] </ref>. In this specification, the dataflow graph is partitioned into different regions, each of which is scheduled independently at compile time. Within a region, control flows sequentially, whereas the execution of a region is triggered by inter-region data availability. 2.2.2. <p> A large performance penalty is incurred due to the ``stretching'' of the communication arcs of the dataflow graph, which manifests as associative searches through the entire program using matching hardware. The recent advent of more restricted forms of dataflow architectures <ref> [37, 77, 109, 113, 114] </ref> bear testimony to this fact. <p> The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines <ref> [24, 77] </ref>. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [78] <author> W. M. Johnson, </author> <title> Superscalar Microprocessor Design. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes <ref> [78] </ref>. Scoreboard: Scoreboard is a hardware technique for dynamic scheduling, originally used in the CDC 6600 [151], which issued instructions one at a time. However, the scoreboard technique can be extended to issue multiple instructions per cycle.
Reference: [79] <author> R. Jolly, ``A 9-ns 1.4 Gigabyte/s, </author> <title> 17-Ported CMOS Register File,'' </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 26, </volume> <pages> pp. 1407-1412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The common way of providing this bandwidth is to use a multi-ported register file. Whereas a few multi-ported register files have been built, for example the register files for the Cydra 5 [127], the SIMP processor [101], Intel's iWarp <ref> [79] </ref>, and the XIMD processor [162], centralized, multi-ported register files do not appear to be a good long-term solution, as its design becomes very complex for large values of I. Therefore, decentralization of the inter-operation communication mechanism is essential for future ILP processors.
Reference: [80] <author> N. P. Jouppi and D. W. Wall, </author> <title> ``Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pp. 272-282, </pages> <year> 1989. </year>
Reference-contexts: The effectiveness of these approaches for applications that are not amenable to static analysis techniques (to extract parallelism) is not clear. 2.4.5. The Superscalar Paradigm An alternate paradigm that was developed at the same time as the VLIW paradigm (early 80's) was the superscalar paradigm <ref> [80, 101, 111, 116] </ref>. The superscalar paradigm attempts to bring together the good aspects of control-driven specification and data-driven execution, by doing data-driven execution within a window of instructions established by control-driven fetching along a single flow of control. Parallelism may be extracted at compile time and at run time.
Reference: [81] <author> D. R. Kaeli and P. G. Emma, </author> <title> ``Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns,'' </title> <booktitle> Proceedings of 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 34-42, </pages> <year> 1991. </year>
Reference-contexts: For effectively predicting the return addresses, the global control unit uses a stack-like mechanism similar to the one discussed in <ref> [81] </ref>. When the predictor predicts a procedure call as the next task to be executed, the return address is pushed onto the stack 81 like mechanism. When the predictor predicts a procedure return as the next task to be executed, the return address is popped from the stack.
Reference: [82] <author> G. Kane, </author> <title> MIPS R2000 RISC Architecture. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: The multiscalar simulator is described below in detail. 7.1.1. Multiscalar Simulator The multiscalar simulator is the heart of the experimental framework. It uses the MIPS R2000 - R2010 instruction set and functional unit latencies <ref> [82] </ref>. All important features of the multiscalar processor, such as the control unit, the execution units, the distributed instruction caches, the ARB, and the data cache, are included in the simulator. The simulator accepts executable images of programs (with task demarcations), and simulates their execution; it is not trace driven.
Reference: [83] <author> S. W. Keckler and W. J. Dally, </author> <title> ``Processor Coupling: Integrating Compile Time and Runtime Scheduling for Parallelism,'' </title> <booktitle> Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 202-213, </pages> <year> 1992. </year>
Reference-contexts: Serious thought is being given to expanding the abilities of the VLIW model to allow the execution of multiple branches per cycle; several proposals have been made <ref> [46, 83, 100, 162] </ref>. The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams [162]. <p> The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams [162]. The processor coupling approach is similar in spirit to XIMD, but it allows dynamic interleaving of multiple threads to tolerate long memory latencies <ref> [83] </ref>. In the IBM VLIW research project [46], the form of a VLIW instruction is expanded to that of a directed binary tree, with provision to express conditional operations that depend on multiple condition codes.
Reference: [84] <author> R. M. Keller, </author> <title> ``Look-Ahead Processors,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 7, </volume> <pages> pp. 66-72, </pages> <month> De-cember </month> <year> 1975. </year> <month> 192 </month>
Reference-contexts: A hardware solution to decrease such storage conflicts is to provide additional physical registers, which are then dynamically allocated by hardware register renaming techniques <ref> [84] </ref>. With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming [66, 73, 116, 147, 153]. 2.3.3.3.
Reference: [85] <author> D. J. Kuck, A. H. Sameh, R. Cytron, A. V. Veidenbaum, C. D. Polychronopoulos, G. Lee, T. McDaniel, B. R. Leasure, C. Beckman, J. R. B. Davies, and C. P. Kruskal, </author> <title> ``The Effects of Program Restructuring, Algorithm Change, and Architecture Choice on Program performance,'' </title> <booktitle> Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pp. 129-138, </pages> <year> 1984. </year>
Reference-contexts: In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies. Some of these techniques are if-conversion, loop unrolling, loop peeling, loop conditioning, loop exchange [12], function inlining <ref> [28, 85] </ref>, replacing a set of IF-THEN statements by a jump table [129], and even changing data structures. All these techniques modify the CFG of the program, mostly by reducing the number of control decision points in the CFG.
Reference: [86] <author> D. J. Kuck, E. S. Davidson, D. H. Lawrie, and A. H. Sahmeh, </author> <title> ``Parallel Supercomputing Today and the Cedar Approach,'' </title> <journal> Science, </journal> <volume> vol. 231, </volume> <month> February </month> <year> 1986. </year>
Reference-contexts: For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays [62, 97]. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler <ref> [14, 49, 54, 86, 94] </ref>. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis. Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers.
Reference: [87] <author> M. Kuga, K. Murakami, and S. Tomita, ``DSNS (Dynamically-hazard-resolved, Statically-code-scheduled, </author> <title> Nonuniform Superscalar): Yet Another Superscalar Processor Architecture,'' </title> <journal> Computer Architecture News, </journal> <volume> vol. 19, </volume> <pages> pp. 14-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [88] <author> H. T. Kung, </author> <title> ``Why Systolic Architectures?,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 17, </volume> <pages> pp. 45-54, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: The Systolic Paradigm and Other Special Purpose Paradigms Several special purpose paradigms have been proposed over the years to exploit localities of communication in some way or other. The noted one among them is the systolic paradigm <ref> [88] </ref>, which is excellent in exploiting ILP in numeric applications such as signal processing and computer vision [16, 23]. Some specialized computers even hard-wire FFTs and other important algorithms to 42 give tremendous performance for signal processing applications.
Reference: [89] <author> M. S. Lam, </author> <title> ``Software Pipelining: An Effective Scheduling Technique for VLIW Machines,'' </title> <booktitle> Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 318-328. </pages>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining <ref> [45, 89, 102, 103, 161] </ref>, perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [90] <author> M. S. Lam and R. P. Wilson, </author> <title> ``Limits of Control Flow on Parallelism,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 46-57, </pages> <year> 1992. </year>
Reference-contexts: Second, studies have found little ILP within a small sequential block of instructions, but significant amounts in large blocks <ref> [18, 25, 90, 159] </ref>. There are several inter-related factors that contribute to this. <p> Recent work has suggested that, given the basic block sizes and branch prediction accuracies for some common C programs, following a single thread of control while establishing a window may not be sufficient: the maximum ILP that can be extracted from such a window is limited to about 7 <ref> [90] </ref>. A more complex window, which contains instructions from multiple threads of control might be needed; analysis of the control dependence graph [38, 51] of a program can aid in the selection of the threads of control. <p> Such a division into tasks will not only allow the overall large window to be accurate, but also facilitate the execution of (mostly) control-independent code in parallel, thereby allowing multiple flows of control, which is 64 needed to exploit significant levels of ILP in non-numeric applications <ref> [90] </ref>. By encompassing complex control structures within a task, the overall prediction accuracy is significantly improved. 3.3.3. Speculative Execution The multiscalar paradigm is an epitome for speculative execution; almost all of the execution in the multiscalar hardware is speculative in nature. <p> The importance of speculative execution for exploiting parallelism in non-numeric codes was underscored in <ref> [90] </ref>. 3.3.4. Parallel Execution of Data Dependent Tasks Another important feature and big advantage of the multiscalar paradigm is that it does not require the parallelly executed tasks to be data independent either. <p> If a task T has a single target, then the subsequent task is control-independent of T. Executing multiple, control-independent tasks in parallel allows the multiscalar processor to execute multiple, independent flows of control, which is needed to exploit significant levels of ILP in non-numeric applications <ref> [90] </ref>. However, most non-numeric programs have very complex control structures, and constructing reasonable size tasks with single targets may not be possible most of the time. <p> The multiscalar processor appears to have all the features desirable/essential in an ILP processor as we understand them today; abstract machines with these capabilities appear to be capable of sustaining ILP in C programs far beyond 10+ IPC <ref> [18, 90] </ref>. The preliminary performance results are very optimistic in this regard. With no multiscalar-specific optimizations, a multiscalar processor configuration is able to sustain 1.73 to 3.74 useful instructions per cycle for our non-numeric benchmark programs, and 1.38 to 6.06 useful instructions per cycle for our numeric benchmark programs.
Reference: [91] <author> L. Lamport, </author> <title> ``How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-28, </volume> <pages> pp. 241-248, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Finally, we would like to extend the ARB so as to enforce different memory consistency models such as weak consistency [41], data-race-free-0 (DRF0) [5], data-race-free-1 (DRF1) [6], processor consistency [65], and sequential consistency <ref> [91] </ref> in a multiprocessor system. 9.2.4. Multi-Version Data Cache A major challenge that we face with the ARB connected as described in chapter 6 is the latency of the interconnection network connecting the execution units to the ARB and the data cache.
Reference: [92] <author> J. R. Larus and P. N. Hilfinger, </author> <title> ``Detecting Conflicts Between Structure Accesses,'' </title> <booktitle> Proceedings of SIGPLAN'88 Symposium on Compiler Construction, </booktitle> <pages> pp. 21-34, </pages> <month> July </month> <year> 1988. </year> <note> Published as SIGPLAN Notices Vol. 23, Num. 7. </note>
Reference-contexts: Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers. Hence they are useful mainly for a class of scientific programs. Ordinary nonnumerical applications, however, abound with unions and pointers <ref> [30, 67, 70, 92] </ref>.
Reference: [93] <author> J. K. F. Lee and A. J. Smith, </author> <title> ``Branch Prediction Strategies and Branch Target Buffer Design,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 17, </volume> <pages> pp. 6-22, </pages> <month> January </month> <year> 1984. </year> <month> 193 </month>
Reference-contexts: With speculative fetching, rather than waiting for the outcome of a conditional branch to be determined, the branch outcome is predicted, and operations from the predicted path are entered into the window for execution. Dynamic prediction techniques have significantly evolved over the years <ref> [93, 112, 133, 166] </ref>.
Reference: [94] <author> J. M. Levesque and J. W. Williamson, </author> <title> A Guidebook to Fortran on Supercomputers. </title> <address> San Diego, California: </address> <publisher> Academic Press, Inc., </publisher> <year> 1989. </year>
Reference-contexts: For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays [62, 97]. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler <ref> [14, 49, 54, 86, 94] </ref>. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis. Moreover, existing static techniques are suitable only for memory references involving array subscripts, and not suitable for those references involving data types such as unions and pointers.
Reference: [95] <author> S. A. Mahlke, W. Y. Chen, W. W. Hwu, B. R. Rau, and M. S. Schlansker, </author> <title> ``Sentinel Scheduling for VLIW and Superscalar Processors,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pp. 238-247, </pages> <year> 1992. </year>
Reference-contexts: These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling <ref> [95] </ref>. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths. Originally developed for microcode compaction [53], trace scheduling later found application in ILP processing [34, 49].
Reference: [96] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann, </author> <title> ``Effective Compiler Support for Predicated Execution using the Hyperblock,'' </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture (MICRO-25), </booktitle> <pages> pp. 45-54, </pages> <year> 1992. </year>
Reference-contexts: In order to reduce the effect of control dependencies, operations are speculatively moved above conditional branches. 27 Hyperblock Scheduling: In hyperblock scheduling, the operation window is a hyperblock. A hyperblock is an enhancement on superblock <ref> [96] </ref>. A hyperblock is a set of predicated basic blocks in which control may enter only from the top, but may exit from one or more points.
Reference: [97] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam, </author> <title> ``Efficient and Exact Data Dependence Analysis,'' </title> <booktitle> Proceedings of 1991 SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pp. 1-14, </pages> <year> 1991. </year>
Reference-contexts: Otherwise, the two references are guaranteed not to conflict. For arbitrary multidimensional arrays and complex array subscripts, unfortunately, these tests can often be too conservative; several techniques have been proposed to produce exact dependence relations for certain subclasses of multidimensional arrays <ref> [62, 97] </ref>. Good static memory disambiguation is fundamental to the success of any parallelizing/vectorizing/optimizing compiler [14, 49, 54, 86, 94]. Existing static disambiguation techniques are limited to single procedures, and do not perform much of inter-procedural analysis.
Reference: [98] <author> S. McFarling and J. Hennessy, </author> <title> ``Reducing the Cost of Branches,'' </title> <booktitle> Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 396-304, </pages> <year> 1986. </year>
Reference-contexts: Initial static 20 prediction schemes were based on branch opcodes, and were not accurate. Now, static prediction schemes are much more sophisticated, and use profile information or heuristics to take decisions <ref> [28, 75, 98, 118, 160] </ref>. In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies.
Reference: [99] <author> K. Miura and K. Uchida, </author> <title> ``FACOM Vector Processor VP-100/VP-200,'' </title> <booktitle> in Proc. NATO Advanced Research Workshop on High-Speed Computing, Julich, </booktitle> <address> Germany, </address> <publisher> Springer-Verlag, </publisher> <month> June 20-22, </month> <year> 1983. </year>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines [2, 3, 130, 131], CDC Cyber-205 [1], and Fujitsu VP-200 <ref> [99] </ref>. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131]. Vector machines perform remarkably well in executing codes that fit the vector paradigm [157].
Reference: [100] <author> S-M. Moon and K. Ebcioglu, </author> <title> ``An Efficient Resource-Constrained Global Scheduling Technique for Superscalar and VLIW Processors,'' </title> <booktitle> Proceedings of The 25th Annual International Symposium on Microarchitecture (MICRO-25), </booktitle> <pages> pp. 55-71, </pages> <year> 1992. </year>
Reference-contexts: With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes <ref> [100, 126] </ref>. <p> Serious thought is being given to expanding the abilities of the VLIW model to allow the execution of multiple branches per cycle; several proposals have been made <ref> [46, 83, 100, 162] </ref>. The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams [162]. <p> previous scheme, namely, (i) a check instruction is needed for every ambiguous load-store pair, and (ii) when a store is 116 performed, in the worst case, an associative compare has to be performed among all general purpose registers, which in a VLIW processor can be as much as 128 registers <ref> [100] </ref>. 6.3.2.
Reference: [101] <author> K. Murakami, N. Irie, M. Kuga, and S. Tomita, </author> <title> ``SIMP (Single Instruction Stream / Multiple Instruction Pipelining): A Novel High-Speed Single-Processor Architecture,'' </title> <booktitle> Proceedings of 16th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 78-85, </pages> <year> 1989. </year>
Reference-contexts: The common way of providing this bandwidth is to use a multi-ported register file. Whereas a few multi-ported register files have been built, for example the register files for the Cydra 5 [127], the SIMP processor <ref> [101] </ref>, Intel's iWarp [79], and the XIMD processor [162], centralized, multi-ported register files do not appear to be a good long-term solution, as its design becomes very complex for large values of I. Therefore, decentralization of the inter-operation communication mechanism is essential for future ILP processors. <p> The effectiveness of these approaches for applications that are not amenable to static analysis techniques (to extract parallelism) is not clear. 2.4.5. The Superscalar Paradigm An alternate paradigm that was developed at the same time as the VLIW paradigm (early 80's) was the superscalar paradigm <ref> [80, 101, 111, 116] </ref>. The superscalar paradigm attempts to bring together the good aspects of control-driven specification and data-driven execution, by doing data-driven execution within a window of instructions established by control-driven fetching along a single flow of control. Parallelism may be extracted at compile time and at run time. <p> There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [102] <author> T. Nakatani and K. Ebcioglu, </author> <title> ````Combining'' as a Compilation Technique for VLIW Architectures,'' </title> <booktitle> Proceedings of the 22nd Annual International Workshop on Microprogramming and Microarchitecture (Micro 22), </booktitle> <pages> pp. 43-55, </pages> <year> 1989. </year> <month> 194 </month>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining <ref> [45, 89, 102, 103, 161] </ref>, perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [103] <author> T. Nakatani and K. Ebcioglu, </author> <title> ``Using a Lookahead Window in a Compaction-Based Parallel-izing Compiler,'' </title> <booktitle> Proceedings of the 23rd Annual Workshop on Microprogramming and Mi-croarchitecture (Micro 23), </booktitle> <pages> pp. 57-68, </pages> <year> 1990. </year>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining <ref> [45, 89, 102, 103, 161] </ref>, perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [104] <editor> SPEC Newsletter, </editor> <volume> vol. 1, </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: For instance, if the number of units is 12, the interleaving factor used is 32. 7.1.3. Benchmarks and Performance Metrics For benchmarks, we used the SPEC '92 benchmark suite <ref> [104] </ref> and some other programs, namely Tycho (a sophisticated cache simulator) [69] and Sunbench (an Object Oriented Database benchmark written in C++) [26]. The benchmarks and their features are listed in Table 7.1.
Reference: [105] <author> A. Nicolau, </author> <title> ``Parallelism, Memory Anti-Aliasing and Correctness for Trace Scheduling Compilers,'' </title> <type> Ph.D. Dissertation, </type> <institution> Yale University, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time.
Reference: [106] <author> A. Nicolau and J. A. Fisher, </author> <title> ``Measuring the Parallelism Available for Very Long Instruction Word Architectures,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-33, </volume> <pages> pp. 968-976, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Exploitation of parallelism at the instruction level is very important to speed up such programs. Recent studies have confirmed that there exists a large amount of instruction-level parallelism in ordinary programs <ref> [18, 25, 36, 106, 159] </ref>. Even in other applications, no matter how much parallelism is exploited by coarse-grain parallel processors such as the multiprocessors, a substantial amount of parallelism will still remain to be exploited at the instruction level.
Reference: [107] <author> A. Nicolau, </author> <title> ``Percolation Scheduling: A Parallel Compilation Technique,'' </title> <type> Technical Report TR 85-678, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1985. </year>
Reference-contexts: Global scheduling is more useful, as they consider large operation windows. Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling <ref> [57, 107] </ref>, superblock scheduling [28], software pipelining [45, 89, 102, 103, 161], perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. <p> Percolation Scheduling: Percolation scheduling considers a subgraph of the CFG (such as an entire function) as a window, using a system of semantics-preserving transformations for moving operations between adjacent blocks <ref> [107] </ref>. Repeated application of the transformations allows operations to ``percolate'' towards the top of the program, irrespective of their starting point. A complete set of semantics-preserving transformations for moving operations between adjacent blocks is described in [107]. <p> a window, using a system of semantics-preserving transformations for moving operations between adjacent blocks <ref> [107] </ref>. Repeated application of the transformations allows operations to ``percolate'' towards the top of the program, irrespective of their starting point. A complete set of semantics-preserving transformations for moving operations between adjacent blocks is described in [107]. Ebcioglu and Nakatani describe an enhanced implementation of percolation scheduling for VLIW machines having conditional evaluation capabilities [47]. In their implementation of percolation scheduling, a code motion across a large number of basic blocks is allowed only if each of the pair-wise transformations is beneficial.
Reference: [108] <author> A. Nicolau, </author> <title> ``Run-Time Disambiguation: Coping With Statically Unpredictable Dependencies,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <pages> pp. 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Realizing this, researchers have proposed schemes that allow ambiguous references to be statically reordered, with checks made at run time to determine if any dependencies are violated by the static code motions <ref> [31, 108, 132] </ref>. Ambiguous references that are statically reordered are called statically unresolved references. A limitation of this scheme, however, is that the run-time checks need extra code and in some schemes associative compare of store addresses with preceding load addresses in the active window. <p> We shall review the techniques proposed for dynamic disambiguation of such statically unresolved memory references that are reordered at compile time. Run-Time Disambiguation (RTD): The run-time disambiguation (RTD) scheme proposed by Nicolau <ref> [108] </ref> adds extra code in the form of explicit address comparison and conditional branch instructions to do run-time checking of statically unresolved references. Corrective action is taken when a violation is detected. This corrective action typically involves executing special recovery code to restore state and restart execution. <p> Although it is possible to extend the capabilities of a VLIW processor to allow statically unresolved references, such schemes typically have a high overhead <ref> [108] </ref>. Streamlined hardware mechanisms (the ARB) that dynamically detect memory data dependency violations and recover are an integral part of the multis-calar processor.
Reference: [109] <author> R. S. Nikhil and Arvind, </author> <title> ``Can Dataflow Subsume von Neumann Computing?,'' </title> <booktitle> Proceedings of 16th International Symposium on Computer Architecture, </booktitle> <pages> pp. 262-272, </pages> <year> 1989. </year>
Reference-contexts: A large performance penalty is incurred due to the ``stretching'' of the communication arcs of the dataflow graph, which manifests as associative searches through the entire program using matching hardware. The recent advent of more restricted forms of dataflow architectures <ref> [37, 77, 109, 113, 114] </ref> bear testimony to this fact. <p> The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon [113], P-RISC <ref> [109] </ref>, TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines [24, 77]. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [110] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind, </author> <title> ``*T: A Multithreaded Massively Parallel Architecture,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 156-167, </pages> <year> 1992. </year>
Reference-contexts: The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T <ref> [110] </ref>, and other hybrid machines [24, 77]. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [111] <author> R. R. Oehler and R. D. Groves, </author> <title> ``IBM RISC System/6000 Processor Architecture,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 23-36, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The effectiveness of these approaches for applications that are not amenable to static analysis techniques (to extract parallelism) is not clear. 2.4.5. The Superscalar Paradigm An alternate paradigm that was developed at the same time as the VLIW paradigm (early 80's) was the superscalar paradigm <ref> [80, 101, 111, 116] </ref>. The superscalar paradigm attempts to bring together the good aspects of control-driven specification and data-driven execution, by doing data-driven execution within a window of instructions established by control-driven fetching along a single flow of control. Parallelism may be extracted at compile time and at run time. <p> There have been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 <ref> [66, 111] </ref>, decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116]. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism.
Reference: [112] <author> S.-T. Pan, K. So, and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pp. 76-84, </pages> <year> 1992. </year> <month> 195 </month>
Reference-contexts: With speculative fetching, rather than waiting for the outcome of a conditional branch to be determined, the branch outcome is predicted, and operations from the predicted path are entered into the window for execution. Dynamic prediction techniques have significantly evolved over the years <ref> [93, 112, 133, 166] </ref>.
Reference: [113] <author> G. M. Papadopoulos and D. E. Culler, ``Monsoon: </author> <title> An Explicit Token-Store Architecture,'' </title> <booktitle> Proceedings of 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 82-91, </pages> <year> 1990. </year>
Reference-contexts: Data-Driven Specification with Control-Driven Tasks: In an attempt to capture the benefits of control-driven specification, researchers have proposed using control-driven specification at the lower level with data-driven specification at the higher level <ref> [77, 113] </ref>. In this specification, the dataflow graph is partitioned into different regions, each of which is scheduled independently at compile time. Within a region, control flows sequentially, whereas the execution of a region is triggered by inter-region data availability. 2.2.2. <p> A large performance penalty is incurred due to the ``stretching'' of the communication arcs of the dataflow graph, which manifests as associative searches through the entire program using matching hardware. The recent advent of more restricted forms of dataflow architectures <ref> [37, 77, 109, 113, 114] </ref> bear testimony to this fact. <p> The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures [39, 158], to the more recent explicit token store machines such as the Monsoon <ref> [113] </ref>, P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines [24, 77]. In the Monsoon architecture, the associative waiting-matching store of these machines was replaced by an explicitly managed, directly addressed token store.
Reference: [114] <author> G. M. Papadopoulos and K. R. Traub, </author> <title> ``Multithreading: A Revisionist View of Dataflow Architectures,'' </title> <booktitle> Proceedings of 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 342-351, </pages> <year> 1991. </year>
Reference-contexts: Lack of a static ordering has other ramifications as well. Data-driven specification suffers from the inability to express critical sections and imperative operations that are essential for the efficient execution of operating system functions, such as resource management <ref> [36, 114] </ref>. Furthermore, in data-driven specification, data tokens have to be routed through control-decision points such as switch nodes and merge nodes, and often get collected at these control-decision points, waiting for the appropriate control decisions to be taken. <p> A large performance penalty is incurred due to the ``stretching'' of the communication arcs of the dataflow graph, which manifests as associative searches through the entire program using matching hardware. The recent advent of more restricted forms of dataflow architectures <ref> [37, 77, 109, 113, 114] </ref> bear testimony to this fact.
Reference: [115] <author> Y. N. Patt, S. W. Melvin, W. W. Hwu, and M. Shebanow, </author> <title> ``Critical Issues Regarding HPS, A High Performance Microarchitecture,'' </title> <booktitle> in Proceedings of 18th Annual Workshop on Microprogramming, </booktitle> <address> Pacific Grove, CA, </address> <pages> pp. 109-116, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Chapter 6 further addresses the issues involved in dynamic disambiguation. Over the years, different techniques have been proposed for performing dynamic disambiguation <ref> [15, 115, 139] </ref>; these are also described in detail in chapter 6. Another important aspect in connection with dynamic disambiguation is that existing schemes execute memory references after performing disambiguation. <p> If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. <p> The stunt box is simpler than the store queue, allows only limited reordering of memory references, and is even more restrictive than the store queue. Dependency Matrix: This is a scheme proposed by Patt, et al in connection with the HPS restricted dataflow architecture <ref> [115] </ref>, which performs dynamic code reordering and dynamic disambiguation. In this scheme, a dependency matrix is provided for relating each memory reference to every other reference in the active instruction window.
Reference: [116] <author> Y. N. Patt, W. W. Hwu, and M. Shebanow, ``HPS, </author> <title> A New Microarchitecture: Rationale and Introduction,'' </title> <booktitle> Proceedings of 18th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 103-108, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming <ref> [66, 73, 116, 147, 153] </ref>. 2.3.3.3. Scheduling Instructions In parallel to establishing a window and enforcing the register and memory dependencies, the hardware performs scheduling of ready-to-execute instructions. Instructions that are speculatively fetched from beyond unresolved branches are executed speculatively, i.e., before determining that their execution is needed. <p> If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. <p> The effectiveness of these approaches for applications that are not amenable to static analysis techniques (to extract parallelism) is not clear. 2.4.5. The Superscalar Paradigm An alternate paradigm that was developed at the same time as the VLIW paradigm (early 80's) was the superscalar paradigm <ref> [80, 101, 111, 116] </ref>. The superscalar paradigm attempts to bring together the good aspects of control-driven specification and data-driven execution, by doing data-driven execution within a window of instructions established by control-driven fetching along a single flow of control. Parallelism may be extracted at compile time and at run time. <p> been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS <ref> [73, 116] </ref>. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism.
Reference: [117] <author> C. Peterson, J. Sutton, and P. Wiley, </author> <title> ``iWarp: A 100-MOPS, LIW Microprocessor for Multi-computers,'' </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 26-29, 81-87, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The applicability of such special purpose processing paradigms to non-numeric applications, having complex control and data flow patterns, is not clear. This is evidenced by the recent introduction of the iWarp machine <ref> [23, 117] </ref>, where support is provided for both systolic communication and coarse-grain communication. 2.4.3. The Vector Paradigm The earliest paradigm for ILP processing was the vector paradigm. In fact, much of the compiler work to extract parallelism was driven by vector machines.
Reference: [118] <author> K. Pettis and R. C. Hansen, </author> <title> ``Profile Guided Code Positioning,'' </title> <booktitle> Proceedings of ACM SIG-PLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 16-27, </pages> <year> 1990. </year>
Reference-contexts: Initial static 20 prediction schemes were based on branch opcodes, and were not accurate. Now, static prediction schemes are much more sophisticated, and use profile information or heuristics to take decisions <ref> [28, 75, 98, 118, 160] </ref>. In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies.
Reference: [119] <author> A. R. Pleszkun, G. S. Sohi, B. Z. Kahhaleh, and E. S. Davidson, </author> <title> ``Features of the Structured Memory Access (SMA) Architecture,'' </title> <booktitle> Digest of Papers, COMPCON Spring 1986, </booktitle> <month> March </month> <year> 1986. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [120] <author> A. R. Pleszkun and G. S. Sohi, </author> <title> ``The Performance Potential of Multiple Functional Unit Processors,'' </title> <booktitle> Proceedings of 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 37-44, </pages> <year> 1988. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [121] <author> D. N. Pnevmatikatos, M. Franklin, and G. S. Sohi, </author> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture (MICRO-26), </booktitle> <year> 1993. </year> <month> 196 </month>
Reference-contexts: This process is repeated. We call the type of prediction used by the multiscalar paradigm as control flow prediction <ref> [121] </ref>. In the multiscalar paradigm, the execution of all active tasks, except the first, is speculative in nature. The hardware provides facilities for recovery when it is determined that an incorrect control flow prediction has been made.
Reference: [122] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> ``Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: When parallelizing sequential code into multiple tasks statically, synchronization operations can be avoided if it is known that the tasks are independent. If tasks are known to be dependent (e.g. FORALL loops and DOACR loops <ref> [122] </ref> ), then synchronization operations can be added to parallelize the code. However, there are several loops and other code in which it cannot be statically determined if the tasks are dependent or independent, because of ambiguous data dependencies through memory.
Reference: [123] <author> V. Popescu, M. Schultz, J. Spracklen, G. Gibson, B. Lightner, and D. Isaman, </author> <booktitle> ``The Metaflow Architecture,'' IEEE Micro, </booktitle> <pages> pp. 10-13, 63-72, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm [153], decoupled execution [134], register update unit (RUU) [147], dispatch stack [43, 44], deferred-scheduling, register-renaming instruction shelf (DRIS) <ref> [123] </ref>, etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. Scoreboard: Scoreboard is a hardware technique for dynamic scheduling, originally used in the CDC 6600 [151], which issued instructions one at a time. <p> DRIS: The deferred-scheduling, register-renaming instruction shelf (DRIS) technique proposed by Popescu et al <ref> [123] </ref>, is a variant of the RUU technique described earlier, and performs superscalar issue coupled with speculative execution. The hardware features for carrying out this scheme are: two counter fields associated with each register, and a register update unit. <p> Just like the RUU, the DRIS also holds the instructions in their sequential order. The DRIS hardware scheduler has been implemented in the Lightning processor <ref> [123] </ref>, which is a superscalar implementation of the Sparc architecture. Decoupled Execution: Decoupled architectures exploit the ILP between the memory access operations and the core computation operations in a program. <p> These advantages have led to the adoption of small-issue superscalar processors as the paradigm of choice for several modern microprocessors <ref> [35, 40, 123] </ref>.
Reference: [124] <author> B. R. Rau and C. D. Glaeser, </author> <title> ``Some Scheduling Techniques and an Easily Schedulable Horizontal Architecture for High Performance Scientific Computing,'' </title> <booktitle> Proceedings of 20th Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <pages> pp. 183-198, </pages> <year> 1981. </year>
Reference-contexts: Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining [10], enhanced pipeline scheduling [47], GURPR* [149], modulo scheduling <ref> [48, 124] </ref>, and polycyclic scheduling [125]. Boosting: Boosting is a technique for statically specifying speculative execution [141-143].
Reference: [125] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard, ``Efficient Code Generation For Horizontal Architectures: </title> <booktitle> Compiler Techniques and Architectural Support,'' Proceedings of 9th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 131-139, </pages> <year> 1982. </year>
Reference-contexts: Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining [10], enhanced pipeline scheduling [47], GURPR* [149], modulo scheduling [48, 124], and polycyclic scheduling <ref> [125] </ref>. Boosting: Boosting is a technique for statically specifying speculative execution [141-143]. <p> Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [126] <author> B. R. Rau, </author> <title> ``Cydra 5 Directed Dataflow Architecture,'' </title> <booktitle> Digest of Papers, COMPCON Spring 1988, </booktitle> <pages> pp. 106-113, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes <ref> [100, 126] </ref>.
Reference: [127] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. Towle, </author> <title> ``The Cydra 5 Departmental Supercomputer: Design Philosophies, Decisions, and Trade-offs,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 12-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: The common way of providing this bandwidth is to use a multi-ported register file. Whereas a few multi-ported register files have been built, for example the register files for the Cydra 5 <ref> [127] </ref>, the SIMP processor [101], Intel's iWarp [79], and the XIMD processor [162], centralized, multi-ported register files do not appear to be a good long-term solution, as its design becomes very complex for large values of I. Therefore, decentralization of the inter-operation communication mechanism is essential for future ILP processors. <p> The pros and cons of the VLIW approach have been discussed in detail in the literature [29, 34, 49, 53, 54, 56, 125, 146]. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 <ref> [127] </ref>, is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126]. <p> The third option is a combination of the first two, in that both the writes and reads are performed over multiple register files. The second option has been used in the context register matrix scheme of Cydra 5 <ref> [127] </ref> and in the shadow register files of Torch [141, 142].
Reference: [128] <author> B. R. Rau, M. S. Schlansker, and D. W. L. Yen, </author> <title> ``The Cydra TM Stride-Insensitive Memory System,'' </title> <booktitle> in 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Illinois, </publisher> <pages> pp. </pages> <address> I-242-I-246, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The cache blocks are interleaved amongst the multiple banks, with a cache block entirely present in a single cache bank. A straightforward way to do the interleaving of cache blocks is to use low-order interleaving of block addresses. Other interleaving schemes, such as those described in <ref> [128, 145] </ref> could be used, and need further study. One potential drawback of a multi-banked cache is the additional delay introduced by the interconnection network (ICN) from the execution units to the multiple banks (see Figure 6.1).
Reference: [129] <author> B. R. Rau and J. A. Fisher, </author> <title> ``Instruction-Level Parallel Processing: History, Overview and Perspective,'' </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 7, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies. Some of these techniques are if-conversion, loop unrolling, loop peeling, loop conditioning, loop exchange [12], function inlining [28, 85], replacing a set of IF-THEN statements by a jump table <ref> [129] </ref>, and even changing data structures. All these techniques modify the CFG of the program, mostly by reducing the number of control decision points in the CFG. <p> Thus, instead of the two operations I 1 : if (condition) branch to I 2 I 2 : computation operation we can have, I 1 : guard condition is true I 2 : if (guard) computation operation This conversion process is called if-conversion <ref> [13, 14, 21, 35, 71, 129] </ref>. Because the guard is an input operand to I 2 , the relationship between I 1 and I 2 has now been converted from a control dependency to a data dependency.
Reference: [130] <institution> Cray Research, Inc., ``The Cray Y-MP Series of Computer Systems,'' Cray Research Inc., </institution> <note> Publication No. CCMP-0301, </note> <month> February </month> <year> 1988. </year>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines <ref> [2, 3, 130, 131] </ref>, CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131].
Reference: [131] <author> R. M. Russel, </author> <title> ``The CRAY-1 Computer System,'' </title> <journal> CACM, </journal> <volume> vol. 21, </volume> <pages> pp. 63-72, </pages> <month> January </month> <year> 1978. </year> <month> 197 </month>
Reference-contexts: Examples for vector machines are CDC Star-100 [150], TI ASC [150], Cray machines <ref> [2, 3, 130, 131] </ref>, CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131]. <p> Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage <ref> [131] </ref>. Vector machines perform remarkably well in executing codes that fit the vector paradigm [157]. Unfortunately, very few non-numeric programs can be vectorized with existing compiler technology, rendering vector machines as an inappropriate way of exploiting ILP in such programs. This is especially the case for programs that use pointers.
Reference: [132] <author> G. M. Silberman and K. Ebcioglu, </author> <title> ``An Architectural Framework for Migration from CISC to Higher Performance Platforms,'' </title> <booktitle> Proceedings of International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: Realizing this, researchers have proposed schemes that allow ambiguous references to be statically reordered, with checks made at run time to determine if any dependencies are violated by the static code motions <ref> [31, 108, 132] </ref>. Ambiguous references that are statically reordered are called statically unresolved references. A limitation of this scheme, however, is that the run-time checks need extra code and in some schemes associative compare of store addresses with preceding load addresses in the active window. <p> By deferring the checks to run time, RTD allows general static code movement across ambiguous stores. Although static disambiguation can help reduce the extra code to be inserted, the extra code can still be large when aggressive code reordering is performed. Non-Architected Registers: Silberman and Ebcioglu <ref> [132] </ref> describe a technique for supporting 115 statically unresolved loads and multiprocessor consistency in the context of object code migration from CISC to higher performance platforms, such as the superscalars and the VLIWs. They propose to use non-architected registers to temporarily store the results of statically unresolved loads.
Reference: [133] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies,'' </title> <booktitle> Proceedings of 8th International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: With speculative fetching, rather than waiting for the outcome of a conditional branch to be determined, the branch outcome is predicted, and operations from the predicted path are entered into the window for execution. Dynamic prediction techniques have significantly evolved over the years <ref> [93, 112, 133, 166] </ref>.
Reference: [134] <author> J. E. Smith, </author> <title> ``Decoupled Access/Execute Architectures,'' </title> <booktitle> Proceedings of 9th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 112-119, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm [153], decoupled execution <ref> [134] </ref>, register update unit (RUU) [147], dispatch stack [43, 44], deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. <p> There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [135] <author> J. E. Smith, A. R. Pleszkun, R. H. Katz , and J. R. Goodman, </author> <title> ``PIPE: A High Performance VLSI Architecture,'' </title> <booktitle> in Workshop on Computer Systems Organization, </booktitle> <address> New Orleans, LA, </address> <year> 1983. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [136] <author> J. E. Smith, S. Weiss, and N. Pang, </author> <title> ``A Simulation Study of Decoupled Architecture Computers,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-35, </volume> <pages> pp. 692-702, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Several decoupled architectures have been proposed, and some have been built. The noted ones are the MAP-200 [33], the DAE <ref> [136] </ref>, the PIPE [63], the ZS-1 [139], and the WM architecture [165]. A drawback in decou-pled architectures is AP-EP code unbalance. When there is unbalance, the sustained instruction issue rate is low because of poor utilization.
Reference: [137] <author> J. E. Smith, et al, </author> <title> ``The ZS-1 Central Processor,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), </booktitle> <pages> pp. 199-204, </pages> <year> 1987. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 <ref> [137, 139] </ref>, and HPS [73, 116]. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism.
Reference: [138] <author> J. E. Smith and A. R. Pleszkun, </author> <title> ``Implementing Precise Interrupts in Pipelined Processors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 562-573, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity.
Reference: [139] <author> J. E. Smith, </author> <title> ``Dynamic Instruction Scheduling and the Astronautics ZS-1,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 21-35, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Chapter 6 further addresses the issues involved in dynamic disambiguation. Over the years, different techniques have been proposed for performing dynamic disambiguation <ref> [15, 115, 139] </ref>; these are also described in detail in chapter 6. Another important aspect in connection with dynamic disambiguation is that existing schemes execute memory references after performing disambiguation. <p> Several decoupled architectures have been proposed, and some have been built. The noted ones are the MAP-200 [33], the DAE [136], the PIPE [63], the ZS-1 <ref> [139] </ref>, and the WM architecture [165]. A drawback in decou-pled architectures is AP-EP code unbalance. When there is unbalance, the sustained instruction issue rate is low because of poor utilization. <p> There have been several proposals for superscalar machines in the 1980s [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164]; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 <ref> [137, 139] </ref>, and HPS [73, 116]. Advantages of the superscalar approach include object code compatibility and the ability to make run-time decisions and adapt to run-time uncertainities (for example variable memory latencies encountered in cache-based systems), potentially extracting more parallelism. <p> Thus, to handle the worst case, the latter requires hardware for i=1 i=S 2 S (2W - S - 1) hhhhhhhhhhhh compares, where S is the maximum number of stores execut ed per cycle. 117 91 [15, 22] and Astronautics ZS-1 <ref> [139] </ref>. In this scheme, the stores are allowed to wait in a queue until their store values are ready.
Reference: [140] <author> M. D. Smith, M. Johnson, and M. A. Horowitz, </author> <title> ``Limits on Multiple Instruction Issue,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pp. 290-302, </pages> <year> 1989. </year>
Reference-contexts: Control units with instruction decoders that feed centralized windows are a major impediment to performance in superscalar processors, as shown in <ref> [140] </ref>. 4.2.1. Task Prediction After a task is assigned to an execution unit, the global control unit needs to predict the next task to be executed.
Reference: [141] <author> M. D. Smith, M. S. Lam, and M. A. Horowitz, </author> <title> ``Boosting Beyond Static Scheduling in a Superscalar Processor,'' </title> <booktitle> Proceedings of 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 344-354, </pages> <year> 1990. </year> <month> 198 </month>
Reference-contexts: Going from the head unit towards the tail unit, there is an extra level of speculation being carried out at each unit, necessitating a separate physical storage to store the speculative state of each unit. Although techniques such as shadow register files <ref> [141, 142] </ref> provide a separate shadow register file per unresolved branch to support multiple levels of speculative execution in control-driven execution processors, they require a complex interconnect between the functional units and the register files, because a register read operation must have access to all physical register files. <p> The third option is a combination of the first two, in that both the writes and reads are performed over multiple register files. The second option has been used in the context register matrix scheme of Cydra 5 [127] and in the shadow register files of Torch <ref> [141, 142] </ref>.
Reference: [142] <author> M. D. Smith, M. Horowitz, and M. S. Lam, </author> <title> ``Efficient Superscalar Performance Through Boosting,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pp. 248-259, </pages> <year> 1992. </year>
Reference-contexts: Going from the head unit towards the tail unit, there is an extra level of speculation being carried out at each unit, necessitating a separate physical storage to store the speculative state of each unit. Although techniques such as shadow register files <ref> [141, 142] </ref> provide a separate shadow register file per unresolved branch to support multiple levels of speculative execution in control-driven execution processors, they require a complex interconnect between the functional units and the register files, because a register read operation must have access to all physical register files. <p> The third option is a combination of the first two, in that both the writes and reads are performed over multiple register files. The second option has been used in the context register matrix scheme of Cydra 5 [127] and in the shadow register files of Torch <ref> [141, 142] </ref>. <p> Current Instance 1 is Current Instance 2 is Valid Instance 0 is Valid Instance 1 is Valid Instance 2 is Current Instance 0 0 Instance 1 0 Instance 2 0 Instance 2 0 Instance 0 0 Instance 1 0 Value 31 103 as in the shadow register file scheme of <ref> [142] </ref>. 5.3.5. Recovery Actions When an execution unit i gets squashed due to a recovery event, its create mask is forwarded one full round through the circular unit queue, one unit at a time, just like the forwarding done when a new task is allocated.
Reference: [143] <author> M. D. Smith, </author> <title> ``Support for Speculative Execution in High-Performance Processors,'' </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> November </month> <year> 1992. </year>
Reference: [144] <author> G. S. Sohi and S. Vajapeyam, </author> <title> ``Instruction Issue Logic for High-Performance, Interruptible Pipelined Processors,'' </title> <booktitle> in Proceedings of 14th Annual Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 27-34, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Tomasulo's algorithm does distribute the dynamic scheduling logic throughout the execution unit; however, the results of all computation operations have to be distributed throughout the execution unit using the CDB, resulting in ``global'' communication. RUU: The register update unit (RUU) technique proposed by Sohi and Vajapeyam <ref> [144, 147] </ref>, is an enhancement of Tomasulo's algorithm, and guarantees precise interrupts. The hardware features for carrying out this scheme are: two counter fields associated with each register, and a register update unit.
Reference: [145] <author> G. S. Sohi, </author> <title> ``High-Bandwidth Interleaved Memories for Vector Processors A Simulation Study,'' </title> <type> Computer Sciences Technical Report #790, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: The cache blocks are interleaved amongst the multiple banks, with a cache block entirely present in a single cache bank. A straightforward way to do the interleaving of cache blocks is to use low-order interleaving of block addresses. Other interleaving schemes, such as those described in <ref> [128, 145] </ref> could be used, and need further study. One potential drawback of a multi-banked cache is the additional delay introduced by the interconnection network (ICN) from the execution units to the multiple banks (see Figure 6.1).
Reference: [146] <author> G. S. Sohi and S. Vajapeyam, </author> <title> ``Tradeoffs in Instruction Format Design For Horizontal Architectures,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pp. 15-25, </pages> <year> 1989. </year>
Reference-contexts: Thus, the VLIW hardware needs to provide a crossbar-like inter-operation communication mechanism (e.g. multi-ported register file). The pros and cons of the VLIW approach have been discussed in detail in the literature <ref> [29, 34, 49, 53, 54, 56, 125, 146] </ref>. With suitable compiler technology, a VLIW machine, typified by the Multiflow Trace series [34] and the Cydrome Cydra-5 [127], is able to perform well for vectorizable and non-vectorizable numeric codes [100, 126].
Reference: [147] <author> G. S. Sohi, </author> <title> ``Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming <ref> [66, 73, 116, 147, 153] </ref>. 2.3.3.3. Scheduling Instructions In parallel to establishing a window and enforcing the register and memory dependencies, the hardware performs scheduling of ready-to-execute instructions. Instructions that are speculatively fetched from beyond unresolved branches are executed speculatively, i.e., before determining that their execution is needed. <p> If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. <p> This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm [153], decoupled execution [134], register update unit (RUU) <ref> [147] </ref>, dispatch stack [43, 44], deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. <p> Tomasulo's algorithm does distribute the dynamic scheduling logic throughout the execution unit; however, the results of all computation operations have to be distributed throughout the execution unit using the CDB, resulting in ``global'' communication. RUU: The register update unit (RUU) technique proposed by Sohi and Vajapeyam <ref> [144, 147] </ref>, is an enhancement of Tomasulo's algorithm, and guarantees precise interrupts. The hardware features for carrying out this scheme are: two counter fields associated with each register, and a register update unit.
Reference: [148] <author> G. S. Sohi and M. Franklin, </author> <title> ``High-Bandwidth Data Memory Systems for Superscalar Processors,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 53-62, </pages> <year> 1991. </year>
Reference-contexts: For providing a high bandwidth data memory system for the multiscalar processor, we use the multi-ported non-blocking cache that we had proposed earlier for superscalar processors <ref> [148] </ref>. The multiple ports improve the bandwidth to more than one request per cycle, and the non-blocking feature helps to reduce the bandwidth degradation due to misses. Figure 6.1 shows the block diagram of the multiscalar processor's data memory system. The multiple ports are implemented by using multiple cache banks.
Reference: [149] <author> B. Su and J. Wang, ``GURPR*: </author> <title> A New Global Software Pipelining Algorithm,'' </title> <booktitle> Proceedings of the 24th Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <pages> pp. 212-216, </pages> <year> 1991. </year>
Reference-contexts: Scheduling is done on the unrolled loop, and the re-rolled loop is shown in Figure 2.3 (iii). Different techniques have been proposed to do software pipelining: perfect pipelining [10], enhanced pipeline scheduling [47], GURPR* <ref> [149] </ref>, modulo scheduling [48, 124], and polycyclic scheduling [125]. Boosting: Boosting is a technique for statically specifying speculative execution [141-143].
Reference: [150] <author> D. J. Theis, </author> <title> ``Special Tutorial: Vector Supercomputers,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 52-61, </pages> <month> April </month> <year> 1974. </year> <month> 199 </month>
Reference-contexts: Examples for vector machines are CDC Star-100 <ref> [150] </ref>, TI ASC [150], Cray machines [2, 3, 130, 131], CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131]. <p> Examples for vector machines are CDC Star-100 <ref> [150] </ref>, TI ASC [150], Cray machines [2, 3, 130, 131], CDC Cyber-205 [1], and Fujitsu VP-200 [99]. Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131].
Reference: [151] <author> J. E. Thornton, </author> <title> ``Parallel Operation in the Control Data 6600,'' </title> <booktitle> Proceedings of AFIPS Fall Joint Computers Conference, </booktitle> <volume> vol. 26, </volume> <pages> pp. 33-40, </pages> <year> 1964. </year>
Reference-contexts: Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard <ref> [151] </ref>, Tomasulo's algorithm [153], decoupled execution [134], register update unit (RUU) [147], dispatch stack [43, 44], deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. <p> Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78]. Scoreboard: Scoreboard is a hardware technique for dynamic scheduling, originally used in the CDC 6600 <ref> [151] </ref>, which issued instructions one at a time. However, the scoreboard technique can be extended to issue multiple instructions per cycle. The hardware features in the scoreboard technique are: a buffer associated with each functional unit, and a scoreboard. <p> The limitations of the scoreboard technique are: (i) instruction issue stops when a functional unit is needed by multiple instructions (this was a restriction of the scoreboard described in <ref> [151] </ref>, and not an inherent limitation of the scoreboard technique), (ii) instruction issue stops when there is an output dependency, (iii) the centralized scoreboard becomes very complex when the window becomes large.
Reference: [152] <author> J. E. Thornton, </author> <title> Design of a Computer -- The Control Data 6600. Scott, </title> <publisher> Foresman and Co., </publisher> <year> 1970. </year>
Reference-contexts: The store queue also does not support dynamically unresolved references, rendering it as a poor candidate for use in the multiscalar processor. Stunt Box: This is a mechanism used in the CDC 6600 <ref> [152] </ref> for dynamically reordering the memory references that are waiting due to memory bank conflicts. The way out-of-order loads and stores to the same memory location are prevented is by disallowing loads and stores to be present at the same time in the stunt box.
Reference: [153] <author> R. M. Tomasulo, </author> <title> ``An Efficient Algorithm for Exploiting Multiple Arithmetic Units,'' </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp. 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: With hardware register renaming, typically a free physical register is allocated for every assignment to a register in the window, much like the way software register renaming allocates architectural registers. Several techniques have been proposed to implement hardware register renaming <ref> [66, 73, 116, 147, 153] </ref>. 2.3.3.3. Scheduling Instructions In parallel to establishing a window and enforcing the register and memory dependencies, the hardware performs scheduling of ready-to-execute instructions. Instructions that are speculatively fetched from beyond unresolved branches are executed speculatively, i.e., before determining that their execution is needed. <p> Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. A number of dynamic scheduling techniques have been proposed: CDC 6600's scoreboard [151], Tomasulo's algorithm <ref> [153] </ref>, decoupled execution [134], register update unit (RUU) [147], dispatch stack [43, 44], deferred-scheduling, register-renaming instruction shelf (DRIS) [123], etc. Below, we briefly review each of these schemes; Johnson provides a more detailed treatment of these schemes [78].
Reference: [154] <author> H. C. Torng and M. Day, </author> <title> ``Interrupt Handling for Out-of-Order Execution Processors,'' </title> <type> Technical Report EE-CEG-90-5, </type> <institution> School of Electrical Engineering, Cornell University, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: If the prediction was incorrect, then the results of speculatively executed instructions are discarded, and instructions are fetched and executed from the correct path. Several dynamic techniques have been proposed to implement speculative execution coupled with precise state recovery <ref> [73, 74, 115, 116, 138, 147, 154] </ref>. Hardware schedulers often use simplistic heuristics to choose from the instructions that are ready for execution. This is because any sophistication of the instruction scheduler directly impacts the hardware complexity. <p> Results are forwarded to the register file in sequential order, as and when instructions are committed from the RUU. Dispatch Stack: The dispatch stack is a technique proposed by Acosta et al <ref> [4, 154] </ref>. The hardware features for carrying out this scheme are: two dependency count fields associated with each register, and a dispatch stack.
Reference: [155] <author> A. K. Uht, </author> <title> ``Hardware Extraction of Low-level Concurrency from Sequential Instruction Streams,'' </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Department of Electrical and Computer Engineering, </institution> <year> 1985. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [156] <author> A. K. Uht and R. G. Wedig, </author> <title> ``Hardware Extraction of Low-level Concurrency from a Serial Instruction Stream,'' </title> <booktitle> Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pp. 729-736, </pages> <year> 1986. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [157] <author> S. Vajapeyam, G. S. Sohi, and W.-C. Hsu, </author> <title> ``Exploitation of Instruction-Level Parallelism in a Cray X-MP Processor,'' </title> <booktitle> in Proceedings of International Conference on Computer Design (ICCD 90), </booktitle> <address> Boston, MA, </address> <pages> pp. 20-23, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Vector machines are a good example for exploiting localities of communication; the chaining mechanism directly forwards a result from the producer to the consumer without an intermediate storage [131]. Vector machines perform remarkably well in executing codes that fit the vector paradigm <ref> [157] </ref>. Unfortunately, very few non-numeric programs can be vectorized with existing compiler technology, rendering vector machines as an inappropriate way of exploiting ILP in such programs. This is especially the case for programs that use pointers.
Reference: [158] <author> A. H. Veen, </author> <title> ``Dataflow Machine Architectures,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 18, </volume> <pages> pp. 365-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The recent advent of more restricted forms of dataflow architectures [37, 77, 109, 113, 114] bear testimony to this fact. The dataflow paradigm has evolved significantly over the years, from the early tagged token store architectures <ref> [39, 158] </ref>, to the more recent explicit token store machines such as the Monsoon [113], P-RISC [109], TAM (Threaded Abstract Machine) [37], and *T [110], and other hybrid machines [24, 77].
Reference: [159] <author> D. W. Wall, </author> <title> ``Limits of Instruction-Level Parallelism,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 176-188, </pages> <year> 1991. </year> <month> 200 </month>
Reference-contexts: Exploitation of parallelism at the instruction level is very important to speed up such programs. Recent studies have confirmed that there exists a large amount of instruction-level parallelism in ordinary programs <ref> [18, 25, 36, 106, 159] </ref>. Even in other applications, no matter how much parallelism is exploited by coarse-grain parallel processors such as the multiprocessors, a substantial amount of parallelism will still remain to be exploited at the instruction level. <p> Second, studies have found little ILP within a small sequential block of instructions, but significant amounts in large blocks <ref> [18, 25, 90, 159] </ref>. There are several inter-related factors that contribute to this. <p> These tasks are executed in parallel, although the paradigm preserves logical sequentiality among the tasks. The units are connected as a circular queue to obtain a sliding or continuous big window (as opposed to a fixed window), a feature that allows more parallelism to be exploited <ref> [159] </ref>. When the execution of the subgraph at the head unit is over (determined by checking when control flows out of the task), the head pointer is advanced by one unit. A task could be as simple as a basic block or part of a basic block. <p> The ARB stages are logically configured as a circular queue so as to correspond to the circular queue nature of a sliding (or continuous) instruction window described in <ref> [159] </ref>. When a load or a store with sequence number i is executed, it is treated the same way as a load or store from execution unit i of the multiscalar processor is handled in section 6.4.
Reference: [160] <author> D. W. Wall, </author> <title> ``Predicting Program Behavior Using Real or Estimated Profiles,'' </title> <booktitle> Proceedings of ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 59-70, </pages> <year> 1991. </year>
Reference-contexts: Initial static 20 prediction schemes were based on branch opcodes, and were not accurate. Now, static prediction schemes are much more sophisticated, and use profile information or heuristics to take decisions <ref> [28, 75, 98, 118, 160] </ref>. In addition to branch prediction, the compiler uses several other techniques to overcome the effects of control dependencies.
Reference: [161] <author> S. Weiss and J. E. Smith, </author> <title> ``A Study of Scalar Compilation Techniques for Pipelined Supercomputers,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), </booktitle> <pages> pp. 105-109, </pages> <year> 1987. </year>
Reference-contexts: Several global scheduling techniques have been developed over the years to establish large static windows and to carry out static code motions in the windows. These include: trace scheduling [53], percolation scheduling [57, 107], superblock scheduling [28], software pipelining <ref> [45, 89, 102, 103, 161] </ref>, perfect pipelining [8, 9], boosting [141-143], and sentinel scheduling [95]. Trace Scheduling: The key idea of trace scheduling is to reduce the execution time along the more frequently executed paths, possibly by increasing the execution time in the less frequently executed paths.
Reference: [162] <author> A. Wolfe and J. P. Shen, </author> <title> ``A Variable Instruction Stream Extension to the VLIW Architecture,'' </title> <booktitle> Proceedings of Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 2-14, </pages> <year> 1991. </year>
Reference-contexts: The common way of providing this bandwidth is to use a multi-ported register file. Whereas a few multi-ported register files have been built, for example the register files for the Cydra 5 [127], the SIMP processor [101], Intel's iWarp [79], and the XIMD processor <ref> [162] </ref>, centralized, multi-ported register files do not appear to be a good long-term solution, as its design becomes very complex for large values of I. Therefore, decentralization of the inter-operation communication mechanism is essential for future ILP processors. <p> Serious thought is being given to expanding the abilities of the VLIW model to allow the execution of multiple branches per cycle; several proposals have been made <ref> [46, 83, 100, 162] </ref>. The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams [162]. <p> The XIMD approach allows a VLIW processor to adapt to the varying parallelism in an application, as well as to execute multiple instruction streams <ref> [162] </ref>. The processor coupling approach is similar in spirit to XIMD, but it allows dynamic interleaving of multiple threads to tolerate long memory latencies [83].
Reference: [163] <author> M. J. Wolfe, </author> <title> ``Optimizing Supercompilers for Supercomputers,'' </title> <type> Ph.D. Dissertation, </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Two memory references may be dependent in one instance of program execution and not dependent in another instance, and static disambiguation has to consider all possible executions of the program. Various techniques have been proposed to do static disambiguation of memory references involving arrays <ref> [11, 19, 20, 49, 105, 163] </ref>. These techniques involve the use of conventional flow analyses of reaching definitions to derive symbolic expressions for array indexes. The symbolic expressions contain compile-time constants, loop-invariants, and induction variables, as well as variables whose values cannot be derived at compile time.
Reference: [164] <author> W. A. Wulf, </author> <title> ``The WM Computer Architecture,'' </title> <journal> Computer Architecture News, </journal> <volume> vol. 16, </volume> <month> March </month> <year> 1988. </year>
Reference-contexts: There have been several proposals for superscalar machines in the 1980s <ref> [4, 33, 87, 101, 119, 120, 134, 135, 155, 156, 164] </ref>; notable ones include the IBM Cheetah, America and RIOS projects which culminated in the IBM RS/6000 [66, 111], decoupled architectures, which resulted in the Astronautics ZS-1 [137, 139], and HPS [73, 116].
Reference: [165] <author> W. A. Wulf, </author> <title> ``Evaluation of the WM Computer Architecture,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 382-390, </pages> <year> 1992. </year>
Reference-contexts: Several decoupled architectures have been proposed, and some have been built. The noted ones are the MAP-200 [33], the DAE [136], the PIPE [63], the ZS-1 [139], and the WM architecture <ref> [165] </ref>. A drawback in decou-pled architectures is AP-EP code unbalance. When there is unbalance, the sustained instruction issue rate is low because of poor utilization. Further, if the access and execute processors are single-issue processors, then the issue rate can never exceed two operations per cycle.
Reference: [166] <author> T. Y. Yeh and Y. N. Patt, </author> <title> ``Alternative Implementations of Two-Level Adaptive Training Branch Prediction,'' </title> <booktitle> Proceedings of 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-134, </pages> <year> 1992. </year>
Reference-contexts: With speculative fetching, rather than waiting for the outcome of a conditional branch to be determined, the branch outcome is predicted, and operations from the predicted path are entered into the window for execution. Dynamic prediction techniques have significantly evolved over the years <ref> [93, 112, 133, 166] </ref>. <p> Dynamic prediction techniques have significantly evolved over the years [93, 112, 133, 166]. Although the accuracies of contemporary dynamic branch prediction techniques are fairly high, averaging 95% for the SPEC '89 non-numeric programs <ref> [166] </ref>, the accuracy of a large window obtained through n independent branch predictions in a row is only (0.95) n on the average, and is therefore poor even for moderate values of n. Notice that this problem is an inherent limitation of 31 following a single line of control. <p> keep track the number of times each target of a task was previously taken, or a two-level scheme with a register keeping track of the last several outcomes, and a set of counters keeping track of the number of times each target was taken for different patterns of previous outcomes <ref> [166] </ref>. For branch predictions, the later scheme has been shown to be very effective, giving branch prediction accuracies to the tune of 95% for non-numeric programs. Another issue to be addressed in dynamic task prediction is the issue of basing prediction decisions on obsolete history.
References-found: 166

