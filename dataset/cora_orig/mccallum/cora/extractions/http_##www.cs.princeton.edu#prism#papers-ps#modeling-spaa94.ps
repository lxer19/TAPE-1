URL: http://www.cs.princeton.edu/prism/papers-ps/modeling-spaa94.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Modeling Communication in Parallel Algorithms: A Fruitful Interaction between Theory and Systems?  
Author: Jaswinder Pal Singh Edward Rothberg and Anoop Gupta 
Address: 14924 NW Greenbrier Pkwy, CO6-09 Stanford, CA 94305 Beaverton, OR 97006  
Affiliation: Computer Systems Laboratory Intel Supercomputer Systems Stanford University  
Abstract: Recently, several theoretical models of parallel architectures have been proposed to replace the PRAM as the model that is presented to an algorithm designer. A primary focus of the new models is to include the cost of interprocessor communication, which is increasingly important in modern parallel architectures. We argue that modeling the communication costs in the architecture or system is only one part of the problem. The other, and usually much more difficult, part is modeling the communication properties of the algorithm itself, which provides necessary inputs into the architectural model to determine overall complexity. In this context, we make three main points in this paper: (i) It is incomplete to describe communication without regard to its relationship with replication. We propose a description of the communication-replication relationship in terms of the working set hierarchy of an algorithm. (ii) Both inherent communication and the communication-replication relationship can be very difficult to model in irregular, dynamic computations that are crucial in many real-world applications. We present some examples that demonstrate this difficulty. (iii) We believe that substantial leverage can be obtained in this effort from the computer systems community, which can provide a hierarchy of simulation and profiling toolsfrom abstract to detailedtailored to the needs of the algorithm designers. We propose an initial set of simulation tools, and we discuss possible future refinements to this set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A. Chandra and M. Snir, </author> <title> On Communication Latency in PRAM Computations. </title> <booktitle> In Proc. 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM <ref> [1] </ref>, Local-memory PRAM [1], Parallel Memory Hierarchy [12], Bulk Synchronous Processor or BSP [21], the Postal Model [3], logP [6], and various asynchronous PRAMs [5][11]. <p> Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM <ref> [1] </ref>, Local-memory PRAM [1], Parallel Memory Hierarchy [12], Bulk Synchronous Processor or BSP [21], the Postal Model [3], logP [6], and various asynchronous PRAMs [5][11].
Reference: [2] <author> A. Aggarwal, A. Chandra and M. Snir, </author> <title> Communication Complexity of PRAMs. </title> <booktitle> In Theoretical Computer Science, </booktitle> <volume> vol. 71, no. 1, </volume> <pages> pp: 3-28, </pages> <year> 1990. </year>
Reference: [3] <author> A. Bar-Noy and S. Kipnis, </author> <title> Designing Broadcasting Algorithms in the Postal Model for Message Passing Systems. </title> <booktitle> In Proc. 1992 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM [1], Local-memory PRAM [1], Parallel Memory Hierarchy [12], Bulk Synchronous Processor or BSP [21], the Postal Model <ref> [3] </ref>, logP [6], and various asynchronous PRAMs [5][11]. Indeed, the topic of alternative complexity models is an active research area, and the theory community has yet to reach a consensus on which parallel machine model is the best one to present to an algorithm designer.
Reference: [4] <author> Jim Christy and David Wilkins. </author> <title> Parallel Ray Tracing Without Database Duplication. </title> <type> CS315B Project Report, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference: [5] <author> R. Cole and O. Zajicek. </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model. </title> <booktitle> In Proc. 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1989. </year>
Reference: [6] <author> D. Culler, R. Karp, et al. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM Sigplan Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM [1], Local-memory PRAM [1], Parallel Memory Hierarchy [12], Bulk Synchronous Processor or BSP [21], the Postal Model [3], logP <ref> [6] </ref>, and various asynchronous PRAMs [5][11]. Indeed, the topic of alternative complexity models is an active research area, and the theory community has yet to reach a consensus on which parallel machine model is the best one to present to an algorithm designer. <p> Clearly, what an end user of computers wants is an algorithm-architecture combination that delivers high performance at low cost. The factors that determine the cost of a machine are quite similar across different parallel computers. In fact, as argued by various articles including one in the context of modeling <ref> [6] </ref>, parallel computers are converging toward an architecture that uses the same commodity processors that are used in workstations. This architecture (see Figure 1) consists of some number of processing nodes connected by a general interconnection network. <p> These new models represent an architectures communication costs and capabilities by a small number of parameters, which may represent bandwidth, latency, overhead and perhaps topology, for example. Two of the more prominent models are the Bulk Synchronous Parallel (BSP) model [21] and the more recent logP model <ref> [6] </ref>. The BSP model, somewhat simplified here, requires that computation be organized in a series of bulk-synchronous super-steps, separated by global barrier synchronization.
Reference: [7] <author> Helen Davis, Stephen Goldschmidt and John L. Hennessy. </author> <title> Multiprocessor Simulation and Tracing using Tango. </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The data shown are for 16-processor runs with different cache sizes. The data was obtained using the Tango multiprocessor simulator <ref> [7] </ref>, coupled with a parallel memory system simulator. Clearly, the amount of communication depends very strongly on the amount of replication in all cases, supporting our claim that communication should not be described without reference to replication.
Reference: [8] <author> P. de la Torre and C. Kruskal, </author> <title> Towards a Single Model of Efficient Computation in Real Parallel Machines. </title> <booktitle> In Fifth Generation Computer Systems, </booktitle> <volume> vol. 8, no. 4, </volume> <pages> pp. 395-408. </pages>
Reference: [9] <author> Peter J. Denning. </author> <title> The working set model of program behavior. </title> <journal> Communications of the ACM, </journal> <volume> vol. 11, no. 5, </volume> <month> May </month> <year> 1968, </year> <pages> pp. 323-333. </pages>
Reference-contexts: With a shared address space, communication happens implicitly through references to shared data. Replication is also implicitly managed through automatic caching (typically by hardware). We assume a shared address space model in this paper, primarily be The working set model of program behavior <ref> [9] </ref> is based on the temporal locality exhibited by the data referencing patterns of programs. Under this model, a program (or process in a parallel program) has a set of data that it reuses substantially for a period of time, before moving on to other data.
Reference: [10] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Proc. Tenth ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1978. </year>
Reference-contexts: Among these, the aspect whose performance effects interact most closely with the architecture is communication, so we focus on communication costs in this paper. The dominant architectural model used by algorithm designers in the computer science theory community continues to be the PRAM (Parallel Random Access Machine) <ref> [10] </ref>. This model is very effective for reasoning purely about concurrency, and for proving complexity bounds.
Reference: [11] <author> P. B. Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proc. 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1989. </year>
Reference: [12] <author> T. Heywood and S. Ranka, </author> <title> A Practical Hierarchical Model of Parallel Computation: I. The Model. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <volume> vol. 16, </volume> <pages> pp. 212-232, </pages> <year> 1992. </year>
Reference-contexts: Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM [1], Local-memory PRAM [1], Parallel Memory Hierarchy <ref> [12] </ref>, Bulk Synchronous Processor or BSP [21], the Postal Model [3], logP [6], and various asynchronous PRAMs [5][11].
Reference: [13] <author> Donald E. Knuth. </author> <title> The Stanford GraphBase: A Platform for Combinatorial Computing. </title> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Since a large part of the difficulty of modeling communication properties is the non-uniformity and variance in the input data sets that an algorithm may be applied to, the development of scalable, representative input data sets (for example, <ref> [13] </ref>) is also very important in this regard. Techniques used in the parallel systems research community may provide a starting point for building a suite of parallel algorithm analysis tools. <p> These results about input dependence point to the need, where possible, to develop scalable input data sets for important algorithms that are represen tative of the important domains to which these algorithms are applied (Knuth <ref> [13] </ref> has already made a substantial step forward in this direction for graph algorithms.) When results for different data sets do not vary tremendously, this can lead to a characterization of the algorithm that holds for many important domains; and when the variations are large, the data sets can lead to
Reference: [14] <author> Daniel E. Lenoski et al. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference: [15] <author> Edward Rothberg and Anoop Gupta. </author> <title> An Efficient Block-Oriented Approach to Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. </booktitle> <address> Supercomputing93, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Scalable approaches to parallel sparse Cholesky factorization distribute rectangular patches (or blocks) of the sparse matrix among the processors using a 2-D distribution The 2-D parallel factorization approach we consider here, the block fanout method <ref> [15] </ref>, forms blocks by partitioning the rows and columns of the matrix into contiguous subsets. A matrix block L IJ then consists of all non-zeroes in the intersection of a row subset I and a column-subset J. Since the matrix is sparse, many blocks will contain no non-zeroes.
Reference: [16] <author> Edward Rothberg, Jaswinder Pal Singh and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity for Large-Scale Multiprocessors. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: The amount of communication thus depends strongly on the amount of replication that is done, making it important to describe the communication-replication relationship rather than simply communication. We propose a description of this relationship in terms of the hierarchy of working sets of a parallel algorithm <ref> [16] </ref>. As we shall see, this relationship is even more difficult to determine analytically for complex algorithms and problems than the amount of inherent communication assuming unlimited replication.
Reference: [17] <author> Jaswinder Pal Singh. </author> <title> Parallel hierarchical N-body methods and their implications for multiprocessors. </title> <type> Ph.D. Thesis, Technical Report no. </type> <institution> CSL-TR-93-565, Stanford University, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Our results apply to a message passing model as well, although this model requires that we explicitly determine which data to replicate and when to replicate it (which is often a difficult task <ref> [17] </ref>). 3.4 Sources of Communication There are several sources of communication on a multiprocessor that supports a shared address space with automatic caching of shared data.
Reference: [18] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: methodology and examples. </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1993. </year>
Reference: [19] <author> Jaswinder Pal Singh et al. </author> <title> Load balancing and data locality in parallel hierarchial N-body simulation. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: For parallel computation, excellent load balancing and data locality are obtained by introducing a low-cost partitioning phase called costzones <ref> [19] </ref> in every time-step, which partitions particles so that the particles assigned to a processor are load balanced (in work, not necessarily in number) and are close together in space for data locality.
Reference: [20] <author> Susan Spach and Ronald Pulleyblank. </author> <title> Parallel Raytraced Image Generation. </title> <journal> Hewlett-Packard Journal, </journal> <volume> vol. 43, no. 3, </volume> <pages> pages 76-83, </pages> <month> June </month> <year> 1992. </year>
Reference: [21] <author> Leslie Valiant. </author> <title> A Bridging Model for Parallel Computation Communications of the ACM, </title> <journal> vol. </journal> <volume> 33, no. 8, </volume> <year> 1990, </year> <pages> pp. 103-111. </pages>
Reference-contexts: A successful bridging model for parallel computers <ref> [21] </ref> is essential if parallel algorithms are to become as portable and usefully analyzable as sequential algorithms. Analyzing parallel algorithms, however, is inherently more difficult than analyzing sequential algorithms, since parallel algorithms and architectures introduce several new characteristics that must be modeled. <p> Recently, several alternative models have been proposed to address this limitation (as well as others such as asynchrony and contention) of the PRAM model. These include the Block-PRAM [1], Local-memory PRAM [1], Parallel Memory Hierarchy [12], Bulk Synchronous Processor or BSP <ref> [21] </ref>, the Postal Model [3], logP [6], and various asynchronous PRAMs [5][11]. Indeed, the topic of alternative complexity models is an active research area, and the theory community has yet to reach a consensus on which parallel machine model is the best one to present to an algorithm designer. <p> Reducing the communication requirements of an algorithm therefore allows that algorithm to be run effectively on a lower cost machine. Reducing communication in an algorithm has the following three benefits: (i) it reduces the latency of data accesses, resulting in higher performance, (ii) it reduces the parallel slackness <ref> [21] </ref> required to hide the remaining communication latency, and (iii) it reduces the bandwidth required of the architecture to satisfy communication and hide communication latencies, allowing good performance on lower cost architectures. <p> These new models represent an architectures communication costs and capabilities by a small number of parameters, which may represent bandwidth, latency, overhead and perhaps topology, for example. Two of the more prominent models are the Bulk Synchronous Parallel (BSP) model <ref> [21] </ref> and the more recent logP model [6]. The BSP model, somewhat simplified here, requires that computation be organized in a series of bulk-synchronous super-steps, separated by global barrier synchronization.
References-found: 21

