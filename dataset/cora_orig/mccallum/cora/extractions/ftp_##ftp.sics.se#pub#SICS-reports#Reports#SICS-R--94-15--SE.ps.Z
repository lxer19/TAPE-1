URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--94-15--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Email: e-mail: fans,tim,retrac,landin,seifg@sics.se  
Title: An Argument for Simple COMA  
Author: Ashley Saulsbury, Tim Wilkinson John Carter Anders Landin and Seif Haridi 
Note: SICS Research Report Number R94:15 Permanent address:  
Date: August 1, 1994  
Address: Box 1263, S-164 28 Kista, Sweden  3190 MEB, Salt Lake City, UT 84112  
Affiliation: Swedish Institute of Computer Science  University of Utah, Department of Computer Science,  
Abstract: We present design details and some initial performance results of a novel scalable shared memory multiprocessor architecture that incorporates the major strengths of several contemporary multiprocessor architectures while avoiding their most serious weaknesses. Specifically, our architecture design incorporates the automatic data migration and replication features of cache-only memory architecture (COMA) machines, but replaces much of the complex hardware of COMA with a software layer that manages page-grained cache space allocation, as found in distributed virtual shared memory (DVSM) systems. Unlike DVSM however, pages are sub-divided into cache-line sized blocks, and for shared pages the coherence of these blocks is maintained by hardware. Moving much of COMA's hardware functionality to software simplifies the machine design and reduces development time, while supporting fine-grain coherence in hardware greatly decreases the impact of DVSM software overheads. We call the resulting hybrid hardware and software multiprocessor architecture Simple COMA. By allowing shared data to be replicated in a node's main memory (in addition to its caches), the number of remote memory accesses is greatly reduced compared to a traditional cache coherent non-uniform memory access (CC-NUMA) architecture. Preliminary results indicate that despite the reduced hardware complexity and the need to handle allocation page faults in software, the performance of Simple COMA is comparable to that of more complex all-hardware designs. fl Permanent address: Systems Architecture Research Centre, City University, Northampton Square, London, UK
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference: [2] <author> J-L. Baer and W-H. Wang. </author> <title> On the Inclusion Properties for Multi-Level Cache Hierarchies. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <year> 1988. </year>
Reference-contexts: For each valid cache line in the first or second level caches, there must be corresponding space allocated in the node's main memory. Thus, Simple COMA's attraction memory is fully inclusive <ref> [2] </ref> with respect to the various levels of cache. Because the attraction memory is fully inclusive, no action is required for invalid coherence units on replacement of their containing page. If the state is invalid in the attraction memory, data cannot be in a higher level cache either.
Reference: [3] <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the 4th Annual Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <year> 1991. </year>
Reference-contexts: This naturally leads to pressure when designing CC-NUMAs to build very large (and expensive) second level caches. The addition of operating system managed page migration <ref> [3] </ref> allows the designated "home" of a data page (the node where the corresponding physical memory resides) to be moved. This goes a little way towards solving the problem of incorrect data partitioning.
Reference: [4] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA [19]), such as the SICS DDM [7] and the KSR-1 <ref> [4] </ref>, the machine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Thirteenth Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This allows greater and more flexible data replication, but requires more complex hardware and data coherence protocols. Distributed virtual shared memory (DVSM) systems, such as IVY [11] and Munin <ref> [5] </ref>, forgo all hardware support for coherence management. A DVSM system can exhibit COMA-like properties by allowing pages to migrate and be replicated from node to node while still being kept coherent. <p> In addition to having the potential to implement more sophisticated coherence management mechanisms than are feasible in hardware-only systems, DVSM systems have very simple hardware requirements. DVSM systems can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 [13] to a network of conventional workstations <ref> [5, 11] </ref>. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high. <p> This overhead is increasing as network bandwidths improve. The problem can be mitigated by introducing a dedicated network technology, as was done in the Meiko CS-2 [12], or using delayed consistency to reduce the number of messages sent, as was done in Munin <ref> [5] </ref>, at the cost of more hardware or more complex coherence protocols. 6 Finally, DVSM systems are particularly sensitive to false sharing, wherein unrelated data items that happen to reside on the same page can interfere with one another because the MMU provides support only at the granularity of a page.
Reference: [6] <author> E. Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm/ Swedish Institute of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: This dynamic rebalancing of the data held on a node means that COMA can exhibit all of the properties of CC-NUMA, including those of a CC-NUMA with page migration, for CC-NUMA tailored applications. In addition, COMA machines can run applications that do not map well to CC-NUMA architectures <ref> [6] </ref>, such as applications that have a per-node working set larger than the size of each node's cache and applications with dynamic data access patterns in which data cannot be statically partitioned across the physical memories effectively.
Reference: [7] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA [19]), such as the SICS DDM <ref> [7] </ref> and the KSR-1 [4], the machine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware. <p> For a COMA such as the SICS DDM <ref> [7] </ref>, this extra hardware consists of: (i) a hashing function to map from the physical address used by the processor to the real physical address of the set of DRAM cache lines that might hold this entry, (ii) tag memory and comparators that determine which cache line in the designated set
Reference: [8] <author> E. Hagersten, A. Saulsbury, and A. Landin. </author> <title> Simple COMA node implementations. </title> <booktitle> In Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: followup to this work is Typhoon/Tempest [14], which describes a fine grained user level DVSM system with dedicated hardware to provide fine grained access control and a programmable controller to support the DVSM message communications. 3 Simple COMA In this section we describe in more detail the Simple COMA idea <ref> [8, 15] </ref> and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an Attraction Memory In Simple COMA, the attraction memory is built using a combination of software and hardware.
Reference: [9] <author> T. Joe and J.L. Hennessy. </author> <title> Evaluating the memory overhead required for coma architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <year> 1994. </year>
Reference-contexts: is complicated by the need to preserve the last copy of a data item; unlike a CC-NUMA there is no reserved home for a cache line that is evicted from a node's attraction memory, so care must be taken not to throw away the last copy of a cache line <ref> [9] </ref>. <p> Unlike in a traditional COMA, transferring a coherence unit to another node cannot force further replacement on that node <ref> [9] </ref>, since in Simple COMA that node will already have the space allocated for the coherence units if it is sharing the page.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: We call this design Simple COMA. Possibly the most popular large scale shared memory multiprocessor design is the cache coherent non-uniform memory access (CC-NUMA) architecture, such as embodied by the Stanford DASH <ref> [10] </ref> and the Convex Exemplar machines. In a CC-NUMA, the machine's physical memory is distributed across the nodes in the machine, but every node can map any page of physical memory (local or remote).
Reference: [11] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This allows greater and more flexible data replication, but requires more complex hardware and data coherence protocols. Distributed virtual shared memory (DVSM) systems, such as IVY <ref> [11] </ref> and Munin [5], forgo all hardware support for coherence management. A DVSM system can exhibit COMA-like properties by allowing pages to migrate and be replicated from node to node while still being kept coherent. <p> There are two major advantages to software DVSM systems. As they are implemented entirely in software, they can employ sophisticated algorithms to manage memory and adapt to changing memory access patterns. Even early DVSM systems like IVY <ref> [11] </ref> exhibited many of the desirable properties of COMA, e.g., automatic data migration and replication. In addition to having the potential to implement more sophisticated coherence management mechanisms than are feasible in hardware-only systems, DVSM systems have very simple hardware requirements. <p> In addition to having the potential to implement more sophisticated coherence management mechanisms than are feasible in hardware-only systems, DVSM systems have very simple hardware requirements. DVSM systems can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 [13] to a network of conventional workstations <ref> [5, 11] </ref>. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high.
Reference: [12] <author> Meiko Limited. </author> <title> CS-2 Product Description, </title> <year> 1992. </year>
Reference-contexts: This overhead is increasing as network bandwidths improve. The problem can be mitigated by introducing a dedicated network technology, as was done in the Meiko CS-2 <ref> [12] </ref>, or using delayed consistency to reduce the number of messages sent, as was done in Munin [5], at the cost of more hardware or more complex coherence protocols. 6 Finally, DVSM systems are particularly sensitive to false sharing, wherein unrelated data items that happen to reside on the same page
Reference: [13] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In addition to having the potential to implement more sophisticated coherence management mechanisms than are feasible in hardware-only systems, DVSM systems have very simple hardware requirements. DVSM systems can operate on anything from tightly coupled distributed memory multiprocessors like the CM-5 <ref> [13] </ref> to a network of conventional workstations [5, 11]. However, these advantages of DVSM systems come at a potentially serious cost in terms of performance. Because all coherence management is done in software, the overheads associated with DVSM are high. <p> The latter requires hardware support not available on most platforms, unless, as in the case of the Wisconsin Wind Tunnel, you can exploit a feature of the underlying machine architecture. The Wisconsin Windtunnel <ref> [13] </ref> is a fine grained DVSM system developed to create an efficient parallel simulator for shared memory architectures that uses the CM-5's unique ECC hardware to detect "invalidated" cache lines. Accesses to these cache lines generate ECC exceptions.
Reference: [14] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <year> 1994. </year>
Reference-contexts: The Wisconsin Windtunnel [13] is a fine grained DVSM system developed to create an efficient parallel simulator for shared memory architectures that uses the CM-5's unique ECC hardware to detect "invalidated" cache lines. Accesses to these cache lines generate ECC exceptions. A recent followup to this work is Typhoon/Tempest <ref> [14] </ref>, which describes a fine grained user level DVSM system with dedicated hardware to provide fine grained access control and a programmable controller to support the DVSM message communications. 3 Simple COMA In this section we describe in more detail the Simple COMA idea [8, 15] and examine various aspects of
Reference: [15] <author> A. Saulsbury. </author> <title> Supporting fine-grain shared memory. </title> <note> Technical Report from January 1993, now publically available as SICS Tech Report number T94:06, </note> <institution> Swedish Institute of Computer Science, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: followup to this work is Typhoon/Tempest [14], which describes a fine grained user level DVSM system with dedicated hardware to provide fine grained access control and a programmable controller to support the DVSM message communications. 3 Simple COMA In this section we describe in more detail the Simple COMA idea <ref> [8, 15] </ref> and examine various aspects of the architecture that allow a simpler hardware architecture than COMA or CC-NUMA, but without compromising the performance. 3.1 Building an Attraction Memory In Simple COMA, the attraction memory is built using a combination of software and hardware.
Reference: [16] <author> A. Saulsbury, T. Wilkinson, J. B. Carter, and A. Landin. </author> <title> Handling replacement in simple COMA. </title> <institution> SICS Research Report, Swedish Institute of Computer Science, </institution> <year> 1994. </year> <month> 19 </month>
Reference-contexts: Then, the page being replaced can be evicted by the coherence hardware while the computation continues. A full description of the analytical model used in this study can be found elsewhere <ref> [16] </ref>. 5.4 Simulation Results Figures 2 through 4 illustrate the estimated extra number of machine cycles required to handle first and second level cache misses and inter-node coherence-related communication for a CC-NUMA, a traditional COMA, and a Simple COMA machine.
Reference: [17] <author> J.S. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <institution> Stanford University, </institution> <type> Report, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: information was fed into an analytical model that calculated the approximate number of machine cycles the application computation is stalled for each architecture and the impact of Simple COMA's page replacement overhead on performance. 5.1 Applications The simulations in this study modeled moderately sized machines running applications from the Splash <ref> [17] </ref> benchmark suite. The applications and their data sets chosen from the SPLASH benchmark suite were: MP3D A simple simulator for rarified gas flow over an object in a wind tunnel. Data set : 80000 particles for 40 time steps using the default test geometry.
Reference: [18] <author> M. Thapar and B. Delagi. </author> <title> Stanford Distributed-Directory Protocol. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 78-80, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: software must designate an "owner of last resort" for each page that cannot silently discard the page without designating a new "owner of last resort." Finally, if the list of coherence unit copies is distributed with the data in a coherence protocol design, as in the Stanford Distributed Directory protocol <ref> [18] </ref>, then the coherence units must be unhooked from the linked lists before the page can be reallocated. If the page chosen for replacement is not being shared by another node, the operating system can either write the data out to disk or persuade another node to take the page.
Reference: [19] <author> D.H.D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-a scalable shared virtual memory multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year> <month> 20 </month>
Reference-contexts: However, the amount of remote data that can be replicated locally, and thus accessed efficiently, is limited by the size of a node's cache, which exacerbates the need for large and expensive caches. In a cache-only memory architecture (COMA <ref> [19] </ref>), such as the SICS DDM [7] and the KSR-1 [4], the machine's memory is again distributed across the nodes in the machine. However, instead of being allocated to fixed physical addresses, each node's memory is converted into a large, slow 1 cache (or attraction memory) by additional hardware.
References-found: 19

