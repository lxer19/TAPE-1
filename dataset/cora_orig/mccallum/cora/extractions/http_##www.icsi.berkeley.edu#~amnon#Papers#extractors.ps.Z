URL: http://www.icsi.berkeley.edu/~amnon/Papers/extractors.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~amnon/
Root-URL: http://www.icsi.berkeley.edu
Title: On Extracting Randomness From Weak Random Sources Extended Abstract  
Author: Amnon Ta-Shma 
Abstract: We deal with the problem of extracting as much randomness as possible from a defective random source. We devise a new tool, a "merger", which is a function that accepts d strings, one of which is uniformly distributed, and outputs a single string that is guaranteed to be uniformly distributed. We show how to build good explicit mergers, and how mergers can be used to build better extractors. Previous work has succeeded in extracting "some" of the randomness from sources with "large" min-entropy. We improve on this in two respects. First, we build extractors for any source, whatever its min-entropy is, and second, we extract all the randomness in the given source. Efficient extractors have many applications, and we show that using our extractor we get better results in many of these applications, e.g., we achieve the first explicit N -superconcentrators of linear size and polyloglog(N ) depth. 
Abstract-found: 1
Intro-found: 1
Reference: [AKSS89] <author> M. Ajtai, J. Komlos, W. Steiger, and E. Szemeredi. </author> <title> Almost sorting in one round. </title> <booktitle> In Advances in Computer Research, </booktitle> <volume> volume 5, </volume> <pages> pages 117-125, </pages> <year> 1989. </year>
Reference-contexts: Corollary 1.1 (following [WZ93]) For any N and 1 a N there is an explicitly constructible a expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) 2 . Corollary 1.1 has applications on sorting and se lecting in k rounds. See <ref> [WZ93, Pip87, AKSS89] </ref> for more details.
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: Notice that Theorem 3 and corollary 4.2 take advantage of the simple structure of somewhere random sources, giving us an explicit somewhere ran dom merger that works even for sources with very small min-entropy that can not use the [SZ94] extractor of lemma 3.2. 4.3 Composing Extractors <ref> [CG88] </ref> showed how to extract randomness from sources X that can be "broken" into blocks X 1 ffi X 2 , s.t. X 1 and (X 2 j X 1 = x 1 ) contain a lot of randomness.
Reference: [GW94] <author> O. Goldreich and A. Wigderson. </author> <title> Tiny families of functions with random properties: A quality-size trade-off for hashing. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 574-583, </pages> <year> 1994. </year>
Reference-contexts: The first extractor is actually a restatement of the existence of explicit tiny families of hash func tions <ref> [SZ94, GW94] </ref>, and can easily be achieved using small *-biased sample spaces [SZ94]. We also need the following simple lemma from [NZ93]: Lemma 3.3 [NZ93] Let X and Y be two correlated random variables.
Reference: [NZ93] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in logspace. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: An extractor <ref> [NZ93] </ref> is a function E (x; y) s.t. the distribution of E (x; y) is very close to uniform, for any source X with large min-entropy, when x is taken from X and y is a short truly random string. <p> I.e., the extractor uses the short random string y to extract the randomness present in X. Definition 1.2 ( A variation on <ref> [NZ93] </ref> 1 ) E : f0; 1g n fi f0; 1g t 7! f0; 1g m 0 extractor, if for any distribution X on f0; 1g n whose min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in <p> min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95]. <p> Non-constructive extrac tors require only O (log (n) + log ( 1 * )) truly random bits, matching the current lower bound of <ref> [NZ93] </ref>, while our construction requires poly (log (n); log ( 1 * )) truly random bits. <p> Remark 3.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, where E is called an extractor if E (x; y) ffi y is close to uniform, while we only demand that E (x; y) is close to uniform. 3.2 Explicit Extractors Definition 3.3 We say E = fE n g is an explicit (n; m; t; m 0 ; <p> The first extractor is actually a restatement of the existence of explicit tiny families of hash func tions [SZ94, GW94], and can easily be achieved using small *-biased sample spaces [SZ94]. We also need the following simple lemma from <ref> [NZ93] </ref>: Lemma 3.3 [NZ93] Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> The first extractor is actually a restatement of the existence of explicit tiny families of hash func tions [SZ94, GW94], and can easily be achieved using small *-biased sample spaces [SZ94]. We also need the following simple lemma from <ref> [NZ93] </ref>: Lemma 3.3 [NZ93] Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> We observe that a 2-block merger can be obtained from the previously designed extractors of <ref> [NZ93, SZ94] </ref>.
Reference: [Pip87] <author> N. Pippenger. </author> <title> Sorting and selecting in rounds. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16 </volume> <pages> 1032-1038, </pages> <year> 1987. </year>
Reference-contexts: Corollary 1.1 (following [WZ93]) For any N and 1 a N there is an explicitly constructible a expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) 2 . Corollary 1.1 has applications on sorting and se lecting in k rounds. See <ref> [WZ93, Pip87, AKSS89] </ref> for more details.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" <ref> [WZ93, SSZ95] </ref>. <p> currently known explicitly constructible extractors: m min-ent t random bits m 0 reference (n) O (log 2 n log ( 1 (n) O (log (n) + log ( 1 (n 1=2+fl ) O (log 2 n log ( 1 (n fl ) O (logn) n ffi ; ffi &lt; fl <ref> [SSZ95] </ref> Disperser 1 see remark 3.1. We improve upon previous results in two ways. First, our extractor works for any min-entropy, small or large. <p> The previous upper bound [WZ93, SZ94] was O ( N a 2 log (N) 1=2+o (1) 3 This improves the current upper bound of O (log (N) 1=2+o (1) ). 4 This improves the [SZ94] n O (log (n)) bound. For RP , <ref> [SSZ95] </ref> showed this can be done in polynomial time. The corollary immediately follows Theorem 2. A proof of Theorem 2 will appear in the final version of the paper. 2 Notation We use standard notation for random variables and distributions.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95]. <p> Due to lack of space, we omit many of the technical proofs. Also, the proof of Theorem 2 will only appear in the final version of the paper. 2 The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 3 This improves the current upper bound of O (log (N) 1=2+o (1) ). 4 This improves the [SZ94] n O (log (n)) bound. For RP , [SSZ95] showed this can be done in polynomial time. <p> The previous upper bound [WZ93, SZ94] was O ( N a 2 log (N) 1=2+o (1) 3 This improves the current upper bound of O (log (N) 1=2+o (1) ). 4 This improves the <ref> [SZ94] </ref> n O (log (n)) bound. For RP , [SSZ95] showed this can be done in polynomial time. The corollary immediately follows Theorem 2. A proof of Theorem 2 will appear in the final version of the paper. 2 Notation We use standard notation for random variables and distributions. <p> Remark 3.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, where E is called an extractor if E (x; y) ffi y is close to uniform, while we only demand that E (x; y) is close to uniform. 3.2 Explicit Extractors Definition 3.3 We say E = fE n g is an explicit (n; m; t; m 0 ; <p> As mentioned in the introduction, various explicit extractors have been built so far. Of those we use the following two extractors: Lemma 3.1 <ref> [SZ94] </ref> There is some constant c &gt; 1 s.t. for any m = (log (n)) there is an explicit (n; 2m; m; cm; 2 m=2 )- extractor A m . We denote this constant c by c sz . Lemma 3.2 [SZ94] 5 Let m (n) n 1=2+fl for some constant <p> those we use the following two extractors: Lemma 3.1 <ref> [SZ94] </ref> There is some constant c &gt; 1 s.t. for any m = (log (n)) there is an explicit (n; 2m; m; cm; 2 m=2 )- extractor A m . We denote this constant c by c sz . Lemma 3.2 [SZ94] 5 Let m (n) n 1=2+fl for some constant fl &gt; 0, then for any * there is an explicit (n; m (n); O (log 2 n log ( 1 m 2 (n) n ; *)- extractor. <p> The first extractor is actually a restatement of the existence of explicit tiny families of hash func tions <ref> [SZ94, GW94] </ref>, and can easily be achieved using small *-biased sample spaces [SZ94]. We also need the following simple lemma from [NZ93]: Lemma 3.3 [NZ93] Let X and Y be two correlated random variables. <p> The first extractor is actually a restatement of the existence of explicit tiny families of hash func tions [SZ94, GW94], and can easily be achieved using small *-biased sample spaces <ref> [SZ94] </ref>. We also need the following simple lemma from [NZ93]: Lemma 3.3 [NZ93] Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. <p> In subsection 4.1 we define what 5 The parameters here are simplified. The real parameters appearing in <ref> [SZ94] </ref> are somewhat better. a "merger" is, and in subsection 4.2 we explicitly build "mergers". In subsection 4.3 we use mergers to efficiently compose extractors, and in subsection 4.4 we show how good somewhere random mergers imply good extractors. <p> Then there is an explicit (2 l ; m; l t (m); m l k (m); l *(m)) somewhere random merger. The <ref> [SZ94] </ref> extractor of lemma 3.2 works for any source with H 1 (X) n 1=2+fl . Thus, using lemma 3.4, by repeatedly using the [SZ94] extractor, we can extract from a source having H 1 (X) n 2 at least n 2 n 1=2+fl quasi-random bits. <p> Then there is an explicit (2 l ; m; l t (m); m l k (m); l *(m)) somewhere random merger. The <ref> [SZ94] </ref> extractor of lemma 3.2 works for any source with H 1 (X) n 1=2+fl . Thus, using lemma 3.4, by repeatedly using the [SZ94] extractor, we can extract from a source having H 1 (X) n 2 at least n 2 n 1=2+fl quasi-random bits. Thus, we have a 2-merger that does not lose much randomness in the merging process. Applying Theorem 3 we get a good n-merger. <p> Notice that Theorem 3 and corollary 4.2 take advantage of the simple structure of somewhere random sources, giving us an explicit somewhere ran dom merger that works even for sources with very small min-entropy that can not use the <ref> [SZ94] </ref> extractor of lemma 3.2. 4.3 Composing Extractors [CG88] showed how to extract randomness from sources X that can be "broken" into blocks X 1 ffi X 2 , s.t. X 1 and (X 2 j X 1 = x 1 ) contain a lot of randomness. <p> Al though O (2 p log (n) polylog (n) log ( 1 * )) is quite a large amount of truly random bits, we can use the <ref> [SZ94] </ref> extractor, to extract n 1=3 bits from n 2=3 min-entropy, and then use these n 1=3 &gt;> O (2 p polylog (n) log ( 1 * )) bits, to further extract all of the remaining min-entropy. <p> We observe that a 2-block merger can be obtained from the previously designed extractors of <ref> [NZ93, SZ94] </ref>.
Reference: [WZ93] <author> A. Wigderson and D. Zuckerman. </author> <title> Ex--panders that beat the eigenvalue bound: Explicit construction and applications. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" <ref> [WZ93, SSZ95] </ref>. <p> Most of the applications we list here, can be obtained by plugging our new extractor into previous constructions. Corollary 1.1 (following <ref> [WZ93] </ref>) For any N and 1 a N there is an explicitly constructible a expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) 2 . Corollary 1.1 has applications on sorting and se lecting in k rounds. <p> Corollary 1.1 (following [WZ93]) For any N and 1 a N there is an explicitly constructible a expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) 2 . Corollary 1.1 has applications on sorting and se lecting in k rounds. See <ref> [WZ93, Pip87, AKSS89] </ref> for more details. <p> Corollary 1.1 has applications on sorting and se lecting in k rounds. See [WZ93, Pip87, AKSS89] for more details. Corollary 1.2 (following <ref> [WZ93] </ref>) For every N there is an efficiently constructible depth 2 su perconcentrator over N vertices with size O (N 2 polyloglog (N) ). [WZ93] prove that a direct corollary of cor 1.3 is: Corollary 1.3 ([WZ93]) For any N there is an explicitly constructible superconcentrator over N vertices, with linear <p> Corollary 1.1 has applications on sorting and se lecting in k rounds. See [WZ93, Pip87, AKSS89] for more details. Corollary 1.2 (following <ref> [WZ93] </ref>) For every N there is an efficiently constructible depth 2 su perconcentrator over N vertices with size O (N 2 polyloglog (N) ). [WZ93] prove that a direct corollary of cor 1.3 is: Corollary 1.3 ([WZ93]) For any N there is an explicitly constructible superconcentrator over N vertices, with linear size and polyloglog (N ) depth 3 . <p> Due to lack of space, we omit many of the technical proofs. Also, the proof of Theorem 2 will only appear in the final version of the paper. 2 The obvious lower bound is N a . The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 3 This improves the current upper bound of O (log (N) 1=2+o (1) ). 4 This improves the [SZ94] n O (log (n)) bound. For RP , [SSZ95] showed this can be done in polynomial time. <p> close to X fi B. 3.3 More of The Same Suppose we have an extractor E that extracts randomness from any source having at least m min entropy, how much randomness can we extract from sources having M min-entropy when M &gt;> m ? The following algorithm is implicit in <ref> [WZ93] </ref>: use the same extractor E many times over the same string x, each time with a fresh truly random string r i , until you get M m output bits.
Reference: [Zuc] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors, and constructive leader election. </title> <type> Private Communication. </type>
Reference-contexts: min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95].
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: There are various ways to define what fl This work was supported by BSF grant 92-00043 and by a Wolfeson award administered by the Israeli Academy of Sciences. y Institute of Computer Science, Hebrew University of Jerusalem. email: am@cs.huji.ac.il a "defective" source is, the most general one <ref> [Zuc90] </ref> assumes nothing about the nature of the source, except that no string has too high a probability in the given distribution: Definition 1.1 The min-entropy of a distribution D is H 1 (D) = min x (log (D (x)). <p> min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95].
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95].
Reference: [Zuc93] <author> D. Zuckerman. </author> <title> NP-complete problems have a version that's hard to approximate. </title> <booktitle> In Proceedings of the 8th Structures in Complexity Theory, IEEE, </booktitle> <pages> pages 305-312, </pages> <year> 1993. </year>
Reference-contexts: min-entropy is at least m, the distribution of E (x; y) when choosing x according to the distribution X and y uniformly in f0; 1g t , is within statistical distance of * from uniform. 1.2 Previous Work and Our New Extractor Extractors have been studied extensively in many papers <ref> [Zuc90, Zuc91, NZ93, SZ94, Zuc93, Zuc] </ref>. Much research has also been done on a related structure, called disperser, in which the output bits are not necessarily close to uniform, but have a weaker random property that suffices ,e.g., for "RP simulations" [WZ93, SSZ95]. <p> Our results can also prove a deterministic version of the hardness of approximating the iterated log of MaxClique. See <ref> [Zuc93] </ref> for more details. 1.5 Paper Organization In section 2 we describe our notation, and in section 3 we give some preliminary definitions and results.
References-found: 12

