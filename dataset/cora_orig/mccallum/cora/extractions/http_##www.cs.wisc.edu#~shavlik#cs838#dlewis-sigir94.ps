URL: http://www.cs.wisc.edu/~shavlik/cs838/dlewis-sigir94.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Email: (gale@research.att.com)  
Title: A Sequential Algorithm for Training Text Classifiers  
Author: David D. Lewis (lewis@research.att.com) and William A. Gale In W. Bruce Croft and C. J. van Rijsbergen, 
Date: 3-12.  
Note: eds., SIGIR 94: Proceedings of Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, Springer-Verlag, London, pp.  
Address: Murray Hill, NJ 07974; USA  
Affiliation: AT&T Bell Laboratories;  
Abstract: The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. J. Hayes. </author> <title> Intelligent high-volume text processing using shallow, domain-specific techniques. </title> <editor> In Paul. S. Jacobs, editor, </editor> <booktitle> Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction, and Retrieval, </booktitle> <pages> pages 227-241. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: While using machine learning does require manually annotating training data with class labels, this annotation takes less skill and expense than, for instance, building classification rules by hand <ref> [1] </ref>. There is often more text available than can be economically labeled, so a subset or sample of the data must be chosen to label. 1 Random sampling [3] will usually not be effective.
Reference: 2. <author> P. Biebricher, N. Fuhr, G. Lustig, M. Schwantner, and G. Knorz. </author> <title> The automatic indexing system AIR/PHYS|from research to application. </title> <booktitle> In Proc. SIGIR-88, </booktitle> <pages> pages 333|342, </pages> <year> 1988. </year>
Reference-contexts: algorithm is unlikely to be a legitimate natural language expression, and probably would be uninterpretable by a human teacher. 1 An exception is when large quantities of previously labeled text are available, as when automated text categorization is being deployed to replace or aid an existing staff of manual indexers <ref> [2] </ref>. 2 In this paper "queries" always refers to membership queries, not text retrieval queries. 1 1. Create an initial classifier 2.
Reference: 3. <author> W. G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <note> 3rd edition, </note> <year> 1977. </year>
Reference-contexts: There is often more text available than can be economically labeled, so a subset or sample of the data must be chosen to label. 1 Random sampling <ref> [3] </ref> will usually not be effective. If only 1 in 1000 texts are class members (not atypical), and only 500 texts can be labeled, then a random sample will usually contain 500 negative examples and no positive ones.
Reference: 4. <author> G. Salton and C. Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: If only 1 in 1000 texts are class members (not atypical), and only 500 texts can be labeled, then a random sample will usually contain 500 negative examples and no positive ones. This will not support training a classifier to distinguish positive from negative examples. Relevance feedback <ref> [4] </ref> does a kind of nonrandom sampling. In effect, users are asked to label those texts that the current classifier considers most likely to be class members.
Reference: 5. <author> W. A. Gale, K. W. Church, and D. Yarowsky. </author> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439, </pages> <year> 1993. </year>
Reference-contexts: This approach, which we might call relevance sampling, is a reasonable strategy in a text retrieval context, where the user is more interested in seeing relevant texts than in the effectiveness of the final classifier produced. Relevance feedback has also been proposed for finding examples of unusual word senses <ref> [5] </ref>. However, relevance feedback has many problems as an approach to sampling. It works more poorly as the classifier improves, and is susceptible to selecting redundant examples. Relevance sampling is a sequential approach to sampling, since the labeling of earlier examples influences the selection of later ones [6]. <p> use it for uncertainty sampling and classification. 4.1 A Probabilistic Classifier Classifiers which estimate the posterior probability via Bayes' Rule: P (C i jw) = P q (1) have been applied to a variety of text classification tasks, including text retrieval [18], text categorization [19, 20], and word sense identification <ref> [5] </ref>.
Reference: 6. <author> B. K. Ghosh. </author> <title> A brief history of sequential analysis. </title> <editor> In B. K. Ghosh and P. K. Sen, editors, </editor> <title> Handbook of Sequential Analysis, </title> <booktitle> chapter 1, </booktitle> <pages> pages 1-19. </pages> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: However, relevance feedback has many problems as an approach to sampling. It works more poorly as the classifier improves, and is susceptible to selecting redundant examples. Relevance sampling is a sequential approach to sampling, since the labeling of earlier examples influences the selection of later ones <ref> [6] </ref>. This paper describes an alternative sequential approach, uncertainty sampling, motivated by results in computational learning theory. Uncertainty sampling is an iterative process of manual labeling of examples, classifier fitting from those examples, and use of the classifier to select new examples whose class membership is unclear.
Reference: 7. <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: in the number of examples that must be labeled to produce a classifier with a given effectiveness. 2 Learning with Queries A classifier can often be learned from fewer examples if the learning algorithm is allowed to create artificial examples or membership queries and ask a teacher to label them <ref> [7, 8] </ref>. 2 In many learning tasks creation of artificial examples is no problem.
Reference: 8. <author> M. Plutowski and H. White. </author> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 305-318, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: in the number of examples that must be labeled to produce a classifier with a given effectiveness. 2 Learning with Queries A classifier can often be learned from fewer examples if the learning algorithm is allowed to create artificial examples or membership queries and ask a teacher to label them <ref> [7, 8] </ref>. 2 In many learning tasks creation of artificial examples is no problem. <p> A single classifier approach to uncertainty sampling has several theoretical failings, including underestimation of true uncertainty, and biases caused by nonrepresentative classifiers [9, 10]. On the other hand, experiments using a single classifier to make arbitrary queries [14] or select subsets of labeled data <ref> [8, 15] </ref> have shown substantial speedups in learning. Relevance sampling, which has proven quite effective for text retrieval, also uses a single classifier. 3 An Uncertainty Sampling Algorithm classifier.
Reference: 9. <author> D. Cohn, L. Atlas, and R. Ladner. </author> <title> Improving generalization with self-directed learning, </title> <note> 1992. To appear in Machine Learning. </note>
Reference-contexts: b examples for which the classifier is least certain of class membership (c) Have the teacher label the subsample of b examples (d) Train a new classifier on all labeled examples Recently, several algorithms for learning via queries have been proposed that filter existing examples rather than creating artificial ones <ref> [9, 10, 11] </ref>. These algorithms ask a teacher to label only those examples whose class membership is sufficiently "uncertain". <p> If the classifier can not only make classification decisions, but estimate their certainty, the certainty estimate can be used to select examples. A single classifier approach to uncertainty sampling has several theoretical failings, including underestimation of true uncertainty, and biases caused by nonrepresentative classifiers <ref> [9, 10] </ref>. On the other hand, experiments using a single classifier to make arbitrary queries [14] or select subsets of labeled data [8, 15] have shown substantial speedups in learning.
Reference: 10. <author> D. J. C. MacKay. </author> <title> The evidence framework applied to classification networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 720-736, </pages> <year> 1992. </year>
Reference-contexts: b examples for which the classifier is least certain of class membership (c) Have the teacher label the subsample of b examples (d) Train a new classifier on all labeled examples Recently, several algorithms for learning via queries have been proposed that filter existing examples rather than creating artificial ones <ref> [9, 10, 11] </ref>. These algorithms ask a teacher to label only those examples whose class membership is sufficiently "uncertain". <p> If the classifier can not only make classification decisions, but estimate their certainty, the certainty estimate can be used to select examples. A single classifier approach to uncertainty sampling has several theoretical failings, including underestimation of true uncertainty, and biases caused by nonrepresentative classifiers <ref> [9, 10] </ref>. On the other hand, experiments using a single classifier to make arbitrary queries [14] or select subsets of labeled data [8, 15] have shown substantial speedups in learning.
Reference: 11. <author> H. S. Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294, </pages> <year> 1992. </year>
Reference-contexts: b examples for which the classifier is least certain of class membership (c) Have the teacher label the subsample of b examples (d) Train a new classifier on all labeled examples Recently, several algorithms for learning via queries have been proposed that filter existing examples rather than creating artificial ones <ref> [9, 10, 11] </ref>. These algorithms ask a teacher to label only those examples whose class membership is sufficiently "uncertain". <p> Looking at this as a sampling method rather than a querying one, we call this approach uncertainty sampling. Seung, Opper, and Sompolinsky <ref> [11] </ref> present a theoretical analysis of "query by committee" (QBC), an algorithm that, for each unlabeled example, draws two classifiers randomly from the version space, i.e. the set of all classifiers consistent with the labeled training data [12].
Reference: 12. <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Seung, Opper, and Sompolinsky [11] present a theoretical analysis of "query by committee" (QBC), an algorithm that, for each unlabeled example, draws two classifiers randomly from the version space, i.e. the set of all classifiers consistent with the labeled training data <ref> [12] </ref>. An infinite stream of unlabeled data is assumed, from which QBC asks the teacher for class labels only on those examples for which the two chosen classifiers disagree. Freund, Seung, Shamir, and Tishby extend the QBC result to a wide range of classifier forms [13].
Reference: 13. <author> Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. </author> <title> Information, prediction, and query by committee. </title> <booktitle> In Advances in Neural Informations Processing Systems 5, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An infinite stream of unlabeled data is assumed, from which QBC asks the teacher for class labels only on those examples for which the two chosen classifiers disagree. Freund, Seung, Shamir, and Tishby extend the QBC result to a wide range of classifier forms <ref> [13] </ref>. They prove that, under certain assumptions, the number of queries made after examining m random examples will be logarithmic in m, while generalization error will decrease almost as quickly as it would if queries were made on all examples. More precisely, generalization error decreases as O (1=m).
Reference: 14. <author> J. Hwang, J. J. Choi, S. Oh, and R. J. Marks II. </author> <title> Query-based learning applied to partially trained multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 131-136, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: A single classifier approach to uncertainty sampling has several theoretical failings, including underestimation of true uncertainty, and biases caused by nonrepresentative classifiers [9, 10]. On the other hand, experiments using a single classifier to make arbitrary queries <ref> [14] </ref> or select subsets of labeled data [8, 15] have shown substantial speedups in learning. Relevance sampling, which has proven quite effective for text retrieval, also uses a single classifier. 3 An Uncertainty Sampling Algorithm classifier. <p> In addition, there is also some evidence that training on pairs of examples on opposite sides of a decision boundary is useful <ref> [14] </ref>. 4.4 Classification with the Probabilistic Classifier An advantage of using a classifier which provides accurate estimates of P (Cjw) is that, under certain assumptions, decision theory gives an optimal rule for deciding whether an example should be assigned to class C ([27], p. 15).
Reference: 15. <author> D. T. Davis and J. Hwang. </author> <title> Attentional focus training by boundary region data selection. </title> <booktitle> In International Joint Conference on Neural Networks, pages I-676 to I-681, </booktitle> <address> Baltimore, MD, </address> <month> June 7-11 </month> <year> 1992. </year>
Reference-contexts: A single classifier approach to uncertainty sampling has several theoretical failings, including underestimation of true uncertainty, and biases caused by nonrepresentative classifiers [9, 10]. On the other hand, experiments using a single classifier to make arbitrary queries [14] or select subsets of labeled data <ref> [8, 15] </ref> have shown substantial speedups in learning. Relevance sampling, which has proven quite effective for text retrieval, also uses a single classifier. 3 An Uncertainty Sampling Algorithm classifier.
Reference: 16. <author> P. E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:515-516, </volume> <month> May </month> <year> 1968. </year>
Reference-contexts: Perhaps the most difficult requirement is that measurements of relative certainty be produced even when the classifier was formed from very few training examples. Uncertainty sampling is similar to the strategy of training on misclassified instances <ref> [16, 17] </ref>. The difference is that when data is not labeled we must use the classifier itself to guess at which examples are being misclassified.
Reference: 17. <author> P. E. Utgoff. </author> <title> Improved training via incremental learning. </title> <booktitle> In Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 362-365, </pages> <year> 1989. </year>
Reference-contexts: Perhaps the most difficult requirement is that measurements of relative certainty be produced even when the classifier was formed from very few training examples. Uncertainty sampling is similar to the strategy of training on misclassified instances <ref> [16, 17] </ref>. The difference is that when data is not labeled we must use the classifier itself to guess at which examples are being misclassified.
Reference: 18. <author> N. Fuhr. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year>
Reference-contexts: describe how the classifier is trained and how we use it for uncertainty sampling and classification. 4.1 A Probabilistic Classifier Classifiers which estimate the posterior probability via Bayes' Rule: P (C i jw) = P q (1) have been applied to a variety of text classification tasks, including text retrieval <ref> [18] </ref>, text categorization [19, 20], and word sense identification [5].
Reference: 19. <author> D. D. Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In Proc. SIGIR-92, </booktitle> <pages> pages 37-50, </pages> <year> 1992. </year>
Reference-contexts: classifier is trained and how we use it for uncertainty sampling and classification. 4.1 A Probabilistic Classifier Classifiers which estimate the posterior probability via Bayes' Rule: P (C i jw) = P q (1) have been applied to a variety of text classification tasks, including text retrieval [18], text categorization <ref> [19, 20] </ref>, and word sense identification [5]. <p> In text classification there typically is a huge set of potential w i 's, for instance all the types (distinct words) in a collection of documents. Using feature selection to reduce this set (or, equivalently, to lock all but a few values at 0) can improve effectiveness <ref> [19] </ref>. As a feature quality measure we used: (c pi + c ni ) log P (w i j C) We selected features in order of this value until a specified fraction (0.7 in the experiments reported here) of the total score of all training examples was reached. <p> This means that some aspect of our classifier training is not making effective use of large training sets. Feature selection is the most likely villain. Our method produced several thousand features when applied to the full training set, and previous work suggests this is too many <ref> [19] </ref>. The graphs of average effectiveness hide some variation from run to run. Several of the standard deviations shown in Table 2 amount to 10% or more of the mean effectiveness, meaning that the quality of the final classifiers is somewhat unpredictable.
Reference: 20. <author> M. E. Maron. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: classifier is trained and how we use it for uncertainty sampling and classification. 4.1 A Probabilistic Classifier Classifiers which estimate the posterior probability via Bayes' Rule: P (C i jw) = P q (1) have been applied to a variety of text classification tasks, including text retrieval [18], text categorization <ref> [19, 20] </ref>, and word sense identification [5].
Reference: 21. <author> W. S. Cooper. </author> <title> Some inconsistencies and misnomers in probabilistic information retrieval. </title> <booktitle> In Proc. SIGIR-91, </booktitle> <pages> pages 57-61, </pages> <year> 1991. </year>
Reference-contexts: By making certain independence assumptions <ref> [21] </ref>, we can make the following decomposition: P (Cjw) = P ( C) d Y P (w i jC) (3) Then, using the fact that P ( Cjw) = 1 P (Cjw), plus some arithmetic manipulations, we can get the following expression for P (Cjw): P (Cjw) = P (C) P
Reference: 22. <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> London, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Another problem is that P (C) is typically small and thus hard to estimate, a problem which is compounded when the training set is not a random sample. Logistic regression <ref> [22] </ref> provides a partial solution to these problems. It is a general technique for combining multiple predictor values to estimate a posterior probability.
Reference: 23. <author> W. S. Cooper, F. C. Gey, and D. P. Dabney. </author> <title> Probabilistic retrieval based on staged logistic regression. </title> <booktitle> In Proc. SIGIR-92, </booktitle> <pages> pages 198-210, </pages> <year> 1992. </year>
Reference-contexts: The form of the estimate is: P (Cjx) = 1 + exp (a + b 1 x 1 + ::: + b m x m ) A number of approaches to using logistic regression in text classification have been proposed <ref> [23, 24, 25] </ref>.
Reference: 24. <author> N. Fuhr and U. Pfeifer. </author> <title> Combining model-oriented and description-oriented approaches for probabilistic indexing. </title> <booktitle> In Proc. SIGIR-91, </booktitle> <pages> pages 46-56, </pages> <year> 1991. </year>
Reference-contexts: The form of the estimate is: P (Cjx) = 1 + exp (a + b 1 x 1 + ::: + b m x m ) A number of approaches to using logistic regression in text classification have been proposed <ref> [23, 24, 25] </ref>.
Reference: 25. <author> S. Robertson and J. Bovey. </author> <title> Statistical problems in the application of probabilistic models to information retrieval. </title> <type> Report 5739, </type> <address> British Library, London, </address> <year> 1982. </year>
Reference-contexts: The form of the estimate is: P (Cjx) = 1 + exp (a + b 1 x 1 + ::: + b m x m ) A number of approaches to using logistic regression in text classification have been proposed <ref> [23, 24, 25] </ref>.
Reference: 26. <author> W. A. Gale and K. W. Church. </author> <title> Poor estimates of context are worse than none. </title> <booktitle> In Speech and Natural Language Workshop, </booktitle> <pages> pages 283-287, </pages> <address> San Mateo, CA, </address> <month> June </month> <year> 1990. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is an ad hoc estimator, loosely justified by its analogy to the expected likelihood estimator <ref> [26] </ref>. The above estimator attempts to avoid extreme estimates of the log likelihood ratio when N p and N n are of very different sizes, for instance before our sampling procedure starts finding positive examples.
Reference: 27. <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: 28. <author> N. Goldstein, </author> <title> editor. The Associated Press Stylebook and Libel Manual. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference: 29. <author> W. B. Croft and D. J. Harper. </author> <title> Using probabilistic models of document retrieval without relevance feedback. </title> <journal> Journal of Documentation, </journal> <volume> 35(4) </volume> <pages> 285-295, </pages> <year> 1979. </year>
Reference-contexts: of positive examples in both the training and test sets. 5.2 Training The initial classifier required by the uncertainty sampling algorithm (Figure 3) could be produced from a set of words suggested by a teacher, just as classifiers are constructed from the texts of user requests in text retrieval systems <ref> [29] </ref>. To avoid experimenter bias, we instead used a starting subsample of 3 positive examples of the category randomly selected from the training set. Feature selection always used the words from these 3 examples in addition to words chosen as described in Section 4.2.
Reference: 30. <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <note> second edition, </note> <year> 1979. </year>
Reference: 31. <author> A. Bookstein. </author> <title> Information retrieval: A sequential learning process. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 34 </volume> <pages> 331-342, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: Text retrieval is an obvious application, though the tradeoff between retrieving the texts most useful to the user vs. the texts from which the system will learn the most must be considered <ref> [31] </ref>. The tradeoff is less of an issue in filtering, routing, and information dissemination applications, since the cost of judging nonrelevant examples can be amortized over a longer period of operation. Uncertainty sampling should also benefit classification-based approaches to natural language processing tasks.
Reference: 32. <author> David D. Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <year> 1994. </year> <title> To appear. random (dashed line) samples of titles from AP corpus. Means are over 10 runs for uncertainty and relevance sampling, and over 20 runs for random sampling. Results shown for 9 categories. Frequency of category on training set shown in parentheses. </title>
Reference-contexts: We need to determine the relationship between subsample size and effectiveness, since larger subsamples require less computation. It also seems likely that subsample size can be increased if redundancy within subsamples is decreased. Other efficiency improvements include using a less accurate but more efficiently trained classifier during sampling <ref> [32] </ref>, and picking the first examples satisfying a threshold on uncertainty rather than the most uncertain examples. Simultaneous training of classifiers for multiple classes is also of interest. As currently formulated, uncertainty sampling requires that the underlying training algorithm produce reasonable classifiers even from very small training sets.
References-found: 32

