URL: http://www.neci.nj.nec.com/homepages/giles/papers/Cog.Sci.conf.14th.NNPDA.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Email: rupa@cs.colorado.edu  giles@research.nec.nj.com  sun@sunext.umiacs.umd.edu  
Title: Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory  
Author: Sreerupa Das C. Lee Giles Guo-Zheng Sun 
Address: Boulder, CO 80309  4 Independence Way Princeton, NJ 08540  College Park, MD 20742  
Affiliation: Department of Computer Science University of Colorado  NEC Research Institute  Institute for Advanced Computer Studies University of Maryland  
Abstract: This work describes an approach for inferring Deterministic Context-free (DCF) Grammars in a Connectionist paradigm using a Recurrent Neural Network Pushdown Automaton (NNPDA). The NNPDA consists of a recurrent neural network connected to an external stack memory through a common error function. We show that the NNPDA is able to learn the dynamics of an underlying pushdown automaton from examples of grammatical and non-grammatical strings. Not only does the network learn the state transitions in the automaton, it also learns the actions required to control the stack. In order to use continuous optimization methods, we develop an analog stack which reverts to a discrete stack by quantization of all activations, after the network has learned the transition rules and stack actions. We further show an enhancement of the network's learning capabilities by providing hints. In addition, an initial comparative study of simulations with first, second and third order recurrent networks has shown that the increased degree of freedom in a higher order networks improve generalization but not necessarily learning speed. 
Abstract-found: 1
Intro-found: 1
Reference: [ Allen 90 ] <author> Allen, R.B., </author> <year> 1990. </year> <title> Connectionist Language Users. </title> <booktitle> Connection Science 2(4): p 279. </booktitle>
Reference-contexts: This is performed by extracting information only from the training data. The learning capabilities of the inferred Pushdown Automaton is enhanced by providing more information, hints, about the training strings. For other work on the use of recurrent neural networks for DCF inference, see <ref> [ Allen 90 ] </ref> and [ Pollack 90 ] . The stack is external and continuous. The reason for using an external stack, as opposed to an internal one, [ Pollack 90 ] , is that the external stack requires lesser resources for training.
Reference: [ Elman 90 ] <author> Elman, J.L., </author> <year> 1990. </year> <title> Finding Structure in Time. </title> <journal> Cognitive Science 14:p. </journal> <volume> 179. </volume>
Reference-contexts: Introduction Considerable interest has been shown in language inference using neural networks. (For more traditional approaches to inference of grammars see [ Miclet 90 ] .) Recurrent networks in particular, with various training algorithms, have proved successful in learning regular languages, the simplest in the Chomsky hierarchy. Work by <ref> [ Elman 90 ] </ref> , [ Giles 90 ] , [ Mozer 90 ] , [ Pollack 91 ] , [ Servan-Schreiber 91 ] , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of
Reference: [ Giles 90 ] <author> Giles, C.L.; Sun, G.Z.; Chen, H.H.; Lee, Y.C.; Chen, D., </author> <year> 1990. </year> <title> Higher Order Recurrent Networks & Grammatical Inference. </title> <booktitle> Advances in Neural Information Systems 2, </booktitle> <editor> D.S. Touretzky (ed), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <publisher> Ca:p. </publisher> <pages> 380. </pages>
Reference-contexts: Work by [ Elman 90 ] , <ref> [ Giles 90 ] </ref> , [ Mozer 90 ] , [ Pollack 91 ] , [ Servan-Schreiber 91 ] , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. [ <p> This work is concerned with inference of DCF grammars moving up the Chomsky hierarchy. This recurrent neural network model, previously described by [ Sun 90 ] and <ref> [ Giles 90 ] </ref> , has an external stack memory integrated through a hybrid error function, hence making it powerful enough to learn DCF grammars. <p> Once the network was trained, the actions and states were quantized so as to extract a perfect pushdown automaton. This extracted pushdown automaton can recognize strings of arbitrary length. For a discussion of this extraction method, see [ Sun 90 ] and <ref> [ Giles 90 ] </ref> and, more recently, for finite state automata [ Giles 92a ] . The same simulation criteria and initial conditions described above were used for training NNPDA of various orders.
Reference: [ Giles 92a ] <author> Giles, C.L.; Miller, C.B.; Chen, D.; Chen, H.H.; Sun, G.Z.; Lee, Y.C., </author> <year> 1992. </year> <title> Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks. </title> <booktitle> Neural Computation 4(3):p. </booktitle> <pages> 393. </pages>
Reference-contexts: [ Giles 90 ] , [ Mozer 90 ] , [ Pollack 91 ] , [ Servan-Schreiber 91 ] , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. <ref> [ Giles 92a ] </ref> and [ Wa-trous 92 ] have used higher order (higher dimensional fl Published in the Proceedings of The Fourteenth Annual Conference of The Cognitive Science Society, Morgan Kauf-mann, San Mateo, CA. p. 791-795, 1992. weights) recurrent neural networks with no hidden layer and showed that such models <p> Using a heuristic clustering method, <ref> [ Giles 92a ] </ref> showed that finite state automata could be extracted from the neural networks both during and after training. [ Giles 92b ] successfully demonstrated a method for learning an unknown grammar. This work is concerned with inference of DCF grammars moving up the Chomsky hierarchy. <p> This extracted pushdown automaton can recognize strings of arbitrary length. For a discussion of this extraction method, see [ Sun 90 ] and [ Giles 90 ] and, more recently, for finite state automata <ref> [ Giles 92a ] </ref> . The same simulation criteria and initial conditions described above were used for training NNPDA of various orders.
Reference: [ Giles 92b ] <author> Giles, C.L.; Miller, C.B.; Chen, D.; Sun, G.Z.; Chen, H.H.; Lee, Y.C., </author> <year> 1992. </year> <title> Extracting and Learning an Unknown Grammar with Recurrent Neural Networks. </title> <booktitle> Advances in Neural Information Systems 4, </booktitle> <editor> J.E. Moody, S.J. Hanson, R.P. Lipp-mann (eds), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Ca. </address>
Reference-contexts: Using a heuristic clustering method, [ Giles 92a ] showed that finite state automata could be extracted from the neural networks both during and after training. <ref> [ Giles 92b ] </ref> successfully demonstrated a method for learning an unknown grammar. This work is concerned with inference of DCF grammars moving up the Chomsky hierarchy.
Reference: [ Miclet 90 ] <author> Miclet, L., </author> <year> 1990. </year> <title> Grammatical Inference, Syntactic and Structural Pattern Recognition; Theory and Applications, </title> <editor> H. Bunke and A. Sanfeliu (eds), </editor> <publisher> World Scientific, </publisher> <address> Singapore, Ch 9. </address>
Reference-contexts: Introduction Considerable interest has been shown in language inference using neural networks. (For more traditional approaches to inference of grammars see <ref> [ Miclet 90 ] </ref> .) Recurrent networks in particular, with various training algorithms, have proved successful in learning regular languages, the simplest in the Chomsky hierarchy.
Reference: [ Mozer 90 ] <author> Mozer, M.C.; Bachrach, J., </author> <year> 1990. </year> <title> Discovering the Structure of a Reactive Environment by Exploration. </title> <booktitle> Neural Computation 2(4):p. </booktitle> <pages> 447. </pages>
Reference-contexts: Work by [ Elman 90 ] , [ Giles 90 ] , <ref> [ Mozer 90 ] </ref> , [ Pollack 91 ] , [ Servan-Schreiber 91 ] , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. [ Giles 92a ] and [
Reference: [ Omlin 92 ] <author> Omlin, C.W.; Giles, C.L., </author> <year> 1992. </year> <title> Training Second-Order Recurrent Neural Networks Using Hints. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <editor> D. Sleeman and P. Edwards (eds). </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Ca. </address>
Reference-contexts: Hints in this form made learning faster, helped in learning of exact pushdown automata and made better generalizations. For certain languages, these hints actually made learning possible. There are methods for inserting hints (rules) directly into recurrent neural networks <ref> [ Omlin 92 ] </ref> ; it would be interesting to see the effect of using these methods in training a NNPDA. Simulations The training data consisted of sequence of strings generated in alphabetical order from the input alphabet set. Incremental, real-time learning was used to train the NNPDA.
Reference: [ Pollack 90 ] <author> Pollack, J.B., </author> <year> 1990. </year> <title> Recursive Distributed Representations. </title> <journal> J. of Artificial Intelligence 46:p. </journal> <volume> 77. </volume>
Reference-contexts: This is performed by extracting information only from the training data. The learning capabilities of the inferred Pushdown Automaton is enhanced by providing more information, hints, about the training strings. For other work on the use of recurrent neural networks for DCF inference, see [ Allen 90 ] and <ref> [ Pollack 90 ] </ref> . The stack is external and continuous. The reason for using an external stack, as opposed to an internal one, [ Pollack 90 ] , is that the external stack requires lesser resources for training. <p> For other work on the use of recurrent neural networks for DCF inference, see [ Allen 90 ] and <ref> [ Pollack 90 ] </ref> . The stack is external and continuous. The reason for using an external stack, as opposed to an internal one, [ Pollack 90 ] , is that the external stack requires lesser resources for training. The continuous part permits the use of a continuous optimization method, in our case gradient-descent.
Reference: [ Pollack 91 ] <author> Pollack, J. B. </author> <year> 1991. </year> <title> The Induction of Dynamical Recognizers. </title> <journal> Machine Learning 7:p. </journal> <volume> 227. </volume>
Reference-contexts: Work by [ Elman 90 ] , [ Giles 90 ] , [ Mozer 90 ] , <ref> [ Pollack 91 ] </ref> , [ Servan-Schreiber 91 ] , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. [ Giles 92a ] and [ Wa-trous 92 ] have used
Reference: [ Servan-Schreiber 91 ] <author> Servan-Schreiber, D.; Cleere-mans, A.; McClelland, J.L., </author> <year> 1991. </year> <title> Graded State Machine: The Representation of Temporal Contingencies in Simple Recurrent Networks. </title> <journal> Machine Learning 7:p. </journal> <volume> 161. </volume>
Reference-contexts: Work by [ Elman 90 ] , [ Giles 90 ] , [ Mozer 90 ] , [ Pollack 91 ] , <ref> [ Servan-Schreiber 91 ] </ref> , [ Watrous 92 ] , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. [ Giles 92a ] and [ Wa-trous 92 ] have used higher order (higher dimensional fl
Reference: [ Sun 90 ] <author> Sun, G Z.; Chen, H.H.; Giles, C.L.; Lee, Y.C.; Chen, D., </author> <year> 1990. </year> <title> Neural Networks with External Memory Stack that Learn Context-Free Grammars from Examples. </title> <booktitle> Proceedings of the Conference on Information Science and Systems, Vol. II: </booktitle> <address> p. 649. Princeton University, Princeton, NJ: </address> <booktitle> Conference on Information Science and Systems, </booktitle> <publisher> Inc. </publisher>
Reference-contexts: This work is concerned with inference of DCF grammars moving up the Chomsky hierarchy. This recurrent neural network model, previously described by <ref> [ Sun 90 ] </ref> and [ Giles 90 ] , has an external stack memory integrated through a hybrid error function, hence making it powerful enough to learn DCF grammars. <p> Once the network was trained, the actions and states were quantized so as to extract a perfect pushdown automaton. This extracted pushdown automaton can recognize strings of arbitrary length. For a discussion of this extraction method, see <ref> [ Sun 90 ] </ref> and [ Giles 90 ] and, more recently, for finite state automata [ Giles 92a ] . The same simulation criteria and initial conditions described above were used for training NNPDA of various orders.
Reference: [ Watrous 92 ] <author> Watrous, R.L.; Kuhn, G.M., </author> <year> 1992. </year> <title> Induction of Finite-State Languages Using Second-Order Recurrent Networks, </title> <booktitle> Neural Computation 4(3). </booktitle>
Reference-contexts: Work by [ Elman 90 ] , [ Giles 90 ] , [ Mozer 90 ] , [ Pollack 91 ] , [ Servan-Schreiber 91 ] , <ref> [ Watrous 92 ] </ref> , and [ Williams 89 ] have demonstrated that the recurrent nature of these networks is able to capture the dynamics of the underlying computation automaton. [ Giles 92a ] and [ Wa-trous 92 ] have used higher order (higher dimensional fl Published in the Proceedings of

References-found: 13

