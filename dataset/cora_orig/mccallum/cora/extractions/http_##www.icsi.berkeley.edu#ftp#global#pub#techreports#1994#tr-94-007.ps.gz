URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/tr-94-007.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/
Root-URL: http://www.icsi.berkeley.edu
Title: Precise n-gram Probabilities from Stochastic Context-free Grammars  
Author: Andreas Stolcke Jonathan Segal 
Note: To appear in ACL-94.  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  1947 Center Street, Berkeley, CA 94704,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  University of California at Berkeley, and International Computer Science Institute,  
Pubnum: TR-94-007  
Email: e-mail stolcke@icsi.berkeley.edu. Idem, e-mail jsegal@icsi.berkeley.edu.  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: January 1994 (Revised April 1994)  
Abstract: We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. We discuss efficient implementation of the algorithm and report our practical experience with it. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baker, James K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, </booktitle> <editor> ed. by Jared J. Wolf & Dennis H. Klatt, </editor> <address> 547-550, </address> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm <ref> (Baker 1979) </ref>.
Reference: <author> Booth, Taylor L., & Richard A. Thompson. </author> <year> 1973. </year> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers C-22.442-450. </journal>
Reference-contexts: It should be mentioned that there are some technical conditions that have to be met for a SCFG to be well-defined and consistent <ref> (Booth & Thompson 1973) </ref>. These condition are also sufficient to guarantee that the linear equations given by (3) have positive probabilities as solutions. The details of this are discussed in the Appendix.
Reference: <author> Briscoe, Ted, & John Carroll. </author> <year> 1993. </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <note> Computational Linguistics 19.25-59. </note>
Reference: <author> Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, & Robert L. Mercer. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <note> Computational Linguistics 18.467-479. </note>
Reference: <author> Church, Kenneth W., & William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language 5.19-54. </booktitle>
Reference-contexts: Unfortunately, working with these grammars can be problematic for several reasons: they have large numbers of parameters, so reliable estimation requires a very large training corpus and/or sophisticated smoothing techniques <ref> (Church & Gale 1991) </ref>; it is very hard to directly model linguistic knowledge (and thus these grammars are practically incomprehensible to human inspection); and the models are not easily extensible, i.e., if a new word is added to the vocabulary, none of the information contained in an existing n-gram will tell
Reference: <author> Corazza, Anna, Renato De Mori, Roberto Gretter, & Giorgio Satta. </author> <year> 1991. </year> <title> Computation of probabilities for an island-driven parser. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13.936-950. </journal>
Reference: <author> Earley, Jay. </author> <year> 1970. </year> <title> An efficient context-free parsing algorithm. </title> <journal> Communications of the ACM 6.451-455. </journal>
Reference-contexts: Recently, Stolcke (1993) has shown how to perform this computation efficiently for sparsely parameterized SCFGs using a probabilistic version of Earley's parser <ref> (Earley 1970) </ref>. Computing suffix probabilities is obviously a symmetrical task; for example, one could create a `mirrored' SCFG (reversing the order of right-hand side symbols in all productions) and then run any prefix probability computation on that mirror grammar.
Reference: <author> Graham, Susan L., Michael A. Harrison, & Walter L. Ruzzo. </author> <year> 1980. </year> <title> An improved context-free recognizer. </title> <journal> ACM Transactions on Programming Languages and Systems 2.415-462. </journal>
Reference-contexts: In short, we can, without loss of generality, assume that the SCFGs we are dealing with are in CNF. In fact, our algorithm generalizes straightforwardly to the more general Canonical Two-Form <ref> (Graham et al. 1980) </ref> format, and in the case of bigrams (n = 2) it can even be modified to work directly for 2 @ @ J J J J Y Z w @ @ J J J J J J J J . . . . . . . .

Reference: <author> Jones, Mark A., & Jason M. Eisner. </author> <year> 1992. </year> <title> A probabilistic parser applied to software testing documents. </title> <booktitle> In Proceedings of the 8th National Conference on Artificial Intelligence, </booktitle> <pages> 332-328, </pages> <address> San Jose, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Jurafsky, Daniel, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stol-cke, & Nelson Morgan. </author> <year> 1994. </year> <title> Integrating advanced models of syntax, phonology and accent/dialect in a speech recognizer. </title> <booktitle> In AAAI Workshop on the Integration of Natural Language and Speech Processing , ed. by Paul McKevitt, </booktitle> <address> Seattle, WA. </address> <note> To appear. </note>
Reference-contexts: Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules <ref> (Jurafsky et al. 1994) </ref>. If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. <p> jw 1 ), by dividing the bigram expectation c (w 1 w 2 jS) by the unigram expectation c (w 1 jS). 7 Experiments The algorithm described here has been implemented, and is being used to generate bigrams for a speech recognizer that is part of the BeRP spoken-language system <ref> (Jurafsky et al. 1994) </ref>.
Reference: <author> Magerman, David M., & Mitchell P. Marcus. </author> <year> 1991. </year> <title> Pearl: A probabilistic chart parser. </title> <booktitle> In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <address> Berlin, Germany. 8 Ney, </address> <publisher> Hermann. </publisher> <year> 1984. </year> <title> The use of a one-stage dynamic programming algorithm for connected word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing 32.263-271. </journal>
Reference: <author> Press, William H., Brian P. Flannery, Saul A. Teukolsky, & William T. Vetterling. </author> <year> 1988. </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing . Cambridge: </booktitle> <publisher> Cambridge University Press. </publisher>
Reference: <author> Schwartz, Richard, & Yen-Lu Chow. </author> <year> 1990. </year> <title> The N -best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing , volume 1, </booktitle> <pages> 81-84, </pages> <address> Albuquerque, NM. </address>
Reference-contexts: A standard approach is therefore to use simple language models to generate a preliminary set of candidate hypotheses. These hypotheses, e.g., represented as word lattices or N -best lists <ref> (Schwartz & Chow 1990) </ref>, are re-evaluated later using additional criteria that can afford to be more costly due to the more constrained outcomes.
Reference: <author> Stolcke, Andreas. </author> <year> 1993. </year> <title> An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, CA. </institution> <note> To appear in Computational Linguistics. </note>
Reference: <author> Zue, Victor, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Po-lifroni, & Stephanie Seneff. </author> <year> 1991. </year> <title> Integration of speech recognition and natural language processing in the MIT Voyager system. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 713-716, </pages> <address> Toronto. </address> <month> 9 </month>
References-found: 15

