URL: http://www.csc.calpoly.edu/~tpederse/paces99-cmpl.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: tpederse@csc.calpoly.edu  
Title: Integrating Natural Language Subtasks with Bayesian Belief Networks  
Author: Ted Pedersen 
Address: San Luis Obispo, CA 93407  
Affiliation: Los Angeles, CA  Department of Computer Science California Polytechnic State University  
Date: February 11-12 1999,  
Note: Appears in the Proceedings of the Pacific Asia Conference on Expert Systems,  
Abstract: The development of automatic natural language understanding systems remains an elusive goal. Given the highly ambiguous nature of the syntax and semantics of natural language, it is often impossible to develop rule-based approaches to understanding even very limited domains of text. The difficulty in specifying rules and their exceptions has led to the rise of probabilistic approaches where models of natural language are learned from large corpora of text. These models usually serve as simple classifiers for particular subtasks such as word sense disambiguation or discourse segmentation. While successful in these limited roles, it is unclear that multiple classifiers can be combined to create comprehensive natural language understanding systems. Instead, we believe that recent advances in modeling and reasoning with uncertain information offer an appropriate framework for building such systems. We are developing and evaluating new algorithms that learn Bayesian belief networks from large corpora of text. These networks will integrate multiple natural language processing subtasks in a single model and will support inferencing mechanisms that go beyond simple classification. We are also developing and evaluating novel sets of features that will allow us to represent and reason with the inherent relationships that exist among natural language processing subtasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> 1974. </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control AC-19(6):716-723. </journal>
Reference-contexts: We propose to investigate the applicability of a general class of methods known as the information criteria to belief network learning. We will focus on Akaike's Information Criteria (AIC) <ref> (Akaike 1974) </ref> and the Bayesian Information Criteria (BIC) (Schwarz 1978).
Reference: <author> Bruce, R., and Wiebe, J. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 139-146. </pages>
Reference-contexts: Another commonly used representation of context is the so-called bag of words, where each word that occurs in the training sample is represented by a feature variable (e.g., (Mooney 1996)). Syntactic structure has also proven useful. For example, the part-of-speech of surrounding words are common representations of context (e.g. <ref> (Bruce & Wiebe 1994) </ref>) as are verb-object structure (e.g., (Ng & Lee 1996)). Thus, current approaches to word sense disambiguation generally focus on syntactic and lexical representations of context.
Reference: <author> Dempster, A.; Laird, N.; and Rubin, D. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B 39 </journal> <pages> 1-38. </pages>
Reference-contexts: Two popular methods of imputing values for missing data are the Expectation Maximization (EM) algorithm <ref> (Dempster, Laird, & Rubin 1977) </ref> and Gibbs Sampling (Geman & Geman 1984). We have used both in our previous work ((Pedersen & Bruce 1997a)) to learn parameter estimates for Naive Bayesian classifiers that were applied to word sense disambiguation.
Reference: <author> Gale, W.; Church, K.; and Yarowsky, D. </author> <year> 1992. </year> <title> A method for disambiguating word senses in a large corpus. </title> <booktitle> Computers and the Humanities 26 </booktitle> <pages> 415-439. </pages>
Reference-contexts: Corpus-based approaches have generally relied upon a window of surrounding words to provide context. A window of 100 words was suggested in <ref> (Gale, Church, & Yarowsky 1992) </ref>, although small windows of one or two surrounding words have also proven effective (e.g., (Ng & Lee 1996)). <p> For example, the part-of-speech of surrounding words are common representations of context (e.g. (Bruce & Wiebe 1994)) as are verb-object structure (e.g., (Ng & Lee 1996)). Thus, current approaches to word sense disambiguation generally focus on syntactic and lexical representations of context. However, the one-sense-per-discourse hypothesis <ref> (Gale, Church, & Yarowsky 1992) </ref> holds that content words will largely be confined to one sense when they appear in specific domains. Despite the intuitive appeal of this hypothesis, discourse level features are generally not included in probabilistic word sense disambiguation algorithms.
Reference: <author> Geman, S., and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 6 </journal> <pages> 721-741. </pages>
Reference-contexts: Two popular methods of imputing values for missing data are the Expectation Maximization (EM) algorithm (Dempster, Laird, & Rubin 1977) and Gibbs Sampling <ref> (Geman & Geman 1984) </ref>. We have used both in our previous work ((Pedersen & Bruce 1997a)) to learn parameter estimates for Naive Bayesian classifiers that were applied to word sense disambiguation. Here we extend their use to belief networks for integrated natural language processing.
Reference: <author> Hearst, M. </author> <year> 1997. </year> <title> TextTiling: Segmenting text into multi-paragraph subtopic passages. </title> <booktitle> Computational Linguistics 23(1) </booktitle> <pages> 33-64. </pages>
Reference-contexts: The underlying premise to most approaches to this problem is that changes in the distribution and occurrence of content words in a text will signal changes in subtopic (e.g., <ref> (Hearst 1997) </ref>). Several well known statistical tests have been employed to identify significant changes in vocabulary, among them the log-likelihood ratio G 2 and the t-test. We also believe that features from semantic level subtasks such as word sense disambiguation can be of benefit for subtopic shift identification.
Reference: <author> Koller, D., and Sahami, M. </author> <year> 1997. </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning, </booktitle> <pages> 170-178. </pages>
Reference-contexts: This representation of context has been widely used to create Naive Bayesian classifiers. Recent approaches require smaller amounts of training data and do not necessarily include all the content words in the training sample in the model (e.g., <ref> (Koller & Sahami 1997) </ref>). A number of other approaches have come from information retrieval research. For example, the vector space model (Salton & McGill 1983) has been applied to document classification. In general, each word in a document is treated as an axis in a highly dimensional space.
Reference: <author> Madigan, D., and Raftery, A. </author> <year> 1994. </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's Window. </title> <journal> Journal of American Statistical Association 89 </journal> <pages> 1535-1546. </pages>
Reference-contexts: This process will be repeated a number of times and the results from each stage will be combined into a composite network. This is a variation on the idea of model averaging (e.g., <ref> (Madigan & Raftery 1994) </ref>), where averaged or composite networks are learned as opposed to selecting a single best-fitting network. Information Criteria The degradation and improvement in fit of candidate networks relative to the current network is assessed by an evaluation criterion.
Reference: <author> Mooney, R. </author> <year> 1996. </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 82-91. </pages>
Reference-contexts: Another commonly used representation of context is the so-called bag of words, where each word that occurs in the training sample is represented by a feature variable (e.g., <ref> (Mooney 1996) </ref>). Syntactic structure has also proven useful. For example, the part-of-speech of surrounding words are common representations of context (e.g. (Bruce & Wiebe 1994)) as are verb-object structure (e.g., (Ng & Lee 1996)). Thus, current approaches to word sense disambiguation generally focus on syntactic and lexical representations of context.
Reference: <author> Ng, H., and Lee, H. </author> <year> 1996. </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, </booktitle> <pages> 40-47. </pages>
Reference-contexts: Corpus-based approaches have generally relied upon a window of surrounding words to provide context. A window of 100 words was suggested in (Gale, Church, & Yarowsky 1992), although small windows of one or two surrounding words have also proven effective (e.g., <ref> (Ng & Lee 1996) </ref>). Another commonly used representation of context is the so-called bag of words, where each word that occurs in the training sample is represented by a feature variable (e.g., (Mooney 1996)). Syntactic structure has also proven useful. <p> Syntactic structure has also proven useful. For example, the part-of-speech of surrounding words are common representations of context (e.g. (Bruce & Wiebe 1994)) as are verb-object structure (e.g., <ref> (Ng & Lee 1996) </ref>). Thus, current approaches to word sense disambiguation generally focus on syntactic and lexical representations of context. However, the one-sense-per-discourse hypothesis (Gale, Church, & Yarowsky 1992) holds that content words will largely be confined to one sense when they appear in specific domains.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, CA. </address>
Reference-contexts: While the joint distribution supports inferences about any feature in the domain, it becomes intractably large as the number of feature variables increases. In practice it is usually not feasible to specify or obtain evidence for all of the probabilities needed to define a joint distribution. Bayesian belief networks <ref> (Pearl 1988) </ref>, hereafter simply belief networks, offer a solution to the problems caused by large joint probability distributions. They provide a concise description of joint distributions based strictly on local causal relationships among variables. A belief network is conveniently represented by a graph where the following conditions hold: 1.
Reference: <author> Pedersen, T., and Bruce, R. </author> <year> 1997a. </year> <title> Distinguishing word senses in untagged text. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 197-207. </pages>
Reference: <author> Pedersen, T., and Bruce, R. </author> <year> 1997b. </year> <title> A new supervised learning algorithm for word sense disambiguation. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 604-609. </pages>
Reference-contexts: Our previous work ((Pedersen, Bruce, & Wiebe 1997), <ref> (Pedersen & Bruce 1997b) </ref>) utilized sequential search strategies and information criteria to select probabilistic classifiers for word sense disambiguation. Here we extend those methodologies to learn belief networks that will support inference on any variable in the domain, not just a single classification variable.
Reference: <author> Pedersen, T.; Bruce, R.; and Wiebe, J. </author> <year> 1997. </year> <title> Sequential model selection for word sense disambiguation. </title> <booktitle> In Proceedings of the Fifth Conference on Applied Natural Language Processing, </booktitle> <pages> 388-395. </pages>
Reference: <author> Salton, G., and McGill, M. </author> <year> 1983. </year> <title> An Introduction to Modern Information Retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Recent approaches require smaller amounts of training data and do not necessarily include all the content words in the training sample in the model (e.g., (Koller & Sahami 1997)). A number of other approaches have come from information retrieval research. For example, the vector space model <ref> (Salton & McGill 1983) </ref> has been applied to document classification. In general, each word in a document is treated as an axis in a highly dimensional space.
Reference: <author> Schutze, H. </author> <year> 1993. </year> <title> Word space. </title> <editor> In Hanson, S.; Cowan, J.; and Giles, C., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: While there are manually sense-tagged corpora available, they are generally not of sufficient breadth to provide adequate quantities of training data for document classification and subtopic shift as well. Therefore, we will create sense-tagged text using the pseudo-word methodology ((Gale, Church, & Yarowsky 1992), <ref> (Schutze 1993) </ref>). Rather than manually annotating different instances of a word with sense indicators, this approach combines two unrelated words and creates a new single word that is ambiguous; the possible meanings are those of the individual words.
Reference: <author> Schwarz, G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics 6(2) </journal> <pages> 461-464. </pages>
Reference-contexts: We propose to investigate the applicability of a general class of methods known as the information criteria to belief network learning. We will focus on Akaike's Information Criteria (AIC) (Akaike 1974) and the Bayesian Information Criteria (BIC) <ref> (Schwarz 1978) </ref>. These criteria are based on the log-likelihood ratio G 2 , a frequently used test statistic that measures the deviance between what is observed in the data and what would be expected to be observed, if the network under evaluation adequately characterizes or fits the data.
References-found: 17

