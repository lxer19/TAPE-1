URL: ftp://ftp.cs.washington.edu/tr/1998/12/UW-CSE-98-12-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: Computational Models for Decision Making in Dynamic and Uncertain Domains  
Author: Omid Madani 
Abstract: Decision making in dynamic, uncertain environments has traditionally been a rich source of many important mathematical and computational problems. In this report, I will describe three models of control in dynamic systems consisting of a finite set of states where decisions influence state transitions. I will explain control objectives and motivations for the models, and will discuss open problems on computational complexity and algorithmic issues and possibilities for hybrid models. 
Abstract-found: 1
Intro-found: 1
Reference: [BDH95] <author> Craig Boutilier, Thomas Dean, and Steve Hanks. </author> <title> Planning under uncertainty: structural assumptions and computational leverage. </title> <booktitle> In New Directions in AI Planning, </booktitle> <pages> pages 157-171, </pages> <year> 1995. </year>
Reference-contexts: Applications of such models are again diverse and include medical decision making, design of teaching systems, cost control in accounting and equipment maintenance and replacement [Put94, Mon82]. Recently, computer scientists have looked at these models as a framework for planning under uncertainty <ref> [BDH95] </ref> and the models find applications in areas such as robot navigation [BRS96, KGS95] and medical decision making [Hau97]. There are several modeling questions and open algorithmic problems about MDPs and their generalizations.
Reference: [BRS96] <author> Dima Burago, Michel De Rougemont, and Anatol Slissenko. </author> <title> On the complexity of partially observed Markov decision processes. </title> <booktitle> Theoretical Computer Science, </booktitle> <pages> pages 161-183, </pages> <year> 1996. </year>
Reference-contexts: Recently, computer scientists have looked at these models as a framework for planning under uncertainty [BDH95] and the models find applications in areas such as robot navigation <ref> [BRS96, KGS95] </ref> and medical decision making [Hau97]. There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in [PT87]. <p> POMDPs have been shown to be intractable to solve in general [PT87, Lit96], which partially explains the limited use of these models in practice so far. However we report on work in <ref> [BRS96] </ref> showing that studying restricted models can be fruitful for designing approximation algorithms. There remain questions on different restrictions on partial observability and whether these restricted models and perhaps their combinations are effective in modeling real world problems. <p> The hope is to understand enough of the model to be able to contrast the MDP and DES models in their choice of objective criteria and solution characteristics. 1.1 Paper Overview In Section 2, I will introduce several MDP model concepts as I report on the work in <ref> [BRS96] </ref>. In Section 3 I describe the game generalization of MDPs. Each of these sections concludes with a discussion of related work and interesting open problems in the area. <p> I will close with a discussion on relating the models in Section 5 and conclude in Section 6. 2 MDPs and POMDPs In this section, by going through the work in <ref> [BRS96] </ref> as an example, I will familiarize the reader with important concepts in Markov decision process models (MDPs) and their generalization, partially observable Markov decision processes (POMDPs). As sufficient background is built, I will describe the problems that [BRS96] and others have addressed. <p> MDPs and POMDPs In this section, by going through the work in <ref> [BRS96] </ref> as an example, I will familiarize the reader with important concepts in Markov decision process models (MDPs) and their generalization, partially observable Markov decision processes (POMDPs). As sufficient background is built, I will describe the problems that [BRS96] and others have addressed. <p> Infinite Horizon For the above problem, we are asking for a strategy that maximizes the probability of reaching the target state in no more than K ( generally a polynomial in n) steps. This is called a finite horizon criterion, and it is the problem treated in <ref> [BRS96] </ref>. Alternatively, we may ask for a strategy that simply maximizes the probability of reaching the target, regardless of the number of steps. This is an example of a problem with the infinite horizon criterion. <p> A dynamic programming algorithm can compute optimal partitions but unfortunately it may run in exponential time. In fact, this inefficiency is likely to be an inherent property of the problem: POMDPs are hard to solve in general (see below). The approximation algorithm in <ref> [BRS96] </ref> is based on an approximation of the optimal partition of the simplex. 2.4 Computational Complexity Substantial work on MDPs and POMDPs began in the 40's and the 60's respectively. However, the computational complexity of the problems had not been addressed until recently. <p> With unobservability the problem seems to be slightly easier than partial observability: in [PT87] it is shown that the unobservable problem with a horizon of n is N P -complete. Still, perhaps surprisingly, even a weak approximation is hard, as is established in the following theorem appearing in <ref> [BRS96] </ref>, proved by using a reduction from the 3SAT problem. <p> Decide whether case 1 holds. Given the hardness of these problems, one way to approach the problem of partial observ-ability is to come up with plausible restrictions so that computing optimal or approximate strategies become tractable. In the next section, I'll go over one such model investigated in <ref> [BRS96] </ref>. 2.5 A Restricted Problem and an Approximation Algorithm The hardness results above show that unless we restrict the uncertainty in the unobservability of the system, partially observable problems will very likely remain intractable. <p> The number m is called the coloring multiplicity. Let us consider the target problem with the above restriction. The complexity of exactly solving the problem remains N P -hard for m 3, shown by a nontrivial reduction from the partition problem 2 <ref> [BRS96] </ref>: Theorem 2.2 Constructing an optimal strategy for the target problem with coloring multiplicity 3 is N P -hard. <p> The approximation algorithm runs in time polynomial in n, 1=* and horizon K. I will next describe the algorithm in <ref> [BRS96] </ref>. Without loss of generality, assume each color has multiplicity exactly m. <p> The following result follows from the intuitive fact that changing the probability of any state by some amount * can change the maximum proability of reaching the target by at most *. Lemma 2.3 <ref> [BRS96] </ref> All v k;c are Lipschitz-1 functions. <p> The following claim, shown by an induction on k and using the Lipschitz-1 properties of the functions v k;c , establishes the approximation claim: Claim 2.4 <ref> [BRS96] </ref> kv k;c ~ f k;c k kffi, for all k 0 and 1 c jCj. To summarize, the algorithm keeps track of piecewise linear value functions on X for each color c, for each stage up to K. <p> At each execution step, the uncertainty is on at most two of states, and half the time there is no uncertainty. An important question is whether real world problems with a POMDP flavor exhibit special structures like the kinds mentioned above. The work in <ref> [BRS96] </ref> was motivated by a robot navigation problem. Unfortunately, I don't have information on whether the model was useful for the problem and whether the algorithm was implemented.
Reference: [Con92] <author> Anne Condon. </author> <title> The complexity of simple stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: However we report on work in [BRS96] showing that studying restricted models can be fruitful for designing approximation algorithms. There remain questions on different restrictions on partial observability and whether these restricted models and perhaps their combinations are effective in modeling real world problems. In <ref> [Con92] </ref>, Condon investigates a two player game generalization of the MDP model. The research brings out several outstanding open problems on the existence of polynomial time algorithms not only for the game model but also for MDPs. <p> Numerous variations of Shapley's model have been studied and many algorithms have been proposed (see [Con93] for references). Con-don in <ref> [Con92] </ref> investigates the computational complexity of a decision problem associated with SSGs. <p> From a computational complexity point of view, the SSG value problem, to be defined below, is complete for the class of languages accepted by log-space bounded alternating Turing machines that can also choose moves randomly (see <ref> [Con92] </ref> for details). If the value problem is in P , then randomizing does not add (significant) power to log-space bounded alternating machines: such alternating Turing machines already accept exactly the languages in P . <p> The rule may be randomized and it may use any information about the state and 11 the history of the game. However, here we restrict outselves to pure stationary and positional strategies to be defined below. Condon in <ref> [Con92] </ref> shows that for a restricted but important class of SSGs, we do not lose generality in restricting to these types of strategies. <p> find an optimal strategy oe fl for player 1 so that max oe min o v oe;o (s) = min o v oe fl ;o (s). 3.2 Membership in N P " coN P In this section, I will describe several important and conceptually simplifying properties of SSGs proven in <ref> [Con92] </ref>. A notable property is the fact that both players possess "optimal" strategies that guarantee the best possible outcome regardless of the start vertex or the other 12 player's choice of strategy. <p> Note that G oe;o can be considered a Markov chain. We say that a SSG halts with probability one if for all pairs of strategies oe and o , G oe;o has a path to a sink vertex. Lemmas 3.1, 3.2, and 3.3 are proved in <ref> [Con92] </ref> for SSGs that halt with probability 1 for simplicity, and I report them in their general form here. <p> The following property is one more important step for showing that the problem lies in both N P and coN P . Lemma 3.4 <ref> [Con92] </ref> The value of a simple stochastic game with n vertices is of the form p=q, where p and q are integers, 0 p; q 4 n1 . <p> I will first place SSGs in the context of MDPs and Shapley's stochastic games. Note that the SSG model is basically a special two player infinite-horizon MDP, where states are called vertices, and there are two actions available to each player. Condon in <ref> [Con92] </ref> shows that the problem remains in N P " coN P with natural extensions to the model such as presence of multiple edges (actions) at each vertex, extending the transition probabilities to be rational fractions other than 1=2 or associating rewards with edges or vertices and modifying the objective criteria <p> of the game and an optimal strategy for the player. 15 If we restrict the vertices to be only max and min, that is if we take the stochastic nature of the game out, then the problem is solvable in polynomial time and a simple algorithm for such appears in <ref> [Con92] </ref> (also follows from an algorithm in [ZP96]). Next, I will describe an algorithm for SSGs, called the Hoffman-Karp algorithm [HK66], which is based on the strategy improvement method attributed to Howard and Bellman for infinite-horizon MDPs (see [Put94]).
Reference: [Con93] <author> Anne Condon. </author> <title> On algorithms for simple stochastic games. </title> <booktitle> In Advances in computational complexity theory, volume 13 of DIMACS series in discrete mathematics and theoretical computer science. </booktitle> <year> 1993. </year>
Reference-contexts: Numerous variations of Shapley's model have been studied and many algorithms have been proposed (see <ref> [Con93] </ref> for references). Con-don in [Con92] investigates the computational complexity of a decision problem associated with SSGs. A significant motivation for studying the SSG model is best stated in [Con93]: "it is the simplest possible restriction of Shapley's games which retains just enough complexity so that no polynomial time algorithm for <p> Numerous variations of Shapley's model have been studied and many algorithms have been proposed (see <ref> [Con93] </ref> for references). Con-don in [Con92] investigates the computational complexity of a decision problem associated with SSGs. A significant motivation for studying the SSG model is best stated in [Con93]: "it is the simplest possible restriction of Shapley's games which retains just enough complexity so that no polynomial time algorithm for the problem is known." As we will see, the study of SSGs further motivates investigation of outstanding open problems regarding algorithms for MDPs (the one player game versions). <p> Counterexamples showing that the resulting algorithms and several other plausible algorithms are incorrect can be found in <ref> [Con93] </ref>. It is not known whether the Hoffman-Karp algorithm finds optimal strategies in polynomial time or there are graphs and starting points where it can take exponentially many iterations to converge. Interestingly, a similar question can be asked of the strategy improvement algorithm of Howard for (infinite-horizon) MDPs. <p> They show that k = 4n 3 jW j iterations is sufficient and necessary (by showing an example) to find the average costs of each vertex using their algorithm. Condon investigates several correct algorithms for SSGs in <ref> [Con93] </ref> including one based on a quadratic program. Although any of the proposed algorithms in [Con93] and others for 2 player games may run in polynomial time, it would perhaps be more insightful to take a more direct 4 Several vertex selection strategies are investigated. 17 approach to solving these games <p> Condon investigates several correct algorithms for SSGs in <ref> [Con93] </ref> including one based on a quadratic program. Although any of the proposed algorithms in [Con93] and others for 2 player games may run in polynomial time, it would perhaps be more insightful to take a more direct 4 Several vertex selection strategies are investigated. 17 approach to solving these games models that would utilize more of the structural properties of MDPs, specifically the graph theoretic
Reference: [Dan91] <author> George B. Dantzig. </author> <title> Linear programming. </title> <editor> In J.K. Lenstra, A.H.G. Rinnooy Kan, and A. Schrijver, editors, </editor> <booktitle> History of Mathematical Programming, A Collection of Personal Reminiscences, </booktitle> <pages> page 30. </pages> <year> 1991. </year>
Reference-contexts: Dynamic decision making models and related problems have had an impressive record in stimulating fruitful research, as the following quote by George Dantzig, one of the pioneers of the widely applicable linear programming model, suggests in reference to the model <ref> [Dan91] </ref>: "... it is interesting to note that the original problem that started my research is still outstanding | namely the problem of planning or scheduling dynamically over time, particularly planning dynamically under uncertainty.
Reference: [DW91] <author> Thomas L. Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Problems in control and decision making in a dynamic environment have been a source of rich theoretical and practical research in several disciplines <ref> [NM80, DW91] </ref>. A good portion of this work has traditionally focused on understanding models and model consequences. Recently many researchers, interested in implementing systems based on such models, are paying more attention to related computational tractability issues.
Reference: [Hau97] <author> Milos Hauskrecht. </author> <title> Dynamic decision making in a stochastic partially observable medical domain: Ischemic heart disease example. </title> <booktitle> In Artificial Intelligence in Medicine, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 296-299. </pages> <year> 1997. </year>
Reference-contexts: Recently, computer scientists have looked at these models as a framework for planning under uncertainty [BDH95] and the models find applications in areas such as robot navigation [BRS96, KGS95] and medical decision making <ref> [Hau97] </ref>. There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in [PT87]. <p> The work in [BRS96] was motivated by a robot navigation problem. Unfortunately, I don't have information on whether the model was useful for the problem and whether the algorithm was implemented. Works on a similar problem of robot navigation [KGS95] and medical decision making <ref> [Hau97] </ref> do not reference this work, and rely on heuristics or worst-case exponential time exact algorithms to solve their problems. To conclude, there remains several interesting open problems with regards to finding plausible restricted POMDP problems that yield to efficient approximation.
Reference: [HK66] <author> A. Hoffman and R. Karp. </author> <booktitle> On nonterminating stochatic games. Management Science, </booktitle> <volume> 12(5), </volume> <year> 1966. </year>
Reference-contexts: Next, I will describe an algorithm for SSGs, called the Hoffman-Karp algorithm <ref> [HK66] </ref>, which is based on the strategy improvement method attributed to Howard and Bellman for infinite-horizon MDPs (see [Put94]). The algorithm is conceptually simple, and can be thought of as a local search in the space of strategies.
Reference: [Kal92] <author> Gil Kalai. </author> <title> A subexponential randomized simplex algorithm. </title> <booktitle> In 24th annual ACM STOC, </booktitle> <year> 1992. </year>
Reference-contexts: Does the nonrestricted ( "parallel" switching) strategy improvement avoid this? A subexponential randomized algorithm for SSGs was proposed by Ludwig in [Lud95]. Ludwig adapts methods used by Kalai <ref> [Kal92] </ref> who develops a subexponential randomized simplex method for linear programming. In Ludwig's algorithm, if G has no max vertices, then an optimal strategy for player 0 can be computed in polynomial time.
Reference: [KGS95] <author> Sven Koenig, Richard Goodwin, and Reid Simmons. </author> <title> Robot navigation with markov models: A framework for path planning and learning with limited computational resources. In Reasoning with Uncertainty in Robotics, </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 322-337. </pages> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Recently, computer scientists have looked at these models as a framework for planning under uncertainty [BDH95] and the models find applications in areas such as robot navigation <ref> [BRS96, KGS95] </ref> and medical decision making [Hau97]. There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in [PT87]. <p> The work in [BRS96] was motivated by a robot navigation problem. Unfortunately, I don't have information on whether the model was useful for the problem and whether the algorithm was implemented. Works on a similar problem of robot navigation <ref> [KGS95] </ref> and medical decision making [Hau97] do not reference this work, and rely on heuristics or worst-case exponential time exact algorithms to solve their problems. To conclude, there remains several interesting open problems with regards to finding plausible restricted POMDP problems that yield to efficient approximation.
Reference: [Lit96] <author> Michael Littman. </author> <title> Markov Decision Processes and Reinforcement learning. </title> <type> PhD thesis, </type> <institution> Brown, </institution> <year> 1996. </year>
Reference-contexts: There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in [PT87]. POMDPs have been shown to be intractable to solve in general <ref> [PT87, Lit96] </ref>, which partially explains the limited use of these models in practice so far. However we report on work in [BRS96] showing that studying restricted models can be fruitful for designing approximation algorithms. <p> We should note however that both the (stochastic) MDP problem and the two player game, when payoffs are constants (0 or 1 say), are P -complete, whereas the deterministic MDP problem and the one player mean-payoff game are in N C (see [PT87] and <ref> [Lit96] </ref>), so the deterministic versions are very likely strictly easier. We may still ask whether graph theoretic properties and methods based on them should be abandoned when we move from one player to two players in mean payoff games or from deterministic transitions to stochastic transitions in MDPs.
Reference: [Lov91] <author> W. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable markov decision processes. </title> <journal> Annals of Operations Research, </journal> <pages> pages 47-66, </pages> <year> 1991. </year>
Reference-contexts: However, the computational complexity of the problems had not been addressed until recently. Even today it is not known whether the widely used strategy improvement algorithm for (infinite horizon) MDPs runs in polynomial time. Lovejoy <ref> [Lov91] </ref> surveying computational aspects of POMDPs notes, "The significant applied potential for such processes remain largely unrealized, due to an historical lack of tractable solution methodologies." The study of computational complexity of MDPs and POMDPs began with the work in [PT87] who show that many variants of the classical fully observable
Reference: [Lud95] <author> W. Ludwig. </author> <title> A subexponential randomized algorithm for the simple stochastic game problem. </title> <journal> Information and computation, </journal> <volume> 117 </volume> <pages> 151-155, </pages> <year> 1995. </year>
Reference-contexts: Does the nonrestricted ( "parallel" switching) strategy improvement avoid this? A subexponential randomized algorithm for SSGs was proposed by Ludwig in <ref> [Lud95] </ref>. Ludwig adapts methods used by Kalai [Kal92] who develops a subexponential randomized simplex method for linear programming. In Ludwig's algorithm, if G has no max vertices, then an optimal strategy for player 0 can be computed in polynomial time.
Reference: [MC94] <author> Mary Melekopoglou and Anne Condon. </author> <title> On the complexity of the policy improvement algorithm for markov decision processes. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(2), </volume> <year> 1994. </year>
Reference-contexts: This algorithm has excellent performance in practice. However <ref> [MC94] </ref> show that restrictions on the algorithm where only one switchable vertex is switched at each iteration 4 can go through exponentially many strategies before finding an optimal one.
Reference: [ML94] <author> Oded Maimon and Mark Last. </author> <title> Information efficient robotic control. </title> <journal> Robotica, </journal> <volume> 12 </volume> <pages> 157-163, </pages> <year> 1994. </year>
Reference-contexts: I will briefly go over one example application appearing in [ML97] to further illustrate the concepts. In <ref> [ML94] </ref> an aircraft maintenance application is presented. 23 Imagine a robot that is in charge of testing several barrels of chemicals containing dangerous material. The danger is that uncontrolled sequence of chemical events can evolve suddenly and rapidly that may lead to explosion and damage. <p> Given the event probabilities and costs associated with the different outcomes, the problem is when to take measurement 2 as a function of the outcome of measurement 1. Problem 4 is clearly in N P , and the authors claim that the problem is N P -complete in <ref> [ML94] </ref> (no proof is provided) and present a heuristic algorithm to solve the problem.
Reference: [ML97] <author> Oded Maimon and Mark Last. </author> <title> Information-efficient control of discrete event stochastic systems. </title> <journal> IEEE transactions on Systems, Man, and Cybernetics, </journal> <volume> 27(1) </volume> <pages> 23-32, </pages> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: I will cover two control problems in this area. The first appears in [RW87] which introduced the formalism of DES. The second appearing in <ref> [ML97] </ref> has a stochastic flavor and is closer to MDPs. <p> Next, I will describe a DES model that is closer conceptually to the MDP framework. 4.3 A Discrete Event Stochastic System Maimon et al present a version of a DES system called a Discrete Event Stochastic System (DESS) in <ref> [ML97] </ref> which trades off the value of the information against the cost of gathering information. Here, the similarities to Markov decision processes are apparent. <p> I will briefly go over one example application appearing in <ref> [ML97] </ref> to further illustrate the concepts. In [ML94] an aircraft maintenance application is presented. 23 Imagine a robot that is in charge of testing several barrels of chemicals containing dangerous material.
Reference: [Mon82] <author> George Monahan. </author> <title> A survey of partially observable markov decision processes: Theory, models, and algorithms. </title> <journal> Management Sceience, </journal> <volume> 28(1) </volume> <pages> 1-15, </pages> <year> 1982. </year>
Reference-contexts: The partially observable MDP (POMDP) is an important generalization where the decision maker has only partial information on the current state of the system. Applications of such models are again diverse and include medical decision making, design of teaching systems, cost control in accounting and equipment maintenance and replacement <ref> [Put94, Mon82] </ref>. Recently, computer scientists have looked at these models as a framework for planning under uncertainty [BDH95] and the models find applications in areas such as robot navigation [BRS96, KGS95] and medical decision making [Hau97]. There are several modeling questions and open algorithmic problems about MDPs and their generalizations.
Reference: [NM80] <author> John Von Neumann and Oskar Morgenstern. </author> <title> Theory of games and economic behavior. </title> <publisher> Princeton University Press, 3rd edition, </publisher> <year> 1980. </year>
Reference-contexts: 1 Introduction Problems in control and decision making in a dynamic environment have been a source of rich theoretical and practical research in several disciplines <ref> [NM80, DW91] </ref>. A good portion of this work has traditionally focused on understanding models and model consequences. Recently many researchers, interested in implementing systems based on such models, are paying more attention to related computational tractability issues.
Reference: [PT87] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of operations research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in <ref> [PT87] </ref>. POMDPs have been shown to be intractable to solve in general [PT87, Lit96], which partially explains the limited use of these models in practice so far. However we report on work in [BRS96] showing that studying restricted models can be fruitful for designing approximation algorithms. <p> There are several modeling questions and open algorithmic problems about MDPs and their generalizations. The computational complexity of solving MDP problems was addressed only relatively recently beginning with the work in [PT87]. POMDPs have been shown to be intractable to solve in general <ref> [PT87, Lit96] </ref>, which partially explains the limited use of these models in practice so far. However we report on work in [BRS96] showing that studying restricted models can be fruitful for designing approximation algorithms. <p> Lovejoy [Lov91] surveying computational aspects of POMDPs notes, "The significant applied potential for such processes remain largely unrealized, due to an historical lack of tractable solution methodologies." The study of computational complexity of MDPs and POMDPs began with the work in <ref> [PT87] </ref> who show that many variants of the classical fully observable MDP problems, both for the finite and the infinite horizon, can be solved in polynomial time using dynamic programming or linear programming methods, and in fact the problems are P -complete, where the input length consists of the number of <p> Unfortunately, in the case of partial observability, the problem becomes very hard. In <ref> [PT87] </ref>, it is shown that the problem of coming up with an optimal strategy for partially observed problem is P SP ACE-hard even when the finite horizon is restricted to be n. <p> With unobservability the problem seems to be slightly easier than partial observability: in <ref> [PT87] </ref> it is shown that the unobservable problem with a horizon of n is N P -complete. Still, perhaps surprisingly, even a weak approximation is hard, as is established in the following theorem appearing in [BRS96], proved by using a reduction from the 3SAT problem. <p> others for 2 player games may run in polynomial time, it would perhaps be more insightful to take a more direct 4 Several vertex selection strategies are investigated. 17 approach to solving these games models that would utilize more of the structural properties of MDPs, specifically the graph theoretic properties. <ref> [PT87] </ref> note that the problem of deriving a "clean" polynomial time algorithm for MDPs, without using general linear programming or approximate techniques, is an important open question that has not been emphasized in the literature. Current methods do not take advantage of the MDP directed graph representation. <p> We should note however that both the (stochastic) MDP problem and the two player game, when payoffs are constants (0 or 1 say), are P -complete, whereas the deterministic MDP problem and the one player mean-payoff game are in N C (see <ref> [PT87] </ref> and [Lit96]), so the deterministic versions are very likely strictly easier. We may still ask whether graph theoretic properties and methods based on them should be abandoned when we move from one player to two players in mean payoff games or from deterministic transitions to stochastic transitions in MDPs.
Reference: [Put94] <author> Martin L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> Wiley Inter-science, </publisher> <year> 1994. </year>
Reference-contexts: The concepts behind these models date as far back as the calculus of variations problems of the 17th century. Many researchers including Masse, Walde, Bellman, Shapley and Karlin conducted the research that originated the field in the forties and fifties (see <ref> [Put94] </ref> for a historical background). Applications of MDPs are diverse: MDPs are used to model problems in inventory management, highway pavement maintenance, quality control and more (see for example [Put94] and [Whi88]). <p> researchers including Masse, Walde, Bellman, Shapley and Karlin conducted the research that originated the field in the forties and fifties (see <ref> [Put94] </ref> for a historical background). Applications of MDPs are diverse: MDPs are used to model problems in inventory management, highway pavement maintenance, quality control and more (see for example [Put94] and [Whi88]). The partially observable MDP (POMDP) is an important generalization where the decision maker has only partial information on the current state of the system. <p> The partially observable MDP (POMDP) is an important generalization where the decision maker has only partial information on the current state of the system. Applications of such models are again diverse and include medical decision making, design of teaching systems, cost control in accounting and equipment maintenance and replacement <ref> [Put94, Mon82] </ref>. Recently, computer scientists have looked at these models as a framework for planning under uncertainty [BDH95] and the models find applications in areas such as robot navigation [BRS96, KGS95] and medical decision making [Hau97]. There are several modeling questions and open algorithmic problems about MDPs and their generalizations. <p> fact that the next state is only a probabilistic function of just the current state and the action executed in the current state (and, for instance, not on how the current state was arrived at) is the so-called "Markov property", and the name Markov decision process coined by Bellman (see <ref> [Put94] </ref>) originates from this property. We are assuming that each state is observed only indirectly, through the color it emits (partial observability). <p> Next, I will describe an algorithm for SSGs, called the Hoffman-Karp algorithm [HK66], which is based on the strategy improvement method attributed to Howard and Bellman for infinite-horizon MDPs (see <ref> [Put94] </ref>). The algorithm is conceptually simple, and can be thought of as a local search in the space of strategies. It is a good candidate for being a polynomial time algorithm for SSGs.
Reference: [PV87] <author> H Peters and O. Vrieze. </author> <title> Surveys in game theory and related topics. In CWI Tract 39. </title> <address> Amsterdam, </address> <year> 1987. </year>
Reference-contexts: Lemmas 3.1, 3.2, and 3.3 are proved in [Con92] for SSGs that halt with probability 1 for simplicity, and I report them in their general form here. Some of the results have appeared in earlier literature on games (see for example <ref> [PV87] </ref>) and Condon includes them for completeness and, in some cases, proves stronger versions of them for SSGs. Recall that in the SSG value problem, we are interested in the value of the start vertex of the graph. <p> It is not hard to see that in Shapley's general case, optimal strategies may need to be randomized (mixed), so these games are probably more complex to solve. The value of a simultaneous stochastic game need not be rational (see <ref> [PV87] </ref>), and as a consequence, the corresponding value problem is unlikely to be in N P " coN P . SSGs have just enough complexity so that no polynomial time algorithm for the problem is known.
Reference: [RW87] <author> P. J. Ramadge and W. M. Wonham. </author> <title> Supervisory control of a class of discrete event processes. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 25(1) </volume> <pages> 206-230, </pages> <year> 1987. </year>
Reference-contexts: We will also look at a model of control called the discrete event system (DES) started and mainly developed by Wonham and his students <ref> [RW87] </ref>. <p> I will cover two control problems in this area. The first appears in <ref> [RW87] </ref> which introduced the formalism of DES. The second appearing in [ML97] has a stochastic flavor and is closer to MDPs. <p> The language accepted by the automaton E, L (E), is exactly the set of permissible sequence of events. Without loss of generality, we may assume L (E) L (G). In the paper <ref> [RW87] </ref>, the sought after supervisor is also a language recognizer (a finite automaton here), a natural choice, since we want the supervisor to recognize some subset of sequences of event. The means of influence of the supervisor is blocking or permitting events. <p> L (SkG) = L (E), where L (SkG) is the set of strings of events on which the transition function for the composite automaton SkG is defined. The following example appearing in <ref> [RW87] </ref> should make a few of the concepts more concrete. Consider two users of a single resource, each modeled as in Figure 5a. The combined system which is the generator G to supervise is shown in Figure 5b. <p> Given a system G and a specification E, where L (E) L (G), a supervisor does not necessarily exist. It is shown in <ref> [RW87] </ref> that a supervisor exists only if the plant cannot generate a sequence of events w that is legal, followed by a sequence u composed of uncontrollable events only, that makes the whole sequence, wu, illegal, i.e. wu 62 L (E). <p> In fact, in this case, we rather seek a supervisor that allows a maximal set of of legal behavior without allowing any nonlegal sequence of events to occur. We can do better: <ref> [RW87] </ref> show that the set of languages that are prefix-closed, controllable and contained in L (E) is closed under arbitrary unions. <p> We covered several possible restrictions to partial observability in POMDPs and conjectured that presence of multiple types of restrictions might lead to more efficient algorithms. The original discrete event systems of <ref> [RW87] </ref> seemed to be an entirely different model of control due to its assumptions and objective criteria.
Reference: [Sha53] <author> L.S. Shapley. </author> <title> Stochastic games. </title> <booktitle> Proceedings of the National Academy of sceinces USA, </booktitle> <volume> 39 </volume> <pages> 1095-1100, </pages> <year> 1953. </year>
Reference-contexts: A major motivating factor for studying these problems further would be real world problems that can be satisfactorily solved utilizing such models. 10 3 Simple Stochastic Games Simple stochastics games (SSGs) are a special kind of two player stochastic games, also referred to as Markov games, introduced by Shapley <ref> [Sha53] </ref>. Numerous variations of Shapley's model have been studied and many algorithms have been proposed (see [Con93] for references). Con-don in [Con92] investigates the computational complexity of a decision problem associated with SSGs. <p> Clearly a stopping SSG halts with probability 1 (hence the above lemmas apply). Furthermore, Shapley in <ref> [Sha53] </ref> shows that stopping SSGs have unique fixed-points 3 . cn 0-sink j Condon shows that any SSG G can be converted in polynomial time to a special kind of a stopping SSG G 0 such that the value of G is greater than 1=2 if and only if the value
Reference: [SS73] <author> R. Smallwood and E. Sondik. </author> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: Though there are infinitely many distributions over actions (infinitely many points in the simplex), it is not hard to show that the number of connected regions where the same action is optimal is finite in finite horizon problems (see figure 1b) and the borders between these regions are linear <ref> [SS73] </ref>. Figure 1b shows a 3 dimensional unit simplex representing the set of probability distributions over 3 states. The simplex is partitioned into two regions with a piecewise linear border. A dynamic programming algorithm can compute optimal partitions but unfortunately it may run in exponential time.
Reference: [Whi88] <author> Douglas J. White. </author> <title> Further real applications of Markov decision processes. </title> <journal> Interfaces, </journal> <volume> 18 </volume> <pages> 55-61, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Applications of MDPs are diverse: MDPs are used to model problems in inventory management, highway pavement maintenance, quality control and more (see for example [Put94] and <ref> [Whi88] </ref>). The partially observable MDP (POMDP) is an important generalization where the decision maker has only partial information on the current state of the system.
Reference: [ZP96] <author> Uri Zwick and Mike Paterson. </author> <title> The complexity of mean payoff games on graphs. </title> <institution> Theorteical Computer Science, 158(1-2):343-359, </institution> <month> May </month> <year> 1996. </year> <month> 27 </month>
Reference-contexts: If the value problem is in P , then randomizing does not add (significant) power to log-space bounded alternating machines: such alternating Turing machines already accept exactly the languages in P . From an application viewpoint, a variation on SSGs, called mean-payoff games in <ref> [ZP96] </ref>, arise naturally in the analysis of worst case behavior of algorithms for several on-line problems, for example finite-window string matching and metrical task systems. <p> N P " coN P with natural extensions to the model such as presence of multiple edges (actions) at each vertex, extending the transition probabilities to be rational fractions other than 1=2 or associating rewards with edges or vertices and modifying the objective criteria for each player accordingly (see also <ref> [ZP96] </ref>). Hence we note that simple stochastic games, are a generalization of fully observable MDPs where there are two decision makers instead of one. In Shapley's games model, the players simultaneously choose an action at each state of the game. <p> for the player. 15 If we restrict the vertices to be only max and min, that is if we take the stochastic nature of the game out, then the problem is solvable in polynomial time and a simple algorithm for such appears in [Con92] (also follows from an algorithm in <ref> [ZP96] </ref>). Next, I will describe an algorithm for SSGs, called the Hoffman-Karp algorithm [HK66], which is based on the strategy improvement method attributed to Howard and Bellman for infinite-horizon MDPs (see [Put94]). <p> The deterministic version of SSGs (i.e. no average vertices) with weights, rewards or costs, associated with edges is relatively hard too. Zwick and Paterson investigate a version of deterministic 2-player games called mean-payoff games which arise in the study of some online problems <ref> [ZP96] </ref>. These are basically the deterministic version of SSGs with edges having payoff (rewards or costs) associated with them and one player (player 1) wants to maximize the average payoff obtained per move over the infinite horizon, while the adversary (player 0) wants to minimize it.
References-found: 26

