URL: http://www.cs.umd.edu/~hollings/cs818z/f96/papers/pvmArticle.ps
Refering-URL: http://www.cs.umd.edu/~hollings/cs818z/f96/readingList.htm
Root-URL: 
Title: The PVM Concurrent Computing System: Evolution, Experiences, and Trends  
Author: V. S. Sunderam G. A. Geist J. Dongarra R. Manchek 
Note: Research supported by the Applied Mathematical Sciences program, Office of Basic Energy Sciences, U. S. Department of Energy, under Grant No. DE-FG05-91ER25105, and contract DE-AC05-84OR21400 with Martin Marietta Energy Systems, Inc.; and the National Science Foundation, under Award Nos. CCR 9118787 and CCR-8809615.  
Address: Atlanta, GA 30322, USA  Oak Ridge, TN 37831, USA  Knoxville, TN 37996, USA  
Affiliation: Department of Mathematics and Computer Science Emory University,  Mathematical Sciences Section Oak Ridge National Laboratory  Computer Science Department University of Tennessee,  
Abstract: The PVM system, a software framework for heterogeneous concurrent computing in networked environments, has evolved in the past several years into a viable technology for distributed and parallel processing in a variety of disciplines. PVM supports a straightforward but functionally complete message passing model, and is capable of harnessing the combined resources of typically heterogeneous networked computing platforms to deliver high levels of performance and functionality. In this paper, we describe the architecture of PVM system, and discuss its computing model, the programming interface it supports, auxiliary facilities for process groups and MPP support, and some of the internal implementation techniques employed. Performance issues, dealing primarily with communication overheads, are analyzed, and recent findings as well as experimental enhancements to are presented. In order to demonstrate the viability of PVM for large scale scientific supercomputing, the paper includes representative case studies in materials science, environmental science, and climate modeling. We conclude with a discussion of related projects and future directions, and comment on near and long-term potential for network computing with the PVM system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adam Beguelin, </author> <title> "Xab: A Tool for Monitoring PVM Programs", </title> <booktitle> Workshop on Heterogeneous Processing, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, California, </publisher> <pages> pp. 92-97, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: A few are mentioned below: * HeNCE is a graphical programming system for PVM; this toolkit generates PVM programs, from depictions of parallelism dependencies as directed graphs, and provides an interactive administrative interface for virtual machine configuration, application execution, and animated visualization. * Xab <ref> [1] </ref> is a graphical tool for the run time monitoring of PVM programs. It gathers monitoring events from applications, and displays this information, which can be useful for profiling, error detection, and optimization. * The DoPVM subsystem [17] is aimed at supporting the "shared object" paradigm in PVM.
Reference: [2] <editor> H. J. Siegel, et. al., </editor> <title> "An Overview of the PASM Parallel Processing System", in it Computer Architecture, </title> <editor> D. D. Gajski, et. al. (eds), </editor> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, DC, </address> <pages> pp. 319-407, </pages> <year> 1987. </year>
Reference-contexts: While explicit attention to the heterogeneous aspects and "functionality exploitation" in such scenarios has only recently received formalized attention, the concept, in some form has been explored previously, e.g. <ref> [2] </ref>, although usually with a narrow focus or based on a specialized architecture.
Reference: [3] <author> V. S. Sunderam, </author> <title> "PVM : A Framework for Parallel Distributed Computing", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The original version of the system <ref> [3] </ref> was ambitious, in that it attempted to be heterogeneous in terms of programming model as well support for emulated shared memory, in addition to message passing, was incorporated.
Reference: [4] <author> A. Beguelin, et. al., </author> <title> "HeNCE Users Guide", </title> <institution> University of Tennessee Technical Report, </institution> <month> May </month> <year> 1992. </year>
Reference: [5] <author> N. Carriero and D. Gelernter, </author> <title> "Linda in Context", </title> <journal> Communications of the ACM, </journal> <volume> 32(4), </volume> <pages> pp. 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The more widely adopted of these systems are described in detail elsewhere in this volume; in this subsection, we mention a few representative systems and comment on differences in functionality and performance. Linda <ref> [5] </ref> is a concurrent programming model based on the concept of a "tuple-space", a distributed shared memory abstraction via which cooperating processes communicate. P4 and (its derivative Parmacs) [6] are libraries of macros and subroutines developed at Argonne National Laboratory and GMD, for programming a variety of parallel machines.
Reference: [6] <author> R. Butler and E. Lusk, </author> <title> "User's Guide to the P4 Programming System", </title> <institution> Argonne National Laboratory, </institution> <type> Technical Report ANL-92/17, </type> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Linda [5] is a concurrent programming model based on the concept of a "tuple-space", a distributed shared memory abstraction via which cooperating processes communicate. P4 and (its derivative Parmacs) <ref> [6] </ref> are libraries of macros and subroutines developed at Argonne National Laboratory and GMD, for programming a variety of parallel machines. They support both the shared-memory model (based on monitors) and the distributed-memory model (using message-passing).
Reference: [7] <author> A. Kolawa, </author> <title> "The Express Programming Environment", </title> <booktitle> Workshop on Heterogeneous Network-Based Concurrent Computing, </booktitle> <address> Tallahassee, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: They support both the shared-memory model (based on monitors) and the distributed-memory model (using message-passing). Express is a collection of tools, including a message passing interface, for programming distributed memory multiprocessors, including network clusters <ref> [7] </ref>. Various other systems with similar capabilities are also in existence; a reasonably comprehensive listing may be found in [8].
Reference: [8] <author> Louis Turcotte, </author> <title> "A Survey of Software Environments for Exploiting Networked Computing Resources", </title> <type> Draft Report, </type> <institution> Engineering Research Center for Computational Field Simulations, Mississippi State, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Express is a collection of tools, including a message passing interface, for programming distributed memory multiprocessors, including network clusters [7]. Various other systems with similar capabilities are also in existence; a reasonably comprehensive listing may be found in <ref> [8] </ref>. With the exception of Linda, whose programming model is not based on conventional message passing, most other systems support very similar facilities, the core primitives being system-specific variants of send and receive.
Reference: [9] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> "A machine-independent communication library", </title> <editor> In J. Gustafson, editor, </editor> <booktitle> The Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pp. 565-568, </pages> <address> P.O. Box 428, Los Altos, CA, 1990. </address> <publisher> Golden Gate Enterprises. </publisher>
Reference-contexts: These omissions are deliberate to a certain extent; the PVM philosophy is to support a core kernel of primitives above which auxiliary layers may be added (e.g. a PICL <ref> [9] </ref> port to PVM [10]), while ensuring that facilities that can only be provided at the system level are comprehensive and functionally complete. In terms of performance, most message passing systems exhibit only marginal differences, although systematic comparative studies have not been undertaken.
Reference: [10] <author> G. A. Geist and V. S. Sunderam, </author> <title> "Network Based Concurrent Computing on the PVM System", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <volume> 4(4), </volume> <pages> pp. 293-311, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: These omissions are deliberate to a certain extent; the PVM philosophy is to support a core kernel of primitives above which auxiliary layers may be added (e.g. a PICL [9] port to PVM <ref> [10] </ref>), while ensuring that facilities that can only be provided at the system level are comprehensive and functionally complete. In terms of performance, most message passing systems exhibit only marginal differences, although systematic comparative studies have not been undertaken.
Reference: [11] <author> B. Schmidt and V. S. Sunderam, </author> <title> "Empirical Analysis of Overheads in Cluster Environments", </title> <journal> Journal of Concurrency: Practice and Experience, </journal> <note> (to appear), </note> <year> 1993. </year>
Reference-contexts: Even in a dedicated networked environment, with no external use, the above is true since operating system activity, window and filesystem overheads, and administrative network traffic can contribute to deviated measurements. If these factors are ignored, network computing systems behave in a manner that is reasonably predictable <ref> [11] </ref>.
Reference: [12] <author> A. Beguelin, J. J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam. </author> <title> "A Users' Guide to PVM Parallel Virtual Machine", </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Detailed expositions as well as pedagogical material on various aspects of the PVM system may be found, for example, in <ref> [12, 13, 14] </ref>, in addition to the papers already cited. At the time of writing, PVM continues to be a popular and widely used system for concurrent computing, and it is expected that the project will mature and evolve even further in the future.
Reference: [13] <author> A. Beguelin, J. Dongarra, G. Geist, R. Manchek, and V. Sunderam, </author> <title> "Solving Computational Grand Challenges Using a Network of Supercomputers", </title> <booktitle> Proceedings of the Fifth SIAM Conference on Parallel Processing, </booktitle> <editor> D. Sorensen, ed., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Detailed expositions as well as pedagogical material on various aspects of the PVM system may be found, for example, in <ref> [12, 13, 14] </ref>, in addition to the papers already cited. At the time of writing, PVM continues to be a popular and widely used system for concurrent computing, and it is expected that the project will mature and evolve even further in the future.
Reference: [14] <author> G. A. Geist and V. S. Sunderam, </author> <title> "The Evolution of the PVM Concurrent Computing System", </title> <booktitle> Proceedings 26th IEEE Compcon Symposium, </booktitle> <pages> pp. 471-478, </pages> <address> San Fransisco, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Detailed expositions as well as pedagogical material on various aspects of the PVM system may be found, for example, in <ref> [12, 13, 14] </ref>, in addition to the papers already cited. At the time of writing, PVM continues to be a popular and widely used system for concurrent computing, and it is expected that the project will mature and evolve even further in the future.
Reference: [15] <author> J. Leon, et. al., </author> <title> "Fail Safe PVM: A Portable Package for Distributed Programming with Transparent Recovery", </title> <institution> School of Computer Science Technical Report, Carnegie-Mellon University, CMU-CS-93-124, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Small-scale SMM's are re-emerging, and a version of PVM that utilizes physical shared memory 13 for interaction between the daemon and all user processes on such machines is being de-veloped. Another project is aimed at providing fail-safe capabilities in PVM <ref> [15] </ref>. This enhanced version uses checkpointing and rollback to recover from single-node failures in an application-transparent manner, provided the application is not dependent on real-time events. Several other enhancements are also in progress, including load balancing extensions, integrating debugging support, and task queue management [16].
Reference: [16] <author> J. Dongarra, et. al., </author> <title> Abstracts: PVM User's Group Meeting, </title> <institution> University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: This enhanced version uses checkpointing and rollback to recover from single-node failures in an application-transparent manner, provided the application is not dependent on real-time events. Several other enhancements are also in progress, including load balancing extensions, integrating debugging support, and task queue management <ref> [16] </ref>. One somewhat different and more extensive subsystem under development is a generalized distributed computing (GDC) layer for PVM. While scientific applications have provided the technical impetus for the PVM project, more general and commercially oriented uses are now evolving.
Reference: [17] <author> C. Hartley and V. S. Sunderam, </author> <title> "Concurrent Programming with Shared Objects in Networked Environments", </title> <booktitle> Proceedings 7th Intl. Parallel Processing Symposium, </booktitle> <pages> pp. 471-478, </pages> <address> Los Angeles, </address> <month> April </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: It gathers monitoring events from applications, and displays this information, which can be useful for profiling, error detection, and optimization. * The DoPVM subsystem <ref> [17] </ref> is aimed at supporting the "shared object" paradigm in PVM.
References-found: 17

