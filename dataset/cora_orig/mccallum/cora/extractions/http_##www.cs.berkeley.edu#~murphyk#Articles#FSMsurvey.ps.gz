URL: http://www.cs.berkeley.edu/~murphyk/Articles/FSMsurvey.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~murphyk/publ.html
Root-URL: 
Title: Passively Learning Finite Automata  
Author: Kevin P. Murphy 
Note: Contents  
Date: 16 November 1995  
Abstract: We provide a survey of methods for inferring the structure of a finite automaton from passive observation of its behavior. We consider both deterministic automata and probabilistic automata (similar to Hidden Markov Models). While it is computationally intractible to solve the general problem exactly, we will consider heuristic algorithms, and also special cases which are tractible. Most of the algorithms we consider are based on the idea of building a tree which encodes all of the examples we have seen, and then merging equivalent nodes to produce a (near) minimal automaton. 
Abstract-found: 1
Intro-found: 1
Reference: [AHU83] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: If we want to represent an FSM, each arc can be labelled by an input/output pair; this is sometimes called a behavioral specification. Left: the maximal canonical automaton (which is non-deterministic). Middle: the prefix tree canonical automaton. (A prefix tree is also called a trie <ref> [AHU83] </ref>.) Right: the DAG (Directed Acyclic Graph) canonical automaton. 14 We can always construct a canonical automaton M compatible with any given finite input set I, as illustrated in Figure 2. (An automaton M is called canonical if L (M ) = I.) What we would like is to find the
Reference: [Ang78] <author> D. Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39(3) </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: If the sample isn't uniform complete, and is missing even an arbitrarily small fixed fraction 0 &lt; * &lt; 1 (i.e., the input contains only (j n j) * strings) then Angluin <ref> [Ang78] </ref> showed that the problem is NP-hard. Obviously, therefore, the case of learning the smallest DFA consistent with an arbitrary set of positive and negative examples is also NP-hard [Gol78, Ang78]. <p> Obviously, therefore, the case of learning the smallest DFA consistent with an arbitrary set of positive and negative examples is also NP-hard <ref> [Gol78, Ang78] </ref>. <p> He therefore made the approximation that a uniform complete sample must be of size (2 2 log 2 n2+4+1+1 1)=(2 1) = 16n 2 1. Contrary to Angluin's result <ref> [Ang78] </ref>, he found that it was possible to (approximately) learn these random DFAs quite well even if the training set only contained about 3% of the uniform complete sample. 3.4.3 Learning random DFAs from sparse samples: the Greedy Russian algorithm We briefly present Lang's algorithm [Lan92], which is a variation of
Reference: [Ang87] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: This is called active learning, or learning with queries; for a good account of actively learning DFAs, see [KV94]. Angluin <ref> [Ang87] </ref> showed that the learner must be able to ask both membership queries (is x in the language? 9 ) and equivalence queries (is the hypothesis M 0 equal to the target concept M , and if not, please give me a counterexample) in order to exactly learn DFAs in time
Reference: [Ang92] <author> D. Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 351-369, </pages> <year> 1992. </year>
Reference-contexts: Since this is a difficult goal to achieve, often we will find it acceptable if, with high probability, we can learn a model which is approximately equal to the target concept. This notion, called PAC-learning (Probably Approximately Correct), is due to Valiant, and is discussed further in <ref> [Ang92, KV94] </ref>. The formal definition is given below.
Reference: [AS83] <author> D. Angluin and C. H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction In their excellent survey paper, Angluin and Smith <ref> [AS83] </ref> define "inductive inference" as the "process of hypothesizing a general rule from examples".
Reference: [AU72] <author> A. V. Aho and J. D. Ullman. </author> <title> The Theory of Parsing, Translation and Compiling. Vol. I. </title> <publisher> Prentice Hall, </publisher> <year> 1972. </year>
Reference-contexts: We shall define these more precisely, and then show how PFAs relate to Markov Chains and Hidden Markov Models (HMMs). DFAs are discussed more fully in <ref> [AU72, HU79] </ref> and PFAs in [Paz71]; for a good tutorial article on HMMs, see [Rab89]. 2.1 Deterministic Finite Automata Definition 1 A Deterministic Finite Automaton (DFA) is a tuple M = (; Q; q 0 ; F; ffi), where is the input alphabet, Q is the set of states, q 0 <p> There is a simple algorithm for minimizing a given DFA with n states in O (n 2 ) time and a slightly more complex one which runs in O (n log n) time <ref> [AU72, Hop71] </ref>. This algorithm uses an equivalence relation to partition the states into equivalence classes. <p> If we allow the empty string as an input (which does not make sense for Mealy machines), then the degree of distinguishability is d n 2 = 2 <ref> [AU72, p.125] </ref>. 3.4.2 The complexity of the Russian algorithm When comparing two nodes for equivalence, it isn't always necessary to compare the whole of their subtrees, as we now show. <p> Clearly q k+1 q 0 , q k q 0 and Also, it can be shown [HU79] that any partition of the states induced by will lead to the minimal DFA. This suggests the following iterative strategy for minimizing a DFA, known as Moore's algorithm <ref> [AU72] </ref>. k := 0 compute 0 while k n 2 and k 6= k1 do k := k + 1 compute k in terms of k1 merge the k-equivalent states This can be implemented to run in O (jjn 2 ) time as follows [HU79].
Reference: [AW92] <author> N. Abe and M. K. Warmuth. </author> <title> On the computational complexity of approximating ditributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: The KL divergence only satisfies (1). 24 The KL divergence enjoys several other nice properties, as we now show <ref> [AW92] </ref>. Firstly, note that D KL (P jjQ) = E P log (1=Q ()) H (P ), where H (P ) = P x2X P (x) log (1=P (x)) is the entropy of the distribution P . <p> Stolcke et al. achieve this by using a prior based on the description length of the (structure of the) model. This introduces a bias towards models with fewer states. (This bias is called an Occam factor). 25 4.2 Complexity results Abe and Warmuth <ref> [AW92] </ref> showed that PFAs are trainable in time polynomial in *, ffi and m (the length of each input string), but not in jj, assuming RP 6= N P . 12 However, this is a representation dependent result, that is, it assumes the hypothesis (as well as the target) must be
Reference: [BC89] <author> A. R. Barron and T. M. </author> <title> Cover. Miniumum complexity density estimation. </title> <journal> IEEE Trans. Info. Theory, </journal> <year> 1989. </year>
Reference-contexts: starting from q visits every state in M with probability at least 1/2. 23 Several measures of distance between two probability distributions (with set of support X ) have been pro-posed, including the 2 distance, the L 1 distance, the variation distance, the quadratic distance [KS90], and the Hellinger distance <ref> [BC89] </ref>. However, we shall use the popular Kullback-Leibler divergence (also known as the cross or relative entropy [CT91]), defined by D KL (P jjQ) = x2X P (x) We think of P as being the true, target distribution, and Q as being our best guess.
Reference: [BCHM94] <author> P. Baldi, Y. Chauvin, T. Hunkapiller, and M. A. McClure. </author> <title> Hidden markov models of biological primary sequence information. </title> <booktitle> Proc. of the National Academy of Science, USA, </booktitle> <volume> 91 </volume> <pages> 1059-1063, </pages> <year> 1994. </year>
Reference-contexts: HMMs have been very widely used in the speech and handwriting recognition communities (see e.g., [Rab89, RST95]), and have also recently been applied to recognizing patterns in biological sequences such as DNA and proteins (see e.g., <ref> [KMH93, KBM + 94, BCHM94] </ref>).
Reference: [BF72] <author> A. W. Biermann and J. A. Feldman. </author> <title> On the synthesis of finite-state machines from samples of their behavior. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 592-597, </pages> <year> 1972. </year>
Reference-contexts: Since each pair (p; q) is considered at most jj times (it could be on up to jj lists), the running time is O (jjn 2 ). As Miclet [Mic90] points out, the k-tails algorithm of Biermann and Feldman <ref> [BF72] </ref> can be thought of as running Moore's algorithm but terminating at a particular value of k. If we stop at a low value of k, the resulting machine will be small but highly non-deterministic (and therefore not equivalent to the target DFA).
Reference: [BHK + 93] <author> M. P. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Using dirichlet mixtures priors to derive hidden Markov models for protein families. </title> <booktitle> In International Conf. on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 47-55, </pages> <year> 1993. </year>
Reference-contexts: the effect of adding "virtual samples" to the data set, which is similar to the somewhat more ad hoc approach to data smoothing of simply adding a constant to all the empirical counts, of which we saw an example above. (Similar approaches are used in other applications of HMMs, e.g., <ref> [BHK + 93] </ref>.) 5.5 A new algorithm for learning NPFAs Intuitively, it seems that it ought to be possible to extend the idea of merging states whose signatures (subtrees) are similar to the case of learning non-deterministic PFAs.
Reference: [BK76] <author> A. W. Biermann and R. Krishnaswamy. </author> <title> Constructing programs from example computations. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-2:141-153, </volume> <year> 1976. </year> <month> 34 </month>
Reference-contexts: This runs in time which is exponential in n, the number of states in the final minimal automaton. (This algorithm is similar to the one by Biermann et al. <ref> [BK76] </ref>). 5 Exponential time may be necessary because we may need to consider all possible subsets of nodes, to find which ones should be merged. [Pfl73] proved the general problem of minimizing an FSM from a partial specification is NP-complete. 12 In more detail, the algorithm works as follows.
Reference: [CO94] <author> R. C. Carassco and J. Oncina. </author> <title> Learning stochastic regular grammars by means of a state merging method. </title> <booktitle> In 2nd Intl. Colloq. on Grammatical Inference and Applications, </booktitle> <pages> pages 139-152, </pages> <year> 1994. </year>
Reference-contexts: If there are many type 2 errors, the inferred distribution will not be close to the target distribution; to reduce the type 2 error rate, we can make our definition of similarity more restrictive (at the risk of introducing more type 1 errors). Carrasco and Oncina <ref> [CO94] </ref> show that in their algorithm, the type 2 error rate vanishes in the limit of infinite data, and hence they can achieve identification in the limit with probability 1. <p> However, if the learner is given access to an oracle which can specify the probability distribution induced by any string, then the input-PFA can be learnt in polynomial time. He assumes throughout that the probabilities are rational. 4.3 The Spanish algorithm The algorithm due to Carrasco and Oncina <ref> [CO94] </ref>, which we call the "Spanish" algorithm for brevity, is basically a stochastic version of the "Greedy Russian" algorithm we discussed earlier. The difference is that the input is now a multiset of positive strings instead of a set of positive and negative strings.
Reference: [Con71] <author> J. H. Conway. </author> <title> Regular Algebra and Finite Machines. </title> <publisher> Chapman and Hall, </publisher> <year> 1971. </year>
Reference-contexts: In the more general case of inferring the structure of an FSM from a series of experiments, see <ref> [Con71] </ref>. (See also Exercise 3.22 in [HU79].) 22 Rivest and Schapire [RS89] show how the DFA can still be learnt in this case, by using homing sequences.
Reference: [Cru91] <author> J. P. Crutchfield. </author> <title> Reconstructing language hierarchies. </title> <editor> In H. Atmanspacher and H. Schein-graber, editors, </editor> <booktitle> Information Dynamics, </booktitle> <pages> pages 45-60. </pages> <publisher> Plenum Press, </publisher> <year> 1991. </year>
Reference-contexts: transition probabilities. (A similar comment can be made about neural networks. 2 ) We are interested in the more general problem of learning the topology of the model, as well as the transition probabilities, starting from scratch. (The even more general problem of choosing a model class is discussed in <ref> [Cru91] </ref>.) The hope is that PFAs can then be applied to domains where we have little or no prior knowledge as to what the correct structure should look like.
Reference: [CT91] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: However, we shall use the popular Kullback-Leibler divergence (also known as the cross or relative entropy <ref> [CT91] </ref>), defined by D KL (P jjQ) = x2X P (x) We think of P as being the true, target distribution, and Q as being our best guess. <p> The L 1 norm is defined as jjP Qjj 1 = x2X the L 2 norm (quadratic distance) as jjP Qjj 2 = X (P (x) Q (x)) 2 2 and the variational distance as max (P (B) Q (B)) = 2 Now, it can be shown <ref> [CT91, p.300] </ref> that D KL (P jjQ) 1 2 ln 2 jjP Qjj 2 1 , so that if D KL (P jjQ) *, then jjP Qjj 1 *2 ln 2, which is less than * when * &gt; 2 ln 2 = 1:36.
Reference: [CY89] <author> J. P. Crutchfield and K. Young. </author> <title> Inferring statistical complexity. </title> <journal> Physical Review Letters, </journal> <volume> 63(2) </volume> <pages> 105-108, </pages> <year> 1989. </year>
Reference-contexts: As an example of this, Crutchfield and Young <ref> [CY89, You91] </ref> propose learning a PFA from a series of measurements taken from a non-linear dynamical system, and using the "complexity" of the resulting PFA as a measure of the complexity of the original system. (See also [Li90].) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem <p> This means that, to learn Markov chains which only slowly converge to their stationary distribution, you need more data, and hence more time. 4.6 The Crutchfield algorithm Crutchfield and Young <ref> [CY89] </ref> proposed an algorithm very similar to the two-pass Russian algorithm, but which has different input and output.
Reference: [DM93] <author> S. Das and M. Mozer. </author> <title> A unified gradient-descent/ clustering algorithm for finite state machine induction. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 6, </volume> <year> 1993. </year>
Reference-contexts: Consequently, most early attempts to learn DFAs with recurrent NNs (both first and second order) from positive and negative examples were not very successful: they took a long time to run, and couldn't even learn machines with as few as 5 states. More recently, Das and Mozer <ref> [DM93] </ref> have suggested clustering points in the hidden layer's state space before passing it on to other layers, to encourage the formation of discrete representations during training.
Reference: [FAK94] <author> Y. Fujiwara, M. Asogawa, and A. Konagaya. </author> <title> Stochastic motif extraction using hidden markov model. </title> <booktitle> In International Conf. on Intelligent Systems for Molecular Biology, </booktitle> <year> 1994. </year>
Reference-contexts: In <ref> [FAK94] </ref>, the authors are interested in using HMMs for finding common patterns in proteins called motifs. 31 to the state are copied to the new state. <p> In the next two columns we illustrate how a particular state may be connected up, what the new topology will look like after duplication, and how it may end up looking after pruning. From Fig. 16 of <ref> [FAK94] </ref>. Their topology learning algorithm is as follows. They start out with a fully connected graph on n nodes.
Reference: [FKM + 95] <author> Y. Freund, M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, and R. E. Schapire. </author> <title> Efficient algorithms for learning to play repeated games against computationally bounded adversaries. </title> <booktitle> In IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1995. </year>
Reference-contexts: DFAs have also been proposed as a model of players (in a game-theoretic sense) with bounded rationality; learning to play optimally against such players is very similar to the inference problem defined above; see for example <ref> [FKM + 95] </ref> and references therein. Finally, we can imagine the problem of a robot trying to learn the structure of its environment, which can be modelled as a DFA; see [RS87, RS89, MB91].
Reference: [FKR + 93] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1993. </year>
Reference-contexts: The "maximally random" case, when both the topology and the labelling of the states (as either accept or reject) are randomly chosen, is studied by Trahktenbrot and Barzdin'. More recently, Freund et al. <ref> [FKR + 93] </ref> have studied the case where the topology may be adversarially chosen, and only the labelling is random; such DFAs are called typical. <p> Freund et al. <ref> [FKR + 93] </ref> extend the definitions and theorems of the somewhat less random case considered by Trakhtenbrot and Barzdin' as follows. <p> Intuitively, a homing sequence is an input sequence which, when executed, may allow the learner to determine "where it is" in the machine, based on the observed output sequence. Freund et al. <ref> [FKR + 93] </ref> show how to extend this method to the case of passively learning typical DFAs from a random walk, (i.e., learning random DFAs without an oracle) in the no-reset case.
Reference: [Fu82] <author> K. S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: We concentrate on recent results which are not mentioned in the 1982 survey on inductive inference by Angluin and Smith and the 1990 survey on grammatical inference by Miclet [Mic90]. (See also the book by Fu <ref> [Fu82] </ref>.) 1.1 Applications of automaton inference Most of the work on automaton inference (especially the early work) is theoretical in nature: finite automata are simple, so we can characterize more easily how hard it is to learn them, both in terms of how large the 1 We will consider two cases:
Reference: [Gai76] <author> B. R. Gaines. </author> <title> Behaviour/structure transformations under uncertainty. </title> <journal> Intnl. J. of Man-Machine Studies, </journal> <volume> 8 </volume> <pages> 337-365, </pages> <year> 1976. </year>
Reference-contexts: They show that the performance of this algorithm is comparable to their explicit enumeration method. 3.3.3 Enumerating all compatible automata of a given size Gaines <ref> [Gai76] </ref> presents an algorithm which produces an exhaustive enumeration of all the n-state Moore machines which are compatible with the input. If n is too small, the Moore machine may be non-deterministic.
Reference: [Gol76] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1976. </year>
Reference-contexts: If the strings are unlabelled, we assume they are positive examples of the target (stochastic) regular language we are trying to learn. Unfortunately, in the deterministic case, it is not possible to learn a language from positive data alone <ref> [Gol76] </ref>, so often the strings will be labelled as either in the language (positive examples) or not in the language (negative examples). <p> In much of the literature, the goal has been to learn the right rule (i.e., the rule believed to be generating the examples) in the limit of infinite data. This notion is called identification in the limit, and was invented by Gold <ref> [Gol76] </ref>. In the case of DFAs, the "right rule" means that the inferred model should be as small as possible, and contain all the strings in the set of positive examples and none of the strings in the set of negative examples. <p> A positive presentation just consists of all the strings in L. We assume L could be an infinite set. Gold <ref> [Gol76] </ref> showed that it is not possible to exactly identify L, even in the limit of infinite data, given only a positive presentation. This seems to be a paradox in view of the fact that humans seem to learn languages from only positive examples.
Reference: [Gol78] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: Obviously, therefore, the case of learning the smallest DFA consistent with an arbitrary set of positive and negative examples is also NP-hard <ref> [Gol78, Ang78] </ref>.
Reference: [GS94] <author> D. Gillman and M. Sipser. </author> <title> Inference and minimization of hidden markov chains. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-158, </pages> <year> 1994. </year>
Reference-contexts: A 3-CNF (Conjunctive Normal Form) formula is a conjunction of disjunctions, each of which contains exactly three literals, e.g., (p _ :q _ r) ^ (s _ p _ :p). 14 In <ref> [GS94] </ref>, they give an information-theoretic argument, which does not make any assumptions, that learning a hidden Markov chain is hard. 26 the first i bits of x that we have examined (as determined by a) have even parity. <p> Adapted from Figure 1 of [RST95]. they induce on the states, i.e., the probability that the automaton ends up in each state after accepting the string. He shows this problem is NP-complete (see also <ref> [GS94] </ref>). However, if the learner is given access to an oracle which can specify the probability distribution induced by any string, then the input-PFA can be learnt in polynomial time.
Reference: [Gus95] <author> Dan Gusfield. </author> <title> Algorithms on Strings: A Dual View from Computer Science and Computational Molecular Biology. </title> <note> 1995. In preparation. </note>
Reference-contexts: The stationary distribution M () is the unique distribution satisfying M (q) = q 0 16 A suffix tree for a set of strings S is the prefix tree of the set of suffixes of the strings in S (see <ref> [Gus95] </ref>). A probabilistic suffix tree is a suffix tree with probabilities attached to each branch.
Reference: [HGC94] <author> D. Heckerman, D. Geiger, and M. Chickering. </author> <title> Learning bayesian networks: the combination of knowledge and statistical data. </title> <type> Technical Report MSR-TR-09-09, </type> <institution> Microsoft Research, </institution> <year> 1994. </year>
Reference-contexts: system, and using the "complexity" of the resulting PFA as a measure of the complexity of the original system. (See also [Li90].) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem of learning probabilistic models which are more powerful than PFAs, such as belief (Bayesian) networks <ref> [Pea88, HGC94] </ref> or stochastic Context Free Grammars [JLM92], but learning PFAs will prove to be hard enough. In fact, the problem is so hard (i.e., time-consuming) that we will mostly concentrate on a special case, which we call "deterministic" PFAs (DPFAs).
Reference: [HKP91] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> An Introduction to the Theory of Neural Comptuation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Also, the results of this model applied to a task which involved locating genes in E. coli DNA were inferior to the results of an HMM with a hand-crafted topology (see [KMH93]), although this may be because of the postprocessing they perform to cope with overlapping genes. 2 See <ref> [HKP91] </ref> for a good book on neural nets. 5 1.3 The input to/output from the algorithms The input to the algorithms will usually be a set or multi-set of finite-length strings.
Reference: [Hop71] <author> J. E. Hopcroft. </author> <title> An n log n algorithm for minimizing the states in a finite automaton. </title> <editor> In Z. Kohavi, editor, </editor> <booktitle> The Theory of Machines and Computation. </booktitle> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: There is a simple algorithm for minimizing a given DFA with n states in O (n 2 ) time and a slightly more complex one which runs in O (n log n) time <ref> [AU72, Hop71] </ref>. This algorithm uses an equivalence relation to partition the states into equivalence classes.
Reference: [HRSJ91] <author> G. Hachtel, J.-K. Rho, F. Somenzi, and R. </author> <title> Jacoby. Exact and heuristic algorithms for the miminimization of incompletely specified state machines. </title> <booktitle> In Proc. European Design Automation Conference, </booktitle> <year> 1991. </year> <note> UCB Engin TK7868 .E93. </note>
Reference-contexts: Indeed, minimizing an incompletely specified FSM is NP-complete [Pfl73]. Nevertheless, it is an important problem in the synthesis of sequential logic circuits, so fast heuristic algorithms have been developed. There are basically two kinds, based on an explicit <ref> [HRSJ91] </ref> or implicit [KVBSV94] enumeration of the compatibles. (A compatible is a set of states equivalent in the sense that they can be merged without affecting the behavior of the machine.
Reference: [HU79] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year> <month> 35 </month>
Reference-contexts: We shall define these more precisely, and then show how PFAs relate to Markov Chains and Hidden Markov Models (HMMs). DFAs are discussed more fully in <ref> [AU72, HU79] </ref> and PFAs in [Paz71]; for a good tutorial article on HMMs, see [Rab89]. 2.1 Deterministic Finite Automata Definition 1 A Deterministic Finite Automaton (DFA) is a tuple M = (; Q; q 0 ; F; ffi), where is the input alphabet, Q is the set of states, q 0 <p> In other words, they have the same k-tails. Clearly q k+1 q 0 , q k q 0 and Also, it can be shown <ref> [HU79] </ref> that any partition of the states induced by will lead to the minimal DFA. <p> a DFA, known as Moore's algorithm [AU72]. k := 0 compute 0 while k n 2 and k 6= k1 do k := k + 1 compute k in terms of k1 merge the k-equivalent states This can be implemented to run in O (jjn 2 ) time as follows <ref> [HU79] </ref>. Construct an n fi n array and mark cell (i; j) (and (j; i)) as soon as q i and q j are found to be distinguishable. Start by marking (i; j) for all q i 2 F; q j 62 F . <p> In the more general case of inferring the structure of an FSM from a series of experiments, see [Con71]. (See also Exercise 3.22 in <ref> [HU79] </ref>.) 22 Rivest and Schapire [RS89] show how the DFA can still be learnt in this case, by using homing sequences. Intuitively, a homing sequence is an input sequence which, when executed, may allow the learner to determine "where it is" in the machine, based on the observed output sequence.
Reference: [JLM92] <author> F. Jelinek, J. D. Lafferty, and R. L. Mercer. </author> <title> Basic methods of probabilistic context-free gram-mars. </title> <note> Computational Linguistics, To appear, </note> <year> 1992. </year>
Reference-contexts: resulting PFA as a measure of the complexity of the original system. (See also [Li90].) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem of learning probabilistic models which are more powerful than PFAs, such as belief (Bayesian) networks [Pea88, HGC94] or stochastic Context Free Grammars <ref> [JLM92] </ref>, but learning PFAs will prove to be hard enough. In fact, the problem is so hard (i.e., time-consuming) that we will mostly concentrate on a special case, which we call "deterministic" PFAs (DPFAs). These are less powerful than HMMs, and hence easier to learn.
Reference: [KBM + 94] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden markov models in computational biology: Applications to protein modelling. </title> <journal> J. of Molecular Biology, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <year> 1994. </year>
Reference-contexts: HMMs have been very widely used in the speech and handwriting recognition communities (see e.g., [Rab89, RST95]), and have also recently been applied to recognizing patterns in biological sequences such as DNA and proteins (see e.g., <ref> [KMH93, KBM + 94, BCHM94] </ref>). <p> Consider, for example, the approach of "model surgery" used in HMM models of protein sequences <ref> [KBM + 94] </ref>: they start out with a "backbone" of n states, with transitions to skip over states, and transitions to side states with self-loops, which allow the insertion of extra symbols.
Reference: [KMH93] <author> A. Krogh, I. S. Mian, and D. Haussler. </author> <title> A hidden markov model that finds genes in E. Coli DNA. </title> <type> Technical Report USCS-CRL-93-33, </type> <address> U. C. Santa Cruz, </address> <institution> Dept. Comp. Sci., </institution> <year> 1993. </year> <note> Available from ftp.cse.ucsc.edu, directory pub/dna. </note>
Reference-contexts: HMMs have been very widely used in the speech and handwriting recognition communities (see e.g., [Rab89, RST95]), and have also recently been applied to recognizing patterns in biological sequences such as DNA and proteins (see e.g., <ref> [KMH93, KBM + 94, BCHM94] </ref>). <p> Also, the results of this model applied to a task which involved locating genes in E. coli DNA were inferior to the results of an HMM with a hand-crafted topology (see <ref> [KMH93] </ref>), although this may be because of the postprocessing they perform to cope with overlapping genes. 2 See [HKP91] for a good book on neural nets. 5 1.3 The input to/output from the algorithms The input to the algorithms will usually be a set or multi-set of finite-length strings.
Reference: [KMNR95] <author> M. Kearns, Y. Mansour, A. Y. Ng, and D. Ron. </author> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: since we can incrementally try n = 1, n = 2, : : : , until the results (as determined by, say, a cross-validation test) are deemed acceptable. (Some other methods of computing the right size of model have been proposed, but cross-validation seems to work the best in practice <ref> [KMNR95] </ref>.) The more serious problem is that a fully connected graph on n nodes has O (n 2 ) arcs, which is a large number of free parameters; not only are many of these potentially redundant, but models with a large number of free parameters need more training data to avoid
Reference: [KMR + 94] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> On the learnability of discrete distributions. </title> <booktitle> In ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 273-282, </pages> <year> 1994. </year>
Reference-contexts: The intuition is that, by performing repeated trials to reduce the error, we can be sure that the string is definitely accepted or rejected, since all probabilities are bounded away from , and hence we can convert the PFA to a DFA. 2.2.2 Generators vs. evaluators As shown in <ref> [KMR + 94] </ref>, when talking about a discrete probability distribution, it is important to distinguish between generators and evaluators. An evaluator for a probability distribution D (over fixed length strings, say n ) takes as input a string x and returns its probability under D. <p> The definition of acceptable precision is captured in the following definition, which comes originally from <ref> [KMR + 94] </ref>, and which has been used subsequently in [RST94, RST95]. It is inspired by the definition of PAC-learning a concept, given earlier. Definition 4 Let M be the target PFA we are trying to learn, and ^ M be a hypothesis PFA. <p> For example, assuming RP 6= N P , the class of 3-term DNF formulae is not efficiently PAC learnable unless we use 3-CNF to represent our hypotheses (see [KV94]). 13 However, Kearns et al. <ref> [KMR + 94] </ref> prove that, under a certain assumption 14 , it is not possible to efficiently PAC learn PFAs using any kind of hypothesis, when the hypothesis must be an evaluator.
Reference: [KS90] <author> M. Kearns and R. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: random walk of length t starting from q visits every state in M with probability at least 1/2. 23 Several measures of distance between two probability distributions (with set of support X ) have been pro-posed, including the 2 distance, the L 1 distance, the variation distance, the quadratic distance <ref> [KS90] </ref>, and the Hellinger distance [BC89]. However, we shall use the popular Kullback-Leibler divergence (also known as the cross or relative entropy [CT91]), defined by D KL (P jjQ) = x2X P (x) We think of P as being the true, target distribution, and Q as being our best guess.
Reference: [KV89] <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <year> 1989. </year>
Reference-contexts: Even learning near-minimal DFAs, or PAC-learning DFAs, is hard: Pitt showed that learning a near-minimal DFA (i.e., one with n k states, where n is the minimal number of states, for some fixed k) is NP-hard [PW93], and Kearns and Valiant <ref> [KV89] </ref> showed that PAC-learning a DFA with any reasonable hypothesis class is as hard as breaking various cryptographic protocols which are based on factoring.
Reference: [KV94] <author> M. J. Kearns and U. V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Since this is a difficult goal to achieve, often we will find it acceptable if, with high probability, we can learn a model which is approximately equal to the target concept. This notion, called PAC-learning (Probably Approximately Correct), is due to Valiant, and is discussed further in <ref> [Ang92, KV94] </ref>. The formal definition is given below. <p> This is called active learning, or learning with queries; for a good account of actively learning DFAs, see <ref> [KV94] </ref>. <p> For example, assuming RP 6= N P , the class of 3-term DNF formulae is not efficiently PAC learnable unless we use 3-CNF to represent our hypotheses (see <ref> [KV94] </ref>). 13 However, Kearns et al. [KMR + 94] prove that, under a certain assumption 14 , it is not possible to efficiently PAC learn PFAs using any kind of hypothesis, when the hypothesis must be an evaluator.
Reference: [KVBSV94] <author> T. Kam, T. Villa, R. K. Brayton, and A. Sangiovanni-Vincentelli. </author> <title> A fully implicit algorithm for exact state minimization. </title> <booktitle> In Proc. Design Automation Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Indeed, minimizing an incompletely specified FSM is NP-complete [Pfl73]. Nevertheless, it is an important problem in the synthesis of sequential logic circuits, so fast heuristic algorithms have been developed. There are basically two kinds, based on an explicit [HRSJ91] or implicit <ref> [KVBSV94] </ref> enumeration of the compatibles. (A compatible is a set of states equivalent in the sense that they can be merged without affecting the behavior of the machine.
Reference: [Lan92] <author> K. J. Lang. </author> <title> Random DFAs can be approximately learned from sparse uniform examples. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, 1992. </booktitle> <volume> Volume 5. </volume>
Reference-contexts: More precisely, E [d] = log jj log jj n and E [a] = C log jj n, where C is a constant. Lang <ref> [Lan92] </ref> found experimentally (for = = f0; 1g) that for a large fraction of randomly generated DFAs (with several thousand states), a 2 log 2 n 2 and d 4. <p> Contrary to Angluin's result [Ang78], he found that it was possible to (approximately) learn these random DFAs quite well even if the training set only contained about 3% of the uniform complete sample. 3.4.3 Learning random DFAs from sparse samples: the Greedy Russian algorithm We briefly present Lang's algorithm <ref> [Lan92] </ref>, which is a variation of the Russian algorithm. Essentially he combines the two phases of looking for states and then computing the transitions into one single pass over the tree.
Reference: [Li90] <author> W. Li. </author> <title> A relation between complexity and entropy for markov chains and regular languages. </title> <type> Technical Report 90-025, </type> <institution> Santa Fe Institute, </institution> <year> 1990. </year> <note> Submitted to J. </note> <institution> Physics A. </institution>
Reference-contexts: As an example of this, Crutchfield and Young [CY89, You91] propose learning a PFA from a series of measurements taken from a non-linear dynamical system, and using the "complexity" of the resulting PFA as a measure of the complexity of the original system. (See also <ref> [Li90] </ref>.) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem of learning probabilistic models which are more powerful than PFAs, such as belief (Bayesian) networks [Pea88, HGC94] or stochastic Context Free Grammars [JLM92], but learning PFAs will prove to be hard enough.
Reference: [MB91] <author> M. C. Mozer and J. Bachrach. SLUG: </author> <title> A connectionist architecture for inferring the structure of finite-state environments. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 139-160, </pages> <year> 1991. </year>
Reference-contexts: Finally, we can imagine the problem of a robot trying to learn the structure of its environment, which can be modelled as a DFA; see <ref> [RS87, RS89, MB91] </ref>. The applications of PFAs, of which Hidden Markov Models (HMMs) are a special case, are much more extensive.
Reference: [Mic80] <author> L. Miclet. </author> <title> Regular inference with a tail-clustering method. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-10(11):737-743, </volume> <year> 1980. </year>
Reference-contexts: Since we can always reconstruct a DFA given its set of k-tails (for k n 2), in some sense each state is uniquely identified by its k-tail. Hence, if two k-tails are similar (in the same cluster), their corresponding states should be merged. Miclet <ref> [Mic80] </ref> calls this the k-tails clustering algorithm. (Note that clusters are not the same as blocks in a partition, since we are not claiming that two states in the same cluster are equivalent, just that they have similar k-tails.) We have to specify how the cluster will change when we add
Reference: [Mic90] <author> L. Miclet. </author> <title> Grammatical inference. </title> <editor> In H. Bunke and A. Sanfeliu, editors, </editor> <title> Syntactic and Structural Pattern Recognition. Theory and Applications, chapter 9. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: We concentrate on recent results which are not mentioned in the 1982 survey on inductive inference by Angluin and Smith and the 1990 survey on grammatical inference by Miclet <ref> [Mic90] </ref>. (See also the book by Fu [Fu82].) 1.1 Applications of automaton inference Most of the work on automaton inference (especially the early work) is theoretical in nature: finite automata are simple, so we can characterize more easily how hard it is to learn them, both in terms of how large <p> Another method uses fuzzy logic | see <ref> [Mic90] </ref> for a few references. 10 3 Learning DFAs 3.1 Definition of success Our goal will be to find the smallest automaton which is compatible with the input. In the limit of infinite data, this will be equivalent to the target concept we are trying to learn. <p> Since each pair (p; q) is considered at most jj times (it could be on up to jj lists), the running time is O (jjn 2 ). As Miclet <ref> [Mic90] </ref> points out, the k-tails algorithm of Biermann and Feldman [BF72] can be thought of as running Moore's algorithm but terminating at a particular value of k.
Reference: [Min67] <author> M. L. Minsky. </author> <title> Computation: Finite and Infinite Machines. </title> <publisher> Prentice Hall, </publisher> <year> 1967. </year>
Reference-contexts: whereas Ron and Rubinfeld [RR95] do not; however, their algorithm only works for DFAs with small cover time 10 , which intuitively means that all the states in the DFA are easy to get to (Angluin's proof relies on states which are hard to reach). 3.5.2 Neural network methods Minsky <ref> [Min67] </ref> proved that "every FSM is equivalent to, and can be simulated by, some [recurrent] neural network". However, this does not imply that it is easy to learn the structure of an unknown FSM using a neural network (NN).
Reference: [OE95] <author> A. Oliveira and S. Edwards. </author> <title> Inference of state machines from examples of behavior. </title> <type> Technical report, </type> <institution> Dept. of Electrical Engineering, </institution> <address> U. C. Berkeley, </address> <year> 1995. </year> <note> Available from http://www.eecs.berkeley.edu/~sedwards </note>
Reference-contexts: However, finite automaton inference also has several "real world" applications. In the electrical engineering community, there is considerable interest in synthesising Finite State Machines (FSMs are DFAs with output) from a partial specification of their behavior; see for example <ref> [OE95] </ref> and the references therein. DFAs have also been proposed as a model of players (in a game-theoretic sense) with bounded rationality; learning to play optimally against such players is very similar to the inference problem defined above; see for example [FKM + 95] and references therein. <p> Experimental results <ref> [OE95] </ref> verify that the top-down approach is faster in practice. However, the 6 exponential in t behavior arises because (1) the input is incomplete, and (2) we insist on an exact answer. Removing either of these conditions will yield a much faster (polynomial time) solution, as we will see. <p> Indeed, they show that they can implement their algorithm in a neural network. (We will briefly discuss other neural network methods later.) 3.3.2 Exactly learning FSMs from an incomplete specification Oliveira and Edwards <ref> [OE95] </ref> deal with the more general case of when the input consists of an arbitrary set of consistent input/output pairs. This case arises in practice when synthesising FSMs from a partial behavioral specification. <p> The number of compatibles may be exponential in the number of states.) In practice, however, it seems that top-down strategies work better than minimization strategies, at least for synthesising FSMs designed by human engineers, which may be highly structured <ref> [OE95] </ref>.
Reference: [Paz71] <author> A. Paz. </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: We shall define these more precisely, and then show how PFAs relate to Markov Chains and Hidden Markov Models (HMMs). DFAs are discussed more fully in [AU72, HU79] and PFAs in <ref> [Paz71] </ref>; for a good tutorial article on HMMs, see [Rab89]. 2.1 Deterministic Finite Automata Definition 1 A Deterministic Finite Automaton (DFA) is a tuple M = (; Q; q 0 ; F; ffi), where is the input alphabet, Q is the set of states, q 0 is the initial or start
Reference: [Pea88] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: system, and using the "complexity" of the resulting PFA as a measure of the complexity of the original system. (See also [Li90].) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem of learning probabilistic models which are more powerful than PFAs, such as belief (Bayesian) networks <ref> [Pea88, HGC94] </ref> or stochastic Context Free Grammars [JLM92], but learning PFAs will prove to be hard enough. In fact, the problem is so hard (i.e., time-consuming) that we will mostly concentrate on a special case, which we call "deterministic" PFAs (DPFAs).
Reference: [PF91] <author> S. Porat and J. A. Feldman. </author> <title> Learning automata from ordered examples. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 109-138, </pages> <year> 1991. </year>
Reference-contexts: Dotted lines denote mutable links (which may be subsequently deleted), solid lines denote permanent links. l denotes , the empty string. 0 is an accept state, 1 is a reject state. 3.3 Top-down algorithms 3.3.1 Exactly learning DFAs using an ordered complete presentation Porat and Feldman <ref> [PF91] </ref> give an online algorithm for incrementally learning DFAs, i.e., at each step, they output a DFA which is the best guess so far (in the sense of being the smallest model consistent with the data seen so far), based only on the previous guess and the current input string.
Reference: [Pfl73] <author> C. E. Pfleeger. </author> <title> State reduction in incompletely specified finite state machines. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-22:1099-1102, </volume> <year> 1973. </year>
Reference-contexts: which is exponential in n, the number of states in the final minimal automaton. (This algorithm is similar to the one by Biermann et al. [BK76]). 5 Exponential time may be necessary because we may need to consider all possible subsets of nodes, to find which ones should be merged. <ref> [Pfl73] </ref> proved the general problem of minimizing an FSM from a partial specification is NP-complete. 12 In more detail, the algorithm works as follows. It tries to build a deterministic Moore or Mealy machine with no more than n states which is compatible with the input seen so far. <p> Intuitively we must consider all combinations of ways of labelling these "don't know" nodes, which leads to exponential time behavior. Indeed, minimizing an incompletely specified FSM is NP-complete <ref> [Pfl73] </ref>. Nevertheless, it is an important problem in the synthesis of sequential logic circuits, so fast heuristic algorithms have been developed.
Reference: [PW93] <author> L. Pitt and M. K. Warmuth. </author> <title> The minimum consistent dfa cannot be approximated within any polynomial. </title> <journal> J. of the ACM, </journal> <volume> 40(1) </volume> <pages> 95-142, </pages> <year> 1993. </year>
Reference-contexts: Even learning near-minimal DFAs, or PAC-learning DFAs, is hard: Pitt showed that learning a near-minimal DFA (i.e., one with n k states, where n is the minimal number of states, for some fixed k) is NP-hard <ref> [PW93] </ref>, and Kearns and Valiant [KV89] showed that PAC-learning a DFA with any reasonable hypothesis class is as hard as breaking various cryptographic protocols which are based on factoring.
Reference: [Rab63] <author> M. O. Rabin. </author> <title> Probabilistic automata. </title> <journal> Information and Control, </journal> <volume> 6(3) </volume> <pages> 230-245, </pages> <year> 1963. </year>
Reference-contexts: However, in view of the fact that DFAs and NFAs are equivalent in power (i.e., accept the same set of languages), we often use the phrase "deterministic finite automaton" to mean "non-probabilistic finite automaton", which covers both DFAs and NFAs. 8 The original definition of a PFA, due to Rabin <ref> [Rab63] </ref>, was in fact as an input-NPFA, and stems from the tradition of viewing automata as acceptors of strings. In Rabin's formulation, a string is said to be accepted if the probability of its being accepted exceeds some threshold 2 [0; 1).
Reference: [Rab89] <author> L. R. Rabiner. </author> <title> A tutorial in hidden markov models and selected applications in speech recognition. </title> <journal> Proc. of the IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: The applications of PFAs, of which Hidden Markov Models (HMMs) are a special case, are much more extensive. HMMs have been very widely used in the speech and handwriting recognition communities (see e.g., <ref> [Rab89, RST95] </ref>), and have also recently been applied to recognizing patterns in biological sequences such as DNA and proteins (see e.g., [KMH93, KBM + 94, BCHM94]). <p> We shall define these more precisely, and then show how PFAs relate to Markov Chains and Hidden Markov Models (HMMs). DFAs are discussed more fully in [AU72, HU79] and PFAs in [Paz71]; for a good tutorial article on HMMs, see <ref> [Rab89] </ref>. 2.1 Deterministic Finite Automata Definition 1 A Deterministic Finite Automaton (DFA) is a tuple M = (; Q; q 0 ; F; ffi), where is the input alphabet, Q is the set of states, q 0 is the initial or start state, F Q is the set of final states, <p> However, we can use the following dynamic programming algorithm, called the forward-backward algorithm <ref> [Rab89] </ref>, to compute it in O (mn 2 ) time. We define ff t (i) as the probability of a walk from the start state to state i labelled by x 1 : : : x t : ff t (i) def We can compute this inductively as follows. <p> The advantage of this formulation (besides removing the independence assumption) is that one can have silent arcs, that is, arcs that emit , the empty symbol. This can be useful for "skipping over" parts of a model, or for "looping back"; see <ref> [Rab89] </ref> for some examples in the context of speech recognition.
Reference: [RR95] <author> D. Ron and R. Rubinfeld. </author> <title> On the exact learnability of automata with small cover time. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Rivest and Schapire need an equivalence oracle for their homing sequence method, whereas Ron and Rubinfeld <ref> [RR95] </ref> do not; however, their algorithm only works for DFAs with small cover time 10 , which intuitively means that all the states in the DFA are easy to get to (Angluin's proof relies on states which are hard to reach). 3.5.2 Neural network methods Minsky [Min67] proved that "every FSM
Reference: [RS87] <author> R. L. Rivest and R. E. Schapire. </author> <title> Diversity based inference of finite automata. </title> <booktitle> In IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 78-87, </pages> <year> 1987. </year>
Reference-contexts: Finally, we can imagine the problem of a robot trying to learn the structure of its environment, which can be modelled as a DFA; see <ref> [RS87, RS89, MB91] </ref>. The applications of PFAs, of which Hidden Markov Models (HMMs) are a special case, are much more extensive.
Reference: [RS89] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 411-420, </pages> <year> 1989. </year>
Reference-contexts: Finally, we can imagine the problem of a robot trying to learn the structure of its environment, which can be modelled as a DFA; see <ref> [RS87, RS89, MB91] </ref>. The applications of PFAs, of which Hidden Markov Models (HMMs) are a special case, are much more extensive. <p> In the more general case of inferring the structure of an FSM from a series of experiments, see [Con71]. (See also Exercise 3.22 in [HU79].) 22 Rivest and Schapire <ref> [RS89] </ref> show how the DFA can still be learnt in this case, by using homing sequences. Intuitively, a homing sequence is an input sequence which, when executed, may allow the learner to determine "where it is" in the machine, based on the observed output sequence.
Reference: [RST93] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems, 1993. </booktitle> <volume> Volume 6. </volume>
Reference-contexts: We now consider a subclass of DPFAs, called Probabilistic Suffix Automata (PSAs), and show how they can be used to succinctly model variable order Markov chains <ref> [RST93, RST94] </ref>. In a PSA, every state is labelled by a finite length string in fl . If every label is at most length L, we call it an L-PSA.
Reference: [RST94] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, </booktitle> <year> 1994. </year>
Reference-contexts: chain in which every state has order exactly n; such a model has O (jj n ) states (where is the alphabet), and hence requires an exponential amount of data to learn the transition probabilities. (A Markov chain of order n is also called an n-gram model.) Experimental results in <ref> [RST94] </ref> show it is possible to learn a DPFA that performs well on a task which involves correcting errors in text, but it is not yet clear if this method is better than other sparse n-gram methods such as hash tables. <p> However, the 6 exponential in t behavior arises because (1) the input is incomplete, and (2) we insist on an exact answer. Removing either of these conditions will yield a much faster (polynomial time) solution, as we will see. Ron, Singer and Tishby <ref> [RST94] </ref> present a top-down algorithm for learning a PSA (a Probabilistic Suffix Automaton, which can be thought of as a variable-order Markov chain). <p> We now consider a subclass of DPFAs, called Probabilistic Suffix Automata (PSAs), and show how they can be used to succinctly model variable order Markov chains <ref> [RST93, RST94] </ref>. In a PSA, every state is labelled by a finite length string in fl . If every label is at most length L, we call it an L-PSA. <p> The definition of acceptable precision is captured in the following definition, which comes originally from [KMR + 94], and which has been used subsequently in <ref> [RST94, RST95] </ref>. It is inspired by the definition of PAC-learning a concept, given earlier. Definition 4 Let M be the target PFA we are trying to learn, and ^ M be a hypothesis PFA. <p> algorithms, which generate every possible model in turn, will obviously find the simplest model, but since the number of possible models grows exponentially in the number of states, these algorithms are too inefficient to be useful. "Top down" algorithms, which only add states when forced to by the data (e.g., <ref> [RST94] </ref>), will also often find the simplest model (but not always, since they may not be able to delete states in the light of new data). <p> Ron, Singer and Tishby <ref> [RST94] </ref> show how to efficiently PAC-learn a PSA using a Probabilistic Suffix Tree (PST) as the hypothesis representation. 16 We shall just give a brief sketch of their algorithm. 17 Start with the empty tree and add a new node v with label s when given a string s only if, <p> They have used APFAs to segment cursive handwriting into characters, and PSAs to correct corrupted strings of characters, and combined the two in a complete handwriting recognition system. 4.5.1 When the input is a single string Another interesting aspect of <ref> [RST94] </ref> algorithm is that it can take as input a single long string of length m. <p> A probabilistic suffix tree is a suffix tree with probabilities attached to each branch. This is similar to a DPFA. 17 For an algorithm for learning a "normal" Markov chain in the limit with probability 1, see [Rud85]. 18 In <ref> [RST94] </ref> they say that it is also necessary that the machine be aperiodic, i.e., that the greatest common divisor of the lengths of the cycles in the underlying graph is 1. 30 Let R M be the state transition matrix, and ~ R M the time reversal of R M ,
Reference: [RST95] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> On the learnability and usage of acyclic probabilistic finite automata. </title> <booktitle> In Proc. of the Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: The applications of PFAs, of which Hidden Markov Models (HMMs) are a special case, are much more extensive. HMMs have been very widely used in the speech and handwriting recognition communities (see e.g., <ref> [Rab89, RST95] </ref>), and have also recently been applied to recognizing patterns in biological sequences such as DNA and proteins (see e.g., [KMH93, KBM + 94, BCHM94]). <p> In fact, the problem is so hard (i.e., time-consuming) that we will mostly concentrate on a special case, which we call "deterministic" PFAs (DPFAs). These are less powerful than HMMs, and hence easier to learn. In fact, experimental results in <ref> [RST95] </ref> show that it is 10-100 times faster to learn a pronunciation model for spoken words using a (certain kind of) DPFA than it is to learn a corresponding HMM, and yet the performance of the DPFA is actually slightly better. <p> Many batch algorithms can be converted to on-line algorithms by alternating between "growing" and "shrinking" phases: in the growth stage, new strings are added to the current model, and whenever it becomes too big, the state merging algorithm is invoked (see <ref> [RST95, SO94] </ref> for examples). 1.5 Bottom-up vs. top-down algorithms Most of the batch algorithms we study work in roughly the same way, namely: construct some canonical, tree-shaped automaton which represents the input set (see Figure 2 for an example), and then merge equivalent states to get a smaller model. (We will <p> The definition of acceptable precision is captured in the following definition, which comes originally from [KMR + 94], and which has been used subsequently in <ref> [RST94, RST95] </ref>. It is inspired by the definition of PAC-learning a concept, given earlier. Definition 4 Let M be the target PFA we are trying to learn, and ^ M be a hypothesis PFA. <p> sum is 2 (1 2), and assuming is bounded away from 1 2 , this is large. jjP Qjj 1 = s2 nl = 2 nl 2 (nl) ((1 2) + (1 2)) By imposing an extra condition on the class of Acyclic, Deterministic PFAs (APFAs), Ron, Singer and Tishby <ref> [RST95] </ref> prove that it is possible to efficiently learn this class using an algorithm we will study later. (They also conjecture that it is possible to learn cyclic DPFAs (with the distinguishability property) using their algorithm, but were unable to prove this case. 15 ) The extra condition they impose is <p> C is the result of merging all the children of 1 and 2. Solid edges are transitions labelled by 0, dashed egdes (which always lead into leaves) are labelled by the final symbol, and dotted edges are labelled by 1. Adapted from Figure 1 of <ref> [RST95] </ref>. they induce on the states, i.e., the probability that the automaton ends up in each state after accepting the string. He shows this problem is NP-complete (see also [GS94]). <p> This algorithm can therefore learn arbitrary DPFAs with probability 1 in the limit, from positive examples only. 4.4 The APFA algorithm Ron, Singer and Tishby <ref> [RST95] </ref> present an algorithm which is very similar to the Spanish algorithm, in that it merges states based on a probabilistic similarity criterion. The difference is that they can characterize precisely how well the algorithm will perform on finite data sets.
Reference: [Rud85] <author> S. Rudich. </author> <title> Inferring the structure of a markov chain from its output. </title> <booktitle> In IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1985. </year>
Reference-contexts: A probabilistic suffix tree is a suffix tree with probabilities attached to each branch. This is similar to a DPFA. 17 For an algorithm for learning a "normal" Markov chain in the limit with probability 1, see <ref> [Rud85] </ref>. 18 In [RST94] they say that it is also necessary that the machine be aperiodic, i.e., that the greatest common divisor of the lengths of the cycles in the underlying graph is 1. 30 Let R M be the state transition matrix, and ~ R M the time reversal of
Reference: [Seb84] <author> G. A. F. Seber. </author> <title> Multivariate Observations. </title> <publisher> John Wiley, </publisher> <year> 1984. </year>
Reference-contexts: If this is not the case | the so-called no-reset model | learning becomes much harder, because the learner starts in an unknown state and ends up in an unknown state. However, 8 For a good review of clustering algorithms, see <ref> [Seb84, ch. 7] </ref> 9 Asking whether a string is in the language is like performing an experiment in which the string is "fed into" the black box, and we observe whether it is accepted or rejected.
Reference: [SO92] <author> A. Stolcke and S. M. Omohundro. </author> <title> Hidden markov model induction by bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems, 1992. </booktitle> <volume> Volume 5. </volume>
Reference-contexts: Carrasco and Oncina [CO94] show that in their algorithm, the type 2 error rate vanishes in the limit of infinite data, and hence they can achieve identification in the limit with probability 1. Stolcke and Omohundro <ref> [SO92, SO94] </ref> adopt a Bayesian approach in their bottom-up algorithm, and attempt to learn the HMM which has the greatest posterior probability, i.e., which maximises Pr (M jx). (This is called the Maximum A Posteriori (MAP) model.) Bayes' theorem tells us Pr (M jx) = P r (x) so if we <p> interested in fully automatic means of inferring the correct topology. (It would be interesting to consider the possibility of using the handcrafted topology as a prior to an automatic Bayesian method.) 32 5.4 Using a Bayesian state-merging method The most well-principled approach to learning HMMs is by Stolcke and Omohundro <ref> [SO92, SO94] </ref>. As in many of the methods for learning PFAs, they repeatedly merge states in the prefix tree (i.e., the maximum likelihood HMM), but they no longer recursively merge the children of two merged nodes, and hence the resulting machine becomes non-deterministic.
Reference: [SO94] <author> A. Stolcke and S. M. Omohundro. </author> <title> Best-first model merging for hidden markov model induction. </title> <type> Technical Report TR-94-003, </type> <institution> International Computer Science Institute, </institution> <year> 1994. </year> <note> Available from http://www.icsi.berkeley.edu. </note>
Reference-contexts: Many batch algorithms can be converted to on-line algorithms by alternating between "growing" and "shrinking" phases: in the growth stage, new strings are added to the current model, and whenever it becomes too big, the state merging algorithm is invoked (see <ref> [RST95, SO94] </ref> for examples). 1.5 Bottom-up vs. top-down algorithms Most of the batch algorithms we study work in roughly the same way, namely: construct some canonical, tree-shaped automaton which represents the input set (see Figure 2 for an example), and then merge equivalent states to get a smaller model. (We will <p> They claim that their top-down scheme is equivalent to a bottom-up scheme, but that the top-down scheme is "somewhat more intuitive, simpler to implement, [and] more easily adapted to an online setting". However, Stolcke and Omohundro <ref> [SO94] </ref> say that, "our experience has been that modelling approaches based on splitting tend to fit the structure of a domain less well than those based on merging". <p> Carrasco and Oncina [CO94] show that in their algorithm, the type 2 error rate vanishes in the limit of infinite data, and hence they can achieve identification in the limit with probability 1. Stolcke and Omohundro <ref> [SO92, SO94] </ref> adopt a Bayesian approach in their bottom-up algorithm, and attempt to learn the HMM which has the greatest posterior probability, i.e., which maximises Pr (M jx). (This is called the Maximum A Posteriori (MAP) model.) Bayes' theorem tells us Pr (M jx) = P r (x) so if we <p> interested in fully automatic means of inferring the correct topology. (It would be interesting to consider the possibility of using the handcrafted topology as a prior to an automatic Bayesian method.) 32 5.4 Using a Bayesian state-merging method The most well-principled approach to learning HMMs is by Stolcke and Omohundro <ref> [SO92, SO94] </ref>. As in many of the methods for learning PFAs, they repeatedly merge states in the prefix tree (i.e., the maximum likelihood HMM), but they no longer recursively merge the children of two merged nodes, and hence the resulting machine becomes non-deterministic.
Reference: [TB73] <author> B. A. Trakhtenbrot and Ya. M. Barzdin'. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: it is possible to reconstruct the original machine, as we will see next. (Of course, the set of all strings of length at most 2n 1 is exponentially large!) 3.4.1 Learning FSMs from a uniform complete sample: the Russian algorithm We now describe an algorithm due to Trakhtenbrot and Barzdin' <ref> [TB73] </ref> which forms the basis of much of our subsequent discussion. We will call this the "Russian algorithm" for convenience. The input is a complete labelled prefix tree of height 2n 1. The algorithm has two phases. <p> All the other nodes are labelled with the states they are indistinguishable from, because they have the same subtrees (where defined). A and B are used to identify nodes discussed in the text. From Figure 15 of <ref> [TB73] </ref>. is q 1 .
Reference: [TS94] <author> J. Takami and S. Sagayama. </author> <title> Automatic generation of hidden markov networks by a successive state splitting algorithm. </title> <journal> Systems and Computers in Japan, </journal> <volume> 25(12), </volume> <year> 1994. </year> <title> Translated from Japanese. </title>
Reference-contexts: Since the Baum-Welch algorithm gets stuck in local optima, they repeat the whole process from different random starting points. A similar approach, called the "Successive State Splitting" Algorithm, is described in <ref> [TS94] </ref> in the context of left-right speech recognition models. 5.3 Model surgery Another approach is to construct a topology by hand, and then "fine tune" it after it has been trained.
Reference: [Tze89] <author> W-G. Tzeng. </author> <title> The equivalence and learning of probabilistic automata. </title> <booktitle> In IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 268-273, </pages> <year> 1989. </year>
Reference-contexts: Note that the distributions between any pair of states in the noisy parity PFA are far apart in the L 1 sense, but not in the L 1 sense, and hence are not -distinguishable. Finally, we mention a result on the complexity of learning PFAs with oracles. Tzeng <ref> [Tze89] </ref> discusses how to learn an input-PFA when the input consists of a set of strings and the corresponding probability distributions 15 Dana Ron, personal communication, 8/15/95. 27 nodes 1 and 2, and corresponds to a non-deterministic automaton.
Reference: [You91] <author> K. Young. </author> <title> The Grammar and Statistical Mechanics of Complex Physical Systems. </title> <type> PhD thesis, </type> <institution> Physics Dept., U.C. Santa Cruz, </institution> <year> 1991. </year> <month> 37 </month>
Reference-contexts: As an example of this, Crutchfield and Young <ref> [CY89, You91] </ref> propose learning a PFA from a series of measurements taken from a non-linear dynamical system, and using the "complexity" of the resulting PFA as a measure of the complexity of the original system. (See also [Li90].) 1.2 Why PFAs instead of other probabilistic models? We could consider the problem
References-found: 69

