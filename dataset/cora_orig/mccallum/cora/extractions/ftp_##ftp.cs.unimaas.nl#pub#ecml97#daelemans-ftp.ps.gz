URL: ftp://ftp.cs.unimaas.nl/pub/ecml97/daelemans-ftp.ps.gz
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: 
Phone: 2  
Title: Empirical Learning of Natural Language Processing Tasks  
Author: Walter Daelemans Antal van den Bosch Ton Weijters 
Address: Netherlands;  Netherlands  
Affiliation: 1 Computational Linguistics, Tilburg University, The  Dept. of Computer Science MATRIKS, Universiteit Maastricht, The  
Note: Pages 1 to 10 of W. Daelemans, A. van den Bosch, and A. Weijters (Editors), Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, April 26, 1997, Prague, Czech Republic  
Abstract: Language learning has thus far not been a hot application for machine-learning (ML) research. This limited attention for work on empirical learning of language knowledge and behaviour from text and speech data seems unjustified. After all, it is becoming apparent that empirical learning of Natural Language Processing (NLP) can alleviate NLP's all-time main problem, viz. the knowledge acquisition bottleneck: empirical ML methods such as rule induction, top down induction of decision trees, lazy learning, inductive logic programming, and some types of neural network learning, seem to be excellently suited to automatically induce exactly that knowledge that is hard to gather by hand. In this paper we address the question why NLP is an interesting application for empirical ML, and provide a brief overview of current work in this area.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., and Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 37-66. </pages>
Reference: <author> Andernach, T. </author> <title> (1996) A machine learning approach to the classification of dialogue utterances. </title> <editor> In K. Oflazer and H. Somers (Eds.), </editor> <booktitle> Proceedings of the Second International Conference on New Methods in Language Processing, NeMLaP, </booktitle> <pages> pp. 98-109. </pages>
Reference-contexts: The features are the tags of the words preceding the word to be tagged. Schmid reports robustness relative to training set size: treetagger `degrades gracefully' with smaller training set sizes. An example application of rule induction to the semantic domain is the classification of dialogue acts <ref> (Andernach, 1996) </ref>. In this work, rule induction is employed to automatically generate and test theories on what are useful cues in texts for classifying them as dialogue acts. The output of rule induction offers interesting alternative insights to what existing theories consider relevant (Andernach, 1996). <p> domain is the classification of dialogue acts <ref> (Andernach, 1996) </ref>. In this work, rule induction is employed to automatically generate and test theories on what are useful cues in texts for classifying them as dialogue acts. The output of rule induction offers interesting alternative insights to what existing theories consider relevant (Andernach, 1996). The use of rule induction to find heuristics for disambiguating between discourse use and sentential use of cue phrases in text was investigated by Litman (1996).
Reference: <author> Black, E., Jelinek, F., Lafferty, J, Mercer, R., and Roukos, S. </author> <year> (1992). </year> <title> Decision tree models applied to the labeling of text with parts-of-speech Darpa Workshop on Speech and Natural Language. </title>
Reference-contexts: The goal of rule induction (e.g., C4.5rules, Quinlan, 1993; CN2, Clark and Boswell, 1991) is, more than it is with decision-tree learning, to induce limited sets of interpretable rules from examples or decision trees. Work on parsing (including tagging) of text with decision trees was pioneered at IBM <ref> (Black et al., 1992, Magerman, 1995) </ref>. spatter (Magerman, 1995) starts from the premise that a parse tree can be viewed as the result of a series of classification problems. The most probable sequence of decisions for a sentence, given a training corpus, is its most probable analysis.
Reference: <author> Cardie, C. </author> <year> (1994). </year> <title> Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis. </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Cardie, C. </author> <year> (1996). </year> <title> Embedded Machine Learning Systems for Natural Language Processing: A General Framework. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler, (Eds.), </editor> <booktitle> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp. 315-328. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Clark, P., and Boswell, R. </author> <year> (1991). </year> <title> Rule induction with cn2: Some recent improvements. </title> <booktitle> In Machine Learning: Proceedings of the Fifth European Conference, </booktitle> <pages> pp. 151-163. </pages>
Reference: <author> Cussens, J. </author> <year> (1996). </year> <title> Part-of-speech disambiguation using ILP. </title> <type> Technical report PRG-TR-25-96, </type> <institution> Oxford University Computing Laboratory. </institution>
Reference-contexts: in which the relatively novel method of ILP is used for NLP tasks, the results are impressive: the rich representation language and the use of background knowledge in ILP enables the learning of complex NLP tasks such as (semantic) parsing (Zelle and Mooney, 1994; Muggleton et al. 1996), and tagging <ref> (Cussens, 1996) </ref>. Dehaspe et al. (1996) uses ILP for small-scale linguistic tasks: grammaticality checking and Dutch diminutive forming. 6 Conclusion Some general trends become clear when analysing the results of these studies.
Reference: <author> Daelemans, W. </author> <year> (1995). </year> <title> Memory-based lexical acquisition and processing. </title> <editor> In P. Stef-fens (Ed.), </editor> <booktitle> Machine Translation and the Lexicon, </booktitle> <pages> pp. 85-98. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Cf. http://www.cs.unimaas.nl/signll/signll-www.html for more links to home pages of corpora. context. All linguistic problems can be described as a mapping of one of two types of classification <ref> (Daelemans, 1995) </ref>: * Disambiguation. Given a set of possible categories and a relevant context in terms of attribute values, determine the correct category for this context. An example from text-to-speech conversion: given a letter in its context (a word), determine its pronunciation.
Reference: <author> Daelemans, W. </author> <year> (1996). </year> <title> Abstraction considered harmful: Lazy learning of language processing. </title> <editor> In H. J. van den Herik and A. Weijters (Eds.), </editor> <booktitle> Proceedings of the 6th Belgian-Dutch Conference on Machine Learning, Maastricht, The Netherlands, </booktitle> <pages> pp. 3-12. </pages>
Reference-contexts: An example application of rule induction to a morpho-phonological task is the application of C4.5rules to Dutch diminutive formation <ref> (Daelemans et al., 1996) </ref>. 4 Artificial Neural Networks During the last decade, the study of connectionist models or Artificial Neural Networks (ANNs), has also led to applications in the NLP domain. <p> This proves the point that ML techniques may help considerably in solving knowledge acquisition bottlenecks in NLP. Second, it depends on the goal of the system whether lazy learning or greedy learning algorithms at an advantage. If the goal is optimal accuracy, lazy learning is preferable <ref> (Daelemans, 1996) </ref>. We find that simple lazy-learning algorithms, extended with feature weighting and probabilistic decision rules, consistently obtain the best generalisation accuracy on a large collection of linguistic tasks (e.g., within the morpho-phonological domain, Van den Bosch, forthcoming).
Reference: <author> Daelemans, W., Berck, P, and Gillis, S. </author> <year> (1996). </year> <title> Unsupervised discovery of phonological categories through supervised learning of morphological rules. </title> <booktitle> In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, Denmark, </booktitle> <pages> pp. 95-100. </pages>
Reference-contexts: An example application of rule induction to a morpho-phonological task is the application of C4.5rules to Dutch diminutive formation <ref> (Daelemans et al., 1996) </ref>. 4 Artificial Neural Networks During the last decade, the study of connectionist models or Artificial Neural Networks (ANNs), has also led to applications in the NLP domain. <p> This proves the point that ML techniques may help considerably in solving knowledge acquisition bottlenecks in NLP. Second, it depends on the goal of the system whether lazy learning or greedy learning algorithms at an advantage. If the goal is optimal accuracy, lazy learning is preferable <ref> (Daelemans, 1996) </ref>. We find that simple lazy-learning algorithms, extended with feature weighting and probabilistic decision rules, consistently obtain the best generalisation accuracy on a large collection of linguistic tasks (e.g., within the morpho-phonological domain, Van den Bosch, forthcoming).
Reference: <author> Daelemans, W., Van den Bosch, A., and Weijters, A. </author> <note> (to appear). </note> <author> igtree: </author> <title> using trees for classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <note> special issue on Lazy Learning. To appear. </note>
Reference: <author> Dehaspe, L., Blockeel, H., and De Raedt, L. </author> <year> (1996). </year> <title> Induction, </title> <booktitle> logic and natural language processing. In Proceedings of the Joint elsnet/compulog-net/eagles Workshop on Computational Logic for Natural Language Processing, </booktitle> <address> South Queensferry, Scotland. </address>
Reference: <author> Dietterich, T. G., Hild, H., and Bakiri, G. </author> <year> (1990). </year> <title> A comparison of id3 and Backpropagation for English text-to-speech mapping. </title> <type> Technical Report 90-20-4, </type> <institution> Oregon State University. </institution>
Reference: <author> Jones, D. </author> <title> Analogical Natural Language Processing. </title> <publisher> London: UCL Press, </publisher> <year> 1996. </year>
Reference-contexts: Ng and Lee (1996) report results superior to previous statistical methods when applying a lazy learning method to word sense disambiguation. The exemplar-based reasoning aspects of lazy learning are also prominent in the large literature on example-based machine translation <ref> (see Jones, 1996, for an overview) </ref>. 3 Decision-Tree Learning and Rule Induction The decision-tree learning paradigm is based on the assumption that similarities between examples can be used to automatically extract decision-trees and categories with explanatory and generalisation power.
Reference: <author> Kolodner, J. </author> <year> (1992). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lavrac, N. and Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming. </title> <address> Chichester, UK: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Lawrence, S., Fong, S., and Giles, C. </author> <title> Lee (1991) Natural language grammatical inference: A comparison of recurrent neural networks and machine learning methods. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler (Eds.), </editor> <title> Connectionist, Statistical, </title> <booktitle> and Symbolic Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp. 33-47. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Litman, D. J. </author> <year> (1996). </year> <title> Cue phrase classification using machine learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 5 </volume> <pages> 53-94, </pages> <year> 1996. </year>
Reference: <author> Magerman, D. </author> <year> (1995). </year> <title> Statistical decision tree models for parsing. </title> <booktitle> In Proceedings of the Association for Computational Linguistics., </booktitle> <year> 1995. </year>
Reference-contexts: Work on parsing (including tagging) of text with decision trees was pioneered at IBM (Black et al., 1992, Magerman, 1995). spatter <ref> (Magerman, 1995) </ref> starts from the premise that a parse tree can be viewed as the result of a series of classification problems. The most probable sequence of decisions for a sentence, given a training corpus, is its most probable analysis.
Reference: <author> Muggleton, S., and De Raedt, L. </author> <year> (1994). </year> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, 19,20:629-679. </journal>
Reference: <author> Muggleton, S., Page, D., and Srinivasan, A. </author> <year> (1996). </year> <title> Learning to read by theory revision. </title> <type> Technical Report PRG-TR-26-96, </type> <institution> Oxford University Computing Laboratory. </institution>
Reference: <author> Ng, H. T. and H. B. </author> <title> Lee (1996). Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. </title> <booktitle> In Proceedings of the annual meeting of the ACL, </booktitle> <address> ACL-96. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> c4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Reilley, R. G. and Sharkey, N. E., Eds. </author> <year> (1992). </year> <title> Connectionist Approaches to Natural Language Processing. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, </volume> <pages> pp. 318-362. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The functionality of trained ANNs nonetheless displays the same interesting properties as that of lazy learning and decision-tree learning: an ANN can represent abstractions as well as store specific input-output mappings. A commonly-used learning algorithm for supervised learning of classification tasks in ANNs is back-propagation (BP) <ref> (Rumelhart, Hinton, and Williams, 1986) </ref>. In the current development of applying ANNs in NLP, one finds a stress on the issue of representation in syntactic and semantic applications, and on generalisation in morpho-phonological applications.
Reference: <author> Schmid, H. </author> <year> (1994). </year> <title> Probabilistic Part-of-Speech Tagging Using Decision Trees. </title> <booktitle> Proceedings of the International Conference on New Methods in Language Processing, NeMLaP, Manchester, </booktitle> <pages> 44-49. </pages>
Reference: <author> Sejnowski T. J. and Rosenberg C. S. </author> <year> (1987). </year> <title> Parallel Networks That Learn to Pronounce English Text. </title> <journal> Complex Systems, </journal> <volume> Vol. 1, </volume> <pages> pp. 145-168. </pages>
Reference: <author> Van den Bosch, A., and Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> pp. 45-53. </pages>
Reference-contexts: The use of rule induction to find heuristics for disambiguating between discourse use and sentential use of cue phrases in text was investigated by Litman (1996). In the morpho-phonological domain the decision-tree learning algorithm IGTree (Daelemans et al., to appear) has been applied successfully to grapheme-phoneme conversion <ref> (Van den Bosch and Daelemans, 1993) </ref> and morphological analysis (Van den Bosch et al., 1996).
Reference: <author> Van den Bosch, A., Daelemans, W., and Weijters, A. </author> <year> (1996). </year> <title> Morphological analysis as classification: an inductive-learning approach In K. </title> <editor> Oflazer and H. Somers (Eds.), </editor> <booktitle> Proceedings of the Second International Conference on New Methods in Language Processing, NeMLaP, Ankara, Turkey, </booktitle> <pages> pp. 79-89. </pages>
Reference-contexts: In the morpho-phonological domain the decision-tree learning algorithm IGTree (Daelemans et al., to appear) has been applied successfully to grapheme-phoneme conversion (Van den Bosch and Daelemans, 1993) and morphological analysis <ref> (Van den Bosch et al., 1996) </ref>.
Reference: <author> Van den Bosch, A. </author> <title> (forthcoming). Learning to Pronounce Written Words: A Study in Inductive Language Learning. </title> <publisher> PhD-Thesis, Universiteit Maastricht. </publisher>
Reference: <author> Weijters, A. </author> <year> (1991). </year> <title> A simple look-up procedure superior to NETtalk? In Proceedings of the International Conference on Artificial Neural Networks, </title> <address> Espoo, Finland. </address>
Reference: <author> Wermter, S., Riloff, E., and Scheler, G. </author> <year> (1996). </year> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Wettschereck, D., Aha, D. W. & Mohri, T. </author> <year> (1996). </year> <title> A review and comparative valuation of feature weighting methods for lazy learning algorithms. </title> <type> Technical Report AIC-95-012. </type> <institution> Washington, DC: NRL Navy Center for Applied Research in AI. </institution>
Reference: <author> Wolters, M. </author> <year> (1996). </year> <title> A dual-route approach to grapheme-to-phoneme conversion. </title> <booktitle> To appear in Proceedings of the International Conference on Artificial Neural Networks 1996, </booktitle> <address> Bochum, Germany. </address>
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> (1994). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <pages> pp. 748-753. </pages>
References-found: 35

