URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P611.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: Compiler Blockability of Dense Matrix Factorizations  
Author: Steve Carr R. B. Lehoucq 
Date: August 29, 1996  
Abstract: The goal of the LAPACK project is to provide efficient and portable software for dense numerical linear algebra computations. By recasting many of the fundamental dense matrix computations in terms of calls to an efficient implementation of the BLAS (Basic Linear Algebra Subprograms), the LAPACK project has, in large part, achieved its goal. Unfortunately, the efficient implementation of the BLAS often results in machine-specific code that is not portable across multiple architectures without a significant loss in performance or a significant effort to re-optimize them. This paper examines whether most of the hand optimizations performed on matrix factorization codes are unnecessary because they can (and should) be performed by the compiler. We believe that it is better for the programmer to express algorithms in a machine-independent form and allow the compiler to handle the machine-dependent details. This gives the algorithms portability across architectures and removes the error-prone, expensive and tedious process of hand optimization. Although there currently exist no production compiler that can perform all the loop transformations discussed in this paper, a description of current research in compiler technology is provided that will prove beneficial to the numerical linear algebra community. We show that the Cholesky and LU factorizations may be optimized automatically by a compiler to be as efficient as the same hand-optimized version found in LAPACK. We also show that the QR factorization may be optimized by the compiler to perform comparably with the hand-optimized LAPACK version on modest matrix sizes. Our approach allows us to conclude that with the advent of the compiler optimizations discussed in this paper, matrix factorizations may be efficiently implemented in a machine-independent form. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: A dependence is carried by a loop if the references at the source and sink (beginning and end) of the dependence are on different iterations of the loop and the dependence is not carried by an outer loop <ref> [1] </ref>. In the loop below, there is a true dependence from A (I,J) to A (I-1,J) carried by the I-loop, a true dependence from A (I,J) to A (I,J-1) carried by the J-loop and an input dependence from A (I,J-1) to A (I-1,J) carried by the I-loop. <p> For superscalar architectures whose performance is bound by cache, outer loop unrolling on non-rectangular loops can be applied to the J- and I-loops to further improve performance [7, 8]. For vector architectures, a different loop optimization strategy may be more beneficial <ref> [1] </ref>. Many of the transformations that we have used to obtain the block version of LU factorization are well known in the compiler community and exist in many commercial compilers (e.g., HP, DEC and SGI). <p> Our results show that for modest-sized matrices on advanced microprocessors with a memory hierarchy, the compiler-derived variants are often superior. These matrix sizes are typical on workstations. We remark that a different set of optimizations is required to optimize the codes for vector machines <ref> [1] </ref>. However, our use of section analysis is still required to enable these transformations. Given that future machine designs are certain to have increasingly complex memory hierarchies, compilers will need to adopt increasingly sophisticated memory-management strategies so that programmers can remain free to concentrate on program logic.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> second edition, </note> <year> 1995. </year>
Reference-contexts: These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization. The motivation of the LAPACK <ref> [2] </ref> project was to recast the algorithms in the EISPACK [32] and LINPACK [15] software libraries with block ones. <p> The second result, which we discuss in this paper, reveals that it is possible to block matrix factorizations automatically. Our results show that the block algorithms derived by the compiler are competitive with those of LAPACK <ref> [2] </ref>. For modest sized matrices (on the order of 200 or less), the compiler-derived variants are often superior. We begin our presentation with a review of background material related to compiler optimiza 2 tion.
Reference: [3] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing, </booktitle> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Our compiler-optimized versions were obtained by hand using the algorithms in the literature. The reason that this process could not be fully automated is because of a current deficiency in the dependence analyzer of our tool <ref> [3, 5] </ref>.
Reference: [4] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: DO 10 I = 1,N A (I,J) = A (I-1,J) + A (I,J-1)10 To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [4, 19] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. As an example of section analysis consider the following loop. 3 DO 10 I = 1,N 10 A (J,I) = ...
Reference: [5] <author> S. Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Our compiler-optimized versions were obtained by hand using the algorithms in the literature. The reason that this process could not be fully automated is because of a current deficiency in the dependence analyzer of our tool <ref> [3, 5] </ref>.
Reference: [6] <author> S. Carr, C. Ding, and P. Sweany. </author> <title> Improving software pipelining with unroll-and-jam. </title> <booktitle> In Proceedings of the 29th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Maui, HI, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: However, software pipelining requires a lot of registers to be successful. In our code, we performed unroll-and-jam to improve cache performance. However, unroll-and-jam can significantly increase register pressure and cause software pipelining to fail <ref> [6] </ref>. On our version of LU decomposition, the HP compiler diagnostics reveal that software pipelining failed on the main computational loop due to high register pressure. Given that the hand-optimized version is highly software pipelined, the result would be a highly parallel hand-optimized loop and a not-as-parallel compiler-derived loop.
Reference: [7] <author> S. Carr and K. Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 114-124, </pages> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Therefore, interchanging the loops requires the KK-loop to iterate over a trapezoidal region with an upper bound of I-1 until I-1 &gt; K+KS-1 (see Wolfe, and Carr and Kennedy for more details on transforming non-rectangular loop nests <ref> [35, 7] </ref>). <p> The loop nest surrounding statement 10 is a matrix-matrix multiply that can be further optimized depending upon the architecture. For superscalar architectures whose performance is bound by cache, outer loop unrolling on non-rectangular loops can be applied to the J- and I-loops to further improve performance <ref> [7, 8] </ref>. For vector architectures, a different loop optimization strategy may be more beneficial [1]. Many of the transformations that we have used to obtain the block version of LU factorization are well known in the compiler community and exist in many commercial compilers (e.g., HP, DEC and SGI). <p> The derived algorithm depends upon the compiler for efficiency in contrast to the LAPACK algorithm that depends on hand optimization of the BLAS. Cd-QR can be obtained from the point algorithm for QR decomposition using array section analysis <ref> [7] </ref>. For reference, segments of the code for the point algorithm after strip mining of the outer loop are shown in Figure 5.
Reference: [8] <author> Steve Carr and Ken Kennedy. </author> <title> Improving the ratio of memory operations to floating-point operations in loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(6) </volume> <pages> 1768-1810, </pages> <year> 1994. </year>
Reference-contexts: However, we show an alternative block algorithm for QR that can be derived using the same compiler methods as those used for LU and Cholesky factorizations. This study has yielded two major results. The first, which is detailed in another paper <ref> [8] </ref>, reveals that the hand loop unrolling performed when optimizing the level 2 and 3 BLAS [11, 12] is often unnecessary. While the BLAS are useful, the hand optimization that is required to obtain good performance on a particular architecture may be left to the compiler. <p> The loop nest surrounding statement 10 is a matrix-matrix multiply that can be further optimized depending upon the architecture. For superscalar architectures whose performance is bound by cache, outer loop unrolling on non-rectangular loops can be applied to the J- and I-loops to further improve performance <ref> [7, 8] </ref>. For vector architectures, a different loop optimization strategy may be more beneficial [1]. Many of the transformations that we have used to obtain the block version of LU factorization are well known in the compiler community and exist in many commercial compilers (e.g., HP, DEC and SGI).
Reference: [9] <author> Stephanie Coleman and Kathryn S. McKinley. </author> <title> Tile size selection using cache organization. </title> <journal> SIGPLAN Notices, </journal> <volume> 30(6) </volume> <pages> 279-280, </pages> <month> June </month> <year> 1995. </year> <booktitle> Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: This transformation is used to create the block update of A. To apply this transformation, we first strip the K-loop into fixed size sections (this size is dependent upon the target architectures cache characteristics and is beyond the scope of this paper <ref> [25, 9] </ref>) as shown below. DO 10 K = 1,N-1,KS DO 20 I = KK+1,N DO 10 J = KK+1,N 10 A (I,J) = A (I,J) - A (I,KK) * A (KK,J) Here KS is the machine-dependent strip size that is related to the cache size. <p> Items 1 and 2 were discussed in Section 2. Items 3 through 7 were discussed in Section 3.1. Item 8 was discussed in the compiler literature <ref> [25, 9] </ref>. Item 9 is discussed in Section 3.1.2. <p> CBlk: The best blocking factor for the compiler-derived algorithm. CMf: The best megaflop rate for the compiler-derived algorithm (corresponding to CBlk). 1 Although the compiler can effectively choose blocking factors automatically, we do not have an implementation of the available algorithms <ref> [25, 9] </ref>. 13 Table 3 LU Performance on IBM, HP, DEC and SGI IBM POWER2 HP 712 Size LABlk LAMf CBlk CMf Speedup LABk LAMf CBlk CMf Speedup 25x25 1,16,32,64 21 8,16 44 1 21 8 21 1.00 75x75 16 81 16 95 1 26 8 31 1.17 150x150 16 132 <p> Additionally, the hand-optimized algorithm utilized the following architectural features that we do not [22]: * The use of temporary arrays to eliminate conflicts in the level-1 direct-mapped cache and the translation lookaside buffer <ref> [25, 9] </ref>. * The use of the memory-prefetch feature on the Alpha to hide latency between cache and memory. Although each of these optimizations could be done in the DEC product compiler, they are not.
Reference: [10] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. Van der Vorst. </author> <title> Solving Linear systems on Vector and shared memory computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA., </address> <year> 1991. </year>
Reference-contexts: We then show how to handle pivoting. 3.1.1 No Pivoting Consider the following algorithm for LU factorization. 4 DO 10 K = 1,N-1 20 A (I,K) = A (I,K) / A (K,K) DO 10 I = K+1,N This point algorithm is refered to as an unblocked right-looking <ref> [10] </ref> algorithm. It exhibits poor cache performance on large matrices. To transform the point algorithm to the block algorithm, the compiler must perfom strip-mine-and-interchange on the K-loop [35, 30, 33]. This transformation is used to create the block update of A. <p> = KK+1,N DO 30 J = KK+1,MIN (K+KS-1,N) 30 A (I,J) = A (I,J) - A (I,KK) * A (KK,J) DO 10 I = K+1,N 10 A (I,J) = A (I,J) - A (I,KK) * A (KK,J) N 1 K K+KS-1 N-1 I = KK+1 At this point, a right-looking <ref> [10] </ref> block algorithm has been obtained. Therefore, LU factorization is blockable. The loop nest surrounding statement 10 is a matrix-matrix multiply that can be further optimized depending upon the architecture.
Reference: [11] <author> J.J. Dongarra, J. DuCroz, I. S. Duff, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: LAPACK blocks many dense matrix algorithms by restructuring them to use the level 2 and 3 BLAS <ref> [11, 12] </ref>. The motivation for the Basic Linear Algebra Subprograms, BLAS [26], was to provide a set of commonly used vector operations so that the programmer could invoke the subprograms instead of writing the code directly. <p> This study has yielded two major results. The first, which is detailed in another paper [8], reveals that the hand loop unrolling performed when optimizing the level 2 and 3 BLAS <ref> [11, 12] </ref> is often unnecessary. While the BLAS are useful, the hand optimization that is required to obtain good performance on a particular architecture may be left to the compiler. Experiments show that, in most cases, the compiler can automatically unroll loops as effectively as hand optimization.
Reference: [12] <author> J.J. Dongarra, J. DuCroz, S. Hammarling, and R. J. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: LAPACK blocks many dense matrix algorithms by restructuring them to use the level 2 and 3 BLAS <ref> [11, 12] </ref>. The motivation for the Basic Linear Algebra Subprograms, BLAS [26], was to provide a set of commonly used vector operations so that the programmer could invoke the subprograms instead of writing the code directly. <p> This study has yielded two major results. The first, which is detailed in another paper [8], reveals that the hand loop unrolling performed when optimizing the level 2 and 3 BLAS <ref> [11, 12] </ref> is often unnecessary. While the BLAS are useful, the hand optimization that is required to obtain good performance on a particular architecture may be left to the compiler. Experiments show that, in most cases, the compiler can automatically unroll loops as effectively as hand optimization.
Reference: [13] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen, and H.A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Finally, we summarize our results and provide and draw some general conclusions. 2 Background The transformations that we use to create the block versions of matrix factorizations from their corresponding point versions are well known in the mathematical software community <ref> [13] </ref>. This section introduces the fundamental tools that the compiler needs to perform the same transformations automatically. <p> The rules for the preservation of data dependence prohibit the reversing of a dependence direction. This would seem to preclude the existence of a block analogue similar to the non-pivoting case. However, a block algorithm that ignores the preventing recurrence and distributes the KK-loop can still be mathematically derived <ref> [13] </ref>. Consider the following. <p> P i only depends upon the first i columns of A, allowing the computation of k P i 's and ^ M i 's, where k is the blocking factor, and then the block application of the ^ M i 's <ref> [13] </ref>. <p> For a more detailed discussion of the QR factorization see Golub and Van Loan [18]. The LAPACK block QR factorization is an attempt to recast the algorithm in terms of calls to level 3 BLAS <ref> [13] </ref>. If the level 3 BLAS are hand-tuned for a particular architecture, the block QR algorithm may perform significantly better than the point version on large matrix sizes (those that cause the working set to be much larger than the cache size). <p> Unfortunately, the block QR algorithm in LAPACK is not automatically derivable by a compiler. The block application of a number of elementary reflectors involves both computation and storage that does not exist in the original point algorithm <ref> [13] </ref>.
Reference: [14] <author> J.J. Dongarra, F.G. Gustavson, and A. Karp. </author> <title> Implementing linear algebra algorithms for dense matrices on a vector pipeline machine. </title> <journal> SIAM Review, </journal> <volume> 26(1) </volume> <pages> 91-112, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: With the advent of vector and parallel supercomputers, the efficiency of the factorizations were seen to depend dramatically upon the algorithmic form chosen for the implementation <ref> [14, 16, 29] </ref>. These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization.
Reference: [15] <author> J.J. Dongarra, C.B. Moler, J.R. Bunch, and G.W. Stewart. </author> <title> LINPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA., </address> <year> 1979. </year> <month> 18 </month>
Reference-contexts: These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization. The motivation of the LAPACK [2] project was to recast the algorithms in the EISPACK [32] and LINPACK <ref> [15] </ref> software libraries with block ones. A block form of an algorithm restructures the algorithm in terms of matrix operations that attempt to minimize the amount of data moved within the memory hierarchy while keeping the arithmetic units of the machine occupied.
Reference: [16] <author> K.A. Gallivan, R.J. Plemmons, and A.H. Sameh. </author> <title> Parllel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32 </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: With the advent of vector and parallel supercomputers, the efficiency of the factorizations were seen to depend dramatically upon the algorithmic form chosen for the implementation <ref> [14, 16, 29] </ref>. These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization.
Reference: [17] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins, </publisher> <address> Baltimore, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: However, we present a performance-competitive version of the QR factorization that is derivable by the compiler. 3.1 LU Factorization The LU decomposition factors a non-singular matrix A into the product of two matrices, L and U , such that A = LU <ref> [17] </ref>. L is a unit lower triangular matrix and U is an upper triangular matrix.
Reference: [18] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Efficiency of the implementation of the level 2 BLAS subroutines determines the rate at which the factorization is computed. For a more detailed discussion of the QR factorization see Golub and Van Loan <ref> [18] </ref>. The LAPACK block QR factorization is an attempt to recast the algorithm in terms of calls to level 3 BLAS [13].
Reference: [19] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: DO 10 I = 1,N A (I,J) = A (I-1,J) + A (I,J-1)10 To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [4, 19] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. As an example of section analysis consider the following loop. 3 DO 10 I = 1,N 10 A (J,I) = ...
Reference: [20] <author> B. Kagstrom, P. Ling, and C. Van Loan. </author> <title> GEMM-based level 3 BLAS: High-performance model implementations and performance evaluation benchmark. </title> <type> Technical Report UMINF-95.18, </type> <institution> Department of Computing Science, University of Umea, S-901 87 Umea, Sweden, </institution> <month> October </month> <year> 1995. </year> <note> Submitted to ACM Transactions on Mathematical Software. Also available as LAPACK Working Note 107. </note>
Reference-contexts: We present an experiment comparing the performance of hand-optimized LAPACK algorithms with the compiler-derived algorithms attained using our techniques. We also breifly discuss a recent approach of Kagstrom, Ling and Van Loan <ref> [20] </ref> that aims to reduce the software costs associated with optimizing the level 3 BLAS. <p> As discussed in the introduction, an efficient set of BLAS requires a non-trivial effort in software engineering. See <ref> [20] </ref> for a discussion on software efforts to provide optimal implementations of the level 3 BLAS. An approach that is both efficient and practical is the GEMM-based one proposed by Kagstrom, Ling and Van Loan [20] in a recent study. <p> See <ref> [20] </ref> for a discussion on software efforts to provide optimal implementations of the level 3 BLAS. An approach that is both efficient and practical is the GEMM-based one proposed by Kagstrom, Ling and Van Loan [20] in a recent study. Their approach advocates optimizing the general matrix-matrix multiply and add kernel GEMM and then rewriting the remainder of the level 3 BLAS in terms of calls to this kernel.
Reference: [21] <author> B. Kagstrom, P. Ling, and C. Van Loan. </author> <title> GEMM-based level 3 BLAS: Installation, tuning, and use of the model implementations and the performance evaluation benchmark. </title> <type> Technical Report UMINF-95.19, </type> <institution> Department of Computing Science, University of Umea, S-901 87 Umea, Sweden, </institution> <month> October </month> <year> 1995. </year> <note> Submitted to ACM Transactions on Mathematical Software. Also available as LAPACK Working Note 108. </note>
Reference-contexts: Their thorough analysis highlights the many issues that must be considered when attempting to construct a set of highly tuned BLAS. Most importantly, they provide high quality implementations of the BLAS for general use as well as a performance evaluation benchmark <ref> [21] </ref>. We emphasize that our study examines only whether the necessary optimizations may be left to the compiler, and, also whether they should be applied directly to the matrix factorizations.
Reference: [22] <author> C. Kamath, R. Ho, </author> <title> and D.P. Manley. DXML: A high-performance scientific subroutine library. </title> <journal> Digital Technical Journal, </journal> <volume> 6(3) </volume> <pages> 44-56, </pages> <year> 1994. </year>
Reference-contexts: After size 100x100, the second-level cache on the 14 Alpha, which is 96K, begins to overflow. Our compiler-derived version is not blocked for multiple levels of cache, while the hand-optimized version is blocked for 2 levels of cache <ref> [22] </ref>. Thus, the compiler-derived algorithm suffers many more cache misses in the level-2 cache than the LAPACK version. It is possible for the compiler to perform the extra blocking for multiple levels of cache, but we know of no compiler that currently does this. <p> It is possible for the compiler to perform the extra blocking for multiple levels of cache, but we know of no compiler that currently does this. Additionally, the hand-optimized algorithm utilized the following architectural features that we do not <ref> [22] </ref>: * The use of temporary arrays to eliminate conflicts in the level-1 direct-mapped cache and the translation lookaside buffer [25, 9]. * The use of the memory-prefetch feature on the Alpha to hide latency between cache and memory.
Reference: [23] <author> D. Kuck. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Dependence is necessary for determining the legality of compiler transformations to create blocked versions of matrix factorizations. A dependence exists between two statements if there exists a control flow path from the first statement to the second, and both statements reference the same memory location <ref> [23] </ref>. * If the first statement writes to the location and the second reads from it, there is a true dependence, also called a flow dependence. * If the first statement reads from the location and the second writes to it, there is an antide pendence. * If both statements write
Reference: [24] <author> Monica Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <journal> SIG-PLAN Notices, </journal> <volume> 23(7) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1988. </year> <booktitle> Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: For a matrix this small, cache performance is not a factor. We believe the performance difference comes from the way code is generated. For superscalar architectures like the HP, a code generation scheme called software pipelining is used to generate highly parallel code <ref> [24, 31] </ref>. However, software pipelining requires a lot of registers to be successful. In our code, we performed unroll-and-jam to improve cache performance. However, unroll-and-jam can significantly increase register pressure and cause software pipelining to fail [6].
Reference: [25] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, California, </address> <year> 1991. </year>
Reference-contexts: This transformation is used to create the block update of A. To apply this transformation, we first strip the K-loop into fixed size sections (this size is dependent upon the target architectures cache characteristics and is beyond the scope of this paper <ref> [25, 9] </ref>) as shown below. DO 10 K = 1,N-1,KS DO 20 I = KK+1,N DO 10 J = KK+1,N 10 A (I,J) = A (I,J) - A (I,KK) * A (KK,J) Here KS is the machine-dependent strip size that is related to the cache size. <p> Items 1 and 2 were discussed in Section 2. Items 3 through 7 were discussed in Section 3.1. Item 8 was discussed in the compiler literature <ref> [25, 9] </ref>. Item 9 is discussed in Section 3.1.2. <p> CBlk: The best blocking factor for the compiler-derived algorithm. CMf: The best megaflop rate for the compiler-derived algorithm (corresponding to CBlk). 1 Although the compiler can effectively choose blocking factors automatically, we do not have an implementation of the available algorithms <ref> [25, 9] </ref>. 13 Table 3 LU Performance on IBM, HP, DEC and SGI IBM POWER2 HP 712 Size LABlk LAMf CBlk CMf Speedup LABk LAMf CBlk CMf Speedup 25x25 1,16,32,64 21 8,16 44 1 21 8 21 1.00 75x75 16 81 16 95 1 26 8 31 1.17 150x150 16 132 <p> Additionally, the hand-optimized algorithm utilized the following architectural features that we do not [22]: * The use of temporary arrays to eliminate conflicts in the level-1 direct-mapped cache and the translation lookaside buffer <ref> [25, 9] </ref>. * The use of the memory-prefetch feature on the Alpha to hide latency between cache and memory. Although each of these optimizations could be done in the DEC product compiler, they are not.
Reference: [26] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 308-329, </pages> <year> 1979. </year>
Reference-contexts: LAPACK blocks many dense matrix algorithms by restructuring them to use the level 2 and 3 BLAS [11, 12]. The motivation for the Basic Linear Algebra Subprograms, BLAS <ref> [26] </ref>, was to provide a set of commonly used vector operations so that the programmer could invoke the subprograms instead of writing the code directly.
Reference: [27] <author> Richard Lehoucq. </author> <title> Implementing efficient and portable dense matrix factorizations. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1992. </year>
Reference-contexts: Although the compiler transformation techniques may be applied directly to the BLAS, it is interesting to draw a comparison with applying them directly to the factorizations. The benefit is the possibility of a BLAS-less linear algebra package that is nearly as efficient as LAPACK. For example, in <ref> [27] </ref>, it was demonstrated that on some computers, the best LU factorization was an inlined approach even when a highly optimized set of BLAS were available.
Reference: [28] <author> David Levine, David Callahan, and Jack Dongarra. </author> <title> A comparative study of automatic vector-izing compilers. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1223-1244, </pages> <year> 1991. </year>
Reference-contexts: Forms of pattern matching are already done in commercially available compilers. Vectorizing compilers pattern match for specialized computations such as searching vectors for particular conditions <ref> [28] </ref>. Other preprocessors pattern match to recognize matrix multiplication and, in turn, output a predetermined solution that is optimal for a particular machine.
Reference: [29] <author> James M. Ortega. </author> <title> Introduction to Parallel and Vector Solutions of Linear Systems. </title> <publisher> Plenum Press, </publisher> <address> New York, New York, </address> <year> 1988. </year>
Reference-contexts: With the advent of vector and parallel supercomputers, the efficiency of the factorizations were seen to depend dramatically upon the algorithmic form chosen for the implementation <ref> [14, 16, 29] </ref>. These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization.
Reference: [30] <author> A.K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <month> 19 </month>
Reference-contexts: It exhibits poor cache performance on large matrices. To transform the point algorithm to the block algorithm, the compiler must perfom strip-mine-and-interchange on the K-loop <ref> [35, 30, 33] </ref>. This transformation is used to create the block update of A. To apply this transformation, we first strip the K-loop into fixed size sections (this size is dependent upon the target architectures cache characteristics and is beyond the scope of this paper [25, 9]) as shown below.
Reference: [31] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(7) </volume> <pages> 283-299, </pages> <month> July </month> <year> 1992. </year> <booktitle> Proceedings of the ACM SIG-PLAN '92 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: For a matrix this small, cache performance is not a factor. We believe the performance difference comes from the way code is generated. For superscalar architectures like the HP, a code generation scheme called software pipelining is used to generate highly parallel code <ref> [24, 31] </ref>. However, software pipelining requires a lot of registers to be successful. In our code, we performed unroll-and-jam to improve cache performance. However, unroll-and-jam can significantly increase register pressure and cause software pipelining to fail [6].
Reference: [32] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> EISPACK Guide. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <note> second edition, </note> <year> 1976. </year> <booktitle> Volume 6 of Lecture Notes in Computer Science. </booktitle>
Reference-contexts: These studies concluded that managing the memory hierarchy is the single most important factor governing the efficiency of the software implementation computing the factorization. The motivation of the LAPACK [2] project was to recast the algorithms in the EISPACK <ref> [32] </ref> and LINPACK [15] software libraries with block ones. A block form of an algorithm restructures the algorithm in terms of matrix operations that attempt to minimize the amount of data moved within the memory hierarchy while keeping the arithmetic units of the machine occupied.
Reference: [33] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIG-PLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: It exhibits poor cache performance on large matrices. To transform the point algorithm to the block algorithm, the compiler must perfom strip-mine-and-interchange on the K-loop <ref> [35, 30, 33] </ref>. This transformation is used to create the block update of A. To apply this transformation, we first strip the K-loop into fixed size sections (this size is dependent upon the target architectures cache characteristics and is beyond the scope of this paper [25, 9]) as shown below.
Reference: [34] <author> M. Wolfe. </author> <title> Advanced loop interchange. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: To complete the transformation, the KK-loop must be distributed around the loop that surrounds statement 20 and around the loop nest that surrounds statement 10 before being interchanged to the innermost position of the loop surrounding statement 10 <ref> [34] </ref>. This distribution yields: DO 10 K = 1,N-1,KS DO 20 I = KK+1,N DO 10 KK = K,MIN (K+KS-1,N-1) DO 10 I = KK+1,N Unfortunately, the loop is no longer correct. This loop scales a number of values before it updates them.
Reference: [35] <author> M. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year> <month> 20 </month>
Reference-contexts: It exhibits poor cache performance on large matrices. To transform the point algorithm to the block algorithm, the compiler must perfom strip-mine-and-interchange on the K-loop <ref> [35, 30, 33] </ref>. This transformation is used to create the block update of A. To apply this transformation, we first strip the K-loop into fixed size sections (this size is dependent upon the target architectures cache characteristics and is beyond the scope of this paper [25, 9]) as shown below. <p> iteration space of the loop surrounding statement 10, we can split the J-loop into two loops one loop iterating over the portion of A where the dependence cycle exists, and one loop iterating over the portion of A where the cycle does not exist using a transformation called index-set splitting <ref> [35] </ref>. <p> Therefore, interchanging the loops requires the KK-loop to iterate over a trapezoidal region with an upper bound of I-1 until I-1 &gt; K+KS-1 (see Wolfe, and Carr and Kennedy for more details on transforming non-rectangular loop nests <ref> [35, 7] </ref>).
References-found: 35

