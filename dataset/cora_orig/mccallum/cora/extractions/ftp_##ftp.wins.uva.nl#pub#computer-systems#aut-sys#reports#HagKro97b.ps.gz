URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/HagKro97b.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: stephanh@wins.uva.nl  
Title: A Short Introduction to Reinforcement Learning  
Author: Stephan ten Hagen and Ben Krose 
Address: Kruislaan 403, 1098 SJ Amsterdam  
Affiliation: Department of Mathematics, Computer Science, Physics and Astronomy University of Amsterdam  
Note: In: Proc. BENELEARN-97, 7th Belgian-Dutch Conference on Machine Learning, pp 7-12,1997  
Abstract: This introduction is meant for readers with no knowledge about reinforcement learning. It presents the basic framework and introduce the basic terminology. We hope that this will make it easier to read other reinforcement learning literature. Pointers to more tutorial sources will be given at the end.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <year> 1996. </year>
Reference-contexts: Given a policy it is possible to define a value function V . It is defined as the expected sum of (discounted) future evaluations. For state s the value V (s) is defined as: V (s) = E f i=k The discount factor fl 2 <ref> [0; 1] </ref> is used to weight future reinforcements. If fl = 0 this corresponds to the immediate reinforcement. If fl = 1 this corresponds to the expected sum of future reinforcements. So fl = 1 is a good choice for Example 1 and Example 2. <p> For this reason the temporal difference update (7) can be changed so that the update has effect on all previous states. Usually not all previous states are updated. A discount factor 2 <ref> [0; 1] </ref> is used. The longer ago the state was visited, the less it will be effected by the present update. Using this method is called T D () learning. 4.2 Q-Learning Using the value function to solve the optimization problem results in a dual optimization task.
Reference: [2] <author> Mance E. Harmon. </author> <title> Reinforcement learning: A tutorial. </title> <address> http://eureka1.aa.wpafb.af.mil/rltutorial/. </address>
Reference: [3] <author> Leslie P. Kaebling, Michael L. Littmann, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <year> 1996. </year>
Reference: [4] <author> S. Sathya Keerthi and B. Ravindran. </author> <title> A tutorial survey of reinforcement learning. </title> <journal> Sadhana, </journal> <volume> 19(6) </volume> <pages> 851-889, </pages> <year> 1994. </year>
Reference: [5] <author> Satinder Singh, Peter Norvig, and David Cohn. </author> <title> How to make software agents do the right thing: An introduction to RL. </title> <address> http://envy.cs.umass.edu/People/singh/RLMasses/RL.html. </address>
Reference: [6] <author> Richard S. Sutton and Andrew G. Barto. </author> <title> Reinforcement learning: </title> <booktitle> Course notes. ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/IntroRL/fRL1.ps,RL2.ps,RL3.psg. </booktitle> <pages> 6 </pages>
References-found: 6

