URL: http://www.cs.duke.edu/~large/Papers/buffer.ps
Refering-URL: http://www.cs.duke.edu/~large/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Buffer Tree: A New Technique for Optimal I/O Algorithms  
Author: Lars Arge 
Date: August 1996  
Address: Aarhus, Denmark  
Affiliation: BRICS Department of Computer Science University of Aarhus  
Abstract: In this paper we develop a technique for transforming an internal-memory tree data structure into an external-memory structure. We show how the technique can be used to develop a search tree like structure, a priority queue, a (one-dimensional) range tree and a segment tree, and give examples of how these structures can be used to develop efficient I/O algorithms. All our algorithms are either extremely simple or straightforward generalizations of known internal-memory algorithms|given the developed external data structures. We believe that algorithms relying on the developed structure will be of practical interest due to relatively small constants in the asymptotic bounds.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal and J. S. Vitter. </author> <title> The Input/Output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: Therefore a number of researchers have developed techniques and algorithms for solving large-scale off-line problems without using external memory data structures <ref> [1, 11, 14] </ref>. In this paper we develop external data structures that take advantage of the large main memory. This is done by only requiring good amortized performance of the operations on the structures, and by allowing search operations to be batched. <p> areas arise in many large-scale computations in e.g. object-oriented, deductive and spatial databases, VLSI design and simulation programs, geographic informations systems, constraint logic programming, statistics, virtual reality systems, and computer graphics. 1.1 I/O Model and Previous Results We will be working in an I/O model introduced by Aggarwal and Vitter <ref> [1] </ref>. The model has the following parameters: N = # of elements in the problem instance; M = # of elements that can fit into main memory; B = # of elements per block; where M &lt; N and 1 B M=2. <p> The number D of disks range up to 10 2 in current disk arrays. Early work on I/O algorithms concentrated on algorithms for sorting and permutation-related problems in the single disk model <ref> [1] </ref>, as well as in the extended version of the I/O-model [19, 20, 30, 31]. External sorting requires fi (n log m n) I/Os, 1 which is the external memory equivalent of the well-known fi (N log N ) time bound for sorting in internal memory. <p> The non optimality of disk striping can be demonstrated via the sorting bound. While sorting N elements using disk striping and one of the the one-disk sorting algorithms requires O (n=D log m=D n) I/Os the optimal bound is O (n=D log m n) I/Os <ref> [1] </ref>. Note that the optimal bound results in a linear speedup in the number of disk. Nodine and Vitter [19] managed to 20 develop a theoretical optimal D-disk sorting algorithm based on merge sort, and later they also developed an optimal algorithm based on distribution sort [20].
Reference: [2] <author> L. Arge. </author> <title> The I/O-complexity of ordered binary-decision diagram manipulation. </title> <booktitle> In Proc. Int. Symp. on Algorithms and Computation, </booktitle> <volume> LNCS 1004, </volume> <pages> pages 82-91, </pages> <year> 1995. </year>
Reference-contexts: We believe that our technique and the developed structures will be useful in the development of algorithms for other problems in the mentioned areas as well as in other areas. Examples of this can be found in <ref> [2, 3, 5, 9] </ref>. More specifically, the results in this paper are the following: Sorting: We develop a simple dynamic tree structure (The Buffer Tree) with operations insert, delete and write. <p> We have also used our priority queue to develop an extremely simple algorithm for "circuit evaluation", improving on the previously know algorithm. Recently, several authors have used the structures developed in this paper or modified versions of them to solve important external-memory problems. In <ref> [2] </ref> the priority queue is used to develop new I/O efficient algorithms for ordered binary-decision diagram manipulation, and in [9] it is used in the development of several efficient external graph algorithm.
Reference: [3] <author> L. Arge. </author> <title> Efficient External-Memory Data Structures and Applications. </title> <type> PhD thesis, </type> <institution> University of Aarhus, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: We believe that our technique and the developed structures will be useful in the development of algorithms for other problems in the mentioned areas as well as in other areas. Examples of this can be found in <ref> [2, 3, 5, 9] </ref>. More specifically, the results in this paper are the following: Sorting: We develop a simple dynamic tree structure (The Buffer Tree) with operations insert, delete and write. <p> Thus, n leaves 3 4 2 1 0 m nodes EA B C D F p m nodes and EF , are shown. 17 CD is stored in the list associated with the multislab <ref> [ 1 ; 3 ] </ref>. All segments that are not long are called short segments and are not stored in any multislab list. Instead, they are passed down to lower levels of the tree where they may span recursively defined slabs and be stored.
Reference: [4] <author> L. Arge, M. Knudsen, and K. Larsen. </author> <title> A general lower bound on the I/O-complexity of comparison-based algorithms. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 709, </volume> <pages> pages 83-94, </pages> <year> 1993. </year>
Reference-contexts: Most notably I/O-efficient algorithms have been developed for a large number of computational 1 We define for convenience log m n = maxf1; (log n)=(log m)g. 2 geometry [5, 14] and graph problems [11]. In <ref> [4] </ref> a general connection between the comparison--complexity and the I/O-complexity of a given problem is shown in the "comparison I/O model" where comparison of elements is the only allowed operation in internal memory. 1.2 Our Results In this paper we develop a technique for transforming an internal-memory tree data structure into <p> But an optimal solution for this problem only requires O (n log m n + r) I/Os <ref> [4, 14] </ref>. For typical systems B is less than m so log B n is larger than log m n, but more important, the B-tree solution will be slower than the optimal solution by a factor of B. <p> That the algorithm 16 is optimal in the comparison I/O model follows from the (N log 2 N + R) comparison model lower bound, and the general connection between comparison and I/O lower bounds proved in <ref> [4] </ref>. 6 An External Segment Tree In this section we use our technique to develop an external memory version of the segment tree. As mentioned this will enable us to solve the batched range searching and the pairwise rectangle intersection problems in the optimal number of I/Os. <p> The multislabs for the root are then defined as contiguous ranges of slabs, such as for example <ref> [ 1 ; 4 ] </ref>. There are m=2 p m=2 multislabs and the lists associated with a node are precisely a list for each of the multislabs. <p> That both algorithms are optimal in the comparison I/O model follows by the internal memory comparison lower bound and the result in <ref> [4] </ref>.
Reference: [5] <author> L. Arge, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <booktitle> In Proc. Annual European Symposium on Algorithms, </booktitle> <volume> LNCS 979, </volume> <pages> pages 295-310, </pages> <year> 1995. </year> <note> A full version is to appear in special issue of Algorithmica. </note>
Reference-contexts: More recently external-memory researchers have designed algorithms for a number of problems in different areas. Most notably I/O-efficient algorithms have been developed for a large number of computational 1 We define for convenience log m n = maxf1; (log n)=(log m)g. 2 geometry <ref> [5, 14] </ref> and graph problems [11]. <p> We believe that our technique and the developed structures will be useful in the development of algorithms for other problems in the mentioned areas as well as in other areas. Examples of this can be found in <ref> [2, 3, 5, 9] </ref>. More specifically, the results in this paper are the following: Sorting: We develop a simple dynamic tree structure (The Buffer Tree) with operations insert, delete and write. <p> In some applications (e.g. in <ref> [5] </ref>) we would like to use less internal memory for the external priority queue structure. <p> In [2] the priority queue is used to develop new I/O efficient algorithms for ordered binary-decision diagram manipulation, and in [9] it is used in the development of several efficient external graph algorithm. In <ref> [5] </ref> an extension of the segment tree is used to develop efficient new external algorithms 3 Note that we could actually do without the sorting by distributing elements in sorted order when emptying a buffer in the buffer tree, precisely as we in the range tree structure distribute them in time
Reference: [6] <author> L. Arge and J. S. Vitter. </author> <title> Optimal dynamic interval management in external memory. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <year> 1996. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os. <p> we could actually do without the sorting by distributing elements in sorted order when emptying a buffer in the buffer tree, precisely as we in the range tree structure distribute them in time order representation. 21 for a number of important problems involving line segments in the plane, and in <ref> [6] </ref> the main idea behind the external segment tree (the notion of multislabs) is used to develop an optimal "on-line" versions of the interval tree. We believe that several of our structures will be efficient in practice due to small constants in the asymptotic bounds.
Reference: [7] <author> R. Bayer and E. McCreight. </author> <title> Organization and maintenance of large ordered indizes. </title> <journal> Acta Informatica, </journal> <volume> 1 </volume> <pages> 173-189, </pages> <year> 1972. </year>
Reference-contexts: This means that if we used these structures to solve the problems we consider in this paper, we would not be able to take full advantage of the large main memory. Consider for example the well-known B-tree <ref> [7, 12, 18] </ref>. On such a tree one can do an insert or delete operation in O (log B n) I/Os and a rangesearch operation in O (log B n + r) I/Os.
Reference: [8] <author> J. L. Bentley and D. Wood. </author> <title> An optimal worst case algorithm for reporting intersections of rectangles. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 29 </volume> <pages> 571-577, </pages> <year> 1980. </year>
Reference-contexts: As mentioned, large-scale computational geometry problems arise in many areas. The three intersection reporting problems mentioned especially arise in VLSI design and are certainly large-scale in such applications. The pairwise rectangle intersection problem is of special interest, as it is used in VLSI design rule checking <ref> [8] </ref>. Optimal algorithms for the three problems are also developed in [14], but as noted earlier these algorithms are very I/O-specific, while we manage to "hide" all the I/O-specific parts in the data structures and use the known internal-memory algorithms. <p> As mentioned this will enable us to solve the batched range searching and the pairwise rectangle intersection problems in the optimal number of I/Os. The segment tree <ref> [8, 22] </ref> is a well-known data structure used to maintain a dynamically changing set of segments whose endpoints belongs to a fixed set, such that given a query point all segments that contain the point can be found efficiently. Such queries are normally called stabbing queries. <p> The problem of pairwise rectangle intersection is defined similar to the orthogonal line segment intersection problem. Given N rectangles in the plane (with sides parallel to the axes) we should report all intersecting pairs. In <ref> [8] </ref> it is shown that if we|besides the orthogonal line segment intersection problem|can solve the batched range searching problem in time O (N log 2 N + R), we will in total obtain a solution to the rectangle intersection problem with the same (optimal) time bound.
Reference: [9] <author> Y.-J. Chiang. </author> <title> Dynamic and I/O-Efficient Algorithms for Computational Geometry and Graph Problems: Theoretical and Experimental Results. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <month> August </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: We believe that our technique and the developed structures will be useful in the development of algorithms for other problems in the mentioned areas as well as in other areas. Examples of this can be found in <ref> [2, 3, 5, 9] </ref>. More specifically, the results in this paper are the following: Sorting: We develop a simple dynamic tree structure (The Buffer Tree) with operations insert, delete and write. <p> We hope in the future to be able to implement some of the structures in the transparent parallel I/O environment (TPIE) developed by Vengroff [27]. Results of experiments on the practical performance of several algorithms developed for the I/O model are reported in <ref> [9, 10, 28] </ref>. The main organization of the rest of this paper is the following: In the next section we 4 sketch our general technique. <p> Recently, several authors have used the structures developed in this paper or modified versions of them to solve important external-memory problems. In [2] the priority queue is used to develop new I/O efficient algorithms for ordered binary-decision diagram manipulation, and in <ref> [9] </ref> it is used in the development of several efficient external graph algorithm.
Reference: [10] <author> Y.-J. Chiang. </author> <title> Experiments on the practical I/O efficiency of geometric algorithms: Dis--tribution sweep vs. plane sweep. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 955, </volume> <pages> pages 346-357, </pages> <year> 1995. </year>
Reference-contexts: We hope in the future to be able to implement some of the structures in the transparent parallel I/O environment (TPIE) developed by Vengroff [27]. Results of experiments on the practical performance of several algorithms developed for the I/O model are reported in <ref> [9, 10, 28] </ref>. The main organization of the rest of this paper is the following: In the next section we 4 sketch our general technique.
Reference: [11] <author> Y.-J. Chiang, M. T. Goodrich, E. F. Grove, R. Tamassia, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <year> 1995. </year>
Reference-contexts: Therefore a number of researchers have developed techniques and algorithms for solving large-scale off-line problems without using external memory data structures <ref> [1, 11, 14] </ref>. In this paper we develop external data structures that take advantage of the large main memory. This is done by only requiring good amortized performance of the operations on the structures, and by allowing search operations to be batched. <p> This is done by only requiring good amortized performance of the operations on the structures, and by allowing search operations to be batched. The data structures developed can then be used in simple and I/O-efficient algorithms for computational geometry and graph problems. As pointed out in <ref> [11] </ref> and [14] problems from these two areas arise in many large-scale computations in e.g. object-oriented, deductive and spatial databases, VLSI design and simulation programs, geographic informations systems, constraint logic programming, statistics, virtual reality systems, and computer graphics. 1.1 I/O Model and Previous Results We will be working in an I/O <p> More recently external-memory researchers have designed algorithms for a number of problems in different areas. Most notably I/O-efficient algorithms have been developed for a large number of computational 1 We define for convenience log m n = maxf1; (log n)=(log m)g. 2 geometry [5, 14] and graph problems <ref> [11] </ref>. <p> We prove an O ( log m n B ) amortized bound on the number of I/Os used by this operation. Using the structure it is straightforward to develop an extremely simple algorithm for "circuit-like" computations as defined in <ref> [11] </ref>. This algorithm is then an alternative to the "time-forward processing technique" developed in the same paper. The time-forward processing technique only works for large values of m, while our algorithm works for all m. In [11] the time-forward processing technique is used to develop an efficient I/O algorithm for external-memory <p> is straightforward to develop an extremely simple algorithm for "circuit-like" computations as defined in <ref> [11] </ref>. This algorithm is then an alternative to the "time-forward processing technique" developed in the same paper. The time-forward processing technique only works for large values of m, while our algorithm works for all m. In [11] the time-forward processing technique is used to develop an efficient I/O algorithm for external-memory list-ranking, which in turn is used to develop efficient algorithms for a large number of graph-problems. 2 All these algorithms thus inherit the constraint on m and our new algorithm removes it from all of them. <p> &lt; c 1) blocks of internal memory, by decreasing the fan-out and the size of the buffers to fi (m 1=c ) as discussed in Section 2. 4.1 Application: Time-Forward Processing As mentioned in the introduction a technique for evaluating circuits (or "circuit-like" computations) in external memory is developed in <ref> [11] </ref>. This technique is called time-forward processing. The problem is the following: We are given a bounded fan-in boolean circuit, whose description is stored in external memory, and want to evaluate the function computed by the network. <p> Thinking of vertex v as being evaluated at "time" v motivates calling an evaluation of a circuit a time-forward processing. The main issue in such an evaluation is to ensure that when one evaluates a particular vertex one has the values of its inputs in internal memory. In <ref> [11] </ref> an external-memory algorithm using O (n log m n) I/Os is developed (here N is the number of nodes plus the number of edges). The algorithm uses a number of known as well as new I/O-algorithm design techniques and is not particularly simple. <p> We can then obtain the inputs to the next node in the topological order just by performing a number of deletemin operations on the queue. The O (n log m n) I/O-bound follows immediately from Theorem 3. 10 In <ref> [11] </ref> a randomized and two deterministic algorithms for external-memory list ranking are developed. One of these algorithms uses time-forward processing and therefore inherits the constraint that m should not be too small.
Reference: [12] <author> D. Cormer. </author> <title> The ubiquitous B-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2) </volume> <pages> 121-137, </pages> <year> 1979. </year>
Reference-contexts: This means that if we used these structures to solve the problems we consider in this paper, we would not be able to take full advantage of the large main memory. Consider for example the well-known B-tree <ref> [7, 12, 18] </ref>. On such a tree one can do an insert or delete operation in O (log B n) I/Os and a rangesearch operation in O (log B n + r) I/Os.
Reference: [13] <author> H. Edelsbrunner and M. Overmars. </author> <title> Batched dynamic solutions to decomposable searching problems. </title> <journal> Journal of Algorithms, </journal> <volume> 6 </volume> <pages> 515-542, </pages> <year> 1985. </year>
Reference-contexts: In general, problems where the whole sequence of operations on a data structure is known in advance, and where there is no requirement on the order in which the queries should be answered, are known as batched dynamic problems <ref> [13] </ref>. As mentioned some work has already been done on designing external versions of known internal dynamic data structures, but practically all of it has been done in the I/O model where the size of the internal memory equals the block size.
Reference: [14] <author> M. T. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 714-723, </pages> <year> 1993. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os. <p> Therefore a number of researchers have developed techniques and algorithms for solving large-scale off-line problems without using external memory data structures <ref> [1, 11, 14] </ref>. In this paper we develop external data structures that take advantage of the large main memory. This is done by only requiring good amortized performance of the operations on the structures, and by allowing search operations to be batched. <p> This is done by only requiring good amortized performance of the operations on the structures, and by allowing search operations to be batched. The data structures developed can then be used in simple and I/O-efficient algorithms for computational geometry and graph problems. As pointed out in [11] and <ref> [14] </ref> problems from these two areas arise in many large-scale computations in e.g. object-oriented, deductive and spatial databases, VLSI design and simulation programs, geographic informations systems, constraint logic programming, statistics, virtual reality systems, and computer graphics. 1.1 I/O Model and Previous Results We will be working in an I/O model introduced <p> More recently external-memory researchers have designed algorithms for a number of problems in different areas. Most notably I/O-efficient algorithms have been developed for a large number of computational 1 We define for convenience log m n = maxf1; (log n)=(log m)g. 2 geometry <ref> [5, 14] </ref> and graph problems [11]. <p> The three intersection reporting problems mentioned especially arise in VLSI design and are certainly large-scale in such applications. The pairwise rectangle intersection problem is of special interest, as it is used in VLSI design rule checking [8]. Optimal algorithms for the three problems are also developed in <ref> [14] </ref>, but as noted earlier these algorithms are very I/O-specific, while we manage to "hide" all the I/O-specific parts in the data structures and use the known internal-memory algorithms. A note should also be made on the fact that the search operations are batched. <p> But an optimal solution for this problem only requires O (n log m n + r) I/Os <ref> [4, 14] </ref>. For typical systems B is less than m so log B n is larger than log m n, but more important, the B-tree solution will be slower than the optimal solution by a factor of B. <p> As mentioned an algorithm for the problem is also developed in <ref> [14] </ref>, but this algorithm is very I/O specific whereas our algorithm `'hides" the I/O in the range tree. <p> That both algorithms are optimal in the comparison I/O model follows by the internal memory comparison lower bound and the result in [4]. Like in the orthogonal line segment intersection case, optimal algorithms for the two problems are also developed in <ref> [14] </ref>. 7 Extending the Results to the D-disk Model As mentioned in the introduction an approach to increase the throughput of I/O systems is to use a number of disks in parallel.
Reference: [15] <author> S. Huddleston and K. Mehlhorn. </author> <title> A new data structure for representing sorted lists. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 157-184, </pages> <year> 1982. </year>
Reference-contexts: In later sections we then extend this basic structure in order to obtain an external priority queue and an external (one-dimensional) range tree. O (log m n) m blocks 1 4 m : : :m The buffer tree is an (a; b)-tree <ref> [15] </ref> with a = m=4 and b = m, extended with a buffer in each node. In such a tree all nodes except for the root have fan-out between m=4 and m, and thus the height of the tree is O (log m n). <p> The emptying of a leaf buffer is a bit more complicated as we also need to consider rebalancing of the structure when we empty such a buffer. The algorithm is given in Figure 5. Basically the rebalancing is done precisely as on normal (a; b)-trees <ref> [15] </ref>. After finding the position of a new element among the elements in the leaves of an (a; b)-tree, the rebalancing is done by a series of "splits" of node in the structure. We give the algorithm in Figure 3. <p> Furthermore, it is easy to realize that the administrative work in a rebalancing operation|updating partitioning elements and so on|can be performed in O (m) I/Os. In <ref> [15] </ref> it is shown that the number of rebalancing operations in a sequence of K updates on an initially empty (a; b)-tree is bounded by O (K=(b=2 a)) if b &gt; 2a.
Reference: [16] <author> C. Icking, R. Klein, and T. Ottmann. </author> <title> Priority search trees in secondary memory. </title> <booktitle> In Proc. Graph-Theoretic Concepts in Computer Science, </booktitle> <volume> LNCS 314, </volume> <pages> pages 84-93, </pages> <year> 1987. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [17] <author> P. C. Kanellakis, S. Ramaswamy, D. E. Vengroff, and J. S. Vitter. </author> <title> Indexing for data models with constraints and classes. </title> <booktitle> In Proc. ACM Symp. Principles of Database Systems, </booktitle> <year> 1993. </year> <note> Invited to special issue of JCSS on Principles of Database Systems (to appear). A complete version appears as technical report 90-31, </note> <institution> Brown University. </institution>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [18] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. 3 Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os. <p> This means that if we used these structures to solve the problems we consider in this paper, we would not be able to take full advantage of the large main memory. Consider for example the well-known B-tree <ref> [7, 12, 18] </ref>. On such a tree one can do an insert or delete operation in O (log B n) I/Os and a rangesearch operation in O (log B n + r) I/Os.
Reference: [19] <author> M. H. Nodine and J. S. Vitter. </author> <title> Large-scale sorting in parallel memories. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <year> 1991. </year>
Reference-contexts: The number D of disks range up to 10 2 in current disk arrays. Early work on I/O algorithms concentrated on algorithms for sorting and permutation-related problems in the single disk model [1], as well as in the extended version of the I/O-model <ref> [19, 20, 30, 31] </ref>. External sorting requires fi (n log m n) I/Os, 1 which is the external memory equivalent of the well-known fi (N log N ) time bound for sorting in internal memory. <p> Note that the optimal bound results in a linear speedup in the number of disk. Nodine and Vitter <ref> [19] </ref> managed to 20 develop a theoretical optimal D-disk sorting algorithm based on merge sort, and later they also developed an optimal algorithm based on distribution sort [20].
Reference: [20] <author> M. H. Nodine and J. S. Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <year> 1993. </year>
Reference-contexts: The number D of disks range up to 10 2 in current disk arrays. Early work on I/O algorithms concentrated on algorithms for sorting and permutation-related problems in the single disk model [1], as well as in the extended version of the I/O-model <ref> [19, 20, 30, 31] </ref>. External sorting requires fi (n log m n) I/Os, 1 which is the external memory equivalent of the well-known fi (N log N ) time bound for sorting in internal memory. <p> The external version of the segment tree is developed in section 6. Using techniques from <ref> [20] </ref> all the developed structures can be modified to work in the D-disk model|that is, the I/O bounds can be divided by D. We discuss such an extension in Section 7. <p> Note that the optimal bound results in a linear speedup in the number of disk. Nodine and Vitter [19] managed to 20 develop a theoretical optimal D-disk sorting algorithm based on merge sort, and later they also developed an optimal algorithm based on distribution sort <ref> [20] </ref>. In the latter algorithm it is assumed that 4DB M M 1=2+fi for some 0 &lt; fi &lt; 1=2, an assumption which clearly is non-restrictive in practice. <p> As mentioned Nodine and Vitter <ref> [20] </ref> developed an optimal way of doing distribution, but only when the distribution is done O ( p m)-wise. As already mentioned we can make our buffer tree work with fan-out and buffer size fi ( p instead of fi (m), and thus we can use the algorithm from [20] to <p> Vitter <ref> [20] </ref> developed an optimal way of doing distribution, but only when the distribution is done O ( p m)-wise. As already mentioned we can make our buffer tree work with fan-out and buffer size fi ( p instead of fi (m), and thus we can use the algorithm from [20] to make our structure work in the D-disk model. The external segment tree already has fan-out p m, but we still distribute elements (segments) to fi (m) multislab list. <p> Thus to make our external segment tree work on D disks we decrease the fan-out to m 1=4 , which does not change the asymptotic I/O bounds of the operations, but decreases the number of multislab lists to p m. Thus we can use the algorithm from <ref> [20] </ref> to do the distribution.
Reference: [21] <author> Y. N. Patt. </author> <title> The I/O subsystem | a candidate for improvement. Guest Editor's Introduction in IEEE Computer, </title> <booktitle> 27(3) </booktitle> <pages> 15-16, </pages> <year> 1994. </year>
Reference-contexts: This is due to the fact that communication between fast internal memory and slower external memory is the bottleneck in many large-scale computations. The significance of this bottleneck is increasing as internal computation gets faster, and especially as parallel computing gains popularity <ref> [21] </ref>. Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24].
Reference: [22] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The optimal plane-sweep algorithm (see e.g. <ref> [22] </ref>) makes a vertical sweep with a horizontal line, inserting the x coordinate of a vertical segments in a search tree when its top endpoint is reached, and deleting it again when its bottom endpoint is reached. <p> As mentioned this will enable us to solve the batched range searching and the pairwise rectangle intersection problems in the optimal number of I/Os. The segment tree <ref> [8, 22] </ref> is a well-known data structure used to maintain a dynamically changing set of segments whose endpoints belongs to a fixed set, such that given a query point all segments that contain the point can be found efficiently. Such queries are normally called stabbing queries.
Reference: [23] <author> S. Ramaswamy and S. Subramanian. </author> <title> Path caching: A technique for optimal external searching. </title> <booktitle> In Proc. ACM Symp. Principles of Database Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [24] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <year> 1994. </year>
Reference-contexts: The significance of this bottleneck is increasing as internal computation gets faster, and especially as parallel computing gains popularity [21]. Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually <ref> [24] </ref>.
Reference: [25] <author> M. Smid. </author> <title> Dynamic Data Structures on Multiple Storage Media. </title> <type> PhD thesis, </type> <institution> University of Amsterdam, </institution> <year> 1989. </year> <month> 23 </month>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [26] <author> S. Subramanian and S. Ramaswamy. </author> <title> The p-range tree: A new data structure for range searching in secondary memory. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 378-387, </pages> <year> 1995. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [27] <author> D. E. Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proc. 1994 DAGS Symposium on Parallel Computation, </booktitle> <year> 1994. </year>
Reference-contexts: Furthermore, we believe that our structures will be of practical interest due to relatively small constants in the asymptotic bounds. We hope in the future to be able to implement some of the structures in the transparent parallel I/O environment (TPIE) developed by Vengroff <ref> [27] </ref>. Results of experiments on the practical performance of several algorithms developed for the I/O model are reported in [9, 10, 28]. The main organization of the rest of this paper is the following: In the next section we 4 sketch our general technique. <p> We believe that several of our structures will be efficient in practice due to small constants in the asymptotic bounds. We hope in the future to be able to implement some of the structures in the transparent parallel I/O environment (TPIE) developed by Vengroff <ref> [27] </ref>. Acknowledgments I would like to thank all the people in the algorithms group at University of Aarhus for valuable help and inspiration.
Reference: [28] <author> D. E. Vengroff and J. S. Vitter. </author> <title> I/O-efficient scientific computation using TPIE. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Computing, </booktitle> <year> 1995. </year> <note> Appears also as Duke University Dept. of Computer Science technical report CS-1995-18. </note>
Reference-contexts: We hope in the future to be able to implement some of the structures in the transparent parallel I/O environment (TPIE) developed by Vengroff [27]. Results of experiments on the practical performance of several algorithms developed for the I/O model are reported in <ref> [9, 10, 28] </ref>. The main organization of the rest of this paper is the following: In the next section we 4 sketch our general technique. <p> Even though disk striping does not in theory achieve asymptotic optimality when D is very large, it is often the method of choice in practice for using parallel disks <ref> [28] </ref>. The non optimality of disk striping can be demonstrated via the sorting bound. While sorting N elements using disk striping and one of the the one-disk sorting algorithms requires O (n=D log m=D n) I/Os the optimal bound is O (n=D log m n) I/Os [1].
Reference: [29] <author> D. E. Vengroff and J. S. Vitter. </author> <title> Efficient 3-d range searching in external memory. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 192-201, </pages> <year> 1996. </year>
Reference-contexts: Currently, technological advances are increasing CPU speed at an annual rate of 40-60% while disk transfer rates are only increasing by 7-10% annually [24]. A lot of work has already been done on designing external memory versions of known internal-memory data structures (e.g. <ref> [6, 14, 16, 17, 18, 23, 25, 26, 29] </ref>), but practically all of these data structures are designed to be used in on-line settings, where queries should be answered immediately and within a good worst case number of I/Os.
Reference: [30] <author> J. S. Vitter. </author> <title> Efficient memory access in large-scale computation (invited paper). </title> <booktitle> In Symposium on Theoretical Aspects of Computer Science, </booktitle> <volume> LNCS 480, </volume> <pages> pages 26-41, </pages> <year> 1991. </year>
Reference-contexts: The number D of disks range up to 10 2 in current disk arrays. Early work on I/O algorithms concentrated on algorithms for sorting and permutation-related problems in the single disk model [1], as well as in the extended version of the I/O-model <ref> [19, 20, 30, 31] </ref>. External sorting requires fi (n log m n) I/Os, 1 which is the external memory equivalent of the well-known fi (N log N ) time bound for sorting in internal memory.
Reference: [31] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory, I: Two-level memories. </title> <journal> Algorithmica, </journal> <volume> 12(2-3):110-147, </volume> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Therefore, we will use n as shorthand for N=B and m for M=B. Furthermore, we say that an algorithm uses a linear number of I/O operations if it uses at most O (n) I/Os to solve a problem of size N . In <ref> [31] </ref> the I/O model is extended with a parameter D. Here the external memory is partitioned into D distinct disk drives, and if no two blocks come from the same disk, D blocks can be transferred per I/O. <p> The number D of disks range up to 10 2 in current disk arrays. Early work on I/O algorithms concentrated on algorithms for sorting and permutation-related problems in the single disk model [1], as well as in the extended version of the I/O-model <ref> [19, 20, 30, 31] </ref>. External sorting requires fi (n log m n) I/Os, 1 which is the external memory equivalent of the well-known fi (N log N ) time bound for sorting in internal memory.
References-found: 31

