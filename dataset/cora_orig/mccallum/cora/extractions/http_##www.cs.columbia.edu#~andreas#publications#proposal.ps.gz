URL: http://www.cs.columbia.edu/~andreas/publications/proposal.ps.gz
Refering-URL: http://www.cs.columbia.edu/~andreas/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Email: andreas@cs.columbia.edu  
Title: On the Management of Distributed Learning Agents  
Author: CUCS-- Andreas L. Prodromidis 
Degree: Ph.D. Thesis Proposal  
Date: July 10, 1998  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Furnkranz abd G. </author> <title> Widmer. Incremental reduced error pruning. </title> <booktitle> In Proc. 11th Intl. Conf. Mach. Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: ID3, its successor C4.5 [38], and Cart are decision tree based algorithms, Bayes, described in [16], is a naive bayesian classifier that is based on computing conditional probabilities, and Ripper [13] is a rule induction algorithm based on IREP <ref> [1] </ref>. Learning tasks Two data sets of real credit card transactions and two molecular biology sequence analysis data sets, were used in our experiments.
Reference: [2] <author> K. Ali and M. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 173-202, </pages> <year> 1996. </year>
Reference-contexts: Kwok and Carter [23] showed that ensembles with decision trees that were more syntactically diverse achieved lower error rates than ensembles consisting of less diverse decision trees. On the same subject Ali and Pazzani <ref> [2] </ref> suggested that when the average number of gain ties 1 is large, the syntactic diversity of the ensemble is greater which may lead to less correlated errors among the classifiers and hence lower error rates.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart <ref> [3] </ref>, locally weighted regression [5], MARS [20]) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An. <p> To evaluate the classifier produced by the learning program, instances from the test set are presented for classification and accuracy is measured; this is the testing phase. To ensure randomness and acquire results with confidence, we use a k-fold cross validation <ref> [3] </ref> approach, that is, we perform the experiment k different times and we average the results. Each time we use a different partitioning of the data into training and testing sets.
Reference: [4] <author> C. Brodley and T. Lane. </author> <title> Creating and exploiting coverage and diversity. In Work. </title> <booktitle> Notes AAAI-96 Workshop Integrating Multiple Learned Models, </booktitle> <pages> pages 8-14, </pages> <year> 1996. </year>
Reference-contexts: The information gain measure favors the attribute whose addition as the next split-node in a decision tree (or as the next clause to the clause body of a rule) would result in a tree (rule) that would separate into the different classes as many as examples possible. 6 Lane <ref> [4] </ref> defined as coverage the fraction of instances for which at least one of the base classifiers produces the correct prediction.
Reference: [5] <author> A. W. Moore C. G. Atkeson, S. A. Schaal. </author> <title> Locally weighted learning. </title> <journal> AI Review, </journal> <note> To Appear, </note> <year> 1997. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart [3], locally weighted regression <ref> [5] </ref>, MARS [20]) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: [6] <author> P. Chan. </author> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: According to this definition, when the value of diversity grows, the predictions from the base-classifiers are more evenly distributed (higher entropy) and, therefore, more diverse. In this study <ref> [6] </ref>, Chan examined several characteristics of the base classifiers (i.e. diversity, coverage, correlated error and specialty) and explored the effects of these characteristics on the accuracy of the various integrating meta learning schemes. <p> Class specialty: The term class specialty defines a family of evaluation metrics that concentrate on the "bias" of a classifier towards certain classes. However, in this study, instead of calculating the combined specialty of the resulting meta-classifiers <ref> [6] </ref>, the class specialty metrics focus on the specialty of each (base-) classifier for each class. A classifier specializing in one class, should exhibit, for that class, both, a high True Positive (T P ) and a low False Positive (F P ) rate. <p> Other parameters include the host of the CM, the Cross-Validation Fold, the Meta-Learning Fold, the Meta-Learning Level, the names of the local learning agent and the local meta-learning agent, etc. Refer to <ref> [6] </ref> for more information on the meaning and use of these parameters. (Notice that Marmalade has established that Strawberry and Mango are its peer Datasites, having acquired this information from the CM.) Then, Marmalade partitions the thyroid database (noted as thyroid.1.bld and thyroid.2.bld in the Data Set panel) for the 2-Cross-Validation
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Information and Knowledge Management, </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: These strategies and variations are described in more detail in <ref> [7] </ref>. Briefly, an arbiter [9] is the result of a learning algorithm that learns to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. <p> This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. The left diagram of Figure 1, depicts how the final prediction is made with input predictions from two base classifiers and a single arbiter. In the combiner <ref> [7] </ref> strategy, the predictions of the learned base classifiers on the validation set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of the meta-level training examples for the meta-learner.
Reference: [8] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: Conditional probability distributions used by Bayesian classifiers are derived from the frequency distributions of attribute values and reflect the likelihood of a certain instance belonging to a particular classification [11]. Implicit decision rules classify according to maximal probabilities. Meta-learning <ref> [8] </ref> is itself a learning process aiming to improve accuracy and efficiency. Loosely defined, meta-learning is about learning from learned knowledge.
Reference: [9] <author> P. Chan and S. Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Knowledge Discovery in Databases, </booktitle> <pages> pages 227-240, </pages> <year> 1993. </year>
Reference-contexts: These strategies and variations are described in more detail in [7]. Briefly, an arbiter <ref> [9] </ref> is the result of a learning algorithm that learns to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions.
Reference: [10] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 2-7, </pages> <year> 1996. </year>
Reference-contexts: All these strategies address the data schema integration problem and meta-learning over these models should proceed in a straightforward manner <ref> [10] </ref>. 5 Evaluation and Discussion In this chapter we describe in detail the current status of the JAM system and we present our experiments in the management of classification models.
Reference: [11] <author> P. Chesseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> Autoclass: A bayesian classification system. </title> <booktitle> In Proc. Fifth Intl. Conf. Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: Conditional probability distributions used by Bayesian classifiers are derived from the frequency distributions of attribute values and reflect the likelihood of a certain instance belonging to a particular classification <ref> [11] </ref>. Implicit decision rules classify according to maximal probabilities. Meta-learning [8] is itself a learning process aiming to improve accuracy and efficiency. Loosely defined, meta-learning is about learning from learned knowledge.
Reference: [12] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: However, both are able to compute concept y and classify unknown records (examples). Decision trees are used in ID3 [37], where each concept is represented as a conjunction of terms on a path from the root of a tree to a leaf. Rules in CN2 <ref> [12] </ref> are if-then expressions, where the antecedent is a pattern expression and the consequent is a class label. Each version space learned in VS [29] defines the most general and specific description boundaries of a concept using a restricted version of first order formulae.
Reference: [13] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5 [38], and Cart are decision tree based algorithms, Bayes, described in [16], is a naive bayesian classifier that is based on computing conditional probabilities, and Ripper <ref> [13] </ref> is a rule induction algorithm based on IREP [1]. Learning tasks Two data sets of real credit card transactions and two molecular biology sequence analysis data sets, were used in our experiments.
Reference: [14] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: The learning and meta-learning agents are designed as objects. J AM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart, Bayes [15], Wpebls <ref> [14] </ref>, CN2, etc.) is then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [15] <author> R. Duda and P. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: The learning and meta-learning agents are designed as objects. J AM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart, Bayes <ref> [15] </ref>, Wpebls [14], CN2, etc.) is then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: [16] <author> C. </author> <type> Elkan. </type> <institution> Boosting and naive bayesian learning [http://www-cse.ucsd.edu/~elkan/papers/bnb.ps]. Department of Computer Science and Engineering, Univ. of California, </institution> <address> San Diego, CA, </address> <year> 1997. </year>
Reference-contexts: Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5 [38], and Cart are decision tree based algorithms, Bayes, described in <ref> [16] </ref>, is a naive bayesian classifier that is based on computing conditional probabilities, and Ripper [13] is a rule induction algorithm based on IREP [1]. Learning tasks Two data sets of real credit card transactions and two molecular biology sequence analysis data sets, were used in our experiments.
Reference: [17] <author> E.R.Carson and U.Fischer. </author> <title> Models and computers in diabetes research and diabetes care. Computer methods and programs in biomedicine, </title> <journal> special issue, </journal> <volume> 32, </volume> <year> 1990. </year> <month> 34 </month>
Reference-contexts: Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value [31]. Machine-learning algorithms have been deployed in heart disease diagnosis [39], in predicting glucose levels for diabetic patients <ref> [17] </ref>, in detecting credit card fraud [41], in steering vehicles driving autonomously on public highways at 70 miles an hour [33], in predicting stock option pricing [32], in computing customizing electronic newspapers [21] etc.
Reference: [18] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Symth. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <journal> Comm. ACM, </journal> <volume> 39(11) </volume> <pages> 27-34, </pages> <year> 1996. </year>
Reference-contexts: attribute vectors. 3 Overview of Proposed Research Knowledge discovery in databases (KDD) is an emerging field that spans across several areas such as databases, data mining, machine learning, statistics, data visualization, summarization, distributed systems and high performance computing and refers to the overall process of discovering useful knowledge from data <ref> [18] </ref>. <p> Right: A ID3 tree-structured classifier is being displayed in the Classifier Visualization Panel Specifically, J AM provides graph drawing tools to help users understand the learned knowledge <ref> [18] </ref>. There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In J AM we have employed major components of Grappa [24], an extensible visualization system, that displays the classifier and allows the user to analyze the graph.
Reference: [19] <author> Y. Freund and R. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proc. Thirteenth Conf. Machine Learning, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: The results strengthened the belief that larger accuracy improvement can actually be achieved by employing more diverse base classifiers with higher coverage and fewer correlated errors. Selecting models Margineantu and Dietterich [26] study the problem of selecting a subset of the hypothesis (classifiers) obtained by the boosting algorithm ADABOOST <ref> [19] </ref>. In essense, boosting learns a set of classifiers where each classifier concentrates on the examples of the training set misclassified by its predecessors.
Reference: [20] <author> J.H.Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19(1) </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: To be more specific, by deploying regression methods (e.g. Cart [3], locally weighted regression [5], MARS <ref> [20] </ref>) for continuous attributes and machine learning algorithms for categorical attributes, data site A can compute one or more auxiliary classifier agents C Aj 0 that predict the value of attribute A n+1 based on the common attributes A 1 ; :::; An.
Reference: [21] <author> K.Lang. </author> <title> News weeder: Learning to filter net news. </title> <editor> In A.Prieditis and S.Russel, editors, </editor> <booktitle> Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 331-339. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Machine-learning algorithms have been deployed in heart disease diagnosis [39], in predicting glucose levels for diabetic patients [17], in detecting credit card fraud [41], in steering vehicles driving autonomously on public highways at 70 miles an hour [33], in predicting stock option pricing [32], in computing customizing electronic newspapers <ref> [21] </ref> etc. Many large business institutions and market analysis firms attempt to distinguish the low-risk (high profit) potential customers by learn simple categorical classifications of their potential customer data base.
Reference: [22] <author> M. Kubat and S. Matwin. </author> <title> Addressing the curse of imbalanced training sets: One-sided selection. </title> <booktitle> In Proc. 14th Intl. Conf. Machine Learning, </booktitle> <pages> pages 179-186, </pages> <year> 1997. </year>
Reference-contexts: In our case, AS (C j ) has high values when classifier C j performs relatively well on all c classes. A highly specialized classifier, on the other hand, exhibits lower AS (C j ) values. This metric can prove very useful with skewed data sets <ref> [22] </ref> in which some classes appear much more frequently that others. In this cases, the aggregate specialty metric distinguishes the classifiers that can focus on the sparse examples.
Reference: [23] <author> S. Kwok and C. Carter. </author> <title> Multiple decision trees. </title> <booktitle> In Uncertainty in Aritificial Intelligence 4, </booktitle> <pages> pages 327-335, </pages> <year> 1990. </year>
Reference-contexts: Evaluation and comparison metrics Several researchers from the Machine Learning and KDD communities have studied and defined metrics for the evaluation of ensembles (groups) of classifiers. Kwok and Carter <ref> [23] </ref> showed that ensembles with decision trees that were more syntactically diverse achieved lower error rates than ensembles consisting of less diverse decision trees.
Reference: [24] <author> Wenke Lee, Naser S. Barghouti, and John Moccenigo. Grappa: </author> <title> Graph package in java. In Graph Drawing, </title> <address> Rome, Italy, Rome, Italy, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In J AM we have employed major components of Grappa <ref> [24] </ref>, an extensible visualization system, that displays the classifier and allows the user to analyze the graph.
Reference: [25] <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 5(2) </volume> <pages> 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Each version space learned in VS [29] defines the most general and specific description boundaries of a concept using a restricted version of first order formulae. Neural networks compute separating hyperplanes in n-dimensional feature space to classify data <ref> [25] </ref>. The learned distance functions in exemplar-based learning algorithms (or nearest neighbor algorithms) define a similarity or "closeness" measure between two instances [40].
Reference: [26] <author> D. Margineantu and T. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Proc. Fourteenth Intl. Conf. Machine Learning, </booktitle> <pages> pages 211-218, </pages> <year> 1997. </year>
Reference-contexts: The results strengthened the belief that larger accuracy improvement can actually be achieved by employing more diverse base classifiers with higher coverage and fewer correlated errors. Selecting models Margineantu and Dietterich <ref> [26] </ref> study the problem of selecting a subset of the hypothesis (classifiers) obtained by the boosting algorithm ADABOOST [19]. In essense, boosting learns a set of classifiers where each classifier concentrates on the examples of the training set misclassified by its predecessors.
Reference: [27] <author> C. Merz and P. Murphy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html]. Dept. </title> <institution> of Info. and Computer Sci., Univ. of California, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: The credit card data sets were provided by the Chase and First Union Banks, members of FSTC (Financial Services Technology Consortium) and the molecular biology sequences were obtained from the UCI Machine Learning repository <ref> [27] </ref>. The first two data sets contained credit card transactions labelled as fraudulent or legitimate. Each bank supplied .5 million records spanning one year.
Reference: [28] <author> R. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 83-134. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Many organizations seeking similar added value from their data are already dealing with overwhelming amounts of global information that in time will likely grow in size faster than available improvements in machine resources. Inductive learning (or learning from examples <ref> [28] </ref>) is the task of identifying regularities in some given set of training examples with little or no knowledge about the domain from which the examples are drawn.
Reference: [29] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Rules in CN2 [12] are if-then expressions, where the antecedent is a pattern expression and the consequent is a class label. Each version space learned in VS <ref> [29] </ref> defines the most general and specific description boundaries of a concept using a restricted version of first order formulae. Neural networks compute separating hyperplanes in n-dimensional feature space to classify data [25]. <p> Meta-learning improves accuracy by combining different learning systems each having different inductive bias (e.g representation, search heuristics, search space) <ref> [29] </ref>. Furthermore, by combining separately learned concepts, meta-learning is expected to derive a higher level learned model that explains a large database more accurately than any of the individual learners.
Reference: [30] <author> Tom Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: 1 Introduction Learning, in general, denotes the ability to acquire knowledge, skills or behavioral tendencies on one or more domains through experience, study or instruction. In machine learning <ref> [30] </ref>, a computer program is said to learn with respect to a class of tasks if its performance on these tasks, as measured by some performance measure, improves with its experience and interactions with its environment.
Reference: [31] <author> Tom M. Mitchell. </author> <title> Does machine learning really work? AI Magazine, </title> <booktitle> 18(3) </booktitle> <pages> 11-20, </pages> <year> 1997. </year>
Reference-contexts: Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value <ref> [31] </ref>.
Reference: [32] <author> M.Malliaris and L.Salchenberger. </author> <title> A neural network model for estimating option prices. </title> <journal> Applied Intelligence, </journal> <volume> 3(3) </volume> <pages> 193-206, </pages> <year> 1993. </year>
Reference-contexts: Machine-learning algorithms have been deployed in heart disease diagnosis [39], in predicting glucose levels for diabetic patients [17], in detecting credit card fraud [41], in steering vehicles driving autonomously on public highways at 70 miles an hour [33], in predicting stock option pricing <ref> [32] </ref>, in computing customizing electronic newspapers [21] etc. Many large business institutions and market analysis firms attempt to distinguish the low-risk (high profit) potential customers by learn simple categorical classifications of their potential customer data base.
Reference: [33] <author> D. Pomerleau. </author> <title> Neural network perception for mobile robot guidance. </title> <type> PhD thesis, </type> <institution> School of Computer Sci., Carnegie Mellon Univ., </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year> <type> (Tech. Rep. </type> <institution> CMU-CS-92-115). </institution>
Reference-contexts: Machine-learning algorithms have been deployed in heart disease diagnosis [39], in predicting glucose levels for diabetic patients [17], in detecting credit card fraud [41], in steering vehicles driving autonomously on public highways at 70 miles an hour <ref> [33] </ref>, in predicting stock option pricing [32], in computing customizing electronic newspapers [21] etc. Many large business institutions and market analysis firms attempt to distinguish the low-risk (high profit) potential customers by learn simple categorical classifications of their potential customer data base.
Reference: [34] <author> F. Provost and T. Fawcett. </author> <title> Anaylysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proc. Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 43-48, </pages> <year> 1997. </year>
Reference-contexts: We study the more general setting where classifiers can be obtained by training (possibly) different learning algorithms over (possibly) distinct databases. Furthermore, instead of voting over the predictions of classifiers for the final classification, we adopt meta-learning to discover the importance of the individual classifiers. Provost and Fawcett in <ref> [34] </ref> introduce the ROC convex hull method as a means to manage, analyze and compare classifiers.
Reference: [35] <author> F. Provost and V. Kolluri. </author> <title> Scaling up inductive algorithms: An overview. </title> <booktitle> In Proc. Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 239-242, </pages> <year> 1997. </year> <month> 35 </month>
Reference-contexts: On the other hand, the literature is quite rich of methods that facilitate the use of inductive learning algorithms for mining very large databases. In fact, Provost and Kolluri <ref> [35] </ref> have conducted a survey on the available methods and have categorized them into tree main groups; the methods that rely on the design of fast algorithms, the methods that reduce the problem size by partitioning the data and the methods that employ a relational representation.
Reference: [36] <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol., </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year>
Reference-contexts: Some of the fields are arithmetic and the rest categorical, i.e. numbers were used to represent a few discrete categories. The secondary protein structure data set (SS) <ref> [36] </ref>, courtesy of Qian and Sejnowski, contains 21,625 sequences of amino acids and secondary structures at the corresponding positions. There are three structures (classes) and 20 amino acids (21 attributes because of a spacer [36]) in the data. <p> The secondary protein structure data set (SS) <ref> [36] </ref>, courtesy of Qian and Sejnowski, contains 21,625 sequences of amino acids and secondary structures at the corresponding positions. There are three structures (classes) and 20 amino acids (21 attributes because of a spacer [36]) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [36]. There is one such sequence per example. <p> There are three structures (classes) and 20 amino acids (21 attributes because of a spacer <ref> [36] </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [36]. There is one such sequence per example. The DNA splice junction data set (SJ) [44], courtesy of Towell, Shavlik and Noordewier, contains 3,190 sequences of nucleotides and the type of splice junction, if any, at the center of each sequence (three classes).
Reference: [37] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: For example, decision trees are declarative and thus more comprehensible to humans than weights computed within a neural network architecture. However, both are able to compute concept y and classify unknown records (examples). Decision trees are used in ID3 <ref> [37] </ref>, where each concept is represented as a conjunction of terms on a path from the root of a tree to a leaf. Rules in CN2 [12] are if-then expressions, where the antecedent is a pattern expression and the consequent is a class label.
Reference: [38] <author> J. R. Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The entire data set is divided into k randomly chosen subsets, and each of which is used in turn as the test set while the rest form the training set. Learning algorithms Five inductive learning algorithms are used in our experiments. ID3, its successor C4.5 <ref> [38] </ref>, and Cart are decision tree based algorithms, Bayes, described in [16], is a naive bayesian classifier that is based on computing conditional probabilities, and Ripper [13] is a rule induction algorithm based on IREP [1].
Reference: [39] <author> R.Detrano, A.Janosi, W.Steinbrunn, M.Pfisterer, J.Schmid, S.Sandhu, K.Guppy, S.Lee, and V.Froelicher. </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310, </pages> <year> 1989. </year>
Reference-contexts: Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value [31]. Machine-learning algorithms have been deployed in heart disease diagnosis <ref> [39] </ref>, in predicting glucose levels for diabetic patients [17], in detecting credit card fraud [41], in steering vehicles driving autonomously on public highways at 70 miles an hour [33], in predicting stock option pricing [32], in computing customizing electronic newspapers [21] etc.
Reference: [40] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Comm. ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: Neural networks compute separating hyperplanes in n-dimensional feature space to classify data [25]. The learned distance functions in exemplar-based learning algorithms (or nearest neighbor algorithms) define a similarity or "closeness" measure between two instances <ref> [40] </ref>. Conditional probability distributions used by Bayesian classifiers are derived from the frequency distributions of attribute values and reflect the likelihood of a certain instance belonging to a particular classification [11]. Implicit decision rules classify according to maximal probabilities.
Reference: [41] <author> S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan. </author> <title> Credit card fraud detection using meta-learning: Issues and initial results. </title> <booktitle> Working notes of AAAI Workshop on AI Approaches to Fraud Detection and Risk Management, </booktitle> <year> 1997. </year>
Reference-contexts: Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value [31]. Machine-learning algorithms have been deployed in heart disease diagnosis [39], in predicting glucose levels for diabetic patients [17], in detecting credit card fraud <ref> [41] </ref>, in steering vehicles driving autonomously on public highways at 70 miles an hour [33], in predicting stock option pricing [32], in computing customizing electronic newspapers [21] etc. <p> Other possible scenarios involve blending general classifiers (classifiers with high aggregate specialties) and highly specialized classifiers, or general classifiers that also exhibit high diversity. In another study <ref> [41] </ref> concerning credit card fraud detection the authors employ evaluation formulas for selecting classifiers that are based on characteristics such as diversity, coverage and correlated error or their combinations, i.e. True Positive rate and diversity.
Reference: [42] <author> S. Stolfo, W.D. Fan, A. Prodromidis W.Lee, S. Tselepis, and P. K. Chan. </author> <title> Agent-based fraud and intrusion detection in financial information systems. </title> <booktitle> Submitted to IEEE Symposium on Security and Privacy, </booktitle> <year> 1998. </year>
Reference-contexts: The schemas of the databases was developed over years of experience and continuous analysis by bank personnel to capture important information for fraud detection. We cannot reveal the details of the schema beyond what is described in <ref> [42] </ref>. The records have a fixed length of 137 bytes each and about 20 30 numeric attributes including the binary class label (fraud/legitimate transaction). Some of the fields are arithmetic and the rest categorical, i.e. numbers were used to represent a few discrete categories.
Reference: [43] <author> S. Stolfo, A. Prodromidis, S. Tselepis, W. Lee, W. Fan, and P. Chan. </author> <title> JAM: Java agents for meta-learning over distributed databases. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 74-81, </pages> <year> 1997. </year>
Reference-contexts: The platform-independence 5 of JAVA technology makes it easy to port J AM and delegate its agents to any participating site. (The modules that are implemented in native C++ are not yet platform independent.) For more details on the J AM system refer to <ref> [43] </ref>. Next we describe the various components of our distributed data mining system. 5.1.1 Configuration Manager The CM assumes a role equivalent to that of a name server of a network system.
Reference: [44] <author> G. Towell, J. Shavlik, and M. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [36]. There is one such sequence per example. The DNA splice junction data set (SJ) <ref> [44] </ref>, courtesy of Towell, Shavlik and Noordewier, contains 3,190 sequences of nucleotides and the type of splice junction, if any, at the center of each sequence (three classes).
Reference: [45] <author> P. Utgoff. ID5: </author> <title> An incremental ID3. </title> <booktitle> In Proc. 5th Intl. Conf. Mach. Learning, </booktitle> <pages> pages 107-120. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <month> 36 </month>
Reference-contexts: First, learning programs do not scale very well with large databases and second, the main memory requirement by the majority of learning programs poses a physical limitation to the size of the training databases. A second alternative would be to employ incremental machine learning programs, (e.g. ID5 <ref> [45] </ref>, an incremental version of ID3) i.e. machine learning programs that are not constrained to retain all training examples in main memory. The classifiers C k initially trained over DB i can be updated later by resuming their training on the new database 16 DB j once it becomes available.
References-found: 45

