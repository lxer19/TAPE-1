URL: http://www.cs.unm.edu/~bap/papers/icgi98.ps.gz
Refering-URL: http://www.cs.unm.edu/stats/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: kevin@research.nj.nec.com  bap@cs.unm.edu  rod@emtex.com  
Phone: 2  
Title: State Merging Algorithm  
Author: Kevin J. Lang Barak A. Pearlmutter and Rodney A. Price 
Address: New Mexico, Albuquerque, NM 87131,  3 Emtex, Milton Keynes, England,  
Affiliation: Comp Sci Dept, FEC 313, Univ of  
Note: Competition and a New Evidence-Driven  Part I: Abbadingo  
Abstract: Results of the Abbadingo One DFA Learning Abstract. This paper first describes the structure and results of the Abbadingo One DFA Learning Competition. The competition was designed to encourage work on algorithms that scale wellboth to larger DFAs and to sparser training data. We then describe and discuss the winning algorithm of Rodney Price, which orders state merges according to the amount of evidence in their favor. A second winning algorithm, of Hugues Juille, will be described in a separate paper.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Trakhtenbrot and Ya. Barzdin'. </author> <title> (1973) Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland Publishing Company, Amsterdam. </publisher>
Reference-contexts: However, DFA learning does not seem to be so hard in the average case. <ref> [1] </ref> proved that a simple state merging algorithm is guaranteed to find the smallest DFA consistent with a complete training set consisting of all strings out to a given length. [6] showed empirically that this same algorithm can often construct an approximately correct hypothesis from a sparse subset of a complete <p> Lang, Barak A. Pearlmutter, and Rodney A. Price Part II: Evidence driven state merging 7 Background A simple and effective method for DFA induction from positive and negative examples is the state merging method <ref> [1, 6, 7] </ref>. This method starts with the prefix tree acceptor for the training set and folds it up into a compact hypothesis by merging compatible pairs of states. 5 Two states are compatible when no suffix leads from them to differing labels.
Reference: 2. <author> D. Angluin. </author> <title> (1978) On the Complexity of Minimum Inference of Regular Sets. </title> <journal> Information and Control, </journal> <volume> Vol. 39, </volume> <pages> pp. 337-350. </pages>
Reference: 3. <author> L. Veelenturf. </author> <title> (1978) Inference of Sequential Machines from Sample Computations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 27, </volume> <pages> pp. 167-170. </pages>
Reference: 4. <author> M. Kearns and L. Valiant. </author> <title> (1989) Cryptographic Limitations on Learning Boolean Formulae and Finite Automata. </title> <publisher> STOC-89. </publisher>
Reference-contexts: problems. 1.2 Some history DFA learning can be very hard in the worst case. [5] proved that it is NP-hard to find a DFA that is consistent with a given set of training strings and whose size is within a polynomial factor of the size of the smallest such DFA. <ref> [4] </ref> proved that predicting the output of a DFA can be as hard as breaking cryptosystems widely believed secure.
Reference: 5. <author> L. Pitt and M. Warmuth. </author> <title> (1989) The Minimum DFA Consistency Problem Cannot be Approximated Within any Polynomial. </title> <publisher> STOC-89. </publisher>
Reference-contexts: Training set sizes for the Abbadingo One competition problems. 1.2 Some history DFA learning can be very hard in the worst case. <ref> [5] </ref> proved that it is NP-hard to find a DFA that is consistent with a given set of training strings and whose size is within a polynomial factor of the size of the smallest such DFA. [4] proved that predicting the output of a DFA can be as hard as breaking
Reference: 6. <author> Kevin J. Lang. </author> <title> Random DFA's can be Approximately Learned from Sparse Uniform Examples. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pp 45-52, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, DFA learning does not seem to be so hard in the average case. [1] proved that a simple state merging algorithm is guaranteed to find the smallest DFA consistent with a complete training set consisting of all strings out to a given length. <ref> [6] </ref> showed empirically that this same algorithm can often construct an approximately correct hypothesis from a sparse subset of a complete training set, when both the target concept and training sets are randomly chosen from uniform distributions. [8] proved the approximate learnability of DFA's with worst-case graph structure and randomly labeled <p> The upper bound U was determined by visually inspecting the learning curves for the Trakhtenbrot-Barzdin algorithm which appeared in <ref> [6] </ref>. In addition, some rounding was performed on the training set sizes for targets of size 512. <p> Lang, Barak A. Pearlmutter, and Rodney A. Price Part II: Evidence driven state merging 7 Background A simple and effective method for DFA induction from positive and negative examples is the state merging method <ref> [1, 6, 7] </ref>. This method starts with the prefix tree acceptor for the training set and folds it up into a compact hypothesis by merging compatible pairs of states. 5 Two states are compatible when no suffix leads from them to differing labels. <p> Because there is a snowballing of right or wrong decisions, it is critically important for the algorithm's early decisions to be correct, and hence a good strategy is to first perform those merges that are supported by the most evidence. <ref> [6] </ref> claimed that this consideration supported the choice of breadth-first order for candidate merges, because then the earliest merges must survive the comparison of the largest trees of suffixes. [10] suggested that a better strategy is to look at the training data and perform merges exactly in order of the amount <p> Much previous work has employed a restriction of this type, including the papers of <ref> [6, 7, 10] </ref>; and the Abbadingo competition programs of Price and Juille. Note that the restriction shrinks the pool of merge candidates, so it increases the failure rate of the algorithm as compared to the unrestricted algorithm of section 9. However, the idea is well worth describing.
Reference: 7. <author> J. Oncina and P. Garcia. </author> <title> Inferring Regular Languages in Polynomial Updated Time. </title> <booktitle> In Pattern Recognition and Image Analysis. </booktitle> <pages> pp. 49-61, </pages> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: Lang, Barak A. Pearlmutter, and Rodney A. Price Part II: Evidence driven state merging 7 Background A simple and effective method for DFA induction from positive and negative examples is the state merging method <ref> [1, 6, 7] </ref>. This method starts with the prefix tree acceptor for the training set and folds it up into a compact hypothesis by merging compatible pairs of states. 5 Two states are compatible when no suffix leads from them to differing labels. <p> Much previous work has employed a restriction of this type, including the papers of <ref> [6, 7, 10] </ref>; and the Abbadingo competition programs of Price and Juille. Note that the restriction shrinks the pool of merge candidates, so it increases the failure rate of the algorithm as compared to the unrestricted algorithm of section 9. However, the idea is well worth describing.
Reference: 8. <author> Yoav Freund, Michael Kearns, Dana Ron, Ronitt Rubinfeld, Robert Schapire, and Linda Sellie. </author> <title> Efficient Learning of Typical Finite Automata from Random Walks, </title> <booktitle> STOC-93, </booktitle> <pages> pp. 315-324. </pages>
Reference-contexts: a complete training set consisting of all strings out to a given length. [6] showed empirically that this same algorithm can often construct an approximately correct hypothesis from a sparse subset of a complete training set, when both the target concept and training sets are randomly chosen from uniform distributions. <ref> [8] </ref> proved the approximate learnability of DFA's with worst-case graph structure and randomly labeled states, from randomly chosen training strings. 1 We note that many papers have been published on the application of generic methods such as neural networks and genetic search to the problem of DFA learning. <p> A training set for a target of nominal size n consisted of a random sample drawn without replacement from a uniform distribution over the collection of 16n 2 1 binary 1 The <ref> [8] </ref> theorem concerns a slightly different protocol, in which the learner sees the label of every state that is encountered rather than just the label of the final state. 2 By analogy to trees, the depth of a DFA is maximum over all nodes x of the length of the shortest
Reference: 9. <author> P. Dupont, L. Miclet, and E. Vidal. </author> <title> What is the search space of the regular inference? In Proceedings of the International Colloquium on Grammatical Inference ICGA-94, </title> <booktitle> Lecture Notes in Artificial Intelligence 862, </booktitle> <pages> pp. 25-37, </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: 10. <author> C. de la Higuera, J. Oncina, and E. Vidal. </author> <title> Identification of DFA: Data-Dependent Versus Data-Independent Algorithms. </title> <booktitle> In Proceedings of the International Colloquium on Grammatical Inference ICGA-96 Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> pp. 313-325, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: to be correct, and hence a good strategy is to first perform those merges that are supported by the most evidence. [6] claimed that this consideration supported the choice of breadth-first order for candidate merges, because then the earliest merges must survive the comparison of the largest trees of suffixes. <ref> [10] </ref> suggested that a better strategy is to look at the training data and perform merges exactly in order of the amount of evidence, rather than in a predetermined order that hopefully correlates with that quantity. While this is a very good point, the actual algorithm described in [10] does not <p> of suffixes. <ref> [10] </ref> suggested that a better strategy is to look at the training data and perform merges exactly in order of the amount of evidence, rather than in a predetermined order that hopefully correlates with that quantity. While this is a very good point, the actual algorithm described in [10] does not work well on the Abbadingo challenge problems due to a couple of flaws. One was a mistake in the algorithm's control strategy that will be described in section 10. <p> Much previous work has employed a restriction of this type, including the papers of <ref> [6, 7, 10] </ref>; and the Abbadingo competition programs of Price and Juille. Note that the restriction shrinks the pool of merge candidates, so it increases the failure rate of the algorithm as compared to the unrestricted algorithm of section 9. However, the idea is well worth describing. <p> Otherwise (if no blue node is promoteable), perform the highest scoring red/blue merge that we know about, then goto step 1. 4. Halt. Note that the algorithm of <ref> [10] </ref> has the priority of steps 2 and 3 reversed, which drastically reduces its effectiveness. 7 It is important to not start merging until many merge candidates have accumulated, so that one with a high score is likely to be available. 11 A comparison of two EDSM implementations We have described
Reference: 11. <author> Joe Kilian and Kevin J. Lang. </author> <title> (1997) A Scheme for Secure Pass-Fail Tests. </title> <note> NECI Technical Note 97-016N. </note>
Reference-contexts: Thanks to a new cryptographic technique of Joe Kilian's, the testing oracle was implemented without storing the answers anywhere online <ref> [11] </ref>. This reduced the temptation to break into the Abbadingo web server. 3.3 Additional rules Two rules governed the selection of competition winners. The priority rule stated that the first person to solve a problem (to 99% accuracy) would get the credit for solving it. 4 Kevin J.
Reference: 12. <author> Hugues Juille and Jordan B. Pollack. </author> <title> (1998) SAGE: a Sampling-based Heuristic for Tree Search. </title> <note> Submitted to Machine Learning. </note>
Reference-contexts: Merge a blue node with a red node. 6 This basic framework of invariants and actions can be turned into different algorithms of widely varying performance, depending on the details of the policy for choosing which action to perform when. A particularly good policy is described in <ref> [12] </ref>: 6 Note that the last two actions might also require some white nodes to be recolored blue. 10 Kevin J. Lang, Barak A. Pearlmutter, and Rodney A. <p> Table 4 shows that this combined approach works better than either program alone. In fact, the combined performance level is well into the range reported by <ref> [12] </ref> for the search-intensive SAGE system. 12 Notes on run time The run time of Trakhtenbrot-Barzdin is upper bounded by P H 2 , where P is the size of the inital PTA, and H is the number of nodes in the final hypothesis. <p> We recommend that anyone faced with a DFA learning task give this algorithm a try. Acknowledgements We thank Hugues Juille for sending us code and an early draft of <ref> [12] </ref>, which is the source of the blue-fringe control policy described in section 10. 9 This is due to randomness in the training data and the fact that even high scoring merges can be wrong. 12 Kevin J. Lang, Barak A. Pearlmutter, and Rodney A.
References-found: 12

