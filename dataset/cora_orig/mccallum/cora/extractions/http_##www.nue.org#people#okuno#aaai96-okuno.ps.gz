URL: http://www.nue.org/people/okuno/aaai96-okuno.ps.gz
Refering-URL: http://www.aaai.org/Press/Proceedings/AAAI/1996/aaai96.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: okuno@nue.org nakatani@horn.brl.ntt.jp kaw@idea.brl.ntt.jp  
Title: Interfacing Sound Stream Segregation to  
Author: Hiroshi G. Okuno, Tomohiro Nakatani and Takeshi Kawabata 
Address: 3-1 Morinosato-Wakamiya, Atsugi, Kanagawa 243-01, JAPAN  
Affiliation: NTT Basic Research Laboratories Nippon Telegraph and Telephone Corporation  
Abstract: Automatic Speech Recognition | Preliminary Results on Abstract This paper reports the preliminary results of experiments on listening to several sounds at once. Two issues are addressed: segregating speech streams from a mixture of sounds, and interfacing speech stream segregation with automatic speech recognition (ASR). Speech stream segregation (SSS) is modeled as a process of extracting harmonic fragments, grouping these extracted harmonic fragments, and substituting some sounds for non-harmonic parts of groups. This system is implemented by extending the harmonic-based stream segregation system reported at AAAI-94 and IJCAI-95. The main problem in interfacing SSS with HMM-based ASR is how to improve the recognition performance which is degraded by spectral distortion of segregated sounds caused mainly by the binaural input, grouping, and residue substitution. Our solution is to re-train the parameters of the HMM with training data binauralized for four directions, to group harmonic fragments according to their directions, and to substitute the residue of harmonic fragments for non-harmonic parts of each group. Experiments with 500 mixtures of two women's utterances of a word showed that the cumulative accuracy of word recognition up to the 10th candidate of each woman's utterance is, on average, 75%. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blauert, J. </author> <year> 1983. </year> <title> Spatial Hearing: the Psychophysics of Human Sound Localization. </title> <publisher> MIT Press. </publisher>
Reference-contexts: However, the usage of binaural inputs may cause spectral distortion, because the spectrum of a binaural input is not the same as that of the original sound due to the shape of the human head. Such transformation is called the head-related transfer function (HRTF) <ref> (Blauert 1983) </ref>. Due to the HRTF, the power of lower frequencies is usually decreased while that of higher frequencies is increased. Thus, it may make it difficulty to segregate a person's speech. The literature mentioned above did not examine this possibility.
Reference: <author> Bodden, M. </author> <year> 1993. </year> <title> Modeling human sound-source localization and the cocktail-party-effect. </title> <journal> Acta Acustica 1 </journal> <pages> 43-55. </pages>
Reference: <author> Bregman, A.S. </author> <year> 1990. </year> <title> Auditory Scene Analysis the Perceptual Organization of Sound. </title> <publisher> MIT Press. </publisher>
Reference-contexts: This capability is known as the cocktail party effect (Cherry 1953). Perceptual segregation of sounds, called auditory scene analysis, has been studied by psychoacoustic researchers for more than forty years. Although many observations have been analyzed and reported <ref> (Bregman 1990) </ref>, it is only recently that researchers have begun to use computer modeling of auditory scene analysis (Cooke et al. 1993; Green et al. 1995; Nakatani et al. 1994).
Reference: <author> Brown, G.J. </author> <year> 1992. </year> <title> Computational auditory scene analysis: A representational approach. </title> <type> Ph.D diss., </type> <institution> Dept. of Computer Science, University of Sheffield. </institution>
Reference-contexts: Sound Stream Segregation Sound segregation should be incremental, because CASA is used as a front-end system for ASR systems and other applications that should run in real time. Many representations of a sound have been proposed, for example, auditory maps <ref> (Brown 1992) </ref> and synchrony strands (Cooke et al. 1993), but most of them are unsuitable for incremental processing. Nakatani and Okuno proposed using a sound stream (or simply stream) to represent a sound (Nakatani et al. 1994).
Reference: <author> Cherry, E.C. </author> <year> 1953. </year> <title> Some experiments on the recognition of speech, with one and with two ears. </title> <journal> Journal of Acoustic Society of America 25 </journal> <pages> 975-979. </pages>
Reference-contexts: Introduction Usually, people hear a mixture of sounds, and people with normal hearing can segregate sounds from the mixture and focus on a particular voice or sound in a noisy environment. This capability is known as the cocktail party effect <ref> (Cherry 1953) </ref>. Perceptual segregation of sounds, called auditory scene analysis, has been studied by psychoacoustic researchers for more than forty years.
Reference: <author> Cooke, M.P.; Brown, G.J.; Crawford, M.; and Green, P. </author> <year> 1993. </year> <title> Computational Auditory Scene Analysis: listening to several things at once. </title> <journal> Endeavour, </journal> <volume> 17(4) </volume> <pages> 186-190. </pages>
Reference-contexts: Sound Stream Segregation Sound segregation should be incremental, because CASA is used as a front-end system for ASR systems and other applications that should run in real time. Many representations of a sound have been proposed, for example, auditory maps (Brown 1992) and synchrony strands <ref> (Cooke et al. 1993) </ref>, but most of them are unsuitable for incremental processing. Nakatani and Okuno proposed using a sound stream (or simply stream) to represent a sound (Nakatani et al. 1994). A sound stream is a group of sound components that have some consistent attributes.
Reference: <author> Handel, S. </author> <year> 1989. </year> <title> Listening An Introduction to the Perception of Auditory Events. </title> <publisher> MIT Press. </publisher>
Reference-contexts: However, the top-down approach is also needed for CASA, because a human listener's knowledge and experience plays an essential role in listening and understanding <ref> (Handel 1989) </ref>. (8) To integrate bottom-up and top-down processes, system architecture is essential. The HBSS and Bi-HBSS systems are modeled on the residue-driven architecture with multi-agent systems. These systems can be extended for such integration by using subsumption architecture (Nakatani et al. 1994).
Reference: <author> Hansen, J.H.L.; Mammone, R.J.; and Young, S. </author> <year> 1994. </year> <title> Editorial for the special issue of the IEEE transactions on speech and audio processing on robust speech processing". </title> <booktitle> Transactions on Speech and Audio Processing 2(4) </booktitle> <pages> 549-550. </pages>
Reference-contexts: At present, one of the hottest topics of ASR research is how to make more robust ASR systems that perform well outside laboratory conditions <ref> (Hansen et al. 1994) </ref>. Usually the approaches taken are to reduce noise and use speaker adaptation, and treat sounds other than human voices as noise. CASA takes an opposite approach. First, it deals with the problems of handling general sounds to develop methods and technologies.
Reference: <author> Green, </author> <title> P.D.; Cooke, M.P.; and Crawford, </title> <address> M.D. </address> <year> 1995. </year> <title> Auditory Scene Analysis and Hidden Markov Model Recognition of Speech in Noise. </title> <booktitle> In Proceedings of 1995 International Conference on Acoustics, Speech and Signal Processing, Vol.1:401-404, IEEE. </booktitle>
Reference: <author> Kita, K.; Kawabata, T.; and Shikano, H. </author> <year> 1990. </year> <title> HMM continuous speech recognition using generalized LR parsing. </title> <journal> Transactions of the Information Processing Society of Japan 31(3) </journal> <pages> 472-480. </pages>
Reference-contexts: Influence of SSS on ASR In this section, we assess the effect of segregation on ASR and propose methods to reduce this effect. The ASR system used in this paper The "HMM-LR" developed by ATR Inc. <ref> (Kita et al. 1990) </ref> is used system in this paper. The HMM-LR is a continuous speech recognition system that uses generalized LR parsing with a single discrete codebook. The size of the codebook is 256 and it was created from a set of standard data.
Reference: <author> Lesser, V.; Nawab, S.H.; Gallastegi, I.; and Klassner, F. </author> <year> 1993. </year> <title> IPUS: An Architecture for Integrated Signal Processing and Signal Interpretation in Complex Environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 249-255. </pages>
Reference: <author> Minami, Y, and Furui, S. </author> <year> 1995. </year> <title> A Maximum Likelihood Procedure for A Universal Adaptation Method based on HMM Composition. </title> <booktitle> In Proceedings of 1995 International Conference on Acoustics, Speech and Signal Processing, Vol.1:129-132, IEEE. </booktitle>
Reference: <author> Nakatani, T.; Okuno, H.G.; and Kawabata, T. </author> <year> 1994. </year> <title> Auditory Stream Segregation in Auditory Scene Analysis with a Multi-Agent System. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 100-107, </pages> <publisher> AAAI. </publisher>
Reference-contexts: Many representations of a sound have been proposed, for example, auditory maps (Brown 1992) and synchrony strands (Cooke et al. 1993), but most of them are unsuitable for incremental processing. Nakatani and Okuno proposed using a sound stream (or simply stream) to represent a sound <ref> (Nakatani et al. 1994) </ref>. A sound stream is a group of sound components that have some consistent attributes. By using sound streams, the Prince Shotoku effect can be modeled as shown in Fig. 1. <p> The HBSS and Bi-HBSS systems are modeled on the residue-driven architecture with multi-agent systems. These systems can be extended for such integration by using subsumption architecture <ref> (Nakatani et al. 1994) </ref>. A common system architecture for such integration is the black board architecture (Cooke et al. 1993; Lesser et al. 1993). The modeling of CASA represents an important area for future work.
Reference: <author> Nakatani, T.; Kawabata, T.; and Okuno, H.G. </author> <year> 1995a. </year> <title> A computational model of sound stream segregation with the multi-agent paradigm. </title> <booktitle> In Proceedings of 1995 International Conference on Acoustics, Speech and Signal Processing, Vol.4:2671-2674, IEEE. </booktitle>
Reference: <author> Nakatani, T.; Okuno, H.G.; and Kawabata, T. </author> <year> 1995b. </year> <title> Residue-driven architecture for Computational Auditory Scene Analysis. </title> <booktitle> In Proceedings of the th International Joint Conference on Artificial Intelligence, Vol.1:165-172, IJCAI. </booktitle>
Reference-contexts: They developed two systems: the harmonic-based stream segregation (HBSS) (Nakatani et al. 1994; Nakatani et al. 1995a), and the binaural harmonic-based stream segregation (Bi-HBSS) systems (Nakatani et al. 1996). Both systems were designed and implemented in a multi-agent system with the residue-driven architecture <ref> (Nakatani et al. 1995b) </ref>. We adopted these two systems to extract stream fragments from a mixture of sounds, since they run incrementally by using lower level sound characteristics. This section explains in detail how HBSS and Bi-HBSS work.
Reference: <author> Nakatani, T.; Goto, M.; and Okuno, H.G. </author> <year> 1996. </year> <title> Localization by harmonic structure and its application to harmonic sound stream segregation. </title> <booktitle> In Proceedings of 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <publisher> IEEE. Forthcoming. </publisher>
Reference-contexts: Nakatani et al. used a harmonic structure 1 and the direction of the sound source as consistent attributes for segregation. They developed two systems: the harmonic-based stream segregation (HBSS) (Nakatani et al. 1994; Nakatani et al. 1995a), and the binaural harmonic-based stream segregation (Bi-HBSS) systems <ref> (Nakatani et al. 1996) </ref>. Both systems were designed and implemented in a multi-agent system with the residue-driven architecture (Nakatani et al. 1995b). We adopted these two systems to extract stream fragments from a mixture of sounds, since they run incrementally by using lower level sound characteristics. <p> When both fundamental frequencies cross, the HBSS cannot know whether two fundamental frequencies are crossing or approaching and departing. To cope with such problems and improve the segregation performance, binaural harmonic-based stream segregation (Bi-HBSS), which incorporates di rection information into the HBSS, was proposed <ref> (Nakatani et al. 1996) </ref>. The Bi-HBSS takes a binaural input and extracts the direction of the sound source by calculating the interaural time difference (ITD) and interaural intensity difference (IID). <p> The Bi-HBSS improves the segregation performance of the HBSS (Nakatani et al. 1995b; Nakatani et al. 1996). In addition, the spectral distortion of segregated sounds became very small when benchmarking was used with various mixtures of two women's utterances of Japanese vowels and interfering sounds <ref> (Nakatani et al. 1996) </ref>. However, the usage of binaural inputs may cause spectral distortion, because the spectrum of a binaural input is not the same as that of the original sound due to the shape of the human head. Such transformation is called the head-related transfer function (HRTF) (Blauert 1983). <p> In the next section, the degradation of the recognition performance caused by segregation will be assessed and methods of recovery will be proposed. The pitch error of Bi-HBSS for simple benchmarks is small <ref> (Nakatani et al. 1996) </ref>. However, its evaluation with larger benchmarks is also needed. The onset of a segregated stream is detected only from the harmonic structures in HBSS.
Reference: <author> Okuno, H.G.; Nakatani, T.; and Kawabata, T. </author> <year> 1995. </year> <title> Cocktail-Party Effect with Computational Auditory Scene Analysis | Preliminary Report |. In Symbiosis of Human and Artifact | Proceedings of the Sixth International Conference on Human-Computer Interaction, </title> <publisher> Vol.2:503-508, Elsevier Science B.V. </publisher>
Reference-contexts: CASA is not simply a hearing aid for ASR systems, though. Computer audition can listen to several things at once by segregating sounds from a mixture of sounds. This capability to listen to several sounds simultaneously has been called the Prince Shotoku effect by Okuno <ref> (Okuno et al. 1995) </ref> after Prince Shotoku (574-622 A.D.) who is said to have been able to listen to ten people's petitions at the same time.
Reference: <author> Ramalingam, C.S., and Kumaresan, R. </author> <year> 1994. </year> <title> Voiced-speech analysis based on the residual interfering signal canceler (RISC) algorithm. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, Vol.I:473-476, IEEE. </booktitle>
Reference-contexts: Listening to Several Sounds Simultaneously 2. Stream fragment grouping | stream fragments are grouped into a stream according to some consistent attributes. Most sound segregation systems developed so far have limitations. Some systems assume the number of sounds, or the characteristics of sounds such as voice or music (e.g., <ref> (Ramalingam 1994) </ref>). Some run in a batch mode (e.g., (Brown 1992; Cooke et al. 1993)). Since CASA tries to manipulate any kind of sound, it should be able to segregate any kind of sound from a mixture of sounds.
Reference: <author> Rosenthal, D., and Okuno, H.G. </author> <month> eidtors </month> <year> 1996. </year> <title> Computational Auditory Scene Analysis, </title> <publisher> LEA. Forthcoming. </publisher>
Reference-contexts: This emerging research area is called computational auditory scene analysis (CASA) and a workshop on CASA was held at IJCAI-95 <ref> (Rosenthal & Okuno 1996) </ref>. One application of CASA is as a front-end system for automatic speech recognition (ASR) systems. Hearing impaired people find it difficult to listen to sounds in a noisy environment.
Reference: <author> Stadler, R.W., and Rabinowitz, W.M. </author> <year> 1993. </year> <title> On the potential of fixed arrays for hearing aids. </title> <journal> Journal of Acoustic Society of America 94(3) Pt.1:1332-1342. </journal>
Reference: <author> Warren, R.M. </author> <year> 1970. </year> <title> Perceptual restoration of missing speech sounds. </title> <publisher> Science167:392-393. </publisher>
Reference-contexts: Residue substitution The idea behind the residue substitution is based on the observation that human listeners can perceptually restore a missing sound component if it is very brief and replaced by appropriate sounds. This auditory mechanism of phonemic restoration is known as auditory induction <ref> (Warren 1970) </ref>. After harmonic grouping, harmonic components are included in a segregated stream or group, while non-harmonic components are left out. Since the missing components are non-harmonic, they cannot be extracted by either HBSS or Bi-HBSS and remain in the residue.
References-found: 21

