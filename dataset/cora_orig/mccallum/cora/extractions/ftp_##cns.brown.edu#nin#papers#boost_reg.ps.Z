URL: ftp://cns.brown.edu/nin/papers/boost_reg.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: Email: favni,ning@math.tau.ac.il  
Title: Boosting Regression Estimators  
Author: Ran Avnimelech Nathan Intrator 
Keyword: Boosting, Ensemble Averaging, Mixture of Experts, PAC Learning, Regression Estimators, Time series.  
Date: June 1, 1997  
Address: Ramat Aviv 69978, Tel-Aviv, Israel.  
Affiliation: Department of Computer Science Sackler Faculty of Exact Sciences Tel-Aviv University,  
Abstract: The Boosting algorithm (Schapire, 1990) is extended to fit a wide range of regression problems. This is done via an appropriate theorem on the equivalence of weak and strong learning in a regression framework. The practical capabilities of this model are demonstrated on the laser data from the Santa Fe times series competition, where the results surpass those of standard ensemble average. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bias, variance and arcing classifiers. </title> <type> Technical Report TR-460, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference-contexts: Boosting was also applied to decision trees and achieved improved performance on various data sets <ref> (Breiman, 1996) </ref>. 1 In this paper, we extend the Boosting algorithm to regression problems by introducing the notion of weak and strong learning and an appropriate equivalence theorem between them.
Reference: <author> Drucker, H., Schapire, R., and Simard, P. </author> <year> (1993). </year> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 42-49. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The boosting algorithm was successfully used in various real-world classification tasks. Ensembles of neural networks, constructed by the boosting algorithm, significantly outperformed a single network or a simple ensemble on a digit recognition task <ref> (Drucker et al., 1993) </ref> and on a phoneme recognition task (Waterhouse and Cook, 1996). <p> This results from the proof presented here and also makes sense, as the choice of the different training sets "allows" a predictor to have a big error wherever both its peers produce accurate predictions. On the other hand, this may seem to contradict the empirical results presented in <ref> (Drucker et al., 1993) </ref>. In that work averaging of the different outputs outperformed voting. However, there is a difference between the meaning of averaging in classification and regression tasks: Continuous values in classification are an indication of the confidence level.
Reference: <author> Efron, B. and Tibshirani, R. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference-contexts: The simplest is to split the data into independent sets (Meir, 1995), however, such a reduction in the number of training patterns may degrade the results of each predictor too much. Another approach is to bootstrap several training sets with a small percentage of non-overlapping patterns <ref> (Efron and Tibshirani, 1993) </ref>. A recently proposed method increases independence between the predictors by adding large amounts of noise to the training patterns (Raviv and Intrator, 1996). A different approach to ensemble averaging is the adaptive mixture of experts (Jacobs et al., 1991).
Reference: <author> Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In 3rd annual workshop on computational learning theory, </booktitle> <pages> pages 202-216. </pages>
Reference-contexts: There have been various improvements to the original boosting algorithm: While original Boosting uses hierarchies of three-classifier ensembles, Boosting-by-Majority uses a simple ensemble which may consist of more classifiers, thus achieving the same improvement with less classifiers <ref> (Freund, 1990) </ref>. Another improvement is considering the different error levels of the various classifiers, when the data is adaptively re-sampled for providing a training set for the next classifier. (Freund and Schapire, 1995).
Reference: <author> Freund, Y. and Schapire, R. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In 2nd European Conference on Computational Learning Theory. </booktitle>
Reference-contexts: Another improvement is considering the different error levels of the various classifiers, when the data is adaptively re-sampled for providing a training set for the next classifier. <ref> (Freund and Schapire, 1995) </ref>. This modification makes Boosting effective when performance on the difficult sets is much worse than on the original set. The boosting algorithm was successfully used in various real-world classification tasks.
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: Ensemble averaging of predictors can improve and robustify performance of single predictors under these circumstances. This occurs when the errors made by different predictors are independent and thus the ensemble average reduces the variance portion of the error <ref> (Geman et al., 1992) </ref>. There are various ways to increase the independence of the errors. The simplest is to split the data into independent sets (Meir, 1995), however, such a reduction in the number of training patterns may degrade the results of each predictor too much.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87. </pages>
Reference-contexts: A recently proposed method increases independence between the predictors by adding large amounts of noise to the training patterns (Raviv and Intrator, 1996). A different approach to ensemble averaging is the adaptive mixture of experts <ref> (Jacobs et al., 1991) </ref>. This method is a divide-and-conquer algorithm which co-trains a gating network for (soft) partitioning the input space and expert networks modeling the underlying function in each of these partitions.
Reference: <author> Meir, R. </author> <year> (1995). </year> <title> Bias, variance and the combination of least square estimators. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 295-302. </pages> <publisher> The MIT Press. </publisher>
Reference-contexts: This occurs when the errors made by different predictors are independent and thus the ensemble average reduces the variance portion of the error (Geman et al., 1992). There are various ways to increase the independence of the errors. The simplest is to split the data into independent sets <ref> (Meir, 1995) </ref>, however, such a reduction in the number of training patterns may degrade the results of each predictor too much. Another approach is to bootstrap several training sets with a small percentage of non-overlapping patterns (Efron and Tibshirani, 1993).
Reference: <author> Nix, D. A. and Weigend, A. S. </author> <year> (1995). </year> <title> Learning local error bars for nonlinear regression. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 489-496. </pages> <publisher> The MIT Press. 9 Raviv, </publisher> <editor> Y. and Intrator, N. </editor> <year> (1996). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <journal> Connection Science, Special issue on Combining Estimators, </journal> <volume> 8 </volume> <pages> 356-372. </pages>
Reference-contexts: However, there is a difference between the meaning of averaging in classification and regression tasks: Continuous values in classification are an indication of the confidence level. The equivalent in regression is a confidence measure based on estimation of local error bars, see <ref> (Nix and Weigend, 1995) </ref>. 3.4 Practical Considerations and Limitations The above model refers to a threshold fl for big errors. However, In regression problems the goal is usually to reduce the MSE and this presents a dilemma about the desired value of fl. <p> When scaled to the range [-1,1] the laser data had mean -0.5326 and S.D. 0.3676. The results reported for NMSE of a single predictor in the boosting context ignore the difference between the three predictors. The performance achieved by our single predictor is similar to that reported in <ref> (Nix and Weigend, 1995) </ref> and the better performing participants in the Santa Fe Time Series Competition (Weigend and Gershenfeld, 1993). 5 Discussion This work extends Schapire's boosting algorithm to fit regression problems.
Reference: <author> Schapire, R. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227. </pages>
Reference-contexts: 1 The Boosting Algorithm Boosting is an ensemble learning algorithm which achieves improved performance by training different learners on different distributions of the data and combining their output <ref> (Schapire, 1990) </ref>. It was suggested in the context of the PAC learning model (Valiant, 1984) and theoretically, it enables achieving arbitrarily low error rate, requiring the basic learners only to have performance which is slightly better than random guessing. <p> By reducing the number of large errors, we are able to reduce the MSE. 3 The Regressor-Boosting Algorithm 3.1 Model Definitions We introduce a regression notion of weak learning taken from the PAC framework <ref> (Schapire, 1990) </ref>. The essence of the regression problem is constructing a function f (x) based on a "training set" (x 1 ; y 1 ) : : : (x N ; y N ), for the purpose of approximating y at future observations of x. <p> Therefore, a fl could be chosen that would achieve a greater error reduction. 3.3 Proof: The Regressor-Boosting Theorem We suggest a constructive proof which follows Schapire's original proof <ref> (Schapire, 1990) </ref>. Instead of the majority vote of an ensemble used for classification tasks, we propose to use the median of an ensemble for regression problems. This principle may also be applied to more recent versions of the boosting algorithm.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference-contexts: 1 The Boosting Algorithm Boosting is an ensemble learning algorithm which achieves improved performance by training different learners on different distributions of the data and combining their output (Schapire, 1990). It was suggested in the context of the PAC learning model <ref> (Valiant, 1984) </ref> and theoretically, it enables achieving arbitrarily low error rate, requiring the basic learners only to have performance which is slightly better than random guessing.
Reference: <author> Waterhouse, S. R. and Cook, G. </author> <year> (1996). </year> <title> Ensemble methods for phoneme classification. </title> <note> To appear in NIPS 9. </note>
Reference-contexts: The boosting algorithm was successfully used in various real-world classification tasks. Ensembles of neural networks, constructed by the boosting algorithm, significantly outperformed a single network or a simple ensemble on a digit recognition task (Drucker et al., 1993) and on a phoneme recognition task <ref> (Waterhouse and Cook, 1996) </ref>. Boosting was also applied to decision trees and achieved improved performance on various data sets (Breiman, 1996). 1 In this paper, we extend the Boosting algorithm to regression problems by introducing the notion of weak and strong learning and an appropriate equivalence theorem between them.
Reference: <author> Weigend, A. S. and Gershenfeld, N. A., </author> <title> editors (1993). TIME SERIES PREDICTION: Forecasting the Future and Understanding the Past. </title> <booktitle> Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis held in Santa Fe, </booktitle> <address> New Mexico, May 14-17, 1992. </address> <publisher> Addison-Wesley. </publisher> <pages> 10 </pages>
Reference-contexts: A simple criterion we use is including those patterns on which the difference between the predictions is g.t. the threshold fl by which we defined big errors. 4 Results We demonstrate the capabilities of the boosting algorithm on laser data from the Santa Fe times series competition (data set A) <ref> (Weigend and Gershenfeld, 1993) </ref>. This time series is the intensity of a NH 3 -FIR laser, which exhibits Lorenz-like chaos Figure 2. It has a sampling noise due to the A/D conversion to 256 discrete values. <p> The results reported for NMSE of a single predictor in the boosting context ignore the difference between the three predictors. The performance achieved by our single predictor is similar to that reported in (Nix and Weigend, 1995) and the better performing participants in the Santa Fe Time Series Competition <ref> (Weigend and Gershenfeld, 1993) </ref>. 5 Discussion This work extends Schapire's boosting algorithm to fit regression problems.
References-found: 13

