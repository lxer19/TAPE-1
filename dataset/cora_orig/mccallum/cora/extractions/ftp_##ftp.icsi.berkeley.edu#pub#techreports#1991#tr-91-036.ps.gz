URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-036.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks  
Author: Krste Asanovic Nelson Morgan 
Date: October 1991  
Pubnum: TR-91-036  
Abstract: The impact of reduced weight and output precision on the back-propagation training algorithm [Wer74, RHW86] is experimentally determined for a feed-forward multilayer perceptron. In contrast with previous such studies, the network is large with over 20,000 weights, and is trained with a large, real-world data set of over 130,000 patterns to perform a difficult task, that of phoneme classification for a continuous speech recognition system. The results indicate that 16b weight values are sufficient to achieve training and classification results comparable to 32b floating point, provided that weight and bias values are scaled separately, and that rounding rather than truncation is employed to reduce the precision of intermediary values. Output precision can be reduced to 8 bits without significant effects on performance. 
Abstract-found: 1
Intro-found: 1
Reference: [BH88] <author> T. Baker and D. Hammerstrom. </author> <title> Modifications to artificial neural network models for digital hardware implementation. </title> <type> Technical Report CS/E 88-035, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Center, </institution> <year> 1988. </year>
Reference-contexts: This agrees with other researcher's findings with smaller networks and training sets <ref> [BH88] </ref>. Lower weight precisions gave significantly poorer results. Achieving this performance required careful attention to the properties of short word length arithmetic. Truncation was shown to give very poor results; rounding should be used to reduce the precision of intermediary results.
Reference: [Her90] <author> H. Hermansky. </author> <title> Perceptual Linear Predictive (PLP) Analysis of Speech. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 87(4), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: The system utilizes a layered ANN to generate emission probabilities for a hidden Markov model (HMM) speech recognizer. Initial experiments indicate that this method compares favourably with conventional HMM speech recognition methods [MB90]. The network has 26 inputs. These are 12th order Perceptual Linear Prediction (PLP) coefficients <ref> [Her90] </ref> plus first order derivative terms for each speech frame. The input values are normalized to zero-mean and unity variance across the set of training data. These inputs are directly and fully connected to a layer of 256 hidden units.
Reference: [MB90] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition using Multilayer Perceptrons with Hidden Markov models. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 413-416, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: The system utilizes a layered ANN to generate emission probabilities for a hidden Markov model (HMM) speech recognizer. Initial experiments indicate that this method compares favourably with conventional HMM speech recognition methods <ref> [MB90] </ref>. The network has 26 inputs. These are 12th order Perceptual Linear Prediction (PLP) coefficients [Her90] plus first order derivative terms for each speech frame. The input values are normalized to zero-mean and unity variance across the set of training data. <p> The test set is used to check the performance of the training algorithm to avoid over-fitting of the network to the training set. There are two phases in the training scheme, which is a recent variant of the cross-validation learning approach used in <ref> [MB90] </ref>. Training starts by assigning some range of random values to all the weights, say r. An initial value for ff, the learning constant, is chosen and this value remains constant throughout the first training phase.
Reference: [MBAB90] <author> N. Morgan, J. Beck, E. Allman, and J. Beer. </author> <title> RAP: A Ring Array Processor for Multilayer Perceptron applications. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 1005-1008, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: Training performance has been found to be relatively insensitive to the exact value of these parameters. 3 Experimental Methods All simulations were executed on various configurations of the RAP neurocomputer <ref> [MBAB90] </ref>, the largest containing 24 processors. The RAP uses TMS320C30 DSPs that implement 32-bit floating-point arithmetic using a proprietary format [Tex88]. Reduced precision weight representations were simulated by adapting an existing ANN training program, "mlp", to call a weight quantization routine after each training pattern.
Reference: [MHB + 91] <author> N. Morgan, H. Hermansky, H. Bourlard, P. Kohn, and C. Wooters. </author> <title> Continuous speech recognition using PLP analysis with Multilayer Perceptrons. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 49-52, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference-contexts: These results would seem to indicate that around 17-18 bits are required to achieve scores comparable to that of 32b floating 1 The best scores in these experiments are somewhat lower than the scores we have reported recently <ref> [MHB + 91] </ref>, but these latter studies used networks with 200,000-300,000 weights, and still only achieved about 6% better performance at the frame level than the nets used in the current experiment.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing. </title> <journal> Exploration of the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In this paper, the impact of reducing weight and output precisions is experimentally determined for a feed-forward multi-layer perceptron that forms part of a speech recognition system. The ANN is trained using the popular error back-propagation algorithm <ref> [Wer74, RHW86] </ref>. Earlier studies have examined artificial problems or small real data sets to determine acceptable ranges for these precisions.
Reference: [Tex88] <institution> Texas Instruments, Houston, Texas, USA. </institution> <note> Third-Generation TMS320 User's Guide, </note> <year> 1988. </year>
Reference-contexts: The RAP uses TMS320C30 DSPs that implement 32-bit floating-point arithmetic using a proprietary format <ref> [Tex88] </ref>. Reduced precision weight representations were simulated by adapting an existing ANN training program, "mlp", to call a weight quantization routine after each training pattern. The quantization routine rounds each updated floating-point weight to the nearest value allowed by the simulated fixed-point representation, saturating if necessary.
Reference: [Wer74] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Harvard University, </institution> <year> 1974. </year> <month> 6 </month>
Reference-contexts: In this paper, the impact of reducing weight and output precisions is experimentally determined for a feed-forward multi-layer perceptron that forms part of a speech recognition system. The ANN is trained using the popular error back-propagation algorithm <ref> [Wer74, RHW86] </ref>. Earlier studies have examined artificial problems or small real data sets to determine acceptable ranges for these precisions.
References-found: 8

