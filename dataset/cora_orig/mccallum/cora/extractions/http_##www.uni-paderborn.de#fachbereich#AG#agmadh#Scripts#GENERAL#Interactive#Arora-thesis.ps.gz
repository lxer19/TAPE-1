URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/Scripts/GENERAL/Interactive/Arora-thesis.ps.gz
Refering-URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/WWW/english/scripts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Probabilistic Checking of Proofs and Hardness of Approximation Problems survey into a book at some
Author: Sanjeev Arora 
Degree: of a dissertation submitted at  
Note: The author plans to convert this  this version are most welcome. This survey is also available in hard-copy form for a charge of $9.50 (only US dollar checks accepted) from the following address. Please request report CS-TR-476-94.  
Address: Berkeley, in  
Affiliation: CS Division, UC  
Date: (Revised version  August 1994)  
Abstract: Technical Reports Department of Computer Science Princeton University 35 Olden Street Princeton, NJ 08544-2087 
Abstract-found: 1
Intro-found: 1
Reference: [ABSS93] <author> S. Arora, L. Babai, J. Stern, and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes and linear equations. </title> <booktitle> In Proc. 34th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 724-733, </pages> <year> 1993. </year>
Reference-contexts: Optimization problems on lattices, codes and linear systems. We show the hardness of approximating many problems including the well-known Nearest Lattice Vector 1.1. THIS DISSERTATION 3 and the Nearest Codeword problems <ref> [ABSS93] </ref>. A hardness result is also obtained for a version of the Shortest Lattice Vector problem, namely the version using the ` 1 norm. We note that tightening the ` 1 result would prove the hardness of exact optimization in the ` 2 norm, a longstanding open problem. <p> We do not make this a part of the definition because it makes our reductions more difficult to describe. 2 The Label Cover problem was implicit in [LY94] and was defined in <ref> [ABSS93] </ref>. Although an ungainly problem at first sight, it is quite useful in reductions. <p> Better Results For NEAREST-CODEWORD and NV p for all p 1, we we can prove almost-NP-hardness up to a factor 2 log 1* n instead of 2 log 0:5* n . These results appear in <ref> [ABSS93] </ref>. Also, in our reductions the number of variables, dimensions, input size etc. are polynomially related, so n could be any of these. Previous or Independent Work Bruck and Naor ([BN90]) have shown the hardness of approximating the NEAREST-CODEWORD problem to within some 1 + * factor. <p> Other Finite Norms. Changing the norm from ` 1 to ` p changes the gap from c to p p c, hence the result claimed for ` p norms also follows. l 1 Norm. See <ref> [ABSS93] </ref> for details Nearest-Codeword. View the vectors b 0 0 m obtained from the reduction to "NV 1 with 0=1 vectors" as generators of a binary code. Let the received message be b 0 0 . <p> Subclass IIIa. This contains problems for which inapproximability results are based upon Label Cover. Some of these problems are Nearest Lattice Vector, Nearest Codeword, Min-Unsatisfy, Learning Halfspaces in presence of error (all in <ref> [ABSS93] </ref> and in Section 6.4), Quadratic Programming ([FL92, BR93]), and an entire family of problems called MAX--Subgraph ([LY93]). <p> Hardness of SV 1 Recall that the hardness result for SV 1 used Lemma 6.15. In this section we give some idea of how this result is proved; more details appear in <ref> [ABSS93] </ref>. The main idea is that Label Cover instances produced in the above reduction represent the following algebraic object. Let m be an integer and F a field. Let lines in F m be defined as in Section 4.4.3.
Reference: [ADP77] <author> G. Ausiello, A. D'Atri, and M. Protasi. </author> <title> On the structure of combinatorial problems and structure preserving reductions. </title> <booktitle> In Proc. 4th Intl. Coll. on Automata, Languages and Programming, </booktitle> <year> 1977. </year>
Reference-contexts: History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered [Joh74, SG76]. (See [GJ79] for a discussion). Much of the work attempted to discover a classification framework for optimization (and aproxima-tion) problems analogous to the framework of NP-completeness for decision problems. (See <ref> [ADP77, ADP80, AMSP80] </ref> for some of these attempts.) The most successful attempt was due to Papadimitriou and Yannakakis, who based their classification around a complexity class they called MAX-SNP (see Chapter 6).
Reference: [ADP80] <author> G. Ausiello, A. D'Atri, and M. Protasi. </author> <title> Structure preserving reductions among convex optimization problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 136-153, </pages> <year> 1980. </year>
Reference-contexts: History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered [Joh74, SG76]. (See [GJ79] for a discussion). Much of the work attempted to discover a classification framework for optimization (and aproxima-tion) problems analogous to the framework of NP-completeness for decision problems. (See <ref> [ADP77, ADP80, AMSP80] </ref> for some of these attempts.) The most successful attempt was due to Papadimitriou and Yannakakis, who based their classification around a complexity class they called MAX-SNP (see Chapter 6).
Reference: [AFK89] <author> M. Abadi, J. Feigenbaum, and J. Kilian. </author> <title> On hiding information from an oracle. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 39, </volume> <year> 1989. </year>
Reference-contexts: The phenomenon in some sense underlies cryptography. For example, the pseudorandom generator of [BM84] uses the fact that the discrete log problem is random self-reducible. The term random-self-reducible appears to have been explicitly defined first in <ref> [AFK89] </ref> (see also [FKN90]). Many flavors of random-self-reducibility have since been defined and studied ([FF93, Fei93]). Blum ([BK89]) and Lipton ([Lip89]) rephrased the r.s.r. property as follows. Given any program that computes f on "most" inputs, a randomized algorithm can recover the value of f at an arbitrary input x.
Reference: [AFWZ93] <author> N. Alon, U. Feige, A. Wigderson, and D. Zuckerman. </author> <title> Derandomized graph products. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: Proving n * -approximations NP-hard In this section we give some idea of how to prove the NP-hardness of n * -approximations. Original proofs of these results were more difficult, and we present a simplification due to <ref> [AFWZ93] </ref>. All the results could also be proved in a weaker form using reductions from Label Cover, but that would prove only almost-NP-hardness of large factors (although see the open problems in Chapter 9; specifically Conjecture 9.1). Often problems in this class have a self-improvement property. <p> The problem with this booster is that its size is n = O (n k ), hence k must be O (1) for any use in polynomial time reductions. The following theorem appears in <ref> [AFWZ93] </ref>. Its proof uses explicit constructions of expander graphs ([GG81]). Theorem 6.19: For any k = O (log n) and ff &gt; 0 an (n; k; ff) booster of size poly (n) can be constructed in poly (n) time. 2 Let G be a graph on n vertices. <p> The first hardness result for Clique was obtained in [FGL + 91]. NP-hardness (of constant-factor approximation) was proved in [AS92]. Further, as observed first in [Zuc91], the constant factor hardness result can be improved to larger factors by using a pseudo-random graph-product. The result of <ref> [AFWZ93] </ref> stated in Section 6.5 is the cleanest statement of such a construction. The NP-hardness of n * -approximation is due to [ALM + 92], although with a different reduction (namely, the one due to [FGL + 91], plus the idea of [Zuc93]).
Reference: [AK93] <author> E. Amaldi and V. Kanna. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <type> Technical report, TR # ORWP-11-93, </type> <institution> Dept. of Mathematics, Swiss Federal Institute of Technology, Lausanne, </institution> <year> 1993. </year>
Reference: [AKM94] <author> S. Arora, S. Khanna, and R. Motwani. </author> <title> A PTAS for planar MAX-SAT. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference: [AL95] <author> S. Arora and C. Lund. </author> <title> Hardness of approximations. Survey chapter to appear in a book on Approximation Algorithms, </title> <address> D. Hochbaum, ed.. </address> <note> Available from the authors., </note> <year> 1995. </year>
Reference: [ALM + 92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and intractability of approximation problems. </title> <booktitle> In Proc. 33rd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 13-22, </pages> <year> 1992. </year>
Reference-contexts: Some of the important problems to which our result applies are the following. Clique and Independent Set. We show that approximating these problems within any constant factor is NP-hard ([AS92]). Further, in <ref> [ALM + 92] </ref> we show that some positive constant * exists such that approximating these problems within a factor of n * (n = number of vertices in the graph) is NP-hard. <p> To the best of our knowledge, this dissertation represents the first self-contained exposition of the entire proof of the PCP Theorem, incorporating all necessary lemmas from the papers ([AS92, ALM + 92]), and other previous work. For other (almost complete) expositions we refer the reader to <ref> [Sud92, ALM + 92] </ref>). However, the exposition in [Sud92] takes a different viewpoint: Its main results concern program checking, and the PCP theorem is derived as a corollary to those results. <p> At this point, several people (for example, the authors of [AMS + 92]) realized that proving NP = PCP (log n; 1) would prove the inapproximability of MAX-3SAT. This result was actually obtained in the paper (Arora, Lund, Motwani, Sudan and Szegedy <ref> [ALM + 92] </ref>). Owing to its great dependence upon [AS92], the proof of the PCP Theorem is often attributed to jointly to [ALM + 92, AS92]. <p> This result was actually obtained in the paper (Arora, Lund, Motwani, Sudan and Szegedy [ALM + 92]). Owing to its great dependence upon [AS92], the proof of the PCP Theorem is often attributed to jointly to <ref> [ALM + 92, AS92] </ref>. Among other papers that were influential in the above developments were those by Beaver and Feigenbaum ([BF90]), Lapidot and Shamir ([LS91]), Rubinfeld and Sudan [RS92], and Feige and Lovasz ([FL92]). Their contributions will be described in appropriate places later. Other characterizations of NP. <p> Composition was termed "Recursion" in [AS92] because in that paper verifiers were composed only with themselves. Full use of the lemma (as described in this section) 3.3. HISTORY AND BACKGROUND 25 was made in <ref> [ALM + 92] </ref>, where Theorems 3.2 and 3.4 were proven. (The best verifier of [AS92] was (log n; (log log n) 2 ; (log log n) 2 )-restricted.) 26 CHAPTER 3. <p> Thus Lemma 4.5 could be proved with minor modifications of the above-mentioned results, although no previous paper had proved it explicitly before [AS92]. All other results in this chapter, except the Low-degree Test and the idea of checking split assignments, are from <ref> [ALM + 92] </ref>. The history of the Low-degree Test will be covered in the next section. The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of [LS91, FL92], although the ideas used in this lemma can be traced back to [BF90, LFKN92]. <p> The idea of combining the work of [AS92] and [RS92] to obtain the low-degree test of this chapter is due to <ref> [ALM + 92] </ref>.This combination allows the test to work even when the success rate is a constant (independent of the degree). <p> For instance, vertex-cover and independent-set are another "complementary" pair of problems, and seem to differ greatly in how well they can be approximated in polynomial time. (Vertex cover can be approximated within a factor 2, and independent-set is NP-hard up to a factor n c for some c &gt; 0 <ref> [FGL + 91, AS92, ALM + 92] </ref>.) We find it interesting that large factor approximation is hard for our two complementary problems. We do not know of any other complementary pair with the same behavior. We know of no good approximation algorithms for any of these problems. <p> The booster property implies that the size of B is as claimed. 2 Theorem 6.21: Approximating Clique within a factor n * for some * &gt; 0 is NP-hard. Before proving Theorem 6.21 (which is due to <ref> [ALM + 92] </ref>), we prove a weaker result about a related problem, Vertex Cover. Let V C min (G) denote the size of the minimum vertex cover in graph G. 102 CHAPTER 6. <p> The following is a partial list of MAX-SNP-hard problems : MAX-SAT, MAX-2SAT (13), Independent Set, Vertex Cover, Max-Cut (all in [PY91]), Metric TSP ([PY93b]), Steiner Tree ([BP89]), Shortest Superstring ([BJL + 91]), Multiway Cuts ([DJP + 92]), and 3-D Matching ([Kan92]). Many more continue to be found. Since <ref> [ALM + 92] </ref>, there have been improvements in the value of the constant * for which the above results are 6.6. OTHER INAPPROXIMABILITY RESULTS: A SURVEY 105 known to hold. Currently the constant is of the order of 10 2 for most problems, and about 0:02 for MAX-3SAT ([BGLR93, BS94]). <p> Further, as observed first in [Zuc91], the constant factor hardness result can be improved to larger factors by using a pseudo-random graph-product. The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction. The NP-hardness of n * -approximation is due to <ref> [ALM + 92] </ref>, although with a different reduction (namely, the one due to [FGL + 91], plus the idea of [Zuc93]). The connection between MAX-SNP and Clique (albeit with a randomized booster construction) was first discovered in [BS92]. <p> A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements <ref> [ALM + 92, BGLR93, FK94b, BS94] </ref>. The latest improvements center around the concept of free bits ([FK94b]). This is a new parameter associated with the PCP verifier that is upperbounded by the number of query bits, but is often (e.g., in the verifier we constructed) much smaller.
Reference: [AMS + 92] <author> S. Arora, R. Motwani, M. Safra, M. Sudan, and M. Szegedy. </author> <title> PCP and approximation problems. </title> <type> Manuscript, </type> <year> 1992. </year>
Reference-contexts: Hence we defined the class PCP (definition 2.5) with two parameters (instead of the single parameter used in [FGL + 91]). We showed that NP = PCP (log n; (log log n) 2 ). At this point, several people (for example, the authors of <ref> [AMS + 92] </ref>) realized that proving NP = PCP (log n; 1) would prove the inapproximability of MAX-3SAT. This result was actually obtained in the paper (Arora, Lund, Motwani, Sudan and Szegedy [ALM + 92]).
Reference: [AMSP80] <author> G. Ausiello, A. Marchetti-Spaccamela, and M. Protasi. </author> <title> Toward a unified approach for the classification of NP-complete optimization problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 12 </volume> <pages> 83-96, </pages> <year> 1980. </year>
Reference-contexts: History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered [Joh74, SG76]. (See [GJ79] for a discussion). Much of the work attempted to discover a classification framework for optimization (and aproxima-tion) problems analogous to the framework of NP-completeness for decision problems. (See <ref> [ADP77, ADP80, AMSP80] </ref> for some of these attempts.) The most successful attempt was due to Papadimitriou and Yannakakis, who based their classification around a complexity class they called MAX-SNP (see Chapter 6).
Reference: [Aro94] <author> S. Arora. </author> <title> Reductions, codes, PCPs, and inapproximability. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: Proof (Sketch). Note that the verifier will not give different answers on two strings unless 2 Our argument in this section is somewhat imprecise. It is made more precise in <ref> [Aro94] </ref>. 9.3. DOES THE PCP THEOREM HAVE A SIMPLER PROOF? 131 if it queries a bit-position in which they differ.
Reference: [AS92] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: A new characterization of NP. </title> <booktitle> In Proc. 33rd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 2-13, </pages> <year> 1992. </year> <note> 135 136 BIBLIOGRAPHY </note>
Reference-contexts: Second, since C (log n) is trivially a subclass of NP, such a result would imply a new characterization of NP, namely, NP = C (log n). 2.5. HISTORY AND BACKGROUND 15 This step was taken in the paper (Arora and Safra <ref> [AS92] </ref>). Somewhat curiously, we found that although the number of random bits used by the verifier cannot be sub-logarithmic (or else P = NP; see [AS92]), there was no such restriction on the number of query-bits. <p> HISTORY AND BACKGROUND 15 This step was taken in the paper (Arora and Safra <ref> [AS92] </ref>). Somewhat curiously, we found that although the number of random bits used by the verifier cannot be sub-logarithmic (or else P = NP; see [AS92]), there was no such restriction on the number of query-bits. Hence we defined the class PCP (definition 2.5) with two parameters (instead of the single parameter used in [FGL + 91]). We showed that NP = PCP (log n; (log log n) 2 ). <p> This result was actually obtained in the paper (Arora, Lund, Motwani, Sudan and Szegedy [ALM + 92]). Owing to its great dependence upon <ref> [AS92] </ref>, the proof of the PCP Theorem is often attributed to jointly to [ALM + 92, AS92]. Among other papers that were influential in the above developments were those by Beaver and Feigenbaum ([BF90]), Lapidot and Shamir ([LS91]), Rubinfeld and Sudan [RS92], and Feige and Lovasz ([FL92]). <p> This result was actually obtained in the paper (Arora, Lund, Motwani, Sudan and Szegedy [ALM + 92]). Owing to its great dependence upon [AS92], the proof of the PCP Theorem is often attributed to jointly to <ref> [ALM + 92, AS92] </ref>. Among other papers that were influential in the above developments were those by Beaver and Feigenbaum ([BF90]), Lapidot and Shamir ([LS91]), Rubinfeld and Sudan [RS92], and Feige and Lovasz ([FL92]). Their contributions will be described in appropriate places later. Other characterizations of NP. <p> a low decision time. (Actually their exact result was somewhat stronger.) This observation motivated the work in (<ref> [AS92] </ref>), where the Composition Lemma is implicit. The use of large-distance codes in the definition of the normal form verifier was motivated by a similar situation in [BFLS91]. Composition was termed "Recursion" in [AS92] because in that paper verifiers were composed only with themselves. Full use of the lemma (as described in this section) 3.3. HISTORY AND BACKGROUND 25 was made in [ALM + 92], where Theorems 3.2 and 3.4 were proven. (The best verifier of [AS92] was (log n; (log log n) 2 <p> Composition was termed "Recursion" in <ref> [AS92] </ref> because in that paper verifiers were composed only with themselves. Full use of the lemma (as described in this section) 3.3. HISTORY AND BACKGROUND 25 was made in [ALM + 92], where Theorems 3.2 and 3.4 were proven. (The best verifier of [AS92] was (log n; (log log n) 2 ; (log log n) 2 )-restricted.) 26 CHAPTER 3. PCP : AN OVERVIEW Chapter 4 A Proof of the PCP Theorem This chapter contains a proof of the PCP theorem. <p> The polynomial extension used in this chapter does not suffer from this problem. It is due to ([BFLS91]). The Sum-check procedure (Procedure 4.3) is due to [LFKN92]. Thus Lemma 4.5 could be proved with minor modifications of the above-mentioned results, although no previous paper had proved it explicitly before <ref> [AS92] </ref>. All other results in this chapter, except the Low-degree Test and the idea of checking split assignments, are from [ALM + 92]. The history of the Low-degree Test will be covered in the next section. <p> The other part of the chapter, Section 5.1, shows the correctness of the bivariate case. The proof of the bivariate case is a consequence of a more general (multivariate) result in <ref> [AS92] </ref>, which is stated in the next section. (The proof of the bivariate case as given here uses a little modification due to [Sud92].) No simpler proof of the general result of [AS92] is known. But a somewhat simpler proof for the bivariate case appears in [PS94]. <p> The proof of the bivariate case is a consequence of a more general (multivariate) result in <ref> [AS92] </ref>, which is stated in the next section. (The proof of the bivariate case as given here uses a little modification due to [Sud92].) No simpler proof of the general result of [AS92] is known. But a somewhat simpler proof for the bivariate case appears in [PS94]. More importantly, that paper lowers the field size required in the lemma to O (d); an improvement over O (d 3 ) as required by our proof. <p> Actually, the above list of tests includes two kinds of low-degree tests. The first kind upper-bounds the the degree of the given function in each variable. The second kind and the one in this chapter is of this kind - upper-bounds the total degree. The tests in <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref> are of the first kind. (A particularly beautiful exposition of this type of test ([She91]) was never published, although a later version appears in [FSH94].) Low-degree tests of the first kind consist in estimating the success rate of the given function on axis-parallel lines. (A <p> The strongest result about low-degree tests of the first kind is due to <ref> [AS92] </ref>. Theorem 5.13 ([AS92]): Let f : F m ! F be a function. <p> In fact, these latter tests are identical to the one used in this chapter, but their analysis was not as good (it could not show that the test works when the success rate is less than 1 O (1=d)). The idea of combining the work of <ref> [AS92] </ref> and [RS92] to obtain the low-degree test of this chapter is due to [ALM + 92].This combination allows the test to work even when the success rate is a constant (independent of the degree). <p> For instance, vertex-cover and independent-set are another "complementary" pair of problems, and seem to differ greatly in how well they can be approximated in polynomial time. (Vertex cover can be approximated within a factor 2, and independent-set is NP-hard up to a factor n c for some c &gt; 0 <ref> [FGL + 91, AS92, ALM + 92] </ref>.) We find it interesting that large factor approximation is hard for our two complementary problems. We do not know of any other complementary pair with the same behavior. We know of no good approximation algorithms for any of these problems. <p> Historical Notes/Further Reading The lure of proving better inapproximability results for Clique has motivated many developments in the PCP area. The first hardness result for Clique was obtained in [FGL + 91]. NP-hardness (of constant-factor approximation) was proved in <ref> [AS92] </ref>. Further, as observed first in [Zuc91], the constant factor hardness result can be improved to larger factors by using a pseudo-random graph-product. The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction.
Reference: [Bab85] <author> L. Babai. </author> <title> Trading group theory for randomness. </title> <booktitle> In Proc. 17th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 421-429, </pages> <year> 1985. </year>
Reference-contexts: Roots of PCP. The roots of the definition of PCP (specifically, the fact that the verifier is randomized) go back to the definition of Interactive Proofs (Goldwasser, Micali, and Rackoff [GMR89]) and Arthur-Merlin games (Babai <ref> [Bab85] </ref>, see also [BM88]). Two complexity classes arise from their definitions: IP and AM respectively.
Reference: [Bab86] <author> L. Babai. </author> <title> On Lovasz's lattice reduction and the nearest lattice point problem. </title> <journal> Combi-natorica, </journal> <volume> 6 </volume> <pages> 1-14, </pages> <year> 1986. </year>
Reference-contexts: For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan [Kan87]. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m . A modification of this algorithm <ref> [Bab86] </ref> yields the same for NV p . Schnorr modified the Lovasz algorithm and obtained, for every * &gt; 0, approximations within O (2 *m ) in polynomial time for these problems [Sch85].
Reference: [Bab94] <author> L. Babai. </author> <title> Transparent proofs and limits to approximations. </title> <booktitle> In Proceedings of the First European Congress of Mathematicians. </booktitle> <publisher> Birkhauser, </publisher> <year> 1994. </year>
Reference: [BF90] <author> D. Beaver and J. Feigenbaum. </author> <title> Hiding instances in multioracle queries. </title> <booktitle> In Proceedings of the 7th Symp. on Theoretical Aspects of Computing, </booktitle> <pages> pages 37-48. </pages> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 415, </volume> <year> 1990. </year>
Reference-contexts: The history of the Low-degree Test will be covered in the next section. The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of [LS91, FL92], although the ideas used in this lemma can be traced back to <ref> [BF90, LFKN92] </ref>. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from [BLR90, Fre79]. 4.6.
Reference: [BF91] <author> L. Babai and L. Fortnow. Arithmetization: </author> <title> a new method in structural complexity theory. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 41-66, </pages> <year> 1991. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan [BK89], Lipton [Lip89], and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow <ref> [BF91] </ref>, Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund [BFL91]). <p> This observation was the starting point for the theory of self-testing/self-correcting programs ([BK89, Lip89, BLR90]). Most functions to which this theory seemed to apply were algebraic in nature (see [Yao90] for some nonalgebraic examples, though). A new insight was made in <ref> [BF91, Sha92, LFKN92] </ref>: logical formulae can be represented as polynomials of low degree. But recall that different classes of logical formulae are complete for complexity classes like PSPACE, NEXPTIME, NP etc.. This at once suggests that randomized techniques from program-testing/correcting should be applicable to the study of conventional complexity classes.
Reference: [BFL91] <author> L. Babai, L. Fortnow, and C. Lund. </author> <title> Non-deterministic exponential time has two-prover interactive protocols. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 3-40, </pages> <year> 1991. </year>
Reference-contexts: [Lip89], and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund <ref> [BFL91] </ref>). These connections between traditional and nontraditional complexity classes were proved using novel algebraic technques, some of which will be covered later in this book. Emergence of PCP. <p> A PROOF OF THE PCP THEOREM 4.2.1. A Less Efficient Verifier This section contains a proof of Lemma 4.5. The exposition of the proof uses a mix of ideas from <ref> [BFL91, BFLS91, FGL + 91] </ref>. Let ' be an instance of 3SAT and F a finite field. The verifier will use the fact (see Definition 4.3) that every assignment can be encoded by its polynomial extension. <p> This at once suggests that randomized techniques from program-testing/correcting should be applicable to the study of conventional complexity classes. The results in this chapter are precisely of this character. Techniques of this chapter: attributions. The first algebraic representation of 3SAT is due to <ref> [BFL91] </ref>. That representation (using multilinear polynomials) has a problem: the field-size and number of variables required to represent a string of n bits are such that generating a random point in the space F m requires more than O (log n) random bits. <p> Actually, the above list of tests includes two kinds of low-degree tests. The first kind upper-bounds the the degree of the given function in each variable. The second kind and the one in this chapter is of this kind - upper-bounds the total degree. The tests in <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref> are of the first kind. (A particularly beautiful exposition of this type of test ([She91]) was never published, although a later version appears in [FSH94].) Low-degree tests of the first kind consist in estimating the success rate of the given function on axis-parallel lines. (A <p> The following is a restatement of Strong Form 2. Theorem 8.3: Ntime (t (n)) = PCP ( log t (n); 1) 8 t (n) poly (n).2 This characterization generalizes the result MIP = NEXPTIME in <ref> [BFL91] </ref>, which can be equivalently stated as Ntime (2 poly (n) ) = PCP (poly (n); poly (n)). It also generalizes the work of [BFLS91, FGL + 91] whose result could be interpreted as saying that Ntime (t (n)) PCP (poly (log t (n)); poly (log t (n))). 8.2.2.
Reference: [BFLS91] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in polyloga-rithmic time. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 21-31, </pages> <year> 1991. </year>
Reference-contexts: The use of large-distance codes in the definition of the normal form verifier was motivated by a similar situation in <ref> [BFLS91] </ref>. Composition was termed "Recursion" in [AS92] because in that paper verifiers were composed only with themselves. Full use of the lemma (as described in this section) 3.3. <p> Definition 4.2: For a 0:1-close function f the unique nearest polynomial is denoted by e f . Polynomials are useful to us as encoding objects. We define below a canonical way (due to <ref> [BFLS91] </ref>) to encode a sequence of bits with a polynomial. For convenience we describe a more general method that encodes a sequence of field elements with a polynomial. <p> A PROOF OF THE PCP THEOREM 4.2.1. A Less Efficient Verifier This section contains a proof of Lemma 4.5. The exposition of the proof uses a mix of ideas from <ref> [BFL91, BFLS91, FGL + 91] </ref>. Let ' be an instance of 3SAT and F a finite field. The verifier will use the fact (see Definition 4.3) that every assignment can be encoded by its polynomial extension. <p> Thus, assuming Lemma 4.7, Lemma 4.6 has been proved. 2 The following lemma concerns a family of polynomials that is useful for testing whether or not a function is identically zero on the cube [0; h] j for any integers h; j. Lemma 4.7: ["Zero-tester" Polynomials, <ref> [BFLS91, FGL + 91] </ref>] There exists a family of q O (m) polynomials fR 1 ; R 2 ; : : :g in F 4mh [x 1 ; : : : ; x 4m ] such that if f : [0; h] 4m ! F is any function not identically 0, <p> We do not give the proof here. 5.3.1. History Improvements to the Low-degree Test have accompanied most advances in the theory of probabilistically checkable proofs. The first such test, the multilinearity test, enabled the result MIP =NEXPTIME ([BFL91]. Subsequent tests given in <ref> [BFLS91, FGL + 91] </ref> were crucial for scaling down the result for NEXPTIME to NP. An improvement in the efficiency of those tests was in turn used to prove a new characterization of NP in terms of PCP ([AS92]). <p> Actually, the above list of tests includes two kinds of low-degree tests. The first kind upper-bounds the the degree of the given function in each variable. The second kind and the one in this chapter is of this kind - upper-bounds the total degree. The tests in <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref> are of the first kind. (A particularly beautiful exposition of this type of test ([She91]) was never published, although a later version appears in [FSH94].) Low-degree tests of the first kind consist in estimating the success rate of the given function on axis-parallel lines. (A <p> A similar statement holds for the sum-check. Recall that in all those cases, m and d are poly (log t (n)).) To improve Lemma 4.6 we need an idea from <ref> [BFLS91] </ref>. Use Levin's Theorem ([Lev73]) to reduce the decision problem on input x to an instance of Tiling. <p> We don't give further details here.) This allows the verifier to run in time poly (t + log n). 2 118 CHAPTER 8. APPLICATIONS OF PCP TECHNIQUES Note: This theorem was proved in <ref> [BFLS91] </ref> in a somewhat weaker form; their verifier required poly (log t (n)) random bits and as many query bits. 8.2. The Applications This section describes various applications of the above Strong Forms. 8.2.1. Exact Characterization of Nondeterministic Time Classes This application uses Strong Form 2. <p> It also generalizes the work of <ref> [BFLS91, FGL + 91] </ref> whose result could be interpreted as saying that Ntime (t (n)) PCP (poly (log t (n)); poly (log t (n))). 8.2.2. Transparent Math Proofs In this section we show that formal proofs in first order logic can be checked very efficiently by a probabilistic algorithm. <p> The certificate's length is polynomial in the running time of the computation, 120 CHAPTER 8. APPLICATIONS OF PCP TECHNIQUES and checking it requires examining only O (1) bits in the certificate. (This application of PCP-type results was also suggested in <ref> [BFLS91] </ref>.) The situation we have in mind is a restatement of Situation 3. Suppose a user has a nondeterministic program P and an input x. The user would like to know if the nondeterministic program has an accepting computation on the input. <p> By "cryptographic hashing" we mean that although it is possible to produce a fraudulent certificate, doing so takes a lot of time (which the chip does not have). Micali's idea works also with the weaker result about checking nondeterministic computations that appears in <ref> [BFLS91] </ref>, but efficiency is better using the PCP theorem. Also, he points out an interesting extension of the class IP. (Section 2.5). 8.2.5. <p> Kilian ([Kil92]) shows how to reduce the communication requirement. His idea is to hash down, in a cryptographic fashion, the "probabilistically checkable" database of Strong Form 1. (Recall that Micali's idea is similar.) He needs something more than the Strong Form 1, specifically, the fact (proved in <ref> [BFLS91, PS94] </ref>) that the database given by Strong Form 1 has size n 1+* , where n is the size of the 3SAT formula. 8.2. THE APPLICATIONS 123 8.2.8.
Reference: [BGKW88] <author> M. Ben-or, S. Goldwasser, J. Kilian, and A. Wigderson. </author> <title> Multi prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> In Proc. 20th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 113-121, </pages> <year> 1988. </year>
Reference: [BGLR93] <author> M. Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient multi-prover interactive proofs with applications to approximation problems. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 113-131, </pages> <year> 1993. </year>
Reference-contexts: The reduction can be trivially modified to work with Label Cover. (It is also known that any constant factor approximation is NP-hard <ref> [BGLR93] </ref>; this result can also be proved using Label Cover.) Class III. This class contains problems for which -approximation is almost-NP-hard, where is a large factor (large factors are defined in Definition 6.2). These problems may be further divided into two subclasses, based upon how inapproximability is proved for them. <p> A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements <ref> [ALM + 92, BGLR93, FK94b, BS94] </ref>. The latest improvements center around the concept of free bits ([FK94b]). This is a new parameter associated with the PCP verifier that is upperbounded by the number of query bits, but is often (e.g., in the verifier we constructed) much smaller.
Reference: [BJL + 91] <author> A. Blum, T. Jiang, M. Li, J. Tromp, and M. Yannakakis. </author> <title> Linear approximation of shortest superstrings. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 328-336, </pages> <year> 1991. </year>
Reference: [BK89] <author> M. Blum and S. Kannan. </author> <title> Designing programs that check their work. </title> <booktitle> In Proc. 21st ACM Symp. on Theory of Computing, </booktitle> <pages> pages 86-97, </pages> <year> 1989. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan <ref> [BK89] </ref>, Lipton [Lip89], and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and
Reference: [BLR90] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 73-83, </pages> <year> 1990. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan [BK89], Lipton [Lip89], and Blum, Luby and Rubin-feld <ref> [BLR90] </ref>), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund [BFL91]). <p> The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of [LS91, FL92], although the ideas used in this lemma can be traced back to [BF90, LFKN92]. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from <ref> [BLR90, Fre79] </ref>. 4.6. <p> Note that Lemma 5.3, which was used in our proof of the bivariate case, is equivalent to the special subcase m = 2 of the above theorem. Low-degree tests of the second kind arose in program checking. The earliest such test appears in <ref> [BLR90] </ref>, where it is called the multilinearity test. Subsequent tests appearing in [GLR + 91, RS92] work for higher degree. <p> Improving the low-degree test. Does the low-degree test work even for high error rates? In other words, is Theorem 5.1 in Chapter 5 true even when the success rate is less than 0:5 (say), and jFj = poly (d)? Self-correction on polynomials. Can polynomials be self-corrected (for definition see <ref> [BLR90] </ref>) in the presence of high error-rates? A self-corrector is a probabilistic program that is given a rational number p &gt; 0 and a function f : F m ! F that is (1 p)-close to F d [x 1 ; : : : ; x m ]. <p> OPEN PROBLEMS ([GL89]). At the heart of this result is a simple self-corrector (in the sense of <ref> [BLR90] </ref>) for linear functions over GF (2) (these functions were also encountered in Section 4.3). Now we know of much stronger results about polynomials (e.g., those in Chapter 5). What are the applications (if any) for cryptography? One possible application could be pseudo-random generation in parallel, a longstanding open problem.
Reference: [BM84] <author> M. Blum and S. Micali. </author> <title> How to generate cryptographically strong sequences of pseudorandom bits. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13 </volume> <pages> 850-864, </pages> <year> 1984. </year>
Reference-contexts: The phenomenon in some sense underlies cryptography. For example, the pseudorandom generator of <ref> [BM84] </ref> uses the fact that the discrete log problem is random self-reducible. The term random-self-reducible appears to have been explicitly defined first in [AFK89] (see also [FKN90]). Many flavors of random-self-reducibility have since been defined and studied ([FF93, Fei93]). Blum ([BK89]) and Lipton ([Lip89]) rephrased the r.s.r. property as follows.
Reference: [BM88] <author> L. Babai and S. Moran. </author> <title> Arthur-Merlin games: a randomized proof system, and a hierarchy of complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> pages 254-276, </pages> <year> 1988. </year>
Reference-contexts: Roots of PCP. The roots of the definition of PCP (specifically, the fact that the verifier is randomized) go back to the definition of Interactive Proofs (Goldwasser, Micali, and Rackoff [GMR89]) and Arthur-Merlin games (Babai [Bab85], see also <ref> [BM88] </ref>). Two complexity classes arise from their definitions: IP and AM respectively.
Reference: [BN90] <author> J. Bruck and M. Naor. </author> <title> The hardness of decoding linear codes with preprocessing. </title> <journal> IEEE Transactions on Inform. Theory, </journal> <pages> pages 381-385, </pages> <year> 1990. </year>
Reference: [Bol86] <author> B. Bollobas. </author> <title> Combinatorics. </title> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: Since this happens for every e, labelling (P x 2 ) pseudo-cover. 2 The reduction uses an ` fi ` Hadamard matrix i.e. a (1) matrix such that H t ` H ` = `I ` . (H ` exists e.g. when ` is a power of 2, cf. <ref> [Bol86, p.74] </ref>). Lemma 6.18: Let z 2 Z ` . If z has at least k nonzero entries then jjH ` zjj 1 p Proof: The columns of 1 p ` H ` form an orthonormal basis.
Reference: [BP89] <author> M. Bern and P. Plassmann. </author> <title> The Steiner problem with edge lengths 1 and 2. </title> <journal> Information Processing Letters, </journal> <volume> 32 </volume> <pages> 171-176, </pages> <year> 1989. </year>
Reference: [BR93] <author> M. Bellare and P. Rogaway. </author> <title> The complexity of approximating non-linear programs. In P.M. </title> <editor> Pardalos, editor, </editor> <title> Complexity of Numerical Optimization. </title> <publisher> World Scientific, </publisher> <year> 1993. </year> <note> Preliminary version: IBM Research Report RC 17831 (March 1992). BIBLIOGRAPHY 137 </note>
Reference: [BS92] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <journal> Information and Computation, </journal> <volume> 96(1) </volume> <pages> 77-94, </pages> <year> 1992. </year>
Reference-contexts: The NP-hardness of n * -approximation is due to [ALM + 92], although with a different reduction (namely, the one due to [FGL + 91], plus the idea of [Zuc93]). The connection between MAX-SNP and Clique (albeit with a randomized booster construction) was first discovered in <ref> [BS92] </ref>. A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements [ALM + 92, BGLR93, FK94b, BS94].
Reference: [BS94] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> In Proc. 26th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 184-193, </pages> <year> 1994. </year>
Reference-contexts: A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements <ref> [ALM + 92, BGLR93, FK94b, BS94] </ref>. The latest improvements center around the concept of free bits ([FK94b]). This is a new parameter associated with the PCP verifier that is upperbounded by the number of query bits, but is often (e.g., in the verifier we constructed) much smaller.
Reference: [BW] <author> E. Berlekamp and L. Welch. </author> <title> Error correction of algebraic block codes. </title> <type> US Patent Number 4,633,470. </type>
Reference-contexts: Specifically, such a low-degree test enables 5.3. DISCUSSION 77 queries to be "aggregated" (see Section 4.1.2), a property crucial in constructing the verifier in Theorem 3.2. The algebraic ideas used in the proof of Lemma 5.2 were originally inspired by a Lemma of <ref> [BW] </ref>, as used in [GS92]. 78 CHAPTER 5.
Reference: [CFLS93] <author> A. Condon, J. Feigenbaum, C. Lund, and P. Shor. </author> <title> Random debaters and the hardness of approximating stochastic functions. </title> <booktitle> In Proc. of the 9th Structure in Complexity Theory Conference, </booktitle> <pages> pages 280-293, </pages> <year> 1993. </year> <note> Also available as DIMACS Techreport TR 93-79. </note>
Reference-contexts: Strong Form 1 implies that the verifier can check this certificate by examining only O (1) bits in it. Only one task remains: how to verify that the debate encoded in such a certificate is the actual debate that took place. In the <ref> [CFLS93] </ref> paper it is shown (using Shamir's result) that the verifier only needs to decode O (1) bits from the encoded debate (the decoding requires reading only O (1) bits, according to Strong Form 1), and check them off against the corresponding bits in the actual debate.
Reference: [Chr76] <author> N. Christofides. </author> <title> Worst case analysis of a new heuristic for the travelling salesman problem. </title> <type> Technical report, </type> <institution> Grad. School of Industrial Optimization, Carnegie-Mellon University, </institution> <year> 1976. </year>
Reference: [CK94] <author> P. Crescenzi and V. Kann. </author> <title> A compendium of NP optimization problems. </title> <type> manuscript, </type> <year> 1994. </year>
Reference-contexts: Further Reading A recent unpublished survey by the author and Carsten Lund ([AL95]) provides a more comprehensive treatment of results on the hardness of approximations than the one given here. For a listing of optimization problems according to their approximation properties, consult <ref> [CK94] </ref>. 108 CHAPTER 6. HARDNESS OF APPROXIMATIONS Chapter 7 PCP Verifiers that make 2 queries The verifiers referred to in the title of this chapter are better known as 2 Prover 1 Round interactive proof systems.
Reference: [CL89] <author> A. Condon and R. Ladner. </author> <title> On the complexity of space bounded interactive proofs. </title> <booktitle> In Proc. 30th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 462-467, </pages> <year> 1989. </year>
Reference: [Con93] <author> A. Condon. </author> <title> The complexity of the max-word problem and the power of one-way interactive proof systems. </title> <journal> Computational Complexity, </journal> <volume> 3 </volume> <pages> 292-305, </pages> <year> 1993. </year>
Reference: [Coo71] <author> S. Cook. </author> <title> The complexity of theorem-proving procedures. </title> <booktitle> In Proc. 3rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 151-158, </pages> <year> 1971. </year>
Reference: [DF95] <author> R. Downey and M. Fellows. </author> <title> Fixed-parameter tractability and completeness ii: Completeness for W[1]. </title> <booktitle> Theoretical Computer Science, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference: [DJP + 92] <author> E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou, P. D. Seymour, and M. Yannakakis. </author> <title> The complexity of multiway cuts. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 241-451, </pages> <year> 1992. </year>
Reference: [Fag74] <author> R. Fagin. </author> <title> Generalized first-order spectra and polynomial-time recognizable sets. </title> <editor> In Richard Karp, editor, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 43-73. </pages> <publisher> AMS, </publisher> <year> 1974. </year>
Reference: [Fei93] <author> J. Feigenbaum. </author> <title> Locally random reductions in interactive complexity theory. </title> <booktitle> In Advances in Computational Complexity, </booktitle> <pages> pages 73-98. </pages> <publisher> American Mathematical Society, </publisher> <address> Providence, </address> <year> 1993. </year> <journal> DIMACS Series on Disc. Maths. and Theoretical CS, </journal> <volume> volume 13. </volume>
Reference: [FF93] <author> J. Feigenbaum and L. Fortnow. </author> <title> On the random-self-reducibility of complete sets. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22 </volume> <pages> 994-1005, </pages> <year> 1993. </year> <title> Prelim. </title> <booktitle> version in Proc. of the 6th Annual Structure in Complexity Theory Conference, </booktitle> <year> 1991. </year>
Reference: [FGL + 91] <author> U. Feige, S. Goldwasser, L. Lovasz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> In Proc. 32nd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 2-12, </pages> <year> 1991. </year>
Reference-contexts: papers of Babai, Fortnow, Levin, and Szegedy ([BFLS91]) and Feige, Goldwasser, Lovasz, Safra, and Szegedy (<ref> [FGL + 91] </ref>). Although only the latter talked explicitly about NP (the former dealt with checking nondeterministic computations; including NP computations as a subcase), their techniques were actually scaling down the MIP=NEXPTIME result. The paper [FGL + 91] implicitly defined a hierarchy of complexity classes unnamed there, but which we can call C. <p> Their class C (t (n)) is identical to the class PCP (t (n); t (n)) as defined in this chapter, and their main result was that NP C (log n log log n). What caused great interest in <ref> [FGL + 91] </ref> was their corollary: If the clique number of a graph can be approximated within any fixed constant factor, then all NP problems can be solved deterministically in time n O (log log n) . (Computing the clique number is a well-known NP-complete problem [Kar72].) They showed how to <p> Hence we defined the class PCP (definition 2.5) with two parameters (instead of the single parameter used in <ref> [FGL + 91] </ref>). We showed that NP = PCP (log n; (log log n) 2 ). At this point, several people (for example, the authors of [AMS + 92]) realized that proving NP = PCP (log n; 1) would prove the inapproximability of MAX-3SAT. <p> Condon and Ladner ([CL89]) further strengthened Lipton's characterization. Even more interestingly, Condon ([Con93]) then used the result of ([CL89]) to show the hardness of approximating the max-word problem. This unapproximability result appeared somewhat before (and was independent of) the more well-known <ref> [FGL + 91] </ref> paper. An older characterization of NP, in terms of spectra of second-order formulae, is due to Fagin ([Fag74]). His result, since it involves no notion of computation, is an interesting alternative viewpoint of NP. <p> A PROOF OF THE PCP THEOREM 4.2.1. A Less Efficient Verifier This section contains a proof of Lemma 4.5. The exposition of the proof uses a mix of ideas from <ref> [BFL91, BFLS91, FGL + 91] </ref>. Let ' be an instance of 3SAT and F a finite field. The verifier will use the fact (see Definition 4.3) that every assignment can be encoded by its polynomial extension. <p> Thus, assuming Lemma 4.7, Lemma 4.6 has been proved. 2 The following lemma concerns a family of polynomials that is useful for testing whether or not a function is identically zero on the cube [0; h] j for any integers h; j. Lemma 4.7: <ref> ["Zero-tester" Polynomials, [BFLS91, FGL + 91] </ref>] There exists a family of q O (m) polynomials fR 1 ; R 2 ; : : :g in F 4mh [x 1 ; : : : ; x 4m ] such that if f : [0; h] 4m ! F is any function not <p> Thus, assuming Lemma 4.7, Lemma 4.6 has been proved. 2 The following lemma concerns a family of polynomials that is useful for testing whether or not a function is identically zero on the cube [0; h] j for any integers h; j. Lemma 4.7: ["Zero-tester" Polynomials, <ref> [BFLS91, FGL + 91] </ref>] There exists a family of q O (m) polynomials fR 1 ; R 2 ; : : :g in F 4mh [x 1 ; : : : ; x 4m ] such that if f : [0; h] 4m ! F is any function not identically 0, <p> We do not give the proof here. 5.3.1. History Improvements to the Low-degree Test have accompanied most advances in the theory of probabilistically checkable proofs. The first such test, the multilinearity test, enabled the result MIP =NEXPTIME ([BFL91]. Subsequent tests given in <ref> [BFLS91, FGL + 91] </ref> were crucial for scaling down the result for NEXPTIME to NP. An improvement in the efficiency of those tests was in turn used to prove a new characterization of NP in terms of PCP ([AS92]). <p> Actually, the above list of tests includes two kinds of low-degree tests. The first kind upper-bounds the the degree of the given function in each variable. The second kind and the one in this chapter is of this kind - upper-bounds the total degree. The tests in <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref> are of the first kind. (A particularly beautiful exposition of this type of test ([She91]) was never published, although a later version appears in [FSH94].) Low-degree tests of the first kind consist in estimating the success rate of the given function on axis-parallel lines. (A <p> For instance, vertex-cover and independent-set are another "complementary" pair of problems, and seem to differ greatly in how well they can be approximated in polynomial time. (Vertex cover can be approximated within a factor 2, and independent-set is NP-hard up to a factor n c for some c &gt; 0 <ref> [FGL + 91, AS92, ALM + 92] </ref>.) We find it interesting that large factor approximation is hard for our two complementary problems. We do not know of any other complementary pair with the same behavior. We know of no good approximation algorithms for any of these problems. <p> Historical Notes/Further Reading The lure of proving better inapproximability results for Clique has motivated many developments in the PCP area. The first hardness result for Clique was obtained in <ref> [FGL + 91] </ref>. NP-hardness (of constant-factor approximation) was proved in [AS92]. Further, as observed first in [Zuc91], the constant factor hardness result can be improved to larger factors by using a pseudo-random graph-product. The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction. <p> The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction. The NP-hardness of n * -approximation is due to [ALM + 92], although with a different reduction (namely, the one due to <ref> [FGL + 91] </ref>, plus the idea of [Zuc93]). The connection between MAX-SNP and Clique (albeit with a randomized booster construction) was first discovered in [BS92]. A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. <p> It also generalizes the work of <ref> [BFLS91, FGL + 91] </ref> whose result could be interpreted as saying that Ntime (t (n)) PCP (poly (log t (n)); poly (log t (n))). 8.2.2. Transparent Math Proofs In this section we show that formal proofs in first order logic can be checked very efficiently by a probabilistic algorithm. <p> Open Problems connected with PCP techniques The class PCP (r (n); r (n)) for r = o (log n). The class PCP (o (log n); o (log n)) is contained in NP , but does not cotain any NP-complete problems if P 6= NP ([AS92]). A result in <ref> [FGL + 91] </ref> shows how to reduce the question of membership in a language in PCP (r (n); r (n)) to an instance of Clique of size 2 O (r (n)) .
Reference: [FK94a] <author> U. Feige and J. Kilian. </author> <title> Towards subexponential algorithms for NP. </title> <type> Manuscript, </type> <month> October, </month> <year> 1994. </year>
Reference: [FK94b] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols-low error at affordable rates. </title> <booktitle> In Proc. 26th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 172-183, </pages> <year> 1994. </year>
Reference-contexts: A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements <ref> [ALM + 92, BGLR93, FK94b, BS94] </ref>. The latest improvements center around the concept of free bits ([FK94b]). This is a new parameter associated with the PCP verifier that is upperbounded by the number of query bits, but is often (e.g., in the verifier we constructed) much smaller.
Reference: [FKN90] <author> J. Feigenbaum, S. Kannan, and N. Nisan. </author> <title> Lower bounds on random-self-reducibility. </title> <booktitle> In Proceedings of the 5th Structure in Complexity Theory, </booktitle> <pages> pages 100-109, </pages> <year> 1990. </year> <note> 138 BIBLIOGRAPHY </note>
Reference-contexts: The phenomenon in some sense underlies cryptography. For example, the pseudorandom generator of [BM84] uses the fact that the discrete log problem is random self-reducible. The term random-self-reducible appears to have been explicitly defined first in [AFK89] (see also <ref> [FKN90] </ref>). Many flavors of random-self-reducibility have since been defined and studied ([FF93, Fei93]). Blum ([BK89]) and Lipton ([Lip89]) rephrased the r.s.r. property as follows. Given any program that computes f on "most" inputs, a randomized algorithm can recover the value of f at an arbitrary input x.
Reference: [FL92] <author> U. Feige and L. Lovasz. </author> <title> Two-prover one-round proof systems: Their power and their problems. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 733-741, </pages> <year> 1992. </year>
Reference-contexts: The history of the Low-degree Test will be covered in the next section. The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of <ref> [LS91, FL92] </ref>, although the ideas used in this lemma can be traced back to [BF90, LFKN92]. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from [BLR90, Fre79]. 4.6. <p> HARDNESS OF APPROXIMATIONS 6.4.6. Hardness of Approximating SV 1 Proving the correctness of our reduction to SV 1 involves delving into the geometric structure of Label Cover instances produced with a specific proof system, namely, the one due to Feige-Lovasz proof-system <ref> [FL92] </ref>). We do not know whether a gap-preserving reduction exists from Label-Cover. For the reduction to SV 1 we'll need to prove the hardness of a related (and not very natural) covering problem. <p> The lone problem that does not fit into the above classification is the Shortest Vector Problem using the ` 1 norm. The reduction to it outlined in Section 6.4.6 uses in an 106 CHAPTER 6. HARDNESS OF APPROXIMATIONS intimate way the structure of the proof-system in <ref> [FL92] </ref>, specifically, the fact that the protocol involves a higher dimensional geometry of lines and points. Finally, we mention a recent result by Ran Raz ([Raz94], see Chapter 7 for an introduction) that allows the hardness of Label Cover to be proved using just the hardness result for MAX-3SAT (13). <p> An immediate consequence is that the inapproximability result for Label Cover can be derived from the inapproximabil-ity result for MAX-3SAT (13). Before this development we knew how to prove the hardness of Label Cover only by using the result in <ref> [FL92] </ref> a result that seemed independent of the PCP Theorem. Raz proves the so-called parallel repetition conjecture, a longstanding conjecture from the theory of interactive proofs. We describe only the consequence for restricted PCP verifiers. Let V be any restricted verifier using alphabet .
Reference: [Fre79] <author> R. Freivalds. </author> <title> Fast probabilistic algorithms. </title> <booktitle> In LNCS 74, </booktitle> <pages> pages 57-69. </pages> <publisher> Springer Verlag, </publisher> <year> 1979. </year>
Reference-contexts: The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of [LS91, FL92], although the ideas used in this lemma can be traced back to [BF90, LFKN92]. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from <ref> [BLR90, Fre79] </ref>. 4.6.
Reference: [FRS88] <author> L. Fortnow, J. Rompel, and M. Sipser. </author> <title> On the power of multi-prover interactive protocols. </title> <booktitle> In Proceedings of the 3rd Conference on Structure in Complexity Theory, </booktitle> <pages> pages 156-161, </pages> <year> 1988. </year>
Reference-contexts: Here the single all-powerful prover in the IP scenario is replaced by many all-powerful provers who cannot communicate with one another during the protocol. Again, the mo 14 CHAPTER 2. OLD VS. NEW VIEWS OF NP tivation was cryptography, although soon Fortnow, Rompel and Sipser <ref> [FRS88] </ref> analyzed the new model from a complexity-theoretic viewpoint. They proved that the class MIP is exactly the class of languages for which membership proofs can be checked by a probabilistic polynomial time verifier that has random access to the proof.
Reference: [FSH94] <author> K. Friedl, A. Shen, and Z. Hatsagi. </author> <title> The low-degree test. </title> <booktitle> In Proc. 5th SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994. </year>
Reference-contexts: The tests in [BFL91, BFLS91, FGL + 91, AS92] are of the first kind. (A particularly beautiful exposition of this type of test ([She91]) was never published, although a later version appears in <ref> [FSH94] </ref>.) Low-degree tests of the first kind consist in estimating the success rate of the given function on axis-parallel lines. (A line is called axis-parallel if for some i 2 f1; : : : ; mg and a 1 ; : : : ; a i1 ; a i+1 ; :
Reference: [FT85] <author> A. Frank and E. Tardos. </author> <title> An application of simultaneous approximation in combinatorial optimization. </title> <booktitle> In Proc. 26th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 459-463, </pages> <year> 1985. </year>
Reference-contexts: PROBLEMS ON LATTICES, CODES, LINEAR SYSTEMS 93 host of applications, including integer programming, solving low-density subset-sum problems and breaking knapsack-based codes [LO85], simultaneous diophantine approximation and factoring polynomials over the rationals [LLL82], and strongly polynomial-time algorithms in combinatorial optimization <ref> [FT85] </ref>. For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan [Kan87]. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m .
Reference: [Fur94] <author> M. Furer. </author> <title> Improved hardness results for approximating the chromatic number. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference: [GG81] <author> O. Gabber and Z. Galil. </author> <title> Explicit constructions of linear sized superconcentrators. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 407-425, </pages> <year> 1981. </year>
Reference: [GGJ76] <author> M.R. Garey, R.L. Graham, and D.S. Johnson. </author> <title> Some NP-complete geometric problems. </title> <booktitle> In Proc. 8th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 10-22, </pages> <year> 1976. </year>
Reference: [GJ79] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: a guide to the theory of NP-completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: wide array of optimization problems arising in practice (and which had hitherto defied all efforts of algorithm designers to find efficient algorithms) were proved NP-hard in one swoop, using essentially the same kind of reductions. (For a list of NP-hard problems c. 1979, see the survey by Garey and Johnson <ref> [GJ79] </ref>.) 1 2 CHAPTER 1. INTRODUCTION But one major group of problems seemed not to fit in the framework of NP-completeness: approximation problems. <p> INTRODUCTION 1.1.3. Knowledge assumed of the Reader This dissertation has been written as a survey for the nonspecialist. We assume only familiarity with Turing Machines (and standard conventions about them), asymptotic notation and polynomial time, and NP-completeness. For an introduction to all these see <ref> [GJ79] </ref>. A list of assumed algebraic facts, with brief proofs, appears in Appendix A. However, most readers should find that they can understand most of the dissertation on the basis of just the following mantra. A non-zero univariate polynomial of degree d has at most d roots in a field. <p> The Cook-Levin result underlies classical work on NP-completeness, since most other problems are proven NP-complete by doing reductions from 3SAT. We briefly recall the textbook version of their construction (for further details, see <ref> [GJ79] </ref>). How can a generic reduction be given from all NP languages to 3SAT? After all, the notion of membership proof differs widely for different NP languages. <p> where jth entry in line i contains the following information : (i) the contents of the jth cell of the tape at time i; and (ii) whether or not the finite control was in that cell or not, and if so, what state it was in. (see Figure 2.3 in <ref> [GJ79] </ref>). Let L be an NP-language, and M be the verifier for L. A tableau is valid for M if each step of the computation followed correctly from the previous step, and the verifier is in an accept state in the last line. <p> The overall formula is the ^ of the formulae expressing checks (a), (b), and (c). For instance the formula expressing (c) is ^ (Formula expressing correctness of window around jth cell of ith line): Example 2.2: (We give some details of the Cook-Levin construction; see <ref> [GJ79] </ref> for further details.) Assume the machine's alphabet is f0; 1g. The corresponding formula has for each i; j T the variables z ij , y ij and for each state q in the finite control, a variable s ijq . <p> This imparts our reduction a global structure. In contrast, classical NP-completeness reductions usually perform local transformations of the input. 2.5. History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered [Joh74, SG76]. (See <ref> [GJ79] </ref> for a discussion). <p> understandably, this new way of doing NP-completeness reductions is generally perceived as not very "user-friendly." Traditionally, NP-completeness reductions which prove NP-hardness only of exact optimization are far simpler: each one is based, in a simple way, upon one of a few canonical NP-complete problems. (The number of canonical problems in <ref> [GJ79] </ref> is six.) In this chapter we attempt to identify problems that can serve as canonical problems for proving inapproximability results. The aim is to to derive all known inapproximability results using as few assumptions (in other words, as few canonical problems) as possible. <p> Otherwise it is said to misclassify the point. Finding a hypothesis that minimizes the number of misclassfications is the open hemispheres problem, which is NP-hard <ref> [GJ79] </ref>. Define the error of the algorithm as the number of misclassifications by its hypothesis, and the noise of the sample as the error of the best possible algorithm. Let the failure ratio of the algorithm be the ratio of the error to noise. Definition 6.10: Failure Ratio. <p> Proof: Consider the transformation from MAX-3SAT (13) to Vertex Cover in <ref> [GJ79] </ref>, p.54. The degree of any vertex is no more than the maximum number of clauses that a variable appears in, in this case 13. So in particular, the number of edges is linear in the number of vertices. <p> Proof: In general for any graph G we have (see <ref> [GJ79] </ref> again) !(G) = n V C min (G); where G is the complement graph of G, that is, a graph with the same vertex set as G but with edge-set f (u; v) : (u; v) 62 Gg . Let G be the graph of Theorem 6.22.
Reference: [GL89] <author> O. Goldreich and L.A. Levin. </author> <title> A hard-core predicate for all one-way functions. </title> <booktitle> In Proc. 21st ACM Symp. on Theory of Computing, </booktitle> <pages> pages 25-32, </pages> <year> 1989. </year>
Reference: [GLR + 91] <author> P. Gemmell, R. Lipton, R. Rubinfeld, M. Sudan, and A. Wigderson. </author> <title> Self-testing/correcting for polynomials and for approximate functions. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 32-42, </pages> <year> 1991. </year>
Reference-contexts: Low-degree tests of the second kind arose in program checking. The earliest such test appears in [BLR90], where it is called the multilinearity test. Subsequent tests appearing in <ref> [GLR + 91, RS92] </ref> work for higher degree. In fact, these latter tests are identical to the one used in this chapter, but their analysis was not as good (it could not show that the test works when the success rate is less than 1 O (1=d)).
Reference: [GMR89] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <journal> SIAM J. of Computation, </journal> <volume> 18 </volume> <pages> 186-208, </pages> <year> 1989. </year>
Reference-contexts: The desire to prove such an unapproximability result motivated the discovery of the PCP theorem. Roots of PCP. The roots of the definition of PCP (specifically, the fact that the verifier is randomized) go back to the definition of Interactive Proofs (Goldwasser, Micali, and Rackoff <ref> [GMR89] </ref>) and Arthur-Merlin games (Babai [Bab85], see also [BM88]). Two complexity classes arise from their definitions: IP and AM respectively.
Reference: [GMS87] <author> O. Goldreich, Y. Mansour, and M. Sipser. </author> <title> Interactive proof systems: Provers that never fail and random selection. </title> <booktitle> In Proc. 28th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 449-461, </pages> <year> 1987. </year>
Reference-contexts: the given input is in the language. (The difference between the two classes is that in the definition of IP, the prover cannot read the verifier's random string.) Early results about these classes fleshed out their properties, including the surprising fact that they are the same class ([GS86]), see also <ref> [GMS87] </ref>). The next step involved the invention of multi-prover interactive proofs, and the associated complexity class MIP by Ben-Or, Goldwasser, Killian, and Wigderson ([BGKW88]). Here the single all-powerful prover in the IP scenario is replaced by many all-powerful provers who cannot communicate with one another during the protocol.
Reference: [Gol94] <author> O. Goldreich. </author> <title> Probabilistic proof systems. </title> <type> Technical Report RS-94-28, </type> <institution> Basic Research in Computer Science, Center of the Danish National Research Foundation, </institution> <month> September </month> <year> 1994. </year> <booktitle> (To appear in the Proceedings of the International Congress of Mathematicians, 1994. </booktitle> <publisher> Birkhauser Verlag.). </publisher>
Reference: [GS86] <author> S. Goldwasser and M. Sipser. </author> <title> Private versus public coins in interactive proof systems. </title> <booktitle> In Proc. 18th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 59-68, </pages> <year> 1986. </year>
Reference: [GS92] <author> P. Gemmell and M. Sudan. </author> <title> Highly resilient correctors for polynomials. </title> <journal> Information and Computation, </journal> <volume> 43 </volume> <pages> 169-174, </pages> <year> 1992. </year>
Reference-contexts: Specifically, such a low-degree test enables 5.3. DISCUSSION 77 queries to be "aggregated" (see Section 4.1.2), a property crucial in constructing the verifier in Theorem 3.2. The algebraic ideas used in the proof of Lemma 5.2 were originally inspired by a Lemma of [BW], as used in <ref> [GS92] </ref>. 78 CHAPTER 5.
Reference: [GVY93] <author> N. Garg, V.V. Vazirani, and M. Yannakakis. </author> <title> Approximate max-flow min-(multi)-cut theorems and their applications. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 698 -707, </pages> <year> 1993. </year> <note> BIBLIOGRAPHY 139 </note>
Reference-contexts: following clean problem seems to be a good candidate to prove hard to approximate: Given an instance of MAX-2SAT, delete the smallest number of clauses so as to make it satisfiable. (To see why this fits the edge-deletion framework, and also an approximation algorithm for a related problem, refer to <ref> [KARR90, GVY93] </ref>.) 9.1.2. Improving existing hardness results There are two ways to improve existing hardness results. First, to base the result on the assumption P 6= NP (many results are currently based upon stronger assumptions). 9.1.
Reference: [GW94] <author> M. Goemans and D. Williamson. </author> <title> A 0.878 approximation algorithm for MAX-2SAT and MAX-CUT. </title> <booktitle> In Proc. 26th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 422-431, </pages> <year> 1994. </year>
Reference-contexts: given any input its output is an assignment B such that OPT () val (B=) OPT (): This dissertation addresses the following question: For what values of c can c-approximations to MAX-3SAT be computed in polynomial time? For c = 4=3 this was known to be possible ([Yan92], see also <ref> [GW94] </ref>). (The algorithm for c = 2 is actually quite straightforward.) Whether or not the same was true for every fixed constant c &gt; 1 was not known. The Cook-Levin reduction does not rule out the existence of polynomial-time algorithms that compute c-approximations for every fixed c &gt; 1. <p> The best polynomial time algorithms achieve approximation factors of 2, 3=2 and 4=30:01 respectively. We only know that (1+*)-approximations are hard for * 0:01. Can the hardness result be improved? A surprising development in this area is the result of Goemans and Williamson <ref> [GW94] </ref>, where it is shown that MAX-2SAT and MAX-CUT, two other MAX-SNP-hard problems with with classic 2-approximation and 4=3-approximation algorithms respectively, can be approximated within a factor better than 1:13. 9.1.3. Obtaining Logical Insight into Approximation Problems In Section 6.6 we gave a survey of known inapproximability results.
Reference: [HS92] <author> K-U. Hoeffgen and H-U. Simon. </author> <title> Robust trainability of single neurons. </title> <booktitle> In Proceedings of the Conference of Learning Theory, </booktitle> <pages> pages 428-439, </pages> <year> 1992. </year>
Reference-contexts: Kannan has shown us a simple polynomial time algorithm that uses Helly's theorem to approximate MIN-UNSATISFY within a factor of m + 1. 94 CHAPTER 6. HARDNESS OF APPROXIMATIONS Failure Ratio. That the failure ratio is hard to approximate has often been conjectured in learning theory <ref> [HS92] </ref>. We know of no good approximation algorithms. A failure ratio m can be achieved by Kannan's idea, mentioned above. 6.4.4. A Set of Vectors In this section we use Label Cover instances to define a set of vectors that will be used by all the reductions.
Reference: [Joh74] <author> D. S. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 256-278, </pages> <year> 1974. </year>
Reference-contexts: This imparts our reduction a global structure. In contrast, classical NP-completeness reductions usually perform local transformations of the input. 2.5. History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered <ref> [Joh74, SG76] </ref>. (See [GJ79] for a discussion).
Reference: [Joh92] <author> D. S. Johnson. </author> <title> The NP-completeness column: an ongoing guide. </title> <journal> Journal of Algorithms, </journal> <volume> 13 </volume> <pages> 502-524, </pages> <year> 1992. </year>
Reference: [JP78] <author> D. S. Johnson and F. P. Preparata. </author> <title> The densest hemisphere problem. </title> <journal> Theoretical Computer Science, </journal> <volume> 6 </volume> <pages> 93-107, </pages> <year> 1978. </year>
Reference-contexts: Problems on Linear Systems. Note that a solution to MAX-SATISFY is exactly the complement of a solution to MIN-UNSATISFY, and therefore the two problems have the same complexity. (Indeed, it is known that both are NP-hard; this is implicit e.g. in <ref> [JP78] </ref>). However, the same need not be true for approximate solutions.
Reference: [Kan87] <author> R. Kannan. </author> <title> Minkowski's convex body theorem and integer programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3), </volume> <year> 1987. </year>
Reference-contexts: For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan <ref> [Kan87] </ref>. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m . A modification of this algorithm [Bab86] yields the same for NV p . <p> Schnorr modified the Lovasz algorithm and obtained, for every * &gt; 0, approximations within O (2 *m ) in polynomial time for these problems [Sch85]. On the other hand, Van Emde Boas showed that NV p is NP-hard for all p 1 ([vEB81]; see <ref> [Kan87] </ref> for a simpler proof). Lagarias showed that the shortest vector problem is NP-hard under the ` 1 (i.e. max) norm. But it is still an open problem whether SV p is NP-hard for any other p, and specifically for p = 2. <p> To see this for SV 1 , notice that the solutions in SV 1 and SV 2 are always within a factor p m of each other. For NV 2 the implication follows from Kannan's result <ref> [Kan87] </ref> that approximating NV 2 within a factor ( p d) is reducible to SV 2 .
Reference: [Kan92] <author> V. Kann. </author> <title> On the approximability of NP-complete optimization problems. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm, Sweden, </institution> <year> 1992. </year>
Reference: [Kar72] <author> R. M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In Miller and Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85-103. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: caused great interest in [FGL + 91] was their corollary: If the clique number of a graph can be approximated within any fixed constant factor, then all NP problems can be solved deterministically in time n O (log log n) . (Computing the clique number is a well-known NP-complete problem <ref> [Kar72] </ref>.) They showed how to reduce every problem in PCP (log n log log n) and as a special subcase, every problem in NP to the Clique problem in such a way that the "gap" (probability 1 versus probability 1=2) used in the definition of PCP translates into a gap in
Reference: [KARR90] <author> P. Klein, A. Agarwal, R. Ravi, and S. Rao. </author> <title> Approximation through multicommodity flow. </title> <booktitle> In Proc. 31st IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 726 -727, </pages> <year> 1990. </year>
Reference-contexts: following clean problem seems to be a good candidate to prove hard to approximate: Given an instance of MAX-2SAT, delete the smallest number of clauses so as to make it satisfiable. (To see why this fits the edge-deletion framework, and also an approximation algorithm for a related problem, refer to <ref> [KARR90, GVY93] </ref>.) 9.1.2. Improving existing hardness results There are two ways to improve existing hardness results. First, to base the result on the assumption P 6= NP (many results are currently based upon stronger assumptions). 9.1.
Reference: [Kil92] <author> J. Kilian. </author> <title> A note on efficient zero-knowledge proofs and arguments. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 723-732, </pages> <year> 1992. </year>
Reference-contexts: A tighter construction ([PS94]) achieves size n 1+* . Can we achieve size n poly (log n)? The size is important for cryptographic applications <ref> [Kil92] </ref>. Size of probabilistically checkable codes. These codes were defined in Section 8.2.6. The best construction ([PS94]) achieves a constant minimum distance (say, ffi min = 0:01) and encodes n bits with n 1+* bits.
Reference: [KLS93] <author> S. Khanna, N. Linial, and S. Safra. </author> <title> On the hardness of approximating the chromatic number. </title> <booktitle> In Proceedings of the 2nd Israel Symposium on Theory and Computing Systems, ISTCS, </booktitle> <pages> pages 250-260. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference: [KMR93] <author> D. Karger, R. Motwani, and G.D.S. Ramkumar. </author> <title> On approximating the longest path in a graph. </title> <booktitle> In Proceedings of Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 421-430. </pages> <publisher> LNCS (Springer-Verlag), v. </publisher> <address> 709, </address> <year> 1993. </year>
Reference: [KMS94] <author> D. Karger, R. Motwani, and M. Sudan. </author> <title> Graph coloring through semi-definite programming. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference: [KMSV94] <author> S. Khanna, R. Motwani, M. Sudan, and U. Vazirani. </author> <title> Computational versus syntactic views of approximability. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 819-830, </pages> <year> 1994. </year>
Reference: [Knu74] <author> D.E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> vol. 2. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: Using Euclidean pseudo-division for polynomials (see <ref> [Knu74] </ref>, p. 407), we obtain polynomials q (x; y), r (x; y) of bidegree (d; k + dl) and (l 1; k + dl) respectively such that (p l (y)) d h (x; y) = q (x; y)g (x; y) + r (x; y): 5.2.
Reference: [Lev73] <author> L. Levin. </author> <title> Universal'nye perebornye zadachi (universal search problems : in Russian). </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference: [LFKN92] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic methods for interactive proof systems. </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 859-868, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan [BK89], Lipton [Lip89], and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan <ref> [LFKN92] </ref>, and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund [BFL91]). These connections between traditional and nontraditional complexity classes were proved using novel algebraic technques, some of which will be covered later in this book. Emergence of PCP. <p> This observation was the starting point for the theory of self-testing/self-correcting programs ([BK89, Lip89, BLR90]). Most functions to which this theory seemed to apply were algebraic in nature (see [Yao90] for some nonalgebraic examples, though). A new insight was made in <ref> [BF91, Sha92, LFKN92] </ref>: logical formulae can be represented as polynomials of low degree. But recall that different classes of logical formulae are complete for complexity classes like PSPACE, NEXPTIME, NP etc.. This at once suggests that randomized techniques from program-testing/correcting should be applicable to the study of conventional complexity classes. <p> The polynomial extension used in this chapter does not suffer from this problem. It is due to ([BFLS91]). The Sum-check procedure (Procedure 4.3) is due to <ref> [LFKN92] </ref>. Thus Lemma 4.5 could be proved with minor modifications of the above-mentioned results, although no previous paper had proved it explicitly before [AS92]. All other results in this chapter, except the Low-degree Test and the idea of checking split assignments, are from [ALM + 92]. <p> The history of the Low-degree Test will be covered in the next section. The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of [LS91, FL92], although the ideas used in this lemma can be traced back to <ref> [BF90, LFKN92] </ref>. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from [BLR90, Fre79]. 4.6.
Reference: [Lip89] <author> R. Lipton. </author> <title> Efficient checking of computations. </title> <booktitle> In Proceedings of 6th STACS, </booktitle> <year> 1989. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan [BK89], Lipton <ref> [Lip89] </ref>, and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir [Sha92]) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund [BFL91]).
Reference: [LLL82] <author> A.K. Lenstra, H.W. Lenstra, and L. Lovasz. </author> <title> Factoring polynomials with rational coefficients. </title> <journal> Math. Ann., </journal> <volume> 261 </volume> <pages> 513-534, </pages> <year> 1982. </year> <note> 140 BIBLIOGRAPHY </note>
Reference-contexts: Significance of the Results We discuss the significance of the problems we defined. Lattice Problems. The SV problem is particularly important because even relatively poor polynomial-time approximate solutions to it (within c m , <ref> [LLL82] </ref>) have been used in a 6.4. PROBLEMS ON LATTICES, CODES, LINEAR SYSTEMS 93 host of applications, including integer programming, solving low-density subset-sum problems and breaking knapsack-based codes [LO85], simultaneous diophantine approximation and factoring polynomials over the rationals [LLL82], and strongly polynomial-time algorithms in combinatorial optimization [FT85]. <p> even relatively poor polynomial-time approximate solutions to it (within c m , <ref> [LLL82] </ref>) have been used in a 6.4. PROBLEMS ON LATTICES, CODES, LINEAR SYSTEMS 93 host of applications, including integer programming, solving low-density subset-sum problems and breaking knapsack-based codes [LO85], simultaneous diophantine approximation and factoring polynomials over the rationals [LLL82], and strongly polynomial-time algorithms in combinatorial optimization [FT85]. For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan [Kan87]. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m <p> breaking knapsack-based codes [LO85], simultaneous diophantine approximation and factoring polynomials over the rationals <ref> [LLL82] </ref>, and strongly polynomial-time algorithms in combinatorial optimization [FT85]. For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan [Kan87]. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m . A modification of this algorithm [Bab86] yields the same for NV p .
Reference: [LLS90] <author> J. Lagarias, H.W. Lenstra, and C.P. Schnorr. </author> <title> Korkine-Zolotarev bases and successive minima of a lattice and its reciprocal lattice. </title> <journal> Combinatorica, </journal> <volume> 10 </volume> <pages> 333-348, </pages> <year> 1990. </year>
Reference-contexts: For NV 2 the implication follows from Kannan's result [Kan87] that approximating NV 2 within a factor ( p d) is reducible to SV 2 . We also note that approximating NV 2 within any factor greater than m 1:5 is unlikely to be NP-complete, since Lagarias et al. <ref> [LLS90] </ref> have shown that this problem lies in NP " co-NP. Problems on Linear Systems.
Reference: [LO85] <editor> J.C. Lagarias and A.M. Odlyzko. </editor> <title> Solving low-density subset-sum problems. </title> <journal> Journal of the ACM, </journal> <volume> 32 </volume> <pages> 229-246, </pages> <year> 1985. </year>
Reference-contexts: Lattice Problems. The SV problem is particularly important because even relatively poor polynomial-time approximate solutions to it (within c m , [LLL82]) have been used in a 6.4. PROBLEMS ON LATTICES, CODES, LINEAR SYSTEMS 93 host of applications, including integer programming, solving low-density subset-sum problems and breaking knapsack-based codes <ref> [LO85] </ref>, simultaneous diophantine approximation and factoring polynomials over the rationals [LLL82], and strongly polynomial-time algorithms in combinatorial optimization [FT85]. For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz [Lov86] or Kannan [Kan87].
Reference: [Lov86] <author> L. Lovasz. </author> <title> Algorithmic theory of numbers, graphs and convexity. </title> <booktitle> NSF-CBMS Reg. Conference Series. </booktitle> <publisher> SIAM, </publisher> <year> 1986. </year>
Reference-contexts: For details and more applications, especially to classical problems in the "geometry of numbers", see the surveys by Lovasz <ref> [Lov86] </ref> or Kannan [Kan87]. Lovasz's celebrated lattice transformation algorithm [LLL82] runs in polynomial time and approximates SV p (p 1) within c m . A modification of this algorithm [Bab86] yields the same for NV p .
Reference: [LPS88] <author> A. Lubotzky, R. Phillips, and P. Sarnak. </author> <title> Ramanujam graphs. </title> <journal> Combinatorica, </journal> <volume> 8 </volume> <pages> 261-277, </pages> <year> 1988. </year>
Reference: [LR88] <author> T. Leighton and S. Rao. </author> <title> An approximate max-flow min-cut theorem for uniform multi-commodity flow problems with applications to approximation algorithms. </title> <booktitle> In Proc. 29th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 422-431, </pages> <year> 1988. </year>
Reference: [LS91] <author> D. Lapidot and A. Shamir. </author> <title> Fully parallelized multi prover protocols for NEXPTIME. </title> <booktitle> In Proc. 32nd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 13-18, </pages> <year> 1991. </year>
Reference-contexts: The history of the Low-degree Test will be covered in the next section. The discovery of the proof of Lemma 4.4 was influenced by the parallelization procedures of <ref> [LS91, FL92] </ref>, although the ideas used in this lemma can be traced back to [BF90, LFKN92]. The design of the verifier of Section 4.3 owes much to existing examples of self-testing/correcting programs from [BLR90, Fre79]. 4.6.
Reference: [Lun92] <author> C. Lund. </author> <title> The Power of Interaction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference: [LV89] <author> N. Linial and U. Vazirani. </author> <title> Graph-products and chromatic number. </title> <booktitle> In Proc. 30th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 124-128, </pages> <year> 1989. </year>
Reference-contexts: if coloring 3-colorable graphs with even poly (log n) colors is hard, then approximating Clique within a factor n 1ffi is hard, where ffi is an arbitrarily small positive constant. (A relevant fact here which Blum also uses in his result is that chromatic number is "self-improvable", as shown in <ref> [LV89] </ref>.) Classic MAX-SNP-hard problems. These include vertex cover, TSP with triangle inequality, MAX-SAT, etc. The best polynomial time algorithms achieve approximation factors of 2, 3=2 and 4=30:01 respectively. We only know that (1+*)-approximations are hard for * 0:01.
Reference: [LY93] <author> C. Lund and M. Yannakakis. </author> <title> The approximation of maximum subgraph problems. </title> <booktitle> In Proceedings of International Colloquium on Automata, Languages and Programming, ICALP, </booktitle> <pages> pages 40-51, </pages> <year> 1993. </year>
Reference: [LY94] <author> Carsten Lund and Mihalis Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> 41(5) </volume> <pages> 960-981, </pages> <year> 1994. </year>
Reference-contexts: We do not make this a part of the definition because it makes our reductions more difficult to describe. 2 The Label Cover problem was implicit in <ref> [LY94] </ref> and was defined in [ABSS93]. Although an ungainly problem at first sight, it is quite useful in reductions. <p> A similar transformation works in the other direction: from Hitting Set to Label Cover. The following weak duality (implicit in <ref> [LY94] </ref>) links the max and min versions of Label Cover. Note how our convention about stating costs/values as ratios allows a clean statement. <p> Currently the constant is of the order of 10 2 for most problems, and about 0:02 for MAX-3SAT ([BGLR93, BS94]). Class II. This class contains problems for which (log n)-approximation is hard. Currently it contains only Set Cover and related problems like Dominating Set. A reduction in <ref> [LY94] </ref> shows that if there is a polynomial-time algorithm that computes !(log n)-approximations for these then all NP problems can be solved in n O (log log n) time.
Reference: [Mic94] <author> S. Micali. </author> <title> CS (Computationally Sound) proofs. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 436-453, </pages> <year> 1994. </year>
Reference-contexts: Our technique does not represent a way to check software. We assumed throughout that software for the nondeterministic program P is reliable. 8.2.4. Micali's Certificates for VLSI Chips Micali <ref> [Mic94] </ref> notes that the above idea of checking computations, since it assumes that software is reliable, makes more sense in the context of checking VLSI chips. Chips are designed carefully (in other words, they are programmed with reliable software), but the fabrication process might introduce bugs in the hardware.
Reference: [MP68] <author> M. Minsky and S. Papert. Perceptrons, </author> <year> 1968. </year>
Reference-contexts: The next problem is a well-known one in learning theory: learning a halfspace in the presence of malicious errors. The problem arises in the context of training a perceptron, a learning model first studied by Minsky and Papert <ref> [MP68] </ref>. Rather than describing the learning problem in the usual PAC setting ([Val84]), we merely present the underlying combinatorial problem.
Reference: [MS77] <author> F. J. MacWilliams and N. J. A. Sloane. </author> <title> The Theory of error-correcting codes. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1977. </year>
Reference-contexts: Can the size of the encoding be reduced to n poly (log n), or better still, to O (n)? Shannon's theorem says the size would be O (n) if we didn't impose the probabilistic checkability condition <ref> [MS77] </ref>). Improving the low-degree test. Does the low-degree test work even for high error rates? In other words, is Theorem 5.1 in Chapter 5 true even when the success rate is less than 0:5 (say), and jFj = poly (d)? Self-correction on polynomials.
Reference: [Pap77] <author> C. Papadimitriou. </author> <title> The Euclidean travelling salesman problem is NP-complete. </title> <journal> Theoretical Computer Science, </journal> <volume> 4 </volume> <pages> 237-244, </pages> <year> 1977. </year>
Reference: [Pap83] <author> C. Papadimitriou. </author> <title> Games against nature. </title> <booktitle> In Proc. 24th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 446-450, </pages> <year> 1983. </year>
Reference-contexts: Since it seemed "clear" that MIP was quite smaller than NEXPTIME, the statement MIP NEXPTIME was considered unsatisfactorily weak. (A similar situation prevailed for IP, where the analogous statement, IP PSPACE -proved implicitly by Papadimitriou in <ref> [Pap83] </ref>- was considered quite weak.) The next development in this area came as a big surprise.
Reference: [PS94] <author> A. Polishchuk and D. Spielman. </author> <title> Nearly linear size holographic proofs. </title> <booktitle> In Proc. 26th ACM Symp. on Theory of Computing, </booktitle> <year> 1994. </year>
Reference-contexts: But a somewhat simpler proof for the bivariate case appears in <ref> [PS94] </ref>. More importantly, that paper lowers the field size required in the lemma to O (d); an improvement over O (d 3 ) as required by our proof. Many people have conjectured the following, although a proof still eludes us. Conjecture 5.1: Let jF j = poly (d). <p> Kilian ([Kil92]) shows how to reduce the communication requirement. His idea is to hash down, in a cryptographic fashion, the "probabilistically checkable" database of Strong Form 1. (Recall that Micali's idea is similar.) He needs something more than the Strong Form 1, specifically, the fact (proved in <ref> [BFLS91, PS94] </ref>) that the database given by Strong Form 1 has size n 1+* , where n is the size of the 3SAT formula. 8.2. THE APPLICATIONS 123 8.2.8. <p> They show that if a polynomial time algorithm exists, then Ntime (t (n)) Dtime (2 t (n) 1* ) for some small positive constant *. They use the version of Strong Form 1 due to <ref> [PS94] </ref> (see the note following Strong Form 1.) Their idea is to do a reduction to clique using this strong form, and then apply a booster-like construction as in Section 6.5. Note: Noam Nisan has since shown the same result without using the PCP Theorem. 124 CHAPTER 8.
Reference: [PY91] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation and complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 43 </volume> <pages> 425-440, </pages> <year> 1991. </year>
Reference-contexts: We show how to change instances of MAX-3SAT into instances of MAX-3SAT (13), without changing the gap of ffi by much. The reduction is a slight simplification of the one in <ref> [PY91] </ref> (although their reduction yields instances of MAX-3SAT (7), and uses weaker expanders). We need the following result about explicit constructions of expander graphs ([LPS88]): There is a procedure that, for every integer k, can construct in poly (k) time a 6-regular 6.1. <p> So what the gap-preserving reduction actually ensures is that Label Cover instances with optimum cost exactly 1 are mapped to instances with cost at least c. This is an example of a reduction for which we do not know whether an L-reduction (in the sense of <ref> [PY91] </ref>) exists. However, the gap-preserving reduction as stated above is still good enough for proving inapproximability in conjunction with the reduction stated in Theorem 6.7, since that other reduction (quite conveniently, it seems) did map YES instances to Label Cover instances with optimum cost exactly 1. <p> Consequently Corollary 6.9 implies the above inapproximability result for them. The following is a partial list of MAX-SNP-hard problems : MAX-SAT, MAX-2SAT (13), Independent Set, Vertex Cover, Max-Cut (all in <ref> [PY91] </ref>), Metric TSP ([PY93b]), Steiner Tree ([BP89]), Shortest Superstring ([BJL + 91]), Multiway Cuts ([DJP + 92]), and 3-D Matching ([Kan92]). Many more continue to be found. Since [ALM + 92], there have been improvements in the value of the constant * for which the above results are 6.6. <p> THE APPLICATIONS 123 8.2.8. Khanna et al.'s Structure Theorem for MAX-SNP The class APX contains optimization problems which can be approximated within some constant factor in polynomial time. An open question posed in <ref> [PY91] </ref> was: is APX contained in MAX-SNP? The answer turns out to depend upon how MAX-SNP is defined. It appears that the definition intended in [PY91] (although never stated explicitly thus) was that every problem that has an approximation-preserving reduction to MAX-3SAT should be considered to be in MAX-SNP. <p> An open question posed in <ref> [PY91] </ref> was: is APX contained in MAX-SNP? The answer turns out to depend upon how MAX-SNP is defined. It appears that the definition intended in [PY91] (although never stated explicitly thus) was that every problem that has an approximation-preserving reduction to MAX-3SAT should be considered to be in MAX-SNP. With this definition, MAX-SNP equals APX ([KMSV94]).
Reference: [PY93a] <author> C. Papadimitriou and M. Yannakakis. </author> <title> On limited nondeterminism and the complexity of the v-c dimension. </title> <booktitle> In Proc. 9th IEEE Structure in Complexity Theory Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Does a polynomial time algorithm exist? This question was raised in <ref> [PY93a] </ref>, and is also related to the study of fixed parameter intractability ([DF95]). Recently, Feige and Killian ([FK94a]) related this question to another question about traditional complexity classes. <p> A result in [FGL + 91] shows how to reduce the question of membership in a language in PCP (r (n); r (n)) to an instance of Clique of size 2 O (r (n)) . So the membership problem seems to involve limited nondeterminism <ref> [PY93a] </ref>) but it is open if there is an exact characterization lurking there. Size of the proof.
Reference: [PY93b] <author> C. Papadimitriou and M. Yannakakis. </author> <title> The traveling salesman problem with distances one and two. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18(1) </volume> <pages> 1-11, </pages> <year> 1993. </year>
Reference: [Raz94] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <type> Manuscript, </type> <year> 1994. </year> <note> BIBLIOGRAPHY 141 </note>
Reference-contexts: The result is proved using an expansion-like property of the graph (V 1 ; V 2 ; E), which we don't state here. 7.2. UNIFYING LABEL-COVER AND MAX-3SAT (13) 113 7.2. Unifying Label-Cover and MAX-3SAT (13) In this section we state a recent result of Ran Raz from <ref> [Raz94] </ref>. An immediate consequence is that the inapproximability result for Label Cover can be derived from the inapproximabil-ity result for MAX-3SAT (13).
Reference: [RS92] <author> R. Rubinfeld and M. Sudan. </author> <title> Testing polynomial functions efficiently and over rational domains. </title> <booktitle> In Proc. 3rd Annual ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 23-32, </pages> <year> 1992. </year>
Reference-contexts: Owing to its great dependence upon [AS92], the proof of the PCP Theorem is often attributed to jointly to [ALM + 92, AS92]. Among other papers that were influential in the above developments were those by Beaver and Feigenbaum ([BF90]), Lapidot and Shamir ([LS91]), Rubinfeld and Sudan <ref> [RS92] </ref>, and Feige and Lovasz ([FL92]). Their contributions will be described in appropriate places later. Other characterizations of NP. Researchers have discovered other probabilistic characterizations of NP. <p> Discussion The results in this chapter comprise two parts. One part, given in Section 5.2, shows that the correctness of the Low-degree Test follows from the correctness of the bivariate case of the test (in other words, from Lemma 5.2). This part is due to Rubinfeld and Sudan <ref> [RS92] </ref>, although our proof is somewhat different in the way it uses symmetry arguments. The other part of the chapter, Section 5.1, shows the correctness of the bivariate case. <p> Low-degree tests of the second kind arose in program checking. The earliest such test appears in [BLR90], where it is called the multilinearity test. Subsequent tests appearing in <ref> [GLR + 91, RS92] </ref> work for higher degree. In fact, these latter tests are identical to the one used in this chapter, but their analysis was not as good (it could not show that the test works when the success rate is less than 1 O (1=d)). <p> In fact, these latter tests are identical to the one used in this chapter, but their analysis was not as good (it could not show that the test works when the success rate is less than 1 O (1=d)). The idea of combining the work of [AS92] and <ref> [RS92] </ref> to obtain the low-degree test of this chapter is due to [ALM + 92].This combination allows the test to work even when the success rate is a constant (independent of the degree).
Reference: [Rub90] <author> R. Rubinfeld. </author> <title> A mathematical theory of self-checking, self-testing and self-correcting Programs. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1990. </year>
Reference: [Sch85] <author> C.P. Schnorr. </author> <title> A hierarchy of polynomial-time basis reduction algorithms. </title> <booktitle> In Proceedings of Conference on Algorithms, Pecs (Hungary), </booktitle> <pages> pages 375-386. </pages> <publisher> North-Holland, </publisher> <year> 1985. </year>
Reference-contexts: A modification of this algorithm [Bab86] yields the same for NV p . Schnorr modified the Lovasz algorithm and obtained, for every * &gt; 0, approximations within O (2 *m ) in polynomial time for these problems <ref> [Sch85] </ref>. On the other hand, Van Emde Boas showed that NV p is NP-hard for all p 1 ([vEB81]; see [Kan87] for a simpler proof). Lagarias showed that the shortest vector problem is NP-hard under the ` 1 (i.e. max) norm.
Reference: [SG76] <author> S. Sahni and T. Gonzalez. </author> <title> P-complete approximation problems. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 555-565, </pages> <year> 1976. </year>
Reference-contexts: This imparts our reduction a global structure. In contrast, classical NP-completeness reductions usually perform local transformations of the input. 2.5. History and Background Approximability. The question of approximability started receiving attention soon after NP-completeness was discovered <ref> [Joh74, SG76] </ref>. (See [GJ79] for a discussion).
Reference: [Sha92] <author> A. Shamir. </author> <title> IP = PSPACE. </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 869-877, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Techniques from program-checking (due to Blum and Kannan [BK89], Lipton [Lip89], and Blum, Luby and Rubin-feld [BLR90]), as well as some new ideas about how to represent logical formulae with polynomials (Babai and Fortnow [BF91], Fortnow, Lund, Karloff and Nisan [LFKN92], and Shamir <ref> [Sha92] </ref>) were used to show that IP= PSPACE ([LFKN92, Sha92]) and MIP= NEXPTIME (Babai, Fortnow, and Lund [BFL91]). These connections between traditional and nontraditional complexity classes were proved using novel algebraic technques, some of which will be covered later in this book. Emergence of PCP. <p> This observation was the starting point for the theory of self-testing/self-correcting programs ([BK89, Lip89, BLR90]). Most functions to which this theory seemed to apply were algebraic in nature (see [Yao90] for some nonalgebraic examples, though). A new insight was made in <ref> [BF91, Sha92, LFKN92] </ref>: logical formulae can be represented as polynomials of low degree. But recall that different classes of logical formulae are complete for complexity classes like PSPACE, NEXPTIME, NP etc.. This at once suggests that randomized techniques from program-testing/correcting should be applicable to the study of conventional complexity classes.
Reference: [She91] <author> A. Shen. </author> <title> Multilinearity test made easy. </title> <type> Manuscript, </type> <year> 1991. </year>
Reference: [Sud92] <author> M. Sudan. </author> <title> Efficient checking of polynomials and proofs and the hardness of approximation problems. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1992. </year>
Reference-contexts: To the best of our knowledge, this dissertation represents the first self-contained exposition of the entire proof of the PCP Theorem, incorporating all necessary lemmas from the papers ([AS92, ALM + 92]), and other previous work. For other (almost complete) expositions we refer the reader to <ref> [Sud92, ALM + 92] </ref>). However, the exposition in [Sud92] takes a different viewpoint: Its main results concern program checking, and the PCP theorem is derived as a corollary to those results. <p> For other (almost complete) expositions we refer the reader to [Sud92, ALM + 92]). However, the exposition in <ref> [Sud92] </ref> takes a different viewpoint: Its main results concern program checking, and the PCP theorem is derived as a corollary to those results. We feel that in the long run the algebraic techniques used in proving the PCP theorem will find many other applications. <p> The proof of the bivariate case is a consequence of a more general (multivariate) result in [AS92], which is stated in the next section. (The proof of the bivariate case as given here uses a little modification due to <ref> [Sud92] </ref>.) No simpler proof of the general result of [AS92] is known. But a somewhat simpler proof for the bivariate case appears in [PS94].
Reference: [Val84] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference: [vEB81] <author> P. van Emde Boas. </author> <title> Another NP-complete problem and the complexity of computing short vectors in a lattice. </title> <type> Technical Report 81-04, </type> <institution> Math. Inst. Univ. </institution> <address> Amsterdam, </address> <year> 1981. </year>
Reference: [VV86] <author> L. G. Valiant and V. V. Vazirani. </author> <title> NP is as easy as detecting unique solutions. </title> <journal> Theoretical Computer Science, </journal> <volume> 47 </volume> <pages> 85-93, </pages> <year> 1986. </year>
Reference: [Yan79] <author> M. Yannakakis. </author> <title> The effect of a connectivity requirement on the complexity of of maximum subgraph problems. </title> <journal> Journal of the ACM, </journal> <volume> 26 </volume> <pages> 618-630, </pages> <year> 1979. </year>
Reference: [Yan81] <author> M. Yannakakis. </author> <title> Edge deletion problems. </title> <journal> SIAM Journal of Computing, </journal> <volume> 10 </volume> <pages> 77-89, </pages> <year> 1981. </year>
Reference: [Yan92] <author> M. Yannakakis. </author> <title> On the approximation of maximum satisfiability. </title> <booktitle> In Proceedings of 3rd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 1-9, </pages> <year> 1992. </year>
Reference: [Yao90] <author> A.C.C. Yao. </author> <title> Coherent functions and program checkers. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 84-94, </pages> <year> 1990. </year>
Reference-contexts: This observation was the starting point for the theory of self-testing/self-correcting programs ([BK89, Lip89, BLR90]). Most functions to which this theory seemed to apply were algebraic in nature (see <ref> [Yao90] </ref> for some nonalgebraic examples, though). A new insight was made in [BF91, Sha92, LFKN92]: logical formulae can be represented as polynomials of low degree. But recall that different classes of logical formulae are complete for complexity classes like PSPACE, NEXPTIME, NP etc..
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proc. 32nd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: Historical Notes/Further Reading The lure of proving better inapproximability results for Clique has motivated many developments in the PCP area. The first hardness result for Clique was obtained in [FGL + 91]. NP-hardness (of constant-factor approximation) was proved in [AS92]. Further, as observed first in <ref> [Zuc91] </ref>, the constant factor hardness result can be improved to larger factors by using a pseudo-random graph-product. The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction.
Reference: [Zuc93] <author> D. Zuckerman. </author> <title> NP-complete problems have a version that's hard to approximate. </title> <booktitle> In 8th Structure in Complexity Theory Conf., </booktitle> <pages> pages 305-312, </pages> <year> 1993. </year> <note> 142 BIBLIOGRAPHY </note>
Reference-contexts: The class includes Clique and Independent Set ([ALM + 92], see Section 6.5), Chromatic Number ([LY94]), Max-Planar-Subgraph, the problem of computing the largest induced planar subgraph of a graph, ([LY93]), Max-Set-Packing, and constrained versions of the 21 problems in Karp's original paper (the last two results are in <ref> [Zuc93] </ref>). All these results are provable using MAX-3SAT (13). The lone problem that does not fit into the above classification is the Shortest Vector Problem using the ` 1 norm. The reduction to it outlined in Section 6.4.6 uses in an 106 CHAPTER 6. <p> The result of [AFWZ93] stated in Section 6.5 is the cleanest statement of such a construction. The NP-hardness of n * -approximation is due to [ALM + 92], although with a different reduction (namely, the one due to [FGL + 91], plus the idea of <ref> [Zuc93] </ref>). The connection between MAX-SNP and Clique (albeit with a randomized booster construction) was first discovered in [BS92]. A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. <p> The NP-hardness of n * -approximation is due to [ALM + 92], although with a different reduction (namely, the one due to [FGL + 91], plus the idea of <ref> [Zuc93] </ref>). The connection between MAX-SNP and Clique (albeit with a randomized booster construction) was first discovered in [BS92]. A result in [Zuc93] shows the hardness of approximating the k times iterated log of the clique number, for any constant k. Free bits. The constant * in the Clique result has seen many improvements [ALM + 92, BGLR93, FK94b, BS94]. The latest improvements center around the concept of free bits ([FK94b]).
References-found: 121

