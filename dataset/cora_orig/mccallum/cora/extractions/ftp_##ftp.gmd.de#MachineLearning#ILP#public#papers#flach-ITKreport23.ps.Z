URL: ftp://ftp.gmd.de/MachineLearning/ILP/public/papers/flach-ITKreport23.ps.Z
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/Online_bib.html
Root-URL: 
Title: of database relations  
Author: Peter A. Flach 
Note: Inductive characterisation  Full version of a paper appearing in Methodologies for Intelligent Systems, 5 Z.W. Ras, M. Zemankova and M.L. Emrich (eds.), North-Holland, Amsterdam, 1990, pp. 371-378. ISSN 0924-7807  
Abstract: Institute for Language Technology and Artificial Intelligence, Tilburg University, The Netherlands ITK Research Report No. 23 November 1990 
Abstract-found: 1
Intro-found: 1
Reference: [Angluin & Smith 1983] <author> D. ANGLUIN & C.H. SMITH, </author> <title> Inductive inference: theory and methods, </title> <journal> Computing Surveys 15:3, </journal> <pages> 238-269. </pages>
Reference-contexts: After each new tuple the current characterisation should be updated. Thus, we can take advantage of the large body of work on inductive learning (for a survey of this work, see <ref> [Angluin & Smith 1983] </ref>). In the spirit of the majority of this work, we will assume that the last tuple of r is not signalled; thus, each intermediate characterisation (or hypothesis) could turn out to be the final one.
Reference: [Armstrong 1974] <author> W.W. ARMSTRONG, </author> <title> Dependency structures of data base relationships, </title> <booktitle> in Proc. IFIP 74, </booktitle> <publisher> North Holland, Amsterdam, </publisher> <pages> 580-583. </pages>
Reference-contexts: While u is infinite, there are also finite relations q that satisfy only trivial fds, thus FD (q) =FD (u). This follows directly from the existence of Armstrong relations for any (consistent) set of fds <ref> [Armstrong 1974, Beeri et al. 1984] </ref>. 4 As an illustration, let r be the relation depicted in table 1. A B C a 2 b 2 c 1 a 4 b 3 c 2 Table 1. A relation satisfying some functional dependencies.
Reference: [Beeri et al. 1984] <author> C. BEERI, M. DOWD, R. FAGIN & R. STATMAN, </author> <title> On the structure of Armstrong relations for functional dependencies, </title> <journal> JACM 31:1, </journal> <pages> 30-46. </pages>
Reference-contexts: While u is infinite, there are also finite relations q that satisfy only trivial fds, thus FD (q) =FD (u). This follows directly from the existence of Armstrong relations for any (consistent) set of fds <ref> [Armstrong 1974, Beeri et al. 1984] </ref>. 4 As an illustration, let r be the relation depicted in table 1. A B C a 2 b 2 c 1 a 4 b 3 c 2 Table 1. A relation satisfying some functional dependencies.
Reference: [Beeri & Vardi 1981] <author> C. BEERI & M.Y. VARDI, </author> <title> The implication problem for data dependencies, </title> <booktitle> in Proc. 8th Int. Conf. on Automata, Languages, and Programming, Lecture Notes in Computer Science 115, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <pages> 73-85. </pages>
Reference-contexts: An mvd, however, says: if it is known that these two tuples are in the relation, then it is also known that two other tuples are in the relation. Put differently, mvds are tuple-generating dependencies rather than equality-testing dependencies <ref> [Beeri & Vardi 1981] </ref>. Consequently, the analogue of Theorem 3 does not hold for mvds. THEOREM 4 (non-monotonicity of mvds). MVD (r) does not monotonically decrease when r increases. Proof.
Reference: [Flach & Veelenturf 1989] <author> P.A. FLACH & L.P.J. VEELENTURF, </author> <title> Concept learning from examples: </title> <booktitle> theoretical foundations, ITK Research Report 2, institute for Language Technology & Artificial Intelligence, </booktitle> <address> Tilburg University, the Netherlands. </address>
Reference-contexts: For instance, formula (4.6) implies formula (4.1) (which will be called strong consistency) if H constitutes a complete theory with respect to the examples <ref> [Flach & Veelenturf 1989] </ref>, i.e. for every instance 10 I either I or I is a theorem in the theory induced by H.
Reference: [Flach 1989] <author> P.A. FLACH, </author> <title> Second-order inductive learning, </title> <booktitle> ITK Research Report 10, institute for Language Technology & Artificial Intelligence, </booktitle> <address> Tilburg University, </address> <note> the Netherlands; a preliminary version appeared in Proc. workshop on Analogical and Inductive Inference AII89, K.P. </note> <editor> Jantke (ed.), </editor> <booktitle> Lecture Notes in Computer Science 397, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Hence, an incomplete positive example results in a number of competing least general hypotheses. A specific learning model that is based on the notion of weak consistency, and which fits the learning problems studied in this paper, is the model of second-order inductive learning <ref> [Flach 1989] </ref>. In this model, examples do not determine submodels of the model to be inferred, but rather elements of it. <p> It should be noted, as a final witness for the novelty of our approach, that the notion of weak consistency, unlike its strong counterpart, does not imply that consistency with a set of examples can be confirmed by testing consistency with every example separately. This property of compositionality <ref> [Flach 1989] </ref> can be expressed as ("i: E i , H =/ ) implies -E i -, H =/ (4.7) If a learning problem is not compositional, every example must be remembered, because it can not be sufficiently summarised by a set of temporary hypotheses. Neither characterisation problem is compositional.
Reference: [Flach 1990] <author> P.A. FLACH, </author> <title> Inductive learning of weak theories, </title> <booktitle> ITK Research Report, institute for Language Technology & Artificial Intelligence, </booktitle> <address> Tilburg University, </address> <publisher> the Netherlands (forthcoming). </publisher>
Reference-contexts: Consequently, a learning model based on weak consistency encompasses every possible learning model. The study of the conditions under which stronger learning models can be derived from weaker ones is called a meta-theory of inductive learning in <ref> [Flach 1990] </ref>. Some preliminary results are discussed in the next section. 4 . 4 Towards a meta-theory of inductive learning Any notion of consistency stronger than weak consistency should be sanctioned by additional information about the nature of the inductive learning task. <p> Another class of learning problems which falsify the SRT is learning from incomplete examples <ref> [Flach 1990] </ref>, because incomplete examples, i.e. examples for which information is missing from the description, do not have unique minimal models (condition (i) above). Hence, an incomplete positive example results in a number of competing least general hypotheses.
Reference: [Gallaire et al. 1984] <editor> H. GALLAIRE, J. MINKER & J.-M. NICOLAS, </editor> <title> Logic and databases: a deductive approach, </title> <journal> Computing surveys. </journal>
Reference-contexts: Alternatively, the correspondence between first-order logic and databases allows us to introduce proof-theoretic concepts, as in the area of deductive databases (for an excellent survey of logic and databases, see <ref> [Gallaire et al. 1984] </ref>). In this paper, we will switch freely from the relational model to logic when appropriate.
Reference: [Genesereth & Nilsson 1987] <author> M.R. GENESERETH & N.J. NILSSON, </author> <booktitle> Logical foundations of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos. </address>
Reference-contexts: The consistency condition can be extended by including background knowledge that aids the hypothesis in explaining the observations (see for instance <ref> [Genesereth & Nilsson 1987] </ref>). The main idea is, that formula (4.1) requires the inductive hypothesis to be as general as each of the examples. This relation of generality imposes an ordering on the space of hypothesis, which can be fruitfully used to describe the set of consistent hypotheses.
Reference: [Gold 1967] <author> E.M. GOLD, </author> <title> Language identification in the limit, </title> <booktitle> Information and Control 10, </booktitle> <pages> 447-474. </pages>
Reference-contexts: That is, our learning criterion is identification in the limit: when given a sufficient set of examples, the learner should output the correct hypothesis after a finite number of steps and never change it afterwards <ref> [Gold 1967] </ref>. Additionally, we allow for the possibility that the learning process is halted before such a sufficient set of examples (i.e., the complete relation) has been supplied.
Reference: [Grant & Jacobs 1982] <author> J. GRANT & B.E. JACOBS, </author> <title> On the family of generalized dependency constraints, </title> <journal> JACM 29:4, </journal> <pages> 986-997. </pages>
Reference-contexts: We proceed by showing how the contradiction test can be implemented; then we give an algorithm for downward fd-characterisation, and we give a method for specialising refuted fds. 5 The satisfaction of an fd XfiA by a relation r can be expressed in Horn clause form as (see for instance <ref> [Grant & Jacobs 1982] </ref>): A=A' :- r XA (X 1 , , X n ,A), r XA (X 1 , , X n ,A'). (2.1) where r XA denotes the projection p XA (r) of r on the attributes in X followed by A.
Reference: [Laird 1986] <author> P.D.LAIRD, </author> <title> Inductive inference by refinement, </title> <booktitle> Proceedings AAAI-86, </booktitle> <pages> 472-476. </pages>
Reference-contexts: We have given algorithms for these problems, and we have adapted these for incremental characterisation by treating it as inductive learning. It must be noted, that our algorithms owe much to the approach of identification by refinement <ref> [Shapiro 1981, 1983; Laird 1986, 1988] </ref>. We consider these problems to be significant from a database perspective. On the other hand, our goal has been to stress that a theory of inductive learning based on strong consistency is not the only possibility.
Reference: [Laird 1988] <author> P.D.LAIRD, </author> <title> Learning from good and bad data, </title> <publisher> Kluwer, </publisher> <address> Boston. </address>
Reference: [Maier 1983] <author> D. MAIER, </author> <title> The theory of relational databases, </title> <publisher> Computer Science Press, </publisher> <address> Rockville. </address> <month> 26 </month>
Reference-contexts: We end the paper with some concluding remarks. 3 2 . Characterising a database relation 2 . 1 Preliminaries We base our discussions on the relational model of data, and our notational conventions are close to <ref> [Maier 1983] </ref>. A relation scheme R is a set of attributes -A 1 , , A n -. Each attribute A i has a domain D i , 1in, consisting of values. Domains are assumed to be countably infinite. <p> With mvds also we only need a cover for MVD (r), containing for instance no trivial mvds; also, if Xfi fiYMVD (r) then also Xfi fiZMVD (r), with Z=R-XY. Such a cover can be represented by a set of dependency bases <ref> [Maier 1983] </ref>. The dependency basis D E P (X ) of X R wrt M V D (r ) is a partition of R containing X , such that Xfi fi Y M V D (r) iff Y is the union of some sets in DEP (X). <p> In this case, p, n is complete for p (in the weaker sense discussed above), and the resulting set of pmvds can consistently be interpreted as a set of mvds. We illustrate this approach with an example session with our Prolog program. The example is taken from <ref> [Maier 1983, p. 123] </ref>. A tuple service (f, d, p) means that flight number f flies on day d and can use plane type p on that day. User input is in bold. 19 Relation: service (flight, day, plane). Dependencies: service:[]-&gt;->[plane] service:[]-&gt;->[flight] service:[]-&gt;->[day] New tuple: service (106, monday, 747). <p> We hope that the problems discussed in this paper support these claims. N o t e s 1 There exist complete proof systems for fds, see <ref> [Maier 1983] </ref>. 2 A similar, but somewhat less efficient algorithm was given in [Mannila & Rih 1986]. 3 Note, that r is allowed to contain either t 1 or t 2 without violating Xfi fiY, as in figure 3. 4 This observation suggests a link with the field of non-monotonic reasoning,
Reference: [Mannila & Rih 1986] <author> H. MANNILA & K.-J. RIH, </author> <title> Design by example: an application of Armstrong relations, </title> <journal> J. Comp. Syst. Sc. </journal> <volume> 33, </volume> <pages> 126-141. </pages>
Reference-contexts: We hope that the problems discussed in this paper support these claims. N o t e s 1 There exist complete proof systems for fds, see [Maier 1983]. 2 A similar, but somewhat less efficient algorithm was given in <ref> [Mannila & Rih 1986] </ref>. 3 Note, that r is allowed to contain either t 1 or t 2 without violating Xfi fiY, as in figure 3. 4 This observation suggests a link with the field of non-monotonic reasoning, and provides an additional justification of the term non-monotonic. 5 By a slight
Reference: [Mitchell 1982] <author> T.M. MITCHELL, </author> <title> Generalization as search, </title> <booktitle> Artificial Intelligence 18:2, </booktitle> <pages> 203-226. </pages>
Reference-contexts: Because H is required to be logically consistent, not every hypothesis more general than E i is consistent with the examples. Therefore, there may also exist most general consistent hypotheses, i.e., hypotheses satisfying formula (4.1), every possible extension of which is logically inconsistent. The well-known Version Space model <ref> [Mitchell 1982] </ref> shows, that most general consistent hypotheses exist for inductive concept learning.
Reference: [Plotkin 1970] <author> G.D. PLOTKIN, </author> <title> A note on inductive generalisation, </title> <booktitle> in Machine Intelligence 5, </booktitle> <editor> B. Meltzer & D. Michie (eds.), </editor> <publisher> Edinburgh University Press, Edinburgh, </publisher> <pages> 153-163. </pages>
Reference-contexts: This conclusion can be reached by looking for attributes, apart from C, for which both witnesses have different values, i.e., D. This set of attributes is called the disagreement of the two witnesses, and can be obtained by computing their anti-unification (the dual of unification) <ref> [Plotkin 1970, 1971; Reynolds 1970] </ref>. The anti-unification of r (a1,b1,c1,d1) and r (a1,b1,c2,d2) is r (a1,b1,C,D), suggesting D as an extension to the lefthand-side of the fd. In the next iteration, A D fi C may itself turn out to be contradicted, if r (a1,b2,c1,d2) happens to be in r.
Reference: [Plotkin 1971] <author> G.D. PLOTKIN, </author> <title> A further note on inductive generalisation, </title> <booktitle> in Machine Intelligence 6, </booktitle> <editor> B. Meltzer & D. Michie (eds.), </editor> <publisher> Edinburgh University Press, Edinburgh, </publisher> <pages> 101-124. </pages>
Reference: [Reynolds 1970] <author> J.C. REYNOLDS, </author> <title> Transformational systems and the algebraic structure of atomic formulas, </title> <booktitle> in Machine Intelligence 5, </booktitle> <editor> B. Meltzer & D. Michie (eds.), </editor> <publisher> Edinburgh University Press, Edinburgh, </publisher> <pages> 135-151. </pages>
Reference-contexts: This conclusion can be reached by looking for attributes, apart from C, for which both witnesses have different values, i.e., D. This set of attributes is called the disagreement of the two witnesses, and can be obtained by computing their anti-unification (the dual of unification) <ref> [Plotkin 1970, 1971; Reynolds 1970] </ref>. The anti-unification of r (a1,b1,c1,d1) and r (a1,b1,c2,d2) is r (a1,b1,C,D), suggesting D as an extension to the lefthand-side of the fd. In the next iteration, A D fi C may itself turn out to be contradicted, if r (a1,b2,c1,d2) happens to be in r.
Reference: [Sagiv et al. 1981] <author> Y. SAGIV, C. DELOBEL, D.S. PARKER, JR. & R. FAGIN, </author> <title> An equivalence between relational database dependencies and a fragment of propositional logic, </title> <journal> JACM 28:3, </journal> <pages> 435-453. </pages>
Reference-contexts: Which approach will be more efficient depends on the actual number of fds satisfied by the relation. There is an important difference, however, between testing for satisfaction and testing for contradiction: contradiction can always be reduced to two witnessing tuples <ref> [Sagiv et al. 1981] </ref>; this can easily be seen if fds are expressed in Horn form. More importantly, these two witnesses provide information about how to specialise the refuted fd, as will be detailed below. For these reasons, we restrict attention to the downward approach.
Reference: [Shapiro 1981] <author> E.Y. SHAPIRO, </author> <title> Inductive inference of theories from facts, </title> <type> Techn. rep. 192, </type> <institution> Comp. Sc. Dep., Yale University. </institution>
Reference-contexts: We have given algorithms for these problems, and we have adapted these for incremental characterisation by treating it as inductive learning. It must be noted, that our algorithms owe much to the approach of identification by refinement <ref> [Shapiro 1981, 1983; Laird 1986, 1988] </ref>. We consider these problems to be significant from a database perspective. On the other hand, our goal has been to stress that a theory of inductive learning based on strong consistency is not the only possibility.
Reference: [Shapiro 1983] <author> E.Y. SHAPIRO, </author> <title> Algorithmic program debugging, </title> <publisher> MIT Press. </publisher>
References-found: 22

