URL: http://www.cs.cmu.edu/~jab/pubs/boyan.lstdl.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/jab/web/cv/cv100.html
Root-URL: 
Email: jab@cs.cmu.edu  
Title: Least-Squares Temporal Difference Learning  
Author: Justin A. Boyan 
Keyword: Category: Reinforcement Learning and Control (TD-learning)  
Address: Pittsburgh, PA 15213  
Affiliation: CMU Computer Science Department  
Abstract: Submitted to NIPS-98 TD() is a popular family of algorithms for approximate policy evaluation in large MDPs. TD() works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes inefficient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto [5] eliminates all stepsize parameters and improves data efficiency. This paper extends Bradtke and Barto's work in three significant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. G. Atkeson and J. C. Santamaria. </author> <title> A comparison of direct and model-based reinforcement learning. </title> <booktitle> In International Conference on Robotics and Automation, </booktitle> <year> 1997. </year>
Reference-contexts: TD () was introduced in [10]; excellent summaries may now be found in several books [2, 13]. For each state on each observed trajectory, TD () incrementally adjusts the co efficients of ~ V toward new target values. The target values depend on the parameter 2 <ref> [0; 1] </ref>. <p> TD () for approximate policy evaluation: Given: * a simulation model for a proper policy in MDP X; * a featurizer : X ! &lt; K mapping states to feature vectors, (END) = 0; * a parameter 2 <ref> [0; 1] </ref>; and * a sequence of stepsizes ff 1 ; ff 2 ; : : : for incremental coefficient updating. Output: a coefficient vector fi for which V (x) fi (x). <p> The advantage of TD () is its low computational burden per step; the advantage of the classical model-based method is that it makes the most 4 of the available training data. The empirical advantages of model-based and model-free reinforcement learning methods have been investigated in, e.g., <ref> [11, 7, 1] </ref>. Where does LSTD () fit in? Let us first consider the case of = 0. In this case, perhaps surprisingly, it precisely duplicates the classical model-based method sketched above.
Reference: [2] <author> D. Bertsekas and J. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: 1 BACKGROUND This paper addresses the problem of approximating the value function V of a fixed policy in a large Markov decision process <ref> [2, 13] </ref>. This is an important subproblem of several algorithms for sequential decision making, including policy iteration [2] and STAGE [4]. V (x) simply predicts the expected long-term sum of future rewards obtained when the process starts in state x and follows policy until termination. <p> 1 BACKGROUND This paper addresses the problem of approximating the value function V of a fixed policy in a large Markov decision process [2, 13]. This is an important subproblem of several algorithms for sequential decision making, including policy iteration <ref> [2] </ref> and STAGE [4]. V (x) simply predicts the expected long-term sum of future rewards obtained when the process starts in state x and follows policy until termination. <p> This is where the TD () family of algorithms applies. TD () was introduced in [10]; excellent summaries may now be found in several books <ref> [2, 13] </ref>. For each state on each observed trajectory, TD () incrementally adjusts the co efficients of ~ V toward new target values. The target values depend on the parameter 2 [0; 1]. <p> The expectations are taken with respect to the distribution of trajectories through the Markov chain. It is shown in <ref> [2] </ref> that C is negative definite and 2 that the noise ! has sufficiently small variance, which together with the stepsize conditions mentioned above, imply that fi converges to a fixed point fi satisfying d + Cfi = 0. <p> also perform a matrix inversion at a cost of O (K 3 ) whenever fi's coefficients are neededin the case of STAGE, once per complete trajectory. (If updated coefficients are required more frequently, then the O (K 3 ) cost can be avoided by recursive least-squares [5] or Kalman-filtering techniques <ref> [2, x3.2.2] </ref>, which update fi on each timestep at a cost of only O (K 2 ).) LSTD () is more computationally expensive than incremental TD (), which updates the coefficients using only O (K) computation per timestep. <p> An exciting possibility for future work is to apply LSTD () in the context of approximation algorithms for general Markov decision problems. LSTD () could provide an efficient alternative to TD () in the inner loop of optimistic policy iteration <ref> [2] </ref>. Acknowledgments: Thanks to Andrew Moore and Jeff Schneider for helpful comments.
Reference: [3] <author> J. A. Boyan. </author> <title> Learning Evaluation Functions for Global Optimization. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1998. </year>
Reference-contexts: At the other extreme, when = 1, LSTD (1) produces the same A and b that would be 3 produced by supervised linear regression on training pairs of (state features 7! observed Monte-Carlo returns) (see <ref> [3] </ref> for proof). Thanks to the algebraic trick of the eligibility vectors, LSTD (1) builds the regression matrices fully incrementallywithout having to store the trajectory while waiting to observe the eventual outcome. When trajectories through the chain are long, this provides significant memory savings over linear regression. <p> On the other hand, some reinforcement-learning applications have been successful with very small numbers of features (e.g., [9, 4]), and in these situations LSTD () should be superior. LSTD () has been successfully applied in the context of STAGE, a reinforcement-learning algorithm for combinatorial optimization <ref> [3] </ref>. An exciting possibility for future work is to apply LSTD () in the context of approximation algorithms for general Markov decision problems. LSTD () could provide an efficient alternative to TD () in the inner loop of optimistic policy iteration [2].
Reference: [4] <author> J. A. Boyan and A. W. Moore. </author> <title> Learning evaluation functions for global optimization and Boolean satisfiability. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1998. </year>
Reference-contexts: 1 BACKGROUND This paper addresses the problem of approximating the value function V of a fixed policy in a large Markov decision process [2, 13]. This is an important subproblem of several algorithms for sequential decision making, including policy iteration [2] and STAGE <ref> [4] </ref>. V (x) simply predicts the expected long-term sum of future rewards obtained when the process starts in state x and follows policy until termination. <p> Note the log scale on the y-axis. LSTD () is best in all cases. [12]. On the other hand, some reinforcement-learning applications have been successful with very small numbers of features (e.g., <ref> [9, 4] </ref>), and in these situations LSTD () should be superior. LSTD () has been successfully applied in the context of STAGE, a reinforcement-learning algorithm for combinatorial optimization [3].
Reference: [5] <author> S. J. Bradtke and A. G. Barto. </author> <title> Linear least-squares algorithms for temporal difference learning. </title> <booktitle> Machine Learning, </booktitle> <address> 22(1/2/3):3357, </address> <year> 1996. </year>
Reference-contexts: Note that A has dimension K fi K, and b, fi, z, and (x) all have dimension K fi 1. When = 0, LSTD (0) reduces precisely to Bradtke and Barto's LSTD algorithm, which they derived using a different approach based on regression with instrumental variables <ref> [5] </ref>. At the other extreme, when = 1, LSTD (1) produces the same A and b that would be 3 produced by supervised linear regression on training pairs of (state features 7! observed Monte-Carlo returns) (see [3] for proof). <p> LSTD () must also perform a matrix inversion at a cost of O (K 3 ) whenever fi's coefficients are neededin the case of STAGE, once per complete trajectory. (If updated coefficients are required more frequently, then the O (K 3 ) cost can be avoided by recursive least-squares <ref> [5] </ref> or Kalman-filtering techniques [2, x3.2.2], which update fi on each timestep at a cost of only O (K 2 ).) LSTD () is more computationally expensive than incremental TD (), which updates the coefficients using only O (K) computation per timestep. <p> However, LSTD () offers several advantages, as pointed out by Bradtke and Barto in their discussion of LSTD (0) <ref> [5] </ref>: * Least-squares algorithms are more efficient estimators in the statistical sense because they extract more information from each additional observation. * TD ()'s convergence can be slowed dramatically by a poor choice of the stepsize parameters ff n .
Reference: [6] <author> L.-J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: This approach, while requiring little computation per iteration, wastes data and may require sampling many trajectories to reach convergence. One technique for using data more efficiently is experience replay <ref> [6] </ref>: explicitly remember all trajectories ever seen, and whenever asked to produce an updated set of coefficients, perform repeated passes of TD () over all the saved trajectories until convergence. This technique is similar to the batch training methods commonly used to train neural networks.
Reference: [7] <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <booktitle> Machine Learning, </booktitle> <address> 13:103130, </address> <year> 1993. </year>
Reference-contexts: The advantage of TD () is its low computational burden per step; the advantage of the classical model-based method is that it makes the most 4 of the available training data. The empirical advantages of model-based and model-free reinforcement learning methods have been investigated in, e.g., <ref> [11, 7, 1] </ref>. Where does LSTD () fit in? Let us first consider the case of = 0. In this case, perhaps surprisingly, it precisely duplicates the classical model-based method sketched above.
Reference: [8] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: Thus, fi can be estimated as A 1 b. I use Singular Value Decomposition to invert A robustly <ref> [8] </ref>. The complete LSTD () algorithm is specified in Table 2. LSTD () for approximate policy evaluation: Given: a simulation model, featurizer, and as in ordinary TD (); no stepsizes necessary. Output: a coefficient vector fi for which V (x) fi (x).
Reference: [9] <author> S. Singh and D. Bertsekas. </author> <title> Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, NIPS-9, </editor> <publisher> page 974. The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Note the log scale on the y-axis. LSTD () is best in all cases. [12]. On the other hand, some reinforcement-learning applications have been successful with very small numbers of features (e.g., <ref> [9, 4] </ref>), and in these situations LSTD () should be superior. LSTD () has been successfully applied in the context of STAGE, a reinforcement-learning algorithm for combinatorial optimization [3].
Reference: [10] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: In either case, we must compute V or an approximation thereof (denoted ~ V ) solely from a collection of trajectories sampled from the chain. This is where the TD () family of algorithms applies. TD () was introduced in <ref> [10] </ref>; excellent summaries may now be found in several books [2, 13]. For each state on each observed trajectory, TD () incrementally adjusts the co efficients of ~ V toward new target values. The target values depend on the parameter 2 [0; 1]. <p> This value has lower variancethe only random component is a single state transitionbut is biased by the potential inaccuracy of the lookahead estimate of V . The parameter trades off between bias and variance. Empirically, intermediate values of seem to perform best <ref> [10, 13] </ref>. TD () provably converges to a good approximation of V when linear architectures are used, assuming a suitable decreasing schedule of stepsizes for the incremental weight updates [14].
Reference: [11] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The advantage of TD () is its low computational burden per step; the advantage of the classical model-based method is that it makes the most 4 of the available training data. The empirical advantages of model-based and model-free reinforcement learning methods have been investigated in, e.g., <ref> [11, 7, 1] </ref>. Where does LSTD () fit in? Let us first consider the case of = 0. In this case, perhaps surprisingly, it precisely duplicates the classical model-based method sketched above.
Reference: [12] <author> R. S. Sutton. </author> <title> Gain adaptation beats least squares. </title> <booktitle> In Proceedings of the 7 th Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 161166, </pages> <year> 1992. </year>
Reference-contexts: The plotted segment shows the mean RMS value function approximation error after 100 trajectories (top of segment) and 10,000 trajectories (bottom of segment). Note the log scale on the y-axis. LSTD () is best in all cases. <ref> [12] </ref>. On the other hand, some reinforcement-learning applications have been successful with very small numbers of features (e.g., [9, 4]), and in these situations LSTD () should be superior. LSTD () has been successfully applied in the context of STAGE, a reinforcement-learning algorithm for combinatorial optimization [3].
Reference: [13] <author> R. S. Sutton and A. G. Barto. </author> <title> Reinforcement Learning: An Introduction. </title> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: 1 BACKGROUND This paper addresses the problem of approximating the value function V of a fixed policy in a large Markov decision process <ref> [2, 13] </ref>. This is an important subproblem of several algorithms for sequential decision making, including policy iteration [2] and STAGE [4]. V (x) simply predicts the expected long-term sum of future rewards obtained when the process starts in state x and follows policy until termination. <p> This is where the TD () family of algorithms applies. TD () was introduced in [10]; excellent summaries may now be found in several books <ref> [2, 13] </ref>. For each state on each observed trajectory, TD () incrementally adjusts the co efficients of ~ V toward new target values. The target values depend on the parameter 2 [0; 1]. <p> This value has lower variancethe only random component is a single state transitionbut is biased by the potential inaccuracy of the lookahead estimate of V . The parameter trades off between bias and variance. Empirically, intermediate values of seem to perform best <ref> [10, 13] </ref>. TD () provably converges to a good approximation of V when linear architectures are used, assuming a suitable decreasing schedule of stepsizes for the incremental weight updates [14].
Reference: [14] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> MIT, </institution> <year> 1996. </year> <month> 7 </month>
Reference-contexts: The parameter trades off between bias and variance. Empirically, intermediate values of seem to perform best [10, 13]. TD () provably converges to a good approximation of V when linear architectures are used, assuming a suitable decreasing schedule of stepsizes for the incremental weight updates <ref> [14] </ref>. Linear architectureswhich include lookup tables, state aggregation methods, CMACs, radial basis function networks with fixed bases, and multi-dimensional polynomial regressionapproximate V (x) by first mapping the state x to a feature vector (x) 2 &lt; K , and then computing a linear combination of those features, (x) T fi.
References-found: 14

