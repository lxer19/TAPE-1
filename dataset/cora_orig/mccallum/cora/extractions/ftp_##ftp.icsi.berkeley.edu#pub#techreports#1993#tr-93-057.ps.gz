URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-057.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: Second Order Backpropagation Efficient Computation of the Hessian Matrix for Neural Networks  
Author: Raul Rojas 
Address: Berlin, Takustr. 9, Berlin 14195  
Affiliation: Freie Universitat  
Date: 28 September 1993  
Pubnum: TR-93-057  
Abstract: Traditional learning methods for neural networks use some kind of gradient descent in order to determine the network's weights for a given task. Some second order learning algorithms deal with a quadratic approximation of the error function determined from the calculation of the Hessian matrix, and achieve improved convergence rates in many cases. We introduce in this paper second order backpropagation, a method to calculate efficiently the Hessian of a linear network of one-dimensional functions. This technique can be used to get explicit symbolic expressions or numerical approximations of the Hessian and could be used in parallel computers to improve second order learning algorithms for neural networks. It can be of interest also for computer algebra systems. 
Abstract-found: 1
Intro-found: 1
Reference: [Battiti 92] <author> Roberto Battiti, </author> <title> "First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method", </title> <journal> Neural Computation, </journal> <volume> Vol. 4, </volume> <year> 1992, </year> <pages> pp. 141-166. 10 </pages>
Reference-contexts: Several authors have proposed going further than the standard first-order methods by using some kind of second-order approximation to the error function and appropriate learning algorithms <ref> [Battiti 92] </ref>. Newton's method, in particular, uses this kind of approach. For these and some other reasons, methods for the determination of the Hessian matrix in the case of multilayered networks have been studied recently [Bishop 92].
Reference: [Becker, le Cun 89] <author> Sue Becker and Yann le Cun, </author> <title> "Improving the Convergence of Back-Propagation Learning with Second Order Methods", </title> <editor> in: D. Touretzky, G. Hinton and T. Sejnowski (eds.), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Since the backpropagation path to a weight intersects itself in its whole length, the computation of the second partial derivative of the associated network function of an output neuron with respect to a given weight can be organized as a recursive backward computation over this path. Pseudo-Newton methods <ref> [Becker, le Cun 89] </ref> used by some learning algorithms can profit from this computational locality.
Reference: [Bishop 92] <author> Chris Bishop, </author> <title> "Exact Calculation of the Hessian Matrix for the Multilayer Perceptron", </title> <journal> Neural Computation, </journal> <volume> Vol. 4, </volume> <year> 1992, </year> <pages> pp. 494-501. </pages>
Reference-contexts: Newton's method, in particular, uses this kind of approach. For these and some other reasons, methods for the determination of the Hessian matrix in the case of multilayered networks have been studied recently <ref> [Bishop 92] </ref>. We show in this paper how to compute efficiently the elements of the Hessian matrix using a graphical approach which reduces the whole problem to a computation by inspection. Our method is more general than Bishop's because arbitrary topologies can be handled.
Reference: [Rojas 93a] <editor> Raul Rojas, Theorie der neuronalen Netze, </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: At the points where alternative paths meet again the partial results are added up. Note that to implement this algorithm only the reversibility of the network is assumed, that is absence of cycles. It is very easy to show <ref> [Rojas 93a] </ref> that the two rules given above lead to the same expressions for the backpropagation algorithm in multilayer networks derived with so much pain in some books. It is only necessary to inspect the network and collect the necessary terms.
Reference: [Rojas 93b] <author> Raul Rojas, </author> <title> "A Graphical Proof of the Backpropagation Learning Algorithm", </title> <editor> in V. Malyshkin (ed.), </editor> <booktitle> Parallel Computing Technologies, PACT 93, </booktitle> <address> Obninsk, </address> <year> 1993. </year> <month> 11 </month>
Reference-contexts: It is only necessary to inspect the network and collect the necessary terms. The proof by induction that the method works with any kind of feed-forward network is easy to perform and can be found in <ref> [Rojas 93b] </ref>. We need a little bit of additional notation in order to describe first and second 3 order backpropagation. We will label the weights in a network consecutively, just like w 1 ; w 2 ; : : : ; w m .
References-found: 5

