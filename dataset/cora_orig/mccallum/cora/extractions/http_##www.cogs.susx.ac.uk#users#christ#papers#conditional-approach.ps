URL: http://www.cogs.susx.ac.uk/users/christ/papers/conditional-approach.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Title: Supervised Learning of Conditional Approach: A Case Study  
Author: Chris Thornton 
Date: November 25, 1993  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Reinforcement learning regimes have been shown to be capable of learning animat behaviours such as `obstacle avoidance' and `wall following'. Such behaviours can usually be learned more quickly using ordinary supervised methods, since in this case the learner receives more direct feedback. However, `conditional approach' behaviour (move in on small objects but stand clear of large ones) seems to be hard to learn even by neural network learning methods such as backpropagation. The paper presents the results of a study which investigated this behaviour and shows how the `hardness' of the behaviour can be accounted for in statistical terms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cliff, D., Husbands, P. and Harvey, I. </author> <year> (1993). </year> <title> Evolving visually guided robots. </title> <editor> In J. Meyer, H. Roitblat and S. Wilson (Eds.), </editor> <booktitle> From Animals to Animats: Proceedings of the Second International Conference on Simulation of Adaptive Behaviour (SAB92). </booktitle> <publisher> MIT/Bradford Books. </publisher>
Reference-contexts: Behaviors of this type are known to be learnable by a variety of methods <ref> [11; 1; 12] </ref>. But the fact that we were able to obtain successful training using our simulation-based methodology shows that the failure of the algorithms to deal satisfactorily with conditional-approach is not necessarily due to a flaw in the basic methodology.
Reference: [2] <author> Harvey, I., Husbands, P. and Cliff, D. </author> <year> (1993). </year> <title> Issues in evolutionary robotics. </title> <editor> In J. Meyer, H. Roitblat and S. Wilson (Eds.), </editor> <booktitle> From Animals to Animats: Proceedings of the Second International Conference on Simulation of Adaptive Behaviour (SAB92). </booktitle> <publisher> MIT/Bradford Books. </publisher>
Reference: [3] <author> Gibson, J. </author> <year> (1986). </year> <booktitle> Artificial intelligence programming environments and the poplog system. </booktitle> <editor> In M. Yazdani (Ed.), </editor> <booktitle> Artificial Intelligence: Principles and Applications (pp. </booktitle> <pages> 35-47). </pages> <address> London: </address> <publisher> Chapman Hall. </publisher>
Reference-contexts: The animat, situated in the lower part of the space, is represented as a small box 1 A colour video showing the simulations performed and results obtained is available from the author. 2 The simulations were run under the Poplog environment <ref> [3] </ref> running on a Sun SPARC station 1+. 2 with an arrow pointing in its direction of motion. The seven dashed lines are its probe rays. The boundaries of the space | here shown as unbroken lines | are actually transparent to the animat.
Reference: [4] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: In practice we tested the performance of a wide range of algorithms including ID3 <ref> [4] </ref> and C4.5 [5], feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation [6], a second-order method based on conjugate-gradient descent [7] and a second-order method based on Newton's method called `quickprop' [8]. <p> And yet the type-1 frequencies show some marked, non-chance values (see the frequencies for the cases &lt;x2=1&gt; and &lt;x2=2&gt;). These would be straightforwardly exploited by an algorithm such as Perceptron [14] or ID3 <ref> [4] </ref>.
Reference: [5] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In practice we tested the performance of a wide range of algorithms including ID3 [4] and C4.5 <ref> [5] </ref>, feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation [6], a second-order method based on conjugate-gradient descent [7] and a second-order method based on Newton's method called `quickprop' [8]. <p> The figures in the `quickprop' row show performance after training with the `quickprop' version [8] of the backpropagation algorithm [6]; the figures in the row labeled `c4' show the performance after training with the C4.5 version of ID3 <ref> [5] </ref>; the figures in the row labeled `NN' show performance after training with the nearest-neighbours algorithm [10]; finally, the figures in the row labeled CS show performance after training with the simple classifier system/genetic algorithm. All the figures are averaged over 10 different runs.
Reference: [6] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). 31 </pages>
Reference-contexts: In practice we tested the performance of a wide range of algorithms including ID3 [4] and C4.5 [5], feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation <ref> [6] </ref>, a second-order method based on conjugate-gradient descent [7] and a second-order method based on Newton's method called `quickprop' [8]. We also tested a constructive network learning method called `cascade-correlation' [8] and a classifier/genetic-algorithm combination based on Goldberg's `simple classifier system' [9]. The standard ID3 algorithm has no user-definable parameters. <p> The figures in the `random' row show the performance obtained using a random move generator. The figures in the `quickprop' row show performance after training with the `quickprop' version [8] of the backpropagation algorithm <ref> [6] </ref>; the figures in the row labeled `c4' show the performance after training with the C4.5 version of ID3 [5]; the figures in the row labeled `NN' show performance after training with the nearest-neighbours algorithm [10]; finally, the figures in the row labeled CS show performance after training with the simple <p> These would be straightforwardly exploited by an algorithm such as Perceptron [14] or ID3 [4]. Even where intrinsically type-2 problems show very little spurious type-1 regularity they may still be solved by sophisticated learning algorithms such as backpropagation <ref> [6] </ref>, cascade-correlation [8] or copycat [15]. 8 It is, of course, well known that backpropagation can solve problems based on parity, symmetry or `shift' relationships and all these typically involve the algorithm deriving what can be thought of as an internal recoding scheme.
Reference: [7] <author> Becker, S. and Cun, Y. </author> <year> (1988). </year> <title> Improving the convergence of back-propagation learning with second-order methods. </title> <institution> CRG-TR-88-5, University of Toronto Connectionist Research Group. </institution>
Reference-contexts: In practice we tested the performance of a wide range of algorithms including ID3 [4] and C4.5 [5], feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation [6], a second-order method based on conjugate-gradient descent <ref> [7] </ref> and a second-order method based on Newton's method called `quickprop' [8]. We also tested a constructive network learning method called `cascade-correlation' [8] and a classifier/genetic-algorithm combination based on Goldberg's `simple classifier system' [9]. The standard ID3 algorithm has no user-definable parameters.
Reference: [8] <author> Fahlman, S. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-Correlation Learning Architecture. </title> <institution> CMU-CS-90-100, School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: In practice we tested the performance of a wide range of algorithms including ID3 [4] and C4.5 [5], feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation [6], a second-order method based on conjugate-gradient descent [7] and a second-order method based on Newton's method called `quickprop' <ref> [8] </ref>. We also tested a constructive network learning method called `cascade-correlation' [8] and a classifier/genetic-algorithm combination based on Goldberg's `simple classifier system' [9]. The standard ID3 algorithm has no user-definable parameters. Thus there is only one way to apply it to a particular training problem. <p> range of algorithms including ID3 [4] and C4.5 [5], feed-forward network learning algorithms of the backpropagation family including `vanilla' backpropagation [6], a second-order method based on conjugate-gradient descent [7] and a second-order method based on Newton's method called `quickprop' <ref> [8] </ref>. We also tested a constructive network learning method called `cascade-correlation' [8] and a classifier/genetic-algorithm combination based on Goldberg's `simple classifier system' [9]. The standard ID3 algorithm has no user-definable parameters. Thus there is only one way to apply it to a particular training problem. <p> The figures in the `random' row show the performance obtained using a random move generator. The figures in the `quickprop' row show performance after training with the `quickprop' version <ref> [8] </ref> of the backpropagation algorithm [6]; the figures in the row labeled `c4' show the performance after training with the C4.5 version of ID3 [5]; the figures in the row labeled `NN' show performance after training with the nearest-neighbours algorithm [10]; finally, the figures in the row labeled CS show performance <p> These would be straightforwardly exploited by an algorithm such as Perceptron [14] or ID3 [4]. Even where intrinsically type-2 problems show very little spurious type-1 regularity they may still be solved by sophisticated learning algorithms such as backpropagation [6], cascade-correlation <ref> [8] </ref> or copycat [15]. 8 It is, of course, well known that backpropagation can solve problems based on parity, symmetry or `shift' relationships and all these typically involve the algorithm deriving what can be thought of as an internal recoding scheme.
Reference: [9] <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We also tested a constructive network learning method called `cascade-correlation' [8] and a classifier/genetic-algorithm combination based on Goldberg's `simple classifier system' <ref> [9] </ref>. The standard ID3 algorithm has no user-definable parameters. Thus there is only one way to apply it to a particular training problem. <p> That is to say, the actions for half the classifiers in any population were output patterns and the actions for the other half were input patterns. The standard bucket-brigade algorithm was used with standard defaults (e.g., as used in <ref> [9] </ref>). The codons in the input and output messages of the classifiers were single bytes encoding a real number in the range 0-1, with two decimal places of accuracy. The genetic algorithm used employed the crossover operator with random bit-mutation applying with a probability of 0.1.
Reference: [10] <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: `quickprop' row show performance after training with the `quickprop' version [8] of the backpropagation algorithm [6]; the figures in the row labeled `c4' show the performance after training with the C4.5 version of ID3 [5]; the figures in the row labeled `NN' show performance after training with the nearest-neighbours algorithm <ref> [10] </ref>; finally, the figures in the row labeled CS show performance after training with the simple classifier system/genetic algorithm. All the figures are averaged over 10 different runs. <p> before, the column headed `Freq' shows the absolute frequency for the main case.) By the argument used previously, the 2nd-order conditional frequencies here are of no interest since there is necessarily exactly one occurrence of each 2nd-order case of the constrained variables. 4 These can be construed as Bayesian probabilities <ref> [10] </ref>. 14 Case Freq. 1 x2=1 0.5 x3=0 0.5 x1=2 0.33 x2=2 + x3=1 0.33 x1=3 + x2=2 0.17 x1=1 + x2=2 0.17 x1=3 + x3=1 0.17 x1=2 + x3=1 0.17 x1=3 + x3=0 0.17 Table 4: 3.2 Type-1 versus type-2 frequencies A clear distinction must be made between cases (such
Reference: [11] <author> Nehmzow, U., Smithers, T. and Hallam, J. </author> <year> (1989). </year> <title> Really useful robots. </title> <editor> In T. Kanade, F. Green and L. Hertzberger (Eds.), </editor> <booktitle> Proceedings of IAS2, Intelligent Autonomous Systems (pp. </booktitle> <pages> 284-292). </pages> <address> Amsterdam. </address>
Reference-contexts: Behaviors of this type are known to be learnable by a variety of methods <ref> [11; 1; 12] </ref>. But the fact that we were able to obtain successful training using our simulation-based methodology shows that the failure of the algorithms to deal satisfactorily with conditional-approach is not necessarily due to a flaw in the basic methodology.
Reference: [12] <author> Millan, J. </author> <title> (forthcoming). On autonomous mobile robots and reinforcement connectionist learning. Neural Networks and a New AI. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Behaviors of this type are known to be learnable by a variety of methods <ref> [11; 1; 12] </ref>. But the fact that we were able to obtain successful training using our simulation-based methodology shows that the failure of the algorithms to deal satisfactorily with conditional-approach is not necessarily due to a flaw in the basic methodology.
Reference: [13] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II (pp. </booktitle> <pages> 318-362). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: regularities only, * Pure type-2 learning problems: problems that involve exploiting type 2 regularities only, and * Hybrid problems: problems that involve exploiting some mixture of both types. 3.4 Parity problems are pure type-2 The distinction between type-1 and type-2 problems is nicely illustrated by the so-called `parity' problems (cf. <ref> [13] </ref>). Complete parity mappings show no type-1 regularity at all. Their empirical frequencies are always exactly at their chance levels.
Reference: [14] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: And yet the type-1 frequencies show some marked, non-chance values (see the frequencies for the cases &lt;x2=1&gt; and &lt;x2=2&gt;). These would be straightforwardly exploited by an algorithm such as Perceptron <ref> [14] </ref> or ID3 [4].
Reference: [15] <author> Hofstadter, D. </author> <year> (1984). </year> <title> The copycat project. A.I. </title> <type> Memo 755, </type> <institution> Masachusetts Institute of Technology. </institution>
Reference-contexts: These would be straightforwardly exploited by an algorithm such as Perceptron [14] or ID3 [4]. Even where intrinsically type-2 problems show very little spurious type-1 regularity they may still be solved by sophisticated learning algorithms such as backpropagation [6], cascade-correlation [8] or copycat <ref> [15] </ref>. 8 It is, of course, well known that backpropagation can solve problems based on parity, symmetry or `shift' relationships and all these typically involve the algorithm deriving what can be thought of as an internal recoding scheme. However, we should not over-estimate the generality of such methods.
Reference: [16] <author> Held, G. </author> <year> (1987). </year> <title> Data Compression: Techniques and Applications, Hardware and Software Considerations (2nd edition). </title> <address> Chichester: </address> <publisher> Wiley. </publisher> <pages> 32 </pages>
Reference-contexts: Thus the relative size of the smallest subset that completely captures the training set measures the overall level of type-1 regularity. A natural way to summarize this measure of type-1 regularity is as a compression ratio <ref> [16] </ref>. The compression ratio we define as the ratio between the number of variable instantiations used in specifying the frequency effects and the number used in the original training set.
References-found: 16

