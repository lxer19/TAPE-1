URL: http://www.cs.cornell.edu/Info/People/chandra/podc97/papers/gadi.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/chandra/podc97/newProgram.html
Root-URL: 
Title: Disentangling Multi-object Operations (Extended Abstract)  
Author: Yehuda Afek Michael Merritt Gadi Taubenfeld Dan Touitou 
Abstract: We consider the problem of implementing atomic operations on multiple shared memory objects, in systems which directly support only single-object atomic operations. Our motivation is to design algorithms that exhibit both low contention between concurrent operations and a high level of concurrency, by disentangling long chains of conflicting operations. That is, operations that access widely disjoint parts of a data structure, or are widely separated in time, should not interfere with each other. The algorithm reported here extends and is based on the work of Attiya and Dagan [AD96], where a non-blocking solution is presented for two-object atomic operations. For any number, k, we present a wait-free solution for atomically accessing up to k objects. Notions of local contention and local step complexity are defined, and it is shown that the solution has low local contention and local step complexity. Relations between multi-objects and the familiar resource allocation problem are explored-the algorithm presented also provides a solution to the resource allocation problem. 
Abstract-found: 1
Intro-found: 1
Reference: [AD96] <author> H. Attiya and E. Dagan. </author> <title> Universal operations: Unary versus binary. </title> <booktitle> In Proc. 15th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 223-232, </pages> <year> 1996. </year>
Reference-contexts: Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. Recent work by Attiya and Dagan achieves this goal for operations on pairs of locations, obtaining a non-blocking algorithm for 2-location atomic updates <ref> [AD96] </ref>. Using algorithmic ideas from the work of Attiya and Dagan, we describe a wait-free solution for any fixed number of lo cations, k. <p> We first develop a non-blocking algorithm and then combine it with a simple mechanism which turns it into a wait-free algorithm. Following Attiya and Dagan <ref> [AD96] </ref>, each multi-object operation in our non-blocking algorithm has four phases: filter, decide, lock, and execute. The full conflict graph on the active operations has no a priori structure, making it difficult to coordinate using a local algorithm. <p> Moreover, an operation has (n) step complexity even if it accesses the data structure alone. A final predecessor to our work, the non-blocking algorithm of Attiya and Dagan implements 2-location atomic updates with helping and waiting chains of length only O (log fl n) <ref> [AD96] </ref>. They also introduce the notion of the sensitivity of one operation to another, and show that in their algorithm, operations are only sensitive to others within an O (log fl n) distance in the conflict graph. <p> Attiya and Dagan defined the notion of sensitivity, which measures the minimum distance between two operations that guarantees that they will not effect each other's step complexity <ref> [AD96] </ref>. Although sensitivity of distance d implies d-local step complexity, the Attiya-Dagan notion of sensitivity is neither robust (independent of the semantics of the shared objects) nor does their algorithm exhibit d-local contention for any d. <p> We begin with a non-blocking version of the algorithm, and then describe how to modify it to produce a wait-free version. 3.1 Outline of the the non-blocking k object algorithm The non-blocking algorithm has four major phases, as in <ref> [AD96] </ref>: filter, decide, lock, and execute. The operations that have passed the filter induce a subgraph of the conflict graph that is a forest of (k-1)-ary trees. <p> Filter: The purpose of the filter phase is to select from all the operations that are simultaneously accessing the shared memory, a subset whose conflict graph is a forest of rooted (k1)-ary trees. It is similar to the filter phase in the Attiya-Dagan algorithm, where k = 2 <ref> [AD96] </ref>. As illustrated in Figure 1, in addition to a lock and a value field, each memory location contains two other fields: a parent field and a child field.
Reference: [ADT95] <author> Y. Afek, D. Dauber, and D. Touitou. </author> <title> Wait-free made fast. </title> <booktitle> In Proc. 27th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 538-547, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Nevertheless, we discuss in Section 3.3 how to replace the large RMW object associated with each location by an implementation that uses LL/SC on single words (given in <ref> [ADT95] </ref>) and does not change the local contention and local step-complexity properties of our algorithm. 1.2 The algorithm in a nutshell In this paper we present a wait-free multi-object algorithm on k objects, that has d-local contention, and d-local step complexity, for d = O (log fl n). <p> In addition, the algorithms neither avoid copying nor have disjoint access. Afek, Dauber, and Touitou present a wait-free, universal translation method that avoids copying the entire data structure and in which the step complexity depends only on the (global) contention, but that does not allow disjoint access <ref> [ADT95] </ref>. In a later paper, Anderson and Moir present a wait-free methodology that avoids copying the data structure, but has (n) step complexity for even a single operation [AM95b]. <p> Our generalization solves an important question [Bar93, IR94], demonstrating the possibility of a truly fast multi-object algorithm, in which complexity of operations is bounded only by the local contention, rather than by the global contention as in <ref> [ADT95] </ref>. We introduce the notions of d-local contention and d-local step complexity, measure our algorithm according to these metrics, and advocate them as appropriate for investigating the local properties of concurrent algorithms. <p> Either O (log fl n) for each memory location, or larger for the wait-free mechanism. The individual update method of Afek, Dauber and Touitou replaces each RMW object by several LL/SC objects, and can be used to transform this implementation into one that uses LL/SC operations on small words <ref> [ADT95] </ref>. (Words which hold O (logn) bits or a constant number of oid's.) The individual update method is one of several methodologies Afek, et al. describe to transform a sequential data structure implementation (in particular a large RMW) into a fast wait-free one using LL/SC.
Reference: [AM94] <author> J. Anderson and M. Moir. </author> <title> Using k-exclusion to implement resilient, scalable shared objects. </title> <booktitle> Proc. of the 13th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 141-150, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Anderson and Moir explore a slightly different problem, proposing to use implementations that assume the concurrent execution of only k &lt; n processes, and to protect the implementations using a k-exclusion and a k-renaming algorithm <ref> [AM94] </ref>. In this way a process that accesses the data structure alone has a O (k) complexity instead of O (n). This fault model is not as general as wait-freedom. In addition, the algorithms neither avoid copying nor have disjoint access.
Reference: [AM95a] <author> J. Anderson and M. Moir. </author> <title> Universal constructions for multi-object operations. </title> <booktitle> In Proc. 14th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 184-193, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: That is, operations that access disjoint parts of the data structure, or are widely separated in time, should not interfere with each other. For example, operations on disjoint sets of components can proceed independently, and avoid concurrency-control overhead <ref> [Bar93, IR94, TSP92, ST95, AM95a] </ref>. When there is considerable contention, conflicting operations could form long chains and complex webs, transitively effecting operations that are otherwise disjoint. Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. <p> Most of the non-blocking multi-object implementations in the literature (e.g. <ref> [IR93, AM95a, ST95] </ref>) are based on the same universal single-location operation, LoadLink/StoreConditional (LL/SC). Instead of LL/SC, our algorithm is based on the existence of a general, atomic, single-location RMW operation, where each location may hold many fields. <p> In addition, both algorithms guarantee that no operation will help another at distance greater than O (log fl n). We apply a familiar technique for converting the non-blocking algorithm to a wait-free version <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its non-blocking operation, must help other pending operations, before it starts a new operation of its own. <p> They also present a wait-free multiword atomic implementation that allows disjoint access, but that gives rise to helping chains of length n <ref> [AM95a] </ref>. Moreover, an operation has (n) step complexity even if it accesses the data structure alone. A final predecessor to our work, the non-blocking algorithm of Attiya and Dagan implements 2-location atomic updates with helping and waiting chains of length only O (log fl n) [AD96]. <p> Unlock & release: After finishing the execute phase, each operation unlocks the k locations and unmarks the child and parent fields. Wait-free solution: The non-blocking algorithm is made wait-free by applying a modification of a standard technique <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its opera tion, must help (some) pending operations be fore it is allowed to start a new operation.
Reference: [AM95b] <author> J. Anderson and M. Moir. </author> <title> Universal construction of large objects. </title> <booktitle> Proceedings of the 9th International Workshop on Distributed Algorithms. </booktitle> <month> September 95. </month>
Reference-contexts: In a later paper, Anderson and Moir present a wait-free methodology that avoids copying the data structure, but has (n) step complexity for even a single operation <ref> [AM95b] </ref>. They also present a wait-free multiword atomic implementation that allows disjoint access, but that gives rise to helping chains of length n [AM95a]. Moreover, an operation has (n) step complexity even if it accesses the data structure alone.
Reference: [AMT96] <author> Y. Afek and M. Merritt and G. Taubenfeld. </author> <title> The power of multi-objects. </title> <booktitle> In Proc. 15th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 213-222, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The synchronization power of other multi-objects is studied in <ref> [AMT96] </ref>. Recent attention has focused on the multi-object problem out of a desire to find truly fast implementations of shared data structures that are also non-blocking or wait-free.
Reference: [AS90] <author> B. Awerbuch and M. Saks. </author> <title> A dining philosophers algorithm with polynomial response time. </title> <booktitle> In Proc. 31th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 65-74, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: These papers assume a fault-free model [CM84]. A few recent papers on the resource allocation problem in asynchronous message passing systems have considered the notion of failure locality <ref> [SP88, AS90, CS95, CS96] </ref>. Failure locality of an algorithm is defined as the smallest m such that any process, for which there are no failures within a distance m in the conflict graph, is free from starvation. <p> It is also possible that highly concurrent resource allocation algorithms, such as that due to Awerbuch and Saks, may be the basis for efficient non-blocking or wait-free multi-object algorithms <ref> [AS90] </ref>. 2 Complexity measures In this section we formally define complexity measures that capture how much operations interfere with, or effect, each other in a given algorithm. <p> It would be interesting to determine whether existing highly concurrent resource allocation algorithms (such as that due to Awerbuch and Saks <ref> [AS90] </ref>) can be modified to also produce efficient multi-object algorithms. Finally, the complexity measures we study are local contention and local step complexity.
Reference: [AT93] <author> R. Alur and G. Taubenfeld. </author> <title> How to share an object: A fast timing-based solution. </title> <booktitle> In Proceedings of the 5th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 470-477, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In addition, while using the wait-free method, each process must take O (n) steps even when accessing the data structure alone. The problem was also considered in a timing based model <ref> [AT93] </ref>. A search for efficient implementations proceeded. Barnes presented algorithms with the idea of "simulating" the update without effecting the shared memory, and then to apply the changes atomically using a non-blocking implementation of a multi-word RMW [Bar93]. This idea was developed further by Israeli and Rappoport [IR94].
Reference: [Bar93] <author> G. Barnes. </author> <title> A Method for implementing lock-free shared data structures. </title> <booktitle> In Proceedings of the 5th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: That is, operations that access disjoint parts of the data structure, or are widely separated in time, should not interfere with each other. For example, operations on disjoint sets of components can proceed independently, and avoid concurrency-control overhead <ref> [Bar93, IR94, TSP92, ST95, AM95a] </ref>. When there is considerable contention, conflicting operations could form long chains and complex webs, transitively effecting operations that are otherwise disjoint. Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. <p> The problem was also considered in a timing based model [AT93]. A search for efficient implementations proceeded. Barnes presented algorithms with the idea of "simulating" the update without effecting the shared memory, and then to apply the changes atomically using a non-blocking implementation of a multi-word RMW <ref> [Bar93] </ref>. This idea was developed further by Israeli and Rappoport [IR94]. Turek, Shasha, and Prakash explored similar constructions using an atomic compare&swap primitive [TSP92]. These non-blocking algorithms avoid copying the entire data structure, and allow disjoint access: operations in disjoint portions of the conflict graph can proceed independently. <p> The result is a non-blocking algorithm implementing atomic k-multi-object atomic operations, for any fixed k. We then show how to modify the non-blocking algorithm and obtain a wait-free version. Our generalization solves an important question <ref> [Bar93, IR94] </ref>, demonstrating the possibility of a truly fast multi-object algorithm, in which complexity of operations is bounded only by the local contention, rather than by the global contention as in [ADT95]. <p> Accesses to the individual locations in the shared memory alternate with updates to the operation record's local variables and program counter. Careful record keeping in both the shared locations and the operation record ensure that no step of the operation is executed more than once on the shared memory <ref> [Bar93, IR94] </ref>. 3.2 Results In this subsection we state the main properties of the algorithm. Proofs are omitted from this extended abstract. Within the filter, a process helps only operations that are either 1- or 2-neighbors of its operation.
Reference: [CM84] <author> K. M. Chandy and J. Misra. </author> <title> The drinking philosophers problem. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 6, </volume> <year> 1984. </year>
Reference-contexts: Chandy and Misra further generalized this by introducing the Drinking Philosophers problem, in which a philosopher needs some non-empty subset of its resources; this subset of resources may change over time. These papers assume a fault-free model <ref> [CM84] </ref>. A few recent papers on the resource allocation problem in asynchronous message passing systems have considered the notion of failure locality [SP88, AS90, CS95, CS96].
Reference: [CS95] <author> M. Choy and A. K. Singh. </author> <title> Efficient fault tolerant algorithms for resource allocation in distributed systems. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 17(3) </volume> <pages> 535-559, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: These papers assume a fault-free model [CM84]. A few recent papers on the resource allocation problem in asynchronous message passing systems have considered the notion of failure locality <ref> [SP88, AS90, CS95, CS96] </ref>. Failure locality of an algorithm is defined as the smallest m such that any process, for which there are no failures within a distance m in the conflict graph, is free from starvation.
Reference: [CS96] <author> M. Choy and A. K. Singh. </author> <title> Localizing failures in distributed synchronization. </title> <journal> IEEE Transactions on parallel and distributed systems, </journal> <volume> 7(7) </volume> <pages> 705-716, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: If helping processes in the critical section is not an option, fault-tolerant multi-object algorithms such as ours can be modified to simply wait for other processes instead of helping them|resulting in resource-allocation algorithms for the fault-free model with small failure locality when failures do occur <ref> [CS96] </ref>. Our motivation then is to design algorithms for shared data structures and shared resources, supporting multi-object operations, that have low contention and a high level of concurrency. <p> These papers assume a fault-free model [CM84]. A few recent papers on the resource allocation problem in asynchronous message passing systems have considered the notion of failure locality <ref> [SP88, AS90, CS95, CS96] </ref>. Failure locality of an algorithm is defined as the smallest m such that any process, for which there are no failures within a distance m in the conflict graph, is free from starvation.
Reference: [CV86] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Control, </journal> <volume> 70(1) 32-53, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Once through the filter, in the decide phase we apply the 3-coloring algorithm of Goldberg, Plotkin, and Shannon [GPS87] to color the operations in each directed tree. The 3-coloring algorithm is a generalization of the Cole-Vishkin algorithm <ref> [CV86] </ref> which was used by Attiya and Dagan for the same purpose over a path (1-ary tree).
Reference: [Dij72] <author> E. W. Dijkstra. </author> <title> Hierarchical ordering of sequential processes. </title> <booktitle> In Operating Systems Techniques, </booktitle> <year> 1972. </year> <editor> Eds: C. A. R. Hoare and R. H. Perrott, Eds. </editor> <publisher> Academic Press. </publisher>
Reference-contexts: Various abstractions have been suggested to model this problem, which is essentially the same as the multi-object problem. In his pioneering work, Dijkstra introduced the dining Philosophers problem in which the potential conflict graph is very simple|a cycle <ref> [Dij72] </ref>. Since then, dozens of solutions to this problem have been proposed. Lynch showed that the dining philosophers problem can be extended to an arbitrary graph network, in which a philosopher needs to acquire the resources on all incident edges [Lyn81].
Reference: [DHW93] <author> C. Dwork, M. Herlihy, and O. Waarts. </author> <title> Contention in shared memory algorithms. </title> <booktitle> In Proc. 25th Ann. ACM Symp. on Theory of Computing, </booktitle> <address> New York, </address> <pages> pages 174-183, </pages> <year> 1993. </year>
Reference-contexts: operations performed by O must be exactly the same, whether the chain exists or not.) 2.2 Local contention Following Dwork, Herlihy and Waarts and the fact that hardware is not magic, we assume that the cost of accessing a shared location is effected by the contention concurrent with the access <ref> [DHW93] </ref>. For example, in a contention sensitive implementation, the time taken by a single low-level operation could be a linear or logarithmic function of the number of simultaneously contending low-level operations.
Reference: [GC96] <author> M. Greenwald and D. R. Cheriton. </author> <title> The synergy between non-blocking synchronization and operating system structure. </title> <booktitle> In Proc. 2nd ACM Symp. on Operating System Design and Implementation. USENIX, Seattle, </booktitle> <pages> pages 123-136, </pages> <month> October, </month> <year> 1996. </year>
Reference: [GPS87] <author> A. V. Goldberg, S. A. Plotkin, and G. E. </author> <title> Shannon Parallel symmetry-breaking in sparse graphs. </title> <booktitle> Proc. 19th Ann. ACM Symp. on Theory of Computing, </booktitle> <address> New York, </address> <pages> pages 315-324, </pages> <year> 1987. </year>
Reference-contexts: In Section 3, Figure 1 illustrates the recursive data structures required by the filter phases, and Figure 2 provides a detailed example. Once through the filter, in the decide phase we apply the 3-coloring algorithm of Goldberg, Plotkin, and Shannon <ref> [GPS87] </ref> to color the operations in each directed tree. The 3-coloring algorithm is a generalization of the Cole-Vishkin algorithm [CV86] which was used by Attiya and Dagan for the same purpose over a path (1-ary tree). <p> To make this decision without creating long waiting chains along the (k 1)-ary tree, we color the operations in the tree with 3 colors using the Goldberg Plotkin and Shannon algorithm <ref> [GPS87] </ref>. Each operation locks its high (parent side) location first if its immediate ancestor in the tree has a larger color, otherwise it locks the low locations first.
Reference: [Her91] <author> M. Herlihy. </author> <title> Wait-free synchronization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: This abstract description ignores many details of the non-blocking algorithm. An important omission is the standard technique of a blocked process helping the process that blocks it, which is how algorithms are made non-blocking <ref> [Her91, Her93] </ref>. If at any phase process p is blocked by another process q, then p helps q to proceed until q does not block p anymore, and then p continues. In addition, both algorithms guarantee that no operation will help another at distance greater than O (log fl n). <p> In addition, both algorithms guarantee that no operation will help another at distance greater than O (log fl n). We apply a familiar technique for converting the non-blocking algorithm to a wait-free version <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its non-blocking operation, must help other pending operations, before it starts a new operation of its own. <p> complexity, we apply this technique separately to each location, and each terminating operation helps only pending operations with which it directly conflicts. 1.3 Related work Early work in multi-objects focused on the synchronization power of m-registers, multi-objects in which the operations are restricted to reads and writes of m locations <ref> [Her91, MT94] </ref>. The synchronization power of other multi-objects is studied in [AMT96]. Recent attention has focused on the multi-object problem out of a desire to find truly fast implementations of shared data structures that are also non-blocking or wait-free. <p> General multi-object algorithms: Early work on wait-free and non-blocking data structures provided methods to transform sequential implementations of arbitrary shared objects into wait-free concurrent implementations, assuming the existence of a universal primitive <ref> [Her91, Plo88, Plo89, JT92] </ref>. Those methods demonstrate the potential of wait-free algorithms, but are too inefficient to use in practice. Herlihy introduced a universal translation method to transform a sequential implementation into either a non-blocking or a wait-free one using LL/SC [Her93]. <p> Unlock & release: After finishing the execute phase, each operation unlocks the k locations and unmarks the child and parent fields. Wait-free solution: The non-blocking algorithm is made wait-free by applying a modification of a standard technique <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its opera tion, must help (some) pending operations be fore it is allowed to start a new operation.
Reference: [Her93] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Transactions on Programming Languages and Systems 15(5): </journal> <pages> 745-770, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This abstract description ignores many details of the non-blocking algorithm. An important omission is the standard technique of a blocked process helping the process that blocks it, which is how algorithms are made non-blocking <ref> [Her91, Her93] </ref>. If at any phase process p is blocked by another process q, then p helps q to proceed until q does not block p anymore, and then p continues. In addition, both algorithms guarantee that no operation will help another at distance greater than O (log fl n). <p> In addition, both algorithms guarantee that no operation will help another at distance greater than O (log fl n). We apply a familiar technique for converting the non-blocking algorithm to a wait-free version <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its non-blocking operation, must help other pending operations, before it starts a new operation of its own. <p> Those methods demonstrate the potential of wait-free algorithms, but are too inefficient to use in practice. Herlihy introduced a universal translation method to transform a sequential implementation into either a non-blocking or a wait-free one using LL/SC <ref> [Her93] </ref>. These methods are simple and easily implementable, but still each process has to copy the entire data structure even for a minor update. In addition, while using the wait-free method, each process must take O (n) steps even when accessing the data structure alone. <p> Unlock & release: After finishing the execute phase, each operation unlocks the k locations and unmarks the child and parent fields. Wait-free solution: The non-blocking algorithm is made wait-free by applying a modification of a standard technique <ref> [Her91, Her93, AM95a] </ref>: A process that properly terminates its opera tion, must help (some) pending operations be fore it is allowed to start a new operation.
Reference: [IR93] <author> A. Israeli and L. Rappoport. </author> <title> Efficient wait free implementation of a concurrent priority queue. </title> <booktitle> In Workshop on Distributed Algorithms on Graphs 1993. Lecture Notes in Computer Science 725, </booktitle> <publisher> Springer Verlag, </publisher> <pages> pages 1-17. </pages>
Reference-contexts: Most of the non-blocking multi-object implementations in the literature (e.g. <ref> [IR93, AM95a, ST95] </ref>) are based on the same universal single-location operation, LoadLink/StoreConditional (LL/SC). Instead of LL/SC, our algorithm is based on the existence of a general, atomic, single-location RMW operation, where each location may hold many fields.
Reference: [IR94] <author> A. Israeli and L. Rappoport. </author> <title> Disjoint-access-parallel implementations of strong shared memory. </title> <booktitle> In Proc. 14th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 151-160, </pages> <year> 1994. </year>
Reference-contexts: That is, operations that access disjoint parts of the data structure, or are widely separated in time, should not interfere with each other. For example, operations on disjoint sets of components can proceed independently, and avoid concurrency-control overhead <ref> [Bar93, IR94, TSP92, ST95, AM95a] </ref>. When there is considerable contention, conflicting operations could form long chains and complex webs, transitively effecting operations that are otherwise disjoint. Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. <p> A search for efficient implementations proceeded. Barnes presented algorithms with the idea of "simulating" the update without effecting the shared memory, and then to apply the changes atomically using a non-blocking implementation of a multi-word RMW [Bar93]. This idea was developed further by Israeli and Rappoport <ref> [IR94] </ref>. Turek, Shasha, and Prakash explored similar constructions using an atomic compare&swap primitive [TSP92]. These non-blocking algorithms avoid copying the entire data structure, and allow disjoint access: operations in disjoint portions of the conflict graph can proceed independently. <p> The result is a non-blocking algorithm implementing atomic k-multi-object atomic operations, for any fixed k. We then show how to modify the non-blocking algorithm and obtain a wait-free version. Our generalization solves an important question <ref> [Bar93, IR94] </ref>, demonstrating the possibility of a truly fast multi-object algorithm, in which complexity of operations is bounded only by the local contention, rather than by the global contention as in [ADT95]. <p> Accesses to the individual locations in the shared memory alternate with updates to the operation record's local variables and program counter. Careful record keeping in both the shared locations and the operation record ensure that no step of the operation is executed more than once on the shared memory <ref> [Bar93, IR94] </ref>. 3.2 Results In this subsection we state the main properties of the algorithm. Proofs are omitted from this extended abstract. Within the filter, a process helps only operations that are either 1- or 2-neighbors of its operation.
Reference: [JT92] <author> P. Jayanti and S. Toueg. </author> <title> Some results on the impossibility, universality, and decidability of consensus. </title> <booktitle> Proc. of the 6th Int. Workshop on Distributed Algorithms: Lecture Notes in Computer Science, </booktitle> <volume> 647, </volume> <pages> pages 69-84. </pages> <publisher> Springer Verlag, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: General multi-object algorithms: Early work on wait-free and non-blocking data structures provided methods to transform sequential implementations of arbitrary shared objects into wait-free concurrent implementations, assuming the existence of a universal primitive <ref> [Her91, Plo88, Plo89, JT92] </ref>. Those methods demonstrate the potential of wait-free algorithms, but are too inefficient to use in practice. Herlihy introduced a universal translation method to transform a sequential implementation into either a non-blocking or a wait-free one using LL/SC [Her93].
Reference: [Lyn81] <author> N. A. Lynch. </author> <title> Upper bounds for static resource allocation in a distributed systems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 23 </volume> <pages> 254-278, </pages> <year> 1981. </year>
Reference-contexts: Since then, dozens of solutions to this problem have been proposed. Lynch showed that the dining philosophers problem can be extended to an arbitrary graph network, in which a philosopher needs to acquire the resources on all incident edges <ref> [Lyn81] </ref>. Chandy and Misra further generalized this by introducing the Drinking Philosophers problem, in which a philosopher needs some non-empty subset of its resources; this subset of resources may change over time. These papers assume a fault-free model [CM84].
Reference: [MT94] <author> M. Merritt and G. Taubenfeld. </author> <title> Atomic m-register operations. </title> <journal> Distributed Computing, </journal> <volume> 7 </volume> <pages> 213-221, </pages> <year> 1994. </year> <note> Also appeared in Proc. 5th Int. Workshop on Distributed Algorithms, </note> <year> 1991, </year> <pages> pages 289-294. </pages>
Reference-contexts: complexity, we apply this technique separately to each location, and each terminating operation helps only pending operations with which it directly conflicts. 1.3 Related work Early work in multi-objects focused on the synchronization power of m-registers, multi-objects in which the operations are restricted to reads and writes of m locations <ref> [Her91, MT94] </ref>. The synchronization power of other multi-objects is studied in [AMT96]. Recent attention has focused on the multi-object problem out of a desire to find truly fast implementations of shared data structures that are also non-blocking or wait-free.
Reference: [Plo88] <author> S. A. Plotkin. </author> <title> Chapter 4: Sticky Bits and Universality of Consensus. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: General multi-object algorithms: Early work on wait-free and non-blocking data structures provided methods to transform sequential implementations of arbitrary shared objects into wait-free concurrent implementations, assuming the existence of a universal primitive <ref> [Her91, Plo88, Plo89, JT92] </ref>. Those methods demonstrate the potential of wait-free algorithms, but are too inefficient to use in practice. Herlihy introduced a universal translation method to transform a sequential implementation into either a non-blocking or a wait-free one using LL/SC [Her93].
Reference: [Plo89] <author> S. A. Plotkin. </author> <title> Sticky bits and universality of consensus. </title> <booktitle> In Proc. 8th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 159-175, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: General multi-object algorithms: Early work on wait-free and non-blocking data structures provided methods to transform sequential implementations of arbitrary shared objects into wait-free concurrent implementations, assuming the existence of a universal primitive <ref> [Her91, Plo88, Plo89, JT92] </ref>. Those methods demonstrate the potential of wait-free algorithms, but are too inefficient to use in practice. Herlihy introduced a universal translation method to transform a sequential implementation into either a non-blocking or a wait-free one using LL/SC [Her93].
Reference: [SP88] <author> E. Styer and G. L. Peterson. </author> <title> Improved algorithms for distributed resource allocation. </title> <booktitle> In Proc. 7th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 105-116, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These papers assume a fault-free model [CM84]. A few recent papers on the resource allocation problem in asynchronous message passing systems have considered the notion of failure locality <ref> [SP88, AS90, CS95, CS96] </ref>. Failure locality of an algorithm is defined as the smallest m such that any process, for which there are no failures within a distance m in the conflict graph, is free from starvation.
Reference: [ST95] <author> N. Shavit and D. Touitou. </author> <title> Software transactional memory. </title> <booktitle> In Proc. 14th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 204-213, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: That is, operations that access disjoint parts of the data structure, or are widely separated in time, should not interfere with each other. For example, operations on disjoint sets of components can proceed independently, and avoid concurrency-control overhead <ref> [Bar93, IR94, TSP92, ST95, AM95a] </ref>. When there is considerable contention, conflicting operations could form long chains and complex webs, transitively effecting operations that are otherwise disjoint. Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. <p> Most of the non-blocking multi-object implementations in the literature (e.g. <ref> [IR93, AM95a, ST95] </ref>) are based on the same universal single-location operation, LoadLink/StoreConditional (LL/SC). Instead of LL/SC, our algorithm is based on the existence of a general, atomic, single-location RMW operation, where each location may hold many fields. <p> Processes waiting for each other recursively can form long chains of size n. Shavit and Touitou present a non-blocking implementation of multi-word atomic RMW from LL/SC <ref> [ST95] </ref>. This non-blocking algorithm also avoids copying the entire data structure and allows disjoint access, and has only constant-length helping chains: a process has to help only its closest neighbors, which reduces contention. Simulation results indicate that it may perform well in practice.
Reference: [TSP92] <author> J. Turek, D. Shasha, and S. Prakash. </author> <title> Locking without blocking: Making lock based concurrent data structure algorithms non-blocking. </title> <booktitle> In Proceedings of the 1992 Conference on the Principles of Database Systems, </booktitle> <pages> pages 212-222, </pages> <year> 1992. </year>
Reference-contexts: That is, operations that access disjoint parts of the data structure, or are widely separated in time, should not interfere with each other. For example, operations on disjoint sets of components can proceed independently, and avoid concurrency-control overhead <ref> [Bar93, IR94, TSP92, ST95, AM95a] </ref>. When there is considerable contention, conflicting operations could form long chains and complex webs, transitively effecting operations that are otherwise disjoint. Extremely fast algorithms might be designed that "disentangle" these webs, and require coordination only among local neighborhoods of contending operations. <p> This idea was developed further by Israeli and Rappoport [IR94]. Turek, Shasha, and Prakash explored similar constructions using an atomic compare&swap primitive <ref> [TSP92] </ref>. These non-blocking algorithms avoid copying the entire data structure, and allow disjoint access: operations in disjoint portions of the conflict graph can proceed independently. Processes waiting for each other recursively can form long chains of size n.
References-found: 29

