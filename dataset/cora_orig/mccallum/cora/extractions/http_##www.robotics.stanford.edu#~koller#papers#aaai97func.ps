URL: http://www.robotics.stanford.edu/~koller/papers/aaai97func.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/aaai97func.html
Root-URL: http://www.robotics.stanford.edu
Email: koller@cs.stanford.edu  dmac@research.att.com  avi@cs.stanford.edu  
Title: Effective Bayesian Inference for Stochastic Programs  
Author: Daphne Koller David McAllester Avi Pfeffer 
Affiliation: Stanford University  AT&T Research  Stanford University  
Date: August 1997  
Address: Providence, Rhode Island,  
Note: In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), pages 740-747,  
Abstract: In this paper, we propose a stochastic version of a general purpose functional programming language as a method of modeling stochastic processes. The language contains random choices, conditional statements, structured values, defined functions, and recursion. By imagining an experiment in which the program is run and the random choices made by sampling, we can interpret a program in this language as encoding a probability distribution over a (potentially infinite) set of objects. We provide an exact algorithm for computing conditional probabilities of the form Pr(P (x) j Q(x)) where x is chosen randomly from this distribution. This algorithm terminates precisely when sampling x and computing P (x) and Q(x) terminates in all possible stochastic executions (under lazy evaluation semantics, in which only values needed to compute the output of the program are evaluated). We demonstrate the applicability of the language and the efficiency of the inference algorithm by encoding both Bayesian networks and stochastic context-free grammars in our language, and showing that our algorithm derives efficient inference algorithms for both. Our language easily supports interesting and useful extensions to these formalisms (e.g., recursive Bayesian networks), to which our inference algorithm will automatically apply.
Abstract-found: 1
Intro-found: 1
Reference: <author> C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <booktitle> In UAI, </booktitle> <year> 1996. </year>
Reference-contexts: In traditional Bayesian networks, the conditional probability tables contain an entry for every combination of values of a node and its parents. There has been much work on more compact representations of conditional probability tables, such as noisy-or models (Pearl 1988) and trees <ref> (Boutilier et al. 1996) </ref>. The latter can be used to model situations where two variables are independent of each other given some values of a third variable and not others. Our language easily expresses both these representations. Our language also supports significant and interesting extensions to the Bayesian network formalism.
Reference: <author> E. Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Modeling uncertainty has shown up in a variety of AI tasks as diverse as diagnosis (Heckerman et al. 1995), natural language processing <ref> (Charniak 1993) </ref>, planning (Dean et al. 1993), and more. <p> The different requirements of these tasks have resulted in the use of different stochastic modeling languages, such as Bayesian networks (Pearl 1988) and dynamic Bayesian networks (Dean and Kanazawa 1989), hidden Markov models (Rabiner and Juang 1986), and stochastic context-free grammars (SCFGs) <ref> (Charniak 1993) </ref>. In many respects, these formalisms appear quite different, and each of them has induced special-purpose probabilistic inference algorithms. Recently, however, there has been a growing understanding that the formalisms have a common basis, primarily the use of probabilistic independence as the key to compact representations and efficient inference. <p> The expressive power of our language is not restricted to extensions of Bayesian networks. For example, it easily models stochastic context-free grammars (SCFG), a formalism which has been used in statistical natural language processing <ref> (Charniak 1993) </ref> and understanding biological structures (Sakakibara et al. 1995). A stochastic context-free grammar (SCFG) is the natural probabilistic extension of a context-free grammar.
Reference: <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: The different requirements of these tasks have resulted in the use of different stochastic modeling languages, such as Bayesian networks (Pearl 1988) and dynamic Bayesian networks <ref> (Dean and Kanazawa 1989) </ref>, hidden Markov models (Rabiner and Juang 1986), and stochastic context-free grammars (SCFGs) (Charniak 1993). In many respects, these formalisms appear quite different, and each of them has induced special-purpose probabilistic inference algorithms. <p> For example, we can easily model Bayesian networks where one function recursively calls another (or itself); our lazy semantics will provide semantics to such networks even when the recursion is infinite. A similar idea can be used to describe complex Markov processes. In a dynamic belief network <ref> (Dean and Kanazawa 1989) </ref>, the state of the world at one instant in time is a stochastic function of the state at the previous instant. We can model this as a user-defined function that takes one state as an input and outputs the new state.
Reference: <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In AAAI, </booktitle> <year> 1993. </year>
Reference-contexts: Modeling uncertainty has shown up in a variety of AI tasks as diverse as diagnosis (Heckerman et al. 1995), natural language processing (Charniak 1993), planning <ref> (Dean et al. 1993) </ref>, and more. The different requirements of these tasks have resulted in the use of different stochastic modeling languages, such as Bayesian networks (Pearl 1988) and dynamic Bayesian networks (Dean and Kanazawa 1989), hidden Markov models (Rabiner and Juang 1986), and stochastic context-free grammars (SCFGs) (Charniak 1993).
Reference: <author> R. Dechter. </author> <title> Bucket elimination: A unifying framework for probabilistic inference. </title> <booktitle> In UAI, </booktitle> <year> 1996. </year>
Reference-contexts: In this case, we say that the query is evidence finite. While termination is good, it is hardly enough. Our algorithm achieves efficient inference by utilizing the principles underlying efficient Bayesian network algorithms <ref> (Dechter 1996) </ref> and SCFG algorithms (Lari and Young 1990). Despite its generality, our algorithm is almost as efficient as these special-purpose algorithms. <p> When applied to a Bayesian network, our algorithm essentially mimics a standard efficient inference algorithm for Bayesian networks, one based on variable elimination (e.g., <ref> (Dechter 1996) </ref>). It follows from our explanation of the algorithm above that PEVAL (N; x; V ) returns a distribution over networks that corresponds to the factor (a product of conditional probability tables) over V obtained by eliminating all other variables in x's cone.
Reference: <author> P. Haddawy. </author> <title> Generating bayesian networks from probability logic knowledge bases. </title> <booktitle> In UAI, </booktitle> <year> 1994. </year>
Reference: <author> D. Heckerman and J. S. Breese. </author> <title> A new look at causal independence. </title> <booktitle> In UAI, </booktitle> <year> 1994. </year>
Reference: <author> D. Heckerman, J. Breese, and K. Rommelse. </author> <title> Decision-theoretic troubleshooting. </title> <journal> CACM, </journal> <volume> 38(3) </volume> <pages> 49-57, </pages> <year> 1995. </year>
Reference-contexts: Modeling uncertainty has shown up in a variety of AI tasks as diverse as diagnosis <ref> (Heckerman et al. 1995) </ref>, natural language processing (Charniak 1993), planning (Dean et al. 1993), and more.
Reference: <author> D. Koller and A. Pfeffer. </author> <title> Object-oriented bayesian networks. </title> <note> Submitted for publication. </note>
Reference: <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: In this case, we say that the query is evidence finite. While termination is good, it is hardly enough. Our algorithm achieves efficient inference by utilizing the principles underlying efficient Bayesian network algorithms (Dechter 1996) and SCFG algorithms <ref> (Lari and Young 1990) </ref>. Despite its generality, our algorithm is almost as efficient as these special-purpose algorithms. Thus, for example, when we run our algorithm on a stochastic program representing a Bayesian network, its computational behavior is the same as that of standard variable elimination algorithms for Bayesian networks.
Reference: <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The different requirements of these tasks have resulted in the use of different stochastic modeling languages, such as Bayesian networks <ref> (Pearl 1988) </ref> and dynamic Bayesian networks (Dean and Kanazawa 1989), hidden Markov models (Rabiner and Juang 1986), and stochastic context-free grammars (SCFGs) (Charniak 1993). In many respects, these formalisms appear quite different, and each of them has induced special-purpose probabilistic inference algorithms. <p> These examples only scratch the surface of the expressive power of the language, but they should give a taste of the possibilities. Lack of space precludes us from presenting more examples. A traditional Bayesian network <ref> (Pearl 1988) </ref> is a DAG in which each node is a random variable. Associated with each node is a conditional probability table defining the probability of each possible value of a node given each possible assignment of values to its parents. <p> In traditional Bayesian networks, the conditional probability tables contain an entry for every combination of values of a node and its parents. There has been much work on more compact representations of conditional probability tables, such as noisy-or models <ref> (Pearl 1988) </ref> and trees (Boutilier et al. 1996). The latter can be used to model situations where two variables are independent of each other given some values of a third variable and not others. Our language easily expresses both these representations.
Reference: <author> J. Pearl. </author> <title> A probabilistic calculus of actions. </title> <booktitle> In UAI, </booktitle> <year> 1994. </year>
Reference-contexts: Our language can be used to extend the framework of 1 This functional perspective is, in fact, the basis for Pearl's recent work on the causal semantics of Bayesian networks <ref> (Pearl 1994) </ref>. Bayesian networks even further. For example, we can easily model Bayesian networks where one function recursively calls another (or itself); our lazy semantics will provide semantics to such networks even when the recursion is infinite. A similar idea can be used to describe complex Markov processes.
Reference: <author> D. Poole. </author> <title> Probabilistic horn abduction and bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1), </volume> <month> November </month> <year> 1993. </year>
Reference: <author> D. V. Pynadath and M. P. Wellman. </author> <title> Generalized queries in probabilistic context-free grammars. </title> <booktitle> In AAAI, </booktitle> <year> 1996. </year>
Reference: <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <month> January </month> <year> 1986. </year>
Reference-contexts: The different requirements of these tasks have resulted in the use of different stochastic modeling languages, such as Bayesian networks (Pearl 1988) and dynamic Bayesian networks (Dean and Kanazawa 1989), hidden Markov models <ref> (Rabiner and Juang 1986) </ref>, and stochastic context-free grammars (SCFGs) (Charniak 1993). In many respects, these formalisms appear quite different, and each of them has induced special-purpose probabilistic inference algorithms.
Reference: <author> Y. Sakakibara, M. Brown, R. C. Underwood, I. S. Mian, and D. Haussler. </author> <title> Stochastic context-free grammars for modeling RNA. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <year> 1995. </year>
Reference-contexts: The expressive power of our language is not restricted to extensions of Bayesian networks. For example, it easily models stochastic context-free grammars (SCFG), a formalism which has been used in statistical natural language processing (Charniak 1993) and understanding biological structures <ref> (Sakakibara et al. 1995) </ref>. A stochastic context-free grammar (SCFG) is the natural probabilistic extension of a context-free grammar. It contains sets of non-terminal and terminal symbols, where each non-terminal symbol is associated with a set of productions which transform it into strings of terminals and non-terminals.
Reference: <author> P. Smyth, D. Heckerman, and M. Jordan. </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <institution> MSR-TR-96-03, Microsoft Research, </institution> <year> 1996. </year>
Reference: <author> S. Srinivas. </author> <title> A probabilistic approach to hierarchical model-based diagnosis. </title> <booktitle> In UAI, </booktitle> <year> 1994. </year>
Reference-contexts: As we have recently shown (Koller and Pfeffer ), this capability provides the foundation for the definition of a hierarchical and even an object-oriented Bayesian network. For example, we can easily model fault diagnosis in component hierarchies (as in <ref> (Srinivas 1994) </ref>), where the inputs to a high-level component are passed to its subcomponents, which in turn return their output value.
References-found: 18

