URL: ftp://ftp.comlab.ox.ac.uk/pub/Packages/ILP/Papers/ulearn10.ps
Refering-URL: http://gruffle.comlab.ox.ac.uk/oucl/groups/machlearn/mlg_pub.html
Root-URL: 
Title: Programming Research Group A LEARNABILITY MODEL FOR UNIVERSAL REPRESENTATIONS  
Author: Stephen Muggleton C. David Page Jr. 
Address: Wolfson Building, Parks Road, Oxford OX1 3QD  
Affiliation: Oxford University Computing Laboratory  
Pubnum: PRG-TR-3-97  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Angluin and M. Kharitonov. </author> <booktitle> When won't membership queries help? In Proceedings of the 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pages 444-454. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]).
Reference: [2] <author> Martin Anthony, Norman Biggs, and John Shawe-Taylor. </author> <title> The learnability of formal con-cepts. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 246-257, </pages> <address> Rochester, NY, August 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]).
Reference: [3] <author> B. Arbab and D. Michie. </author> <title> Generating rules from examples. </title> <booktitle> In IJCAI-85, </booktitle> <pages> pages 631-633, </pages> <address> Los Altos, CA, 1985. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: More sophisticated versions of this assumption are used in Rissanen's [29, 30] Minimal Description Length (MDL) principle. A very different kind of distribution assumption was implicitly implemented by Arbab, Michie <ref> [3] </ref> and Bratko [9] in an algorithm for constructing decision trees. The algorithm constructs a decision list (linear decision tree) where one exists and otherwise returns the most linear decision tree which can be constructed from the data.
Reference: [4] <author> S. Ben-David, B. Chor, O. Goldreich, and M. Luby. </author> <title> On the theory of average case complexity. </title> <journal> Journal of Information and System Sciences, </journal> <volume> 44 </volume> <pages> 193-219, </pages> <year> 1992. </year>
Reference-contexts: See <ref> [4] </ref> for the motivation of this definition of average-case complexity. 5 Notice that the definition of U-learnability is superficially similar to identification in the limit [15]. Nevertheless neither identification-in-the-limit nor U-learnability is a special case of the other, because examples are drawn in very different ways in the two models. <p> Whereas negative results for PAC-learnability can be obtained by showing that the consistency problem for a given concept representation is NP-hard (assuming RP 6= N P ) [26], negative results for U-learnability in some cases can be obtained by showing that the consistency problem is NP-hard-on-average (or DistNP-hard) <ref> [4, 17] </ref> relative to particular distributions on hypotheses and examples.
Reference: [5] <author> A.W. Biermann. </author> <title> The inference of regular LISP programs from examples. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8(8) </volume> <pages> 585-600, </pages> <year> 1978. </year>
Reference-contexts: Gold [15], motivated by the problem of human language acquisition, introduced the first computational model of learning. This model, known as "identification-in-the-limit", describes the conditions for finite convergence of a language identification procedure. Although Gold's model is well suited for learning universal representations <ref> [6, 5] </ref>, it fails to meet our purposes for two reasons. Firstly, at no point can the learning agent give guarantees on the degree of error of a preferred hypothesis. Secondly, although convergence is finite it could take arbitrarily long.
Reference: [6] <author> A.W. Biermann and R. Krishnaswamy. </author> <title> Constructing programs from example computations. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 2(3), </volume> <year> 1976. </year>
Reference-contexts: Gold [15], motivated by the problem of human language acquisition, introduced the first computational model of learning. This model, known as "identification-in-the-limit", describes the conditions for finite convergence of a language identification procedure. Although Gold's model is well suited for learning universal representations <ref> [6, 5] </ref>, it fails to meet our purposes for two reasons. Firstly, at no point can the learning agent give guarantees on the degree of error of a preferred hypothesis. Secondly, although convergence is finite it could take arbitrarily long.
Reference: [7] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: For instance, a very common assumption used in Machine Learning is that textually shorter theories are more likely to be correct a priori (Oc- cam's Razor). Although Occam algorithms have been studied in PAC-learning <ref> [7] </ref>, this study has not led to algorithms capable of learning in universal representations, as it has in Inductive Logic Programming [24]. <p> rewritten as (1 *) m i=1 Substituting ES (H) for k X P r D F (H i )j [H i ]j yields (1 *) m (ES) &lt; ffi This holds just if m ffi (This last step can be verified by the arguments used to derive the Blumer Bound <ref> [7] </ref>.) 2 Definition 4 (Polynomial and Exponential Size-Based Enumerations) Let R be countable.
Reference: [8] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the VapnikChervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year> <month> 20 </month>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]). <p> In each of these results, let ( E ; X; C ; R; c) be the representation of any given learning problem. Before presenting these results, we develop a useful property similar to the Blumer Bound <ref> [8] </ref> that is used in many PAC-learning results. (The Blumer Bound states that for a finite hypothesis space H , a sample of size m ln jHj+ln 1 * is sufficient for accuracy 1 * with confidence 1 ffi.) Let H = hH 1 ; H 2 ; :::; H k
Reference: [9] <author> I. Bratko. </author> <title> Generating human-understandable decision rules. </title> <note> Working paper, </note> <editor> E. </editor> <address> Kardelj Uni-versity Ljubljana, Ljubljana, Yugoslavia, </address> <year> 1983. </year>
Reference-contexts: More sophisticated versions of this assumption are used in Rissanen's [29, 30] Minimal Description Length (MDL) principle. A very different kind of distribution assumption was implicitly implemented by Arbab, Michie [3] and Bratko <ref> [9] </ref> in an algorithm for constructing decision trees. The algorithm constructs a decision list (linear decision tree) where one exists and otherwise returns the most linear decision tree which can be constructed from the data.
Reference: [10] <author> W. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: U-learnability has a natural interpretation within Bayesian analysis. Distributional assumptions over hypotheses equate to subjective prior distributions from which, for a given set of examples, conditional distributions can be derived using Bayes' Theorem. Bayesian approaches to Machine Learning have been discussed previously in the literature. For instance, Buntine <ref> [10] </ref> develops a Bayesian framework for learning, which he applies to the problem of learning class probability trees. Also Haussler et al. [16] analyse the sample complexity of two Bayesian algorithms. This analysis focuses on average case, rather than worst case, accuracy.
Reference: [11] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> AutoClass: a bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Yet another form of distribution assumption popular in "clustering" algorithms is that sets of clusters are preferred to the degree that the clusters are small, separated and dense. This works when examples are known to form small, dense groups, as in the case of celestial data <ref> [12, 11] </ref>. A very different kind of hypothesis distribution is required when learning concepts which mimic human performance in skill tasks [31]. For such tasks predictive accuracy is dependent on hypotheses being evaluable in time similar to that of human reaction.
Reference: [12] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI88), </booktitle> <pages> pages 607-611, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Yet another form of distribution assumption popular in "clustering" algorithms is that sets of clusters are preferred to the degree that the clusters are small, separated and dense. This works when examples are known to form small, dense groups, as in the case of celestial data <ref> [12, 11] </ref>. A very different kind of hypothesis distribution is required when learning concepts which mimic human performance in skill tasks [31]. For such tasks predictive accuracy is dependent on hypotheses being evaluable in time similar to that of human reaction.
Reference: [13] <author> W. W. Cohen. </author> <title> Cryptographic limitations on learning one-clause logic programs. </title> <booktitle> In AAAI-93, </booktitle> <address> Menlo Park, CA, 1993. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example [8, 26, 18, 27, 2, 1]). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted <ref> [13] </ref>). Even arbitrary propositional formulae and finite state automata are unlearnable (based on certain cryptographic assumptions) [18], and propositional formulae in disjunctive normal form are not known to be learnable. This contrasts with the fact that most machine learning algorithms use representations at least this rich.
Reference: [14] <author> J. Fisher. </author> <title> Statistical Methods for Research Workers. </title> <publisher> Oliver and Boyd, </publisher> <address> London, </address> <year> 1970. </year>
Reference-contexts: A very different kind of hypothesis distribution is required when learning concepts which mimic human performance in skill tasks [31]. For such tasks predictive accuracy is dependent on hypotheses being evaluable in time similar to that of human reaction. Within Statistics distributional assumptions include Fisher's <ref> [14] </ref> Maximum Likelihood Principle, which holds when all hypotheses have almost equal prior probability. According to a recent large scale set of comparative trials on industrial data [22] the assumptions in machine learning and statistical algorithms often fit real world problems; in such cases the algorithms succeed.
Reference: [15] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Inf. Control, </journal> (10):447-474, 1967. 
Reference-contexts: Clearly universal representations are also of interest in a number of other learning contexts, including program synthesis [37, 32]. In this paper we present a new computational model of learning which is appropriate for use with representations up to and including universal ones. Gold <ref> [15] </ref>, motivated by the problem of human language acquisition, introduced the first computational model of learning. This model, known as "identification-in-the-limit", describes the conditions for finite convergence of a language identification procedure. <p> See [4] for the motivation of this definition of average-case complexity. 5 Notice that the definition of U-learnability is superficially similar to identification in the limit <ref> [15] </ref>. Nevertheless neither identification-in-the-limit nor U-learnability is a special case of the other, because examples are drawn in very different ways in the two models. The following remark notes that PAC-learnability is a special case of U-learnability. Remark 2 U-learnability generalises PAC.
Reference: [16] <author> D. Haussler, M Kearns, and R. Shapire. </author> <title> Bounds on the sample complexity of bayesian learning using information theory and the vc dimension. </title> <booktitle> In COLT-91: Proceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: Bayesian approaches to Machine Learning have been discussed previously in the literature. For instance, Buntine [10] develops a Bayesian framework for learning, which he applies to the problem of learning class probability trees. Also Haussler et al. <ref> [16] </ref> analyse the sample complexity of two Bayesian algorithms. This analysis focuses on average case, rather than worst case, accuracy.
Reference: [17] <author> D. Johnson. </author> <title> The NP-completeness column|an ongoing guide. </title> <journal> Journal of Algorithms, </journal> <volume> 4:284299, </volume> <year> 1984. </year>
Reference-contexts: Whereas negative results for PAC-learnability can be obtained by showing that the consistency problem for a given concept representation is NP-hard (assuming RP 6= N P ) [26], negative results for U-learnability in some cases can be obtained by showing that the consistency problem is NP-hard-on-average (or DistNP-hard) <ref> [4, 17] </ref> relative to particular distributions on hypotheses and examples.
Reference: [18] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433444. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]). <p> However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]). Even arbitrary propositional formulae and finite state automata are unlearnable (based on certain cryptographic assumptions) <ref> [18] </ref>, and propositional formulae in disjunctive normal form are not known to be learnable. This contrasts with the fact that most machine learning algorithms use representations at least this rich. <p> In addition, just as negative results for PAC-predictability can be obtained (based on certain assumptions) using hard problems from cryptography <ref> [18] </ref>, negative results for U-predictability possibly might be obtained in this same way, but would require additional effort. 7 Specifically, obtaining negative results for U-predictability will require specifying a particular distribution (or family of distributions) to be used with a hard problem from cryptography, such as inverting RSA, and it must
Reference: [19] <author> R. King, S. Muggleton R. Lewis, and M. Sternberg. </author> <title> Drug design by machine learning: The use of inductive logic programming to model the structure-activity relationships of trimethoprim analogues binding to dihydrofolate reductase. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 89(23), </volume> <year> 1992. </year>
Reference-contexts: 1 Introduction Learning is a central feature of human cognition. As overwhelming amounts of data are becoming available in scientific, sensory and linguistic domains, algorithms which allow computers to learn are becoming increasingly important. For instance, recent papers <ref> [20, 19, 23, 36, 35] </ref> report a machine learning algorithm producing novel biological insights beyond those achieved by persistent human visual inspection of the data. The algorithm induces pure logic programs from data.
Reference: [20] <author> R. King and M.J.E. Sternberg. </author> <title> A machine learning approach for the prediction of protein secondary structure. </title> <journal> Journal of Molecular Biology, </journal> <volume> 216 </volume> <pages> 441-457, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Learning is a central feature of human cognition. As overwhelming amounts of data are becoming available in scientific, sensory and linguistic domains, algorithms which allow computers to learn are becoming increasingly important. For instance, recent papers <ref> [20, 19, 23, 36, 35] </ref> report a machine learning algorithm producing novel biological insights beyond those achieved by persistent human visual inspection of the data. The algorithm induces pure logic programs from data.
Reference: [21] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 82-132. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1983. </year>
Reference-contexts: The importance of such soft biases has long been recognised in Machine Learning (for example, see the discussion of preference criteria in <ref> [21] </ref>), but such biases have never before been incorporated into a formal computational model of learning. The paper is arranged as follows. Section 2 defines U-learnability. Section 3 presents some initial, basic results, and Section 4 points to directions for more advanced results.
Reference: [22] <author> D. Michie, D. Spiegelhalter, and C. Taylor. </author> <title> Machine Learning, Neural, and Statistical Classification. </title> <publisher> Ellis Horwood Limited, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Within Statistics distributional assumptions include Fisher's [14] Maximum Likelihood Principle, which holds when all hypotheses have almost equal prior probability. According to a recent large scale set of comparative trials on industrial data <ref> [22] </ref> the assumptions in machine learning and statistical algorithms often fit real world problems; in such cases the algorithms succeed. Unlike PAC-learnability, the model in this paper allows the use of such distributional assumptions. Universal representations are shown to be polynomially learnable for certain families of distributions.
Reference: [23] <author> S. Muggleton, R. King, and M. Sternberg. </author> <title> Protein secondary structure prediction using logicbased machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5(7) </volume> <pages> 647-657, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Learning is a central feature of human cognition. As overwhelming amounts of data are becoming available in scientific, sensory and linguistic domains, algorithms which allow computers to learn are becoming increasingly important. For instance, recent papers <ref> [20, 19, 23, 36, 35] </ref> report a machine learning algorithm producing novel biological insights beyond those achieved by persistent human visual inspection of the data. The algorithm induces pure logic programs from data.
Reference: [24] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 12, </volume> <year> 1994. </year> <note> (to appear). 21 </note>
Reference-contexts: Although Occam algorithms have been studied in PAC-learning [7], this study has not led to algorithms capable of learning in universal representations, as it has in Inductive Logic Programming <ref> [24] </ref>. The Occam assumption was made explicitly by Solomonoff [33] who proposed universality of the distribution which assigns the prior probability of hypothesis h to be 2 length (h) . More sophisticated versions of this assumption are used in Rissanen's [29, 30] Minimal Description Length (MDL) principle.
Reference: [25] <author> B. K. Natarajan. </author> <title> Learning from exercises. </title> <booktitle> In Proceedings of the 1989 Workshop on Computa-tional Learning Theory, </booktitle> <pages> pages 72-86, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Within U-learnability distributions range over hypotheses which include a time bound. The use of such time bounds allows us to characterise not only inductive learning but also speed-up learning. Speed-up learn <p>- 2 ing cannot be viewed as a special case of PAC-learning although Natarajan <ref> [25] </ref> has provided a PAC-style formalisation of speed-up learning, which has as yet yielded few positive results. U-learnability has a natural interpretation within Bayesian analysis. Distributional assumptions over hypotheses equate to subjective prior distributions from which, for a given set of examples, conditional distributions can be derived using Bayes' Theorem. <p> A surprising aspect of the second definition is that it not only captures the notion of inductive learning relative to background knowledge, as desired, but it also provides an alternative to Natarajan's PAC-style formalisation of speed-up learning <ref> [25] </ref>. Although Natarajan's formalisation is quite appealing, few positive results have been proven within it because it is also very demanding.
Reference: [26] <author> L. Pitt and L. G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]). <p> We also expect that negative results for some such distributions will be challenging and useful. Whereas negative results for PAC-learnability can be obtained by showing that the consistency problem for a given concept representation is NP-hard (assuming RP 6= N P ) <ref> [26] </ref>, negative results for U-learnability in some cases can be obtained by showing that the consistency problem is NP-hard-on-average (or DistNP-hard) [4, 17] relative to particular distributions on hypotheses and examples.
Reference: [27] <author> L. Pitt and M. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41 </volume> <pages> 430-467, </pages> <year> 1990. </year>
Reference-contexts: Valiant [38], motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example <ref> [8, 26, 18, 27, 2, 1] </ref>). However, PAC's stringent requirements lead to negative results for universal representations (logic programs are prediction- hard, even when severely restricted [13]).
Reference: [28] <author> G.D. Plotkin. </author> <title> Automatic Methods of Inductive Inference. </title> <type> PhD thesis, </type> <institution> Edinburgh University, </institution> <month> August </month> <year> 1971. </year>
Reference-contexts: The first extension (Section 4.1) is the traditional setting used in Inductive Logic Programming, and its roots go back at least to Plotkin's study of learning logic programs in the limit <ref> [28] </ref>. It 7 We thank Micheal Kearns of AT&T Bell Labs for a very helpful general discussion, with one of the authors, related to this point. 16 assumes that the background theory that is provided is, effectively, a syntactic part of the target concept.
Reference: [29] <author> J. Rissanen. </author> <title> Modeling by Shortest Data Description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: The Occam assumption was made explicitly by Solomonoff [33] who proposed universality of the distribution which assigns the prior probability of hypothesis h to be 2 length (h) . More sophisticated versions of this assumption are used in Rissanen's <ref> [29, 30] </ref> Minimal Description Length (MDL) principle. A very different kind of distribution assumption was implicitly implemented by Arbab, Michie [3] and Bratko [9] in an algorithm for constructing decision trees.
Reference: [30] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by Minimum Description Length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1982. </year>
Reference-contexts: The Occam assumption was made explicitly by Solomonoff [33] who proposed universality of the distribution which assigns the prior probability of hypothesis h to be 2 length (h) . More sophisticated versions of this assumption are used in Rissanen's <ref> [29, 30] </ref> Minimal Description Length (MDL) principle. A very different kind of distribution assumption was implicitly implemented by Arbab, Michie [3] and Bratko [9] in an algorithm for constructing decision trees.
Reference: [31] <author> C. Sammut, S. Hurst, D. Kedzier, and D. Michie. </author> <title> Learning to fly. </title> <editor> In D. Sleeman and P. Edwards, editors, </editor> <booktitle> Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 385-393, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This works when examples are known to form small, dense groups, as in the case of celestial data [12, 11]. A very different kind of hypothesis distribution is required when learning concepts which mimic human performance in skill tasks <ref> [31] </ref>. For such tasks predictive accuracy is dependent on hypotheses being evaluable in time similar to that of human reaction. Within Statistics distributional assumptions include Fisher's [14] Maximum Likelihood Principle, which holds when all hypotheses have almost equal prior probability.
Reference: [32] <author> E. Y. Shapiro. </author> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: The algorithm induces pure logic programs from data. Universal (Turing equivalent) representations, such as logic programs, are particularly appropriate in scientific domains due to the complexity of the objects and relationships involved. Clearly universal representations are also of interest in a number of other learning contexts, including program synthesis <ref> [37, 32] </ref>. In this paper we present a new computational model of learning which is appropriate for use with representations up to and including universal ones. Gold [15], motivated by the problem of human language acquisition, introduced the first computational model of learning.
Reference: [33] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 376-388, </pages> <year> 1964. </year>
Reference-contexts: Although Occam algorithms have been studied in PAC-learning [7], this study has not led to algorithms capable of learning in universal representations, as it has in Inductive Logic Programming [24]. The Occam assumption was made explicitly by Solomonoff <ref> [33] </ref> who proposed universality of the distribution which assigns the prior probability of hypothesis h to be 2 length (h) . More sophisticated versions of this assumption are used in Rissanen's [29, 30] Minimal Description Length (MDL) principle.
Reference: [34] <author> A. Srinivasan, S. Muggleton, R. King, and M. Sternberg. Mutagenesis: </author> <title> Ilp experiments in a non-determinate biological domain. </title> <booktitle> Submitted to the Fourth Workshop on Inductive Logic Programming, </booktitle> <year> 1994. </year>
Reference-contexts: to D G . 18 the case where r is a single definite clause, this learning algorithm can be viewed as an idealisation of the program Progol that already has been successfully applied to two significant scientific domains as well as a number of standard benchmark problems for machine learning <ref> [34] </ref>. 5.2 U-learnability with background knowledge (with re-expression of back <p>- ground knowledge) 5.2.1 Preliminaries Let ( E ; X; C ; R; B; c) be the representation of a learning problem with background knowledge, where X is the domain of examples (first-order expressions over the alphabet E ), R is
Reference: [35] <author> M. Sternberg, R. King, R. Lewis, and S. Muggleton. </author> <title> Application of machine learning to structural molecular biology. </title> <journal> Philosophical Transactions of the Royal Society B, </journal> <note> 1994 (to appear). </note>
Reference-contexts: 1 Introduction Learning is a central feature of human cognition. As overwhelming amounts of data are becoming available in scientific, sensory and linguistic domains, algorithms which allow computers to learn are becoming increasingly important. For instance, recent papers <ref> [20, 19, 23, 36, 35] </ref> report a machine learning algorithm producing novel biological insights beyond those achieved by persistent human visual inspection of the data. The algorithm induces pure logic programs from data.
Reference: [36] <author> M. Sternberg, R. Lewis, R. King, and S. Muggleton. </author> <title> Modelling the structure and function of enzymes by machine learning. </title> <journal> Proceedings of the Royal Society of Chemistry: Faraday Discussions, </journal> <volume> 93 </volume> <pages> 269-280, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Learning is a central feature of human cognition. As overwhelming amounts of data are becoming available in scientific, sensory and linguistic domains, algorithms which allow computers to learn are becoming increasingly important. For instance, recent papers <ref> [20, 19, 23, 36, 35] </ref> report a machine learning algorithm producing novel biological insights beyond those achieved by persistent human visual inspection of the data. The algorithm induces pure logic programs from data.
Reference: [37] <author> P.D. Summers. </author> <title> Program construction from examples. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1975. </year>
Reference-contexts: The algorithm induces pure logic programs from data. Universal (Turing equivalent) representations, such as logic programs, are particularly appropriate in scientific domains due to the complexity of the objects and relationships involved. Clearly universal representations are also of interest in a number of other learning contexts, including program synthesis <ref> [37, 32] </ref>. In this paper we present a new computational model of learning which is appropriate for use with representations up to and including universal ones. Gold [15], motivated by the problem of human language acquisition, introduced the first computational model of learning.
Reference: [38] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Firstly, at no point can the learning agent give guarantees on the degree of error of a preferred hypothesis. Secondly, although convergence is finite it could take arbitrarily long. Valiant <ref> [38] </ref>, motivated by the needs of expert system technology, developed a model that provides guarantees on the degree of error and efficiency of convergence for learning. This model, known as Probably-Approximately-Correct (PAC) learning, has gained wide acceptance and stimulated much high-quality research (for example [8, 26, 18, 27, 2, 1]).
References-found: 38

