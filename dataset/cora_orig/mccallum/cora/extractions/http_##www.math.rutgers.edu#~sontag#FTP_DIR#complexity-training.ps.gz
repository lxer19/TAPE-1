URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/complexity-training.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Note: Abstract  
Abstract: We deal with computational issues of loading a fixed-architecture neural network with a set of positive and negative examples. This is the first result on the hardness of loading networks which do not consist of the binary-threshold neurons, but rather utilize a particular continuous activation function, commonly used in the neural network literature. We observe that the loading problem is polynomial-time if the input dimension is constant. Otherwise, however, any possible learning algorithm based on particular fixed architectures faces severe computational barriers. Similar theorems have already been proved by Megiddo and by Blum and Rivest, to the case of binary-threshold networks only. Our theoretical results lend further justification to the use of incremental (architecture-changing) techniques for training networks rather than fixed architectures. Furthermore, they imply hardness of learnability in the probably-approximately-correct sense as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barron, </author> <title> A.R., "Approximation and estimation bounds for artificial neural networks", </title> <booktitle> Proc. 4th Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 243-249. </pages>
Reference-contexts: This type of research ignores the training question itself, asking instead what is the best one could do, in this sense of overall error, if the best possible network with a given architecture were to be eventually found. Some recent papers along these lines are <ref> [1, 11, 5] </ref>, which deal with single hidden layer nets, and [6], which dealt with multiple hidden layers. <p> A -net C is an unbounded fan-in directed acyclic graph. To each vertex v, an activation function v 2 is assigned, and we assume that C has a single sink w. The network C computes a function f C : <ref> [0; 1] </ref> n ! IR as follows. The components of the input vector x = (x 1 ; : : :; x n ) 2 [0; 1] n are assigned to the sources of C. <p> The network C computes a function f C : <ref> [0; 1] </ref> n ! IR as follows. The components of the input vector x = (x 1 ; : : :; x n ) 2 [0; 1] n are assigned to the sources of C. Let v 1 ; : : :; v k be the immediate predecessors of a vertex v. <p> We denote such a behavior as the function fi A (IR r ; <ref> [0; 1] </ref> n ) 7! f0; 1g : The set of inputs which cause the output of the network to be 0 (resp. 1) are termed as the set of negative (resp. positive) examples. <p> The loading problem is defined as follows: Given an architecture A and a set of positive and negative examples M = f (x; y) j x 2 <ref> [0; 1] </ref> n ; y 2 f0; 1gg, so that jM j = O (n); find weights ~w so that for all pairs (x; y) 2 M : 5 The decision version of the loading problem is to decide (rather than to find the weights) whether such weights exist that load <p> Moreover, all the inputs are connected to both the nodes N 1 and N 2 . 2.3 Loading the k H-Node Architecture We consider two kinds of inputs: analog and binary. An analog input is in <ref> [0; 1] </ref> d , where d is a fixed constant, also called the input dimension. In the binary case, the input is in f0; 1g fl , that is, it has unbounded dimensions. <p> Proof of Theorem 1. The computational view of the loading problem of analog input is very similar to the model of Lemma 2.2. However, in this case the points are in <ref> [0; 1] </ref> d rather than Z d . The second discrepancy is that the output of the k H-node architecture is a linear threshold function of the hyper-planes rather than an arbitrary Boolean function. The proof of Lemma 2.2 holds for the analog input as well. <p> Consider a unit G that computes H ( P n i=1 ff i x i ), where ff i 's are real constants and x 1 to x n are input variables which assume any real value in <ref> [0; 1] </ref>. We say that this unit G computes a Boolean NAND (i.e., negated AND) function of its inputs provided its weights and threshold satisfy the following requirements: ff i &lt; &lt; 0 1 i n (3) For justification, assume that the inputs to node G are binary.
Reference: [2] <author> Baum, E.B., and Haussler, D., </author> <title> "What size net gives valid generalization?," </title> <journal> Neural Computation, </journal> <volume> 1(1989): </volume> <pages> 151-160 </pages>
Reference-contexts: Some recent references to such work, establishing sample complexity results, and hence "weak learnabil-ity" in the Valiant model, for neural nets, are the papers <ref> [2, 18, 10, 17] </ref>; the first of these references deals with networks that employ hard threshold activations, the second and third cover continuous activation functions of a type (piecewise polynomial) close to those used in this paper, and the last one provides results for networks employing the standard sigmoid activation function.
Reference: [3] <author> Blum, A., and Rivest, R. L., </author> <title> "Training a 3-node neural network is NP-complete," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D.S. </booktitle> <editor> Touretzky, ed), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <title> pp. 9-18; also as "Training a 3-Node Neural Network is NP-Complete," </title> <booktitle> Neural Networks, </booktitle> <volume> 5(1992): </volume> <pages> 117-127. </pages>
Reference-contexts: Yet another direction in which to approach theoretical questions regarding learning by neural networks, and the one that concerns us here, originates with the work of Judd (see for instance [12, 13], as well as the related work <ref> [3, 16, 28] </ref>). Judd, like us, was motivated by the observation that the "backpropagation" algorithm often runs very slowly, especially for high-dimensional data. Recall that this algorithm is used in order to find a network (that is, find the weights, assuming a fixed architecture) that reproduces the observed data. <p> Furthermore, our result implies non-learnability in the PAC sense under the complexity-theoretic assumption of RP 6= N P . We generalize our result to another similar architecture. The work most closely related to ours is that due to Blum and Rivest; see <ref> [3] </ref>. They showed a similar NP-completeness result for networks having the same architecture but where the activation functions are all of a hard threshold type, that is, they provide a binary output y equal to 1 if the sum in equation (1) is positive, and 0 otherwise. <p> An analog input is in [0; 1] d , where d is a fixed constant, also called the input dimension. In the binary case, the input is in f0; 1g fl , that is, it has unbounded dimensions. Blum and Rivest <ref> [3] </ref> showed when the inputs are binary and the training set is sparse (i.e. if n is the length of the longest string in the training set M , then jM j is polynomial in n) the loading problem is NP-Complete for the 2 H-node architecture. <p> (2,3)-set splitting problem to it: Given an instance I of the (2,3)-SSP: I: S = fs i g, C = fc j g, c j S, j S j= n, j c j j= 3 for all j we create the instance I 0 of the 3-hyperplane problem (like in <ref> [3] </ref>): ? The origin (0 n ) is labeled 0 + 0 ; for each element s j , the point p j having 1 in the j th coordinate only is labeled 0 0 ; and for each clause c l = fs i ; s j ; s k <p> This is a solution type (a) of the 3-hyperplane problem. ( (A) If there is a separation of type (a), the solution of the set-splitting is analogous to <ref> [3] </ref>: Let S 1 and S 2 be the set of 0 0 points p j separated from the origin by H 1 and H 2 , respectively (any point separated by both is placed arbitrarily in one of them). <p> Consider the following classification again on a 3-dimensional hypercube: (0,0,0), (1,0,1), and (0,1,1) are labeled 0 + 0 , and (0,0,1), (0,1,0), (1,0,0), and (1,1,1) are labeled 0 0 . Then, the following statements are true due to the result in <ref> [3] </ref>: (a) No single hyperplane can correctly classify the 0 + 0 and 0 0 points. (b) No two halfspaces H 1 and H 2 exist such that all the 0 + 0 points belong to H 1 _ H 2 and all the 0 0 points belong to H 1 <p> Next, we show that a solution for I exists iff there exists a solution to I 0 . Given a solution to the (2,3)-SSP, by lemma 3.1 (part (b)) and the result in <ref> [3] </ref> the two solution halfspaces to I 0 are as follows (assume the last 5 dimensions are x n+1 to x n+5 ): H 1 : ( i=1 1 H 2 : ( i=1 1 where a i = 1 if s i 2 S 1 2 otherwise b i = <p> N 1 = [(( i=1 N 2 = [(( i=1 N 3 = 1 N 1 N 2 &gt; 1 Conversely, given a solution to I 0 , by Lemma 3.1 (part (a)), Lemma 3.2 and the result in <ref> [3] </ref> (as discussed above) the only type of classification produced by the 2 -node architecture consistent with the classifications on the lower 5 dimensions is of type 2 (a) (with H 1 6= H 2 ) or 2 (b) only, which was shown to be NP-complete in theorem 3.3. 2 17 <p> Inspired by Blum and Rivest <ref> [3] </ref> who considered loading a few variations of the k H-node network, in which all activations functions were discrete; we consider a variations of the k -node architecture in which two nodes compute continuous activation functions.
Reference: [4] <author> Bruck, J., and Goodman, J. W., </author> <title> "On the power of neural networks for solving hard problems", </title> <journal> Journal of Complexity, </journal> <volume> 6(1990): </volume> <pages> 129-135. </pages>
Reference-contexts: A different power is demonstrated by the recurrent threshold nets. It was proved in [22] that the problem of determining whether a recurrent network with threshold units (that is, the number of states in the network is finite) has a stable configuration is P-hard. Bruck and Goodman <ref> [4] </ref> showed that a recurrent threshold network of polynomial size cannot solve NP-complete problems unless NP=co-NP. The result was further extended by Yao [27] who showed that a polynomial size threshold recurrent network cannot solve NP-complete problems even approximately within a guaranteed performance ratio unless NP=co-NP.
Reference: [5] <author> Darken, C., Donahue, M., Gurvits, L., and Sontag, E., </author> <title> "Rate of approximation results motivated by robust neural network learning," </title> <booktitle> Proc. 6th ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July </month> <year> 1993, </year> <pages> pp. 303-309. </pages>
Reference-contexts: This type of research ignores the training question itself, asking instead what is the best one could do, in this sense of overall error, if the best possible network with a given architecture were to be eventually found. Some recent papers along these lines are <ref> [1, 11, 5] </ref>, which deal with single hidden layer nets, and [6], which dealt with multiple hidden layers.
Reference: [6] <author> DasGupta, B., and Schnitger, G., </author> <title> "The power of approximating: a comparison of activation functions," </title> <booktitle> in Advances in Neural Information Processing Systems 5 (Giles, </booktitle> <editor> C.L., Hanson, S.J., and Cowan, J.D., eds), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 615-622. </pages>
Reference-contexts: Some recent papers along these lines are [1, 11, 5], which deal with single hidden layer nets, and <ref> [6] </ref>, which dealt with multiple hidden layers. Yet another direction in which to approach theoretical questions regarding learning by neural networks, and the one that concerns us here, originates with the work of Judd (see for instance [12, 13], as well as the related work [3, 16, 28]).
Reference: [7] <author> Fischer, P. and Simon, H. U., </author> <title> "On Learning Ring-Sum Expansions", </title> <journal> SIAM J. Computing, </journal> <volume> 21, 1(1992): </volume> <pages> 181-192. </pages>
Reference: [8] <author> Garey, M. R., and Johnson, D., </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <publisher> W.H.Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Readers familiar with this material are recommended to skip to Section 2. 1.1 Some complexity classes We informally discuss some well known structural-complexity classes (the reader is referred to any standard text on structural complexity classes (e.g. <ref> [8, 9] </ref>) for more details). Here, whenever we say polynomial 3 time we mean polynomial time in the length of any reasonable encoding of the input (see [8] for a dis-cussion of a "reasonable" encoding of the inputs), and problems referred to here are always decision problems. <p> Here, whenever we say polynomial 3 time we mean polynomial time in the length of any reasonable encoding of the input (see <ref> [8] </ref> for a dis-cussion of a "reasonable" encoding of the inputs), and problems referred to here are always decision problems. A problem is in the class P when there is a polynomial time algorithm which solves the problem. <p> Corollary 3.1 The class of Boolean functions computable by the 2 -node architecture with binary inputs is not learnable, unless RP = N P . 8 To prove theorem 3.1 we reduce a restricted version of the set splitting problem, which is known to be NP-complete <ref> [8] </ref>, to this problem in polynomial time. However, due to the continuity of this activation function, many technical difficulties arise. The proof is organized as follows: 1. Providing a geometric view of the problem [subsection 3.1]. 2. <p> S j = for i 6= j, [ k i=1 S i = S, and c j 6 S i for Note that the (k; l)-SSP is solvable in polynomial time if both k 2 and l 2, but remains NP complete if k 2 and l = 3 (see <ref> [8] </ref>).
Reference: [9] <author> Gill, J., </author> <title> "Computational Complexity of Probabilistic Turing Machines", </title> <journal> SIAM J. Computing, </journal> <volume> 7, 4(1977): </volume> <pages> 675-695. </pages>
Reference-contexts: Readers familiar with this material are recommended to skip to Section 2. 1.1 Some complexity classes We informally discuss some well known structural-complexity classes (the reader is referred to any standard text on structural complexity classes (e.g. <ref> [8, 9] </ref>) for more details). Here, whenever we say polynomial 3 time we mean polynomial time in the length of any reasonable encoding of the input (see [8] for a dis-cussion of a "reasonable" encoding of the inputs), and problems referred to here are always decision problems.
Reference: [10] <author> Goldberg, P., and Jerrum, M., </author> <title> "Bounding the Vapnik-Chervonenkis dimension of concept classes parametrized by real numbers," </title> <booktitle> Proc. 6th ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July </month> <year> 1993, </year> <pages> pp. 361-369. </pages>
Reference-contexts: Some recent references to such work, establishing sample complexity results, and hence "weak learnabil-ity" in the Valiant model, for neural nets, are the papers <ref> [2, 18, 10, 17] </ref>; the first of these references deals with networks that employ hard threshold activations, the second and third cover continuous activation functions of a type (piecewise polynomial) close to those used in this paper, and the last one provides results for networks employing the standard sigmoid activation function.
Reference: [11] <author> Jones, K.L., </author> <title> "A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training," </title> <journal> Annals of Statistics, </journal> <note> to appear. </note>
Reference-contexts: This type of research ignores the training question itself, asking instead what is the best one could do, in this sense of overall error, if the best possible network with a given architecture were to be eventually found. Some recent papers along these lines are <ref> [1, 11, 5] </ref>, which deal with single hidden layer nets, and [6], which dealt with multiple hidden layers.
Reference: [12] <author> Judd, J.S., </author> <title> "On the complexity of learning shallow neural networks," </title> <journal> J. of Complexity, </journal> <volume> 4(1988): </volume> <pages> 177-192. </pages>
Reference-contexts: Yet another direction in which to approach theoretical questions regarding learning by neural networks, and the one that concerns us here, originates with the work of Judd (see for instance <ref> [12, 13] </ref>, as well as the related work [3, 16, 28]). Judd, like us, was motivated by the observation that the "backpropagation" algorithm often runs very slowly, especially for high-dimensional data.
Reference: [13] <author> Judd, J.S., </author> <title> Neural Network Design and the Complexity of Learning, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Yet another direction in which to approach theoretical questions regarding learning by neural networks, and the one that concerns us here, originates with the work of Judd (see for instance <ref> [12, 13] </ref>, as well as the related work [3, 16, 28]). Judd, like us, was motivated by the observation that the "backpropagation" algorithm often runs very slowly, especially for high-dimensional data.
Reference: [14] <author> Kearns, M., Li, M., Pitt, L., and Valiant, L., </author> <title> "On the learnability of Boolean formulae," </title> <booktitle> Proc. of the 19th ACM Symp. Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference-contexts: of the architecture A), 4 (b) L outputs a hypothesis g 2 C n such that with probability at least 1 ffi the following conditions are satisfied: X D + (x) &lt; * x2g 1 (1) A class C of representations of an architecture A is said to be learnable <ref> [14] </ref> iff it is (*; ffi)-learnable for all * and ffi (where 0 &lt; *; ffi &lt; 1). <p> Proof of Corollary 3.1. The proof uses a similar technique to the one applied in the proof of theorem 9 of <ref> [14] </ref>. We assume that the functions computed by the 2 -node architecture are learnable and show that it implies an RP algorithm for solving a known NP-complete problem, that is, NP=RP.
Reference: [15] <author> Kilian, J. and Siegelmann, H. T., </author> <title> "Computability With The Classical Sigmoid," </title> <booktitle> Proc. of the 5th ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July </month> <year> 1993, </year> <pages> pp. 137-143. </pages>
Reference-contexts: Furthermore, in [25], they showed that if real numbers are allowed in the weights of these specific networks (rather than rational ones) the network is equivalent to a non-uniform version of Turing machines (i.e. Turing machine with advice) which is stronger than the common model. Kilian and Siegelmann <ref> [15] </ref> proved universality for the sigmoidal network and a large class of sigmoidal-type nets. They concluded that Turing-universality is a common property among recurrent nets (and not only for the specific case of the saturated linear function). A different power is demonstrated by the recurrent threshold nets.
Reference: [16] <author> Lin, J-H., and Vitter, J. S., </author> <title> "Complexity results on learning by neural networks," </title> <journal> Machine Learning, </journal> <volume> 6(1991): </volume> <pages> 211-230. </pages>
Reference-contexts: Yet another direction in which to approach theoretical questions regarding learning by neural networks, and the one that concerns us here, originates with the work of Judd (see for instance [12, 13], as well as the related work <ref> [3, 16, 28] </ref>). Judd, like us, was motivated by the observation that the "backpropagation" algorithm often runs very slowly, especially for high-dimensional data. Recall that this algorithm is used in order to find a network (that is, find the weights, assuming a fixed architecture) that reproduces the observed data. <p> are the threshold functions H H (x) = 0 if x 0 and the piecewise linear or "saturating" activation functions defined as 6 (x) = &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : Another model, called the 2-cascade architecture, was investigated by Lin and Vitter <ref> [16] </ref> (see fig. 2). A 2-cascade architecture consists of two processors N 1 and N 2 each of which computes a binary threshold function H. The output of the node N 1 in the hidden layer is provided to the input of the output node N 2 . <p> In another related paper, Lin and Vitter <ref> [16] </ref> proved a slightly stronger result by showing that the loading problem of 2-cascade threshold net with binary input is NP-complete. Howvever, when the input is analog, loading a 1-hidden layer network requires a polynomial time only in the size of the training set.
Reference: [17] <author> Macintyre, A., and Sontag, E. D., </author> <title> "Finiteness results for sigmoidal `neural' networks," </title> <booktitle> Proc. 25th Annual Symp. Theory Computing, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1993, </year> <pages> pp. 325-334. </pages>
Reference-contexts: Some recent references to such work, establishing sample complexity results, and hence "weak learnabil-ity" in the Valiant model, for neural nets, are the papers <ref> [2, 18, 10, 17] </ref>; the first of these references deals with networks that employ hard threshold activations, the second and third cover continuous activation functions of a type (piecewise polynomial) close to those used in this paper, and the last one provides results for networks employing the standard sigmoid activation function.
Reference: [18] <author> Maass, W., </author> <title> "Bounds for the computational power and learning complexity of analog neural nets," </title> <booktitle> Proc. of the 25th ACM Symp. Theory of Computing, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 335-344 </pages> . 
Reference-contexts: Some recent references to such work, establishing sample complexity results, and hence "weak learnabil-ity" in the Valiant model, for neural nets, are the papers <ref> [2, 18, 10, 17] </ref>; the first of these references deals with networks that employ hard threshold activations, the second and third cover continuous activation functions of a type (piecewise polynomial) close to those used in this paper, and the last one provides results for networks employing the standard sigmoid activation function. <p> We also distinguish the case of analog versus binary inputs, and observe that the problem is solvable in polynomial time for analog inputs using standard linear-programming techniques (see <ref> [18] </ref> for further positive results on PAC-learnability when the input dimension is a fixed constant and the activation functions are piecewise polynomials). <p> a polynomial algorithm to test each separating configuration of the hyperplanes to assure that the output of the network is indeed a linear threshold function of the hyperplanes.2 Remark 2.1 A k H-node network with analog inputs is also learnable; this follows as a consequence of a result proven in <ref> [18] </ref>. 3 The Loading Problem For the 2 -node Architecture One can generalize Theorem 1 and show that it is possible to load the 2 -node architecture with analog inputs in polynomial time.
Reference: [19] <author> Maass, W., Schnitger, G., and Sontag, E. D., </author> <title> "On the computational power of sigmoid versus boolean threshold circuits", </title> <booktitle> Proc. of the 32nd Annual Symp. on Foundations of Computer Science,1991, </booktitle> <pages> pp. 767-776. 23 </pages>
Reference-contexts: In fact, it is already known that there are functions which cannot be computed by threshold networks with one hidden layer and a constant number of nodes, but can be computed by threshold networks with two hidden layers and a constant number of nodes, see <ref> [19] </ref>. * Is there a characterization of the activation functions for which the loading problem is intractable? 6 Acknowledgments. The first author wishes to thank Piotr Berman and Vwani P. Roychowdhury for helpful discussions.
Reference: [20] <author> Megiddo, M., </author> <title> "On the complexity of polyhedral separability," </title> <journal> Discrete Computational Geometry, </journal> <volume> 3(1988): </volume> <pages> 325-337. </pages>
Reference-contexts: Howvever, when the input is analog, loading a 1-hidden layer network requires a polynomial time only in the size of the training set. This result is achieved by utilizing a result described by Megiddo <ref> [20] </ref>. Theorem 1 Let k &gt; 0 be an integer. It is possible to load any k H-node architecture in polynomial time if the input is analog. Before proving Theorem 1, we summarize the related result of Megiddo in [20] regarding polyhedral separability in fixed dimension and hyperplanes. <p> This result is achieved by utilizing a result described by Megiddo <ref> [20] </ref>. Theorem 1 Let k &gt; 0 be an integer. It is possible to load any k H-node architecture in polynomial time if the input is analog. Before proving Theorem 1, we summarize the related result of Megiddo in [20] regarding polyhedral separability in fixed dimension and hyperplanes. The following definition is due to Megiddo [20]: 7 Definition 2.1 k-Polyhedral Separability: Given two sets of points A and B in IR d , and an integer k, decide whether there exist k hyperplanes H j = fp : (x j <p> It is possible to load any k H-node architecture in polynomial time if the input is analog. Before proving Theorem 1, we summarize the related result of Megiddo in <ref> [20] </ref> regarding polyhedral separability in fixed dimension and hyperplanes. The following definition is due to Megiddo [20]: 7 Definition 2.1 k-Polyhedral Separability: Given two sets of points A and B in IR d , and an integer k, decide whether there exist k hyperplanes H j = fp : (x j ) T p = x 0 g; (x j 2 IR d ; x 0 2 <p> Every threshold neuron defines a hyperplane, and we ask if there exists a set of hyperplanes that separate the points in the d dimensional space which are labeled 0 + 0 from the points there which are separated as 0 0 . The following Lemma is from <ref> [20] </ref>. Lemma 2.2 [20] Let d; k be constants, and Z represents the integers numbers. M is a set of points in Z d which are labeled +/. <p> The following Lemma is from <ref> [20] </ref>. Lemma 2.2 [20] Let d; k be constants, and Z represents the integers numbers. M is a set of points in Z d which are labeled +/.
Reference: [21] <author> Muroga, S., </author> <title> Threshold Logic and its Applications, </title> <publisher> John Wiley & Sons Inc., </publisher> <year> 1971. </year>
Reference-contexts: Proof of theorem 3.1. First we observe that the problem is in NP as follows. The classifications of the labeled points produced by the 2 -node architecture (as discussed in section 3.1) are 3-polyhedrally separable. Hence, from the result of <ref> [21] </ref> one can restrict all the weights to have at most O (n log n) bits. Thus, a "guessed" solution can be verified in polynomial time. Next, we show that the problem is NP-complete. Consider an instance I = (S; C) of the (2,3)-SSP. <p> The 0 + 0 and 0 0 points are (r + 3)-polyhedrally separated by the output of the network in which the Boolean formula for the polyhedral separation is the formula for the NAND function. Hence, from the result of <ref> [21] </ref> we can restrict all the weights to have at most p (n + r) number of bits (where p (x) is some polynomial in x). Since r is a polynomial in n, any "guessed" solution may be verified in polynomial time. So, the problem is in NP.
Reference: [22] <author> Papadimitriou, C. H., Schaffer, A. A., and Yannakakis M., </author> <title> "On the Complexity of Local Search", </title> <booktitle> Proc. 22nd Annual Symp. Theory Computing, </booktitle> <year> 1990, </year> <pages> pp. 438-445. </pages>
Reference-contexts: They concluded that Turing-universality is a common property among recurrent nets (and not only for the specific case of the saturated linear function). A different power is demonstrated by the recurrent threshold nets. It was proved in <ref> [22] </ref> that the problem of determining whether a recurrent network with threshold units (that is, the number of states in the network is finite) has a stable configuration is P-hard. Bruck and Goodman [4] showed that a recurrent threshold network of polynomial size cannot solve NP-complete problems unless NP=co-NP.
Reference: [23] <author> Papadimitriou, C.H., and Steiglitz, K., </author> <title> Combinatorial Optimization: Algorithms and Complexity, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1982. </year>
Reference-contexts: These two forms of disjunction and implication are used interchangeably. In a manner similar to <ref> [23] </ref>, we create a directed graph G = (V; E), where where V = fd i ; d i j v i is a variableg, and E = f (l i ; l j ) j (i; j 2 f1; : : : ; ng); (l i 2 fd i ; <p> If there is still an unassigned variable, set it arbitrarily and return to step 2. Otherwise, halt. The above algorithm produces a satisfying assignment provided the following condition holds (see, for example, <ref> [23, pp. 377-378] </ref>): The instance of the 2-SAT problem has a solution if and only if there is no directed cycle in G which contains both the vertices d i and d i for some i.
Reference: [24] <author> Siegelmann H. T., and Sontag E. D., </author> <title> "On the computational power of neural nets", </title> <booktitle> Proc. 5th ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, </address> <month> July, </month> <year> 1992. </year>
Reference-contexts: This can be achieved via a linear programming technique. On the other-hand, loading recurrent networks (i.e. networks with feedback loops) is a hard problem. In <ref> [24] </ref>, Siegelmann and Sontag showed the existence of a finite size recurrent network made of a specific saturated linear neurons which is Turing universal. Thus, the loading problem is undecidable for such nets.
Reference: [25] <author> Siegelmann H. T. and Sontag, E. D., </author> <title> "Neural networks with Real Weights: Analog Computational Complexity," </title> <journal> TCS journal, </journal> <note> to appear. </note>
Reference-contexts: In [24], Siegelmann and Sontag showed the existence of a finite size recurrent network made of a specific saturated linear neurons which is Turing universal. Thus, the loading problem is undecidable for such nets. Furthermore, in <ref> [25] </ref>, they showed that if real numbers are allowed in the weights of these specific networks (rather than rational ones) the network is equivalent to a non-uniform version of Turing machines (i.e. Turing machine with advice) which is stronger than the common model.
Reference: [26] <author> Sontag, E.D., </author> <title> "Feedforward nets for interpolation and classification," </title> <journal> J. Comp. Syst. Sci., </journal> <volume> 45(1992): </volume> <pages> 20-48. </pages>
Reference-contexts: In any case, we address here the open problem exactly as posed by Blum and Rivest.) It turns out that a definite answer to the question posed by Blum and Rivest is not possible. It is shown in <ref> [26] </ref> that for certain activation functions , the problem can be solved in constant time, independently of the input size, and hence the question is not NP-complete. <p> The functions used in the construction in <ref> [26] </ref> are however extremely artificial and in no way likely to appear in practical implementations. Nonetheless, the mere existence of such examples means that the mathematical question is far from trivial. <p> The main open question, then, is to understand if "reasonable" activation functions lead to NP-completeness results similar to the ones in the work by Blum and Rivest or if they are closer to the other extreme, the purely mathematical construct in <ref> [26] </ref>. The most puzzling case is that of the standard sigmoid function, 1=(1 + e x ). For that case we do not know the answer yet, but we conjecture that NP-completeness will indeed hold.
Reference: [27] <author> Yao, X., </author> <title> "Finding Approximate Solutions to NP-hard Problems by Neural Networks is hard", </title> <journal> Information Processing Letters, </journal> <volume> 41(1992): </volume> <pages> 93-98. </pages>
Reference-contexts: Bruck and Goodman [4] showed that a recurrent threshold network of polynomial size cannot solve NP-complete problems unless NP=co-NP. The result was further extended by Yao <ref> [27] </ref> who showed that a polynomial size threshold recurrent network cannot solve NP-complete problems even approximately within a guaranteed performance ratio unless NP=co-NP. In the rest of this paper, we focus on feedforward nets only.

References-found: 27

