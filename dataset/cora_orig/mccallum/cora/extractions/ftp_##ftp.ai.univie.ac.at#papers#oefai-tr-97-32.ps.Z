URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-32.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+97-32
Root-URL: 
Email: fbernhard, stefang@ai.univie.ac.at  
Title: Discovering Compressive Partial Determinations in Mixed Numerical and Symbolic Domains  
Author: Bernhard Pfahringer and Stefan Kramer 
Address: Schottengasse 3 A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Partial determinations are an interesting form of dependency between attributes in a relation. They generalize functional dependencies by allowing exceptions. We modify a known MDL formula for evaluating such partial determinations to allow for its use in an admissible heuristic in exhaustive search. Furthermore we describe an efficient preprocessing-based approach for handling numerical attributes. An empirical investigation tries to evaluate the viability of the presented ideas.
Abstract-found: 1
Intro-found: 1
Reference: [ Bell et al., 1990 ] <author> Bell T.C., Cleary J.G., Witten I.H.: </author> <title> Text compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: This coding scheme is plausible as it resembles a special case of an adaptive compression scheme known from text encoding <ref> [ Bell et al., 1990 ] </ref> . 4 Extending Partial Determination to Handle Numerical Attributes Our approach to handling numerical attributes in this framework consists of a pre-processing step and a minor modification of the search procedure.
Reference: [ Flach, 1993 ] <author> Flach P.A.: </author> <title> Predicate Invention in Inductive Data Engineering, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.83-94, </address> <year> 1993. </year>
Reference: [ John et al. 94 ] <author> John G.H., Kohavi R., Pfleger K.: </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <editor> in Cohen W.W. & Hirsh H.(eds.), </editor> <booktitle> Proceedings of the 11th International Machine Learning Conference, </booktitle> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, pp.121-138, </address> <year> 1994. </year>
Reference-contexts: Previous work on PDs such as [ Russell, 1989 ] , [ Shen, 1991 ] , [ Schlimmer, 1993 ] , or work on functional dependencies [ Mannila & Raiha, 1994 ] does not deal with numerical attributes at all. Others (e.g. <ref> [ John et al. 94 ] </ref> ) simply have treated numerical attributes as symbolic attributes with a possibly vast number of distinct values.
Reference: [ Kerber 92 ] <author> Kerber R.: ChiMerge: </author> <title> Discretization of Numeric Attributes, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, pp.123-128, </address> <year> 1992. </year>
Reference-contexts: The maximum number of values is a user-defined parameter. The number of values for the i th attribute derived from each numerical attribute is 2 i . Interval boundaries are determined using a so-called class-blind discretization method, namely the equal-frequency method <ref> [ Kerber 92 ] </ref> . The selection of this method is somewhat ad-hoc, and it will need to be investigated in more detail in further research. At least, it seems to perform reasonably as suggested by the experimental results reported in section 5.
Reference: [ Kononenko 95 ] <author> Kononenko I.: </author> <title> On Biases in Estimating the Multi-Valued Attributes, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1034-1040, </address> <year> 1995. </year>
Reference-contexts: A prerequisite of this approach is that a standard order of examples, attributes, and values is assumed. The strings of values over a multi-valued alphabet are encoded using a function (5) proposed by <ref> [ Kononenko 95 ] </ref> . String is the bucket to be encoded, alphabet is the alphabet of all possible values for this RHS attribute, and f char i is the respective frequency of each such value in the current bucket.
Reference: [ Korf 85 ] <author> Korf R.E.: </author> <title> Depth-First Iterative-Deeping: An Optimal Admissible Tree Search, </title> <journal> Artificial Intelligence, </journal> <volume> 27(1), pp.97-110, </volume> <year> 1985. </year>
Reference-contexts: We have incorporated the MDL-measure as defined above into a depth-first iterative deepening (DFID) search <ref> [ Korf 85 ] </ref> .
Reference: [ Kramer & Pfahringer, 1996 ] <author> Kramer S., Pfahringer B.: </author> <title> Efficient Search for Strong Partial Determinations, </title> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: So, our goal was to incorporate numerical attributes in the compression-based framework for partial determinations that has been defined in [ Pfahringer & Kramer, 1995 ] and <ref> [ Kramer & Pfahringer, 1996 ] </ref> . In the next section we summarize the ideas from [ Pfahringer & Kramer, 1995 ] . Subsequently, we redefine and explain a new compression-based measure for partial determinations ranging over n-valued attributes. <p> Then we describe how numerical attributes can be integrated in our framework. In section 5, we report on experimental results of our method in several "real-world" databases. 2 Compression-Based Evaluation of Multi-Valued Partial Determinations In this section we summarize ideas from [ Pfahringer & Kramer, 1995 ] and <ref> [ Kramer & Pfahringer, 1996 ] </ref> : Partial determinations are expressions of the form X ! d Y , where X is the set of LHS (left-hand side) attributes, and Y is the set of RHS (right-hand side) attributes, and d is the determination factor d (X; Y ) [ Russell, <p> The values of Y need not be transmitted anymore. Thus we achieve some compression of the data, the degree of which is estimated by our measure based on the MDL principle. In <ref> [ Kramer & Pfahringer, 1996 ] </ref> , we generalized the definition of the MDL-measure given in [ Pfahringer & Kramer, 1995 ] to multi-valued attributes. <p> This is the basic idea underlying a further development of the compression-based measures that has been defined in [ Pfahringer & Kramer, 1995 ] and <ref> [ Kramer & Pfahringer, 1996 ] </ref> . number of nice properties, such as a more intuitive interpretation than its predecessors, and the possibility to derive bounds which can be utilized for pruning in otherwise exhaustive search. <p> The second term in our function is the coding cost of the values of the RHS attributes (4). In contrast to the coding scheme proposed in [ Pfahringer & Kramer, 1995 ] and <ref> [ Kramer & Pfahringer, 1996 ] </ref> , there is no distinct encoding of the mapping and the exceptions any longer. The basic idea is that instead just the RHS values of the tables (such as for instance the buckets for attribute C depicted in Figure 3) are transmitted. <p> Both domains are mixed numerical and symbolic in nature and the best PD only incorporates symbolic attributes, regardless of the level of discretization. 6 Related Work The presented work extends our own work from [ Pfahringer & Kramer, 1995 ] and <ref> [ Kramer & Pfahringer, 1996 ] </ref> in order to handle numerical attributes. This new approach casts partial determinations among symbolic and numerical attributes into a single compression-based framework. Furthermore, the pre-processing approach results in an efficient representation allowing even for exhaustive search in quite a few domains.
Reference: [ Mannila & Raiha, 1994 ] <author> Mannila H. and Raiha K.-J.: </author> <title> Algorithms for Inferring Functional Dependencies From Relations. </title> <booktitle> Data & Knowledge Engineering 12 (1994) 83-99, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction When analyzing databases, dependencies among variables are generally of great interest. Functional dependencies <ref> [ Mannila & Raiha, 1994 ] </ref> as known from database research are a fundamental form of dependencies, but they are assumed to have no exceptions, i.e., no examples (tuples) that contradict the assumption that the dependency is functional. <p> Smallest (i.e. best) lengths are in boldface. pendencies that can be discovered has been extended significantly. Previous work on PDs such as [ Russell, 1989 ] , [ Shen, 1991 ] , [ Schlimmer, 1993 ] , or work on functional dependencies <ref> [ Mannila & Raiha, 1994 ] </ref> does not deal with numerical attributes at all. Others (e.g. [ John et al. 94 ] ) simply have treated numerical attributes as symbolic attributes with a possibly vast number of distinct values.
Reference: [ Merz & Murphy 96 ] <author> Merz, C.J., Murphy, </author> <title> P.M.: UCI Repository of machine learning databases, </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: Unless these attributes are not already pre-discretized into a reasonably small number of distinct values (which probably is the case for at least a few of the UCI databases <ref> [ Merz & Murphy 96 ] </ref> ), such attributes will not provide sufficient compression of the data. 7 Discussion and Conclusion Finding compressive partial determinations for databases containing both symbolic and numerical attributes has not been tackled before. The solution described in this paper may appear to be quite ad-hoc.
Reference: [ Pfahringer & Kramer, 1995 ] <author> Pfahringer B., Kramer S.: </author> <title> Compression-Based Evaluation of Partial Determinations, </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: Dependencies having exceptions are called partial determinations (PDs) [ Russell, 1989 ] , or approximate functional dependencies. Partial determinations are important when noise or systematic error is likely to occur in a domain. In <ref> [ Pfahringer & Kramer, 1995 ] </ref> , we introduced the notion of compressive PDs, i.e., PDs that help compress a given database. <p> In our context, this is the evaluation of a partial determination based on the compression achievable by the transformation of the given database using it. So, our goal was to incorporate numerical attributes in the compression-based framework for partial determinations that has been defined in <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] . In the next section we summarize the ideas from [ Pfahringer & Kramer, 1995 ] . Subsequently, we redefine and explain a new compression-based measure for partial determinations ranging over n-valued attributes. <p> So, our goal was to incorporate numerical attributes in the compression-based framework for partial determinations that has been defined in <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] . In the next section we summarize the ideas from [ Pfahringer & Kramer, 1995 ] . Subsequently, we redefine and explain a new compression-based measure for partial determinations ranging over n-valued attributes. Then we describe how numerical attributes can be integrated in our framework. <p> Then we describe how numerical attributes can be integrated in our framework. In section 5, we report on experimental results of our method in several "real-world" databases. 2 Compression-Based Evaluation of Multi-Valued Partial Determinations In this section we summarize ideas from <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] : Partial determinations are expressions of the form X ! d Y , where X is the set of LHS (left-hand side) attributes, and Y is the set of RHS (right-hand side) attributes, and d is the determination <p> To avoid this, we also have to take into account how complex partial determinations are. Therefore, <ref> [ Pfahringer & Kramer, 1995 ] </ref> proposes a compression-based measure using the so-called Minimum Description Length (MDL) principle [ Rissanen, 1978 ] . <p> The theory with the minimal total message length (the most compressive partial determination) is also the most probable theory explaining the data [ Rissanen, 1978 ] . To illustrate the key idea of the measure for partial determinations proposed in <ref> [ Pfahringer & Kramer, 1995 ] </ref> , we consider the hypothetical task of transmitting a given database as efficiently as possible (see the partial determination instead of the raw data may improve efficiency considerably: we just have to transmit the mapping (the model) and transmit the information to correct the values <p> The values of Y need not be transmitted anymore. Thus we achieve some compression of the data, the degree of which is estimated by our measure based on the MDL principle. In [ Kramer & Pfahringer, 1996 ] , we generalized the definition of the MDL-measure given in <ref> [ Pfahringer & Kramer, 1995 ] </ref> to multi-valued attributes. <p> This is the basic idea underlying a further development of the compression-based measures that has been defined in <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] . number of nice properties, such as a more intuitive interpretation than its predecessors, and the possibility to derive bounds which can be utilized for pruning in otherwise exhaustive search. <p> The second term in our function is the coding cost of the values of the RHS attributes (4). In contrast to the coding scheme proposed in <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] , there is no distinct encoding of the mapping and the exceptions any longer. <p> Both domains are mixed numerical and symbolic in nature and the best PD only incorporates symbolic attributes, regardless of the level of discretization. 6 Related Work The presented work extends our own work from <ref> [ Pfahringer & Kramer, 1995 ] </ref> and [ Kramer & Pfahringer, 1996 ] in order to handle numerical attributes. This new approach casts partial determinations among symbolic and numerical attributes into a single compression-based framework.
Reference: [ Quinlan 94 ] <author> Quinlan J.R.: </author> <title> The Minimum Description Length Principle and Categorical Theories, </title> <booktitle> in Co-hen W.W. & Hirsh H.(eds.), Proceedings of the 11th International Machine Learning Conference, </booktitle> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, pp.233-241, </address> <year> 1994. </year>
Reference-contexts: The selection of the LHS attributes (3) is encoded by a coding scheme due to Cameron-Jones (as cited in <ref> [ Quinlan 94 ] </ref> ). It has the property that the more elements are selected from a set, the higher the coding cost of the selection will be. An example for the "behavior" of function is shown in Figure 5.
Reference: [ Rissanen, 1978 ] <author> J. Rissanen: </author> <title> Modeling by Shortest Data Description. In: </title> <journal> Automatica, </journal> <volume> 14, pp.465-471, </volume> <year> 1978. </year>
Reference-contexts: To avoid this, we also have to take into account how complex partial determinations are. Therefore, [ Pfahringer & Kramer, 1995 ] proposes a compression-based measure using the so-called Minimum Description Length (MDL) principle <ref> [ Rissanen, 1978 ] </ref> . <p> The theory with the minimal total message length (the most compressive partial determination) is also the most probable theory explaining the data <ref> [ Rissanen, 1978 ] </ref> .
Reference: [ Russell, 1989 ] <author> Russell S.J.: </author> <title> The Use of Knowledge in Analogy and Induction. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Functional dependencies [ Mannila & Raiha, 1994 ] as known from database research are a fundamental form of dependencies, but they are assumed to have no exceptions, i.e., no examples (tuples) that contradict the assumption that the dependency is functional. Dependencies having exceptions are called partial determinations (PDs) <ref> [ Russell, 1989 ] </ref> , or approximate functional dependencies. Partial determinations are important when noise or systematic error is likely to occur in a domain. In [ Pfahringer & Kramer, 1995 ] , we introduced the notion of compressive PDs, i.e., PDs that help compress a given database. <p> and [ Kramer & Pfahringer, 1996 ] : Partial determinations are expressions of the form X ! d Y , where X is the set of LHS (left-hand side) attributes, and Y is the set of RHS (right-hand side) attributes, and d is the determination factor d (X; Y ) <ref> [ Russell, 1989 ] </ref> , the probability that two randomly chosen tuples have the same values of Y , provided they have the same values of X. In the following, we will restrict ourselves to RHSs consisting of single attributes. <p> Smallest (i.e. best) lengths are in boldface. pendencies that can be discovered has been extended significantly. Previous work on PDs such as <ref> [ Russell, 1989 ] </ref> , [ Shen, 1991 ] , [ Schlimmer, 1993 ] , or work on functional dependencies [ Mannila & Raiha, 1994 ] does not deal with numerical attributes at all.
Reference: [ Schlimmer, 1993 ] <author> Schlimmer J.C.: </author> <title> Efficiently Inducing Determinations: A Complete and Systematic Search Algorithm that Uses Optimal Pruning. </title> <booktitle> Proceedings of the 10 th International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: Smallest (i.e. best) lengths are in boldface. pendencies that can be discovered has been extended significantly. Previous work on PDs such as [ Russell, 1989 ] , [ Shen, 1991 ] , <ref> [ Schlimmer, 1993 ] </ref> , or work on functional dependencies [ Mannila & Raiha, 1994 ] does not deal with numerical attributes at all. Others (e.g. [ John et al. 94 ] ) simply have treated numerical attributes as symbolic attributes with a possibly vast number of distinct values.
Reference: [ Shen, 1991 ] <author> Shen W.-M.: </author> <title> Discovering Regularities from Large Knowledge Bases. </title> <booktitle> Proceedings of the 8 th International Workshop on Machine Learning, </booktitle> <year> 1991. </year>
Reference-contexts: Smallest (i.e. best) lengths are in boldface. pendencies that can be discovered has been extended significantly. Previous work on PDs such as [ Russell, 1989 ] , <ref> [ Shen, 1991 ] </ref> , [ Schlimmer, 1993 ] , or work on functional dependencies [ Mannila & Raiha, 1994 ] does not deal with numerical attributes at all.
Reference: [ Zhang & Korf 95 ] <author> Zhang W., Korf R.E.: </author> <title> Performance of Linear-Space Search Algorithms, </title> <journal> Artificial Intelligence, </journal> <volume> 79(2), pp.241-292, </volume> <year> 1995. </year>
Reference-contexts: We have incorporated the MDL-measure as defined above into a depth-first iterative deepening (DFID) search [ Korf 85 ] . This is a exhaustive, but linear-space search algorithm, which furthermore is preferable to alternative linear-space algorithms where the solution is likely to be polynomial <ref> [ Zhang & Korf 95 ] </ref> , which certainly is the case in our setting, provided there are compressive PDs to be found at all.
References-found: 16

