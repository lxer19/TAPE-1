URL: http://www.ri.cmu.edu/afs/cs/project/nectar-adamb/ftp/ecoipps.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/project/nectar-adamb/ftp/
Root-URL: 
Email: Email: flowekamp,adambg@cs.cmu.edu  
Phone: Phone: 412-268-5295 Fax: 412-268-5576  
Title: ECO: Efficient Collective Operations for Communication on Heterogeneous Networks  
Author: Bruce B. Lowekamp and Adam Beguelin 
Date: September 19, 1995  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: PVM and other distributed computing systems have enabled the use of networks of workstations for parallel computation, but their approach of treating a network as a collection of point-to-point connections does not promote efficient communication| particularly collective communication. ECO is a package which solves this problem with programs which analyze the network and establish efficient communication patterns which are used by a library of collective operations. The analysis is done off-line, so that after paying the one-time cost of analyzing the network, the execution of application programs is not delayed. This paper gives performance results from using ECO to implement the collective communication in CHARMM, a widely used macromolecu-lar dynamics package. ECO facilitates the development of data parallel applications by providing a simple interface to routines which use the available heterogeneous networks efficiently. This approach gives a naive programmer the ability to use the available net works to their full potential without acquiring any knowledge of the network structure.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jose Nagib Cotrim Arabe, Adam Beguelin, Bruce Lowekamp, Erik Seligman, Michael Starkey, and Peter Stephan. Dome: </author> <title> Parallel programming in a heterogeneous multiuser environment. </title> <type> Technical Report CMU-CS-95-137, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Its design makes it possible to utilize more efficient communication techniques, such as those provided by MPP libraries, while maintaining the flexibility to run on an arbitrary collection of machines and networks. ECO is used for collective communication by Dome <ref> [1] </ref>, an object-oriented distributed programming environment under development. It has also been used to provide the collective communication required by CHARMM [?], a macromolecular dynamics program extensively used by chemists. The communication routines provided with CHARMM are highly optimized for a switched or high-speed network. <p> Using wider trees is important on bus-based networks, since it reduces the number of communications which are attempted in parallel, but it hurts the performance on a switched network which can handle the bandwidth. ECO has also been used to implement the collective communication for Dome <ref> [1] </ref>. A molecular dynamics program written in Dome has been run on a network of 20 machines, consisting of six DEC Alphas attached to two ethernets, five IBM Power PCs attached to an ethernet, two DEC alphas attached to switched FDDI, and seven SGI INDYs attached to switched ethernet.
Reference: [2] <author> Vasanth Bala, Jehoshua Bruck, Robert Cypher, Pablo Elustondo, Alex Ho, Ching-Tien Ho, Shlomo Kipnis, and Marc Snir. </author> <title> CCL: A portable and tunable collective communication library for scalable parallel computers. </title> <booktitle> In Proceedings of 8th International Parallel Processing Symposium, </booktitle> <pages> pages 835-844. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and discussions of other packages and approaches to collective communication [13]. Bala, et al. describe a collective communication library originally designed for the IBM SP1 <ref> [2] </ref>. They discuss performance tuning issues, as well as a detailed discussion of the semantics of collective communication and group membership, including the correctness of collective operations.
Reference: [3] <author> Mike Barnett, Satya Gupta, David G. Payne, Lance Shuler, Robert van de Geijn, and Jerrell Watts. </author> <title> Building a high-performance collective communication library. </title> <booktitle> In Proceedings of IEEE Scalable High Performance Computing, </booktitle> <pages> pages 835-834. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: The MPI standard [6] acknowledges the importance of collective communication by including it as a chapter in the specification. Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures <ref> [3] </ref>, and discussions of other packages and approaches to collective communication [13]. Bala, et al. describe a collective communication library originally designed for the IBM SP1 [2].
Reference: [4] <author> K. Efe. </author> <title> Heuristic models of task assignment scheduling in distributed systems. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 897-916, </pages> <year> 1982. </year>
Reference-contexts: In their approach, full load balancing information and communication occurs within subnets, while communication between sub-networks is more carefully controlled. Also related to this subject is the work of Efe on grouping related tasks together in a subnet for a deterministic task system <ref> [4] </ref>. Both of these systems share ECO's principle of limiting communication between subnets. However, a major difference between these systems and ECO is that they attempt to avoid global 4 DRAFT communication whereas ECO tries to optimize it.
Reference: [5] <author> D.J. Evans and W.U.N. Butt. </author> <title> Load balancing with network partitioning using host groups. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 325-345, </pages> <year> 1994. </year>
Reference-contexts: Of particular interest to ECO development is research done on grouping hosts on the basis of network topology. This technique has been used in two areas. Evans and Butt make use of this technique to facilitate load balancing <ref> [5] </ref>. In their approach, full load balancing information and communication occurs within subnets, while communication between sub-networks is more carefully controlled. Also related to this subject is the work of Efe on grouping related tasks together in a subnet for a deterministic task system [4].
Reference: [6] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year> <note> http://www.mcs.anl.gov/mpi/index.html. 20 DRAFT </note>
Reference-contexts: Optimization of both types of communication should be addressed by a complete communications package. Collective operations have long been a component of vendor supplied communication libraries for MPPs, and the supplied routines are optimized for performance on that vendor's hardware. PVM briefly addresses these issues and MPI <ref> [8, 6] </ref> provides a more complete set of collective communication routines, however neither PVM nor MPI's specification address the issues of optimizing the performance of collective communication on heterogeneous networks. <p> These routines provide the same functionality as the collective communication suite in the MPI standard <ref> [6] </ref>. Efficient nearest neighbor communication is provided by routines that map common communication topologies to the network topology. ECO requires no user input to determine the characteristics of the network and has almost no application run-time overhead. <p> Developers of applications in PVM have been forced to design their own collective communication support [15]. While coding this directly forces the developers to make decisions specific to 3 DRAFT their network, it introduces excessive difficulties. There has already been substantial work on collective communication packages. The MPI standard <ref> [6] </ref> acknowledges the importance of collective communication by including it as a chapter in the specification. Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and discussions of other packages and approaches to collective communication [13]. <p> Many developers have expressed a desire for a basic set of topologies, such as ring and mesh topologies. The MPI standard dedicates a chapter to the discussion of a mechanism for describing arbitrary topology needs to the message passing system <ref> [6] </ref>. Of particular interest to ECO development is research done on grouping hosts on the basis of network topology. This technique has been used in two areas. Evans and Butt make use of this technique to facilitate load balancing [5]. <p> The broadcast tree generated by ECO is shown in Figure 3d. 5 Collective Operations Section 4 introduced the techniques that ECO uses with the simple example of a broadcast. As mentioned in Section 1, ECO provides the same functionality as the MPI collective communication standard <ref> [6] </ref>. Table 2 lists the operations supported by ECO. The same communication pattern described in Section 4 is used for all operations, with appropriate modifications. For operations involving a single receiver, that receiver is always the root of the tree.
Reference: [7] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine | A Users' Guide and Tutorial for Net-worked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The availability of networks of high-performance workstations and software packages such as PVM <ref> [7] </ref> has made networks of workstations (NOWs) a legitimate alternative to tradi tional high-performance machines such as supercomputers and massively parallel processors fl Partially supported by an NSF Graduate Research Fellowship y Joint appointment with Pittsburgh Supercomputing Center 1 DRAFT (MPPs).
Reference: [8] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Optimization of both types of communication should be addressed by a complete communications package. Collective operations have long been a component of vendor supplied communication libraries for MPPs, and the supplied routines are optimized for performance on that vendor's hardware. PVM briefly addresses these issues and MPI <ref> [8, 6] </ref> provides a more complete set of collective communication routines, however neither PVM nor MPI's specification address the issues of optimizing the performance of collective communication on heterogeneous networks.
Reference: [9] <author> Ken Hardwick. </author> <title> HIPPI world|the switch is the network. </title> <booktitle> In COMPCON Spring 1992, </booktitle> <pages> pages 234-238. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet [14], FDDI, ATM [16], and HIPPI <ref> [9] </ref> networks has begun to reduce this limitation, however few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of what network bandwidth is available.
Reference: [10] <author> Chengchang Huang and Philip K. McKinley. </author> <title> Design and implementation of global reduction operations across atm networks. </title> <booktitle> In Proceedings of 3rd IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 43-50. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks [12], wormhole routed MPPs [11], and ATM networks <ref> [10] </ref>. Another feature missing from PVM which application developers must provide [15] is the notion of topology among tasks. Many developers have expressed a desire for a basic set of topologies, such as ring and mesh topologies.
Reference: [11] <author> Philip K. McKinley, Yih jia Tsai, and David F. Robinson. </author> <title> A survey of collective communication in wormhole-routed massively parallel computers. </title> <type> Technical Report MSU-CPS-94-35, </type> <institution> Michigan State University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks [12], wormhole routed MPPs <ref> [11] </ref>, and ATM networks [10]. Another feature missing from PVM which application developers must provide [15] is the notion of topology among tasks. Many developers have expressed a desire for a basic set of topologies, such as ring and mesh topologies.
Reference: [12] <author> Philip K. McKinley and Jane W. S. Liu. </author> <title> Multicast tree construction in bus-based networks. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 29-41, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Considerable work has been done on collective operations, and multicast communication in general, which can be used as the underpinnings for collective communication, on a variety of specific physical networks. McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks <ref> [12] </ref>, wormhole routed MPPs [11], and ATM networks [10]. Another feature missing from PVM which application developers must provide [15] is the notion of topology among tasks. Many developers have expressed a desire for a basic set of topologies, such as ring and mesh topologies.
Reference: [13] <author> Prasenjit Mitra, David G. Payne, Lance Shuler, Robert van de Geijn, and Jerrell Watts. </author> <title> Fast collective communication libraries, please. </title> <type> Technical Report TR-95-22, </type> <institution> The University of Texas, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The inclusion of efficient implementations of core collective operations is crucial for achieving maximum performance of applications on message-passing systems <ref> [13] </ref>. PVM currently lacks a powerful implementation of collective communication. Moreover, such an implementation presents difficulties because of the portable, heterogeneous nature of PVM. <p> Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and discussions of other packages and approaches to collective communication <ref> [13] </ref>. Bala, et al. describe a collective communication library originally designed for the IBM SP1 [2]. They discuss performance tuning issues, as well as a detailed discussion of the semantics of collective communication and group membership, including the correctness of collective operations.
Reference: [14] <author> R.A. Quinnell. </author> <title> Emerging 100-Mbit ethernet standards ease system bottlenecks. EDN (European edition), </title> <type> 39(1) </type> <institution> 35-36,40, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet <ref> [14] </ref>, FDDI, ATM [16], and HIPPI [9] networks has begun to reduce this limitation, however few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of what network bandwidth is available.
Reference: [15] <author> Andreas Stathopoulos, Anders Ynnerman, and Charlotte F. Fischer. </author> <title> A PVM implementation of the MCHF atomic structure package. to appear in the International Journal of Supercomputer Applications and High Performance Computing, </title> <note> 1995. http://www.vuse.vanderbilt.edu/ andreas/publications/jsa.ps. 21 DRAFT </note>
Reference-contexts: This does not, however, reduce the need for such a facility in PVM. Developers of applications in PVM have been forced to design their own collective communication support <ref> [15] </ref>. While coding this directly forces the developers to make decisions specific to 3 DRAFT their network, it introduces excessive difficulties. There has already been substantial work on collective communication packages. The MPI standard [6] acknowledges the importance of collective communication by including it as a chapter in the specification. <p> McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks [12], wormhole routed MPPs [11], and ATM networks [10]. Another feature missing from PVM which application developers must provide <ref> [15] </ref> is the notion of topology among tasks. Many developers have expressed a desire for a basic set of topologies, such as ring and mesh topologies. The MPI standard dedicates a chapter to the discussion of a mechanism for describing arbitrary topology needs to the message passing system [6].
Reference: [16] <author> Ronald J. Vetter. </author> <title> ATM concepts, architectures, and protocols. </title> <journal> Communications of the ACM, </journal> <volume> 38(2) </volume> <pages> 30-38, </pages> <month> February </month> <year> 1995. </year> <note> 22 DRAFT </note>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet [14], FDDI, ATM <ref> [16] </ref>, and HIPPI [9] networks has begun to reduce this limitation, however few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of what network bandwidth is available.
References-found: 16

