URL: http://www.cs.ucsd.edu/~calder/papers/PLDI-97-Coloring.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/papers.html
Root-URL: http://www.cs.ucsd.edu
Email: fahashemi,kaelig@ece.neu.edu calder@cs.ucsd.edu  
Title: Efficient Procedure Mapping Using Cache Line Coloring  
Author: Amir H. Hashemi David R. Kaeli Brad Calder 
Address: Boston, MA La Jolla, CA  
Affiliation: Dept. of Electrical and Computer Engineering Dept. of Computer Science and Engineering Northeastern University University of California, San Diego  
Date: June, 1997  
Note: To appear at the ACM SIGPLAN Conference on Programming Language Design and Implementation,  
Abstract: As the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory effectively. Both hardware and software approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques. In this paper we present a link-time procedure mapping algorithm which can significantly improve the effectiveness of the instruction cache. Our algorithm produces an improved program layout by performing a color mapping of procedures to cache lines, taking into consideration the procedure size, cache size, cache line size, and call graph. We use cache line coloring to guide the procedure mapping, indicating which cache lines to avoid when placing a procedure in the program layout. Our algorithm reduces on average the instruction cache miss rate by 40% over the original mapping and by 17% over the mapping algorithm of Pettis and Hansen [12]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Ball and J. Larus. </author> <title> Efficient path profiling. </title> <booktitle> In 29th International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Some of this information can be obtained by using full path profiling, which would allow one to know the frequency of each path <ref> [1, 22] </ref>, although full path profiling still does not provide optimal temporal locality information. One way to obtain additional information on temporal locality is to store the full trace of a program.
Reference: [2] <author> L. Belady. </author> <title> A study of replacement algorithms for a virtual-storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: Reducing the cache miss rate is one factor for improving memory access performance. Cache misses occur for a number of reasons: cold start, capacity, and collisions [13]. A number of cache line replacement algorithms have been proposed to reduce the number of cache collisions <ref> [2, 14, 19] </ref>. Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3].
Reference: [3] <author> B.N. Bershad, D. Lee, T.H, Romer, and J.B. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> In Six International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-170, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches <ref> [3] </ref>. An alternative approach is to perform code repositioning at compile or link-time [4, 9, 11, 12, 16, 20].
Reference: [4] <author> B. Calder and D. Grunwald. </author> <title> Reducing branch costs via branch alignment. </title> <booktitle> In Six International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 242-251. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program.
Reference: [5] <author> B. Calder, D. Grunwald, and A. Srivastava. </author> <title> The predictability of branches in libraries. </title> <booktitle> In 28th International Symposium on Microarchitecture, </booktitle> <pages> pages 24-34, </pages> <address> Ann Arbor, MI, </address> <month> November </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: An important issue involving profiled-based optimizations is how well does a single input capture the typical behavior of future runs of the program. Several researchers have investigated this problem and have found that programs have predictable behavior between different inputs <ref> [5, 8, 21] </ref>. Even so, care must be taken when choosing the inputs to guide optimizations. In this vein, we took the optimized programs used to produce the results in Table 2 and ran them using different inputs.
Reference: [6] <author> B. Calder, D. Grunwald, and B. Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4), </volume> <year> 1994. </year>
Reference-contexts: Studies of program behaviors show that 10% to 30% of a program accounts for 90% of its execution time <ref> [6] </ref>. The rest of the code is rarely visited or not visited at all. Our algorithm takes advantage of this property by dividing each program into frequently visited (popular) and infrequently visited (unpopular) procedures. <p> Our algorithm can be combined and benefit from other code reordering techniques such as basic block reordering, taking into consideration looping structures, and procedure splitting. These are topics of future research, along with applying our algorithm to object oriented languages <ref> [6, 17] </ref>. In this paper we also concentrated on the performance achieved using call edge profiles to guide the optimizations in order to eliminate first-generation cache conflicts.
Reference: [7] <author> P.J. Denning and S. C. Schwartz. </author> <title> Properties of the working-set model. </title> <journal> Communications of the ACM, </journal> <volume> 15(3) </volume> <pages> 191-198, </pages> <month> March </month> <year> 1972. </year>
Reference-contexts: 1 Introduction The increasing gap between processor and main memory speeds has forced computer designers to exploit cache memories. A cache is smaller than the main memory and, if properly managed, can hold a major part of the working set of a program <ref> [7] </ref>. The goal of memory subsystem designers is to improve the average memory access time. Reducing the cache miss rate is one factor for improving memory access performance. Cache misses occur for a number of reasons: cold start, capacity, and collisions [13].
Reference: [8] <author> J. A. Fisher and S. M. Freudenberger. </author> <title> Predicting conditional branch directions from previous runs of a program. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 85-95, </pages> <address> Boston, Mass., </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: An important issue involving profiled-based optimizations is how well does a single input capture the typical behavior of future runs of the program. Several researchers have investigated this problem and have found that programs have predictable behavior between different inputs <ref> [5, 8, 21] </ref>. Even so, care must be taken when choosing the inputs to guide optimizations. In this vein, we took the optimized programs used to produce the results in Table 2 and ran them using different inputs.
Reference: [9] <author> W.W. Hwu and P.P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> In 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 242-251. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program. <p> For application code, our coloring algorithm offers better performance over a recursive threshold partitioning algorithm since we take into consideration the connectivity of the graph. 2.2.2 Procedure Mapping Hwu and Chang described an algorithm for improving instruction cache performance using inlining, basic block reordering, and procedure reordering compiler optimizations <ref> [9] </ref>. Their algorithm builds a call graph with weighted call edges produced by profiling. For the procedure reordering, their algorithm processes the call graph depth first, mapping the procedures to the address space in depth first order. <p> up to the associativity of the cache. 5.2 Color Mapping with Basic Block Re ordering and Procedure Splitting The results in x4 do not show the full potential of our coloring algorithm, since our algorithm can benefit from other code reordering techniques such as basic block reordering and procedure splitting <ref> [9, 12] </ref>.
Reference: [10] <author> D. Kaeli. </author> <title> Issues in Trace-Driven Simulation. </title> <booktitle> Lecture Notes in Computer Science No. 729, Performance Evaluation of Computer and Communication Systems,L. </booktitle> <editor> Do-natiello and R. Nelson eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1993, </year> <pages> pp. 224-244., </pages> <year> 1990. </year>
Reference-contexts: Therefore, the programs we examined are from the SPECInt95 suite, SPECInt92 suite, and three gnu applications. We used trace driven simulation to quantify the instruction cache performance of our algorithm <ref> [10] </ref>. The trace driven simulations were obtained using ATOM, an execution-driven simulation tool available from Digital Equipment Corporation [18]. ATOM allows instrumentation of binaries on DEC Alpha processors and can produce the necessary information about the frequency of procedure calls, procedure sizes, and the program's control flow graph.
Reference: [11] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program. <p> We now discuss relevant previous work and how it relates to our algorithm. 2.2.1 Knowledge of Cache Size McFarling examined improving instruction cache performance by not caching infrequently used instructions and by performing code reordering compiler optimizations <ref> [11] </ref>. His mapping algorithm works at the basic block level and concentrates on laying out the code based on loop structures in the program. The algorithm constructs a control flow graph with basic block, procedure, and loop nodes. <p> It does not model the temporal locality between a procedure and all of the procedures that it can possibly reach in the call graph, and any of these reachable procedures can cause cache conflicts. This emphasizes the fact that finding an optimal mapping to minimize conflicts is NP-complete <ref> [11] </ref>. <p> For future work, we will use control flow analysis of the program's structure to indicate if all the calls from C ! D were done during one invocation of C or whether they were spread out over several invocations, similar to the control flow analysis used by McFarling <ref> [11] </ref>. We will also use control flow analysis to determine how much of procedure C can actually overlap with procedure D for each procedure call, so we only have to include those cache lines in D's unavailable-set of colors.
Reference: [12] <author> K. Pettis and R.C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM, ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program. <p> Code reordering algorithms for improved memory performance can span several different levels of granularity, from basic blocks, to loops, and to procedures. Research has shown that basic block reordering and procedure reordering can significantly improve a program's execution performance. Pettis and Hansen <ref> [12] </ref> found that the reduction in execution time when using procedure reordering was around 8%, and the reduction in execution time for basic block reordering was around 12% on an HP-UX 825 architecture with a 16K direct mapped unified cache. <p> This is seen in Figure 1 where their algorithm processes the edge C ! D before the edge E ! C. This can create significant first-generation cache conflicts in the call graph, as seen by the conflict between procedures E and C in Figure 3. Pettis and Hansen <ref> [12] </ref> also described a number of techniques for improving code layout that include: basic block reordering, procedure splitting, and procedure reordering. Their algorithm employs a closest-is-best strategy to perform procedure reordering. The reordering starts with the heaviest executed call edge in the program call graph. <p> The remaining edges entering and exiting the chain node are coalesced. This process continues until the whole call graph is merged into chains which can no longer be merged. Figure 4 shows the key points of the Pettis and Hansen <ref> [12] </ref> procedure mapping algorithm when processing the call graph in Figure 1. Their algorithm starts by processing edge E ! C, merging nodes E and C into a chain EC. This is followed by edge A ! B, where A and B are merged into a chain A B. <p> up to the associativity of the cache. 5.2 Color Mapping with Basic Block Re ordering and Procedure Splitting The results in x4 do not show the full potential of our coloring algorithm, since our algorithm can benefit from other code reordering techniques such as basic block reordering and procedure splitting <ref> [9, 12] </ref>. <p> Our results show that we were able to reduce the cache miss rate on average by 40% over the original procedure mapping. In comparison to prior work, our algorithm reduced the cache miss rate on average 17% below that of the Pettis and Hansen algorithm <ref> [12] </ref>. In this study we concentrated on applying our color mapping algorithm to procedure reordering. Our algorithm can be combined and benefit from other code reordering techniques such as basic block reordering, taking into consideration looping structures, and procedure splitting.
Reference: [13] <author> S.A. Przybylski. </author> <title> Cache Design: A Performance-Directed Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The goal of memory subsystem designers is to improve the average memory access time. Reducing the cache miss rate is one factor for improving memory access performance. Cache misses occur for a number of reasons: cold start, capacity, and collisions <ref> [13] </ref>. A number of cache line replacement algorithms have been proposed to reduce the number of cache collisions [2, 14, 19]. Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space.
Reference: [14] <author> T.R. Puzak. </author> <title> Analysis of cache replacement-algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA, </address> <year> 1985. </year>
Reference-contexts: Reducing the cache miss rate is one factor for improving memory access performance. Cache misses occur for a number of reasons: cold start, capacity, and collisions [13]. A number of cache line replacement algorithms have been proposed to reduce the number of cache collisions <ref> [2, 14, 19] </ref>. Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3].
Reference: [15] <author> R.W. Quong. </author> <title> Expected I-cache miss rates via the gap model. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 372-383, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Capturing, storing, and processing a full trace can be very time and space consuming, but efficient techniques have been proposed to capture and process this information in a compact form, such as the gap model proposed by Quong <ref> [15] </ref>. We plan on investigating the use of full path profiling and the gap model with our color mapping algorithm in order to eliminate additional cache conflicts for deeper paths in the call graph. 6 Conclusions The performance of the cache-based memory system is critical in today's processors.
Reference: [16] <author> A.D. </author> <title> Samples and P.N. Hilfinger. Code reorganization for instruction caches. </title> <note> Techical Report UCB/CSD 88/447, Oc-tober 1988. </note>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program.
Reference: [17] <author> A. Sampoga. </author> <title> Architectural Implications of C and C++ Programming Models. </title> <type> MS Thesis, </type> <institution> Northeastern University, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: Our algorithm can be combined and benefit from other code reordering techniques such as basic block reordering, taking into consideration looping structures, and procedure splitting. These are topics of future research, along with applying our algorithm to object oriented languages <ref> [6, 17] </ref>. In this paper we also concentrated on the performance achieved using call edge profiles to guide the optimizations in order to eliminate first-generation cache conflicts.
Reference: [18] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: Therefore, the programs we examined are from the SPECInt95 suite, SPECInt92 suite, and three gnu applications. We used trace driven simulation to quantify the instruction cache performance of our algorithm [10]. The trace driven simulations were obtained using ATOM, an execution-driven simulation tool available from Digital Equipment Corporation <ref> [18] </ref>. ATOM allows instrumentation of binaries on DEC Alpha processors and can produce the necessary information about the frequency of procedure calls, procedure sizes, and the program's control flow graph.
Reference: [19] <author> J.G. Thompson. </author> <title> Efficient analysis of caching systems. </title> <type> Ph.D. Dissertation, </type> <institution> University of California, Berkeley, </institution> <year> 1987. </year>
Reference-contexts: Reducing the cache miss rate is one factor for improving memory access performance. Cache misses occur for a number of reasons: cold start, capacity, and collisions [13]. A number of cache line replacement algorithms have been proposed to reduce the number of cache collisions <ref> [2, 14, 19] </ref>. Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3].
Reference: [20] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing instruction cache performance for operating system intensive work-loads. </title> <booktitle> In Proceedings of the First International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Instead of concentrating on cache organization we concentrate on the layout of a program on the memory space. Bershad et.al. suggested remapping cache addresses dynamically to avoid conflict misses in large direct-mapped caches [3]. An alternative approach is to perform code repositioning at compile or link-time <ref> [4, 9, 11, 12, 16, 20] </ref>. The idea is to place frequently used sections of a program next to each other in the address space, thereby reducing the chances of cache conflicts while increasing spatial locality within the program. <p> This allows our algorithm to effectively eliminate first-generation cache conflicts, even when the popular subgraph size is larger than the instruction cache, by using the color mapping and the unavailable-set of colors. Torrellas, Xia and Daigle <ref> [20] </ref> (TXD) also described an algorithm for code layout for operating system intensive workloads. Their work takes into consideration the size of the cache and the popularity of code. Their algorithm partitions the operating system code into executed and non-executed parts at the basic block level.
Reference: [21] <author> D.W. Wall. </author> <title> Predicting program behavior using real or estimated profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 59-70, </pages> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: An important issue involving profiled-based optimizations is how well does a single input capture the typical behavior of future runs of the program. Several researchers have investigated this problem and have found that programs have predictable behavior between different inputs <ref> [5, 8, 21] </ref>. Even so, care must be taken when choosing the inputs to guide optimizations. In this vein, we took the optimized programs used to produce the results in Table 2 and ran them using different inputs.
Reference: [22] <author> Cliff Young and Michael D. Smith. </author> <title> Improving the accuracy of static branch prediction using branch correlation. </title> <booktitle> In Six International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 232-241, </pages> <month> October </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: Some of this information can be obtained by using full path profiling, which would allow one to know the frequency of each path <ref> [1, 22] </ref>, although full path profiling still does not provide optimal temporal locality information. One way to obtain additional information on temporal locality is to store the full trace of a program.
References-found: 22

