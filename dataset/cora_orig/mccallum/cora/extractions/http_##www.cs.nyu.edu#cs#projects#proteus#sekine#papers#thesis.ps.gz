URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/thesis.ps.gz
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Title: Corpus-based Parsing and Sublanguage Studies  
Author: by Satoshi Sekine 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Dissertation Advisor  
Date: May 1998  
Address: New York University  
Affiliation: Computer Science Department  
Abstract-found: 0
Intro-found: 0
Reference: [Biber 93] <author> Douglas Biber: </author> <title> "Using Register-Diversified Corpora for General Language Studies" Journal of Computer Linguistics Vol.19, </title> <type> Num 2, </type> <institution> pp219-241. </institution> <year> (1993) </year>
Reference-contexts: Most of them claim that the notion is important in processing natural language text, owning to lexical, syntactic or semantic restrictions, etc. A number of these studies have analyzed actual texts to try to verify the claim [Kittredge 82] [Grishman and Kittredge 86] [Slocum 86] <ref> [Biber 93] </ref>. Some of these studies and the definition of `sublanguage' will be discussed in Section 3.2. Several successful natural language processing systems have explicitly or 112 implicitly addressed the sublanguage restrictions. <p> The brochure texts exhibited more syntactic phenomena than manual texts, although manual texts were not simply a subset of the other. The experiments suggest the possibilities of sublanguage identification and further the possibilities of sublanguage specific processing in order to maximize the performance of systems. Biber <ref> [Biber 93] </ref> referred to the sublanguages as `registers'. First, he compared the part-of-speech tags of grammatically ambiguous words between two different registers: fiction and exposition. Some examples are shown in Table 122 3.1.
Reference: [Black et al. 91] <author> E.Black, S.Abney, D Flickenger, C.Gdaniec, R.Grishman, P.Harrison, D.Hindle, R.Ingria, F.Jelinek, J.Klavans, M.Liberman, M.Marcus, S.Roukos, B.Santorini, T.Strzalkowski: </author> <title> "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars" Proceedings of the 4th DARPA Speech and Natural Language Workshop (1991) </title>
Reference-contexts: It is natural that the number of sentences which exhaust memory is larger for G-0 than for G-2, because of the larger number of rules. Next, the evaluation using Parseval method on parsed sentences is shown in Table 2.6. "Parseval" is the measurement of parsed result proposed by <ref> [Black et al. 91] </ref>. The result in the table is the result achieved by the G-0 grammar, supplemented by the result using the G-2 grammar, if the larger grammar 42 can't generate a parse tree. <p> The grammars are trained on the PennTreeBank sections 02 - 21, and tested on section 23. The accuracy measure is slightly different from the accuracy measure used in the previous section. In the previous section, the Parseval method <ref> [Black et al. 91] </ref> was used. In that measure, multiple layers of unary trees are collapsed to a single layer of unary tree. Also, the labeling is not considered in the matching 5 .
Reference: [Black et al. 93] <author> Ezra Black, Fred Jelinek, John Lafferty, David Magerman, Robert Mercer and Salim Roukos: </author> <title> "Towards History-Based Grammars: </title> <booktitle> Using Richer Models for Probabilistic Parsing" Proceedings of the 31st 258 Annual Meeting of the Association for Computational Linguistics (ACL) (1993) </booktitle>
Reference-contexts: Furthermore, experiments 7 over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar [Fujisaki 84] [Garside and Leech 85] [Chitrao and Grishman 90] [Magerman and Weir 92] <ref> [Black et al. 93] </ref> [Bod 93] [Magerman 95] [Collins 96]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> There have been several attempts to build context-dependent grammars based on large corpora [Chitrao and Grishman 90] [Simmons and Yu 91] [Magerman and Weir 92] [Schabes and Waters 93] <ref> [Black et al. 93] </ref> [Bod 93] [Magerman 95]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. Some of these will be explained in the next section. <p> In other words, these are completely supervised methods. Some of them use the Wall Street Journal text of the PennTreeBank which will also be used in the experiments of this thesis. A group at IBM <ref> [Black et al. 93] </ref> built a parser which used context infor 17 mation, including the dominating production and the syntactic and semantic categories of words in the prior derivation.
Reference: [Black 93] <editor> Ezra Black "Parsing English By Computer: </editor> <booktitle> The State Of the Art" Proceedings of the ATR International Workshop on Speech Translation (1993) </booktitle>
Reference-contexts: Note that they used lexical information, while the parser described in this section uses almost no lexical information. Compared to so-called traditional, or hand-made grammars, roughly speaking, the performance is similar or better. For example, Black <ref> [Black 93] </ref> cited the best non-crossing 43 score using traditional grammars as 41% and the average of several systems as 22%. 2.5 Five Non-terminal Grammar In the framework of the parser explained in the previous section, the two non-terminals were selected based on intuition.
Reference: [Bod 92] <institution> Rens Bod "Data Oriented Parsing (DOP)" Proceedings of the 14th Conference on Computational Linguistics (COLING) (1992) </institution>
Reference-contexts: Its precision and recall for the sentences shorter than 40 words were 86.3% and 85.8%, respectively. This was the first parser to approach the current state-of-the-art performance on the task of parsing Wall Street Journal sentences. 21 2.2.6 Data Oriented Parsing Bod <ref> [Bod 92] </ref> [Bod 95] [Bod and Scha 96] explored the idea of Data Oriented Parsing. In this framework, all possible tree fragments in a hand annotated corpus are regarded as rules of a probabilistic grammar.
Reference: [Bod 93] <institution> Rens Bod "Using an Annotated Corpus as a Stochastic Grammar" Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics (EACL) (1993) </institution>
Reference-contexts: Furthermore, experiments 7 over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar [Fujisaki 84] [Garside and Leech 85] [Chitrao and Grishman 90] [Magerman and Weir 92] [Black et al. 93] <ref> [Bod 93] </ref> [Magerman 95] [Collins 96]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> There have been several attempts to build context-dependent grammars based on large corpora [Chitrao and Grishman 90] [Simmons and Yu 91] [Magerman and Weir 92] [Schabes and Waters 93] [Black et al. 93] <ref> [Bod 93] </ref> [Magerman 95]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. Some of these will be explained in the next section.
Reference: [Bod 95] <editor> Rens Bod "Enriching Linguistics with Statistics: </editor> <booktitle> Performance Models of Natural Language" Doctoral thesis, </booktitle> <publisher> Universiteit van Amsterdam (1995) </publisher>
Reference-contexts: Its precision and recall for the sentences shorter than 40 words were 86.3% and 85.8%, respectively. This was the first parser to approach the current state-of-the-art performance on the task of parsing Wall Street Journal sentences. 21 2.2.6 Data Oriented Parsing Bod [Bod 92] <ref> [Bod 95] </ref> [Bod and Scha 96] explored the idea of Data Oriented Parsing. In this framework, all possible tree fragments in a hand annotated corpus are regarded as rules of a probabilistic grammar.
Reference: [Bod and Scha 96] <institution> Rens Bod and Remko Scha "Data-Oriented Language Processing: </institution> <note> An Overview" ftp://ftp.fwi.uva.nl/pub/theory/illc/researchreports/LP-96-13.text.ps.gz (1996) 259 </note>
Reference-contexts: The parser and the evaluation program are available as public domain software from [Sekine 96a] and [Sekine and Collins 97]. Also, the work was referenced in a book [Young and Bloothooft 97] and in a review <ref> [Bod and Scha 96] </ref>. Chapter 3 describes a study of the sublanguage notion based on corpora. First, statistical measures are used to find sublanguage from newspaper articles. Then multi-domain corpora are used for a study of sublanguage features in terms of lexical and syntactic characteristics. <p> Its precision and recall for the sentences shorter than 40 words were 86.3% and 85.8%, respectively. This was the first parser to approach the current state-of-the-art performance on the task of parsing Wall Street Journal sentences. 21 2.2.6 Data Oriented Parsing Bod [Bod 92] [Bod 95] <ref> [Bod and Scha 96] </ref> explored the idea of Data Oriented Parsing. In this framework, all possible tree fragments in a hand annotated corpus are regarded as rules of a probabilistic grammar.
Reference: [Brill et al. 90] <author> Eric Brill, David Magerman, </author> <title> Mitchell Marcus and Beatrice Santorini "Deducting Linguistic Structure from the Statistics of Large Corpora" Proceeding of the June DARPA Speech and Natural Language workshop. </title> <address> Hidden Valley, Pennsylvania. </address> <year> (1990) </year>
Reference: [Brill 93] <editor> Eric Brill "Automatic Grammar Induction and Parsing Free Text: </editor> <booktitle> A Transformation-Based Approach" Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL) (1993) </booktitle>
Reference: [Briscoe and Carroll 93] <institution> Ted Briscoe and John Carroll "Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars" Journal of Computational Linguistics Vol.19,No.1 (1993) </institution>
Reference-contexts: It produced a parse tree for 38 input sentences and 35 of them were equivalent to the correct parse tree produced by the augmented version of PUNDIT. They concluded that comparable performance was achieved without the painfully hand-crafted augmentation of the grammar. Briscoe and Carroll <ref> [Briscoe and Carroll 93] </ref> created a probabilistic LR parser with a unification-based grammar. As states of the LR table are related to the parse context, it becomes a context sensitive probabilistic model 16 if the probability is calculated directly from the corpus, instead of derived from probabilistic CFG rules.
Reference: [Bross et al. 72] <author> I D J Bross and P A Shapiro and B B Anderson: </author> <title> "How information is carried in scientific sub-languages" Science", </title> <address> Vol.176, </address> <month> 1303-1307 </month> <year> (1972) </year>
Reference-contexts: From this definition, we can infer that a sublanguage can be defined empirically by observing language behavior. The other type of definition is exemplified by the following citation from Bross, Shapio and Anderson <ref> [Bross et al. 72] </ref>: Informally, we can define a sublanguage as the language used by a particular community of speakers, say, those concerned with a particular subject matter or those engaged in a specialized occu pation.
Reference: [Carroll and Briscoe 96] <author> John Carroll and Ted Briscoe: </author> <title> "Apportioning development effort in a probabilistic LR parsing system through evaluation" Proceedings of the Conference on Empirical Methods in Natural Language Processing. </title> <year> (1996) </year> <month> 260 </month>
Reference-contexts: When we try to parse a text in a particular domain, we should prepare a grammar which suits that domain. This idea naturally contrasts with the idea of robust broad-coverage parsing <ref> [Carroll and Briscoe 96] </ref>, in which a single grammar should be prepared for parsing any kind of text.
Reference: [Charniak 93] <author> Eugene Charniak: </author> <title> Statistical Language Learning The MIT Press (1993) </title>
Reference-contexts: There are several good textbook in natural language processing, for example, [Grishman 86]. Anyone who wants to know the basics of statistics used in this thesis should refer to <ref> [Charniak 93] </ref> or [Krenn and Samuelsson 97]. Chapter 2 describes a probabilistic parser whose grammar is obtained from 4 a syntactically tagged corpus. The distinguishing characteristic of the grammar is that it has only a small number of non-terminals. <p> The core idea of the probability estimation is iterative training. The inside-outside algorithm, which is motivated by the forward-backward algorithm, was used to reduce the expensive computation. For an 11 explanation of the inside-outside algorithm, see for example <ref> [Charniak 93] </ref>; only the iterative training will be explained here. The algorithm tried to assign a probability for each grammar rule, say P i (r j ) is the probability of rule r j after the i-th iteration. Each grammar rule has an initial probability P 0 (r). <p> This looks appealing, but as was described in <ref> [Charniak 93] </ref> there is a local minimum problem. Charniak reported an experiment with the inside-outside algorithm using a grammar of all possible fixed length rules. <p> This problem is the crucial issue for automatic sublanguage definition, because otherwise we need manual intervention or artificial thresholds. In order to explore this problem, the following statistics are proposed. Perplexity, more precisely `uni-gram perplexity' (P P ) is a notion from Information Theory (see <ref> [Charniak 93] </ref> or [Krenn and Samuelsson 97]). It can be formally defined based on unigram Entropy H: H = w P P = 2 H (3.2) Here p (w) is the probability of a token w in the source. <p> CE (T; M ) = i The smaller the value, the more accurately the model represents the test set. If the model is identical to the test set, the value is equal to the entropy of the set itself (see <ref> [Charniak 93] </ref> for details). For each pair of the distributions, cross entropy is computed by taking the distribution as the probability. Figure 3.3 shows the cross entropy of syntactic structure across domains.
Reference: [Charniak 97] <author> Eugene Charniak: </author> <booktitle> "Statistical Parsing with a Context-free Grammar and Word Statistics" Proceedings of the 14th National Conference on Artificial Inteligence (1997) </booktitle>
Reference-contexts: Also [Ratnaparkhi 97] proposed a parser using similar information based on maximum entropy model. The parsing strategy is unique and it can generate multiple trees in linear observed time. The performance is 87.5% and 86.3% of recall and precision. <ref> [Charniak 97] </ref> reported a parser based on similar framework. 2.2.8 Explanation-based Learning This is an interesting framework for constructing grammars which is closely related to the framework presented in this thesis. <p> Recently, in [Collins 97] <ref> [Charniak 97] </ref> [Ratnaparkhi 97], very good parsing performances are reported using lexical dependencies on statistical parsers. The dependency information they used is relatively richer than that used in this experiment.
Reference: [Chitrao and Grishman 90] <author> Mahesh Chitrao, Ralph Grishman: </author> <title> "Statistical Parsing of Message" Proceeding of the June DARPA Speech and Natural Language workshop. </title> <address> Hidden Valley, Pennsylvania. </address> <year> (1990) </year>
Reference-contexts: Furthermore, experiments 7 over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar [Fujisaki 84] [Garside and Leech 85] <ref> [Chitrao and Grishman 90] </ref> [Magerman and Weir 92] [Black et al. 93] [Bod 93] [Magerman 95] [Collins 96]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora <ref> [Chitrao and Grishman 90] </ref> [Simmons and Yu 91] [Magerman and Weir 92] [Schabes and Waters 93] [Black et al. 93] [Bod 93] [Magerman 95]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. <p> The performance with the seeds was significantly better than that without the seeds. This demonstrates the importance of annotated training data. 2.2.2 Context Sensitive Parsing The previous parsers used the pure context free grammar formalism. The method described here tried to use context sensitivity in parsing. Chitrao and Grishman <ref> [Chitrao and Grishman 90] </ref>, [Chitrao 90] used the iterative processing using an unannotated corpus based on a hand crafted grammar. They used an unannotated corpus just like the Fujisaki's experiment.
Reference: [Chitrao 90] <institution> Mahesh Chitrao "Statistical Techniques for Parsing Messages" Doctoral thesis, </institution> <note> New York University (1990) [CL Special Issue I 93] "Special Issue on Using Large Corpora: I" Journal of Computational Linguistics Vol.19,Num.1 (1993) [CL Special Issue II 93] "Special Issue on Using Large Corpora: II" Journal of Computational Linguistics Vol.19,Num.2 (1993) </note>
Reference-contexts: This demonstrates the importance of annotated training data. 2.2.2 Context Sensitive Parsing The previous parsers used the pure context free grammar formalism. The method described here tried to use context sensitivity in parsing. Chitrao and Grishman [Chitrao and Grishman 90], <ref> [Chitrao 90] </ref> used the iterative processing using an unannotated corpus based on a hand crafted grammar. They used an unannotated corpus just like the Fujisaki's experiment. The specialties of the method were `fine grained statistics' and `heuristic 13 penalties'. `Fine grained statistics' tried to capture some context sensitivities.
Reference: [Collins 96] <institution> Michael Collins "A New Statistical Parser Based on Bigram Lexical Dependencies" Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL) (1996) 261 </institution>
Reference-contexts: over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar [Fujisaki 84] [Garside and Leech 85] [Chitrao and Grishman 90] [Magerman and Weir 92] [Black et al. 93] [Bod 93] [Magerman 95] <ref> [Collins 96] </ref>. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> However, as they extracted all possible tree fragments, the number of possible derivations becomes enormous, so some technique to overcome the problem is needed [Sima'an 97]. 2.2.7 Parsing by Lexical Dependency This is another state-of-the-art parser, proposed by Collins <ref> [Collins 96] </ref>. It used an annotated corpus, Wall Street Journal of the PennTreeBank, like the 23 method by Magerman. The difference is that this method heavily relies on the lexical information or lexical dependencies rather than context information. <p> The head is going to be the head of all dependencies to the other elements in the constituent. It is defined by heuristic rules introduced by [Magerman 95] and <ref> [Collins 96] </ref>. For example, in order to find the head of a prepositional phrase, the elements in the phrase are scanned from left to right, and the first preposition encountered in the scan is the head of the phrase. <p> If no item in the table is found, then the left or right-most item will be the head of the constituent. Appendix B shows the rules. This table is created based on the previous work by [Magerman 95] and <ref> [Collins 96] </ref>. 74 2.8.3 Acquire Data from Corpus From 96% of the PennTreeBank corpus (section 00, 01 and 03-24), 1,034,914 dependency relationships for 32,012 distinct head words are extracted. The dependency direction, i.e. if the argument is to the left or right of the head, is also recorded.
Reference: [Collins 97] <editor> Michael Collins "Three Generative, </editor> <booktitle> Lexicalised Models for Statis--tical Parsing" Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL) (1997) </booktitle>
Reference-contexts: The performance is comparable to the parser by Magerman. Its best labeled-recall was 85.3% and labeled-precision was 85.7%. The interesting comparative experiment reported was that if it ignores the lexical information but uses only parts-of-speech information, the performance decrease significantly to 76.1% recall and 76.6% precision. 24 Recently <ref> [Collins 97] </ref> reported a new version of the parser. It includes a generative model of lexicalized context-free grammar and a probabilistic treatment of both subcategorization and wh-movement. The recall and precision of the new parser are 88.1% and 87.5%. <p> Recently, in <ref> [Collins 97] </ref> [Charniak 97] [Ratnaparkhi 97], very good parsing performances are reported using lexical dependencies on statistical parsers. The dependency information they used is relatively richer than that used in this experiment.
Reference: [Cormen et.al 90] <author> Thomas Cormen, Charles Leiserson and Ronald Rivest: </author> <title> Introduction to Algorithms The MIT Press (1990) </title>
Reference-contexts: The time complexity of heap operations is logarithmic in the number of data in the heap. (The reader who is interested may find details in <ref> [Cormen et.al 90] </ref>).
Reference: [Francis and Kucera 64/79] <author> W. Nelson Francis and Henry Kucera: </author> <title> "Manual of information to accompany A Standard Corpus of Present-Day Edited American English for use with Digital Computers" Brown University, </title> <institution> Department of Linguistics (1964/1979) </institution>
Reference-contexts: There are only 68 words which have frequencies over 1000. (The total number of distinct words is 39217 and the total number of articles is 2147). 3.4 Brown Corpus In the experiments in the remainder of the chapter and in the next chapter, the Brown corpus <ref> [Francis and Kucera 64/79] </ref> will be used. A brief review of the Brown corpus will be presented in this section. It is a syntactically tagged corpus consisting of several domains. One of the important features is that the way of tagging is uniform throughout the corpus.
Reference: [Fujisaki 84] <author> Tetsunosuke Fujisaki: </author> <booktitle> "A Stochastic Approach to Sentence Parsing" Proceedings of the 10th Conference on Computational Linguistics (COLING) (1984) </booktitle>
Reference-contexts: Furthermore, experiments 7 over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar <ref> [Fujisaki 84] </ref> [Garside and Leech 85] [Chitrao and Grishman 90] [Magerman and Weir 92] [Black et al. 93] [Bod 93] [Magerman 95] [Collins 96]. <p> The third column indicates if it uses lexical information. In the last row, the method proposed in this thesis is explained. It emphasizes context sensitivity and tries it with and without lexical information. 2.2.1 Iterative Learning Algorithm One of the earliest experiments on corpus-based parsing was conducted by Fujisaki <ref> [Fujisaki 84] </ref> [Fujisaki et al. 89]. It involved probabilities of pure CFG rules computed from an unannotated corpus using the inside-outside algorithm and unsupervised iterative training. They use a hand crafted grammar as the base of the experiment. The core idea of the probability estimation is iterative training.
Reference: [Fujisaki et al. 89] <author> Tetsunosuke Fujisaki, Fred Jelinek, J.Cocke, Ezra Black and T.Nishino: </author> <booktitle> "Probabilistic Parsing Method for Sentence Disambiguation" Proceedings of 1st International Workshop on Parsing Technologies (1989) </booktitle>
Reference-contexts: In the last row, the method proposed in this thesis is explained. It emphasizes context sensitivity and tries it with and without lexical information. 2.2.1 Iterative Learning Algorithm One of the earliest experiments on corpus-based parsing was conducted by Fujisaki [Fujisaki 84] <ref> [Fujisaki et al. 89] </ref>. It involved probabilities of pure CFG rules computed from an unannotated corpus using the inside-outside algorithm and unsupervised iterative training. They use a hand crafted grammar as the base of the experiment. The core idea of the probability estimation is iterative training.
Reference: [Fujio and Matsumoto 97] <author> Masakazu Fujio and Yuji Matsumoto: </author> <title> "Statistic Based Dependency Analysis" Natural Language group Information Processing Society of Japan (In Japanese) (1997) </title>
Reference-contexts: It has been well discussed that, in Japanese, lexical relationships are the most crucial information for analyzing syntactic structures, and there are probabilistic corpus based parser which use lexical information (for example, <ref> [Fujio and Matsumoto 97] </ref> [Shirai et al. 97]).
Reference: [Garside and Leech 85] <editor> Roger Garside, Fanny Leech: </editor> <booktitle> "A Probabilistic Parser" Proceedings of the 2nd Conference of the European Chapter of the Association for Computational Linguistics (EACL) (1985) </booktitle>
Reference-contexts: Furthermore, experiments 7 over the past few years have shown the benefits of using probabilistic infor-mation in parsing, and the large corpus allows us to train the probabilities of a grammar [Fujisaki 84] <ref> [Garside and Leech 85] </ref> [Chitrao and Grishman 90] [Magerman and Weir 92] [Black et al. 93] [Bod 93] [Magerman 95] [Collins 96].
Reference: [Gnanadesikan 77] <author> R.Gnanadesikan: </author> <title> Methods for Statistical Data Analysis of Multivariate Observations John Wiley & Sons, </title> <publisher> Inc (1977) </publisher>
Reference-contexts: These matrices themselves are not easy to observe. In the next subsection, analyses of these matrices using a clustering technique are described. 3.5.3 Clustering First, a brief introduction of the standard clustering techniques using a distance matrix is mentioned <ref> [Gnanadesikan 77] </ref>. Given a set of items and a distance between each pair of items (not necessarily Euclidean distance), similar items are clustered based on the distance. Because there is no unique definition of grouping, the method of clustering is rather heuristic and several different methods have been proposed.
Reference: [Grishman et al. 84a] <author> Ralph Grishman, Ngo Than Nhan, Elaine Marsh and Lynette Hirschmann: </author> <booktitle> "Automated determination of sublanguage syntactic usage" Proceedings of the 10th Conference on Computational Linguistics (COLING) (1984) </booktitle>
Reference: [Grishman et al. 84b] <author> Ralph Grishman, Ngo Than Nhan and Elaine Marsh: </author> <title> "Tuning Natural Language Grammers for New Domains" Conference on Intelligent Systems and Machines, </title> <address> Rochester, Minnesota, </address> <month> pp342-346 </month> <year> (1984) </year>
Reference: [Grishman et al. 86] <author> Ralph Grishman, Lynette Hirschman and Ngo Than Nhan: </author> <title> "Discovery Procedures for Sublanguage Selectional Patterns: </title> <note> Initial Experiments" Journal of Computational Linguistics Vol.12 No.3 (1986) </note>
Reference: [Grishman 86] <author> Ralph Grishman: </author> <note> Computational Linguistics: An introduction Cambridge University Press (1986) </note>
Reference-contexts: It should be possible for anyone with sufficient knowledge of natural language processing and basic knowledge about statistics to work their way through this thesis, though this may require considerable effort for a total novice. There are several good textbook in natural language processing, for example, <ref> [Grishman 86] </ref>. Anyone who wants to know the basics of statistics used in this thesis should refer to [Charniak 93] or [Krenn and Samuelsson 97]. Chapter 2 describes a probabilistic parser whose grammar is obtained from 4 a syntactically tagged corpus.
Reference: [Grishman and Kittredge 86] <editor> Ralph Grishman and Richard Kittredge (ed.): </editor> <title> Analyzing Language in Restricted Domains: </title> <publisher> Sublanguage Description and Processing Lawrence Erlbaum Associates, Publishers (1986) </publisher>
Reference-contexts: Most of them claim that the notion is important in processing natural language text, owning to lexical, syntactic or semantic restrictions, etc. A number of these studies have analyzed actual texts to try to verify the claim [Kittredge 82] <ref> [Grishman and Kittredge 86] </ref> [Slocum 86] [Biber 93]. Some of these studies and the definition of `sublanguage' will be discussed in Section 3.2. Several successful natural language processing systems have explicitly or 112 implicitly addressed the sublanguage restrictions. <p> There are many other such studies, for example in [Kittredge 82] <ref> [Grishman and Kittredge 86] </ref>. Slocum [Slocum 86] reported the experiments which tried to identify sub-languages on syntactic grounds. Four texts were prepared; two were extracted from operating and maintenance manuals, and the other two were sales brochures. These were parsed by their METAL parser.
Reference: [Grishman and Sterling 92] <editor> Ralph Grishman and John Sterling: </editor> <booktitle> "Acquisition of Selectional Patterns" Proceedings of the 14th Conference on Computational Linguistics (COLING) (1992) </booktitle>
Reference-contexts: Among them, several studies have mentioned the importance of the sublanguage notion, for example <ref> [Grishman and Sterling 92] </ref> [Sekine 92]. Although these are still small experiments in terms of coverage, they addressed an important problem of knowledge acquisition for sublanguage.
Reference: [Grishman and Sterling 94] <editor> Ralph Grishman, John Sterling: </editor> <booktitle> "Generalizing Automatically Generated Selectional Patterns" Proceedings of the 15th Conference on Computational Linguistics (COLING) (1994) </booktitle>
Reference: [Grishman et al. 94] <author> Ralph Grishman, Catherine Macleod and Adam Meyers: </author> <title> "Comlex Syntax: </title> <booktitle> Building a Computational Lexicon" Proceedings of the 264 15th Conference on Computational Linguistics (COLING) (1994) </booktitle>
Reference-contexts: So one plausible idea is to build word clusters each of which has similar behavior. In order to build the cluster, there are two possibilities. One is to use an existing dictionary. For example, the COMLEX dictionary <ref> [Grishman et al. 94] </ref> has the class of "report-verb". We can use the verbs in this class for this particular instance. The other possibility is to use automatic clustering based on the data itself.
Reference: [Harris 68] <editor> Zellig Harris: </editor> <publisher> Mathematical Structures of Language John Wiley and Sons, </publisher> <address> New York (1968) </address>
Reference-contexts: We can find two kinds of definitions, although the difference has not received serious attention. One definition is inferable from Harris <ref> [Harris 68] </ref>: Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it. From this definition, we can infer that a sublanguage can be defined empirically by observing language behavior.
Reference: [Hirshman et al. 75] <author> Lynette Hirschman, Ralph Grishman and Naomi Sager: </author> <title> "Grammatically based automatic word class formation" Information Processing and Management vol.11, </title> <address> pp.39-57. </address> <year> (1975) </year>
Reference: [Inui et al. 97] <author> Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka and Takenobu Tokunaga: </author> <booktitle> "A new Formalization of Probabilistic GLR Parsing" Proceedings of 5th International Workshop on Parsing Technologies (1997) </booktitle>
Reference-contexts: Out of 55 test sentences, 41 (74.5%) were parsed correctly with the highest ranked analysis, while in 6 cases, the correct analysis was the second or third most highly ranked. An interesting work on this line was reported in <ref> [Inui et al. 97] </ref>. 2.2.4 History Based Parsing All the approaches to grammar learning in this subsection and the subsections which follow used hand annotated corpora for their training. In other words, these are completely supervised methods.
Reference: [Isabelle 84] <author> P Isabelle: </author> <title> "Machine Translation at the TAUM group" in King,M. </title> <editor> (ed.) </editor> <booktitle> Machine Translation Today: The State of the Art, </booktitle> <publisher> Edinburgh University Press (1984) </publisher>
Reference-contexts: Some of these studies and the definition of `sublanguage' will be discussed in Section 3.2. Several successful natural language processing systems have explicitly or 112 implicitly addressed the sublanguage restrictions. For example, TAUM-METEO <ref> [Isabelle 84] </ref> is a machine translation system in which only sentences in the weather forecast domain are dealt with. It works remarkably well and has been used commercially. It is believed that the system was successful because of the limitation of the task, including the size of vocabulary and grammar.
Reference: [Jelinek 91] <author> Fred Jelinek, B Merialdo, S Roukos, and M Strauss: </author> <title> "A Dynamic Language Model for Speech Recognition" Proceedings of DARPA Speech and Natural Language Workshop (1991) 265 </title>
Reference-contexts: The data can be structured in advance (trigger model) or the raw text data can be retrieved and analyzed on 157 Site Description Improvement References (Year) (word error rate) IBM (91) cache model <ref> [Jelinek 91] </ref> CMU (94) trigger model 19.9 ! 17.8 [Rosenfeld 94] BU (93-94) clustering 11.3 ! 11.2 [Ostendorf et al. 95] (4 topic LM) CMU (96) hand clustering 0.1,0.6% improve. [Seymore 97] (5883 topic) in 2 story SRI (96) clustering 33.1 ! 33.0 [Weng 97] (4 topic LM) CU (96) cache
Reference: [Jones and Somers 97] <editor> Daniel Jones and Harold Somers (ed.): </editor> <title> New Methods in Language Processing UCL Press (1982) </title>
Reference-contexts: Also the work was referenced in a book [McEnery and Wilson 96] and appeared in a chapter of a book <ref> [Jones and Somers 97] </ref>. Chapter 4 describes experiments with the parser based on the sublanguage notion. This combines the approaches described in the previous two chapters. Several sublanguage grammars are acquired from different domain corpora, and the performance of parsing texts from different domain with different grammars is observed.
Reference: [Joshi et al. 75] <author> Aravind Joshi, Leon Levy and M Takahashi: </author> <title> "Tree Adjunct grammars" Journal of the Computer and System Sciences, </title> <month> 10 </month> <year> (1975) </year>
Reference-contexts: The formalism of Tree Adjoining Grammar (TAG) and Lexical Tree Adjoining Grammar (LTAG) were proposed by Joshi and Schabes <ref> [Joshi et al. 75] </ref> [Joshi 87]. [Schabes et al. 88] [Schabes 90]. The idea behind TAG formalism is very similar to the idea of the parser described in this thesis and LTAG is actually a motivation of the modification towards lexicalized probabilistic parser explained in Section 2.9.
Reference: [Joshi 87] <author> Aravind Joshi: </author> <title> "An introduction to Tree Adjoining Grammars" In A.Manaster-Ramer (ed.), Mathematics of Language, </title> <publisher> John Benjamins, </publisher> <address> Amsterdam (1987) </address>
Reference-contexts: The formalism of Tree Adjoining Grammar (TAG) and Lexical Tree Adjoining Grammar (LTAG) were proposed by Joshi and Schabes [Joshi et al. 75] <ref> [Joshi 87] </ref>. [Schabes et al. 88] [Schabes 90]. The idea behind TAG formalism is very similar to the idea of the parser described in this thesis and LTAG is actually a motivation of the modification towards lexicalized probabilistic parser explained in Section 2.9.
Reference: [Joshi 94] <author> Aravind Joshi and B Srinivas: </author> <title> "Disambiguation of Super Parts of Speech (or Supertags): </title> <booktitle> Almost Parsing" Proceedings of the 15th Conference on Computational Linguistics (COLING) (1994) </booktitle>
Reference-contexts: Several frameworks of probabilistic LTAG have been proposed [Resnik 92] [Schabes 92]. However, these seem to have difficulty to define the initial grammar rules. It is not straightforward to assign probabilities to the rules. The recent work by Joshi <ref> [Joshi 94] </ref> tries to use n-gram statistics in order to find an elemental structure for each lexical item, which is called supertag. As a supertag contains structural information, assigning supertags for all the words in a sentence almost results in a parse of the sentence.
Reference: [Joshi 96] <editor> Aravind Joshi: </editor> <booktitle> "NLP Research at UPenn" JSPS workshop; Tokyo (1996) </booktitle>
Reference: [Karlgren and Cutting 94] <author> Jussi Karlgren and Douglass Cutting: </author> <title> "Recognizing Text Genres with Simple Metrics Using Discriminant Analysis" Proceedings of the 15th Conference on Computational Linguistics (COLING) (1994) </title>
Reference: [Katz 87] <author> Slava M. </author> <title> Katz "Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer" IEEE Transactions on Acoustics, Speech, </title> <booktitle> and Signal Processing (1987) </booktitle>
Reference: [Kittredge 82] <editor> Richard Kittredge, John Lehrberger (ed.): </editor> <title> Sublanguage: Studies of Language in Restricted Semantic domains Walter de Gruyter, </title> <address> Berlin (1982) </address>
Reference-contexts: Most of them claim that the notion is important in processing natural language text, owning to lexical, syntactic or semantic restrictions, etc. A number of these studies have analyzed actual texts to try to verify the claim <ref> [Kittredge 82] </ref> [Grishman and Kittredge 86] [Slocum 86] [Biber 93]. Some of these studies and the definition of `sublanguage' will be discussed in Section 3.2. Several successful natural language processing systems have explicitly or 112 implicitly addressed the sublanguage restrictions. <p> There are many other such studies, for example in <ref> [Kittredge 82] </ref> [Grishman and Kittredge 86]. Slocum [Slocum 86] reported the experiments which tried to identify sub-languages on syntactic grounds. Four texts were prepared; two were extracted from operating and maintenance manuals, and the other two were sales brochures. These were parsed by their METAL parser.
Reference: [Krenn and Samuelsson 97] <author> Brigitte Krenn and Christer Samuelsson: </author> <note> "The Linguist's Guide to Statistics" http://www.coli.uni-sb.de/ ~ krenn/stat nlp.ps (1997) </note>
Reference-contexts: There are several good textbook in natural language processing, for example, [Grishman 86]. Anyone who wants to know the basics of statistics used in this thesis should refer to [Charniak 93] or <ref> [Krenn and Samuelsson 97] </ref>. Chapter 2 describes a probabilistic parser whose grammar is obtained from 4 a syntactically tagged corpus. The distinguishing characteristic of the grammar is that it has only a small number of non-terminals. Both a two non-terminal grammar and five non-terminal grammar will be described. <p> This problem is the crucial issue for automatic sublanguage definition, because otherwise we need manual intervention or artificial thresholds. In order to explore this problem, the following statistics are proposed. Perplexity, more precisely `uni-gram perplexity' (P P ) is a notion from Information Theory (see [Charniak 93] or <ref> [Krenn and Samuelsson 97] </ref>). It can be formally defined based on unigram Entropy H: H = w P P = 2 H (3.2) Here p (w) is the probability of a token w in the source.
Reference: [Kurohashi and Nagao 97] <author> Sadao Kurohashi and Makoto Nagao: </author> <title> "Building a Japanese Parsed Corpus while Improving the Parsing System" Proceedings of the Natural Language Pacific Rim Symposium (1997) </title>
Reference-contexts: The difference is caused by the additional 83 two words. In this section, a trial of acquiring a Japanese grammar which uses context information will be presented. 2.10.1 Corpus In this experiment, a Japanese annotated corpus developed at Kyoto University <ref> [Kurohashi and Nagao 97] </ref> is used. It has part-of-speech (POS) tagging and dependency information. They used an automatic part-of-speech tagger, JUMAN [Matsumoto et al. 97] and a dependency analyzer, KN Parser [Kurohashi 96] to build initial structures and then trained annotators corrected the results.
Reference: [Kurohashi 96] <author> Sadao Kurohashi: </author> <title> "Japanese syntactic analyser: </title> <address> KNP" Kyoto University (1996) </address>
Reference-contexts: It has part-of-speech (POS) tagging and dependency information. They used an automatic part-of-speech tagger, JUMAN [Matsumoto et al. 97] and a dependency analyzer, KN Parser <ref> [Kurohashi 96] </ref> to build initial structures and then trained annotators corrected the results. The size of the corpus is about 10,000 sentences, which is about a fifth of PennTreeBank at the moment. Their target size is 200,000. Each segment (BUNSETSU) is identified along with its dependency information.
Reference: [Leech et al. 83] <author> Geoffrey Leech, Roger Garside, E. Atwell: </author> <title> "The Automatic Grammatical Tagging of the LOB Corpus" ICAME News 7, </title> <month> pp13-33 </month> <year> (1983) </year>
Reference: [Leech et al. 87] <author> Geoffrey Leech, Roger Garside, G. Sampson: </author> <title> The computa-tional analysis of English; A corpus based approach London, </title> <publisher> Longman (1987) </publisher>
Reference: [Lehrberger 82] <author> John Lehrberger: </author> <title> "Automatic Translation and the Concept of Sublanguage" in Sublanguage: Study of Language in Restricted Semantic Domains (1982) </title>
Reference-contexts: However, the deductive definition is not always possible and sometimes it may be wrong, as was discussed earlier. The inductive definition is related to quantitative analyses, since it is empirical. This will be described later. 3.2.2 Qualitative Analyses of Sublanguage Lehrberger <ref> [Lehrberger 82] </ref> summarized qualitatively the characteristics of sublanguage into six categories. This analysis is also interesting in terms of quantitative analyses. Since qualitative analyses and quantitative analyses are just like two sides of a coin, many of the characteristics in this analysis are actually the topics of quantitative analyses. 1.
Reference: [Magerman and Marcus 91a] <author> David Margerman and Mitchell Marcus: </author> <title> "Pearl: </title>
Reference-contexts: When the two experiments were conducted, large annotated corpora were not available. The first approach used an existing parser to create training data, and the other also used an existing parser along with human intervention to choose the appropriate analysis. 15 Magerman <ref> [Magerman and Marcus 91a] </ref> proposed a probabilistic parser, Pearl. It did not use an iterative training process; instead, probabilities were defined just by counting the usage of rules in a corpus.
References-found: 54

