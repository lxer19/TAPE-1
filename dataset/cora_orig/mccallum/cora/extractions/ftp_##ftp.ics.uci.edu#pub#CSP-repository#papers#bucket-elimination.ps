URL: ftp://ftp.ics.uci.edu/pub/CSP-repository/papers/bucket-elimination.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: dechter@ics.uci.edu  
Title: Bucket elimination: A unifying framework for probabilistic inference  
Author: Rina Dechter 
Address: Irvine  
Affiliation: Department of Information and Computer Science University of California,  
Abstract: Probabilistic inference algorithms for finding the most probable explanation, the maximum aposteriori hypothesis, and the maximum expected utility and for updating belief are reformulated as an elimination-type algorithm called bucket elimination. This emphasizes the principle common to many of the algorithms appearing in that literature and clarifies their relationship to nonserial dynamic programming algorithms. We also present a general way of combining conditioning and elimination within this framework. Bounds on complexity are given for all the algorithms as a function of the problem's struc ture.
Abstract-found: 1
Intro-found: 1
Reference: [ Arnb 85 ] <author> S. Arnborg, </author> <title> "Efficient algorithms for combinatorial problems on graphs with bounded decomposability A survey" Bit 25 </title> (1985):2-23. 
Reference-contexts: This leads to a hybrid algorithm, one trading off time for space [ Dech 96 ] . The work we present here also fits into the framework of <ref> [ Arnb 85; ArPr 89 ] </ref> . Arnborg presents table-based reductions for various NP-hard graph problems such as the independent set problem, network reliability, vertex cover, graph k-colorability, and Hamilton circuits. <p> The induced width of a graph, wfl, is the minimal induced width over all its orderings; it is also known as the tree-width. A graph has an induced width k iff it can be embedded into a k-tree, in which case it is called a partial k-tree <ref> [ Arnb 85 ] </ref> . A cycle-cutset is a subset of nodes in the graph that, when removed, results in a graph without cycles. <p> Because dynamic programming algorithms work by eliminating variables one by one while computing the effect of each eliminated variable on the remainder of the problem, they can be viewed as elimination algorithms. It is known that most such algorithms have worst-case complexity bounded exponentially by the induced width <ref> [ Dech 90; Arnb 85 ] </ref> of their underlying graph. In belief networks the graph, often called the moral graph, is obtained by connecting all the parents of each node in the acyclic graph and ignoring directionality.
Reference: [ ArPr 89 ] <author> S. Arnborg and A. Proskourowski, </author> <title> "Linear time algorithms for NP-hard problems restricted to partial k-trees" Discrete and Applied Mathematics 23 (1989) 11-24. </title>
Reference-contexts: This leads to a hybrid algorithm, one trading off time for space [ Dech 96 ] . The work we present here also fits into the framework of <ref> [ Arnb 85; ArPr 89 ] </ref> . Arnborg presents table-based reductions for various NP-hard graph problems such as the independent set problem, network reliability, vertex cover, graph k-colorability, and Hamilton circuits.
Reference: [ BeBr 72 ] <author> U. Bertele and F. Brioschi, </author> <title> Nonserial Dynamic Programming, </title> <address> New York, </address> <year> 1972. </year>
Reference-contexts: This view, called bucket elimination, is a generalization of nonserial dynamic programming a la Bertele and Briochi <ref> [ BeBr 72 ] </ref> . It allows a uniform way of combining elimination with conditioning, and provides insight into the relationship between clustering and elimination. To emphasize the generality of bucket elimination we start with a similar algorithm in the area of deterministic reasoning. <p> Complexity is time exponential in the cutset size for the former, time and space exponential in the cluster sizes, bounded by the induced-width, for the latter. 3 DYNAMIC PROGRAMMING We now present elimination algorithms for the various tasks. The algorithms generalize the family of nonserial dynamic programming <ref> [ BeBr 72 ] </ref> . Because dynamic programming algorithms work by eliminating variables one by one while computing the effect of each eliminated variable on the remainder of the problem, they can be viewed as elimination algorithms.
Reference: [ Bacc 95 ] <author> F. Bacchus and A. Groove, </author> <title> "Graphical models for preference and utility," </title> <booktitle> in Uncertainty in Artificial Intelligence (UAI-95), </booktitle> <pages> pp 3-10, </pages> <year> 1995. </year>
Reference: [ Coop 84 ] <author> G.F. Cooper, "Nestor: </author> <title> A computer-based medical diagnosis aid that integrates causal and probabilistic knowledge," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1984. </year>
Reference-contexts: Early attempts are given in <ref> [ Coop 84; PeRe 86; PeRe 89 ] </ref> . Recent proposals include best first-search algorithms [ ShCh 91 ] and algorithms based on linear programming [ Sant 91 ] .
Reference: [ DePe 89 ] <author> R. Dechter and J. Pearl, </author> <title> "Tree clustering for constraint networks," </title> <booktitle> Artificial Intelligence </booktitle> (1989):353-366. 
Reference-contexts: The main virtue of this presentation, beyond uniformity, is that it facilitates transfer of ideas and techniques across areas of research. In particular, having noted that elimination algorithms and clustering algorithms are very similar <ref> [ DePe 89 ] </ref> , we propose a uniform way for improving such algorithms based on conditioning. We show that the idea of conditioning, which is as universal as that of elimination, can be incorporated and exploited naturally within the elimination framework.
Reference: [ Dech 90 ] <author> R. Dechter, </author> <booktitle> "Constraint networks" in Encyclopedia of Artificial Intelligence, 2nd ed., </booktitle> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Because dynamic programming algorithms work by eliminating variables one by one while computing the effect of each eliminated variable on the remainder of the problem, they can be viewed as elimination algorithms. It is known that most such algorithms have worst-case complexity bounded exponentially by the induced width <ref> [ Dech 90; Arnb 85 ] </ref> of their underlying graph. In belief networks the graph, often called the moral graph, is obtained by connecting all the parents of each node in the acyclic graph and ignoring directionality.
Reference: [ DeBe 95 ] <author> R. Dechter and P. van Beek, </author> <title> "Local and global relational consistency Summary of recent results," </title> <booktitle> in Principles and Practice of Constraint Programming (CP-95),, </booktitle> <address> Cadec, France, </address> <year> 1995. </year>
Reference-contexts: The work we present here also fits into the framework of [ Arnb 85; ArPr 89 ] . Arnborg presents table-based reductions for various NP-hard graph problems such as the independent set problem, network reliability, vertex cover, graph k-colorability, and Hamilton circuits. Our paper as well as <ref> [ DeBe 95 ] </ref> extends this approach to a different set of problems. 2 PRELIMINARIES A belief network (BN) is a concise description of a complete probability distribution.
Reference: [ Dech 96 ] <author> R. Dechter, </author> <title> "Topological parameters for time-space trade-off," </title> <booktitle> in Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence (UAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: We show that the idea of conditioning, which is as universal as that of elimination, can be incorporated and exploited naturally within the elimination framework. This leads to a hybrid algorithm, one trading off time for space <ref> [ Dech 96 ] </ref> . The work we present here also fits into the framework of [ Arnb 85; ArPr 89 ] . Arnborg presents table-based reductions for various NP-hard graph problems such as the independent set problem, network reliability, vertex cover, graph k-colorability, and Hamilton circuits. <p> One important method for reducing the space complexity is conditioning. We have shown that conditioning can be incorporated naturally on top of elimination, and that it can reduce the space complexity while still exploiting the structure (see also <ref> [ Dech 96 ] </ref> ). The combination of conditioning with elimination can be viewed as an elegant way for combining the virtues of forward and backward search.
Reference: [ Jens 94 ] <author> F. Jensen, F. Jensen and S. </author> <title> Dittmer "From influence diagrams to junction trees," </title> <booktitle> in Uncertainty in Artificial Intelligence (UAI-94), </booktitle> <pages> pp. 367-373, </pages> <year> 1994. </year>
Reference: [ DeRi 94 ] <author> R. Dechter and I. Rish, </author> <title> "Directional resolution: The Davis-Putnam procedure, revisited," </title> <booktitle> in Principles of Knowledge Representation and Reasoning (KR-94), </booktitle> <year> 1994. </year>
Reference-contexts: The result of this restriction is a systematic elimination of literals from the set of clauses that are candidates for future resolution. This algorithm, which we call directional resolution (DR), is the core of the well-known Davis-Putnam algorithm for satisfi-ability <ref> [ DaPu 60; DeRi 94 ] </ref> . Algorithm DR (see Figure 1) is described using buckets partitioning the set of clauses in the theory '. We call its output theory, E d ('), the directional extension of '. <p> The algorithm processes the buckets in a reverse order of d. When processing bucket i , it resolves over Q i all possible pairs of clauses in the bucket and inserts the resolvents into the appropriate lower buckets. It was shown <ref> [ DeRi 94 ] </ref> that: Theorem 1.1 (model generation) Let ' be a cnf formula, d = Q 1 ; :::; Q n an ordering, and E d (') its directional extension. <p> It was also shown <ref> [ DeRi 94 ] </ref> , that the complexity of DR is exponentially bounded (time and space) in the induced width (also called tree-width) of the interaction graph of the theory, where a node is associated with a proposition and an arc connects any two nodes appearing in the same clause. <p> The performance of elimination algorithms (as well as tree-clustering) is likely to suffer from the known difficulty with dynamic programming algorithms: exponential space (for recording the tables) and exponential time unless the problem has a small induced width. Such performance deficiencies also exist in resolution algorithms like DR <ref> [ DeRi 94 ] </ref> . One important method for reducing the space complexity is conditioning. We have shown that conditioning can be incorporated naturally on top of elimination, and that it can reduce the space complexity while still exploiting the structure (see also [ Dech 96 ] ).
Reference: [ DaPu 60 ] <author> M. Davis and H. Putnam, </author> <title> "A computing procedure for quantification theory," </title> <journal> Journal of the ACM 7 </journal> (1960):201-216. 
Reference-contexts: The result of this restriction is a systematic elimination of literals from the set of clauses that are candidates for future resolution. This algorithm, which we call directional resolution (DR), is the core of the well-known Davis-Putnam algorithm for satisfi-ability <ref> [ DaPu 60; DeRi 94 ] </ref> . Algorithm DR (see Figure 1) is described using buckets partitioning the set of clauses in the theory '. We call its output theory, E d ('), the directional extension of '.
Reference: [ Pear 88 ] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems, 2nd ed., </title> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: It is known that these tasks are NP-hard. Nevertheless, a polynomial propagation algorithm for singly-connected networks <ref> [ Pear 88 ] </ref> exists. The two main approaches to extending this propagation algorithm to multiply-connected networks are the cycle-cutset approach, also called conditioning, and tree-clustering [ Pear 88; Spie 86; Shac 86 ] . These methods work well only for sparse networks with small cycle-cutsets or small clusters. <p> It is known that these tasks are NP-hard. Nevertheless, a polynomial propagation algorithm for singly-connected networks [ Pear 88 ] exists. The two main approaches to extending this propagation algorithm to multiply-connected networks are the cycle-cutset approach, also called conditioning, and tree-clustering <ref> [ Pear 88; Spie 86; Shac 86 ] </ref> . These methods work well only for sparse networks with small cycle-cutsets or small clusters. <p> Lack of space prevents us from showing explicitly how they map into the elimination framework. 3.1 AN ELIMINATION ALGORITHM FOR MPE Following Pearl's propagation algorithm for singly connected networks <ref> [ Pear 88 ] </ref> , researchers have investigated various approaches to finding the MPE in BN. Early attempts are given in [ Coop 84; PeRe 86; PeRe 89 ] . <p> We thus avoid increasing the dimensionality of recorded functions. Processing buckets containing observations in this manner exploits the cutset effect of conditioning automatically <ref> [ Pear 88 ] </ref> . Another important point is that, had the bucket of B been at the top of our ordering, the advantage of this observation could have been exploited earlier in the computation. <p> For example, if algorithm elim-max is given a singly-connected network and we use an ordering having width 1 (always possible for trees), it reduces to Pearl's algorithm for that task <ref> [ Pear 88 ] </ref> . Likewise, elim-bel is identical to Pearl's tree-propagation algorithm for belief update with the exception that it an swers singleton queries. Each new query requires running elim-bel where the queried variables appear first in the ordering.
Reference: [ PeRe 86 ] <author> Y. Peng and J.A. </author> <title> Reggia "Plausability of diagnostic hypothesis", </title> <booktitle> in Proceedings (AAAI-86), </booktitle> <address> Philadelphia, </address> <year> 1986, </year> <pages> pp. 140-145. </pages>
Reference-contexts: Early attempts are given in <ref> [ Coop 84; PeRe 86; PeRe 89 ] </ref> . Recent proposals include best first-search algorithms [ ShCh 91 ] and algorithms based on linear programming [ Sant 91 ] .
Reference: [ PeRe 89 ] <author> Y. Peng and J. Reggia, </author> <title> "A connectionist model for diagnostic problem solving," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 19 (1989): </journal> <pages> pp. </pages> . 
Reference-contexts: Early attempts are given in <ref> [ Coop 84; PeRe 86; PeRe 89 ] </ref> . Recent proposals include best first-search algorithms [ ShCh 91 ] and algorithms based on linear programming [ Sant 91 ] .
Reference: [ Sant 91 ] <author> E. Santos, </author> <title> "On the generation of alternative explanations with implications for belief revision," </title> <booktitle> Uncertainty in Artificial Intelligence (UAI-91), </booktitle> <pages> pp. 339-347, </pages> <year> 1991. </year>
Reference-contexts: Early attempts are given in [ Coop 84; PeRe 86; PeRe 89 ] . Recent proposals include best first-search algorithms [ ShCh 91 ] and algorithms based on linear programming <ref> [ Sant 91 ] </ref> . The problem is to maximize the function max x P (x) = max x i P (x i jx pa i ) when x = (x 1 ; :::; x n ).
Reference: [ Shac 86 ] <author> R.D. </author> <title> Shachter "Evaluating influence diagrams," </title> <journal> Operations Research, </journal> <month> 34 </month> <year> (1986). </year>
Reference-contexts: It is known that these tasks are NP-hard. Nevertheless, a polynomial propagation algorithm for singly-connected networks [ Pear 88 ] exists. The two main approaches to extending this propagation algorithm to multiply-connected networks are the cycle-cutset approach, also called conditioning, and tree-clustering <ref> [ Pear 88; Spie 86; Shac 86 ] </ref> . These methods work well only for sparse networks with small cycle-cutsets or small clusters.
Reference: [ Shac 88 ] <author> R.D. Shachter, </author> <title> "Probabilistic inference and influence diagrams," </title> <journal> Operations Research, </journal> <month> 36 </month> <year> (1988). </year>
Reference: [ ShPe 92 ] <author> R.d. Shachter and P. Peot, </author> <title> "Decision making using probabilistic inference methods," </title> <booktitle> in Uncertainty in Artificial Intelligence (UAI-92), </booktitle> <pages> pp. 276-283, </pages> <year> 1992. </year>
Reference: [ Shac 90 ] <author> R.D. Shachter, B. D'Ambrosio, and B.A. Del Favro, </author> <title> "Symbolic probabilistic inference in belief networks," </title> <booktitle> Automated Reasoning (1990): </booktitle> <pages> 126-131. </pages> <note> In Operations Research Vol. 36, No.4, 198b. </note>
Reference-contexts: In the bucket of X i put all the matrices men tioning X i that do not mention any variable higher in the ordering. The procedure has backward and forward parts and is justified by the following symbolic manipulation (see also <ref> [ Shac 90 ] </ref> ). (1) Backward part.
Reference: [ Shen 92 ] <author> P.P. Shenoy, </author> <title> "Valuation-based systems for Bayesian decision analysis," em Operations Research, </title> <booktitle> 40 (1992): </booktitle> <pages> 463-484. </pages>
Reference: [ ShCh 91 ] <author> S.E. Shimony and E. Charniack, </author> <title> "A new algorithm for finding MAP assignments to belief networks,". </title> <editor> In P. Bonissone, M. Hen-rion, L. Kanal, and J. Lemmer ed., </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pp. 185-193, </pages> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Early attempts are given in [ Coop 84; PeRe 86; PeRe 89 ] . Recent proposals include best first-search algorithms <ref> [ ShCh 91 ] </ref> and algorithms based on linear programming [ Sant 91 ] . The problem is to maximize the function max x P (x) = max x i P (x i jx pa i ) when x = (x 1 ; :::; x n ).
Reference: [ Spie 86 ] <author> D.J. Spiegelhalter, </author> <title> "Probabilistic reasoning in predictive expert systems," </title> <booktitle> in Uncertainty in Artificial Intelligence, </booktitle> <editor> ed. L.N. Kanal and J.F. </editor> <booktitle> Lemmer, </booktitle> <pages> pp. 47-68, </pages> <address> Ams-terdam, </address> <year> 1986. </year>
Reference-contexts: It is known that these tasks are NP-hard. Nevertheless, a polynomial propagation algorithm for singly-connected networks [ Pear 88 ] exists. The two main approaches to extending this propagation algorithm to multiply-connected networks are the cycle-cutset approach, also called conditioning, and tree-clustering <ref> [ Pear 88; Spie 86; Shac 86 ] </ref> . These methods work well only for sparse networks with small cycle-cutsets or small clusters.
Reference: [ Kjae 93 ] <author> U. Kjaerulff, </author> <title> "A computational scheme for reasoning in dynamic probabilistic networks," </title> <booktitle> in Uncertainty in Artificial Intelligence(UAI-93), </booktitle> <pages> pp. 121-129, </pages> <year> 1993. </year>
Reference-contexts: The resulting and are placed into the appropriate bucket. The maximization over the decision variables can be accomplished subsequently by using maximization as the elimination operator. Clearly maximization and summation can be interleaved to some degree, allowing more efficient orderings. The algorithm in <ref> [ Kjae 93 ] </ref> can be viewed as a variation of elim-meu tailored to dynamic probabilistic networks. As before, the algorithm's performance can be bounded as a function of the structure of the augmented graph.
Reference: [ TaSh 90 ] <author> J.A. Tatman and R.D. Shachter, </author> <title> "Dynamic programming and influence diagrams," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 20 (1990): </journal> <pages> 365-379. </pages>
Reference-contexts: The ideas underlying the algorithms we present are not new, and the role of dynamic programming in probabilistic reasoning has already been made explicit in the context of influence diagrams <ref> [ TaSh 90 ] </ref> . What we provide here is a concise and uniform exposition across many tasks, which will facilitate transfer of ideas between areas of research. From a practical point of view, bucket elimination is very easy to implement, since structure building is not separated from inference propagation.
References-found: 25

