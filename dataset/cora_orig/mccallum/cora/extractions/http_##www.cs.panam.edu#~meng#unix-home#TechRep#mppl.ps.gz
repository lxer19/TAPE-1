URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/mppl.ps.gz
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/
Root-URL: http://www.cs.panam.edu
Email: gellerich@informatik.uni-stuttgart.de gutzmann@informatik.uni-jena.de  
Title: Massively Parallel Programming Languages A Classification of Design Approaches  
Author: Wolfgang Gellerich Michael M. Gutzmann 
Address: D-70565 Stuttgart, Germany, D-07743 Jena, Germany  
Affiliation: Department of Computer Science Department of Computer Science University of Stuttgart University of Jena  
Abstract: This paper presents the results of a study in which we examined about 50 parallel programming languages in order to detect typical approaches towards supporting massive parallelism. Based on a classification into nine classes, semantic properties affecting the development of parallel programs are compared. From a consideration of the general function of programming languages in software engineering, we derive basic requirements on parallel languages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W.B. Ackerman. </author> <title> Data Flow Languages. </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 14-25, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Probably, no lan-guage providing a fixed set of skeletons can expected to be general. It has also been suggested to extend functional languages by explicitly parallel constructs to support data-parallel or message-passing computation. Class 7: Data-flow Languages. Data-flow languages (DFL) <ref> [20, 1] </ref> were originally developed to program data-flow architectures which provide parallelism at operator level and require appropriate languages. The central idea in DFL is a value-oriented or applicative view of computation: any operations are considered to operate on values rather than on (abstractions of) memory cells.
Reference: [2] <institution> Ada 95 Reference Manual. Intermetrics, Inc., </institution> <year> 1995. ANSI/ISO/IEC-8652:1995. </year>
Reference-contexts: We were interested in languages that utilize massive parallelism to achieve high performance and therefore excluded approaches that only provide pseudo-parallel concepts primarily for simulation purposes (e.g., coroutines in Modula-2) as well as languages which provide operating-system-like large grain pro cesses (e.g., tasks in Ada <ref> [2, 6, 12] </ref>. Furthermore, we concentrated on approaches that concern languages design and we will mention approaches that basically provide a set of communication routines (e.g. Linda, PVM) only for completeness. <p> [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada <ref> [2, 6, 12] </ref> 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational
Reference: [3] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <title> Compilers. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: As we do not consider assembly languages, code generation is done by the compiler. A compiler performing the analysis step will usually do traditional data-flow analysis and dependence analysis for arrays. This is preceded by control flow analysis if the source language is based on a control flow model <ref> [3, 67, 69, 68] </ref>. In the case of arrays with affine dependence structures, powerful strategies for automatized time/space mapping exist [23, 24, 42]. Our classification and evaluation of parallel languages is based on their support for requirements from software engineering and compiler construction.
Reference: [4] <author> R.G. Babb II and A.H. Karp. </author> <title> A Comparison of 12 Parallel Fortran Dialects. </title> <journal> IEEE Software, </journal> <volume> 5 </volume> <pages> 52-67, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: But, if the compiler can not prove the independence of some statements, it must enforce sequential execution. To achieve substantial efficiency, the programmer must design (or restructure existing) code according to the compiler's parallelization capabilities. Characteristic examples are given in [55] and <ref> [4] </ref>. However, there has been significant progress in the field of automatic parallelization [67, 69, 68], so that today's systems can handle many situations that were not parallelized by early compilers.
Reference: [5] <author> P. Bailey, M. Newwy, D. Sitsky, and R. Stanton. </author> <title> Supporting Coarse and Fine Grain Parallelism in an Extension of ML. </title> <editor> In B. Buchberger and J. Volkert, editors, </editor> <booktitle> Parallel Processing: CONPAR 94 - VAPP VI, </booktitle> <pages> pages 593-604. </pages> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML <ref> [5] </ref> 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to <p> The basic idea is to provide a set of high order functions which capture characteristic "algorithmic structures" found in parallel algorithms, such as divide-and-conquer. These templates for parallel algorithms are then instantiated by the programmer. Recent implementations, e.g., of ML extended by skeletons, yielded good performance <ref> [5] </ref>. However, the set of skeletons required to implement all possible algorithms is large (if not infinite). Probably, no lan-guage providing a fixed set of skeletons can expected to be general. It has also been suggested to extend functional languages by explicitly parallel constructs to support data-parallel or message-passing computation.
Reference: [6] <author> J. Barnes. </author> <title> Programming in Ada 95. </title> <publisher> Addison Wesley, </publisher> <year> 1995. </year>
Reference-contexts: We were interested in languages that utilize massive parallelism to achieve high performance and therefore excluded approaches that only provide pseudo-parallel concepts primarily for simulation purposes (e.g., coroutines in Modula-2) as well as languages which provide operating-system-like large grain pro cesses (e.g., tasks in Ada <ref> [2, 6, 12] </ref>. Furthermore, we concentrated on approaches that concern languages design and we will mention approaches that basically provide a set of communication routines (e.g. Linda, PVM) only for completeness. <p> [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada <ref> [2, 6, 12] </ref> 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational
Reference: [7] <author> J.E. Boillat, H. Burkhart, K.M. Decker, and P.G. </author> <title> Kropf. </title> <booktitle> Parallel Computing in the 1990's: Attacking the Software Problem. Physics Reports, </booktitle> <address> 207(3-5):141-165, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Phrases like "parallel software crisis" [52] or "software dilemma" <ref> [7] </ref> are commonly used when scientists discuss today's situation of parallel programming. To some degree, this is up to programming languages which provide only poor support for parallel software development as some of their properties "hinder advanced analysis and optimization" [68].
Reference: [8] <author> M. M. Brandis. </author> <title> Building an Optimizing Compiler for Oberon: Implications on Programming Language Design. </title> <editor> In P. Schulthess, editor, </editor> <booktitle> Advances in Modular Languages, </booktitle> <pages> pages 123-135. </pages> <address> Universitaetsverlag Ulm, </address> <year> 1994. </year>
Reference-contexts: GOTO statements with statically determined targets can be handled but complicate the compiler as both, data dependences and the control flow representation must be updated when program transformations are applied. If only well-structured control structures can occur, control flow graphs can be avoided <ref> [8] </ref>. Task parallel and data parallel languages employ multi-control flow models that offer constructs like cobegin, explicitly parallel FORALL loops or processes. These allow splitting single control flow into multiple flows of control.
Reference: [9] <author> Th. Braunl. </author> <title> Massiv parallele Programmierung mit dem Parallaxis-Modell. </title> <publisher> Springer, </publisher> <year> 1990. </year>
Reference-contexts: to implement well known sequential languages like Fortran by using System-oriented languages 1. sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis <ref> [9, 10] </ref> Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended
Reference: [10] <author> Th. Braunl. </author> <title> Transparent massively parallel programming with parallaxis. </title> <journal> International Journal of Mini and Microcomputers, </journal> <volume> 14(2) </volume> <pages> 82-87, </pages> <year> 1992. </year>
Reference-contexts: to implement well known sequential languages like Fortran by using System-oriented languages 1. sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis <ref> [9, 10] </ref> Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended
Reference: [11] <author> Th. Braunl. Parallaxis-III: </author> <title> A structured data-parallel programming language. </title> <booktitle> In ICA3PP, </booktitle> <year> 1995. </year>
Reference-contexts: Some languages support both synchronous and asynchronous parallelism. This approach is always limited to data parallelism. The programmer must analyze the algorithm to find 1 Parallaxis-III <ref> [11] </ref> has considerably moved towards being a data parallel language. In particular, statements to be executed in parallel are no longer explicitly marked. those parts which can be executed in parallel. The compiler then maps the data parallel parts onto the parallel hardware structures.
Reference: [12] <author> A. Burns and A.J. Wellings. </author> <title> Ada 95: An Effective Concurrent Programming Language. </title> <booktitle> In Reliable Software Technologies Ada-Europe 1996, volume 1088 of LNCS, </booktitle> <pages> pages 58-77. </pages> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: We were interested in languages that utilize massive parallelism to achieve high performance and therefore excluded approaches that only provide pseudo-parallel concepts primarily for simulation purposes (e.g., coroutines in Modula-2) as well as languages which provide operating-system-like large grain pro cesses (e.g., tasks in Ada <ref> [2, 6, 12] </ref>. Furthermore, we concentrated on approaches that concern languages design and we will mention approaches that basically provide a set of communication routines (e.g. Linda, PVM) only for completeness. <p> [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada <ref> [2, 6, 12] </ref> 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational
Reference: [13] <author> Alan Burns. </author> <title> Programming in Occam 2. </title> <publisher> Addison-Wesley, </publisher> <address> Wokingham, </address> <year> 1988. </year>
Reference-contexts: parallel architectures emerged, the most obvious approach was to implement well known sequential languages like Fortran by using System-oriented languages 1. sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam <ref> [35, 13] </ref> SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6.
Reference: [14] <author> D.C. Cann. </author> <title> Retire Fortran? A Debate Rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: However, such experiments are found very rarely. We only know of one such study <ref> [14] </ref> which is cited in the context of data-flow languages. 2 Classes of Parallel Languages Table 1 shows the structure of our classification and gives some typical examples for each class. The distinction between different classes is made according to the support of parallelism. <p> The problem with strict adherence to applicative semantics is that the array value is constructed step by step by appending one element in every iteration. Although the problem can be solved by copy elimination in most cases <ref> [14] </ref>, the fact remains that, while the single assignment rule removes some of the problems related to imperative languages, it makes optimizations necessary. Another problem is that the single assignment rule would disallow iteration. <p> The properties of DFL turned out advantageous for implementation on any kind of parallel machine: e.g., Sisal is available on a large number of parallel computers and offers a high degree of portability. Significant Fortran programs have been rewritten in Sisal and yielded comparable performance <ref> [14] </ref>. Class 8: Equational Languages. A very problem-oriented design approach for a language intended for scientific and engineering applications is to adopt the notation commonly used in this area. Typically, algorithms used in scientific and engineering computations are based on (some variations of) multi-dimensional arrays [64].
Reference: [15] <author> D.C. Cann, J. Feo, and R. Oldehoeft. </author> <title> A Report on the Sisal Language Project. </title> <type> Technical Report UCRL-102440, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal <ref> [48, 15] </ref> 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. <p> However, the lack of mutable variables introduces new optimizations problems: a Sisal compiler with straight-forward implementation of applicative semantics generated code for a vector algorithm which took about an hour to execute while the Fortran equivalent executed in less than half a second on the same machine <ref> [15] </ref>. The problem with strict adherence to applicative semantics is that the array value is constructed step by step by appending one element in every iteration.
Reference: [16] <author> A. Cheese. </author> <title> Parallel execution of Parlog. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog <ref> [32, 18, 16] </ref>, GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach. <p> If either one or another way succeeds, the goal is proven. Algorithms for parallel unification have been considered, too. However, efficient implementation of parallel logic languages seems to be be difficult <ref> [16] </ref> and requires some mapping and load-balancing at run-time. There are major semantic difference between sequential Pro-log and its parallel versions as sequential prolog allows control of resolution order for efficiency [62].
Reference: [17] <author> M.I. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: It seems very difficult to implement such concepts on parallel machines in a way that yields the same speedup and efficiency as imperative languages do. A currently popular approach for the design of more efficient parallel functional languages are algorithmic skeletons <ref> [41, 17, 19] </ref>. The basic idea is to provide a set of high order functions which capture characteristic "algorithmic structures" found in parallel algorithms, such as divide-and-conquer. These templates for parallel algorithms are then instantiated by the programmer. Recent implementations, e.g., of ML extended by skeletons, yielded good performance [5].
Reference: [18] <author> T. Conlon. </author> <title> Programming in PARLOG. </title> <publisher> Addison-Wesley, </publisher> <address> Wokingham, </address> <year> 1989. </year>
Reference-contexts: : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog <ref> [32, 18, 16] </ref>, GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach.
Reference: [19] <author> J. Darlington and A.J. </author> <title> Field. Parallel Programming Using Skeleton Functions. </title> <booktitle> In PARLE93, Parallel Architectures and Languages Europe, </booktitle> <year> 1993. </year>
Reference-contexts: It seems very difficult to implement such concepts on parallel machines in a way that yields the same speedup and efficiency as imperative languages do. A currently popular approach for the design of more efficient parallel functional languages are algorithmic skeletons <ref> [41, 17, 19] </ref>. The basic idea is to provide a set of high order functions which capture characteristic "algorithmic structures" found in parallel algorithms, such as divide-and-conquer. These templates for parallel algorithms are then instantiated by the programmer. Recent implementations, e.g., of ML extended by skeletons, yielded good performance [5].
Reference: [20] <author> J.B. Dennis. </author> <title> First Version of a Data Flow Procedure Language. </title> <editor> In B. Robinet, editor, </editor> <booktitle> Programming Symposium, volume 19 of LNCS. </booktitle> <publisher> Springer, </publisher> <year> 1974. </year>
Reference-contexts: Probably, no lan-guage providing a fixed set of skeletons can expected to be general. It has also been suggested to extend functional languages by explicitly parallel constructs to support data-parallel or message-passing computation. Class 7: Data-flow Languages. Data-flow languages (DFL) <ref> [20, 1] </ref> were originally developed to program data-flow architectures which provide parallelism at operator level and require appropriate languages. The central idea in DFL is a value-oriented or applicative view of computation: any operations are considered to operate on values rather than on (abstractions of) memory cells.
Reference: [21] <author> K. Ekanadham. </author> <title> A Perspective on Id. </title> <booktitle> (in [65]). </booktitle>
Reference-contexts: : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id <ref> [21, 51] </ref>, languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. <p> Another problem is that the single assignment rule would disallow iteration. However, many algorithms (e.g., array operations or converging approximations) can be expressed more naturally and more efficiently using loops and mutable variables. Therefore, data-flow languages like Sisal or Id provide "iterative con structs for efficiency" <ref> [21] </ref>, and allow multiple assignments yet inside loops. The properties of DFL turned out advantageous for implementation on any kind of parallel machine: e.g., Sisal is available on a large number of parallel computers and offers a high degree of portability.
Reference: [22] <author> W. Erhard and M.M. Gutzmann. </author> <title> ASL Portable Programmierung massiv paralleler Rechner. </title> <publisher> Teubner, </publisher> <year> 1995. </year>
Reference-contexts: 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL <ref> [33, 22] </ref>, EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach. <p> A very problem-oriented design approach for a language intended for scientific and engineering applications is to adopt the notation commonly used in this area. Typically, algorithms used in scientific and engineering computations are based on (some variations of) multi-dimensional arrays [64]. Equational languages such as ASL <ref> [30, 22] </ref> and EPL [64] organize programs around equations over arrays. These languages are primarily designed for scientific application and do not claim to be general. However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets [36]. <p> On the other hand, these restrictions strongly support dependence analysis as well as automatic time/space mapping and high-level algorithm optimization performed by algebraic transformations with respect to specific problem and hardware characteristics <ref> [22] </ref>. Class 9: Parallel Logic Languages. The mathematical model behind logic programming are predicate logic and the principle of resolution that allows inferring new propositions from given propositions. This requires repeated unification of the goals to be proven with facts and rules already known.
Reference: [23] <author> Paul Feautrier. </author> <title> Some Efficient Solutions to the Affine Scheduling Problem, Part I, One Dimensional Time. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(5), </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: This is preceded by control flow analysis if the source language is based on a control flow model [3, 67, 69, 68]. In the case of arrays with affine dependence structures, powerful strategies for automatized time/space mapping exist <ref> [23, 24, 42] </ref>. Our classification and evaluation of parallel languages is based on their support for requirements from software engineering and compiler construction. A study comparing 8 languages by programming a set of typical parallel applications, the so-called Salishan Problems was published by J.
Reference: [24] <author> Paul Feautrier. </author> <title> Some Efficient Solutions to the Affine Scheduling Problem, Part II, Multidimensional Time. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(6), </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: This is preceded by control flow analysis if the source language is based on a control flow model [3, 67, 69, 68]. In the case of arrays with affine dependence structures, powerful strategies for automatized time/space mapping exist <ref> [23, 24, 42] </ref>. Our classification and evaluation of parallel languages is based on their support for requirements from software engineering and compiler construction. A study comparing 8 languages by programming a set of typical parallel applications, the so-called Salishan Problems was published by J.
Reference: [25] <author> J.T. Feo. </author> <title> A Comparative Study of Parallel Programming Languages: The Salishan Problems . North-Holland, </title> <year> 1992. </year>
Reference-contexts: Our classification and evaluation of parallel languages is based on their support for requirements from software engineering and compiler construction. A study comparing 8 languages by programming a set of typical parallel applications, the so-called Salishan Problems was published by J. Feo <ref> [25] </ref>. 1.4 Run-Time Behaviour As languages are usually claimed to be "fast", it would be interesting to compare the run-time be-haviour of one problem programmed in different languages but running on the same machine speedup is the only reason for doing massively parallel computation.
Reference: [26] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D Language Specification. </title> <type> Technical report, </type> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D <ref> [26] </ref> : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog
Reference: [27] <author> A. Geist, A. Beguin, J. Dingarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: languages 1. sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM <ref> [27] </ref>, Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages
Reference: [28] <author> D. Gelernter, N. Carriero, Chandran S, and S. Chang. </author> <title> Parallel Programming in Linda. </title> <editor> In Douglas Degroot, editor, </editor> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 255-263. </pages> <publisher> Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda <ref> [28] </ref>, languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal
Reference: [29] <author> W. Gellerich. </author> <title> Charakteristische Eigenschaften und Konzepte von Programmiersprachen fur Parallelrech-ner. </title> <type> Master's thesis, </type> <institution> Universitat Erlangen-Nurnberg, </institution> <year> 1993. </year>
Reference-contexts: Our goal was to find out what kind of languages have emerged and what their properties and possible strengths and weaknesses are. We examined about 50 parallel programming languages to detect typical design approaches <ref> [29, 30] </ref>. In order to derive basic requirements for a "good" parallel language, we discuss properties of each language class and relate them to steps performed in program development and to major tasks performed by parallelizing compilers.
Reference: [30] <editor> W. Gellerich and M.M. Gutzmann. Konzepte von Parallelrechnersprachen im Vergleich zu den Entwurf-sprinzipien von ASL-2. </editor> <booktitle> In PARS Mitteilungen, </booktitle> <pages> pages 127-140, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Our goal was to find out what kind of languages have emerged and what their properties and possible strengths and weaknesses are. We examined about 50 parallel programming languages to detect typical design approaches <ref> [29, 30] </ref>. In order to derive basic requirements for a "good" parallel language, we discuss properties of each language class and relate them to steps performed in program development and to major tasks performed by parallelizing compilers. <p> A very problem-oriented design approach for a language intended for scientific and engineering applications is to adopt the notation commonly used in this area. Typically, algorithms used in scientific and engineering computations are based on (some variations of) multi-dimensional arrays [64]. Equational languages such as ASL <ref> [30, 22] </ref> and EPL [64] organize programs around equations over arrays. These languages are primarily designed for scientific application and do not claim to be general. However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets [36].
Reference: [31] <author> C. Ghezzi and M. Jazayeri. </author> <title> Programming Language Concepts. </title> <publisher> John Wiley and Sons, </publisher> <year> 1987. </year>
Reference-contexts: In contrast to this distinction, we present a classification according to the kind of support for parallel processing. For a discussion of general properties of programming languages, the reader is referred to textbooks like <ref> [31, 61] </ref>. Programming languages serve as interface between programmers and computers. <p> Access methods based on uniformly calculated addresses (e.g., as in C) are inapplicable here. An additional problem is the possibility of variable names to be aliases. Apart from well-known problems regarding program reliability and optimizations <ref> [31] </ref>, aliasing makes data dependence analysis considerably more difficult and may create artificial dependences. In the presence of pointers, data dependence analysis becomes NP-hard and can only be approximated [45]. But, if the compiler can not prove the independence of some statements, it must enforce sequential execution.
Reference: [32] <author> S. Gregory. </author> <title> Parallel logic programming in PARLOG : the language and its implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Wokingham, </address> <year> 1987. </year>
Reference-contexts: : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog <ref> [32, 18, 16] </ref>, GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach.
Reference: [33] <author> M.M. Gutzmann. Leistungsbewertung von massiv par-allelen Rechnermodellen. </author> <type> PhD thesis, </type> <institution> Universitat Erlangen-Nurnberg, </institution> <month> Sept. </month> <year> 1993. </year> <note> Arbeitsberichte des IMMD, Bd. 26, Nr. 13. </note>
Reference-contexts: 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL <ref> [33, 22] </ref>, EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach. <p> However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets [36]. The model behind ASL are recurrence equations [40, 44]. ASL was developed for static problems that have the same algorithmic power as primitive recursive functions <ref> [33] </ref>. Further properties of ASL are single assignment variables and the renunciation of pointers which results in the absence of side effects. Although these restrictions cause a loss of generality, most algorithms of the intended application area can still be expressed.
Reference: [34] <author> E.A. Heinz, W. F. Tichy, M. Philippsen, and P. Lukowicz. </author> <title> From Modula-2* to Efficient Parallel Code. </title> <type> Technical Report 21/92, </type> <institution> Universitat Karlsruhe, </institution> <year> 1992. </year>
Reference-contexts: [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* <ref> [34] </ref>, Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages :
Reference: [35] <author> C.A.R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice Hall, </publisher> <year> 1986. </year>
Reference-contexts: parallel architectures emerged, the most obvious approach was to implement well known sequential languages like Fortran by using System-oriented languages 1. sequential languages : CFT [56], PTRAN [60] 2. hardware specific : CFD [63, 54, 55], MPL [47] languages : DAP-Fortran [55] 3. architecture class oriented languages MIMD-based : Occam <ref> [35, 13] </ref> SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6.
Reference: [36] <author> C.M. Hoffmann and M.J. O'Donnell. </author> <title> Programming with Equations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 83-112, </pages> <year> 1982. </year>
Reference-contexts: These languages are primarily designed for scientific application and do not claim to be general. However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets <ref> [36] </ref>. The model behind ASL are recurrence equations [40, 44]. ASL was developed for static problems that have the same algorithmic power as primitive recursive functions [33]. Further properties of ASL are single assignment variables and the renunciation of pointers which results in the absence of side effects.
Reference: [37] <author> P. Hudak. </author> <title> Para-functional programming in Haskell. </title> <booktitle> (in [65]). </booktitle>
Reference-contexts: C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell <ref> [37, 38] </ref>, languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel <p> The term purely functional languages subsumes all languages that are based on Church's -calculus. This model is free from side effects and has no explicitly specified execution order. Due to these properties, it is claimed that purely functional languages should be "highly suited for parallel programming" <ref> [37] </ref>. In particular, function arguments can be evaluated in parallel. <p> This model is free from side effects and has no explicitly specified execution order. Due to these properties, it is claimed that purely functional languages should be "highly suited for parallel programming" [37]. In particular, function arguments can be evaluated in parallel. However, modern functional languages (e.g., Haskell <ref> [37, 38] </ref>) provide features like high order functions (allowing the combination existing functions into new ones and then apply these to arguments) and non-strict semantics (i.e., lazy evaluation allowing the specification of infinite data structures, which is also used to model input/output).
Reference: [38] <editor> P. Hudak, S.L. Peyton-Jones, and P.L. Wadler. </editor> <title> Report on the Programming Language Haskell: A Non-Strict, Purely Functional Language. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 16(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell <ref> [37, 38] </ref>, languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel <p> This model is free from side effects and has no explicitly specified execution order. Due to these properties, it is claimed that purely functional languages should be "highly suited for parallel programming" [37]. In particular, function arguments can be evaluated in parallel. However, modern functional languages (e.g., Haskell <ref> [37, 38] </ref>) provide features like high order functions (allowing the combination existing functions into new ones and then apply these to arguments) and non-strict semantics (i.e., lazy evaluation allowing the specification of infinite data structures, which is also used to model input/output).
Reference: [39] <author> M. Jacquemin and J.A. Yang. </author> <title> Crystal Reference Manual Version 3.0. </title> <publisher> Yale University Internal Report, </publisher> <pages> pages 1-13, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: C [46] languages : Actus [53, 55], Fortran D [26] : HPF [43] implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal <ref> [39] </ref> 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16], GHC [66] Table 1: Classification of parallel languages parallelizing compilers, in order to use existing sequential code on parallel machines. Unfortunately, there are several problems with this approach.
Reference: [40] <author> R.M. Karp, R.E. Miller, and S. Winograd. </author> <title> The Organization of Computations for Uniform Recurrence Equations. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 14(3) </volume> <pages> 563-590, </pages> <year> 1967. </year>
Reference-contexts: These languages are primarily designed for scientific application and do not claim to be general. However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets [36]. The model behind ASL are recurrence equations <ref> [40, 44] </ref>. ASL was developed for static problems that have the same algorithmic power as primitive recursive functions [33]. Further properties of ASL are single assignment variables and the renunciation of pointers which results in the absence of side effects.
Reference: [41] <author> P. Kelly. </author> <title> Functional Pogramming for Loosely-coupled Multiprocessors. </title> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: It seems very difficult to implement such concepts on parallel machines in a way that yields the same speedup and efficiency as imperative languages do. A currently popular approach for the design of more efficient parallel functional languages are algorithmic skeletons <ref> [41, 17, 19] </ref>. The basic idea is to provide a set of high order functions which capture characteristic "algorithmic structures" found in parallel algorithms, such as divide-and-conquer. These templates for parallel algorithms are then instantiated by the programmer. Recent implementations, e.g., of ML extended by skeletons, yielded good performance [5].
Reference: [42] <author> Wayne Kelly and William Pugh. </author> <title> Selecting Affine Mappings based on Performance Estimation. </title> <type> Technical Report CS-TR-3108, </type> <institution> Dept. of Computer Science, University of Maryland, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: This is preceded by control flow analysis if the source language is based on a control flow model [3, 67, 69, 68]. In the case of arrays with affine dependence structures, powerful strategies for automatized time/space mapping exist <ref> [23, 24, 42] </ref>. Our classification and evaluation of parallel languages is based on their support for requirements from software engineering and compiler construction. A study comparing 8 languages by programming a set of typical parallel applications, the so-called Salishan Problems was published by J.
Reference: [43] <author> C.H. Koelbel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: MIMD-based : Occam [35, 13] SIMD-based : C* [59, 58], Parallaxis [9, 10] Problem-oriented languages explicitly parallel 4. task parallel : PVM [27], Linda [28], languages : Ada [2, 6, 12] 5. data parallel : Modula-2* [34], Vector C [46] languages : Actus [53, 55], Fortran D [26] : HPF <ref> [43] </ref> implicitly parallel 6. functional : Haskell [37, 38], languages : extended ML [5] 7. data-flow : VAL [49], Id [21, 51], languages : Sisal [48, 15] 8. equational : ASL [33, 22], EPL [64], languages : Crystal [39] 9. logical : Concurrent Prolog [62] languages : Parlog [32, 18, 16],
Reference: [44] <author> P. M. Kogge and H. Stone. </author> <title> A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(8):786-793, </volume> <month> August </month> <year> 1973. </year>
Reference-contexts: These languages are primarily designed for scientific application and do not claim to be general. However, it has also been suggested to extend the equation-based approach by more general data structures like lists and sets [36]. The model behind ASL are recurrence equations <ref> [40, 44] </ref>. ASL was developed for static problems that have the same algorithmic power as primitive recursive functions [33]. Further properties of ASL are single assignment variables and the renunciation of pointers which results in the absence of side effects.
Reference: [45] <author> W. Landi and B.G. Ryder. </author> <title> Pointer-induced Aliasing: </title>
Reference-contexts: Apart from well-known problems regarding program reliability and optimizations [31], aliasing makes data dependence analysis considerably more difficult and may create artificial dependences. In the presence of pointers, data dependence analysis becomes NP-hard and can only be approximated <ref> [45] </ref>. But, if the compiler can not prove the independence of some statements, it must enforce sequential execution. To achieve substantial efficiency, the programmer must design (or restructure existing) code according to the compiler's parallelization capabilities. Characteristic examples are given in [55] and [4].
References-found: 45

