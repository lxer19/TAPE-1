URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-338.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: steve@media.mit.edu, picard@media.mit.edu  
Phone: (Tel) 617-253-0611 (Fax) 617-253-8874  
Title: Video Orbits of the Projective Group: A New Perspective on Image Mosaicing.  
Author: S. Mann and R. W. Picard 
Address: 20 Ames Street; Cambridge, MA 02139  
Affiliation: MIT Media Lab;  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 338 Submitted for publication Abstract We present a new technique for estimating the projective (homographic) coordinate transformation between pairs of images, taken with a camera that is free to pan, tilt, rotate about its optical axis, and zoom. The technique solves the problem for two cases of static scenes: images taken from the same location of an arbitrary 3-D scene, or images taken from arbitrary locations of a flat scene. A new algorithm is presented for the parameter estimation and applied to the task of constructing high resolution still images from video. This approach generalizes inter-frame camera motion estimation methods which have previously used an affine model and/or which have relied upon finding points of correspondence between the image frames. The new projective algorithm which operates directly on the image pixels is shown to be superior in accuracy and ability to enhance resolution. The proposed method works well on image data collected from both good-quality and poor-quality video under a wide variety of conditions (sunny, cloudy, day, night). This new fully-automatic technique is also shown to be robust to deviations from the assumptions of static scene and no parallax.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. B. J.L. Barron, </author> <title> D.J. Fleet, "Systems and experiment performance of optical flow techniques," </title> <journal> International journal of computer vision, </journal> <pages> pp. 43-77, </pages> <year> 1994. </year>
Reference-contexts: panning the camera while mosaicing (e.g. by making a panorama) and the spatial resolution is increased by zooming the camera and by combining overlapping frames from different viewpoints. 2 Background Hundreds of papers have been published on the problems of motion estimation and frame alignment. (For review and comparison, see <ref> [1] </ref>.) In this section we review the basic differences between coordinate transformations and emphasize the importance of using the "exact" 8-parameter projective coordinate transformation. 2.1 Coordinate transformations A coordinate transformation maps the image coordinates, x = [x; y] T to a new set of coordinates, x 0 = [x 0 ; <p> task is not to deal with the 2-D translation flow, but with the 2-D projective flow, estimating the eight parameters in the coordinate transformation: x = x 0 A [x; y] T + b = c T x + 1 The desired eight scalar parameters are denoted by p = <ref> [A; b; c; 1] </ref>, A 2 IR 2fi2 , b 2 IR 2fi1 , and c 2 IR 2fi1 .
Reference: [2] <author> A. Tekalp, M. Ozkan, and M. Sezan, </author> <title> "High-resolution image reconstruction from lower-resolution image sequences and space-varying image restoration," </title> <booktitle> in Proc. of the Int. Conf. on Acoust., Speech and Sig. Proc., </booktitle> <address> (San Francisco, CA), </address> <pages> pp. </pages> <address> III-169, </address> <publisher> IEEE, </publisher> <month> Mar. </month> <pages> 23-26, </pages> <year> 1992. </year>
Reference-contexts: The most common assumption (especially in motion estimation for coding, and optical flow for computer vision) is that the coordinate transformation between frames is translation. Tekalp, Ozkan, and Sezan <ref> [2] </ref> have applied this assumption to high-resolution image reconstruction. Although translation is the least constraining and simplest to implement of the six coordinate transformations in Table 1, it is poor at handling large changes due to camera zoom, rotation, pan and tilt.
Reference: [3] <author> Q. Zheng and R. Chellappa, </author> <title> "A Computational Vision Approach to Image Registration ," IEEE Transactions Image Processing, </title> <month> July </month> <year> 1993. </year> <pages> pages 311-325. </pages>
Reference-contexts: Tekalp, Ozkan, and Sezan [2] have applied this assumption to high-resolution image reconstruction. Although translation is the least constraining and simplest to implement of the six coordinate transformations in Table 1, it is poor at handling large changes due to camera zoom, rotation, pan and tilt. Zheng and Chellappa <ref> [3] </ref> considered a subset of the affine model | translation, rotation and scale | in image registration. Other researchers [4][5] have assumed affine motion (six parameters) between frames.
Reference: [4] <author> M. Irani and S. Peleg, </author> <title> "Improving Resolution by Image Registration," </title> <journal> CVGIP, </journal> <volume> vol. 53, </volume> <pages> pp. 231-239, </pages> <month> May </month> <year> 1991. </year>
Reference: [5] <author> L. Teodosio and W. Bender, </author> <title> "Salient video stills: Content and context preserved," </title> <booktitle> Proc. ACM Multimedia Conf., </booktitle> <year> 1993. </year>
Reference: [6] <author> G. Wolberg, </author> <title> Digital Image Warping. </title> <address> 10662 Los Vaqueros Circle, Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <address> 1990. </address> <publisher> IEEE Computer Society Press Monograph. </publisher>
Reference-contexts: The physical camera model fits exactly in the 8-parameter projective group; therefore, we know that "eight is enough." Hence, it is appealing to find an approximate model with only eight parameters. The 8-parameter bilinear model is perhaps the most widely-used <ref> [6] </ref> in the fields of image processing, medical imaging, remote sensing, and computer graphics. This model is easily obtained from the biquadratic model by removing the four x 2 and y 2 terms. <p> Then set h k = p 0;k ffi h. (This should have nearly the same effect as applying p k to h k1 , except that it will avoid additional interpolation and anti-aliasing errors you would get by resampling an already resampled image <ref> [6] </ref>). Repeat until either the error between h k and g falls below a threshold, or until some maximum number of iterations is achieved.
Reference: [7] <author> G. Adiv, </author> <title> "Determining 3D Motion and structure from optical flow generated by several moving objects," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <pages> pp. 304-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: This model is easily obtained from the biquadratic model by removing the four x 2 and y 2 terms. Although the resulting bilinear model captures the effect of converging lines, it completely fails to capture the effect of chirping. The 8-parameter pseudo-perspective model <ref> [7] </ref>) does, in fact, capture both the converging lines and the chirping of a projective coordinate transformation.
Reference: [8] <author> N. Navab and S. Mann, </author> <title> "Recovery of relative affine structure using the motion flow field of a rigid planar patch.," </title> <booktitle> Mustererkennung 1994, </booktitle> <address> Tagungsband., </address> <year> 1994. </year>
Reference-contexts: This model may be thought of as first, removal of two of the quadratic terms (bf q x 0 y 2 = q y 0 x 2 = 0), which results in a ten pa rameter model (the `q-chirp' of <ref> [8] </ref>) and then constraining the four remaining quadratic parameters to have two degrees of freedom.
Reference: [9] <author> R. Y. Tsai and T. S. Huang, </author> <title> "Estimating three-dimensional motion parameters of a rigid planar patch," </title> <journal> tassp, </journal> <volume> vol. ASSP-29, </volume> <pages> pp. 1147-1152, </pages> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: The parameters for this model have been solved by Tsai and Huang <ref> [9] </ref>, but their solution assumed that features had been identified in the two frames, along with their correspondences. In this paper, we present a simple featureless means of registering images. Other researchers have looked at projective estimation in the context of obtaining 3 D models. <p> We also assume that each element in the camera sensor array returns a quantity that is linearly proportional to the quantity of light received 4 . With these assumptions, the exact camera motion that can be recovered is summarized in Table 2. 2.3 Video orbits Tsai and Huang <ref> [9] </ref> pointed out that the elements of the projective group give the true camera motions with respect to a planar surface.
Reference: [10] <author> O. D. Faugeras and F. Lustman, </author> <title> "Motion and structure from motion in a piecewise planar environment," </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 485-508, </pages> <year> 1988. </year>
Reference-contexts: In this paper, we present a simple featureless means of registering images. Other researchers have looked at projective estimation in the context of obtaining 3 D models. Faugeras and Lustman <ref> [10] </ref>, Shashua and Navab [11], and Sawhney [12] have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [11] <author> A. Shashua and N. Navab, </author> <title> "Relative Affine: Theory and Application to 3D Reconstruction From Perspective Views.," </title> <booktitle> Proc. IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <month> June </month> <year> 1994. 1994. </year>
Reference-contexts: In this paper, we present a simple featureless means of registering images. Other researchers have looked at projective estimation in the context of obtaining 3 D models. Faugeras and Lustman [10], Shashua and Navab <ref> [11] </ref>, and Sawhney [12] have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [12] <author> H. Sawhney, </author> <title> "Simplifying motion and structure analysis using planar parallax and image warping," </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: In this paper, we present a simple featureless means of registering images. Other researchers have looked at projective estimation in the context of obtaining 3 D models. Faugeras and Lustman [10], Shashua and Navab [11], and Sawhney <ref> [12] </ref> have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [13] <author> R. Kumar, P. Anandan, and K. Hanna, </author> <title> "Shape recovery from multiple views: a parallax based approach," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> Nov </month> <year> 1984. </year> <title> 15 (a) (b) as each player traces out a stroboscopic-like path. The proposed method works robustly, despite the movement of players on the field. (a) Images are expressed in the coordinates of the first frame. (b) Images are expressed in a new useful coordinate system corresponding to none of the original frames. Note the slight distortion, due to the fact that football fields are not perfectly flat, but, rather, are raised slightly in the center. </title>
Reference-contexts: Kumar et al. <ref> [13] </ref> have also suggested registering frames of video by computing the flow along the epipolar lines, for which there is also an initial step of calculating the gross camera movement assuming no parallax. However, these methods have relied on feature correspondences, and were aimed at 3-D scene modeling.
Reference: [14] <author> S. Mann, </author> <title> "Compositing multiple pictures of the same scene," </title> <booktitle> in Proceedings of the 46th Annual IS&T Conference, </booktitle> <address> (Cambridge, </address> <institution> Massachusetts), The Society of Imaging Science and Technology, </institution> <month> May 9-14 </month> <year> 1993. </year>
Reference-contexts: Feature correspondences greatly simplify the problem; however, they also have many problems which we review below. The focus of this paper is a simple featureless approach to estimating the projective coordinate transformation between image frames. Two similar efforts exist to the new work presented here. Mann <ref> [14] </ref>, and Szeliski and Coughlan [15] independently proposed featureless registration and compositing of either pictures of a nearly flat object, or of pictures taken from approximately the same location.
Reference: [15] <author> R. Szeliski and J. Coughlan, </author> <title> "Hierarchical spline-based image registration," </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: The focus of this paper is a simple featureless approach to estimating the projective coordinate transformation between image frames. Two similar efforts exist to the new work presented here. Mann [14], and Szeliski and Coughlan <ref> [15] </ref> independently proposed featureless registration and compositing of either pictures of a nearly flat object, or of pictures taken from approximately the same location.
Reference: [16] <author> L. Campbell and A. Bobick, </author> <title> "Correcting for radial lens distortion: A simple implementation," </title> <type> TR 322, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <month> Apr </month> <year> 1995. </year>
Reference-contexts: We briefly review the basics of this theory, before presenting the new solution in the next section. 1 When using low cost wide-angle lenses, there is usually some barrel distortion which we correct using the method of <ref> [16] </ref>. 2 The principal point is where the optical axis intersects the film. 3 Isotropic means that magnification in the x and y directions is the same.
Reference: [17] <author> C. W. Wyckoff, </author> <title> "An experimental extended response film," </title> <booktitle> S.P.I.E. NEWSLETTER, </booktitle> <month> JUNE-JULY </month> <year> 1962. </year>
Reference: [18] <author> S. Mann and R. </author> <title> Picard, "Being `undigital' with digital cameras: Extending dynamic range by combining differently exposed pictures," </title> <type> Tech. Rep. 323, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Boston, Mas-sachusetts, </address> <year> 1994. </year> <note> Also appears, IS&T's 46th annual conference, </note> <month> May </month> <year> 1995. </year>
Reference: [19] <author> M. </author> <title> Artin, Algebra. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: The new set of images that results from applying all possible operators from the group to a particular image from the original set is called the orbit of that image under the group operation <ref> [19] </ref>. The equivalent two cases of Table 2 for this hypothetical "flatland" world of 2-D objects with 1-D pictures correspond to the following. In the first case a camera is at a fixed location, and free to zoom and pan. <p> 6= o 1 = (ax 1 + b)=(cx 1 + 1); 8x 1 6= o 1 (1) 5 In a 2-D world, the "camera" consists of a center of projection (pinhole "lens") and a line (1-D sensor array or 1-D "film"). 6 also known as a group action or G-set <ref> [19] </ref>. images that can be produced by acting on frame 1 with any element of the operator group. Assuming that frames 1 and 2 are from the same scene, frame 2 will be close to one of the possible projective coordinate transformations of frame 1.
Reference: [20] <author> S. Mann, </author> <title> "Wavelets and chirplets: Time-frequency perspectives, with applications," in Advances in Machine Vision, Strategies and Applications (P. </title> <editor> Archibald, ed.), </editor> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: The resulting two (1-D) frames taken by the camera, are related by the coordinate transformation from x 1 to x 2 , given by <ref> [20] </ref>: x 2 = z 2 tan (arctan (x 1 =z 1 ) ); 8x 1 6= o 1 = (ax 1 + b)=(cx 1 + 1); 8x 1 6= o 1 (1) 5 In a 2-D world, the "camera" consists of a center of projection (pinhole "lens") and a line <p> We should emphasize that c, the degree of perspective, has been given the interpretation of a chirp-rate <ref> [20] </ref>. The coordinate transformations of (1) form a group operation.
Reference: [21] <author> S. Mann and R. W. </author> <title> Picard, "Virtual bellows: constructing high-quality images from video," </title> <booktitle> in Proceedings of the IEEE first international conference on image processing, </booktitle> <address> (Austin, Texas), </address> <month> Nov. 13-16 </month> <year> 1994. </year>
Reference-contexts: The coordinate transformations of (1) form a group operation. This result, and the proof of this group's isomor-phism to the group corresponding to nonsingular projections of a flat object are given in <ref> [21] </ref>. 2.3.2 Projective group in 2-D coordinates The theory for the projective, affine, and translation groups also holds for the familiar 2-D images taken of the 3-D world. <p> For the 1-D affine coordinate transformation, the graph of the range coordinate as a function of the domain coordinate is a straight line; for the projective coordinate transformation, the graph of the range coordinate as a function of the domain coordinate is a rectangular hyperbola <ref> [21] </ref>. <p> Results of applying the proposed method to subpixel resolution enhancement are not presented in this paper but may be found in <ref> [21] </ref>. 5.2 Increasing "resolution" in the `pixel sense' system of frame (c), that is, the middle frame was chosen as the reference frame. <p> The frames of Fig 5 were brought into register using the differential parameter estimation, and "cemented" together seamlessly on a common canvas. "Cementing" involves piecing the frames together, for example, by median, mean, or trimmed mean, or combining on a sub-pixel grid <ref> [21] </ref>. (Trimmed mean was used here, but the particular method made little visible difference.) Fig 7 shows this result ("projective/projective"), with a comparison to two non-projective cases. The first comparison is to "affine/affine" where affine parameters were estimated (also multiscale) and used for the coordinate transformation.
Reference: [22] <author> R. Y. Tsai and T. S. Huang, </author> <title> "Multiframe image restoration and registration," </title> <publisher> ACM, </publisher> <year> 1984. </year>
Reference: [23] <author> T. S. Huang and A. Netravali, </author> <title> "Motion and structure from feature correspondences: a review," </title> <booktitle> Proc. IEEE, </booktitle> <month> Feb </month> <year> 1984. </year>
Reference: [24] <author> N. Navab and A. Shashua, </author> <title> "Algebraic Description of Relative Affine Structure: Connections to Euclidean, Affine and Projective Structure.," MIT Media Lab Memo No. </title> <type> 270, </type> <year> 1994. </year>
Reference-contexts: A major difficulty with feature-based methods is finding the features. Good features are often hand-selected, or computed, possibly with some degree of human intervention <ref> [24] </ref>. A second problem with features is their sensitivity to noise and occlusion.
Reference: [25] <author> H. L. </author> <title> Van Trees, Detection, Estimation, and Modulation Theory (Part I). </title> <publisher> John Wiley and Sons, </publisher> <year> 1968. </year>
Reference-contexts: The collection of all possible coordinate transfor mations, when applied to one of the images (say, h) serves 7 In the presence of additive white Gaussian noise, this method, also known as "matched filtering", leads to a max imum likelihood estimate of the parameters <ref> [25] </ref>. 5 to produce a family of templates to which the other image, g, can be compared.
Reference: [26] <author> R. Young, </author> <title> "Wavelet theory and its applications," </title> <year> 1993. </year>
Reference-contexts: A com-putationally efficient algorithm for the cross-wavelet transform has recently been presented <ref> [26] </ref>. (See [27] for a good review on wavelet-based estimation of affine coordinate transformations.) Just like the cross-correlation for the translation group, and the cross-wavelet for the affine group, the `cross-chirplet' can be used to find the parameters of a projective coordinate transformation in 1-D, searching over a 3-parameter space.
Reference: [27] <author> L. G. Weiss, </author> <title> "Wavelets and wideband correlation processing," </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pp. 13-32, </pages> <year> 1993. </year>
Reference-contexts: A com-putationally efficient algorithm for the cross-wavelet transform has recently been presented [26]. (See <ref> [27] </ref> for a good review on wavelet-based estimation of affine coordinate transformations.) Just like the cross-correlation for the translation group, and the cross-wavelet for the affine group, the `cross-chirplet' can be used to find the parameters of a projective coordinate transformation in 1-D, searching over a 3-parameter space.
Reference: [28] <author> S. Mann and S. Haykin, </author> <title> "The chirplet transform | a generalization of Gabor's logon transform," </title> <booktitle> Vision Interface '91, </booktitle> <month> June 3-7 </month> <year> 1991. </year>
Reference-contexts: The chirplet transform <ref> [28] </ref> is a general ization of the wavelet transform. The `projective-chirplet', has the form: h a;b;c = h ( cx + 1 where h is the `mother chirplet', analogous to the mother wavelet of wavelet theory.
Reference: [29] <author> S. Mann and S. Haykin, </author> <title> "Adaptive "Chirplet" Transform: an adaptive generalization of the wavelet transform," </title> <journal> Optical Engineering, </journal> <volume> vol. 31, </volume> <pages> pp. 1243-1256, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: With 2-D images, the search is over an 8-parameter space. A dense sampling of this volume is computation-ally prohibitive. Consequently, combinations of coarse-to-fine and iterative gradient-based search procedures are required. Adaptive variants of the chirplet transform have been previously reported in the literature <ref> [29] </ref>.
Reference: [30] <author> B. Horn and B. Schunk, </author> <title> "Determining Optical Flow," </title> <journal> Artificial Intelligence, </journal> <year> 1981. </year>
Reference-contexts: However, there are still many problems with the adaptive chirplet ap proach; thus, we now consider featureless methods based on spatiotemporal derivatives. 3.3 Featureless methods based on spatiotemporal derivatives 3.3.1 Optical flow - "translation flow" When the change from one image to another is small, optical flow <ref> [30] </ref> may be used. <p> Now we describe our algorithm for estimating the projective coordinate transformation for 2-D images using `p-flow'. We begin with the brightness constancy constraint equation for 2-D images <ref> [30] </ref> which gives the flow velocity components in the x and y directions, analogous to (6): u f E x + v f E y + E t 0 (16) As is well-known [30] the optical flow field in 2-D is underconstrained 8 . <p> We begin with the brightness constancy constraint equation for 2-D images <ref> [30] </ref> which gives the flow velocity components in the x and y directions, analogous to (6): u f E x + v f E y + E t 0 (16) As is well-known [30] the optical flow field in 2-D is underconstrained 8 .
Reference: [31] <author> J. Y. Wang and E. H. Adelson, </author> <title> "Spatio-Temopral Segmentation of Video Data ," in SPIE Image and Video Processing II, </title> <address> (San Jose, California), </address> <pages> pp. 120-128, </pages> <month> February 7-9 </month> <year> 1994. </year>
Reference-contexts: Wang and Adelson have proposed fitting an affine model to an optical flow field <ref> [31] </ref> of 2-D images. We briefly examine their approach with 1-D images (1-D images simplify analysis and comparison to other methods). Denote coordinates in the original image, g, by x, and in the new image, h, by x 0 .
Reference: [32] <author> J. Bergen, P. Burt, R. Hingorini, and S. Peleg, </author> <title> "Computing two motions from three frames," </title> <booktitle> in Proc. Third Int'l Conf. Comput. Vision, </booktitle> <address> (Osaka, Japan), </address> <pages> pp. 27-32, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: This results in what we call the "affine fit" equations: P P x x; x 1 a 1 P x E t =E x (8) Alternatively, the affine coordinate transformation may be directly incorporated into the brightness change constraint equation (5). Bergen et al. <ref> [32] </ref> have proposed this method, which we will call `affine flow', to distinguish it from the `affine fit' model of Wang and Adelson (8). Let us show how `affine flow' and `affine fit' are related. <p> An analogous variant of the `affine flow' method involves multiscale iteration as well, but in this case the iteration and multiscale hierarchy are incorporated directly into the affine estimator <ref> [32] </ref>. With the addition of mul-tiscale analysis, the `fit' and `flow' methods differ in additional respects beyond just the weighting. Our intuition and experience indicates that the direct multiscale `affine flow' performs better than the `affine fit' to the multiscale flow. <p> The summations are over the entire image (all x and y) if computing global motion (as is done in this paper), or over a windowed patch if computing local motion. This equation looks similar to the 6 fi 6 matrix equation presented in Bergen et al. <ref> [32] </ref>. In order to see how well the model describes the coordinate transformation between 2 images, say, g and h, one might warp 11 h to g, using the estimated motion model, and then compute some quantity that indicates how different the resampled version of h is from g.
Reference: [33] <author> Lucas and T. Kanade, </author> <title> "An iterative image-registration technique with an application to stereo vision ," in Image Understanding Workshop, </title> <journal> pp. </journal> <pages> 121-130, </pages> <year> 1981. </year>
Reference-contexts: Both our intuition and our practical experience tends to favor the `affine flow' weighting, but, more generally, perhaps we should ask "What is the best weighting?" (e.g. maybe there is an even better answer than the choice among these two). Lucas and Kanade <ref> [33] </ref>, among others, have considered weighting issues. Another approach to the `affine fit' involves computation of the optical flow field using the multiscale iterative method of Lucas and Kanade, and then fitting to the affine model.
Reference: [34] <author> J. Y. A. Wang and E. H. Adelson, </author> <title> "Representing moving images with layers," Image Processing Spec. Iss: </title> <booktitle> Image Seq. Compression, </booktitle> <volume> vol. 12, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Multiscale optical flow makes the assumption that blocks of the image are moving with pure translational motion , and then, paradoxically, the affine fit refutes this pure-translation assumption. However, `fit' provides some utility over `flow' when it is desired to segment the image into regions undergoing different motions <ref> [34] </ref>, or to gain robustness by rejecting portions of the image not obeying the assumed model. 3.3.3 `Projective fit' and `projective flow': new techniques Analogous to the "affine fit" and "affine flow" of the previous section, we now propose two new methods: `projective fit' and `projective flow'.
Reference: [35] <author> R. Wilson and G. H. </author> <title> Granlund, </title> <booktitle> "The Uncertainty Principle in Image Processing ," IEEE Transactions on Pattern Analysis and Machine Intelligence, </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: The algorithm (in Matlab on an HP 735) takes about six seconds per iteration for a pair of 320x240 images. 9 4.4 Exploiting commutativity for parameter estimation There is a fundamental uncertainty <ref> [35] </ref> involved in the simultaneous estimation of parameters of a noncommutative group, akin to the Heisenberg uncertainty relation of quantum mechanics. In contrast, for a commutative 12 group (in the absence of noise), we can obtain the exact coordinate transformation.
Reference: [36] <author> J. Segman, J. Rubinstein, and Y. Y. Zeevi, </author> <title> "The canonical coordinates method for pattern deformation: Theoretical and computational considerations," </title> <journal> pami, </journal> <volume> vol. 14, </volume> <pages> pp. 1171-1183, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: In contrast, for a commutative 12 group (in the absence of noise), we can obtain the exact coordinate transformation. Segman <ref> [36] </ref> considered the problem of estimating the parameters of a commutative group of coordinate transformations, in particular, the parameters of the affine group [37]. His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 13 [38].
Reference: [37] <author> J. Segman, </author> <title> "Fourier cross correlation and invariance transformations for an optimal recognition of func 16 tions deformed by affine groups," </title> <journal> josa, </journal> <volume> vol. 9, </volume> <pages> pp. </pages> <address> 895--902, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In contrast, for a commutative 12 group (in the absence of noise), we can obtain the exact coordinate transformation. Segman [36] considered the problem of estimating the parameters of a commutative group of coordinate transformations, in particular, the parameters of the affine group <ref> [37] </ref>. His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 13 [38]. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra [39].
Reference: [38] <author> J. Segman and W. Schempp, </author> <title> Two methods of incorporating scale in the Heisenberg group. </title> <note> 1993. JMIV special issue on wavelets. </note>
Reference-contexts: Segman [36] considered the problem of estimating the parameters of a commutative group of coordinate transformations, in particular, the parameters of the affine group [37]. His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 13 <ref> [38] </ref>. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra [39]. We exploit this commutativity for estimating the parameters of the noncommutative 2-D projective group by first estimating the parameters that commute.
Reference: [39] <author> B. Girod and D. Kuo, </author> <title> "Direct estimation of displacement histograms," </title> <booktitle> OSA Meeting on IMAGE UNDERSTANDING AND MACHINE VISION, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 13 [38]. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra <ref> [39] </ref>. We exploit this commutativity for estimating the parameters of the noncommutative 2-D projective group by first estimating the parameters that commute. For example, we improve performance if we first estimate the two parameters of translation, correct for the translation, and then proceed to estimate the eight projective parameters. <p> In practice, we run through the following `commutative initialization' before estimating the parameters of the projective group of coordinate transformations: 1. Assume that h is merely a translated version of g. (a) Estimate this translation using the method of Girod <ref> [39] </ref>. (b) Shift h by the amount indicated by this estimate. (c) Compute the MSE between the shifted h and g, and compare to the original MSE before shifting. (d) If an improvement has resulted, use the shifted h from now on. 2.
Reference: [40] <author> Y. Sheng, C. Lejeune, and H. H. Arsenault, </author> <title> "Frequency-domain Fourier-Mellin descriptors for invariant pattern recognition," Optical Engineering, </title> <month> May </month> <year> 1988. </year>
Reference-contexts: We can also simultaneously estimate both the isotropic-zoom and the rotation about the optical axis by applying a log-polar coordinate transformation followed by a translation estimator. This process may also be achieved by a direct application of the Fourier-Mellin transform <ref> [40] </ref>. Similarly, if the only difference between g and h is a camera pan, then the pan may be estimated through a coordinate transformation to cylindrical coordinates, followed by a translation estimator.
Reference: [41] <author> P. J. Burt and P. Anandan, </author> <title> "Image stabilization by registration to a reference mosaic," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> Nov </month> <year> 1984. </year>
Reference: [42] <author> M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt, </author> <title> "Real-time scene stabilization and mosaic construction," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> Nov </month> <year> 1984. </year>
Reference: [43] <author> S. Mann, </author> <title> "`See the world through my eyes,' a wearable wireless camera," </title> <note> 1995. http://www-white.media.mit.edu/~steve/netcam.html. </note>
Reference-contexts: In addition to consumer video, we believe there will be a large market in the future for small wearable wireless cameras. A prototype, the "wearable wireless webcam" (a head-mounted video camera uplinked to the Internet <ref> [43] </ref>) has provided one of the most extreme testbeds for the algorithms explored in our research, as it captures noisy transmitted video frames, grabbed by a camera attached to a 10 human head, free to move at the will of the individual.
Reference: [44] <author> S. Mann, </author> <title> "Video orbits of the projective group," </title> <note> 1995. http://www-white.media.mit.edu/~steve/orbits/orbits.html. </note>
Reference-contexts: To see this image in color, look at <ref> [44] </ref>, where additional examples (e.g. some where the algorithm still worked despite "crowd noise" where many people were entering and leaving the building) also appear.
Reference: [45] <author> S. Intille, </author> <title> "Computers watching football," 1995. http://www-white.media.mit.edu/vismod/demos/football/football.html. Additional technical notes, examples, computer code, and sequences are available from our world wide web pages, </title> <note> http://www-white.media.mit.edu/vismod/vismod.html and http://www-white.media.mit.edu/~steve/orbits/orbits.html </note> . 
Reference-contexts: In fact, a coordinate system other than one chosen from the input images could also be used. In particular, a coordinate system where parallel lines never meet, and periodic structures are "dechirped" (Fig 11 (b)) lends itself well to machine vision and player-tracking algorithms <ref> [45] </ref>.
References-found: 45

