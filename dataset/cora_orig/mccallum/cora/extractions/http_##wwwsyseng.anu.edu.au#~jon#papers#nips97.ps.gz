URL: http://wwwsyseng.anu.edu.au/~jon/papers/nips97.ps.gz
Refering-URL: 
Root-URL: 
Email: fjon,bartlettg@syseng.anu.edu.au  
Title: The Canonical Distortion Measure in Feature Space and 1-NN Classification  
Author: Jonathan Baxter and Peter Bartlett 
Keyword: Categories: Theory and Applications Oral Presentation Preferred  
Address: Canberra 0200, Australia  
Affiliation: Department of Systems Engineering Australian National University  
Abstract: We prove that the Canonical Distortion Measure (CDM) [2, 3] is the optimal distance measure to use for 1 nearest-neighbour (1-NN) classification, and show that it reduces to squared Euclidean distance in feature space for function classes that can be expressed as linear combinations of a fixed set of features. PAC-like bounds are given on the sample-complexity required to learn the CDM. An experiment is presented in which a neural network CDM was learnt for a Japanese OCR environ ment and then used to do 1-NN classification.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jonathan Baxter. </author> <title> Learning Internal Representations. </title> <booktitle> In Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <pages> pages 311320. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: Remark. The bound on m (the number of functions that need to be sampled from the environment) is independent of the complexity of the class G. This should be contrasted with related bias learning (or equivalently, learning to learn) results <ref> [1] </ref> in which the number of functions does depend on the complexity. The heuristic explanation for this is that here we are only learning a distance function on the input space (the CDM), whereas in bias learning we are learning an entire hypothesis space that is appropriate for the environment. <p> Instead of learning the CDM directly by minimizing (6), it was learnt implicitly by first learning a set of neural network features for the functions in the environment. The features were learnt using the method outlined in <ref> [1] </ref>, which essentially involves learning a set of classifiers with a common final hidden layer. The features were learnt on 400 out of the 3000 classifiers in the environment, using 90% of the data in training and 10% in testing.
Reference: [2] <author> Jonathan Baxter. </author> <title> The Canonical Metric for Vector Quantisation. </title> <type> Technical Report NeuroColt Technical Report 047, </type> <institution> Royal Holloway College, University of London, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The CDM was introduced in <ref> [2, 3] </ref>, where it was analysed primarily from a vector quantization perspective. In particular, the CDM was proved to be the optimal distortion measure to use in vector quantization, in the sense of producing the best approximations to the functions in the environment F.
Reference: [3] <author> Jonathan Baxter. </author> <title> The Canonical Distortion Measure for Vector Quantization and Function Approximation. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <month> July </month> <year> 1997. </year> <note> To Appear. </note>
Reference-contexts: The CDM was introduced in <ref> [2, 3] </ref>, where it was analysed primarily from a vector quantization perspective. In particular, the CDM was proved to be the optimal distortion measure to use in vector quantization, in the sense of producing the best approximations to the functions in the environment F. <p> In particular, the CDM was proved to be the optimal distortion measure to use in vector quantization, in the sense of producing the best approximations to the functions in the environment F. In <ref> [3] </ref> some experimental results were also presented (in a toy domain) showing how the CDM may be learnt. fl The first author (and the author to whom correspondence should be addressed) was supported in part by EPSRC grants #K70366 and #K70373 The purpose of this paper is to investigate the utility <p> In <ref> [3] </ref> an experiment was presented in which G was a neural network class and (6) was minimized directly by gradient descent. <p> Hence in these cases learning the CDM is a more effective method of learning to learn. 5 EXPERIMENT: JAPANESE OCR To verify the optimality of the CDM for 1-NN classification, and also to show how it can be learnt in a non-trivial domain (only a toy example was given in <ref> [3] </ref>), the CDM was learnt for a Japanese OCR environment. Specifically, there were 3018 functions f in the environment F , each one a classifier for a different Kanji character.
Reference: [4] <author> W S Lee, P L Bartlett, and R C Williamson. </author> <title> Efficient agnostic learning of neural networks with bounded fan-in. </title> <journal> IEEE Transactions on Information Theory, </journal> <year> 1997. </year>
Reference-contexts: So by standard results from real-valued function learning with squared loss <ref> [4] </ref>: Pr &lt; x i : sup fi fi fi 2 n=2 X fi i (j) ) (x i (j) ; x 0 fl 2 fi fi fi &gt; 2 = 4N 48B 2 ; G exp 256B 2 : Hence, by the union bound, Pr x : sup j ^er
Reference: [5] <author> S.N. Srihari, T. Hong, and Z. Shi. Cherry Blossom: </author> <title> A System for Reading Unconstrained Handwritten Page Images. </title> <booktitle> In Symposium on Document Image Understanding Technology (SDIUT), </booktitle> <year> 1997. </year>
Reference-contexts: The misclassification error was a surprisingly low 7.5%. The 7.5% error compares favourably with the 4.8% error achieved on the same data by the CEDAR group, using a carefully selected feature set and a hand-tailored nearest-neighbour routine <ref> [5] </ref>. In our case the distance measure was learnt from raw-data input, and has not been the subject of any optimization or tweaking.
References-found: 5

