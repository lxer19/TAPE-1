URL: http://www.cs.umd.edu/users/gagan/papers/pldi95.ps
Refering-URL: http://www.cs.umd.edu/users/gagan/pub.html
Root-URL: 
Email: fgagan, saltz,rajag@cs.umd.edu  
Phone: (301)-405-2756  
Title: Interprocedural Partial Redundancy Elimination and Its Application To Distributed Memory Compilation  
Author: Gagan Agrawal and Joel Saltz and Raja Das 
Address: College Park, MD 20742  
Affiliation: UMIACS and Department of Computer Science University of Maryland  
Abstract: Partial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural com Euler Solver on 10K mesh: 20 iterations No. of No IP opt. IP opt. IP opt. Procs. </title> <journal> preproc. stmts. comm. </journal> <volume> stmt. </volume> <booktitle> 2 47.74 30.26 29.75 8 17.45 9.85 8.51 32 12.72 8.92 5.09 munication optimizations for distributed memory compilation. In Proceedings of the 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-299, </pages> <month> August </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3264. </note>
Reference-contexts: Call Q (a) End Sched (a,c) Call P (a,c) If cond then Call Q (c) Procedure R (y,z) Sched (a,c) ..other computations .. Endif End Call R (a,c) End and cs6 to determine final placement tributed memory compilation <ref> [1] </ref>, but no formal details of the scheme or empirical evaluation was presented. We compare our work with efforts on other flow-sensitive interprocedural problems. Several different program representations have been used for different flow-sensitive interprocedural problems.
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Compiler analysis has been developed for analyzing the data access patterns associated with a given parallel loop and inserting calls to appropriate communication preprocessing routines and collective communication routines <ref> [2, 3, 9] </ref>. After such an initial analysis at a single parallel loop or a single procedure level, placement of these statements must be optimized interprocedu-rally.
Reference: [3] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for paral-lelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: Compiler analysis has been developed for analyzing the data access patterns associated with a given parallel loop and inserting calls to appropriate communication preprocessing routines and collective communication routines <ref> [2, 3, 9] </ref>. After such an initial analysis at a single parallel loop or a single procedure level, placement of these statements must be optimized interprocedu-rally.
Reference: [4] <author> Michael Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in <ref> [4] </ref>. Recompilation in a compiler performing interprocedural analysis has been investigated in [5]. 8 Conclusions In this paper we have addressed the problem of performing partial redundancy elimination interprocedu-rally. This problem was initially motivated by the problem of placement of communication preprocessing statements in distributed memory compilation.
Reference: [5] <author> Michael Burke and Linda Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(3) </volume> <pages> 367-399, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [4]. Recompilation in a compiler performing interprocedural analysis has been investigated in <ref> [5] </ref>. 8 Conclusions In this paper we have addressed the problem of performing partial redundancy elimination interprocedu-rally. This problem was initially motivated by the problem of placement of communication preprocessing statements in distributed memory compilation. We have developed an interprocedural partial redundancy elimination (IPRE).
Reference: [6] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge. Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph <ref> [6] </ref> and Inter-procedural Flow Graph used by Soffa et al. [16]. FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [4].
Reference: [7] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In absence of aliasing, this information can easily be computed by flow-insensitive interprocedural analysis in time linear to the size of call graph of the program <ref> [7] </ref>. This information is used by the CMOD cs function defined later. 4.2 Candidates for Placement We consider only the placement of pure functions.
Reference: [8] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Pon-nusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids <ref> [8] </ref>, a code which accesses data through indirection arrays in parallel loops. The existing compiler for irregular applications [9, 14] generated calls to PARTI routines for communication preprocessing and collective communication [24], but did not perform any in-terprocedural placement of these statements.
Reference: [9] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: Compiler analysis has been developed for analyzing the data access patterns associated with a given parallel loop and inserting calls to appropriate communication preprocessing routines and collective communication routines <ref> [2, 3, 9] </ref>. After such an initial analysis at a single parallel loop or a single procedure level, placement of these statements must be optimized interprocedu-rally. <p> We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids [8], a code which accesses data through indirection arrays in parallel loops. The existing compiler for irregular applications <ref> [9, 14] </ref> generated calls to PARTI routines for communication preprocessing and collective communication [24], but did not perform any in-terprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11.
Reference: [10] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 20, 19, 25] </ref>, but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedu-rally. <p> We believe that it can be applied interprocedurally to optimize placement of communication preprocessing statements and communication statements. 3 Intraprocedural Redundancy Elimination The details of interprocedural redundancy elimination we present are derived from the intraprocedural node based method of Dhamdhere <ref> [10] </ref>, also referred to as Modified Morel Renvoise Algorithm (MMRA). Detailed data flow equations and the meaning of the terms used are given in the appendix.
Reference: [11] <author> K. Drechsler and M. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "Global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 20, 19, 25] </ref>, but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedu-rally.
Reference: [12] <author> Manish Gupta, Edith Schonberg, and Harini Srini-vasan. </author> <title> A unified data flow framework for optimizing communication. </title> <booktitle> In Proceedings of Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: It encompasses traditional optimizations like invariant code motion and redundant computation elimination. It is widely used in optimizing compilers for performing common subex-pression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [12, 15] </ref>. A number of schemes for partial redundancy elimination have been proposed in literature [10, 11, 20, 19, 25], but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure.
Reference: [13] <author> Mary Hall, John M Mellor Crummey, Alan Carle, and Rene G Rodriguez. FIAT: </author> <title> A framework for interpro-cedural analysis and transformations. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 522-545. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [6] and Inter-procedural Flow Graph used by Soffa et al. [16]. FIAT has been introduced as a general framework for performing interprocedural analysis <ref> [13] </ref>, but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [4]. Recompilation in a compiler performing interprocedural analysis has been investigated in [5]. 8 Conclusions In this paper we have addressed the problem of performing partial redundancy elimination interprocedu-rally.
Reference: [14] <author> Reinhard v. Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as CRPC Technical Report CRPC-TR93339-S. </note>
Reference-contexts: We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids [8], a code which accesses data through indirection arrays in parallel loops. The existing compiler for irregular applications <ref> [9, 14] </ref> generated calls to PARTI routines for communication preprocessing and collective communication [24], but did not perform any in-terprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11.
Reference: [15] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. </volume> <pages> 6. </pages>
Reference-contexts: It encompasses traditional optimizations like invariant code motion and redundant computation elimination. It is widely used in optimizing compilers for performing common subex-pression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [12, 15] </ref>. A number of schemes for partial redundancy elimination have been proposed in literature [10, 11, 20, 19, 25], but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure.
Reference: [16] <author> Mary Jean Harrold and Mary Lou Soffa. </author> <title> Efficient computation of interprocedural definition-use chains. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(2) </volume> <pages> 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [6] and Inter-procedural Flow Graph used by Soffa et al. <ref> [16] </ref>. FIAT has been introduced as a general framework for performing interprocedural analysis [13], but is more targeted towards flow-insensitive problems. Interval based approach for solving interprocedural data flow equations has been investigated in [4].
Reference: [17] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Our method is applicable to arbitrary recursive programs and arbitrary control flow within each procedure. We have used interprocedural partial redundancy elimination for optimizing placement of communication statements and communication preprocessing statements in distributed memory compilation. We have implemented our scheme using the the existing Fortran D compilation system <ref> [17] </ref> as infrastructure. We have shown significant performance gains by optimizing placement of communication preprocessing statements. The rest of this paper is organized as follows. <p> The optimized program is shown in Figure 10. 6 Discussion 6.1 Effectiveness of the Scheme We have implemented a preliminary version of our scheme using the existing Fortran D compilation system developed at Rice University <ref> [17] </ref> as the necessary infrastructure. We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids [8], a code which accesses data through indirection arrays in parallel loops.
Reference: [18] <author> J. Knoop and Steffan B. </author> <title> Efficient interprocedural bit-vector data flow analyses: A uniform interprocedural framework. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Kiel, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Knoop et al. extend a scheme for performing earliest possible code motion interprocedurally <ref> [18] </ref>. The main limitation of their work is that if any of the influ-encers of a candidate is a formal parameter, then the candidate is not considered for placement outside procedure boundary (since no renaming of influencers is done).
Reference: [19] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code mo-tion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 20, 19, 25] </ref>, but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedu-rally.
Reference: [20] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 20, 19, 25] </ref>, but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedu-rally. <p> Lemma 2 No insertion is made in any block of code for which INSERT is ?. Using the above two lemmas, the correctness and safety properties of the interprocedural scheme can be established in the same way as the correctness and safety of the original intraprocedural scheme <ref> [20] </ref>. Theorem 1 (Correctness) After insertion of new computations, the computation of the candidate C becomes redundant in an edge satisfying ANTLOC C = Infl i and PPIN C = Infl i .
Reference: [21] <author> E. Morel and C. </author> <title> Renvoise. Interprocedural elimination of partial redundancies. In Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: If the number of egdes in the graph is small, the time required for solution will be small. 7 Related Work We are aware of two efforts on performing interproce-dural partial redundancy elimination. Morel and Ren-voise briefly discuss how their scheme can be extended interprocedurally <ref> [21] </ref>. Their solution is hueristic in nature, and no formal details are available for their in-terprocedural scheme. Their work is restricted to the programs whose call graph is acyclic.
Reference: [22] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Secondly, calling context is not always preserved in propagating partial availability information. Precise computation of partial availability can be expensive, it will require a detailed representation like Myer's Super-Graph <ref> [22] </ref> and use of stacks and/or graph reachability for preserving calling context [23]. Our computation of partial availability still allows loop invariant code motion and redundant computation elimination. <p> We compare our work with efforts on other flow-sensitive interprocedural problems. Several different program representations have been used for different flow-sensitive interprocedural problems. Myer has suggested concept of SuperGraph <ref> [22] </ref> which is constructed by linking control flow graphs of subroutines by inserting edges from call site in the caller to start node in callee. The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge.
Reference: [23] <author> Thomas Reps, Susan Horowitz, and Mooly Sagiv. </author> <title> Precise interprocedural dataflow analysis via graph reach-ability. </title> <booktitle> In Conference Record of the Fourteenth Annual ACM SIGACT/SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Secondly, calling context is not always preserved in propagating partial availability information. Precise computation of partial availability can be expensive, it will require a detailed representation like Myer's Super-Graph [22] and use of stacks and/or graph reachability for preserving calling context <ref> [23] </ref>. Our computation of partial availability still allows loop invariant code motion and redundant computation elimination.
Reference: [24] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In these cases, communication can be optimized by placing a preprocessing statement, which determines the set of data elements to be communicated between the processors at runtime. The preprocessing statement stores this information in a data-structure called communication schedule <ref> [24] </ref>. A collective communication routine then performs the data movement, using the information in the communication schedule. This ensures that for a parallel loop, each processor packages the set of data elements it wants to send to any other processor in a single message. <p> We studied the effectiveness of our scheme in compiling an Euler solver for unstructured grids [8], a code which accesses data through indirection arrays in parallel loops. The existing compiler for irregular applications [9, 14] generated calls to PARTI routines for communication preprocessing and collective communication <ref> [24] </ref>, but did not perform any in-terprocedural placement of these statements. The performance achieved by the compiled code (before interprocedural optimizations) and the code after interprocedural optimizations is presented in Figure 11. The experimental results show that interproce-dural placement of communication preprocessing statements is a must for obtaining reasonable performance.
Reference: [25] <author> A. Sorkin. </author> <title> Some comments on "A solution to a problem with Morel and Renvoise's `Global optimization by suppression of partial redundancies' ". ACM Transactions on Programming Languages and Systems, </title> <booktitle> 11(4) </booktitle> <pages> 666-668, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [12, 15]. A number of schemes for partial redundancy elimination have been proposed in literature <ref> [10, 11, 20, 19, 25] </ref>, but are largely restricted to optimizing code within a single procedure. All these schemes perform data flow analysis on Control Flow Graph (CFG) of the procedure. In this paper, we address the problem of performing partial redundancy elimination interprocedu-rally.
References-found: 25

