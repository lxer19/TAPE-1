URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr91/tr91-009.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr91-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu  
Title: A Concurrent Fast-Fits Memory Manager  
Author: Theodore Johnson 
Date: September 12, 1991  
Address: TR91-009  Florida  
Affiliation: University of Florida, Dept. of CIS Electronic  Dept. of Computer and Information Science, University of  
Abstract: Shared memory multiprocessor systems need efficient dynamic storage allocators, both for system purposes and to support parallel programs. Most memory manager algorithms are based either on a free list, which provides efficient memory use, or on a buddy system, which provides fast allocation and release. In this paper, we present two versions of a memory manager based on the fast fits algorithm, which keeps free memory blocks in a Cartesian tree. A fast fits memory manager provides both efficient memory usage, and fast allocate and release operations. The concurrent implementations of the fast fits algorithm range from a simple moderate concurrency solution to a more complex but high concurrency solution.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Aragon and R. Seidel. </author> <title> Randomized search trees. </title> <booktitle> In Proceedings of the 30th Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 540-545, </pages> <year> 1989. </year>
Reference-contexts: memory managers, in order to take advantage of the fast fits algorithm's properties of fast allocation and release, and efficient memory use. 1 as noted in the malloc manual page 2 2 The Fast Fits Memory Manager The fast fits memory manager keeps the free blocks in a Cartesian tree <ref> [19, 1] </ref>. A Cartesian tree is a binary tree that stores points from the two-dimensional plane. Each node in the tree consists of a single data item and pointers to two children. Each data item consists of an X and a Y coordinate and possibly some associated information.
Reference: [2] <author> R. Bayer and M. Schkolnick. </author> <title> Concurrency of operations on B-trees. </title> <journal> Acta Informatica, </journal> <volume> 9 </volume> <pages> 1-21, </pages> <year> 1977. </year>
Reference-contexts: The tree structure of fast fits also allows the use of concurrency control algorithms developed for tree data structures <ref> [2, 6] </ref>. The fast fits memory manager is used in a number of commercial operating systems, including SUN Microsystem Unix 1 . A number of concurrent free list algorithms have been proposed. <p> The simpler algorithm also serves as a base for understanding the more complex algorithm. 3.1 W-only Algorithm The first algorithm that we present places exclusive (W) locks, and uses lock-coupling (the child node must be locked before the parent can be unlocked) to prevent interference between operations <ref> [2] </ref>. The pseudocode for algorithm 1 is in the appendix. We modify the data structure by adding an additional node to the data structure, the anchor node (see figure 2). The anchor node allows the pointer to the root to be locked, and simplifies restructuring operations involving the root. <p> This is a stricter ordering than is necessary, since two operations might restructure different subtrees, and the ordering is only necessary among operations with conflicting read and write sets. The second algorithm that we propose makes an initial optimistic descent <ref> [2] </ref> in which it places shared (R) locks until it finds a node that will be involved in restructuring, at which point it places a write-upgrade lock (U lock), and exclusive locks after that. A lock compatibility chart is listed in table 1.
Reference: [3] <author> B. Bigler, S. Allan, and R. Oldehoeft. </author> <title> Parallel dynamic storage allocation. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 272-275. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: Memory is added to or removed from a free block using the fetch-and-add instruction, but blocks must be exclusively locked whenever a free block is added to or removed from the list. Bigler, Allan and Oldehoeft <ref> [3] </ref> compare three algorithms, one of which is a concurrent algorithm that searches the free list using lock-coupling (the successor block must be locked before the lock on the current block may be released).
Reference: [4] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse lu factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference-contexts: While this algorithm would be sufficient for a multiprogrammed uniprocessor, it can create a serial bottleneck in a parallel processor. An example application of parallel memory managers is parallel sparse matrix factorization algorithms <ref> [4, 5] </ref>. Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address.
Reference: [5] <author> I. S. Duff. </author> <title> Multiprocessing a sparse matrix code on the Alliant FX/8. </title> <journal> J. Comp. Appl. Math., </journal> <volume> 27 </volume> <pages> 229-239, </pages> <year> 1989. </year>
Reference-contexts: While this algorithm would be sufficient for a multiprogrammed uniprocessor, it can create a serial bottleneck in a parallel processor. An example application of parallel memory managers is parallel sparse matrix factorization algorithms <ref> [4, 5] </ref>. Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm [12], the free blocks are linked together in a list, ordered by starting address.
Reference: [6] <author> C.S. Ellis. </author> <title> Concurrent search and insertion in AVL trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-29(9):811-817, </volume> <year> 1980. </year>
Reference-contexts: The tree structure of fast fits also allows the use of concurrency control algorithms developed for tree data structures <ref> [2, 6] </ref>. The fast fits memory manager is used in a number of commercial operating systems, including SUN Microsystem Unix 1 . A number of concurrent free list algorithms have been proposed.
Reference: [7] <author> C.S. Ellis and T. Olson. </author> <title> Concurrent dynamic storage allocation look it up. </title> <booktitle> In Proceedings of the international Conference on Parallel Processing, </booktitle> <pages> pages 502-511, </pages> <year> 1987. </year>
Reference-contexts: The authors find that the concurrent algorithm is more appropriate for a parallel processing environment than are the serial algorithms. Ellis and Olson <ref> [7] </ref> propose two concurrent free list algorithms. Their first algorithm is similar to Stone's [18], but breaks the memory being managed into several segments, each of which has an independent free list.
Reference: [8] <author> R. Ford. </author> <title> Concurrent algorithms for real time memory management. </title> <journal> IEEE Software, </journal> <pages> pages 10-23, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Ellis and Olson's second algorithm greatly simplifies the locking performed on the free blocks, and keeps around empty blocks until it is guaranteed that no operation will read the header information for that block. Ford <ref> [8] </ref> discusses several real-time memory managers, comparing a locking approach to an optimistic approach. In Ford's algorithms, memory is kept in a free list, with an index to the most recently released blocks. The algorithms allow the release operations to finish quickly at the expense of the allocate operations.
Reference: [9] <author> A. Gottlieb, B. D. Lubachevsky, and L. Rudolph. </author> <title> Coordinating large numbers of processors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing. IEEE, </booktitle> <year> 1981. </year>
Reference-contexts: The fast fits memory manager is used in a number of commercial operating systems, including SUN Microsystem Unix 1 . A number of concurrent free list algorithms have been proposed. Stone [18] proposes a first-fit free list algorithm that uses the fetch-and-add instruction <ref> [9] </ref> and locking for concurrency control. Memory is added to or removed from a free block using the fetch-and-add instruction, but blocks must be exclusively locked whenever a free block is added to or removed from the list.
Reference: [10] <author> A. Gottlieb and J. Wilson. </author> <title> Using the buddy system for concurrent memory allocation. Ultracomputer System Software Note 6, </title> <institution> Courant Institute, </institution> <year> 1981. </year>
Reference-contexts: Ford found that the optimistic memory manager is superior to the locking approach because the critical sections could be much shorter, allowing high-priority operations to gain control of the critical sections sooner. Gottlieb and Wilson developed concurrent buddy systems that use fetch-and-add to coordinate processors. Their first algorithm <ref> [10, 20] </ref> considers a buddy system to be organized as a tree. A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree.
Reference: [11] <author> A. Gottlieb and J. Wilson. </author> <title> Parallelizing the usual buddy algorithm. Ultracomputer System Software Note 37, </title> <institution> Courant Institute, </institution> <year> 1982. </year>
Reference-contexts: A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. Their second algorithm <ref> [11, 20] </ref> is a concurrent version of the commonly described buddy algorithm. This algorithm uses semaphores to enforce concurrency control. All previously published concurrent memory manager algorithms are based on either a free list or a buddy system.
Reference: [12] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 1. </volume> <publisher> Addison Wesley, </publisher> <year> 1968. </year>
Reference-contexts: An example application of parallel memory managers is parallel sparse matrix factorization algorithms [4, 5]. Most heap memory management algorithms use one of two main methods: free lists and buddy systems. In a free list algorithm <ref> [12] </ref>, the free blocks are linked together in a list, ordered by starting address. Initially, all of the memory is free, and the free list consists of a block containing the entire memory. An allocate process searches the list until it finds the most promising block.
Reference: [13] <author> J. l. Peterson and T. A. Norman. </author> <title> Buddy systems. </title> <journal> Communications of the ACM, </journal> <volume> 20(6) </volume> <pages> 421-431, </pages> <year> 1977. </year>
Reference-contexts: One problem with a free list memory manager is that a process must search the free list in order to release or allocate a block of memory, and the free list might be quite long. An alternative is to use a 1 buddy system <ref> [13] </ref>. In a buddy system, memory blocks are available only as one of several fixed sizes. Each memory block has a buddy, with which it can combine and form a larger size block. The available sizes are determined by the choice of buddies.
Reference: [14] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Synchronization without contention. </title> <booktitle> In Fourth Intn's Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <year> 1991. </year>
Reference-contexts: to the W-only algorithm, trading variance in the execution time for increased concurrency. 7 For this implementation, we assume a spin-lock implementation of the R and W locks in which the head of the queue can be read by the processes, such as the one described by Mellor-Crummy and Scott <ref> [14] </ref>. The key to the simulation is the observation that in the RWU algorithm, at most one W lock will be in a node's lock queue at a time (except for the anchor, where there is no problem). The processes use the following protocol to place locks.
Reference: [15] <author> D. Shasha and N. Goodman. </author> <title> Concurrent search structure algorithms. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 53-90, </pages> <year> 1988. </year>
Reference-contexts: Since the operations place exclusive locks using the lock-coupling protocol, if O 1 &lt; O 2 , then O 1 locks the nodes on the intersecting path of O 1 and O 2 before O 2 does. Finally, let us define the keyrange <ref> [15] </ref> of a node to be the set of keys ((X; Y ) pairs) that can exist in the subtree rooted at the node.
Reference: [16] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proc. of the Ninth ACM Symposium of OPerating System Principles, </booktitle> <pages> pages 30-32, </pages> <year> 1983. </year>
Reference-contexts: Although buddy systems execute quickly, they are tend to waste space due to internal fragmentation. Fibonacci buddy systems were introduced to offer more block sizes and reduce internal fragmentation, but they suffer from external fragmentation due to unused block sizes. In <ref> [16, 17] </ref>, Stephenson proposes a fast free list memory management algorithm which he calls fast fits. In the fast fits algorithm, the memory blocks are linked into a Cartesian tree, with one coordinate being the physical location and the other being the size of the block.
Reference: [17] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1983. </year> <month> 10 </month>
Reference-contexts: Although buddy systems execute quickly, they are tend to waste space due to internal fragmentation. Fibonacci buddy systems were introduced to offer more block sizes and reduce internal fragmentation, but they suffer from external fragmentation due to unused block sizes. In <ref> [16, 17] </ref>, Stephenson proposes a fast free list memory management algorithm which he calls fast fits. In the fast fits algorithm, the memory blocks are linked into a Cartesian tree, with one coordinate being the physical location and the other being the size of the block.
Reference: [18] <author> H. Stone. </author> <title> Parallel memory allocation using the fetch-and-add instruction. </title> <type> Technical Report RC 9674, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1982. </year>
Reference-contexts: The fast fits memory manager is used in a number of commercial operating systems, including SUN Microsystem Unix 1 . A number of concurrent free list algorithms have been proposed. Stone <ref> [18] </ref> proposes a first-fit free list algorithm that uses the fetch-and-add instruction [9] and locking for concurrency control. Memory is added to or removed from a free block using the fetch-and-add instruction, but blocks must be exclusively locked whenever a free block is added to or removed from the list. <p> The authors find that the concurrent algorithm is more appropriate for a parallel processing environment than are the serial algorithms. Ellis and Olson [7] propose two concurrent free list algorithms. Their first algorithm is similar to Stone's <ref> [18] </ref>, but breaks the memory being managed into several segments, each of which has an independent free list. <p> If n 1 is a descendant of n 2 , then n 1 :Y n 2 :Y . One can implement search, insert and delete operations on a Cartesian tree using local rebalancing only <ref> [19, 18] </ref>. For a given set of points with unique X and unique Y values, there is a unique Cartesian tree that contains those points, so the rebalancing involves restoring the structural properties of the tree. <p> The allocate operation must follow an allocation strategy to find a suitable block of memory, then either allocate the entire free block (and delete the block), or allocate part of the free block (and demote the node) <ref> [18] </ref>. 3 The Algorithms In this section, we describe the concurrent fast fits memory manager algorithms. The algorithms increase in complexity, but also in the amount of concurrency allowed. <p> This information will simplify the task of finding a suitable child in the allocate procedure. The allocate operation must find a suitable free block from which to allocate. Stevenson <ref> [18] </ref> recommends the better-fit algorithm. To allocate, the better-fit algorithm starts at the root of the Cartesian tree and examines the children of the root. <p> Otherwise the node needs to be demoted (pushed lower in the tree) if it becomes smaller than one or both of its children. The delete procedure performs additional tasks related to the release operation, so we discuss it later. The demote procedure <ref> [18] </ref> restores the second property of the Cartesian tree (a block must be larger than its children). <p> There are two possible neighbors, and if they exist in the tree, they can be found by searching the tree for the address of the block being released until the neighbors are found or a leaf is reached <ref> [18] </ref>, so the release operation can search for the neighbors while it searches for the position to insert the released block. The largest neighbor might be larger or smaller than the block being released; correspondingly the release operation will take one of two actions.
Reference: [19] <author> J. Vuillemin. </author> <title> A unifying look at data structures. </title> <journal> Communications of the ACM, </journal> <volume> 23(4) </volume> <pages> 229-239, </pages> <year> 1980. </year>
Reference-contexts: memory managers, in order to take advantage of the fast fits algorithm's properties of fast allocation and release, and efficient memory use. 1 as noted in the malloc manual page 2 2 The Fast Fits Memory Manager The fast fits memory manager keeps the free blocks in a Cartesian tree <ref> [19, 1] </ref>. A Cartesian tree is a binary tree that stores points from the two-dimensional plane. Each node in the tree consists of a single data item and pointers to two children. Each data item consists of an X and a Y coordinate and possibly some associated information. <p> If n 1 is a descendant of n 2 , then n 1 :Y n 2 :Y . One can implement search, insert and delete operations on a Cartesian tree using local rebalancing only <ref> [19, 18] </ref>. For a given set of points with unique X and unique Y values, there is a unique Cartesian tree that contains those points, so the rebalancing involves restoring the structural properties of the tree. <p> For a given set of points with unique X and unique Y values, there is a unique Cartesian tree that contains those points, so the rebalancing involves restoring the structural properties of the tree. Vuillemin <ref> [19] </ref> shows that the expected number of comparisons needed to insert a point into a Cartesian tree is O (log n) (assuming that the Y components of the entries in the tree form a random permutation).
Reference: [20] <author> J. Wilson. </author> <title> Operating System Data Structures for Shared-memory MIMD Machines with Fetch-and-add. </title> <type> PhD thesis, </type> <institution> NYU, </institution> <year> 1988. </year> <type> 11 12 13 14 15 </type>
Reference-contexts: Ford found that the optimistic memory manager is superior to the locking approach because the critical sections could be much shorter, allowing high-priority operations to gain control of the critical sections sooner. Gottlieb and Wilson developed concurrent buddy systems that use fetch-and-add to coordinate processors. Their first algorithm <ref> [10, 20] </ref> considers a buddy system to be organized as a tree. A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. <p> A count of the number of blocks of each size that are contained in subtree rooted at a node is stored at each node. Concurrent allocators use this information to navigate the tree. Their second algorithm <ref> [11, 20] </ref> is a concurrent version of the commonly described buddy algorithm. This algorithm uses semaphores to enforce concurrency control. All previously published concurrent memory manager algorithms are based on either a free list or a buddy system.
References-found: 20

