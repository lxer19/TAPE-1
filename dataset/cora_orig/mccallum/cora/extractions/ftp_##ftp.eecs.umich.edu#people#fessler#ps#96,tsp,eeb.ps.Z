URL: ftp://ftp.eecs.umich.edu/people/fessler/ps/96,tsp,eeb.ps.Z
Refering-URL: http://www.eecs.umich.edu/~fessler/papers/jour.html
Root-URL: http://www.cs.umich.edu
Title: Exploring Estimator Bias-Variance Tradeoffs Using the Uniform CR Bound  
Author: Alfred O. Hero, Jeffrey A. Fessler, and Mohammad Usman 
Keyword: Key Words: parametric estimation, performance bounds, bias-variance plane, unachievable regions, inverse problems, image reconstruction.  
Date: JULY 1996 1  
Note: TO APPEAR IN IEEE TRANS. ON SIGNAL PROCESSING,  EDICS Number: SP 3.8.1  
Abstract: We introduce a plane, which we call the delta-sigma plane, that is indexed by the norm of the estimator bias gradient and the variance of the estimator. The norm of the bias gradient is related to the maximum variation in the estimator bias function over a neighborhood of parameter space. Using a uniform Cramer-Rao (CR) bound on estimator variance a delta-sigma tradeoff curve is specified which defines an "unachievable region" of the delta-sigma plane for a specified statistical model. In order to place an estimator on this plane for comparison to the delta-sigma tradeoff curve, the estimator variance, bias gradient, and bias gradient norm must be evaluated. We present a simple and accurate method for experimentally determining the bias gradient norm based on applying a bootstrap estimator to a sample mean constructed from the gradient of the log-likelihood. We demonstrate the methods developed in this paper for linear Gaussian and non-linear Poisson inverse problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. M. Kay, </author> <title> Modern Spectral Estimation: Theory and Application, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood-Cliffs N.J., </address> <year> 1988. </year>
Reference-contexts: From b and 2 one can derive other important measures such as signal-to-noise-ratio SN R = jt + b j 2 = 2 , coefficient of variation 1=SN R, and generalized MSE = ffg 1 (b ) + (1 ff)g 2 ( ), where ff 2 <ref> [0; 1] </ref> and g 1 ; g 2 are non-negative functions. The generalized MSE has been used in response surface design [21] and in minimum bias and variance estimation for non linear regression models [22], [23]. <p> For example when estimation of image contrast is of interest, spatially homogeneous biases may be tolerable and C may be chosen to be of rank n 1 having the vector 1 = <ref> [1; : : : ; 1] </ref> T in its nullspace. Let B (; ffi), d min and g () be as defined in Theorem 1. Assume that C is non-negative definite but F + Y + C is positive definite for 0 &lt; &lt; 1.
Reference: [2] <author> P. J. Daniell, </author> <title> "Discussion of paper by m. s. bartlett," </title> <journal> J. Royal Statistical Society, Ser. B, </journal> <volume> vol. 8, </volume> <pages> pp. 27, </pages> <year> 1946. </year>
Reference: [3] <author> R. B. Blackman and J. W. Tukey, </author> <title> "The measurement of power spectra from the point of view of communication engineering," </title> <journal> Bell Syst. Tech. Journ., </journal> <volume> vol. 37, </volume> <pages> pp. 183-282, </pages> <year> 1958. </year>
Reference: [4] <author> R. Lagendijk and J. Biemond, </author> <title> Iterative identification and restoration of images, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: This work was supported in part by National Science Foundation under grant BCS-9024370, a Government of Pak-istan Postgraduate Fellowship, NIH grants CA-54362 and CA-60711, and DOE grant DE-FG02-87ER60561 restoration, regularization is frequently implemented to reduce noise amplification (variance) at the expense of poorer spatial resolution (bias) <ref> [4] </ref>. In multiple regression with multicollinearity, biased shrinkage estimators [5] and biased ridge estimators [6] are used to reduce variance of the ordinary least squares estimator.
Reference: [5] <author> C. M. Stein, </author> <title> "Multiple regression," in Contributions to probability and statistics: </title> <booktitle> Essays in honor of Harold Hotelling, </booktitle> <editor> I. Olkin, S. Ghurge, W. Hoeffding, W. Nadow, and H. Mann, </editor> <booktitle> editors, </booktitle> <pages> pp. 424-443, </pages> <publisher> Stanford University Press, </publisher> <year> 1960. </year>
Reference-contexts: In multiple regression with multicollinearity, biased shrinkage estimators <ref> [5] </ref> and biased ridge estimators [6] are used to reduce variance of the ordinary least squares estimator.
Reference: [6] <author> A. E. Hoerl and R. W. Kennard, </author> <title> "Ridge regression: biased estimation of non-orthogonal components," </title> <journal> Technometrics, </journal> <volume> vol. 12, </volume> <pages> pp. 55-67, </pages> <year> 1970. </year>
Reference-contexts: In multiple regression with multicollinearity, biased shrinkage estimators [5] and biased ridge estimators <ref> [6] </ref> are used to reduce variance of the ordinary least squares estimator. <p> The simplest choice for the penalty matrix P is the identity I, which yields a class of energy penalized least squares estimators variously known as Tikonov regularized least squares in the inverse problem literature [38], and shrinkage estimation or ridge regression in the multivariate statistics literature <ref> [6] </ref>. A popular choice in imaging applications is to use a non-diagonal differencing type operator to enforce smooth ness constraints or roughness priors [40], [41].
Reference: [7] <author> J.-S. Liow and S. C. Strother, </author> <title> "Practical tradeoffs between noise, quantitation, and number of iterations for maximum likelihood-based reconstructions," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 563-571, </pages> <year> 1991. </year>
Reference-contexts: In multiple regression with multicollinearity, biased shrinkage estimators [5] and biased ridge estimators [6] are used to reduce variance of the ordinary least squares estimator. The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction <ref> [7] </ref>, [8], [9], functional imaging [10], non-linear and morphological filtering [11], [12], and spectral estimation of time series [13], [14].
Reference: [8] <author> S. J. Lee, G. R. Gindi, I. G. Zubal, and A. Rangarajan, </author> <title> "Using ground-truth data to design priors in Bayesian SPECT reconstruction," in Information Processing in Medical Imaging, </title> <editor> Y. Bizais, C. Barillot, and R. D. Paola, editors, </editor> <publisher> Kluwer, </publisher> <year> 1995. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], <ref> [8] </ref>, [9], functional imaging [10], non-linear and morphological filtering [11], [12], and spectral estimation of time series [13], [14].
Reference: [9] <author> J. A. Fessler, </author> <title> "Penalized weighted least-squares image reconstruction for positron emission tomography," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 13, no. 2, </volume> <pages> pp. 290-300, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], <ref> [9] </ref>, functional imaging [10], non-linear and morphological filtering [11], [12], and spectral estimation of time series [13], [14].
Reference: [10] <author> R. E. Carson, Y. Yan, B. Chodkowski, T. K. Yap, and M. E. Daube-Witherspoon, </author> <title> "Precision and accuracy of regional radioactivity quantitation using the maximum likelihood EM reconstruction algorithm," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 13, no. 3, </volume> <pages> pp. 526-537, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], [9], functional imaging <ref> [10] </ref>, non-linear and morphological filtering [11], [12], and spectral estimation of time series [13], [14].
Reference: [11] <author> D. Wand, V. Haese-Coat, A. Bruno, and J. Ronsin, </author> <title> "Some statistical properties of mathematical morphology," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 43, no. 8, </volume> <pages> pp. 1955-65, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], [9], functional imaging [10], non-linear and morphological filtering <ref> [11] </ref>, [12], and spectral estimation of time series [13], [14]. However, the plane parameterized by the bias and variance b and 2 is not useful for studying fundamental tradeoffs since an estimator can always be found which makes both the bias and variance zero at a given point .
Reference: [12] <author> N. Himayat and S. A. Kassam, </author> <title> "Approximate performance analysis of edge preserving fil ters," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 41, no. 9, </volume> <pages> pp. 2764-2777, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], [9], functional imaging [10], non-linear and morphological filtering [11], <ref> [12] </ref>, and spectral estimation of time series [13], [14]. However, the plane parameterized by the bias and variance b and 2 is not useful for studying fundamental tradeoffs since an estimator can always be found which makes both the bias and variance zero at a given point .
Reference: [13] <author> M. B. Woodroofe and J. W. Van Ness, </author> <title> "The maximum deviation of sample spectral densities," </title> <journal> Ann. Math. Statist., </journal> <volume> vol. 38, </volume> <pages> pp. 1558-1569, </pages> <year> 1967. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], [9], functional imaging [10], non-linear and morphological filtering [11], [12], and spectral estimation of time series <ref> [13] </ref>, [14]. However, the plane parameterized by the bias and variance b and 2 is not useful for studying fundamental tradeoffs since an estimator can always be found which makes both the bias and variance zero at a given point .
Reference: [14] <author> P. Bloomfield, </author> <title> Fourier Analysis of Time Series, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: The quantitative study of estimator bias and variance has been useful for characterizing statistical performance for many statistical signal processing applications including: tomographic reconstruction [7], [8], [9], functional imaging [10], non-linear and morphological filtering [11], [12], and spectral estimation of time series [13], <ref> [14] </ref>. However, the plane parameterized by the bias and variance b and 2 is not useful for studying fundamental tradeoffs since an estimator can always be found which makes both the bias and variance zero at a given point .
Reference: [15] <author> A. O. Hero, </author> <title> "A Cramer-Rao type lower bound for essentially unbiased parameter estimation," </title> <type> Technical Report 890, </type> <institution> M.I.T. Lincoln Laboratory, Lexington, </institution> <address> MA, Lexington, MA 02173-0073, </address> <month> Jan., </month> <year> 1992. </year> <note> DTIC AD-A246666. </note>
Reference-contexts: This paper provides a means for specifying unachievable regions in the ffi plane via fundamental delta-sigma tradeoff curves. These curves are generated using an extension of the Cramer-Rao (CR) lower bound on the variance of biased estimators presented in <ref> [15] </ref>. This extension is called the uniform CR bound. In [15] the bound was derived only for an unweighted Euclidean norm on the bias gradient and for non-singular Fisher information. <p> This paper provides a means for specifying unachievable regions in the ffi plane via fundamental delta-sigma tradeoff curves. These curves are generated using an extension of the Cramer-Rao (CR) lower bound on the variance of biased estimators presented in <ref> [15] </ref>. This extension is called the uniform CR bound. In [15] the bound was derived only for an unweighted Euclidean norm on the bias gradient and for non-singular Fisher information. Therein the reader was cautioned that the resulting bound will generally depend on the units and dimensions used to express each of the parameters. It was also pointed out in [15] <p> <ref> [15] </ref> the bound was derived only for an unweighted Euclidean norm on the bias gradient and for non-singular Fisher information. Therein the reader was cautioned that the resulting bound will generally depend on the units and dimensions used to express each of the parameters. It was also pointed out in [15] that the user should identify an ellipsoid of expected parameter variations, which will depend in the user's units, and perform a normalizing transformation of the ellipsoid to a spheroid prior to applying the bound. <p> This parameter transformation is equivalent to using a diagonally weighted bias gradient norm constraint in the original untransformed parameter space. The uniform CR bound presented in this paper generalizes <ref> [15] </ref> to allow functional estimation, to 2 TO APPEAR IN IEEE TRANS. ON SIGNAL PROCESSING, JULY 1996 cover the case of singular or ill-conditioned Fisher matrices, and to account for a general norm constraint on bias gradient. Some elements of the latter generalization were first presented in [16]. <p> Alternatively, as discussed in more detail in <ref> [15] </ref>, these results can be used to investigate the reliability of CR bound studies when small estimator biases may be present. <p> Therefore (7) cannot be used to simultaneously bound the variance of several estimators, each of which have different but comparable bias gradients. B. The Uniform CR Bound In <ref> [15] </ref> a "uniform" CR bound was presented as a way to study the reliability of the unbiased CR bound under conditions of very small estimator bias. In [34] this uniform bound was used to trace out curves over the sigma-delta plane which includes both large and small biases. <p> In [34] this uniform bound was used to trace out curves over the sigma-delta plane which includes both large and small biases. The following theorem extends the results of <ref> [15] </ref> and [34] to allow singular Fisher information matrices, arbitrary weighted Euclidean norm k * k C , and arbitrary differentiable function t . For a proof of this theorem see Appendix A. <p> The slope of B (; ffi) at ffi = 0 gives a bias sensitivity index for the unbiased CR bound. For non-singular F Y and single component estimation (t = 1 ) it is shown in <ref> [15] </ref> that = 2 q S c, where c is the first column of F Y and F S is the principal minor of [F Y ] 11 . <p> 1 fi fl 1 and &gt; 0 is given by the unique positive solution of g () = ffi 2 where g () = fi fl 1 C 1 + F Y rt :(14) When C = I and t = 1 these are identical to the results obtained in <ref> [15] </ref>. * In Theorem 1, d min defined in (10) is an optimal bias gradient in the sense that it minimizes the biased CR bound (7) over all vectors rb satisfying the constraint krb k C ffi. <p> It has been shown <ref> [15] </ref> that if F Y is non-singular, if ffi is small, if t = 1 , and if the unbiased matrix CR bound is locally achievable by an unbiased estimator ^ fl in a neighborhood of a point , then one can construct an estimator that locally achieves the uniform bound
Reference: [16] <author> J. Fessler and A. Hero, </author> <title> "Cramer-Rao bounds for biased estimators in image restoration," </title> <booktitle> in Proc. of 36th IEEE Midwest Symposium on Circuits and Systems, </booktitle> <address> Detroit, MI, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: ON SIGNAL PROCESSING, JULY 1996 cover the case of singular or ill-conditioned Fisher matrices, and to account for a general norm constraint on bias gradient. Some elements of the latter generalization were first presented in <ref> [16] </ref>. <p> Thus, under the commutative assumption the bias gradient norm can be viewed as a measure of the geometric resolution of ^ t <ref> [16] </ref>. A.1 Non-Singular Fisher Matrix Assume that F Y is non-singular and compare (32) and (33) to the equations (13) and (12) for d min and the bound B (; ffi), respectively.
Reference: [17] <author> M. Usman, A. Hero, J. A. Fessler, and W. Rogers, </author> <title> "Bias-variance tradeoffs analysis using uniform CR bound for a SPECT system," </title> <booktitle> in Proc. of IEEE Nuclear Science Symposium and Medical Imaging Conf, </booktitle> <pages> pp. 1463-1467, </pages> <address> San Francisco, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Some elements of the latter generalization were first presented in [16]. The methods described herein can be used for system optimization, i.e., to choose the system which minimizes the size of the unachievable region, when estimator unbi-asedness is an overly stringent or unrealistic constraint <ref> [17] </ref>, or they can be used to gauge the closeness to optimality of biased estimators in terms of their nearness to the un-achievable region [18].
Reference: [18] <author> M. Usman, A. Hero, and J. A. Fessler, </author> <title> "Bias-variance tradeoffs analysis using uniform CR bound for images," </title> <booktitle> in Proc. of IEEE Image Processing Conference, </booktitle> <pages> pp. 835-839, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: for system optimization, i.e., to choose the system which minimizes the size of the unachievable region, when estimator unbi-asedness is an overly stringent or unrealistic constraint [17], or they can be used to gauge the closeness to optimality of biased estimators in terms of their nearness to the un-achievable region <ref> [18] </ref>. Alternatively, as discussed in more detail in [15], these results can be used to investigate the reliability of CR bound studies when small estimator biases may be present.
Reference: [19] <author> C. R. Rao, </author> <title> Linear Statistical Inference and Its Applications, </title> <address> Wi-ley, New York, </address> <year> 1973. </year>
Reference-contexts: Such problems arise in image restoration, image reconstruction, and seismic deconvo-lution, to name but a few examples. Note that even for the linear Gaussian problem there may not exist unbiased estimators when the system matrix is ill-conditioned or rank deficient <ref> [19] </ref>. For each model we compare the performance of quadratically penalized maximum likelihood estimators to the fundamental delta-sigma tradeoff curve. <p> For concreteness we will refer to i as the intensity of the source at pixel i. The Fisher information matrix has the well known form <ref> [19] </ref> F Y = A T 1 A: (28) This matrix is non-singular when A is of full column rank n. We will consider estimation of the linear combination t = h T where h is a fixed non-zero vector in IR n . <p> A.2 Singular Fisher Matrix When A has rank less than n, F Y is singular and unbiased estimators may not exist for all linear functions t of <ref> [19] </ref>, [42]. A lower bound on the norm of the bias gradient can derived (see Appendix C) using the relation (6) between the norm and the maximal bias variation over a region of parameter space.
Reference: [20] <author> J. A. Fessler and A. O. Hero, </author> <title> "Space-alternating generalized EM algorithm," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> vol. SP-42, no. 10, </volume> <pages> pp. 2664-2677, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: For the rank deficient linear Gaussian problem the uniform CR bound is shown to be achievable by a different estimator under certain conditions. Finally, for the non-linear Poisson case an asymptotic analysis shows that the penalized maximum likelihood estimator of <ref> [20] </ref> achieves the fundamental delta-sigma tradeoff curve for sufficiently large values of the regularization parameter and a suitably chosen penalty matrix. We present simulation results that empirically validate our asymptotic analysis. A. Variance, Bias and Bias Gradient Let ^ t be an estimator of the scalar differentiable function t . <p> The function of interest was chosen as t = 65 , the intensity of pixel 65 in Fig. 8. We generated L = 1000 realizations of the measurements each having a mean total of P m j=1 j () = 2100 counts, including a 5% background representing random coincidences <ref> [20] </ref>. We computed three types of estimates of : the quadrat-ically penalized maximum likelihood estimator using the "energy penalty" (P = I), a truncated SVD estimator, and a "deconvolve/shrink" estimator. <p> We maximized the non-quadratic penalized likelihood objective using the PML-SAGE algorithm, a variant of the iterative space alternating generalized expectation-maximization (SAGE) algorithm of <ref> [20] </ref> adapted for penalized maximum likelihood image reconstruction [46]. We initialized PML-SAGE with an unweighted penalized least-squares estimate: (A T A + fi ? I) 1 A 0 (Y r); which is linear so can be computed noniteratively.
Reference: [21] <author> G. E. P. Box and N. R. Draper, </author> <title> "A basis for the selection of a response surface design," </title> <journal> J. Am. Statist. Assoc., </journal> <volume> vol. 54, </volume> <pages> pp. 622-654, </pages> <year> 1959. </year>
Reference-contexts: The generalized MSE has been used in response surface design <ref> [21] </ref> and in minimum bias and variance estimation for non linear regression models [22], [23]. Furthermore, since they jointly specify the first two moments of the estimator probability distribution, the pair (b ; 2 ) provides essential information for constructing and evaluating ^ t-based hypothesis tests and confidence intervals.
Reference: [22] <author> M. A. Ali, </author> <title> "On a class of shrinkage estimators of the vector of shrinkage coefficients," </title> <journal> Commun. Statist.-Theory Meth., </journal> <volume> vol. 18, no. 12, </volume> <pages> pp. 4491-4500, </pages> <year> 1981. </year>
Reference-contexts: The generalized MSE has been used in response surface design [21] and in minimum bias and variance estimation for non linear regression models <ref> [22] </ref>, [23]. Furthermore, since they jointly specify the first two moments of the estimator probability distribution, the pair (b ; 2 ) provides essential information for constructing and evaluating ^ t-based hypothesis tests and confidence intervals.
Reference: [23] <author> T. Bednarski, </author> <title> "On minimum bias and variance estimation for parametric models with shrinking contamination," </title> <journal> Probability and Math. Statist., </journal> <volume> vol. 6, no. 2, </volume> <pages> pp. 121-129, </pages> <year> 1985. </year>
Reference-contexts: The generalized MSE has been used in response surface design [21] and in minimum bias and variance estimation for non linear regression models [22], <ref> [23] </ref>. Furthermore, since they jointly specify the first two moments of the estimator probability distribution, the pair (b ; 2 ) provides essential information for constructing and evaluating ^ t-based hypothesis tests and confidence intervals.
Reference: [24] <author> M. Quenouille, </author> <title> "Approximate tests of correlation in time series," </title> <journal> J. Royal Statistical Society, Ser. B, </journal> <volume> vol. 11, </volume> <pages> pp. 18-84, </pages> <year> 1949. </year>
Reference-contexts: Furthermore, since they jointly specify the first two moments of the estimator probability distribution, the pair (b ; 2 ) provides essential information for constructing and evaluating ^ t-based hypothesis tests and confidence intervals. Indeed the popular jack-nife method was originally introduced by <ref> [24] </ref>, [25] to estimate bias and variance of a statistic and to test whether the statistic has prespecified mean [26] . An estimator ^ t whose bias function b : fi ! IR is constant is as good as unbiased since the bias can be removed without knowledge of .
Reference: [25] <author> J. Tukey, </author> <title> "Bias and confidence in not quite large samples," </title> <journal> Ann. Math. Statist., </journal> <volume> vol. 29, </volume> <pages> pp. 614, </pages> <year> 1956. </year>
Reference-contexts: Furthermore, since they jointly specify the first two moments of the estimator probability distribution, the pair (b ; 2 ) provides essential information for constructing and evaluating ^ t-based hypothesis tests and confidence intervals. Indeed the popular jack-nife method was originally introduced by [24], <ref> [25] </ref> to estimate bias and variance of a statistic and to test whether the statistic has prespecified mean [26] . An estimator ^ t whose bias function b : fi ! IR is constant is as good as unbiased since the bias can be removed without knowledge of .
Reference: [26] <author> B. Efron, </author> <title> "Bootstrap methods: another look at the jacknife," </title> <journal> Annals of Statistics, </journal> <volume> vol. 7, </volume> <pages> pp. 1-26, </pages> <year> 1979. </year>
Reference-contexts: Indeed the popular jack-nife method was originally introduced by [24], [25] to estimate bias and variance of a statistic and to test whether the statistic has prespecified mean <ref> [26] </ref> . An estimator ^ t whose bias function b : fi ! IR is constant is as good as unbiased since the bias can be removed without knowledge of .
Reference: [27] <author> I. A. Ibragimov and R. Z. Has'minskii, </author> <title> Statistical estimation: Asymptotic theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: When the density function f Y (y; ) is sufficiently smooth to guarantee existence of the Fisher information matrix (defined below), b is always differentiable regardless of the form of the estimator as long as E [ ^ t 2 ] is upper bounded <ref> [27, Lemma 7.2] </ref>.
Reference: [28] <author> M. Frechet, "Sur l'extension de certaines evaluations statistiques de petits echantillons," </author> <title> Rev. </title> <journal> Inst. Int. Statist, </journal> <volume> vol. 11, </volume> <pages> pp. 128-205, </pages> <year> 1943. </year>
Reference-contexts: While no non-empty unachievable region can exist in the bias-variance plane parameterized by (b ; ), we will show that interesting unachievable regions almost always exist in the delta-sigma plane. A. The Biased CR Bound The Cramer-Rao lower bound on estimator variance, first published by Frechet <ref> [28] </ref> and later by Darmois [29], Cramer [30], and Rao [31], is commonly used to bound the variance of unbiased estimators.
Reference: [29] <author> G. Darmois, </author> <title> "Sur les lois limites de la dispersion de certaines estimations," </title> <journal> Rev. Inst. Int. Statist, </journal> <volume> vol. 13, </volume> <pages> pp. 9-15, </pages> <year> 1945. </year>
Reference-contexts: A. The Biased CR Bound The Cramer-Rao lower bound on estimator variance, first published by Frechet [28] and later by Darmois <ref> [29] </ref>, Cramer [30], and Rao [31], is commonly used to bound the variance of unbiased estimators.
Reference: [30] <author> H. Cramer, </author> <title> "A contribution to the theory of statistical estimation," </title> <journal> Skand. Aktuaries Tidskrift, </journal> <volume> vol. 29, </volume> <pages> pp. 458-463, </pages> <year> 1946. </year> <note> 16 TO APPEAR IN IEEE TRANS. ON SIGNAL PROCESSING, </note> <month> JULY </month> <year> 1996 </year>
Reference-contexts: A. The Biased CR Bound The Cramer-Rao lower bound on estimator variance, first published by Frechet [28] and later by Darmois [29], Cramer <ref> [30] </ref>, and Rao [31], is commonly used to bound the variance of unbiased estimators.
Reference: [31] <author> C. R. Rao, </author> <title> "Minimum variance and the estimation of several parameters," </title> <journal> Proc. Cambridge Phil. Soc., </journal> <volume> vol. 43, </volume> <pages> pp. 280-283, </pages> <year> 1946. </year>
Reference-contexts: A. The Biased CR Bound The Cramer-Rao lower bound on estimator variance, first published by Frechet [28] and later by Darmois [29], Cramer [30], and Rao <ref> [31] </ref>, is commonly used to bound the variance of unbiased estimators.
Reference: [32] <author> H. L. Van-Trees, </author> <title> Detection, Estimation, and Modulation Theory: Part I, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: The non-singular-F Y form of the biased CR bound has been around for some time, e.g. <ref> [32] </ref>.
Reference: [33] <author> J. D. Gorman and A. O. Hero, </author> <title> "Lower bounds for parametric estimation with constraints," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. IT-36, </volume> <pages> pp. 1285-1301, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The more general pseudo-inverse-F Y form given in (7) is less well known but can be easily derived by identifying U = ^ t t and V = r ln f Y (Y ; ) in the relation <ref> [33, Lemma 1] </ref> E U U T fl fi E V V T fl + fi ; and using the well known identities E [r ln f Y (Y ; )] = 0 and E [ ^ tr ln f Y (Y ; )] = rm (easily derivable from (18) below).
Reference: [34] <author> M. Usman, </author> <title> "Biased and unbiased Cramer-Rao bounds: computational issues and applications," </title> <type> PhD Thesis, </type> <institution> Dept EECS, The University of Michigan, </institution> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: B. The Uniform CR Bound In [15] a "uniform" CR bound was presented as a way to study the reliability of the unbiased CR bound under conditions of very small estimator bias. In <ref> [34] </ref> this uniform bound was used to trace out curves over the sigma-delta plane which includes both large and small biases. The following theorem extends the results of [15] and [34] to allow singular Fisher information matrices, arbitrary weighted Euclidean norm k * k C , and arbitrary differentiable function t <p> In <ref> [34] </ref> this uniform bound was used to trace out curves over the sigma-delta plane which includes both large and small biases. The following theorem extends the results of [15] and [34] to allow singular Fisher information matrices, arbitrary weighted Euclidean norm k * k C , and arbitrary differentiable function t . For a proof of this theorem see Appendix A.
Reference: [35] <author> W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, </author> <title> Numerical recipes in C: </title> <booktitle> The art of scientific computing (2nd Ed.), </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Alternatively, the right hand sides of (17) can be approximated by using iterative equation solving methods such as Gauss-Seidel (GS) or preconditioned conjugate gradient (CG) algorithms <ref> [35] </ref>. See [45], [36] for a more detailed discussion of the application of iterative equation solvers to CR bound approximation. This approach can be implemented in the following sequence of steps. 1. Select 2 (0; 1). 2.
Reference: [36] <author> A.O. Hero, M. Usman, A.C. Sauve and J.A. Fessler, </author> <title> "Recursive algorithms for computing the Cramer-Rao bound," </title> <journal> IEEE Trans. </journal> <note> on Signal Processing, in review. </note>
Reference-contexts: Alternatively, the right hand sides of (17) can be approximated by using iterative equation solving methods such as Gauss-Seidel (GS) or preconditioned conjugate gradient (CG) algorithms [35]. See [45], <ref> [36] </ref> for a more detailed discussion of the application of iterative equation solvers to CR bound approximation. This approach can be implemented in the following sequence of steps. 1. Select 2 (0; 1). 2.
Reference: [37] <author> B. E. Efron, </author> <title> The jackknife, the bootstrap, and other resampling plans, </title> <publisher> SIAM Press, </publisher> <address> Philadelphia PA, </address> <year> 1982. </year>
Reference-contexts: 2 (y 1 ; : : : ; y L ) = k d rb k 2 C , the bootstrap estimate of ffi 2 is defined as the expectation of ^ ffi 2 fl = ^ ffi 2 (Y fl L ) with re spect to the resampling distribution <ref> [37] </ref> E fl [ ^ ffi 2 X ^ ffi 2 (Y fl L ) L In (24) c i is the number of times the value y i appears in the set fY fl P fl denotes a summation over all non negative integers c 1 ; : : :
Reference: [38] <author> V. A. Morozov, </author> <title> Methods for Solving Incorrectly Posed Problems, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: To demonstrate the achievability of the fundamental delta-sigma tradeoff curve we consider the quadratically penalized maximum likelihood (QPML) estimator. The QPML strategy is frequently used in order to obtain stable solutions in the presence of small variations in experimental conditions <ref> [38] </ref> and to incorporate parameter constraints or a priori information [39]. <p> The simplest choice for the penalty matrix P is the identity I, which yields a class of energy penalized least squares estimators variously known as Tikonov regularized least squares in the inverse problem literature <ref> [38] </ref>, and shrinkage estimation or ridge regression in the multivariate statistics literature [6]. A popular choice in imaging applications is to use a non-diagonal differencing type operator to enforce smooth ness constraints or roughness priors [40], [41].
Reference: [39] <author> K. Lange, </author> <title> "Convergence of EM image reconstruction algorithms with Gibbs smoothing," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 439-446, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The QPML strategy is frequently used in order to obtain stable solutions in the presence of small variations in experimental conditions [38] and to incorporate parameter constraints or a priori information <ref> [39] </ref>.
Reference: [40] <author> T. Hebert and R. Leahy, </author> <title> "A generalized EM algorithm for 3-D Bayesian reconstruction from Poisson data using Gibbs priors," </title> <journal> IEEE Trans. on Medical Imaging, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 194-203, </pages> <year> 1989. </year>
Reference-contexts: A popular choice in imaging applications is to use a non-diagonal differencing type operator to enforce smooth ness constraints or roughness priors <ref> [40] </ref>, [41]. The minimizer of (29) is the penalized weighted least squares (PLS) estimator ^ = [F Y + fiP] A T 1 Y ; (30) yielding the QPML estimator ^ t = h T ^ .
Reference: [41] <author> V. Johnson, W. Wong, X. Hu, and C. Chen, </author> <title> "Image restoration using Gibbs priors: boundary modeling, treatment of blurring, and selection of hyperparameters," </title> <journal> IEEE Trans. on Pattern Anal. and Machine Intell., </journal> <volume> vol. PAMI-13, no. 5, </volume> <pages> pp. 413-425, </pages> <year> 1991. </year>
Reference-contexts: A popular choice in imaging applications is to use a non-diagonal differencing type operator to enforce smooth ness constraints or roughness priors [40], <ref> [41] </ref>. The minimizer of (29) is the penalized weighted least squares (PLS) estimator ^ = [F Y + fiP] A T 1 Y ; (30) yielding the QPML estimator ^ t = h T ^ .
Reference: [42] <author> R. C. Liu and L. D. Brown, </author> <title> "Nonexistence of informative unbiased estimators in singular problems," </title> <journal> Annals of Statistics, </journal> <volume> vol. 21, no. 1, </volume> <pages> pp. 1-13, </pages> <year> 1993. </year>
Reference-contexts: A.2 Singular Fisher Matrix When A has rank less than n, F Y is singular and unbiased estimators may not exist for all linear functions t of [19], <ref> [42] </ref>. A lower bound on the norm of the bias gradient can derived (see Appendix C) using the relation (6) between the norm and the maximal bias variation over a region of parameter space. <p> However, the reader is cautioned that for non-linear models the bound (55) may not be very tight since unbiased estimators may not exist even for components lying in the range space of F Y <ref> [42] </ref>.
Reference: [43] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations (2nd Edition), </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: F Y F + fl fi I + fiPF + Y h; (36) and the estimator variance is 2 P 1 + fiF + fl 1 Y P 1 + fiF + fl 1 where in (37) we have used the property F + Y F Y F + Y <ref> [43] </ref>. Noting that here rt = h, we conclude that the estimator variance is equal to the lower bound expression B (; ffi) given in (9) when P = C 1 and fi = 1=.
Reference: [44] <author> D. L. Snyder and M. I. Miller, </author> <title> Random Point Processes in Time and Space, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Bias variance tradeoff study for estimation of nullspace compound h T for singular Fisher information (h is illustrated in Fig. 6). The bound is exactly achieved by the smoothed QPML estimator at the point ffi = 5:4 fi 10 6 . inverse problems <ref> [44] </ref>. The observation Y = [Y 1 ; : : : ; Y m ] T is a vector of integers or counts with a vector of means = [ 1 ; : : : ; m ] T .
Reference: [45] <author> A. O. Hero and J. A. Fessler, </author> <title> "A recursive algorithm for computing CR-type bounds on estimator covariance," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1205-1210, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Alternatively, the right hand sides of (17) can be approximated by using iterative equation solving methods such as Gauss-Seidel (GS) or preconditioned conjugate gradient (CG) algorithms [35]. See <ref> [45] </ref>, [36] for a more detailed discussion of the application of iterative equation solvers to CR bound approximation. This approach can be implemented in the following sequence of steps. 1. Select 2 (0; 1). 2. <p> The Fisher information has the form <ref> [45] </ref> F Y () = j=1 j () jfl ; (40) where A T jfl is the j-th row of A. To investigate the achievability of the region above the delta-sigma tradeoff curve, and to illustrate the empirical computation of bias gradient, we consider again the QPML strategy.
Reference: [46] <author> J. A. Fessler and A. O. Hero, </author> <title> "Penalized maximum likelihood image reconstruction using space alternating generalized EM algorithms," </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> vol. </volume> <pages> 3, </pages> , <note> to appear Oct. </note> <year> 1995. </year>
Reference-contexts: We maximized the non-quadratic penalized likelihood objective using the PML-SAGE algorithm, a variant of the iterative space alternating generalized expectation-maximization (SAGE) algorithm of [20] adapted for penalized maximum likelihood image reconstruction <ref> [46] </ref>. We initialized PML-SAGE with an unweighted penalized least-squares estimate: (A T A + fi ? I) 1 A 0 (Y r); which is linear so can be computed noniteratively. Here fi ? = fi P P for k = 65 (cf [47], [48]).
Reference: [47] <author> J. A. Fessler and W. L. Rogers, </author> <title> "Uniform quadratic penalties cause nonuniform image resolution (and sometimes vice versa)," </title> <booktitle> in Proc. of IEEE Nuclear Science Symposium, </booktitle> <volume> volume 4, </volume> <pages> pp. 1915-1919, </pages> <year> 1994. </year>
Reference-contexts: We initialized PML-SAGE with an unweighted penalized least-squares estimate: (A T A + fi ? I) 1 A 0 (Y r); which is linear so can be computed noniteratively. Here fi ? = fi P P for k = 65 (cf <ref> [47] </ref>, [48]). By so initializing, only 30 iterations were needed to ensure convergence to a precision well below the estimate standard deviation. For the truncated SVD estimator, we computed the singular value decomposition (SVD) of A, and computed approximate pseudo-inverses of A by excluding the 10 smallest eigenvalues.
Reference: [48] <author> J. A. Fessler, </author> <title> "Resolution properties of regularized image reconstruction methods," </title> <type> Technical Report 297, </type> <institution> Comm. and Sig. Proc. Lab. (CSPL), Dept. EECS, University of Michigan, </institution> <address> Ann Arbor, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: We initialized PML-SAGE with an unweighted penalized least-squares estimate: (A T A + fi ? I) 1 A 0 (Y r); which is linear so can be computed noniteratively. Here fi ? = fi P P for k = 65 (cf [47], <ref> [48] </ref>). By so initializing, only 30 iterations were needed to ensure convergence to a precision well below the estimate standard deviation. For the truncated SVD estimator, we computed the singular value decomposition (SVD) of A, and computed approximate pseudo-inverses of A by excluding the 10 smallest eigenvalues.
Reference: [49] <author> E. B. Manoukian, </author> <title> Modern Concepts and Theorems of Mathematical Statistics, </title> <publisher> Springer-Verlag, </publisher> <address> New York N.Y., </address> <year> 1986. </year>
Reference-contexts: Averaging (51) over c we obtain the bootstrap estimate of the mean E fl [ ^ ffi 2 X ^ ffi 2 c 1 : : : c L L L (52) 1 L 2 trace From the mean and covariance of the multinomial distri bution <ref> [49, Sec. 3.2] </ref>: E fl [c c T ] = I + L1 where 1 is a vector of ones.
Reference: [50] <author> P. J. Huber, </author> <title> Robust Statistics, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Define the ambiguity function a (u; ) = E [J (u)] and let u = z = z () be the root of the equation (u) = 0 where (u) = r 10 a (u; ). Assuming the technical conditions underlying <ref> [50, Corollary 3.2, Sec. 6.3] </ref> are satisfied 2 we have the following approximation: in the limit of large observation time the estimator ^ is asymptotically normal with mean z and covariance ma trix = fi fl 1 fi fl T where G z = cov (rJ (z)).

References-found: 50

