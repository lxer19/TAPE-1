URL: http://www.tu-chemnitz.de/~ait/publications/ecml98.ps.gz
Refering-URL: http://www.tu-chemnitz.de/~ait/more_infos.html
Root-URL: 
Email: fros,gze,ait,wdig@informatik.tu-chemnitz.de  
Title: A Short Note About the Application of Polynomial Kernels with Fractional Degree in Support Vector Learning  
Author: Rolf Rossius, Gerard Zenker, Andreas Ittner, and Werner Dilger 
Web: http://www.tu-chemnitz.de/informatik/HomePages/KI/  
Address: D-09107 Chemnitz  
Affiliation: Department of Computer Science Artificial Intelligence Group Chemnitz University of Technology  
Abstract: In the mid 90's a fundamental new Machine Learning approach was developed by V. N. Vapnik: The Support Vector Machine (SVM). This new method can be regarded as a very promising approach and is getting more and more attention in the fields where neural networks and decision tree methods are applied. Whilst neural networks may be considered (correctly or not) to be well understood and are in wide use, Support Vector Learning has some rough edges in theoretical details and its inherent numerical tasks prevent it from being easily applied in practice. This paper picks up a new aspect the use of fractional degrees on polynomial kernels in the SVM discovered in the course of an implementation of the algorithm. Fractional degrees on polynomial kernels broaden the capabilities of the SVM and offer the possibility to deal with feature spaces of infinite dimension. We introduce a method to simplify the quadratic programming problem, as the core of the SVM.
Abstract-found: 1
Intro-found: 1
Reference: [CV95] <author> C. Cortes and V. N. Vapnik. </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: The Hesse matrix A consists of the elements A ij = y i y j hx i ; x j i for i; j = 1; : : : ; l <ref> [CV95] </ref>. However in the general case the linear separation in the original feature space will not provide a sufficient classifier.
Reference: [Fri93] <author> B. Fritzke. </author> <title> Growing cell structures a self-organizing network for unsupervised and supervised learning. </title> <type> Technical Report 93-026, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <year> 1993. </year>
Reference-contexts: A simple artificial problem in a two dimensional feature space is presented in Figure 1. <ref> [Fri93] </ref> 1 One could imagine this in the original space: The representing vectors u and v of both participating examples form a sufficient obtuse angle. In: Proc. of the 10th European Conference on Machine Learning (ECML'98) 5 Fig. 1.
Reference: [IRZ98] <author> A. Ittner, R. Rossius, and G. Zenker. </author> <title> Support Vector Learning. </title> <type> Technical Report CSR-98, </type> <institution> Chemnitz University of Technology, Chemnitz, Germany, </institution> <year> 1998. </year>
Reference-contexts: An innovative and still relatively unknown learning approach is the Support Vector Machine (SVM) developed by V. N. Vapnik in the mid 90's. Support Vector Learning <ref> [IRZ98] </ref> is not just another approach to learning techniques, rather it can be regarded as a fundamental new philosophy in the area of Machine Learning. The underlying principle of the SVM is the principle of the Structural Risk Minimization (SRM) [Vap95].
Reference: [Vap95] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: N. Vapnik in the mid 90's. Support Vector Learning [IRZ98] is not just another approach to learning techniques, rather it can be regarded as a fundamental new philosophy in the area of Machine Learning. The underlying principle of the SVM is the principle of the Structural Risk Minimization (SRM) <ref> [Vap95] </ref>. In contrast to a pure minimization of the empirical risk the SRM is based on the "idea of the simplicity" and unifies Empirical Risk Minimization and the problem of Model Selection. <p> The SRM is enforced by controlled bounding of the VC dimensions of the set ff ff g and ensures the excellent generalization ability of the SVM. The underlying theory of the SRM will not be explained in detail in this paper. We refer to <ref> [Vap95] </ref> which covers the SRM and the application in the SVM. The separating hyperplane is characterized by h!; xi + b = 0. The distance between the hyperplane and the examples should be maximized, i. e. one has to solve a problem of mathematical programming.
References-found: 4

