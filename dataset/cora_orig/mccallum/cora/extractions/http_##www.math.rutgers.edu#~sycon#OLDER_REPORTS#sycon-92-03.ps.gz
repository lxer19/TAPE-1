URL: http://www.math.rutgers.edu/~sycon/OLDER_REPORTS/sycon-92-03.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sycon/OLDER_REPORTS/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: albertin@hilbert.rutgers.edu, sontag@hilbert.rutgers.edu  
Title: NETWORKS, FUNCTION DETERMINES FORM  
Author: Francesca Albertini Eduardo D. Sontag 
Keyword: Key words: Neural networks, realization from input/output data, control systems  
Date: May 1992  
Address: New Brunswick, NJ 08903  
Affiliation: Department of Mathematics Rutgers University,  
Note: FOR NEURAL  Research supported in part by US Air Force Grant AFOSR-91-0343. Rutgers Center for Systems and Control  
Abstract: Report SYCON-92-03 ABSTRACT This paper shows that the weights of continuous-time feedback neural networks are uniquely identifiable from input/output measurements. Under very weak genericity assumptions, the following is true: Assume given two nets, whose neurons all have the same nonlinear activation function ; if the two nets have equal behaviors as "black boxes" then necessarily they must have the same number of neurons and |except at most for sign reversals at each node| the same weights. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleeremans, A., D. Servan-Schreiber, and J.L. McClelland, </author> <title> "Finite state automata and simple recurrent networks," </title> <booktitle> Neural Computation 1 (1989): </booktitle> <pages> 372-381. </pages>
Reference-contexts: called, are used as models whose parameters are fit to input/output data. (The purpose may be to use these models instead of a real plant, for purposes of control, or for predictive purposes.) This is done, for instance, in certain approaches to grammatical inference and speech processing; see for instance <ref> [1] </ref>, [10]. Typically, gradient descent algorithms are used in order to fit parameters thorugh the minimization of an error functional that penalizes mismatches between the desired outputs and those that a candidate net produces (the term "continuous backpropagation" is sometimes used for the gradient descent procedure).
Reference: [2] <author> Cohen, M.A., and S. Grossberg, </author> <title> "Absolute stability of global pattern formation and parallel memory storage by competitive neural networks," </title> <journal> IEEE Trans. Systems, Man, and Cybernetics 13(1983): </journal> <pages> 815-826. </pages>
Reference-contexts: 1 Introduction Many recent papers have explored the computational and dynamical properties of systems of interconnected "neurons." For instance, Hopfield ([6]), Cowan ([3]), and Grossberg and his school (see e.g. <ref> [2] </ref>), have all studied devices that can be modelled by sets of nonlinear differential equations such as _x i (t) = x i (t) + @ j=1 m X b ij u j (t) A ; i = 1; : : : ; n ; (1) _x i (t) = x
Reference: [3] <author> Cowan, J.D., </author> <title> "Statistical mechanics of neural nets," in Neural Networks (E.R. Caianiello, </title> <editor> ed.), </editor> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1968, </year> <pages> pp. 181-188. </pages>
Reference: [4] <author> Hecht-Nielsen, R., </author> <title> "Theory of the backpropagation neural network," </title> <booktitle> in Proceedings of the Int. Joint Conf. on Neural Networks, </booktitle> <address> Washington, 1989, </address> <publisher> IEEE Publications, NY, </publisher> <pages> 593-605. </pages>
Reference-contexts: For precisely the above reasons, but restricted to the particular case of feedforward (that is, nondynamic) nets, the question of deciding if the only possible symmetries are indeed the ones that we find was asked by Hecht-Nielsen in <ref> [4] </ref>.
Reference: [5] <author> Hirsch, M.W., </author> <title> "Convergent activation dynamics in continuous-time networks," </title> <booktitle> Neural Networks 2(1989): </booktitle> <pages> 331-349. </pages>
Reference-contexts: [13]) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([14]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example <ref> [5] </ref>, [8], and references there.
Reference: [6] <author> Hopfield, J.J., </author> <title> "Neurons with graded responses have collective computational properties like those of two-state neurons," </title> <booktitle> Proc. </booktitle> <institution> of the Natl. Acad. of Sciences, </institution> <address> USA 81(1984): </address> <pages> 3088-3092. </pages>
Reference-contexts: Electrical circuit implementations of these equations, employing resistively connected networks of n identical nonlinear amplifiers, and adjusting the resistor characteristics to obtain the desired weights, have been proposed as models of analog computers, in particular in the context of constraint satisfaction problems and in content-addressable memory applications (see e.g. <ref> [6] </ref>). We also assume given a certain number p of probes, or measurement devices, whose outputs signal to the environment the collective response of the net to the stimuli presented in the 1 This research was supported in part by US Air Force Grant AFOSR-91-0343.
Reference: [7] <author> Isidori, A., </author> <title> Nonlinear Control Systems: An Introduction, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in <ref> [7] </ref>, [9], [18], can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [8] <author> Michel, A.N., J.A. Farrell, and W. Porod, </author> <title> "Qualitative analysis of neural networks," </title> <journal> IEEE Trans. Circuits and Sys. </journal> <volume> 36(1989): </volume> <pages> 229-243. </pages>
Reference-contexts: when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([14]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example [5], <ref> [8] </ref>, and references there.
Reference: [9] <author> Nijmeijer, H., and A.V. Van der Schaft, </author> <title> Nonlinear Dynamical Control Systems, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in [7], <ref> [9] </ref>, [18], can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [10] <author> Robinson, A.J., and F. Fallside, </author> <title> "Static and dynamic error propagation networks with application to speech coding," </title> <booktitle> in Neural Information Processing Systems, </booktitle> <editor> (D.Z. Anderson, ed.), </editor> <booktitle> American Institute of Physics, </booktitle> <address> NY, </address> <year> 1988, </year> <pages> pp. 632-641. </pages>
Reference-contexts: are used as models whose parameters are fit to input/output data. (The purpose may be to use these models instead of a real plant, for purposes of control, or for predictive purposes.) This is done, for instance, in certain approaches to grammatical inference and speech processing; see for instance [1], <ref> [10] </ref>. Typically, gradient descent algorithms are used in order to fit parameters thorugh the minimization of an error functional that penalizes mismatches between the desired outputs and those that a candidate net produces (the term "continuous backpropagation" is sometimes used for the gradient descent procedure).
Reference: [11] <editor> Schwarzschild, R., and E.D. Sontag, </editor> <booktitle> "Algebraic theory of sign-linear systems," in Proceedings of the Automatic Control Conference, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1991: </year> <pages> 799-804. </pages>
Reference-contexts: Moreover, for suitably sharp nonlinearities , they are approximate models of discontinuous equations such as _x = sign (Ax+ Bu). (See <ref> [11] </ref> for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see [12], [13]) when all weights are rational numbers, and a general model of analog
Reference: [12] <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Turing computability with neural nets," </title> <journal> Appl. Math. Lett. </journal> <volume> 4(6)(1991): </volume> <pages> 77-80. </pages>
Reference-contexts: approximate models of discontinuous equations such as _x = sign (Ax+ Bu). (See [11] for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see <ref> [12] </ref>, [13]) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([14]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example
Reference: [13] <editor> Siegelmann, H.T., and E.D. Sontag, </editor> <booktitle> "On the computational power of neural nets," in Proc. Fifth ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: models of discontinuous equations such as _x = sign (Ax+ Bu). (See [11] for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see [12], <ref> [13] </ref>) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([14]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example [5],
Reference: [14] <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Analog computation, neural networks, and circuits," </title> <note> submitted. </note>
Reference: [15] <author> Sontag, E.D., </author> <title> "On the observability of polynomial systems," </title> <journal> SIAM J.Control and Opt., </journal> <volume> 17(1979): </volume> <pages> 139-151. </pages>
Reference-contexts: so it is fair to say the response of the net to a "random" input will suffice, at least theoretically, for determination of the number of units and unique identification of all weights.) The proof of the above Theorem is immediate from the general results for control systems given in <ref> [15] </ref> and [17], which imply that identifiability is equivalent to "single experiment" identifiability, for systems defined by analytic differential equations and depending analytically on parameters (here, the weights).
Reference: [16] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In the very special case when is the identity, classical linear realization theory |see for instance <ref> [16] </ref>, Chapter 5| implies that, generically, the triple (A; B; C) is determined only up to an invertible change of variables in the state space. <p> For the systems of interest in neural network theory, f (x; u) is always Lipschitz with respect to x, so " = T . (All results that we use on existence of solutions and continous dependence are included in standard texts such as <ref> [16] </ref>.) For each control, we let (u) = (u) be the output function corresponding to the initial state x (0) = 0, that is, (u)(t) := h ((t; 0; u)) ; defined at least on some interval [0; "). <p> f (x; u) = Dx + ~(Ax + Bu) + Gu (19) for some matrices A 2 R nfin ; D 2 R nfin ; B 2 R nfim ; G 2 R nfim ; and C 2 R pfin : These are continuous time systems in the sense of <ref> [16] </ref>. We will call such a system a system, and denote it by = (D; A; B; G; C) . Observe that in the special case in which is the identity, or more generally is linear, we have a linear system in the usual sense. <p> The following is a well-known formula (see e.g. <ref> [16] </ref> page 210): @ k fi fi fi y i L X i u 1 j (x i for all x 0 2 IR n , where L X h denotes the Lie-derivative of the function h along the vector field X. <p> n;m;p consisting of those triples (A; B; C) which are canonical, i.e. observable: rank [C T ; A T C T ; : : : ; (A T ) n1 C T ] = n and controllable: rank [B; AB; : : : ; A n1 B] = n; see <ref> [16] </ref>, section 5.5. This is a generic, in the sense of the introduction, subset of S n;m;p , for each n; m, and p. 15 Proposition 5.5 Assume 1 and 2 are i/o equivalent. <p> in this case (D = diagonal, and G = 0), the assumption rank [A; B] = n is redundant, as it follows from controllability of the pair (A + ffI; B), or equivalently, of the pair (A; B); this is just the case = ff of the Hautus condition (c.f. <ref> [16] </ref>, Lemma 3.3.7). 5.4.2 When D is diagonal and B = 0 Assume that we fix again an ff 2 IR, and this time we restrict our attention to the subclass of systems of the form: = (ffI; A; 0; G; C) where ff is this fixed real number (the same <p> Proof. Given an admissible control u (), we can find a sequence u n () 2 A such that the controls u n () are equibounded and converge to u () almost everywhere. Now we need only apply the approximation results in Theorem 1 of <ref> [16] </ref> to conclude the desired result. Let 1 and 2 be two systems of type (56), and ~ i for i = 1; 2 their corresponding systems of type (57). Proposition 5.12 If 1 and 2 are i/o equivalent, then ~ 1 and ~ 2 are also i/o equivalent. Proof.
Reference: [17] <author> Sussmann, H.J., </author> <title> "Single-input observability of continuous-time systems," </title> <journal> Math. Systems Theory 12(1979): </journal> <pages> 371-393. </pages>
Reference-contexts: is fair to say the response of the net to a "random" input will suffice, at least theoretically, for determination of the number of units and unique identification of all weights.) The proof of the above Theorem is immediate from the general results for control systems given in [15] and <ref> [17] </ref>, which imply that identifiability is equivalent to "single experiment" identifiability, for systems defined by analytic differential equations and depending analytically on parameters (here, the weights).
Reference: [18] <author> Sussmann, H.J., </author> <title> "Existence and uniqueness of minimal realizations of nonlinear systems," Math. Sys. </title> <booktitle> Theory 10 (1977): </booktitle> <pages> 263-284. </pages>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in [7], [9], <ref> [18] </ref>, can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [19] <author> Sussmann, H.J., </author> <title> "Uniqueness of the weights for minimal feedforward nets with a given input-output map," Neural Networks, </title> <note> to appear. 28 </note>
Reference-contexts: The question was partially answered (for so-called "single-hidden layer" nets, and using a particular activation function) by Sussmann in <ref> [19] </ref>, who established a uniqueness result which, in our setting, would apply to systems of the special type _x = ~(Bu); y = Cx, with = tanh (x). (That is, there is no "A" matrix; the result does allow for a constant bias vector inside the sigmoid, however.) A different, though
References-found: 19

