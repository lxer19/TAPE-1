URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS97-03.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fcfu,tyangg@cs.ucsb.edu  
Title: Run-time Techniques for Exploiting Irregular Task Parallelism on Distributed Memory Architectures  
Author: Cong Fu and Tao Yang 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Automatic scheduling for directed acyclic graphs (DAG) and its applications for coarse-grained irregular problems such as large n-body simulation have been studied in the literature. However solving irregular problems with mixed granularities such as sparse matrix factorization is challenging since it requires efficient run-time support to execute a DAG schedule. In this paper, we investigate run-time optimization techniques for executing general asynchronous DAG schedules on distributed memory machines and discuss an approach for exploiting parallelism from commuting operations in the DAG model. Our solution tightly integrates the run-time scheme with a fast communication mechanism to eliminate unnecessary overhead in message buffering and copying. We present a consistency model incorporating the above optimizations, and taking advantage of task dependence properties to ensure the correctness of execution. We demonstrate the applications of this scheme in sparse matrix factorizations and triangular equation solving for which actual speedups are hard to obtain. We provide a detailed experimental study on Meiko CS-2 to show that the automatically scheduled code has achieved good performance for these difficult problems and the run-time overhead is small compared to total execution times.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: We have mentioned Chaos in the introduction. Multipol [27] is a run-time library system which supports distributed data structures for several kinds of scientific applications. Consistency models. The issue of executing a program correctly has been studied in the context of distributed shared memory systems (DSM), e.g. TreadMarks <ref> [1] </ref>. The main difference between our work and DSM is that a DSM system does not have any knowledge of a program to be executed. A DSM system maintains the consistency of shared objects for all processors without knowing if those objects will be further accessed or not.
Reference: [2] <author> C. Ashcraft and R. Grimes. </author> <title> The Influence of Relaxed Supernode Partitions on the Multifrontal Method . ACM Transactions on Mathematical Software, </title> <booktitle> 15(4) </booktitle> <pages> 291-309, </pages> <year> 1989. </year>
Reference-contexts: In order to improve the performance further, our future work is to consider techniques of increasing average supernode sizes, e.g., supernode amalgamation <ref> [2, 19] </ref>. 6.2 Sparse triangular solver and performance impact of granularities From the discussion in the last section we know that the task granularity of an application is important to the performance of irregular code.
Reference: [3] <author> R. Blumfoe, M. Frigo, C. Joerg, C. Leiserson, and K. Randall. </author> <title> Dag-Consistent Distributed Shared Memory. </title> <booktitle> In Proceeding of 10th Inter. Parallel Processing Symposium, </booktitle> <pages> pages 132-141, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Run-time support for parallel computations. The Charm [24] system adopts a message driven approach for general asynchronous computation using dynamic scheduling. The Cilk <ref> [3] </ref> multi-threaded system aims at applications that have "strict" dependence structures. Randomized load balancing and work stealing techniques are used to execute a dynamic DAG.
Reference: [4] <author> E. Brewer, F. Chong, L. Liu, S. Sharma, and J. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [6]. In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code <ref> [4] </ref>. 18 8 Conclusions In this paper, we have presented an efficient run-time scheme for supporting task computations and shown its applications in irregular sparse Cholesky, LU factorization without pivoting and triangular solvers. The experiments indicate that overhead of run-time data communication is relatively small.
Reference: [5] <author> W. Cai. </author> <type> Personal Communication, </type> <year> 1995. </year>
Reference-contexts: Many of algebraic systems arising from the discretization of diffusion and convection problems in fluid dynamics [18] are nonsymmetric and diagonal dominant. The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown <ref> [5] </ref> that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting. It should be noted that parallel sparse LU with pivoting is more complicated [10]. We list the BLAS-3 level sparse LU factorization algorithm in Figure 10 (a).
Reference: [6] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Fine-Grained Irregular DAGs. </title> <journal> Parallel Processing Letters, </journal> <volume> 5(4), </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention automatically parallelized code, especially on distributed memory machines. Recently impressive performance is obtained for sparse Cholesky factorization [19, 20] and sparse triangular solver <ref> [6] </ref>. We discuss how we can exploit task parallelism in sparse Cholesky factorization and LU without pivoting using our techniques in a general run-time support environment. One difficulty in parallelizing sparse Cholesky factorization is that parallelism also arises from operations that commute. <p> In [25], it is found that pre-scheduling improves the performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance. Recently the work by <ref> [6] </ref> demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained for solving fine-grained sparse triangular systems. Run-time support for parallel computations. The Charm [24] system adopts a message driven approach for general asynchronous computation using dynamic scheduling. <p> Hence DSM normally has higher overhead than our scheme and has a difficulty for obtaining good performance for sparse matrix problems. Low overhead communication mechanism. The lower-level communication primitives of our system could be implemented using active messages [26] which have been used in <ref> [6] </ref> for executing fine-grained triangular solving DAGs. We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [6]. <p> primitives of our system could be implemented using active messages [26] which have been used in <ref> [6] </ref> for executing fine-grained triangular solving DAGs. We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [6].
Reference: [7] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proc. of Inter. Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In functional languages, the "single assignment" principle has been used in data flow computation and functional languages [17, 21], i.e., each data item can be written at most once, thus there is neither output nor anti dependence. The renaming techniques <ref> [7] </ref> can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph. The advantage of such a principle is that dependence enforcement in task execution can be simplified to some degree. The disadvantage is that memory usage and data copying overhead increase accordingly.
Reference: [8] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures . Journal of Parallel and Distributed Computing, </title> <booktitle> 22(3) </booktitle> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations [12, 16, 27]. For iterative irregular problems in which communication and computation phases alternate, the CHAOS system <ref> [8] </ref> has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. The problems addressed in CHAOS have no loop-carried dependency at the computation phase and processors can run independently before next communication phase.
Reference: [9] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 237-244, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This will become clearer as we discuss in the subsequent sections. More discussion on the above properties and their relationship can be found in [29]. The transformation to convert a DDG to a dependence complete task graph is described in <ref> [9] </ref>. Most task graphs derived from real scientific applications satisfy these properties without renaming. For example, sparse LU graphs discussed in Section 6.1.2 are dependence-complete. 2.2 Handling commutativity One disadvantage of a dependence-complete task graph (or DDG) is that it cannot model parallelism arising from commuting operations. <p> We have developed a set of library functions <ref> [9] </ref> which allow users to specify data access patterns based on a sequential version of the application. We use the PYRROS scheduling algorithms to produce a task schedule and our run-time system will execute such an irregular schedule. <p> The experiments indicate that overhead of run-time data communication is relatively small. The performance of sparse computation is limited by available parallelism, but not the overhead of the system. In <ref> [9, 10] </ref> we have examined the application of these techniques in sparse LU factorization with partial pivoting, which is a more challenging problem and provided an application programming interface (API) for users to parallelize irregular code using our run-time scheme.
Reference: [10] <author> C. Fu and T. Yang. </author> <title> Sparse LU Factorization with Partial Pivoting on Distributed Memory Machines. </title> <booktitle> In Proceedings of Supercomputing'96, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown [5] that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting. It should be noted that parallel sparse LU with pivoting is more complicated <ref> [10] </ref>. We list the BLAS-3 level sparse LU factorization algorithm in Figure 10 (a). Again we use supernode partitioning to divide a sparse matrix and construct the sparse task graph from the partitioned matrix. It can be shown that a LU task graph is dependence-complete. <p> The experiments indicate that overhead of run-time data communication is relatively small. The performance of sparse computation is limited by available parallelism, but not the overhead of the system. In <ref> [9, 10] </ref> we have examined the application of these techniques in sparse LU factorization with partial pivoting, which is a more challenging problem and provided an application programming interface (API) for users to parallelize irregular code using our run-time scheme.
Reference: [11] <author> G. A. Geist and E. Ng. </author> <title> Task Scheduling for Parallel Sparse Cholesky Factorization . International Journal of Parallel Programming, </title> <booktitle> 18(4) </booktitle> <pages> 291-314, </pages> <year> 1989. </year> <month> 19 </month>
Reference-contexts: Rothberg and Schreiber [19, 20] show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in <ref> [11] </ref>. We will examine the performance of applying general task scheduling and executing techniques to this problem. For a sparse matrix, the BLAS-3 level Cholesky factorization algorithm 3 is shown in Figure 7 (a).
Reference: [12] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In D. </title> <editor> Hsu, A. Rosenberg, and D. Sotteau, editors, </editor> <title> Interconnections Networks and Mappings and Scheduling Parallel Computation. </title> <journal> American Math. Society, </journal> <year> 1995. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [12, 16, 27] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS system [8] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. <p> In a more complicated iterative application, the computation phase may involve task parallelism with irregular dependence structures. For example, in the adaptive n-body simulation using the fast mul-tipole method, parallelism at each time step can be modeled as a directed acyclic task graph (DAG) <ref> [12] </ref>. The DAG schedule can be re-used for a number of iterations before re-scheduling since particle movement is slow. <p> These two issues make timing among tasks different from what is expected at the static time and tend to increase the processor idle time. The performance becomes even more sensitive when task granularities are small <ref> [12] </ref>. <p> Application of graph scheduling for solving scientific problems. As we discussed before, application of graph scheduling has been used in large n-body simulations <ref> [12] </ref>. In [25], it is found that pre-scheduling improves the performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance.
Reference: [13] <author> G. Golub and J. M. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing Compilers . Academic Press, </title> <year> 1993. </year>
Reference-contexts: We choose the sparse triangular solver as an example application. Generally a triangular solver is much less time consuming than factorization. A triangular system with multiple right-hand sides is useful in many scientific applications <ref> [13] </ref>. An interesting property with the triangular solver is that it is very easy to adjust the granularity of tasks by using different number of right-hand sides. Again we first use the supernode partitioning techniques to divide a triangular sparse matrix into N fi N 2-D blocks.
Reference: [14] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: This is done by renaming data items and changing edges as depicted in Figure 3. We explain the procedure as follows. For each reduction task 1 This is called fan-in parallelism in the literature <ref> [14] </ref>. <p> For instance, Figure 7 (b) is a 21 fi 21 sparse matrix with fill-ins. After that we use supernode partitioning <ref> [14] </ref> so that every basic task operation involves only dense matrix or vector operations which can be implemented using BLAS and LAPACK routines. We also partition supernodes into smaller blocks. And for those supernodes smaller than the block size used, they will remain unchanged.
Reference: [15] <author> N. Karmarkar. </author> <title> A New Parallel Architecture for Sparse Matrix Computation Based on Finite Project Geometries . In Proc. </title> <booktitle> of Supercomputing '91, </booktitle> <pages> pages 358-369, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In solving nonlinear equations using iterative methods, sparse matrix factorization usually dominates the computation time at each iteration step and the topology of the iteration matrix remains the same but the numerical values change at each step <ref> [15] </ref>. Efficient parallelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. Sparse matrix 1 factorizations can be modeled as DAGs [23].
Reference: [16] <author> A. Lain and P. Banerjee. </author> <title> Techniques to Overlap Computation and Communication in Irregular Iterative Applications. </title> <booktitle> In Proc. of ACM Inter. Conf. on Supercomputing, </booktitle> <pages> pages 236-245, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [12, 16, 27] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS system [8] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase.
Reference: [17] <author> S. Pande, D. P. Agrawal, and J. Mauney. </author> <title> A Scalable Scheduling Scheme for Functional Parallelism on Distributed Memory Multiprocessor Systems . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 6(4) </booktitle> <pages> 388-399, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: With the presence of anti and output dependence, synchronization in run-time support becomes more complicated in order to ensure the correctness of execution. In functional languages, the "single assignment" principle has been used in data flow computation and functional languages <ref> [17, 21] </ref>, i.e., each data item can be written at most once, thus there is neither output nor anti dependence. The renaming techniques [7] can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph.
Reference: [18] <author> R. Peyret and T. Taylor. </author> <title> Computational Methods for Fluid Flow . Springer-Verlag, </title> <year> 1983. </year>
Reference-contexts: Many of algebraic systems arising from the discretization of diffusion and convection problems in fluid dynamics <ref> [18] </ref> are nonsymmetric and diagonal dominant. The boundary conditions usually make the first row of matrices strictly diagonal dominant. For such matrices, it can be shown [5] that stable solutions can be obtained by LU (Gaussian Elimination) without pivoting.
Reference: [19] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization . PhD thesis, </title> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix computations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention automatically parallelized code, especially on distributed memory machines. Recently impressive performance is obtained for sparse Cholesky factorization <ref> [19, 20] </ref> and sparse triangular solver [6]. We discuss how we can exploit task parallelism in sparse Cholesky factorization and LU without pivoting using our techniques in a general run-time support environment. One difficulty in parallelizing sparse Cholesky factorization is that parallelism also arises from operations that commute. <p> The source code of LINPACK is obtained from the public domain. 10 6.1 Sparse matrix factorizations and run-time overhead analysis 6.1.1 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber <ref> [19, 20] </ref> show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [11]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> The speedup on 32 processors is 11:33 for BCSSTK15 and processor utilization efficiency is 35:4%. In [20], efficiency 25% has been reported for BCSSTK15 on 64 processors (Intel Paragon) after improving the load balancing strategy. This indicates 4 In comparison, Rothberg <ref> [19] </ref> reported that the sequential performance of his code achieved 77% of LINPACK for BC-SSTK15 and 71% in average for other matrices on IBM RS/6000 Model 320. 12 that the parallelism is limited in the sparse problems and our scheduled code has achieved reasonable performance. <p> In order to improve the performance further, our future work is to consider techniques of increasing average supernode sizes, e.g., supernode amalgamation <ref> [2, 19] </ref>. 6.2 Sparse triangular solver and performance impact of granularities From the discussion in the last section we know that the task granularity of an application is important to the performance of irregular code.
Reference: [20] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of Supercomputing'94, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix computations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention automatically parallelized code, especially on distributed memory machines. Recently impressive performance is obtained for sparse Cholesky factorization <ref> [19, 20] </ref> and sparse triangular solver [6]. We discuss how we can exploit task parallelism in sparse Cholesky factorization and LU without pivoting using our techniques in a general run-time support environment. One difficulty in parallelizing sparse Cholesky factorization is that parallelism also arises from operations that commute. <p> The source code of LINPACK is obtained from the public domain. 10 6.1 Sparse matrix factorizations and run-time overhead analysis 6.1.1 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in literature. Rothberg and Schreiber <ref> [19, 20] </ref> show that the supernode-based approach can deliver good performance on both shared and distributed memory machines. Specialized scheduling techniques have been studied for sparse Cholesky factorization in [11]. We will examine the performance of applying general task scheduling and executing techniques to this problem. <p> In average, we have obtained speedup 7 on 16 processors and 11:3 on 32 processors. We will discuss the overhead of run-time execution in Section 6.1.3. The speedup on 32 processors is 11:33 for BCSSTK15 and processor utilization efficiency is 35:4%. In <ref> [20] </ref>, efficiency 25% has been reported for BCSSTK15 on 64 processors (Intel Paragon) after improving the load balancing strategy.
Reference: [21] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors . MIT Press, </title> <year> 1989. </year>
Reference-contexts: Compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because partitioned graphs contain both coarse and fine grained tasks. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g. <ref> [21, 28] </ref> and little research has been conducted on efficient run-time support for executing task schedules. <p> With the presence of anti and output dependence, synchronization in run-time support becomes more complicated in order to ensure the correctness of execution. In functional languages, the "single assignment" principle has been used in data flow computation and functional languages <ref> [17, 21] </ref>, i.e., each data item can be written at most once, thus there is neither output nor anti dependence. The renaming techniques [7] can eliminate the output and anti dependence in transforming a DDG to a "single assignment" graph.
Reference: [22] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1995. </year>
Reference-contexts: We have implemented our system on Meiko CS-2 which provides Direct Memory Access (DMA) as the major way to access non-local memory. Each node of Meiko CS-2 is also equipped with a DMA co-processor for handling communication. It takes the main processor 9 microseconds <ref> [22] </ref> to dispatch the remote memory access descriptor to the co-processor. The co-processor afterwards will be responsible for sending data without interrupting the main processor so that the opportunity of overlapping communication and computation is maximized. <p> The cost to dispatch a RMA request is 9 microseconds. The communication peak bandwidth of Meiko CS-2 is 40MBytes/Sec. The effective peak bandwidth has been reported as 39MB/Sec, but this is obtained through a ping-pong test <ref> [22] </ref> and message caching improves the performance implicitly. In practice, a message is sent only once at a time. We find that the effective bandwidth of a single-message sending or direct memory access is in the range of 10-25 MB/Sec when the sizes of messages vary from 1K to 10K.
Reference: [23] <author> R. Schreiber. </author> <title> Scalability of Sparse Direct Solvers, volume 56 of Graph Theory and Sparse Matrix Computation (Edited by Alan George and John R. </title> <editor> Gilbert and Joseph W.H. </editor> <booktitle> Liu), </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Efficient parallelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. Sparse matrix 1 factorizations can be modeled as DAGs <ref> [23] </ref>. Compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because partitioned graphs contain both coarse and fine grained tasks.
Reference: [24] <author> W. Shu and L. Kale. </author> <title> Chare Kernel A Runtime Support System for Parallel Computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(3) </volume> <pages> 198-211, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Recently the work by [6] demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained for solving fine-grained sparse triangular systems. Run-time support for parallel computations. The Charm <ref> [24] </ref> system adopts a message driven approach for general asynchronous computation using dynamic scheduling. The Cilk [3] multi-threaded system aims at applications that have "strict" dependence structures. Randomized load balancing and work stealing techniques are used to execute a dynamic DAG.
Reference: [25] <author> S. Venugopal, V. Naik, and J. Saltz. </author> <title> Performance of Distributed Sparse Cholesky Factorization with Pre-scheduling. </title> <booktitle> In Proc. of Supercomputing'92, </booktitle> <pages> pages 52-61, </pages> <address> Minneapolis, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Application of graph scheduling for solving scientific problems. As we discussed before, application of graph scheduling has been used in large n-body simulations [12]. In <ref> [25] </ref>, it is found that pre-scheduling improves the performance of distributed sparse Cholesky factorization by 30% to 40% and there is still a lot of room for obtaining better absolute performance.
Reference: [26] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Usually in order to support asynchronous communication, a message passing system such as Intel NX/2 uses system buffer space to manage incoming and outgoing messages. It is well known that message-buffering imposes a significant amount of overheads for copying and space management on source and destination processors <ref> [26] </ref>. Secondly, effort is required to maintain data dependencies among tasks. Each task has to receive the correct copies of the desired data items. Figure 4 shows a situation in which a task could receive a wrong copy of the desired data item. <p> Hence DSM normally has higher overhead than our scheme and has a difficulty for obtaining good performance for sparse matrix problems. Low overhead communication mechanism. The lower-level communication primitives of our system could be implemented using active messages <ref> [26] </ref> which have been used in [6] for executing fine-grained triangular solving DAGs. We find that it is not easy to integrate active messages with a general task graph execution scheme because careful network polling is required as demonstrated in [6].
Reference: [27] <author> C.-P. Wen, S. Chakrabarti, E. Deprit, A. Krishnamurthy, and K. Yelick. </author> <title> Runtime Support for Portable Distributed Data Structures, chapter 9. Languages, Compilers, and Runtime Systems for Scalable Computers. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [12, 16, 27] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS system [8] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. <p> Compared to our work, Charm and Cilk are suitable for relatively coarser grain computation because dynamic load balancing on distributed memory machines has relatively high control overhead. There are also other run-time systems developed for irregular computations. We have mentioned Chaos in the introduction. Multipol <ref> [27] </ref> is a run-time library system which supports distributed data structures for several kinds of scientific applications. Consistency models. The issue of executing a program correctly has been studied in the context of distributed shared memory systems (DSM), e.g. TreadMarks [1].
Reference: [28] <author> M. Y. Wu and D. Gajski. Hypertool: </author> <title> A Programming Aid for Message-passing Systems . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 1(3) </booktitle> <pages> 330-343, </pages> <year> 1990. </year>
Reference-contexts: Compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because partitioned graphs contain both coarse and fine grained tasks. Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g. <ref> [21, 28] </ref> and little research has been conducted on efficient run-time support for executing task schedules.
Reference: [29] <author> T. Yang. </author> <title> Scheduling and Code Generation for Parallel Architectures . PhD thesis, </title> <institution> Dept. of Computer Science, Rutgers University, </institution> <address> New Brunswick, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g. [21, 28] and little research has been conducted on efficient run-time support for executing task schedules. The early work in the PYRROS system <ref> [29, 30] </ref> provides a complete framework for general task computation; however, its run-time support system uses the NX/2 level communication interface and overhead for message management is high, which prevents PYRROS from obtaining good performance for sparse matrix problems. <p> These constraints provide a basis for us to design highly efficient execution scheme, thus to exploit parallelism residing in the graph as much as possible. This will become clearer as we discuss in the subsequent sections. More discussion on the above properties and their relationship can be found in <ref> [29] </ref>. The transformation to convert a DDG to a dependence complete task graph is described in [9]. Most task graphs derived from real scientific applications satisfy these properties without renaming.
Reference: [30] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year>
Reference-contexts: Most of previous work in DAG scheduling has mainly focused on the algorithmic research for task mapping, e.g. [21, 28] and little research has been conducted on efficient run-time support for executing task schedules. The early work in the PYRROS system <ref> [29, 30] </ref> provides a complete framework for general task computation; however, its run-time support system uses the NX/2 level communication interface and overhead for message management is high, which prevents PYRROS from obtaining good performance for sparse matrix problems. <p> Optimizations have to be performed to eliminate redundant messages. Correspondingly, in the destination only one receiving is needed to pull out data from the network. Subsequent receiving operations for this data item should be re-directed to read from the local memory instead. PYRROS <ref> [30] </ref> uses a buffered message-passing mechanism and aggregates communication as much as possible.
References-found: 30

