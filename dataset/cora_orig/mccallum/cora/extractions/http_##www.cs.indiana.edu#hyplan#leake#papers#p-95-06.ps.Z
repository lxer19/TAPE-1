URL: http://www.cs.indiana.edu/hyplan/leake/papers/p-95-06.ps.Z
Refering-URL: ftp://ftp.cs.indiana.edu/pub/leake/INDEX.html
Root-URL: http://www.cs.indiana.edu
Email: leake@cs.indiana.edu  
Title: Towards Goal-Driven Integration of Explanation and Action  
Author: David B. Leake 
Note: To appear in Ram, A. and Leake, D., Eds., Goal-Driven Learning. MIT Press/Bradford Books. In press.  
Date: 812-855-9756  
Address: Lindley Hall 215, Bloomington, IN 47405  
Affiliation: Computer Science Department, Indiana University  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ajjanagadde, V. </author> <year> (1991). </year> <title> Incorporating background knowledge and structured explananda in ab-ductive reasoning: A framework. </title> <institution> Wilhelm Schickard-Institute fur Informatik WSI 91-6, Uni-versitat Tubingen. </institution>
Reference: <author> Barletta, R. and Mark, W. </author> <year> (1988). </year> <title> Explanation-based indexing of cases. </title> <editor> In Kolodner, J., editor, </editor> <booktitle> Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <pages> pages 50-60, </pages> <address> Palo Alto. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference: <author> Birnbaum, L., Collins, G., Freed, M., and Krulwich, B. </author> <year> (1990). </year> <title> Model-based diagnosis of planning failures. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 318-323, </pages> <address> Boston, MA. </address> <publisher> AAAI. </publisher>
Reference: <author> Chien, S. </author> <year> (1989). </year> <title> Using and refining simplifications: Explanation-based learning of plans in intractable domains. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 590-595, </pages> <address> Detroit, MI. IJCAI. </address>
Reference: <author> Clancey, W. and Shortliffe, E., </author> <title> editors (1984). </title> <booktitle> Readings in Medical Artificial Intelligence: The First Decade. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Although differential diagnosis is a common approach in medical expert systems (e.g., <ref> (Clancey and Shortliffe, 1984) </ref>), it has received very little attention in other applications of abductive explanation such as abductive understanding.
Reference: <author> Cullingford, R. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD thesis, </type> <institution> Yale University. </institution> <note> Computer Science Department Technical Report 116. </note> <author> de Kleer, J. and Williams, B. </author> <year> (1989). </year> <title> Diagnosis with behavioral modes. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1324-1330, </pages> <address> Detroit, MI. IJCAI. </address>
Reference: <author> DeJong, G. </author> <year> (1986). </year> <title> An approach to learning from observation. </title> <editor> In Michalski, R., Carbonell, J., and Mitchell, T., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> volume 2, </volume> <pages> pages 571-590. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Cambridge, MA. </address>
Reference: <author> DeJong, G. and Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 145-176. </pages> <editor> desJardins, M. </editor> <year> (1992). </year> <title> Goal-directed learning: A decision-theoretic model for deciding what to learn next. </title> <booktitle> In Proceedings of the Machine Discovery Workshop, Ninth International Machine Learning Conference, </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Doyle, A. C. </author> <year> (1976). </year> <title> The adventure of Silver Blaze. In Memoirs of Sherlock Holmes, </title> <booktitle> chapter 13, </booktitle> <pages> pages 1-16. </pages> <publisher> Schocken Books, </publisher> <address> New York. </address>
Reference-contexts: This reflects an idealized conception of explanation as an act of pure reasoning, following the spirit of Sherlock Holmes' directive that "the art of the reasoner should be used rather for the sifting of details than for the acquiring of fresh evidence" <ref> (Doyle, 1976, p. 1) </ref>. Accordingly, the explanation generation component cannot call upon other parts of the reasoner to aid in the explanation effort; as a result, it has no means to obtain any information beyond the information provided to it at the start of the explanation process.
Reference: <author> Freitag, H. and Friedrich, G. </author> <year> (1991). </year> <title> Goal-driven structural focusing in model-based diagnosis. </title> <booktitle> In Working Notes of the Second International Workshop on Principles of Diagnosis, </booktitle> <institution> Milano, Italy. CISE Tecnologie Innovative and Dipartimento di Informatica of the Universita di Torino. </institution>
Reference: <author> Gratch, J. and DeJong, G. </author> <year> (1993). </year> <title> Assessing the value of information to guide learning systems. </title> <booktitle> In Proceedings of the ML-93 Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <pages> pages 65-71, </pages> <address> Amherst, MA. </address>
Reference: <author> Hammond, K. </author> <year> (1989). </year> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <publisher> Academic Press, </publisher> <address> San Diego. </address> <note> 20 Hammond, </note> <author> K. and Seifert, C. </author> <year> (1991). </year> <title> Towards a content model of strategic explanation. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 149-154, </pages> <address> Chicago, IL. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: Interactive Explanation In learning systems that use explanations, the explanation generation process is generally treated as an isolated subroutine. That subroutine may be provided with descriptions of what should be explained, such as target concepts (Mitchell et al., 1986) or particular anomalous states <ref> (Hammond, 1989) </ref>; it may be provided with additional criteria for what constitutes a useful explanation, such as operationality criteria (Mostow, 1983; Keller, 1988); and it may also be able to access system background knowledge to verify particular assumptions.
Reference: <author> Harvey, A. and Bordley, J. </author> <year> (1970). </year> <title> Differential Diagnosis; The interpretation of clinical evidence. </title> <publisher> Saunders, Philadelphis. </publisher>
Reference-contexts: Maintaining a set of alternative explanations and reasoning about the relative utility of pursuing the competing alternatives changes the explanation process from focusing on single explanations to focusing on a group of competing explanations, in a process more like differential diagnosis in medicine (e.g., <ref> (Harvey and Bordley, 1970) </ref>), in which competing hypotheses are simultaneously considered and weighed against one another. Although differential diagnosis is a common approach in medical expert systems (e.g., (Clancey and Shortliffe, 1984)), it has received very little attention in other applications of abductive explanation such as abductive understanding.
Reference: <author> Hunter, L. </author> <year> (1990a). </year> <title> Knowledge acquisition planning for inference from large datasets. </title> <editor> In Shriver, B., editor, </editor> <booktitle> Proceedings of the Twenty Third Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 35-45, </pages> <address> Kona, HI. </address>
Reference: <author> Hunter, L. </author> <year> (1990b). </year> <title> Planning to learn. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 261-268, </pages> <address> Cambridge, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: requires reasoning about the types of knowledge goals driving the system and the functionality of different types of strategies for performing the needed learning (Ram and Cox, 1994); it also requires reasoning about the resources required to perform particular information-seeking actions, the circumstances under which they apply, and their tradeoffs <ref> (Hunter, 1990b) </ref>.
Reference: <author> Josephson, J. and Josephson, S. </author> <year> (1994). </year> <title> Abductive Inference: Computation, Philosophy, Technology. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference: <author> Kass, A. </author> <year> (1990). </year> <title> Developing Creative Hypotheses by Adapting Explanations. </title> <type> PhD thesis, </type> <institution> Yale University. Northwestern University Institute for the Learning Sciences, </institution> <type> Technical Report 6. </type>
Reference-contexts: However, it augments that process by using a library of explanations of anomalies to explain novel events when its schema-based understanding is inadequate. 1 As part of the SWALE system, ACCEPTER can also call upon an external module, the TWEAKER <ref> (Kass, 1990) </ref>, to adapt existing explanations to fit new situations when needed; as a stand-alone explanation system, its attempt to explain simply fails if none of its stored explanations applies to the new situation. One of the questions addressed by ACCEPTER is the question of when to explain. <p> However, although the case adaptation process for case-based explanation has been extensively studied for repairing plausibility problems <ref> (Kass, 1990) </ref>, little work has been done on developing adaptation strategies responding to particular types of information needs. Developing effective strategies for other needs is an important open problem. 2 ACCEPTER explains each anomaly as soon as it is detected; consequently it explains only one anomaly at a time.
Reference: <author> Kautz, H. and Allen, J. </author> <year> (1986). </year> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 32-37, </pages> <address> Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference: <author> Kedar-Cabelli, S. </author> <year> (1987). </year> <title> Formulating concepts according to purpose. </title> <booktitle> In Proceedings of the Sixth Annual National Conference on Artificial Intelligence, </booktitle> <pages> pages 477-481, </pages> <address> Seattle, WA. </address> <publisher> AAAI. </publisher>
Reference: <author> Keller, R. </author> <year> (1988). </year> <title> Defining operationality for explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 35(2) </volume> <pages> 227-241. </pages>
Reference: <author> Krulwich, B., Birnbaum, L., and Collins, G. </author> <year> (1990). </year> <title> Goal-directed diagnosis of expectation failures. </title> <editor> In O'Rorke, P., editor, </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pages 116-119. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Leake, D. </author> <year> (1988). </year> <title> Evaluating explanations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 251-255, </pages> <address> Minneapolis, MN. </address> <publisher> AAAI, Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Leake, D. </author> <year> (1991a). </year> <title> Goal-based explanation evaluation. </title> <journal> Cognitive Science, </journal> <volume> 15(4) </volume> <pages> 509-545. </pages>
Reference-contexts: Thus a model of goal-driven explanation must be based on a theory of the tasks of a reasoner and the classes of information needs that are generated by particular tasks. (This has been done for a number of tasks in <ref> (Leake, 1991a) </ref>, Chapter 9). It also requires developing mechanisms for identifying each type of need when it arises, either in response to explicit failures or due to other causes. 2. <p> The need to respond to those threats and opportunities may prompt additional information requirements beyond those for merely resolving the anomaly at hand, such as those described in <ref> (Leake, 1991a) </ref>, (Chapter 9 in this volume). In turn, those information requirements may require elaboration of the initial explanation generated for the anomaly, as shown in figure 2. <p> Likewise, when understanding is done in service of additional tasks, ACCEPTER characterizes the information needs for those tasks (see <ref> (Leake, 1991a) </ref>, Chapter 9 of this volume), generating additional knowledge goals and passing them to an adaptation process that selects strategies for adapting the retrieved explanation to satisfy those goals (e.g., by elaborating portions of the explanation that are too vague to provide useful information).
Reference: <author> Leake, D. </author> <year> (1991b). </year> <title> An indexing vocabulary for case-based explanation. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 10-15, </pages> <address> Anaheim, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Leake, D. </author> <year> (1992). </year> <title> Evaluating Explanations: A Content Theory. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Consequently, a model of real-world explanation must address the questions of when to explain, what to explain, and how to focus explanation generation <ref> (Leake, 1992) </ref>. This chapter addresses those questions within the goal-driven learning framework described in Chapter 1 of this volume. In so doing, it provides one set of answers to the goal-driven learning questions of when learning is needed, what should be learned, and how to guide the learning process. <p> A complete description of ACCEPTER is available in <ref> (Leake, 1992) </ref>. 7.2 Deciding when to explain ACCEPTER's basic understanding process is schema-based; it understands routine events in stories by fitting new information into pre-stored schemas in a process along the lines of those discussed by various researchers (Cullingford, 1978; Lebowitz, 1980; Schank, 1982; Schank 11 and Abelson, 1977)).
Reference: <author> Leake, D. </author> <year> (1994). </year> <title> ACCEPTER: Evaluating explanations. </title> <editor> In Schank, R., Riesbeck, C., and Kass, A., editors, </editor> <title> Inside Case-Based Explanation. </title> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Research is underway to perform that integration, integrating explanation with a planner and execution system in a simple simulated world <ref> (Sooriamurthi and Leake, 1994) </ref>. The system explains vehicle malfunctions from the perspective of stranded motorist who is explaining in order to further the goal of reaching a destination under time and resource constraints, in a dynamically changing simulated environment.
Reference: <author> Leake, D. and Owens, C. </author> <year> (1986). </year> <title> Organizing memory for explanation. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 710-715, </pages> <address> Amherst, MA. </address> <institution> Cognitive Science Society. </institution> <note> 21 Lebowitz, </note> <author> M. </author> <year> (1980). </year> <title> Generalization and Memory in an Integrated Understanding System. </title> <type> PhD thesis, </type> <institution> Yale University. Computer Science Department Technical Report 186. </institution>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Mitchell, T., Keller, R., and Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80. </pages>
Reference-contexts: Explanations can play a key role in making that determination. For example, in explanation-based generalization, explanations are used to identify operational features of a training example that are relevant to membership in a target concept <ref> (Mitchell et al., 1986) </ref>; in explanation-based story understanding, explanations determine the features of a situation that are important for establishing coherence with prior knowledge and generating accurate predictions to facilitate future understanding (DeJong, 1986; Leake, 1992; Mooney, 1990; Ram, 1993; Schank, 1986; Wilensky, 1983); in planning and problem-solving, explanations are used <p> Interactive Explanation In learning systems that use explanations, the explanation generation process is generally treated as an isolated subroutine. That subroutine may be provided with descriptions of what should be explained, such as target concepts <ref> (Mitchell et al., 1986) </ref> or particular anomalous states (Hammond, 1989); it may be provided with additional criteria for what constitutes a useful explanation, such as operationality criteria (Mostow, 1983; Keller, 1988); and it may also be able to access system background knowledge to verify particular assumptions. <p> For example, if a mechanic finds a defective engine component when trying to diagnose bad acceleration, an explanation of the acceleration problem might trace how the observed acceleration problem was caused by the defects in the component. The type of derivation may be interpreted as deductive proof <ref> (e.g., Mitchell et al., 1986) </ref>, or as plausible reasoning (e.g., Pearl, 1988; Schank, 1986), but the basic structure remains the same. Thus a fundamental question for such models of explanation is how to effectively generate explanatory chains. <p> When ACCEPTER detects an anomaly, it formulates criteria for the information that the explanation must provide to resolve the anomaly. Thus unlike explanation-based generalization systems in which the target concept to explain is provided as an input to the explanation system <ref> (Mitchell et al., 1986) </ref>, and unlike other systems in which novel events are explained according to a single fixed perspective (e.g., to find the actor's motivation (DeJong, 1986; Mooney, 1990; Wilensky, 1978)), ACCEPTER dynamically decides which features of the situation must be accounted for, according to its current needs to resolve
Reference: <author> Mooney, R. </author> <year> (1990). </year> <title> A General Explanation-based Learning Mechanism and its Application to Narrative Understanding. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo. </address>
Reference-contexts: Some research has examined the change of focus during explanation based on incremental plausibility estimates for the tasks of diagnosis (e.g., (de Kleer and Williams, 1989)) and understanding (e.g., <ref> (Ng and Mooney, 1990) </ref>); plausibility considerations must be augmented with utility criteria reflecting the information contained in a partial explanation, current knowledge goals, their relative priorities, and factors in the external world affecting the difficulty 6 of pursuing the candidate explanation by alternative means.
Reference: <author> Moore, J. and Swartout, W. </author> <year> (1989). </year> <title> A reactive approach to explanation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1504-1510, </pages> <address> Detroit, MI. IJCAI. </address>
Reference: <author> Mostow, J. </author> <year> (1983). </year> <title> Machine transformation of advice into a heuristic search procedure. </title> <editor> In Michalski, R., Carbonell, J., and Mitchell, T., editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Ng, H. and Mooney, R. </author> <year> (1990). </year> <title> On the role of coherence in abductive explanation. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 337-342, </pages> <address> Boston, MA. </address> <publisher> AAAI. </publisher>
Reference-contexts: Some research has examined the change of focus during explanation based on incremental plausibility estimates for the tasks of diagnosis (e.g., (de Kleer and Williams, 1989)) and understanding (e.g., <ref> (Ng and Mooney, 1990) </ref>); plausibility considerations must be augmented with utility criteria reflecting the information contained in a partial explanation, current knowledge goals, their relative priorities, and factors in the external world affecting the difficulty 6 of pursuing the candidate explanation by alternative means.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Pierce, C. </author> <year> (1948). </year> <title> Abduction and induction. </title> <editor> In Buchler, J., editor, </editor> <booktitle> The Philosophy of Pierce: Selected Writings, chapter 11. </booktitle> <publisher> Harcourt, Brace and Company, </publisher> <address> New York. </address>
Reference: <author> Pryor, L. and Collins, G. </author> <year> (1992). </year> <title> Planning to perceive. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Selective Perception, </booktitle> <address> Stanford, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Quilici, A. </author> <year> (1994). </year> <title> Modeling the goal-driven learning of novice unix users. </title> <editor> In desJardins, M. and Ram, A., editors, </editor> <booktitle> Proceedings of the 1994 Spring Symposium on Goal-Driven Learning, </booktitle> <address> Stanford, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Rajamoney, S. </author> <year> (1993). </year> <title> Designing experiments to extend the domain theory. </title> <editor> In DeJong, G., editor, </editor> <title> Investigating Explanation-Based Learning, </title> <booktitle> chapter 5, </booktitle> <pages> pages 166-189. </pages> <publisher> Kluwer. </publisher>
Reference-contexts: In that model both action and reasoning, including explanation, are used to satisfy the learner's knowledge goals. Existing models sometimes take an active approach to verifying explanations in the external environment, using experimentation to choose between candidate explanations <ref> (e.g., Rajamoney, 1993) </ref>. In GDIE, action in the world is also used to help generating explanations, rather than only being used as part of a selection process for choosing between the candidate explanations that have already been built.
Reference: <author> Ram, A. </author> <year> (1991). </year> <title> A theory of questions and question asking. </title> <journal> The Journal of the Learning Sciences, </journal> <volume> 1(3 </volume> & 4):273-318. 
Reference: <author> Ram, A. </author> <year> (1993). </year> <title> Indexing, elaboration and refinement: Incremental learning of explanatory cases. </title> <journal> Machine Learning, </journal> <volume> 10(3) </volume> <pages> 201-248. </pages>
Reference-contexts: to attribute the problem to the cold as an initial hypothesis and to respond to it by keeping the car in the garage on cold nights, but to store other alternative hypotheses (e.g., a weak battery) to be considered if information relevant to the choice between them becomes available opportunistically <ref> (Ram, 1993) </ref>. In GDIE the explanation task itself is pursued not as the highest-level reasoner goal, to be satisfied for its own sake, as is generally the case in diagnosis systems, but explicitly in service of other goals.
Reference: <author> Ram, A. and Cox, M. </author> <year> (1994). </year> <title> Introspective reasoning using meta-explanations for multistrategy learning. </title> <editor> In Michalski, R. and Tecuci, G., editors, </editor> <title> Machine Learning: A Multistrategy Approach. </title> <publisher> Morgan Kaufmann. In Press. </publisher>
Reference-contexts: Choosing which strategies to apply requires reasoning about the types of knowledge goals driving the system and the functionality of different types of strategies for performing the needed learning <ref> (Ram and Cox, 1994) </ref>; it also requires reasoning about the resources required to perform particular information-seeking actions, the circumstances under which they apply, and their tradeoffs (Hunter, 1990b).
Reference: <author> Redmond, M. </author> <year> (1992). </year> <title> Learning by Observing and Understanding Expert Problem Solving. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology. </institution> <note> Technical report GIT-CC-92/43. </note>
Reference: <author> Riesbeck, C. </author> <year> (1981). </year> <title> Failure-driven reminding for incremental learning. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 115-120, </pages> <address> Vancouver, </address> <institution> B.C. IJCAI. </institution>
Reference: <author> Rymon, R. </author> <year> (1993). </year> <title> Diagnostic Reasoning and Planning in Exploratory-Corrective Domains. </title> <type> PhD thesis, </type> <institution> The University of Pennsylvania. </institution>
Reference-contexts: Goal-based considerations have begun to be examined in research on diagnosis (e.g., (Freitag and Friedrich, 1991; Redmond, 1992; Rymon, 1993; Turner, 1994)). Such models of diagnosis include doing explanation as part of a cycle of diagnosis and repair <ref> (Rymon, 1993) </ref> and interleaving reasoning with action (Redmond, 1992; Turner, 1994); and making context-sensitive choices of what to diagnose first (Turner, 1994).
Reference: <author> Schank, R. </author> <year> (1982). </year> <title> Dynamic Memory: A Theory of Learning in Computers and People. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference: <author> Schank, R. </author> <year> (1986). </year> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Another explanation strategy would be to complete examination of an input to find all anomalies before beginning explanation, and to attempt to simultaneously explain those multiple anomalies using strategies such as coordination of anomalies <ref> (Schank, 1986) </ref>. 14 Figures 3 and 4 illustrate this process in context of the previous example of understanding an ATM break-in.
Reference: <author> Schank, R. and Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Schank, R. and Leake, D. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <note> 40(1-3):353-385. Also in Carbonell, </note> <editor> J., editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: <editor> Schank, R., Riesbeck, C., and Kass, A., editors (1994). </editor> <title> Inside Case-Based Explanation. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale New Jersey. </address>
Reference: <author> Schauble, L., Glaser, R., Raghavan, K., and Reiner, M. </author> <year> (1992). </year> <title> The integration of knowledge and experimentation strategies in understanding a physical system. </title> <journal> Applied Cognitive Psychology, </journal> <volume> 6 </volume> <pages> 321-343. </pages>
Reference-contexts: Psychological data show that an important component of human model-building is the joint application of active strategies and prior knowledge that mutually constrain the model-building process <ref> (Schauble et al., 1992) </ref>; the proposed model takes a related view of explanation generation as involving mutually contstraining knowledge and action.
Reference: <author> Sooriamurthi, R. and Leake, D. </author> <year> (1994). </year> <title> Towards situated explanation. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address> <note> AAAI. Research Abstract. </note>
Reference-contexts: Research is underway to perform that integration, integrating explanation with a planner and execution system in a simple simulated world <ref> (Sooriamurthi and Leake, 1994) </ref>. The system explains vehicle malfunctions from the perspective of stranded motorist who is explaining in order to further the goal of reaching a destination under time and resource constraints, in a dynamically changing simulated environment.
Reference: <author> Suchman, L. </author> <year> (1987). </year> <title> Plans and Situated Actions: the Problem of Human-Machine Communication. </title> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: As a result, the goal-driven interactive explanation process is more "situated" <ref> (e.g., Suchman, 1987) </ref> in its environment than conventional explanation models. However, despite the importance of environmental factors, the model is committed to strategic and deliberative reasoning rather than purely reactive activity. <p> As already discussed, the spirit of goal-driven interactive explanation relates to theories of situated activity <ref> (e.g., Suchman, 1987) </ref> in its emphasis on the role of interactions with the environment in guiding action. In GDIE, explanation is viewed as part of an ongoing process of interaction between explanation, other internal processes, and the external world.
Reference: <author> Turner, R. M. </author> <year> (1994). </year> <title> Adaptive Reasoning for Real-World Problems: A Schema-Based Approach. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: A difficult open problem is how to estimate the utility of particular actions in complex and imperfectly understood situations. Likewise, the process must take into account how new information changes the set of explainer goals <ref> (Turner, 1994) </ref>. As circumstances change, the best means for pursuing a particular explanation may change. <p> different means of determining whether a particular part is defective, such as attempting to remember and compare the symptoms of previous failures, disassembling the part, swapping the suspect part with a replacement and observing the effects of the swap, etc.; this explanation process can be modeled as a planning process <ref> (Turner, 1994) </ref>. In general, which operations are appropriate will depend strongly on both the current external environment and system capabilities and resources (e.g., the time and tools available to the mechanic and the mechanic's competing obligations). <p> Such models of diagnosis include doing explanation as part of a cycle of diagnosis and repair (Rymon, 1993) and interleaving reasoning with action (Redmond, 1992; Turner, 1994); and making context-sensitive choices of what to diagnose first <ref> (Turner, 1994) </ref>. A goal of GDIE is to integrate all 17 these aspects of diagnosis into a uniform framework, and to apply that framework to guide explanation and action in support of all types of system knowledge goals.
Reference: <author> Veloso, M. and Carbonell, J. </author> <year> (1993). </year> <title> Derivational analogy in prodigy: Automating case acquisition, storage, and utilization. </title> <journal> Machine Learning, </journal> <volume> 10(3) </volume> <pages> 249-278. </pages>
Reference: <author> Wilensky, R. </author> <year> (1978). </year> <title> Understanding Goal-Based Stories. </title> <type> PhD thesis, </type> <institution> Yale University. Computer Science Department Technical Report 140. </institution>
Reference: <author> Wilensky, R. </author> <year> (1983). </year> <title> Planning and Understanding. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address> <month> 23 </month>
References-found: 56

