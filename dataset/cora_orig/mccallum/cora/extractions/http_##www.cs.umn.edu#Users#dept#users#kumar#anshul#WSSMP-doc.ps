URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/WSSMP-doc.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/
Root-URL: http://www.cs.umn.edu
Title: WSSMP: Watson Symmetric Sparse Matrix Package The User Interface: Version 4.0.1  
Author: Anshul Gupta P. O. Box Mahesh Joshi and Vipin Kumar 
Keyword: LIMITED DISTRIBUTION NOTICE  
Affiliation: Computer Science/Mathematics IBM Research  IBM Research Division T.J. Watson Research Center  Yorktown  Department of Computer Science University of Minnesota  IBM Research Division Almaden Austin China Delhi Haifa Tokyo Watson Zurich  
Address: 20923 (92669)  Heights, NY 10598  Minneapolis, MN 55455  
Note: RC  
Pubnum: Report  
Email: anshul@watson.ibm.com  fmjoshi and kumarg@cs.umn.edu  
Date: July 17, 1997  
Abstract: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g., payment of royalties). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Chu, Alan George, Joseph W.-H. Liu, and Esmond G.-Y. Ng. </author> <title> Users guide for SPARSPAK-A: Waterloo sparse linear equations package. </title> <type> Technical Report CS-84-36, </type> <institution> University of Waterloo, Waterloo, IA, </institution> <year> 1984. </year>
Reference-contexts: DIAG is accessed only in the factorization phase if either IPARM (4) is 1 or IPARM (32) is 1. 4.2.6 PERM, (type I or O): permutation vector INTEGER PERM ( N ) PERM is the permutation vector, as defined in Sparspak <ref> [1] </ref>. If J = PERM (I), then the J-th row and column of the original matrix become the I-th row and column in the permuted matrix to be factored. <p> It is different (but similar in properties) to the permutation on P 0 at the beginning of symbolic factorization. 4.2.7 INVP, (type I or O): inverse permutation vector INTEGER INVP ( N ) INVP is the inverse permutation vector, as defined in Sparspak <ref> [1] </ref>. If J = INVP (I), then the I-th row and column of the original matrix become the J-th row and column in the permuted matrix to be factored.
Reference: [2] <author> A. K. Cline, Cleve B. Moler, G. W. Stewart, and J. H. Wilkinson. </author> <title> An estimate for the condition number of a matrix. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 368-375, </pages> <year> 1979. </year>
Reference-contexts: The condition number of the matrix A is the product DPARM (2) fi DPARM (3). The condition number is a measure of the reliability of the solution of a given linear system and can be used to compute error bounds. The algorithm described in <ref> [2] </ref> is used to estimate jjA 1 jj. If IPARM (25) = 1, then 1-norms are computed, if IPARM (25) = 2, then Frobenius-norms are computed, and if IPARM (25) = 3, then infinity-norms are computed.
Reference: [3] <author> Anshul Gupta. </author> <title> Analysis and Design of Scalable Parallel Algorithms for Scientific Computing. </title> <type> PhD thesis, </type> <institution> University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year>
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm <ref> [7, 6, 3] </ref>. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP [4, 5] for computing fill-reducing orderings.
Reference: [4] <author> Anshul Gupta. </author> <title> Fast and effective algorithms for graph partitioning and sparse matrix ordering. </title> <journal> IBM Journal of Research and Development, </journal> 41(1/2):171-183, January/March, 1997. 
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm [7, 6, 3]. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP <ref> [4, 5] </ref> for computing fill-reducing orderings.
Reference: [5] <author> Anshul Gupta. WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package: Users manual. </title> <type> Technical Report RC 20453 (90427), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 6, </month> <year> 1996. </year>
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm [7, 6, 3]. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP <ref> [4, 5] </ref> for computing fill-reducing orderings. <p> and WSMALZ (analyze, MSR input) WSCALZ ( N, IA, JA, OPTIONS, PERM, INVP, NNZL, WSPACE, AUX, NAUX ) WSMALZ ( N, IA, JA, OPTIONS, PERM, INVP, NNZL, WSPACE, AUX, NAUX ) 30 The description of OPTIONS, AUX, and NAUX inputs is the same as in the calling sequence of WMMRB <ref> [5] </ref>. Note that NAUX for this routine is the number of integers that AUX can hold.
Reference: [6] <author> Anshul Gupta, Mahesh Joshi, and Vipin Kumar. Wssmp: </author> <title> A high-performance serial and parallel symmetric sparse linear solver. </title> <editor> In Bo Kagstrom, Jack J. Dongarra, Eric Elmroth, and Jerzy Wasniewski, editors, </editor> <booktitle> PARA'98 Workshop on Applied Parallel Computing in Large Scale Scientific and Industrial Problems, Umea, </booktitle> <address> Sweden, </address> <month> June </month> <year> 1998, </year> <booktitle> Proceedings Lecture Notes in Computer Science, </booktitle> <volume> No. 1541. </volume> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm <ref> [7, 6, 3] </ref>. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP [4, 5] for computing fill-reducing orderings.
Reference: [7] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(5) </volume> <pages> 502-520, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm <ref> [7, 6, 3] </ref>. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP [4, 5] for computing fill-reducing orderings.
Reference: [8] <author> Mahesh Joshi, Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Two dimensional scalable parallel algorithms for solution of triangular systems. </title> <type> Technical Report TR 97-024, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> July </month> <year> 1997. </year> <booktitle> A short version appears in Proceedings of the 1997 International Conference on High Performance Computing (HiPC'97), </booktitle> <address> Bangalore, India. </address> <month> 40 </month>
Reference-contexts: WSSMP uses a modified version of the multifrontal algorithm [10] for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm [7, 6, 3]. The package also uses scalable parallel sparse triangular solvers <ref> [8] </ref> and an improved and parallelized version of the previously released package WGPP [4, 5] for computing fill-reducing orderings.
Reference: [9] <institution> Optimization Subroutine Library. Guide and Reference. IBM Corporation, </institution> <address> Re--lease 2.1, Fifth Edition, </address> <month> February </month> <year> 1995. </year> <note> Publication number SC23-0519-04. See http://www.research.ibm.com/osl/ for more details. </note>
Reference-contexts: The default communicator is MPI COMM WORLD [12]; however, a user can specify any other communicator (see Section 8.3 for more details) and subgroups of processes can be work on different systems. 3. The WSSMP library can no longer be linked with the Optimization Subroutine Library (OSL) <ref> [9] </ref>. WSSMP is now included in the latest version of OSL. 4. The roles of 0 and 1 as input values of IPARM (29) have been reversed. See Section 4.2.14 for details. 5.
Reference: [10] <author> Joseph W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice. SIAM Review, </journal> <volume> 34 </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: It can can be used as a serial package, or in a shared-memory multiprocessor environment, or as a scalable parallel solver in a message-passing environment, where each node can either be a uniprocessor or a shared-memory multiprocessor. WSSMP uses a modified version of the multifrontal algorithm <ref> [10] </ref> for sparse Cholesky factorization and a highly scalable parallel sparse Cholesky factorization algorithm [7, 6, 3]. The package also uses scalable parallel sparse triangular solvers [8] and an improved and parallelized version of the previously released package WGPP [4, 5] for computing fill-reducing orderings. <p> Note 4.1 Recall that WSSMP supports both C-style (starting from 0) and FORTRAN-style (starting from 1) numbering. The description in this section assumes FORTRAN-style numbering and C users must interpreted it accordingly. For example, IPARM (11) will actually be IPARM <ref> [10] </ref> in a C program calling WSSMP. Note 4.2 The original code for WSSMP is in FORTRAN and expects the parameters to be passed by reference. Therefore, when calling WSSMP from a C program, the addresses of the parameters described in Section 4.2 must be passed.
Reference: [11] <author> Bradford Nichols, Dick Buttlar, and Jacqueline Proulx Farrell. </author> <title> Pthreads Programming. </title> <publisher> O'Reilly and Associates, Inc., </publisher> <address> Sebastopol, California, </address> <year> 1996. </year>
Reference-contexts: When WSSMP runs in a parallel environment on an SP computer, nodes communicate with other nodes via message-passing using Message Passing Interface (MPI) [12] over the SP high-speed switch. The parallelism within multiprocessor nodes is exploited through the POSIX threads library <ref> [11] </ref>. In fact, it is possible to run multiple processes on a single node. The processes communicated with each other via MPI and each process may itself be parallel and may spawn multiple threads if running on a multiprocessor node.
Reference: [12] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Dongarra. </author> <title> MPI: The complete reference. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1996. </year> <month> 41 </month>
Reference-contexts: A node may consist of one or more processors or CPUs. When WSSMP runs in a parallel environment on an SP computer, nodes communicate with other nodes via message-passing using Message Passing Interface (MPI) <ref> [12] </ref> over the SP high-speed switch. The parallelism within multiprocessor nodes is exploited through the POSIX threads library [11]. In fact, it is possible to run multiple processes on a single node. <p> This is true for both WSSMP and PWSSMP libraries and exploiting SMP parallelism requires no intervention from the user. 2. The PWSSMP library now uses MPI for inter-process communication. The default communicator is MPI COMM WORLD <ref> [12] </ref>; however, a user can specify any other communicator (see Section 8.3 for more details) and subgroups of processes can be work on different systems. 3. The WSSMP library can no longer be linked with the Optimization Subroutine Library (OSL) [9].
References-found: 12

