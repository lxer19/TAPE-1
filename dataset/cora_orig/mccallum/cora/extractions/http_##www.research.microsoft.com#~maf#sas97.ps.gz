URL: http://www.research.microsoft.com/~maf/sas97.ps.gz
Refering-URL: http://www.research.microsoft.com/~maf/publications.html
Root-URL: http://www.research.microsoft.com
Title: Program Analysis Using Mixed Term and Set Constraints  
Author: Manuel Fahndrich and Alexander Aiken 
Address: Berkeley  
Affiliation: EECS Department University of California,  
Abstract: There is a tension in program analysis between precision and efficiency. In constraint-based program analysis, at one extreme methods based on unification of equality constraints over terms are very fast but often imprecise. At the other extreme, methods based on the resolution of inclusion constraints over set expressions are quite precise, but are often inefficient in practice. We describe a parameterized framework for constraint-based program analyses that allows the analysis designer to embed terms and set expressions within each other. Constraints over these mixed expressions are partially between equality and inclusion, which enables an entire spectrum of program analyses with varying degrees of precision and efficiency to be expressed. We also show that there are interesting analyses that take advantage of this mixture. In particular, we report on the design and implementation of an uncaught exception analysis for core ML. Our results show that the analysis approaches the efficiency of algorithm W. 
Abstract-found: 1
Intro-found: 1
Reference: [AKVW93] <author> A. Aiken, D. Kozen, M. Vardi, and E. Wimmers. </author> <title> The complexity of set constraints. </title> <editor> In E. Borger, Y. Gurevich, and K. Meinke, editors, </editor> <booktitle> Computer Science Logic '93, volume 832 of Lect. Notes in Comput. Sci., </booktitle> <pages> pages 1-17. </pages> <institution> Eur. Assoc. Comput. Sci. </institution> <address> Logic, </address> <publisher> Springer, </publisher> <month> September </month> <year> 1993. </year>
Reference-contexts: The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints. The best known algorithm for solving the simplest inclusion constraints is dynamic transitive closure which requires cubic time, and for more expressive constraints solvability becomes at least EXPTIME-hard <ref> [AKVW93, MH97] </ref>. To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions [Pot96, FA96, TS96, FF97].
Reference: [And94] <author> L. O. Andersen. </author> <title> Program Analysis and Specialization for the C Programming Language. </title> <type> PhD thesis, </type> <institution> DIKU, University of Copenhagen, </institution> <month> May </month> <year> 1994. </year> <type> DIKU report 94/19. </type>
Reference-contexts: His algorithms runs in nearly linear time. They can unfortunately not be directly expressed in our framework. In [SH97] the authors study points-to analysis w.r.t. the precision-efficiency tradeoff. They contrast an algorithm based on inclusion constraints <ref> [And94] </ref> with the equality based algorithm of [Ste96], and then describe a spectrum of algorithms in between. We are currently using the same analyses to tune and validate our framework. 7 Conclusion We described a parameterized constraint formalism that combines inclusion constraints over terms and sets.
Reference: [AW92] <author> A. Aiken and E. Wimmers. </author> <title> Solving Systems of Set Constraints. </title> <booktitle> In Symposium on Logic in Computer Science, </booktitle> <pages> pages 329-340, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The disadvantage of equality constraints is that they cannot model the direction of value flow within a program. Information always flows in all directions, causing a loss of precision. Another class of program analyses is based on set-inclusion constraints <ref> [Hei92, AW92, AW93] </ref>. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are [Shi88, Hei94, AWL94, FFK + 96, MW97].
Reference: [AW93] <author> A. Aiken and E. Wimmers. </author> <title> Type Inclusion Constraints and Type Inference. </title> <booktitle> In Proceedings of the 1993 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 31-41, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The disadvantage of equality constraints is that they cannot model the direction of value flow within a program. Information always flows in all directions, causing a loss of precision. Another class of program analyses is based on set-inclusion constraints <ref> [Hei92, AW92, AW93] </ref>. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are [Shi88, Hei94, AWL94, FFK + 96, MW97]. <p> Constraints in our formalism are between type expressions, and solutions of the constraints are types. To a first approximation, types can be thought of as sets of values. We refer to Hindley-Milner style types as term types and to types based on set expressions <ref> [AW93] </ref> as set types. The key property of term types is that they have unique head constructors. Our formalism covers an entire spectrum of program analyses with varying degrees of precision and efficiency. <p> Inclusion constraints between these types are solved using unification and rewrite techniques (Section 4). At the Hindley-Milner end of the spectrum, the implementation of the constraint solving process essentially yields algorithm W [Mil78]. (Note that the system described in <ref> [AW93] </ref> can also express Hindley-Milner type inference using only inclusion constraints, but the inference algorithm still requires cubic time in this case.) To make our ideas concrete, we instantiate our framework to a particular analysis near the Hindley-Milner end of the spectrum, namely uncaught exception inference for a subset of ML <p> All types are u-types in this case and the set of type constructors includes, for example, nullary constructors such as int and unary constructors such as list : u ! u. Figure 2 shows the signatures of type constructors corresponding to the set types of <ref> [AW93] </ref>; all types are s-types. Finally, Figure 3 contains the signatures for the type language given in Section 2.2 for our ML exception inference. <p> Signature for Hindley-Milner types. c : s 1 : : : s n ! s (for all n-ary data constructors) ! : s s ! s (Function type) Fig. 2. Signature for set types <ref> [AW93] </ref>. c : u (for all c 2 B basetypes) c : s (for all constant exception constructors c) c : u ! s (for all exception constructors c with arguments) exn : s ! u ! : u s u ! u (Function type) Fig. 3. <p> Similarly, a type X is upward-closed, iff 8t 2 X f?g; t t 0 =) t 0 2 X. We assume D is lifted (i.e. x:? 6= ?). A suitable domain D is described in <ref> [AW93] </ref> where the authors use it to give meaning to set types. We first review the semantics of set types and then describe the necessary generalizations for our framework. <p> Definition 2 (Solution). A pair ( V ; fi V u ) consisting of a type assignment V and a contour assignment fi V u is a solution to a system of constraints S if and only if 1 Different from <ref> [AW93] </ref>. - (T 1 ) (T 2 ) for every constraint T 1 T 2 2 S. ( = u or = s) - fi (T 1 ) = fi (T 2 ) for every constraint T 1 u T 2 2 S. (The contours of u-constrained types must be equal.) <p> By the same reasoning, one can show that the constraints ff u c (1) ^ ff u d (1) have no solution. 4 Computing Solutions This section describes how to compute the solutions of a system of u and s-constraints. Section 4.1 describes why adapting the resolution of <ref> [AW93] </ref> to our type language is difficult due to our parameterized interpretation of types and how this problem can be solved, and Section 4.2 describes the resolution rules. The following terminology is used in the next sections. <p> s s ! s, then in the constraint T 1 ! T 2 s T 3 ! T 4 , the sub-expressions T 1 and T 4 occur negatively and T 2 and T 3 occur positively. 4.1 Upward-closed Monotypes and Type Complement We first review the theory developed in <ref> [AW93] </ref> to solve inclusion constraints and then adapt it to our new formalism. The simple part is to extend the resolution rules to solve u-constraints (Section 4.2). Here, we deal with the more serious problem, namely adapting the resolution to our parameterized interpretations c of type constructors. <p> The type expression :T denotes (D (T )) [ f?g, which is the type complement of T . (To see this, note that T [ :T = 1 and T " :T = 0.) The algorithms for computing T and :T given in <ref> [AW93] </ref> are syntax-directed and depend crucially on the fixed interpretation of constructors described in Section 3.2. Since we parameterize the interpretation of constructors, we have no hope of giving an algorithm that computes upward-closed types and complement syntactically. <p> Instead of using unions in negative positions, constraints need to be written as intersections of Pat. Only unions of upward-closed monotypes remain, and these can only appear in second positions of Pat, where they never need to be decomposed. To summarize, we replace the two rules of <ref> [AW93] </ref> for simplifying intersections in positive positions and unions in negative positions with a single rule, along with an abbreviation Pat containing an implicit complement. The new resolution rule uses only sub-expressions of the original constraints, and does not require the formation of upward-closed and complement types during resolution. <p> Figure 4 shows the resolution rules. Due to the sorted algebra of types the resolution rules preserve sorts, i.e. given constraints between types of the same sort, the resolution rules only produce constraints between equal sorts. The rules should be read as left-to-right rewrite rules. Rules 1-7 are from <ref> [AW93] </ref>. Below the line are the new rules. Rules 8 and 9 are discussed above. Note the side condition on Rule 8, which, along with intersection simplification (Figure 5), avoids a cycle in the rewrite rules. Rules 10 and 11 deal with complement types. <p> applies. 3 The resulting system is either inconsistent (meaning it has a constraint listed in Figure 6 or fails an occurs check), in which case it has no solutions, or the the constraints are all on variables and the solutions can be characterized in the same way as described in <ref> [AW93] </ref> 4 We briefly discuss the soundness of the resolution rules. Rules 1, 2 and 6 are obviously sound. Rule 3 follows from the variance of constructors (Axiom 2), Rules 4 and 5 from standard set theory. Rules 7-9 follow from set theory, disjointness (Axiom 4), and Axiom 6. <p> Some remarks about extending the described exception inference to core SML are in order. - Let-polymorphism is handled as described in <ref> [AW93] </ref>. Exception declarations in SML produce new exceptions at every evaluation. Exception declarations within let expressions can therefore give rise to an unbounded number of distinct exceptions, all sharing the same name. Consequently, only exceptions declared at toplevel can safely be filtered by name in handle expressions.
Reference: [AWL94] <author> A. Aiken, E. Wimmers, </author> <title> and T.K. Lakshman. Soft typing with conditional types. </title> <booktitle> In Twenty-First Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 163-173, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Another class of program analyses is based on set-inclusion constraints [Hei92, AW92, AW93]. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are <ref> [Shi88, Hei94, AWL94, FFK + 96, MW97] </ref>. The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints. <p> infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference <ref> [AWL94, Pot96, FA96, TS96, MW97] </ref> has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach.
Reference: [CC77] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by contruction or approximation of fixed points. </title> <booktitle> In Fourth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: They do not treat exceptions as first class values, and they ignore value-carrying exceptions. In [Yi94], Yi describes a collecting interpretation for estimating uncaught exceptions in ML. His analysis is presented as an abstract interpretation <ref> [CC77] </ref> and is much finer grained than [GS94] or the system described here, but is also slow in practice. 3 Types and Domains Section 2.2 outlined a mixed type language for expressing ML types with refined exception information.
Reference: [FA96] <author> Manuel Fahndrich and Alex Aiken. </author> <title> Making set-constraint based program analyses scale. </title> <booktitle> In First Work--shop on Set Constraints at CP'96, </booktitle> <address> Cambridge, MA, </address> <month> August </month> <year> 1996. </year> <note> Available as Technical Report CSD-TR-96-917, </note> <institution> University of California at Berkeley. </institution>
Reference-contexts: To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions <ref> [Pot96, FA96, TS96, FF97] </ref>. While experimenting with a type-based program analysis system entirely based on inclusion constraints over the past two years, we have noticed that constraints describing the inferred types have many uninteresting solutions. <p> infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference <ref> [AWL94, Pot96, FA96, TS96, MW97] </ref> has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach.
Reference: [FF97] <author> Cormac Flanagan and Matthias Felleisen. </author> <title> Componential set-based analysis. </title> <booktitle> In PLDI'97 [PLD97]. </booktitle>
Reference-contexts: To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions <ref> [Pot96, FA96, TS96, FF97] </ref>. While experimenting with a type-based program analysis system entirely based on inclusion constraints over the past two years, we have noticed that constraints describing the inferred types have many uninteresting solutions. <p> The analysis infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis <ref> [Hei92, FF97] </ref> and inclusion constraint-based type inference [AWL94, Pot96, FA96, TS96, MW97] has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints.
Reference: [FFK + 96] <author> Cormac Flanagan, Matthew Flatt, Shriram Krishnamurthi, Stephanie Weirich, and Matthias Felleisen. </author> <title> Catching Bugs in the Web of Program Invariants. </title> <booktitle> In Proceedings of the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 23-32, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Another class of program analyses is based on set-inclusion constraints [Hei92, AW92, AW93]. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are <ref> [Shi88, Hei94, AWL94, FFK + 96, MW97] </ref>. The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints.
Reference: [FM88] <author> Y. Fuh and P. Mishra. </author> <title> Type inference with subtypes. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming, </booktitle> <pages> pages 94-114, </pages> <year> 1988. </year>
Reference-contexts: Their constraint language is less expressive than ours and they only compute a particular solution of the constraints. In type disciplines based on primitive subtyping <ref> [Mit84, FM88] </ref>, the base types form a partial order. This order induces a partial order on types by structural extension over function type constructors, tuples, etc. Subtype constraints can be solved structurally until only atomic constraints (between atoms) remain.
Reference: [GS94] <author> Juan Carlos Guzman and Ascander Suarez. </author> <title> An extended type system for exceptions. </title> <booktitle> In Proceedings of the ACM SIGPLAN Workshop on ML and its Applications, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: As a result, we know that evaluating catchFail can result in any exceptions raised by the argument function, except Fail (written * " :fFailg). Set expressions are crucial for describing such types. We are aware of two earlier approaches to uncaught exception detection for ML. In <ref> [GS94] </ref>, Guzman and Suarez describe an extended type system for ML similar to, but less powerful than, the one presented here. They do not treat exceptions as first class values, and they ignore value-carrying exceptions. In [Yi94], Yi describes a collecting interpretation for estimating uncaught exceptions in ML. <p> They do not treat exceptions as first class values, and they ignore value-carrying exceptions. In [Yi94], Yi describes a collecting interpretation for estimating uncaught exceptions in ML. His analysis is presented as an abstract interpretation [CC77] and is much finer grained than <ref> [GS94] </ref> or the system described here, but is also slow in practice. 3 Types and Domains Section 2.2 outlined a mixed type language for expressing ML types with refined exception information. This is a particular example of a class of analyses that is expressible in our framework.
Reference: [Hei92] <author> N. Heintze. </author> <title> Set Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: The disadvantage of equality constraints is that they cannot model the direction of value flow within a program. Information always flows in all directions, causing a loss of precision. Another class of program analyses is based on set-inclusion constraints <ref> [Hei92, AW92, AW93] </ref>. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are [Shi88, Hei94, AWL94, FFK + 96, MW97]. <p> The analysis infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis <ref> [Hei92, FF97] </ref> and inclusion constraint-based type inference [AWL94, Pot96, FA96, TS96, MW97] has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints.
Reference: [Hei94] <author> Nevin Heintze. </author> <title> Set Based Analysis of ML Programs. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 306-17, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Another class of program analyses is based on set-inclusion constraints [Hei92, AW92, AW93]. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are <ref> [Shi88, Hei94, AWL94, FFK + 96, MW97] </ref>. The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints.
Reference: [Hen91] <author> F. Henglein. </author> <title> Efficient Type Inference for Higher-Order Binding-Time Analysis. </title> <booktitle> In 5th ACM Conference Proceedings on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 448-72, </pages> <year> 1991. </year>
Reference-contexts: But their inference rules are based on equality constraints which they solve using a generalized unification procedure. Henglein's work on efficient binding time analysis <ref> [Hen91] </ref> and tag inference [Hen92] also combines sub-typing and equality constraints. His algorithms runs in nearly linear time. They can unfortunately not be directly expressed in our framework. In [SH97] the authors study points-to analysis w.r.t. the precision-efficiency tradeoff.
Reference: [Hen92] <author> F. Henglein. </author> <title> Global tagging optimization by type inference. </title> <booktitle> In Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 205-215, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The Hindley-Milner polymorphic type inference system [Mil78] is the classical example of a constraint-based program analysis. It uses equality constraints over a term algebra to infer types for functional programming languages such as ML [MTH90]. This system has inspired many other analyses based on equality constraints (e.g. <ref> [Hen92, Ste96] </ref>). Such systems are appealing because they yield concise results and because the equality constraints can be solved using unification in nearly linear time (in the monomorphic case). The disadvantage of equality constraints is that they cannot model the direction of value flow within a program. <p> But their inference rules are based on equality constraints which they solve using a generalized unification procedure. Henglein's work on efficient binding time analysis [Hen91] and tag inference <ref> [Hen92] </ref> also combines sub-typing and equality constraints. His algorithms runs in nearly linear time. They can unfortunately not be directly expressed in our framework. In [SH97] the authors study points-to analysis w.r.t. the precision-efficiency tradeoff.
Reference: [HM97] <author> Nevin Heintze and David McAllester. </author> <title> Linear-time subtransitive control flow analysis. </title> <booktitle> In PLDI'97 [PLD97]. </booktitle>
Reference-contexts: Rules 10 and 11 deal with complement types. Rules 12 and 13 flip the inclusion for constraints arising from contravariant fields. Rules 14 and 15 instantiate u-variables to satisfy contour equalities (similar to the approach of <ref> [HM97, Mos96] </ref>); these rules introduce fresh variables. We must ensure that the resolution process terminates. The simple constraint ff u c (ff) produces the sequence of constraints ff = c (ff 1 ), ff 1 u ff, ff 1 u c (ff 1 ), etc. ad infinitum. <p> In practice, the complexity of our approach depends on the application. As long as the inferred types are relatively small (as e.g. in the case of ML <ref> [HM97] </ref>), the practical complexity appears to be close to linear. 5 Exception Inference for ML We now instantiate the developed framework to our motivating example from Section 2.2. The signatures of type constructors appear in Figure 3 and have already been described in Section 3.1.
Reference: [ICF97] <institution> Proceedings of the International Conference on Functional Programming (ICFP '97), </institution> <month> June </month> <year> 1997. </year>
Reference: [JG91] <author> Pierre Jouvelot and David K. Gifford. </author> <title> Algebraic reconstruction of types and effects. </title> <booktitle> In Proceedings of the 18th Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 303-310, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: However, they do not show how to solve such constraints and, in fact, in a later paper drop the subset constraints for equality constraints which they solve with generalized unification <ref> [JG91] </ref>. Similarly, Tofte and Talpin [TT94] use a mixture of types and sets in an effect system to infer allocation and deallocation points of memory regions at compile-time. But their inference rules are based on equality constraints which they solve using a generalized unification procedure.
Reference: [LG88] <author> John M. Lucassen and David K. Gifford. </author> <title> Polymorphic effect systems. </title> <booktitle> In Proceedings of the 15th Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 47-57, </pages> <year> 1988. </year>
Reference-contexts: The signatures of type constructors appear in Figure 3 and have already been described in Section 3.1. We cast the type and exception inference for ML as an effect inference system <ref> [LG88] </ref>. In this model, every expression has a type and an effect. The type of an expression describes the set of possible unexceptional values of the expression, whereas the effect describes the set of exceptions that may be raised during evaluation. <p> Our approach differs however in that the constraints between s-types may induce new constraints between u-types, whereas atomic subtyping constraints can never induce new structural constraints. Effect systems [Luc87] naturally contain a mixture of Hindley-Milner types and sets for effects. In <ref> [LG88] </ref> Lucassen and Gifford describe type and effect inference rules using a subset relation on types induced by the subset relation of effect sets contained in the types.
Reference: [Luc87] <author> John M. Lucassen. </author> <title> Types and Effects |Towards the Integration of Functional and Imperative Programming. </title> <type> Ph.D. thesis, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Our approach differs however in that the constraints between s-types may induce new constraints between u-types, whereas atomic subtyping constraints can never induce new structural constraints. Effect systems <ref> [Luc87] </ref> naturally contain a mixture of Hindley-Milner types and sets for effects. In [LG88] Lucassen and Gifford describe type and effect inference rules using a subset relation on types induced by the subset relation of effect sets contained in the types.
Reference: [MH97] <author> David McAllester and Nevin Heintze. </author> <title> On the complexity of set-based analysis. </title> <booktitle> In ICFP'97 [ICF97], </booktitle> <pages> pages 150-63. </pages>
Reference-contexts: The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints. The best known algorithm for solving the simplest inclusion constraints is dynamic transitive closure which requires cubic time, and for more expressive constraints solvability becomes at least EXPTIME-hard <ref> [AKVW93, MH97] </ref>. To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions [Pot96, FA96, TS96, FF97].
Reference: [Mil78] <author> R. Milner. </author> <title> A theory of type polymorphism in programming. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 17 </volume> <pages> 348-375, </pages> <year> 1978. </year>
Reference-contexts: 1 Introduction The Hindley-Milner polymorphic type inference system <ref> [Mil78] </ref> is the classical example of a constraint-based program analysis. It uses equality constraints over a term algebra to infer types for functional programming languages such as ML [MTH90]. This system has inspired many other analyses based on equality constraints (e.g. [Hen92, Ste96]). <p> The formalism is based on a 2-sorted algebra of type expressions (Section 3). Inclusion constraints between these types are solved using unification and rewrite techniques (Section 4). At the Hindley-Milner end of the spectrum, the implementation of the constraint solving process essentially yields algorithm W <ref> [Mil78] </ref>. (Note that the system described in [AW93] can also express Hindley-Milner type inference using only inclusion constraints, but the inference algorithm still requires cubic time in this case.) To make our ideas concrete, we instantiate our framework to a particular analysis near the Hindley-Milner end of the spectrum, namely uncaught
Reference: [Mit84] <author> J. Mitchell. </author> <title> Coercion and type inference (summary). </title> <booktitle> In Eleventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 175-185, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Their constraint language is less expressive than ours and they only compute a particular solution of the constraints. In type disciplines based on primitive subtyping <ref> [Mit84, FM88] </ref>, the base types form a partial order. This order induces a partial order on types by structural extension over function type constructors, tuples, etc. Subtype constraints can be solved structurally until only atomic constraints (between atoms) remain.
Reference: [MNP97] <author> Martin Muller, Joachim Niehren, and Andreas Podelski. </author> <title> Inclusion constraints over non-empty sets of trees. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on the Theory and Practice of Software Development (TAPSOFT'97), </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach. In <ref> [MNP97] </ref> the authors describe In es, a system for solving inclusion constraints over non-empty sets of trees. They give an algorithm for computing the largest solution of the constraints and show that equal ity constraints between set expressions can be solved using unification.
Reference: [Mos96] <author> Christian Mossin. </author> <title> Flow Analysis of Typed Higher-Order Programs. </title> <type> PhD thesis, </type> <institution> DIKU, Department of Computer Science, University of Copenhagen, </institution> <year> 1996. </year>
Reference-contexts: Rules 10 and 11 deal with complement types. Rules 12 and 13 flip the inclusion for constraints arising from contravariant fields. Rules 14 and 15 instantiate u-variables to satisfy contour equalities (similar to the approach of <ref> [HM97, Mos96] </ref>); these rules introduce fresh variables. We must ensure that the resolution process terminates. The simple constraint ff u c (ff) produces the sequence of constraints ff = c (ff 1 ), ff 1 u ff, ff 1 u c (ff 1 ), etc. ad infinitum.
Reference: [MPS84] <author> D. MacQueen, G. Plotkin, and R. Sethi. </author> <title> An ideal model for recursive polymophic types. </title> <booktitle> In Eleventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 165-174, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: To keep notation to a minimum we omit strictness annotations from constructor signatures. Where necessary, constructor strictness will be mentioned explicitly. 3.2 Semantics of Types We give semantics to types using a variation on the standard ideal model <ref> [MPS84] </ref>. The semantic domain D contains a least element ? and is equipped with a complete partial order , where ? t for all t 2 D. Types are downward-closed subsets of D. <p> In pure set theory, the constraint T 1 T 2 [ T 3 is equivalent to T 1 " :T 2 T 3 . However, we interpret set expressions as types (downward-closed sets of values <ref> [MPS84] </ref>) and the complement of a type is not necessarily a downward-closed set, and thus not a type. For example, the complement of the function type 1 ! 0 contains every function except the least function x:?. Only the complements of upward-closed types are themselves types.
Reference: [MTH90] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction The Hindley-Milner polymorphic type inference system [Mil78] is the classical example of a constraint-based program analysis. It uses equality constraints over a term algebra to infer types for functional programming languages such as ML <ref> [MTH90] </ref>. This system has inspired many other analyses based on equality constraints (e.g. [Hen92, Ste96]). Such systems are appealing because they yield concise results and because the equality constraints can be solved using unification in nearly linear time (in the monomorphic case).
Reference: [MW97] <author> Simon Marlow and Philip Wadler. </author> <title> A practical subtyping system for Erlang. </title> <booktitle> In ICFP'97 [ICF97]. </booktitle>
Reference-contexts: Another class of program analyses is based on set-inclusion constraints [Hei92, AW92, AW93]. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are <ref> [Shi88, Hei94, AWL94, FFK + 96, MW97] </ref>. The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints. <p> infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference <ref> [AWL94, Pot96, FA96, TS96, MW97] </ref> has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach.
Reference: [PLD97] <institution> Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </institution> <month> June </month> <year> 1997. </year>
Reference: [Pot96] <author> Fran~cois Pottier. </author> <title> Simplifying subtyping constraints. </title> <booktitle> In Proceedings of the 1996 ACM SIGPLAN International Conference on Functional Programming (ICFP '96), </booktitle> <pages> pages 122-133, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions <ref> [Pot96, FA96, TS96, FF97] </ref>. While experimenting with a type-based program analysis system entirely based on inclusion constraints over the past two years, we have noticed that constraints describing the inferred types have many uninteresting solutions. <p> infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference <ref> [AWL94, Pot96, FA96, TS96, MW97] </ref> has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach.
Reference: [SH97] <author> Marc Shapiro and Susan Horwitz. </author> <title> Fast and accurate flow-insensitive points-to analysis. </title> <booktitle> In Proceedings of the 24th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-14, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Henglein's work on efficient binding time analysis [Hen91] and tag inference [Hen92] also combines sub-typing and equality constraints. His algorithms runs in nearly linear time. They can unfortunately not be directly expressed in our framework. In <ref> [SH97] </ref> the authors study points-to analysis w.r.t. the precision-efficiency tradeoff. They contrast an algorithm based on inclusion constraints [And94] with the equality based algorithm of [Ste96], and then describe a spectrum of algorithms in between.
Reference: [Shi88] <author> O. Shivers. </author> <title> Control flow analysis in Scheme. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 164-174, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Another class of program analyses is based on set-inclusion constraints [Hei92, AW92, AW93]. Because inclusion constraints can model the direction of value flow within a program quite accurately, inclusion constraints yield more precise results than equality-based systems. Examples of such analyses are <ref> [Shi88, Hei94, AWL94, FFK + 96, MW97] </ref>. The disadvantage of inclusion constraints is that they are more expensive to solve than equality constraints.
Reference: [Ste96] <author> Bjarne Steensgaard. </author> <title> Points-to analysis in almost linear time. </title> <booktitle> In Proceedings of the 23rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 32-41, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The Hindley-Milner polymorphic type inference system [Mil78] is the classical example of a constraint-based program analysis. It uses equality constraints over a term algebra to infer types for functional programming languages such as ML [MTH90]. This system has inspired many other analyses based on equality constraints (e.g. <ref> [Hen92, Ste96] </ref>). Such systems are appealing because they yield concise results and because the equality constraints can be solved using unification in nearly linear time (in the monomorphic case). The disadvantage of equality constraints is that they cannot model the direction of value flow within a program. <p> His algorithms runs in nearly linear time. They can unfortunately not be directly expressed in our framework. In [SH97] the authors study points-to analysis w.r.t. the precision-efficiency tradeoff. They contrast an algorithm based on inclusion constraints [And94] with the equality based algorithm of <ref> [Ste96] </ref>, and then describe a spectrum of algorithms in between. We are currently using the same analyses to tune and validate our framework. 7 Conclusion We described a parameterized constraint formalism that combines inclusion constraints over terms and sets.
Reference: [TS96] <author> Valery Trifonov and Scott Smith. </author> <title> Subtyping Constrained Types. </title> <booktitle> In Proceedings of the 3rd International Static Analysis Symposium, </booktitle> <pages> pages 349-365, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: To make inclusion constraint-based analyses practical, a lot of effort is spent on tuning the representation and manipulation of constraints. Most importantly, inclusion constraints must be simplified to obtain more concise representations of solutions <ref> [Pot96, FA96, TS96, FF97] </ref>. While experimenting with a type-based program analysis system entirely based on inclusion constraints over the past two years, we have noticed that constraints describing the inferred types have many uninteresting solutions. <p> infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi [Yi94]. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference <ref> [AWL94, Pot96, FA96, TS96, MW97] </ref> has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints. We deem constraint simplification still necessary on the regular inclusion constraints that arise in our approach.
Reference: [TT94] <author> M. Tofte and J.-P. Talpin. </author> <title> Implementation of the typed call-by-value -calculus using a stack of regions. </title> <booktitle> In Twenty-First Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 188-201, </pages> <year> 1994. </year>
Reference-contexts: However, they do not show how to solve such constraints and, in fact, in a later paper drop the subset constraints for equality constraints which they solve with generalized unification [JG91]. Similarly, Tofte and Talpin <ref> [TT94] </ref> use a mixture of types and sets in an effect system to infer allocation and deallocation points of memory regions at compile-time. But their inference rules are based on equality constraints which they solve using a generalized unification procedure.
Reference: [Yi94] <author> Kwangkeun Yi. </author> <title> Compile-time detection of uncaught exceptions for Standard ML programs. </title> <booktitle> In Proceedings of the 1st International Static Analysis Symposium, volume 864 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: In [GS94], Guzman and Suarez describe an extended type system for ML similar to, but less powerful than, the one presented here. They do not treat exceptions as first class values, and they ignore value-carrying exceptions. In <ref> [Yi94] </ref>, Yi describes a collecting interpretation for estimating uncaught exceptions in ML. <p> The analysis infers the following type for the main function lexGen: lexGen : string -(Match "/ eof "/ error "/ lex_error "/ Subscript)-&gt; unit The five uncaught exceptions correspond exactly to the results reported by Yi <ref> [Yi94] </ref>. 6 Related Work Work on set-based program analysis [Hei92, FF97] and inclusion constraint-based type inference [AWL94, Pot96, FA96, TS96, MW97] has mostly focused on how to simplify constraints to achieve scalability. The developed techniques and heuristics are orthogonal to our approach of restricting the interesting solutions of constraints.
References-found: 36

