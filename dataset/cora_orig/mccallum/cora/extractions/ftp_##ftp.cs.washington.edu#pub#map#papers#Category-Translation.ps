URL: ftp://ftp.cs.washington.edu/pub/map/papers/Category-Translation.ps
Refering-URL: http://www.cs.washington.edu/research/projects/softbots/www/papers.html
Root-URL: 
Email: fmap, etzionig@cs.washington.edu  
Phone: (206) 616-1845 Fax: (206) 543-2969  
Title: Category Translation: Learning to understand information on the Internet  
Author: Mike Perkowitz Oren Etzioni 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington,  
Abstract: This paper investigates the problem of automatically learning declarative models of information sources available on the Internet. We report on ILA, a domain-independent program that learns the meaning of external information by explaining it in terms of internal categories. In our experiments, ILA starts with knowledge of local faculty members, and is able to learn models of the Internet service whois and of the personnel directories available at Berkeley, Brown, Caltech, Cornell, Rice, Rut-gers, and UCI, averaging fewer than 40 queries per information source. ILA's hypothesis language is compositions of first-order predicates, and its bias is compactly encoded as a determination. We analyze ILA's sample complexity both within the Valiant model, and using a probabilistic model specifically tailored to ILA.
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin and Laird, 1988 ] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: We can model the violation of these assumptions as random classification noise and use the bound due to <ref> [ Angluin and Laird, 1988 ] </ref> : n 2 " 2 (12 b ) 2 ln ( ffi ), where b is an upper bound on the frequency of noisy classifications, and the learner chooses the hypothesis that is correct at varying confidence levels. (b) Number of queries required to be
Reference: [ Angluin, 1988 ] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: However, learnability results in function learning theory are specific to classes of functions learned, such as polynomial, or real-valued [ Auer et al., 1995 ] . Similarly, although specific concept classes have been shown to be learnable under the membership oracle <ref> [ Angluin, 1988 ] </ref> , we are not aware of any sample complexity results that apply directly to ILA. For this reason, we chose to make a number of strong simplifying assumptions and analyze ILA under the PAC model.
Reference: [ Auer et al., 1995 ] <author> Peter Auer, Philip M. Long, Wolfgang Maass, and Gerhard J. Woeginger. </author> <title> On the complexity of function learning. </title> <journal> Machine Learning, </journal> 18(2/3):187-230, February/March 1995. 
Reference-contexts: Furthermore, the oracle used by ILA is a generalized membership oracle. However, learnability results in function learning theory are specific to classes of functions learned, such as polynomial, or real-valued <ref> [ Auer et al., 1995 ] </ref> . Similarly, although specific concept classes have been shown to be learnable under the membership oracle [ Angluin, 1988 ] , we are not aware of any sample complexity results that apply directly to ILA.
Reference: [ Etzioni and Perkowitz, 1995 ] <author> O. Etzioni and M. Perkowitz. </author> <title> A probabilistic model of sample complexity. </title> <note> 1995. in preparation. </note>
Reference-contexts: Also, under the worst-case assumption that n 1 hypotheses all have accuracy A b , a similar formula can be derived for a hypothesis space of size n <ref> [ Etzioni and Perkowitz, 1995 ] </ref> . As fl shrinks, ILA requires more queries to achieve high confidence. Note though that when fl is very small, discriminating between g and b is less important. Furthermore, discriminating queries enable ILA to converge on the high-accuracy hypothesis quickly even for small fl. <p> The accuracy of g is fixed at 0.95. The random-queries curve is derived from Equation 1. Due to lack of space, we omit the derivation and assumptions underlying the discriminating-queries curve, but see <ref> [ Etzioni and Perkowitz, 1995 ] </ref> . When A g = 0:95 and fl = 0:50, ILA requires only a single discriminating query to have 90% confidence that it has found the better hypothesis.
Reference: [ Etzioni and Weld, 1994 ] <author> O. Etzioni and D. Weld. </author> <title> A softbot-based interface to the internet. </title> <journal> CACM, </journal> <volume> 37(7) </volume> <pages> 72-76, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: A number of more sophisticated AI systems have emerged, including SIMS [ Knoblock et al., 1994 ] , the Information Manifold at AT&T [ Kirk et al., 1995 ] , and the Internet softbot <ref> [ Etzioni and Weld, 1994 ] </ref> . However, each of these AI systems requires sophisticated models of the different information sources it is able to access.
Reference: [ Haussler, 1988 ] <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: Haussler <ref> [ Haussler, 1988 ] </ref> derives a lower bound on the number of examples necessary for PAC learning. If h is any hypothesis that agrees with at least n queries from I, where n 1 jHj ffi ), then we have the following: P (E (h; I) ") 1 ffi.
Reference: [ Kirk et al., 1995 ] <author> Thomas Kirk, Alon Y. Levy, Yehoshua Sa-giv, and Divesh Srivastava. </author> <title> The information manifold. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <pages> pages 85-91, </pages> <address> Stanford University, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: However, these tools are unable to interpret the results of their searches and are unable to use multiple information sources in concert. A number of more sophisticated AI systems have emerged, including SIMS [ Knoblock et al., 1994 ] , the Information Manifold at AT&T <ref> [ Kirk et al., 1995 ] </ref> , and the Internet softbot [ Etzioni and Weld, 1994 ] . However, each of these AI systems requires sophisticated models of the different information sources it is able to access.
Reference: [ Knoblock et al., 1994 ] <author> Craig Knoblock, Yigal Arens, and Chun-Nan Hsu. </author> <title> Cooperating agents for information retrieval. </title> <booktitle> In Proceedings of the Second International Conference on Cooperative Information Systems, </booktitle> <address> Toronto, Canada, </address> <year> 1994. </year>
Reference-contexts: However, these tools are unable to interpret the results of their searches and are unable to use multiple information sources in concert. A number of more sophisticated AI systems have emerged, including SIMS <ref> [ Knoblock et al., 1994 ] </ref> , the Information Manifold at AT&T [ Kirk et al., 1995 ] , and the Internet softbot [ Etzioni and Weld, 1994 ] . However, each of these AI systems requires sophisticated models of the different information sources it is able to access.
Reference: [ Rajamoney, 1993 ] <author> S. Rajamoney. </author> <title> The design of discrimination experiments. </title> <journal> Machine Learning, </journal> <volume> 12(1/2/3), </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: In addition, ILA employs several heuristics to reduce the number of queries necessary to converge to a satisfactory model of the IS. Most important, ILA attempts to discriminate between two competing hypotheses by choosing an object for which the hypotheses make different predictions (cf. <ref> [ Rajamoney, 1993 ] </ref> ). For example, if ILA has seen the record Oren Etzioni 685-3035 FR-35, it will consider both lastname and userid as Given: 1.
Reference: [ Richards and Mooney, 1992 ] <author> B. L. Richards and R. J. Mooney. </author> <title> Learning relations by pathfinding. </title> <booktitle> In Proc. 10th Nat. Conf. on A.I., </booktitle> <pages> pages 50-55, </pages> <year> 1992. </year>
Reference-contexts: For example, in ILA's model, people are associated with departments and departments associated with mail-stops. The relation between a person and her mail-stop, then, is a composition of department and mail-stop | the mail-stop of P is mail-stop (department (P )). We employ a variant of relational pathfinding <ref> [ Richards and Mooney, 1992 ] </ref> to discover a relation between the query object and each response token. Richards and Mooney's pathfinding technique performs a bidirectional breadth-first search in which constants are nodes in the graph and attributes on constants are edges between nodes.
Reference: [ Wiederhold, 1992 ] <author> G Wiederhold. </author> <booktitle> Mediators in the architecture of future information systems. IEEE Computer, </booktitle> <pages> pages 38-49, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We have identified several problems that ILA does not yet address. Category mismatch occurs when ILA fails to find categories corresponding to those of the external information source <ref> [ Wiederhold, 1992 ] </ref> . For example, the IS records fax numbers, of which ILA is ignorant. Token mismatch occurs when, despite having appropriate categories, ILA fails to find matching tokens due to a difference in representation.
Reference: [ Wittgenstein, 1958 ] <author> Ludwig Wittgenstein. </author> <title> Philosophical Investigations. </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <year> 1958. </year> <title> Translated by G.E.M. </title> <publisher> Anscombe. </publisher>
Reference-contexts: As a first step, this paper investigates the question of learning semantics. Our learning method is based on the following idea, due to St. Augustine <ref> [ Wittgenstein, 1958 ] </ref> . Consider how you might learn the Latin term uxor by example. Suppose I tell you "George Washington's uxor was Martha." You might reason that, because "Martha" was the name of Washington's wife, perhaps "uxor" means "wife".
References-found: 12

