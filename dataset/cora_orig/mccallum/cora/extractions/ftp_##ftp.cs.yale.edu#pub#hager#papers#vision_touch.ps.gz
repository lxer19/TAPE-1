URL: ftp://ftp.cs.yale.edu/pub/hager/papers/vision_touch.ps.gz
Refering-URL: http://www.cs.yale.edu/users/hager/papers.html
Root-URL: http://www.cs.yale.edu
Abstract: Preliminary Results on Grasping with Vision and Touch Abstract This paper presents initial results in integrating touch with vision for delicate manipulation tasks. A generalizable framework of behavioral primitives for tactile and visual feedback control is proposed. Since vision provides position and shape information at a distance, while tactile pr ovides smallscale geometric and force information, we focus on the complimentary roles of vision and touch. We demonstrate that visual feedback can perform the rough positioning needed for tactile sensor feedback, and that grasp for ce and object orientation angles can be sensed and contr olled with tactile sensing. A force sensor based approach provided a comparison measure, and we observed that the use of tactile sensing results in a much more gentle grasp. 
Abstract-found: 1
Intro-found: 1
Reference: <author> P. K. Allen, </author> <title> Robotic object r ecognition using vision and touch. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address> <year> 1987. </year>
Reference-contexts: Most previous work combining vision and touch has concerned perceptual issues. One focus has been the use of touch sensing to supplement visual information for object recognition or exploration <ref> (Allen 1987, Stansfield 1988, Boshra and Zhang 1995) </ref>. Other work has developed representations for the integration of visual and tactile spatial information (Rafla and Merat 1990).
Reference: <author> M. Boshra and H. Zhang, </author> <title> A constraintsatisfaction approach for 3D vision/touch-based object recognition. </title> <booktitle> IEEE/RSJ International Conference on Intelligent Robots and Systems. </booktitle> <volume> vol. 3, </volume> <pages> pp. 368-73. </pages> <year> 1995. </year>
Reference: <author> D. Brock. </author> <title> A Task-level Control System for Automatic Robotic Grasping. MIT AI Lab Number 1469. </title> <month> January </month> <year> 1993. </year>
Reference-contexts: Tactile sensing is uniquely suited to this type of event detection, and a number of workers have used tactile sensing to increase the robustness of task programming and execution <ref> (e.g., Cutkosky and Hyde 1993, Brock 1993) </ref>. In the following, we describe the simple control and event detection primitives used in the grasping task, which are representative of the range of tactile-based behaviors that can be constructed. in some situations may provide better spatial resolution than vision.
Reference: <author> R. R. Burridge, A. A. Rizzi, and D. E. Koditschek. </author> <title> Toward A Systems Theory for the Composition of Dynamically Dexterous Robot Behaviors. </title> <booktitle> Proceedings of the Seventh International Symposium on Robotics Research. </booktitle> <year> 1995. </year>
Reference: <author> N. Chen, H. Zhang, and R. Rink. </author> <title> Edge Tracking Using Tactile Servo. </title> <booktitle> IEEE/RSJ International Confer ence on Intelligent Robots and Systems, IROS 95. </booktitle> <volume> vol. 2, </volume> <pages> pp. 84-89, </pages> <year> 1995. </year>
Reference: <author> M. R. Cutkosky and J. M. Hyde. </author> <title> Manipulation Control with Dynamic Tactile Sensing. </title> <booktitle> 6th International Symposium on Robotics Research. </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Tactile sensing is uniquely suited to this type of event detection, and a number of workers have used tactile sensing to increase the robustness of task programming and execution <ref> (e.g., Cutkosky and Hyde 1993, Brock 1993) </ref>. In the following, we describe the simple control and event detection primitives used in the grasping task, which are representative of the range of tactile-based behaviors that can be constructed. in some situations may provide better spatial resolution than vision.
Reference: <author> R. S. Fearing. </author> <title> Tactile sensing mechanisms. </title> <journal> International Journal of Robotics Research, </journal> <volume> 9(3):323, </volume> <month> June </month> <year> 1990a. </year>
Reference-contexts: the contact forces and torques are reduced considerably despite the fact that the sensitivity of the tactile array sensors used was low due to the extra long cables required for the experiment (the sensor gain is approximately a function of sensor capacitance divided by the line capacitance as explained in <ref> (Fearing 1990a) </ref>). The final orientation angle reached using tactile feedback was also observed to be much closer to the desired value than that achieved by servo float mode.
Reference: <author> R. S. Fearing. </author> <title> Tactile sensing for shape interpretation. </title> <editor> In S. </editor> <booktitle> T. </booktitle>
Reference: <editor> Venkataraman and T. Iberall, editors, </editor> <publisher> Dextrous Robot Hands . Springer-Verlag, </publisher> <pages> pp. 209-238, </pages> <year> 1990b. </year>
Reference: <author> G. D. Hager, W. Chang, and A. S. Morse. </author> <title> Robot Feedback Control Based on Stereo Vision: Towards Calibration-Free Hand-Eye Coordination. </title> <journal> IEEE Cont. Sys. Mag. </journal> <volume> 15(1) </volume> <pages> 30-39, </pages> <year> 1995. </year>
Reference-contexts: Sensing takes place at contact locations that are hidden from view by the hand, and which acts as a pure integrator. This approach neglects many potentially important dynamic ef fects, but for low-velocity tasks it has been experimentally validated as a reasonable model of the system <ref> (Hager et al. 1995) </ref>. This permits us to treat the control problem as one of generating velocity commands from sensor data. Here, we review the essential ideas behind the theory using simple point-to-point motion. <p> Let denote the projections of and and the image Jacobian for cameras . Define . (1) Let denote the generalized inverse of a matrix. It follows that applying feedback (2) to the robot will stabilize it so that <ref> (Hager et al. 1995) </ref>. In general, a manipulator is controlled by a velocity screw which controls six degrees of freedom of motion. Thus, visual kinematic constraints up to and including six degrees of freedom can be specified and carried out by this type of feedback arrangement. <p> The tracking system follows the motion of the two fingers of the gripper using a simple pattern matching method, which tracks by locally optimizing the sum of the squared differences between an area of the live image and a stored reference image <ref> (Hager et al. 1995) </ref>. A setpoint midway between the fingers is computed in each camera, and then the distance from this virtual feature to the object endpoint (which is also tracked) is used to implement point-topoint positioning. The tactile sensors used in the experiment are capacitive tactile array sensors.
Reference: <author> G. D. Hager. </author> <title> A Modular System for Robust Hand-Eye Coordination Using Feedback from Stereo Vision. </title> <journal> To appear in the IEEE Transactions on Robotics and Automation, </journal> <note> submitted Jan. </note> <year> 1995a. </year>
Reference-contexts: Visual tracking for both cameras and visual feedback calculations are performed on the Sun host using the XVision tracking system (Hager 1995b) and the Servomatic hand-eye coordination system <ref> (Hager 1995a) </ref>. In the experiments reported here, initial positioning of the gripper is performed using the point-to-point motion primitive reported above.
Reference: <author> G. D. Hager. </author> <title> The XVision System: A General Purpose Substrate for Real-T ime Vision-Based Robotics. </title> <booktitle> Pr oceedings of the Workshop on Vision for Robotics. </booktitle> <pages> pp. 56-63, </pages> <year> 1995b. </year>
Reference-contexts: Standard linearization techniques and PID control methods are then used to drive the manipulator into the desired configuration. Details on this process can be found in <ref> (Hager 1995b) </ref>. Throughout this paper, we treat the robot as a velocity-controlled device A vast number of tactile sensing devices have been presented in the literature, and many could perform the functions described below. <p> The vision system consists of two Sony XC-77 CCD cameras with 12.5 mm lenses connected to a Sun 10/42 workstation via two Imaging Technologies FG101 framegrabbers. Visual tracking for both cameras and visual feedback calculations are performed on the Sun host using the XVision tracking system <ref> (Hager 1995b) </ref> and the Servomatic hand-eye coordination system (Hager 1995a). In the experiments reported here, initial positioning of the gripper is performed using the point-to-point motion primitive reported above.
Reference: <author> T. Lozano-Prez, M. T. Mason, and R. H. Taylor. </author> <title> Automatic Synthesis of Fine-Motion Strategies for Robots. </title> <journal> International Journal of Robotics Research. </journal> <volume> 3(1) </volume> <pages> 3-24, </pages> <year> 1984. </year>
Reference: <author> B. J. Nelson and P . K. Khosla. </author> <title> Force and Vision Resolvability for Assimilating Disparate Sensory Feedback, </title> <note> To Appear in IEEE Trans. Robot. Automation. </note> <month> Sept. </month> <year> 1996. </year>
Reference: <author> E. Ono, N. Kita, and S. Sakane. </author> <title> Strategy for unfolding a fabric piece by cooperative sensing of touch and vision. </title> <booktitle> IEEE/RSJ International Conference on Intelligent Robots and Systems. </booktitle> <volume> vol. 3, </volume> <pages> pp. 441-5. </pages> <year> 1995. </year>
Reference: <author> N. I. Rafla and F . L. Merat. </author> <title> Vision-taction integration for surface representation. </title> <booktitle> IEEE International Confer ence on Systems Engineering. </booktitle> <address> pp.519-22. </address> <year> 1990. </year>
Reference-contexts: One focus has been the use of touch sensing to supplement visual information for object recognition or exploration (Allen 1987, Stansfield 1988, Boshra and Zhang 1995). Other work has developed representations for the integration of visual and tactile spatial information <ref> (Rafla and Merat 1990) </ref>. Rucci and Bajcsy (1995) have examined coordination of active visual and tactile perception, and Ono et al. (1995), have begun to apply these sensing modalities to manipulation by combining vision with a simple form of tactile sensing for handling flexible materials.
Reference: <author> M. Raibert and J. Craig. </author> <title> Hybrid position/force control of manipulators. </title> <journal> ASME J. of Dyn. Sys., Measurement, and Control. </journal> <volume> 103(2) </volume> <pages> 126-133. </pages> <year> 1981. </year>
Reference-contexts: We are studying the interaction of vision and touch in the control of manipulation in unstructured environments. In many ways, the integration of tactile and visual sensing in this context is reminiscent of prior work in hybrid control <ref> (Raibert and Craig 1981) </ref> or LMT style plan execution (Lozano-Prez et al. 1984). In hybrid control, position control and force control are combined to exert forces in some directions of motion, while motions are executed in an orthogonal set of directions.
Reference: <author> M. Rucci and R. </author> <title> Bajcsy. Learning visuo-tactile coordination in robotic systems. </title> <journal> IEEE International Confer ence on Robotics and Automation. </journal> <volume> vol. 3, </volume> <pages> pp. 2678-83. </pages> <year> 1995. </year>
Reference: <author> J. S. Son, E. A. Monteverde, and R. Howe. </author> <title> A Tactile Sensor for Localizing Transient Events in Manipulation, </title> <booktitle> IEEE International Conference on Robotics and Automation. </booktitle> <volume> vol. 1, </volume> <pages> pp. 471-476. </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The array sensor response is slow because of the multiplexing required, but it can be used to determine many parameters including contact. A more suitable sensor for detecting contact, such as the stress rate sensor <ref> (Son et al. 1994) </ref>, could provide more sensitive and timely response. This primitive begins with the hand approximately centered on the object by visual control, and the fingers are commanded to close.
Reference: <author> J. S. Son, M. R. Cutkosky, and R. D. Howe. </author> <title> A Comparison of Object Localization by Contact Sensors. </title> <journal> To appear in the Journal of Robotics and Autonomous Systems. </journal> <volume> 17(4), </volume> <month> June/July </month> <year> 1996. </year>
Reference-contexts: The fundamental measures are distances and directions, and resolution is limited to a fraction of the field of view. Touch, particularly using tactile arrays, senses local geometry and pressure, providing both length and force measurements <ref> (Son et al. 1996) </ref>. Sensing takes place at contact locations that are hidden from view by the hand, and which acts as a pure integrator. <p> With more elaborate signal processing, many other parameters may be extracted, including object curvature (Fearing 1990b). Intrinsic contact sensing, which uses multi-axis force and torque sensors, could provide similar information about contact location and forces. A detailed comparison of the performance of both sensing schemes can be found in <ref> (Son et al. 1996) </ref>. 2.2.1 Make Contact Sensing first contact between the hand and an object is essential to minimizing forces on the object. <p> The primitive starts with one finger in contact with the object. From a cylindrical sensor contacting a parallel flat sided object, we can determine the orientation of the object <ref> (Son and Howe 1996) </ref>. In the experimental configuration, the object orientation angle relative to the finger is simply , where is the contact pressure centroid location on the sensor surface and is the sensor radius.
Reference: <author> J. S. Son and R. D. Howe. </author> <title> Tactile Sensing and Stif fness Control with Multifingered Hands. </title> <booktitle> To appear in IEEE International Conference on Robotics and Automation. </booktitle> <year> 1996. </year>
Reference-contexts: The fundamental measures are distances and directions, and resolution is limited to a fraction of the field of view. Touch, particularly using tactile arrays, senses local geometry and pressure, providing both length and force measurements <ref> (Son et al. 1996) </ref>. Sensing takes place at contact locations that are hidden from view by the hand, and which acts as a pure integrator. <p> With more elaborate signal processing, many other parameters may be extracted, including object curvature (Fearing 1990b). Intrinsic contact sensing, which uses multi-axis force and torque sensors, could provide similar information about contact location and forces. A detailed comparison of the performance of both sensing schemes can be found in <ref> (Son et al. 1996) </ref>. 2.2.1 Make Contact Sensing first contact between the hand and an object is essential to minimizing forces on the object. <p> The primitive starts with one finger in contact with the object. From a cylindrical sensor contacting a parallel flat sided object, we can determine the orientation of the object <ref> (Son and Howe 1996) </ref>. In the experimental configuration, the object orientation angle relative to the finger is simply , where is the contact pressure centroid location on the sensor surface and is the sensor radius.
Reference: <author> S. A. Stansfield, </author> <title> A robotic perceptual system utilizing passive vision and active touch. </title> <journal> International Journal of Robotics Resear ch. </journal> <volume> 7(6) </volume> <pages> 138-61. </pages> <year> 1988. </year>
References-found: 22

