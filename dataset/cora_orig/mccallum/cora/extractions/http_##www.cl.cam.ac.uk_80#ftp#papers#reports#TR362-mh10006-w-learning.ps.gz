URL: http://www.cl.cam.ac.uk:80/ftp/papers/reports/TR362-mh10006-w-learning.ps.gz
Refering-URL: http://www.cl.cam.ac.uk:80/ftp/papers/reports/
Root-URL: 
Title: W-learning: Competition among selfish Q-learners  
Author: Mark Humphrys 
Keyword: mobile robots, subsumption architecture, action selection, reinforcement learning, Q-learning, multi-module learning, genetic algorithms  
Date: April 1995  
Web: http://www.cl.cam.ac.uk/users/mh10006  
Affiliation: University of Cambridge, Computer Laboratory  
Abstract: W-learning is a self-organising action-selection scheme for systems with multiple parallel goals, such as autonomous mobile robots. It uses ideas drawn from the subsumption architecture for mobile robots (Brooks), implementing them with the Q-learning algorithm from reinforcement learning (Watkins). Brooks explores the idea of multiple sensing-and-acting agents within a single robot, more than one of which is capable of controlling the robot on its own if allowed. I introduce a model where the agents are not only autonomous, but are in fact engaged in direct competition with each other for control of the robot. Interesting robots are ones where no agent achieves total victory, but rather the state-space is fragmented among different agents. Having the agents operate by Q-learning proves to be a way to implement this, leading to a local, incremental algorithm (W-learning) to resolve competition. I present a sketch proof that this algorithm converges when the world is a discrete, finite Markov decision process. For each state, competition is resolved with the most likely winner of the state being the agent that is most likely to suffer the most if it does not win. In this way, W-learning can be viewed as `fair' resolution of competition. In the empirical section, I show how W-learning may be used to define spaces of agent-collections whose action selection is learnt rather than hand-designed. This is the kind of solution-space that may be searched with a genetic algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [Ackley and Littman, 1991] <author> Ackley, David and Littman, </author> <title> Michael (1991), Interactions between learning and evolution, </title> <editor> in Christopher G.Langton et al., eds., </editor> <booktitle> Artificial Life II. </booktitle>
Reference-contexts: It is productive to think of rewards not as something external and informative but as simply some arbitrary internal signals (a similar attitude is taken by Ackley and Littman <ref> [Ackley and Littman, 1991] </ref>, in their evolution-plus-learning experiment). 19 If an agent generates high rewards for itself when it sees predators, then its converged Q fl will cause it to seek out predators. This may not be a good thing, but that is a separate issue.
Reference: [Blumberg, 1994] <author> Blumberg, </author> <note> Bruce (1994), Action-Selection in Hamsterdam: Lessons from Ethology, </note> <editor> in Dave Cliff et al., eds., </editor> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Brooks liberates his modules by giving them full sensing-and-acting powers, but does not go as far as letting them compete for control. Instead, action-selection is a job for the programmer. Brooks' original scheme has been extended (see survey in [Brooks, 1994]), and other schemes have been proposed <ref> [Maes, 1989, Blumberg, 1994, Sahota, 1994] </ref>, but action-selection remains something basically designed rather than self-organised. <p> For somewhat-autonomous, somewhat-competing modules within a single physical robot, [Brooks, 1986] uses layer (though [Brooks, 1994] also uses process), [Minsky, 1986] uses agent (even though most of his agents do not interact directly with the world), <ref> [Blumberg, 1994] </ref> uses activity and [Sahota, 1994] uses behavior. 3 To be more precise, let the collection consist of agents A 1 ; : : : ; A n . Time steps are discrete. Each time step, the robot observes the world to be in some state x. <p> Blumberg <ref> [Blumberg, 1994] </ref> uses information sharing between agents, which may lead to a compromise action. Here there is no explicit information sharing. Also, the division of control is state-based rather than time-based.
Reference: [Brooks, 1986] <author> Brooks, Rodney A. </author> <year> (1986), </year> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE Journal of Robotics and Automation vol.RA-2, </journal> <volume> no.1, </volume> <month> Mar </month> <year> 1986. </year>
Reference-contexts: University of Cambridge Computer Laboratory technical report no.362, available from http://www.cl.cam.ac.uk/users/mh10006/publications.html (or direct from ftp://ftp.cl.cam.ac.uk/papers/reports/TR362-mh10006-w-learning.ps.gz) y postal address: University of Cambridge, Computer Laboratory, New Museums Site, Pem-broke St., Cambridge CB2 3QG, England. tel: +44 1223 335443. fax: +44 1223 334678 or 334679. email: Mark.Humphrys@cl.cam.ac.uk 1 1.1 The subsumption architecture Brooks <ref> [Brooks, 1986, Brooks, 1991] </ref> introduces an architecture for building autonomous mobile robots which he calls the subsumption architecture (Figure 1). <p> Under a stricter definition one might claim that the robot is the only agent here, with varying software inside it. For somewhat-autonomous, somewhat-competing modules within a single physical robot, <ref> [Brooks, 1986] </ref> uses layer (though [Brooks, 1994] also uses process), [Minsky, 1986] uses agent (even though most of his agents do not interact directly with the world), [Blumberg, 1994] uses activity and [Sahota, 1994] uses behavior. 3 To be more precise, let the collection consist of agents A 1 ; :
Reference: [Brooks, 1991] <author> Brooks, Rodney A. </author> <year> (1991), </year> <title> Intelligence without Representation, </title> <booktitle> Artificial Intelligence 47 </booktitle> <pages> 139-160. </pages>
Reference-contexts: University of Cambridge Computer Laboratory technical report no.362, available from http://www.cl.cam.ac.uk/users/mh10006/publications.html (or direct from ftp://ftp.cl.cam.ac.uk/papers/reports/TR362-mh10006-w-learning.ps.gz) y postal address: University of Cambridge, Computer Laboratory, New Museums Site, Pem-broke St., Cambridge CB2 3QG, England. tel: +44 1223 335443. fax: +44 1223 334678 or 334679. email: Mark.Humphrys@cl.cam.ac.uk 1 1.1 The subsumption architecture Brooks <ref> [Brooks, 1986, Brooks, 1991] </ref> introduces an architecture for building autonomous mobile robots which he calls the subsumption architecture (Figure 1).
Reference: [Brooks, 1994] <author> Brooks, Rodney A. </author> <year> (1994), </year> <title> Coherent Behavior from Many Adaptive Processes, </title> <editor> in Dave Cliff et al., eds., </editor> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Brooks liberates his modules by giving them full sensing-and-acting powers, but does not go as far as letting them compete for control. Instead, action-selection is a job for the programmer. Brooks' original scheme has been extended (see survey in <ref> [Brooks, 1994] </ref>), and other schemes have been proposed [Maes, 1989, Blumberg, 1994, Sahota, 1994], but action-selection remains something basically designed rather than self-organised. <p> Under a stricter definition one might claim that the robot is the only agent here, with varying software inside it. For somewhat-autonomous, somewhat-competing modules within a single physical robot, [Brooks, 1986] uses layer (though <ref> [Brooks, 1994] </ref> also uses process), [Minsky, 1986] uses agent (even though most of his agents do not interact directly with the world), [Blumberg, 1994] uses activity and [Sahota, 1994] uses behavior. 3 To be more precise, let the collection consist of agents A 1 ; : : : ; A n
Reference: [Collins, 1992] <author> Collins, Robert J. </author> <year> (1992), </year> <title> Studies in Artificial Evolution, </title> <type> PhD thesis, UCLA. 29 </type>
Reference-contexts: Adaptive agents are ones that happen to generate suitable internal rewards. * A concise genotype. Much current work on evolving control systems for robots involves encoding the entire explicit control program in the genotype seethe Artificial Neural Networks (ANN's) of much current Artificial Life work <ref> [Collins, 1992, Harvey et al., 1993] </ref>, or the Genetic Programming (GP) of Koza [Koza, 1991]. Here, the genotype simply states what is good and what is bad about the world (in each agent's perhaps unadaptive opinion) and nothing else. <p> Many researchers <ref> [Collins, 1992, Sims, 1994, Harvey et al., 1994] </ref> have run into the problem of evolving creatures that present a similar challenge of analysis as natural creatures do. Here, evolving the rewards rather than the explicit behavior will permit at least some analysis of the solutions found.
Reference: [Dawkins, 1986] <author> Dawkins, Richard (1986), </author> <title> The Blind Watchmaker, </title> <publisher> Penguin Books. </publisher>
Reference-contexts: So we can define spaces of possible robot architectures where every point in the space represents a robot that can be built and tried out (for the best exposition of this concept see <ref> [Dawkins, 1986, Ch.3] </ref>). Such a space can be searched in a manner similar to evolution by natural selection [Langton, 1989].
Reference: [Edelman, 1989] <author> Edelman, Gerald M. </author> <year> (1989), </year> <title> The Remembered Present: A Biological Theory of Consciousness, </title> <publisher> Basic Books. </publisher>
Reference-contexts: We expect: E (d kk (x)) = 0 (though we do not actually update W k (x)), and we expect for i 6= k: 4 This can be compared to Edelman's biological theory of Neural Darwinism <ref> [Edelman, 1989, Edelman, 1992] </ref>, in which the mind is viewed as a dynamic, competing collection of what he calls neuronal groups. The idea is appealing it has been compared to a "rainforest" or dynamic ecosystem inside the head. However, Edelman presents no algorithm to show how it could be implemented.
Reference: [Edelman, 1992] <author> Edelman, Gerald M. </author> <year> (1992), </year> <title> Bright Air, Brilliant Fire: On the Matter of the Mind, </title> <publisher> Basic Books. </publisher>
Reference-contexts: We expect: E (d kk (x)) = 0 (though we do not actually update W k (x)), and we expect for i 6= k: 4 This can be compared to Edelman's biological theory of Neural Darwinism <ref> [Edelman, 1989, Edelman, 1992] </ref>, in which the mind is viewed as a dynamic, competing collection of what he calls neuronal groups. The idea is appealing it has been compared to a "rainforest" or dynamic ecosystem inside the head. However, Edelman presents no algorithm to show how it could be implemented.
Reference: [Goldberg, 1989] <author> Goldberg, David E. </author> <year> (1989), </year> <title> Genetic Algorithms: in search, optimization, and machine learning, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Such a space can be searched in a manner similar to evolution by natural selection [Langton, 1989]. I use a Genetic Algorithm (GA) (from [Holland, 1975], for an introduction see <ref> [Goldberg, 1989] </ref>) to search such a space of agent-collections for collections whose W-converged situation is adaptive (by some criterion). 5.1 The Genetic Algorithm A genotype of the GA encodes a collection of real-valued reward functions, each of which defines a Q-learning agent.
Reference: [Harvey et al., 1993] <author> Harvey, Inman; Husbands, Phil; and Cliff, </author> <title> Dave (1993), Issues in Evolutionary Robotics, </title> <editor> in Jean-Arcady Meyer et al., eds., </editor> <booktitle> Proceedings of the Second International Conference on Simulation of Adaptive Behavior (SAB-92). </booktitle>
Reference-contexts: Adaptive agents are ones that happen to generate suitable internal rewards. * A concise genotype. Much current work on evolving control systems for robots involves encoding the entire explicit control program in the genotype seethe Artificial Neural Networks (ANN's) of much current Artificial Life work <ref> [Collins, 1992, Harvey et al., 1993] </ref>, or the Genetic Programming (GP) of Koza [Koza, 1991]. Here, the genotype simply states what is good and what is bad about the world (in each agent's perhaps unadaptive opinion) and nothing else.
Reference: [Harvey et al., 1994] <author> Harvey, Inman; Husbands, Phil; and Cliff, </author> <title> Dave (1994), Seeing The Light: Artificial Evolution, Real Vision, </title> <editor> in Dave Cliff et al., eds., </editor> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Many researchers <ref> [Collins, 1992, Sims, 1994, Harvey et al., 1994] </ref> have run into the problem of evolving creatures that present a similar challenge of analysis as natural creatures do. Here, evolving the rewards rather than the explicit behavior will permit at least some analysis of the solutions found.
Reference: [Holland, 1975] <author> Holland, John H. </author> <year> (1975), </year> <booktitle> Adaptation in Natural and Artificial Systems, </booktitle> <address> Ann Arbor, </address> <publisher> Univ. Michigan Press. </publisher>
Reference-contexts: Such a space can be searched in a manner similar to evolution by natural selection [Langton, 1989]. I use a Genetic Algorithm (GA) (from <ref> [Holland, 1975] </ref>, for an introduction see [Goldberg, 1989]) to search such a space of agent-collections for collections whose W-converged situation is adaptive (by some criterion). 5.1 The Genetic Algorithm A genotype of the GA encodes a collection of real-valued reward functions, each of which defines a Q-learning agent.
Reference: [Kaelbling, 1993] <author> Kaelbling, </author> <title> Leslie Pack (1993), Learning in Embedded Systems, </title> <publisher> The MIT Press/Bradford Books. </publisher>
Reference-contexts: In fact, presenting arguments only about the limitations of various traditional AI models, he draws the extreme conclusion that no computer algorithm could implement his ideas. No mention is made of self-modifying, reinforcement-learning agents embedded in a world (such as the agents described here, but see <ref> [Kaelbling, 1993] </ref> for a broad survey), to which his criticisms do not apply. Indeed, RL seems to be exactly what Edelman is looking for.
Reference: [Koza, 1991] <author> Koza, John R. </author> <year> (1991), </year> <title> Genetic evolution and co-evolution of computer programs, </title> <editor> in Christopher G.Langton et al., eds., </editor> <booktitle> Artificial Life II. </booktitle>
Reference-contexts: Much current work on evolving control systems for robots involves encoding the entire explicit control program in the genotype seethe Artificial Neural Networks (ANN's) of much current Artificial Life work [Collins, 1992, Harvey et al., 1993], or the Genetic Programming (GP) of Koza <ref> [Koza, 1991] </ref>. Here, the genotype simply states what is good and what is bad about the world (in each agent's perhaps unadaptive opinion) and nothing else.
Reference: [Langton, 1989] <author> Langton, Christopher G. </author> <year> (1989), </year> <title> Artificial Life, </title> <editor> in Christopher G.Langton, ed., </editor> <booktitle> Artificial Life. </booktitle>
Reference-contexts: Such a space can be searched in a manner similar to evolution by natural selection <ref> [Langton, 1989] </ref>.
Reference: [Lin, 1993] <author> Lin, </author> <month> Long-Ji </month> <year> (1993), </year> <title> Scaling up Reinforcement Learning for robot control, </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: In autonomous mobile robots (and many other systems) we are also likely to be interested in subtasks acting in parallel, and interrupting each other rather than running to completion. Lin <ref> [Lin, 1993] </ref> learns the decomposition for potentially parallel, non-terminating subtasks. Each agent learns a different Q i (x; a) to solve a subtask, and the robot learns Q (x; i), where i is which agent to choose in state x.
Reference: [Maes, 1989] <author> Maes, </author> <title> Pattie (1989), The dynamics of action selection, </title> <booktitle> Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89). </booktitle>
Reference-contexts: Brooks liberates his modules by giving them full sensing-and-acting powers, but does not go as far as letting them compete for control. Instead, action-selection is a job for the programmer. Brooks' original scheme has been extended (see survey in [Brooks, 1994]), and other schemes have been proposed <ref> [Maes, 1989, Blumberg, 1994, Sahota, 1994] </ref>, but action-selection remains something basically designed rather than self-organised.
Reference: [Minsky, 1986] <author> Minsky, </author> <title> Marvin (1986), The Society of Mind, </title> <publisher> Simon and Schuster, </publisher> <address> New York. </address>
Reference-contexts: Now they are fully autonomous sensing-and-acting agents <ref> [Minsky, 1986] </ref>, 1 not ordered in any hierarchy, but rather in a loose collection. * Have them compete for control, having to make a case that they should be given it. <p> Under a stricter definition one might claim that the robot is the only agent here, with varying software inside it. For somewhat-autonomous, somewhat-competing modules within a single physical robot, [Brooks, 1986] uses layer (though [Brooks, 1994] also uses process), <ref> [Minsky, 1986] </ref> uses agent (even though most of his agents do not interact directly with the world), [Blumberg, 1994] uses activity and [Sahota, 1994] uses behavior. 3 To be more precise, let the collection consist of agents A 1 ; : : : ; A n . <p> (f; 2))) &gt; W c ((e; (f; 1))) &gt; W c ((e; (f; 0))) A very strong `Food', only rarely interrupted by `Clean', would be repre sented by, for a given e: W f ((e; (2; c))) &gt; W f ((e; (1; c))) &gt; W c ((e; (f; 2))) Minsky <ref> [Minsky, 1986] </ref> warns that too simple forms of state-based switching will be unable to engage in opportunistic behavior. His example is of a hungry and thirsty animal. Food is only found in the North, water in the South.
Reference: [Moore, 1990] <author> Moore, Andrew W. </author> <year> (1990), </year> <title> Efficient Memory-based Learning for Robot Control, </title> <type> PhD thesis, </type> <institution> University of Cambridge, Computer Laboratory. </institution>
Reference-contexts: A top-down approach to multi-module RL systems involves identifying the task and decomposing it into subtasks, each of which can be solved by a single RL agent. Moore <ref> [Moore, 1990] </ref> does this by hand, but it is clear that this is only feasible with certain problems. Singh [Singh, 1992], and later Tham and Prager [Tham and Prager, 1994], learn the decomposition, but only in a class of problems where subtasks combine sequentially to solve the main task.
Reference: [Sahota, 1994] <author> Sahota, Michael K. </author> <year> (1994), </year> <title> Action Selection for Robots in Dynamic Environments through Inter-behaviour Bidding, </title> <editor> in Dave Cliff et al., eds., </editor> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Brooks liberates his modules by giving them full sensing-and-acting powers, but does not go as far as letting them compete for control. Instead, action-selection is a job for the programmer. Brooks' original scheme has been extended (see survey in [Brooks, 1994]), and other schemes have been proposed <ref> [Maes, 1989, Blumberg, 1994, Sahota, 1994] </ref>, but action-selection remains something basically designed rather than self-organised. <p> For somewhat-autonomous, somewhat-competing modules within a single physical robot, [Brooks, 1986] uses layer (though [Brooks, 1994] also uses process), [Minsky, 1986] uses agent (even though most of his agents do not interact directly with the world), [Blumberg, 1994] uses activity and <ref> [Sahota, 1994] </ref> uses behavior. 3 To be more precise, let the collection consist of agents A 1 ; : : : ; A n . Time steps are discrete. Each time step, the robot observes the world to be in some state x.
Reference: [Sims, 1994] <author> Sims, </author> <title> Karl (1994), Evolving 3D Morphology and Behavior by Competition, </title> <editor> in Rodney A.Brooks and Pattie Maes, eds., </editor> <booktitle> Artificial Life IV. </booktitle>
Reference-contexts: Many researchers <ref> [Collins, 1992, Sims, 1994, Harvey et al., 1994] </ref> have run into the problem of evolving creatures that present a similar challenge of analysis as natural creatures do. Here, evolving the rewards rather than the explicit behavior will permit at least some analysis of the solutions found.
Reference: [Singh, 1992] <author> Singh, Satinder P. </author> <year> (1992), </year> <title> Transfer of Learning by Composing Solutions of Elemental Sequential Tasks, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 323-339. </pages>
Reference-contexts: A top-down approach to multi-module RL systems involves identifying the task and decomposing it into subtasks, each of which can be solved by a single RL agent. Moore [Moore, 1990] does this by hand, but it is clear that this is only feasible with certain problems. Singh <ref> [Singh, 1992] </ref>, and later Tham and Prager [Tham and Prager, 1994], learn the decomposition, but only in a class of problems where subtasks combine sequentially to solve the main task.
Reference: [Sutton, 1988] <author> Sutton, Richard S. </author> <year> (1988), </year> <title> Learning to Predict by the Methods of Temporal Differences, </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: (x); find k execute ( a [k] ); state y := observe (); for ( all i ) r [i] := A [i].reward (x,y); A [i].updateQ ( x, a [k], y, r [i] ); if (i != k) A [i].updateW ( x, a [i], y, r [i] ); - Alternatively <ref> [Sutton, 1988] </ref>, consider Q-learning as the process: P := P + ff Q (A P) Then W-learning is: W := W + ff W ((P A) W ) Note how the agents learn their Q-values together, rather than alone.
Reference: [Tan, 1993] <author> Tan, </author> <title> Ming (1993), Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: To do the job with local reward functions only, we must adopt a bottom-up approach. A bottom-up approach studies the behavior that emerges when multiple RL agents are combined in different ways. Tan <ref> [Tan, 1993] </ref> studies the benefits of cooperation among agents, where each agent has their own body to control in the world. He focuses on communication of information among agents (analogous, say, to the communication of information among ants). Co-operation is based on explicit interactions with other agents.
Reference: [Tham and Prager, 1994] <author> Tham, Chen K. and Prager, Richard W. </author> <year> (1994), </year> <title> A modular Q-learning architecture for manipulator task decomposition, </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference-contexts: Moore [Moore, 1990] does this by hand, but it is clear that this is only feasible with certain problems. Singh [Singh, 1992], and later Tham and Prager <ref> [Tham and Prager, 1994] </ref>, learn the decomposition, but only in a class of problems where subtasks combine sequentially to solve the main task.
Reference: [Watkins, 1989] <author> Watkins, Christopher J.C.H. </author> <year> (1989), </year> <title> Learning from delayed rewards, </title> <type> PhD thesis, </type> <institution> University of Cambridge, Psychology Department. </institution>
Reference-contexts: By trial-and-error, the agent learns to take the actions which maximise its rewards. 2.1 Q-learning Watkins <ref> [Watkins, 1989] </ref> introduces a method of reinforcement learning which he calls Q-learning. The agent exists within a world that can be modelled as a Markov decision process (MDP).
Reference: [Watkins and Dayan, 1992] <author> Watkins, Christopher J.C.H. and Dayan, </author> <title> Peter (1992), Technical Note: </title> <booktitle> Q-Learning, Machine Learning 8 </booktitle> <pages> 279-292. 30 </pages>
Reference-contexts: bounded, it follows that the Q-values are bounded (Lemma A.2.1). 2.1.3 Convergence of Q-learning If each pair (x; a) is visited an infinite number of times, then Q-learning converges to a unique set of values Q (x; a) = Q fl (x; a) which define a stationary deterministic optimal policy <ref> [Watkins and Dayan, 1992] </ref>. Q-learning is asyn chronous and sampled each Q (x; a) is updated one at a time, and the control policy may visit them in any order, so long as it visits them an infinite number of times.
References-found: 28

