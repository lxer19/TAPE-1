URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-11.ps
Refering-URL: http://www.cs.okstate.edu/~nstreet/research/publications.html
Root-URL: 
Title: Improved Generalization via Tolerant Training  
Author: W. Nick Street -- O. L. Mangasarian 
Keyword: Inductive learning, function approximation, generalization Running head: Tolerant Training  
Address: Stillwater, OK 74078  1210 West Dayton Street, Madison, WI 53706  
Affiliation: Computer Science Department Oklahoma State University 205 Mathematical Sciences,  Department of Computer Sciences University of Wisconsin  
Note: Submitted to Machine Learning,  
Email: nstreet@cs.okstate.edu  olvi@cs.wisc.edu  
Date: July 1995 Resubmitted December, 1996  December 20, 1996  
Abstract: Theoretical and computational justification is given for improved generalization when the training set is learned with less accuracy. The model used for this investigation is a simple linear one. It is shown that learning a training set with a tolerance t improves generalization, over zero-tolerance training, for any testing set satisfying a certain closeness condition to the training set. These results, obtained via a mathematical programming formulation, are placed in the context of some well-known machine learning results. Computational confirmation of improved generalization is given for linear systems (including nine of the twelve real-world data sets tested), as well as for nonlinear systems such as neural networks for which no theoretical results are available at present. In particular, the tolerant training method improves generalization on noisy, sparse, and over-parameterized problems. 
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Inc., </publisher> <address> Pacific Grove, CA. </address> <month> 19 </month>
Reference-contexts: Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees <ref> [Breiman et al., 1984, Quinlan, 1993a] </ref>, k-nearest-neighbors [Hart, 1967], smoothing splines [Wahba, 1990], and feature selection [Draper and Smith, 1966].
Reference: [Draper and Smith, 1966] <author> Draper, N. R. and Smith, H. </author> <year> (1966). </year> <title> Applied Regression Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees [Breiman et al., 1984, Quinlan, 1993a], k-nearest-neighbors [Hart, 1967], smoothing splines [Wahba, 1990], and feature selection <ref> [Draper and Smith, 1966] </ref>. The true test of tolerant training, and other overtraining avoidance techniques, is whether it improves generalization on a wide variety of common learning problems, particularly those in which overtraining is likely to occur. Our computational results, including those in the next section, indicate that it does.
Reference: [Geman et al., 1992] <author> Geman, S., Bienestock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: In order to better understand the circumstances in which tolerant training is useful, we view our theoretical results as a framework for generalization based upon the foundation of mathematical programming. Mathematical models of generalization [Vapnik, 1995, Wolpert, 1995] have grown from several diverse fields: for instance, statistics <ref> [Geman et al., 1992, Wahba, 1990] </ref>, computational complexity [Valiant, 1984], and statistical physics [Tishby et al., 1989]. We believe that the methods and tools of mathematical optimization are an effective way to explore the theory of generalization, which in turn will lead to more effective applications of supervised learning. <p> Our computational results, including those in the next section, indicate that it does. We now examine the error-reduction capabilities of tolerant training in the context of the 12 bias/variance tradeoff <ref> [Geman et al., 1992] </ref>. Generalization error can be decomposed into two components: variance, which refers to the variability of the learned concept depending on the particular training data used, and bias, which measures the limitations of the representational power of the learning system.
Reference: [Harrison and Rubinfeld, 1978] <author> Harrison, D. and Rubinfeld, D. L. </author> <year> (1978). </year> <title> Hedonic prices and the demand for clean air. </title> <journal> J. Environ. Economics and Management, </journal> <volume> 5 </volume> <pages> 81-102. </pages>
Reference-contexts: 55035 63589 [Western, 1996] t &gt; 0 192.2 130.9 51487 63264 Ships (m = 34, n = 3) t = 0 9.794 8.509 152.3 256.3 [McCullagh and Nelder, 1989] t &gt; 0 8.306 5.728 128.3 220.4 Housing (m = 50, n = 13) t = 0 2.781 1.859 43.10 76.36 <ref> [Harrison and Rubinfeld, 1978] </ref> t &gt; 0 2.875 1.885 14.87 27.05 Table 1: Cross-validated estimates of error and standard deviation of error performing regression with and without tolerant training.
Reference: [Hart, 1967] <author> Hart, P. </author> <year> (1967). </year> <title> The condensed nearest neighbor rule. </title> <journal> Transactions on Information Theory, IT-14:515-516. </journal>
Reference-contexts: Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees [Breiman et al., 1984, Quinlan, 1993a], k-nearest-neighbors <ref> [Hart, 1967] </ref>, smoothing splines [Wahba, 1990], and feature selection [Draper and Smith, 1966]. The true test of tolerant training, and other overtraining avoidance techniques, is whether it improves generalization on a wide variety of common learning problems, particularly those in which overtraining is likely to occur.
Reference: [Lang et al., 1990] <author> Lang, K., Waibel, A., and Hinton, G. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43. </pages>
Reference-contexts: Tolerant training, then, is simply an overtraining avoidance technique, and should therefore be viewed in the same light as other overfitting avoidance techniques. In the field of artificial neural networks, such techniques include early stopping <ref> [Lang et al., 1990] </ref>, soft weight sharing [Nowlan and Hinton, 1991], and various methods for deleting weights and nodes, such as optimal brain damage [Le Cun et al., 1990]. <p> This error was divided by 2, 5, 10 and 20, and each resulting value was used as t to generate a tolerant model. These models were evaluated using a tuning or validation set <ref> [Lang et al., 1990] </ref>, and the value that performed best was then used for a final model that 14 Data Set Tolerance Mean Absolute Error Mean Squared Error Cancer Recurrence (m = 47, n = 32) t = 0 83.42 40.53 660.1 431.6 [Mangasarian et al., 1995] t &gt; 0 36.80
Reference: [Le Cun et al., 1990] <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the field of artificial neural networks, such techniques include early stopping [Lang et al., 1990], soft weight sharing [Nowlan and Hinton, 1991], and various methods for deleting weights and nodes, such as optimal brain damage <ref> [Le Cun et al., 1990] </ref>. Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees [Breiman et al., 1984, Quinlan, 1993a], k-nearest-neighbors [Hart, 1967], smoothing splines [Wahba, 1990], and feature selection [Draper and Smith, 1966].
Reference: [Mangasarian, 1986] <author> Mangasarian, O. </author> <year> (1986). </year> <title> Some applications of penalty functions in mathematical programming. </title> <editor> In Conti, R., De Giorgi, E., and Giannessi, F., editors, </editor> <booktitle> Optimization and Related Fields, </booktitle> <pages> pages 307-329. </pages> <publisher> Springer-Verlag, </publisher> <address> Heidelberg. </address> <note> Lecture Notes in Mathematics 1190. </note>
Reference: [Mangasarian et al., 1995] <author> Mangasarian, O. L., Street, W. N., and Wolberg, W. H. </author> <year> (1995). </year> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577. </pages> <note> Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/. </note>
Reference-contexts: evaluated using a tuning or validation set [Lang et al., 1990], and the value that performed best was then used for a final model that 14 Data Set Tolerance Mean Absolute Error Mean Squared Error Cancer Recurrence (m = 47, n = 32) t = 0 83.42 40.53 660.1 431.6 <ref> [Mangasarian et al., 1995] </ref> t &gt; 0 36.80 15.40 585.5 250.9 Servo (m = 167, n = 4) t = 0 1.001 0.351 2.944 1.873 [Quinlan, 1993b] t &gt; 0 1.012 0.344 2.934 1.872 Pollution (m = 60, n = 15) t = 0 62.75 32.55 5447 8250 [McDonald and Schwing,
Reference: [McCullagh and Nelder, 1989] <author> McCullagh, P. and Nelder, J. A. </author> <year> (1989). </year> <title> Generalized Linear Models, chapter 6, page 137. </title> <publisher> Chapman and Hall, </publisher> <address> London, 2 edition. </address>
Reference-contexts: 5447 8250 [McDonald and Schwing, 1973] t &gt; 0 45.93 19.15 3661 3118 Strikes (m = 35, n = 4) t = 0 191.9 117.7 55035 63589 [Western, 1996] t &gt; 0 192.2 130.9 51487 63264 Ships (m = 34, n = 3) t = 0 9.794 8.509 152.3 256.3 <ref> [McCullagh and Nelder, 1989] </ref> t &gt; 0 8.306 5.728 128.3 220.4 Housing (m = 50, n = 13) t = 0 2.781 1.859 43.10 76.36 [Harrison and Rubinfeld, 1978] t &gt; 0 2.875 1.885 14.87 27.05 Table 1: Cross-validated estimates of error and standard deviation of error performing regression with and
Reference: [McDonald and Schwing, 1973] <author> McDonald, G. C. and Schwing, R. C. </author> <year> (1973). </year> <title> Instabilities of regression estimates relating air pollution to mortality. </title> <journal> Technometrics, </journal> <volume> 15 </volume> <pages> 463-482. </pages>
Reference-contexts: 660.1 431.6 [Mangasarian et al., 1995] t &gt; 0 36.80 15.40 585.5 250.9 Servo (m = 167, n = 4) t = 0 1.001 0.351 2.944 1.873 [Quinlan, 1993b] t &gt; 0 1.012 0.344 2.934 1.872 Pollution (m = 60, n = 15) t = 0 62.75 32.55 5447 8250 <ref> [McDonald and Schwing, 1973] </ref> t &gt; 0 45.93 19.15 3661 3118 Strikes (m = 35, n = 4) t = 0 191.9 117.7 55035 63589 [Western, 1996] t &gt; 0 192.2 130.9 51487 63264 Ships (m = 34, n = 3) t = 0 9.794 8.509 152.3 256.3 [McCullagh and Nelder,
Reference: [Meyer, 1989] <author> Meyer, M. </author> <year> (1989). </year> <institution> StatLib. Carnegie Mellon University Satistics Department, </institution> <note> http://lib.stat.cmu.edu. 20 </note>
Reference-contexts: Conversely, the top curve demonstrates (*x (0) + A T p) T A (x (t ) x (0)) &lt; 0, and tolerant training is of no value. 4.2 Real data sets Experiments were performed on a number of data sets collected from Statlib <ref> [Meyer, 1989] </ref> and the UCI machine learning repository [Murphy and Aha, 1994]. Each of these problems has a number of linear input features (both real-valued and integer-valued) and a single linear output.
Reference: [Murphy and Aha, 1994] <author> Murphy, P. M. and Aha, D. W. </author> <year> (1994). </year> <note> UCI repository of ma--chine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </note> <institution> CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: the top curve demonstrates (*x (0) + A T p) T A (x (t ) x (0)) &lt; 0, and tolerant training is of no value. 4.2 Real data sets Experiments were performed on a number of data sets collected from Statlib [Meyer, 1989] and the UCI machine learning repository <ref> [Murphy and Aha, 1994] </ref>. Each of these problems has a number of linear input features (both real-valued and integer-valued) and a single linear output.
Reference: [Nowlan and Hinton, 1991] <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1991). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <editor> In Moody, J., Hanson, S., and Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Tolerant training, then, is simply an overtraining avoidance technique, and should therefore be viewed in the same light as other overfitting avoidance techniques. In the field of artificial neural networks, such techniques include early stopping [Lang et al., 1990], soft weight sharing <ref> [Nowlan and Hinton, 1991] </ref>, and various methods for deleting weights and nodes, such as optimal brain damage [Le Cun et al., 1990].
Reference: [Quinlan, 1993a] <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees <ref> [Breiman et al., 1984, Quinlan, 1993a] </ref>, k-nearest-neighbors [Hart, 1967], smoothing splines [Wahba, 1990], and feature selection [Draper and Smith, 1966].
Reference: [Quinlan, 1993b] <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based and model-based learn-ing. </title> <editor> In Utgoff, P. E., editor, </editor> <booktitle> Proceedings of the 10th International Conference on Machine Learning, </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 14 Data Set Tolerance Mean Absolute Error Mean Squared Error Cancer Recurrence (m = 47, n = 32) t = 0 83.42 40.53 660.1 431.6 [Mangasarian et al., 1995] t &gt; 0 36.80 15.40 585.5 250.9 Servo (m = 167, n = 4) t = 0 1.001 0.351 2.944 1.873 <ref> [Quinlan, 1993b] </ref> t &gt; 0 1.012 0.344 2.934 1.872 Pollution (m = 60, n = 15) t = 0 62.75 32.55 5447 8250 [McDonald and Schwing, 1973] t &gt; 0 45.93 19.15 3661 3118 Strikes (m = 35, n = 4) t = 0 191.9 117.7 55035 63589 [Western, 1996] t
Reference: [Robinson, 1981] <author> Robinson, S. M. </author> <year> (1981). </year> <title> Some continuity properties of polyhedral multifunctions. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 206-214. </pages>
Reference-contexts: 0: Thus x (t ) = (*I + A T II A II ) 1 ((A T II a II ) + (A T II e II )t ); t 2 ft j g # 0: Since x (t ) is continuous, in fact Lipschitz continuous on ft jt 0g <ref> [Robinson, 1981] </ref>, it follows that (15) holds in the limit for t = 0 and hence (9) holds for t 2 ft j g # 0 as well as for t = 0 with the vectors x (0) and x 0 (0) of (9) given by: x (0) = (*I +
Reference: [Schaffer, 1993] <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178. </pages>
Reference-contexts: The conservation of generalization theorem <ref> [Schaffer, 1993, Schaffer, 1994, Wolpert, 1992a, Wolpert, 1992b] </ref> (a.k.a., "no free lunch theorem") states that all learning algorithms are created equal. In the context of classification, this means that the generalization performance 11 of any learning system, when averaged across all possible learning situations, is 0.5.
Reference: [Schaffer, 1994] <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The conservation of generalization theorem <ref> [Schaffer, 1993, Schaffer, 1994, Wolpert, 1992a, Wolpert, 1992b] </ref> (a.k.a., "no free lunch theorem") states that all learning algorithms are created equal. In the context of classification, this means that the generalization performance 11 of any learning system, when averaged across all possible learning situations, is 0.5.
Reference: [Stone, 1974] <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predic-tions. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: In accordance with the theoretical results, m is the number of training examples and n is the dimensionality of the problem. was evaluated on a testing set. Error rates and standard deviations of the errors were estimated using tenfold cross-validation <ref> [Stone, 1974] </ref> and are shown in Table 4.2. In nine of the twelve cases we examined, tolerant training improved the estimated error rate, often dramatically. The standard deviation of the observed error was also lowered in most cases.
Reference: [Tishby et al., 1989] <author> Tishby, N., Solla, S., and Levin, E. </author> <year> (1989). </year> <title> Consistent inference of probabilities in layered networks: Predictions and generalization. </title> <booktitle> In IJCNN International Joint Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 403-409, </pages> <address> New York. </address> <publisher> IEEE. </publisher>
Reference-contexts: Mathematical models of generalization [Vapnik, 1995, Wolpert, 1995] have grown from several diverse fields: for instance, statistics [Geman et al., 1992, Wahba, 1990], computational complexity [Valiant, 1984], and statistical physics <ref> [Tishby et al., 1989] </ref>. We believe that the methods and tools of mathematical optimization are an effective way to explore the theory of generalization, which in turn will lead to more effective applications of supervised learning.
Reference: [Valiant, 1984] <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference-contexts: Mathematical models of generalization [Vapnik, 1995, Wolpert, 1995] have grown from several diverse fields: for instance, statistics [Geman et al., 1992, Wahba, 1990], computational complexity <ref> [Valiant, 1984] </ref>, and statistical physics [Tishby et al., 1989]. We believe that the methods and tools of mathematical optimization are an effective way to explore the theory of generalization, which in turn will lead to more effective applications of supervised learning.
Reference: [Vapnik, 1995] <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Indeed, our theoretical and computational results indicate precisely that this is true. In order to better understand the circumstances in which tolerant training is useful, we view our theoretical results as a framework for generalization based upon the foundation of mathematical programming. Mathematical models of generalization <ref> [Vapnik, 1995, Wolpert, 1995] </ref> have grown from several diverse fields: for instance, statistics [Geman et al., 1992, Wahba, 1990], computational complexity [Valiant, 1984], and statistical physics [Tishby et al., 1989].
Reference: [Wahba, 1990] <author> Wahba, G. </author> <year> (1990). </year> <title> Spline Models for Observational Data. </title> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference-contexts: In order to better understand the circumstances in which tolerant training is useful, we view our theoretical results as a framework for generalization based upon the foundation of mathematical programming. Mathematical models of generalization [Vapnik, 1995, Wolpert, 1995] have grown from several diverse fields: for instance, statistics <ref> [Geman et al., 1992, Wahba, 1990] </ref>, computational complexity [Valiant, 1984], and statistical physics [Tishby et al., 1989]. We believe that the methods and tools of mathematical optimization are an effective way to explore the theory of generalization, which in turn will lead to more effective applications of supervised learning. <p> Other methods for overtraining avoidance in pattern recognition, learning and regression include node pruning in decision trees [Breiman et al., 1984, Quinlan, 1993a], k-nearest-neighbors [Hart, 1967], smoothing splines <ref> [Wahba, 1990] </ref>, and feature selection [Draper and Smith, 1966]. The true test of tolerant training, and other overtraining avoidance techniques, is whether it improves generalization on a wide variety of common learning problems, particularly those in which overtraining is likely to occur.
Reference: [Western, 1996] <author> Western, B. </author> <year> (1996). </year> <title> Vague theory and model uncertainty in macrosociology. Sociological Methodology, </title> <note> to appear. </note>
Reference-contexts: 2.944 1.873 [Quinlan, 1993b] t &gt; 0 1.012 0.344 2.934 1.872 Pollution (m = 60, n = 15) t = 0 62.75 32.55 5447 8250 [McDonald and Schwing, 1973] t &gt; 0 45.93 19.15 3661 3118 Strikes (m = 35, n = 4) t = 0 191.9 117.7 55035 63589 <ref> [Western, 1996] </ref> t &gt; 0 192.2 130.9 51487 63264 Ships (m = 34, n = 3) t = 0 9.794 8.509 152.3 256.3 [McCullagh and Nelder, 1989] t &gt; 0 8.306 5.728 128.3 220.4 Housing (m = 50, n = 13) t = 0 2.781 1.859 43.10 76.36 [Harrison and Rubinfeld,
Reference: [Wolberg et al., 1994] <author> Wolberg, W. H., Street, W. N., and Mangasarian, O. L. </author> <year> (1994). </year> <title> Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates. </title> <journal> Cancer Letters, </journal> <volume> 77 </volume> <pages> 163-171. </pages>
Reference: [Wolpert, 1992a] <author> Wolpert, D. H. </author> <year> (1992a). </year> <title> On overfitting as bias. </title> <type> Technical Report TR 92-03-5001, </type> <institution> The Santa Fe Institute. </institution>
Reference-contexts: The conservation of generalization theorem <ref> [Schaffer, 1993, Schaffer, 1994, Wolpert, 1992a, Wolpert, 1992b] </ref> (a.k.a., "no free lunch theorem") states that all learning algorithms are created equal. In the context of classification, this means that the generalization performance 11 of any learning system, when averaged across all possible learning situations, is 0.5.
Reference: [Wolpert, 1992b] <author> Wolpert, D. H. </author> <year> (1992b). </year> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 47-94. </pages>
Reference-contexts: The conservation of generalization theorem <ref> [Schaffer, 1993, Schaffer, 1994, Wolpert, 1992a, Wolpert, 1992b] </ref> (a.k.a., "no free lunch theorem") states that all learning algorithms are created equal. In the context of classification, this means that the generalization performance 11 of any learning system, when averaged across all possible learning situations, is 0.5.
Reference: [Wolpert, 1995] <author> Wolpert, D. H., </author> <title> editor (1995). The Mathematics of Generalization, </title> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher> <pages> 22 </pages>
Reference-contexts: Indeed, our theoretical and computational results indicate precisely that this is true. In order to better understand the circumstances in which tolerant training is useful, we view our theoretical results as a framework for generalization based upon the foundation of mathematical programming. Mathematical models of generalization <ref> [Vapnik, 1995, Wolpert, 1995] </ref> have grown from several diverse fields: for instance, statistics [Geman et al., 1992, Wahba, 1990], computational complexity [Valiant, 1984], and statistical physics [Tishby et al., 1989].
References-found: 29

