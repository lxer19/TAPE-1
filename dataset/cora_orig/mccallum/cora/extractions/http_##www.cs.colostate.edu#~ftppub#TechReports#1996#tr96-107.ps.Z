URL: http://www.cs.colostate.edu/~ftppub/TechReports/1996/tr96-107.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Recognition  
Author: Mark R. Stevens and J. Ross Beveridge 
Keyword: Interleaving 3D Model Feature Prediction and  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Date: December 16, 1995  
Note: Matching to Support Multi-Sensor Object  This work was sponsored by the Advanced Research Projects Agency (ARPA) under grant DAAH04-93-G-422, monitored by the U.S. Army Research Office.  
Pubnum: Technical Report CS-96-107  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [BDHR94] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Ed-ward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This on-line capability permits us to develop a tight coupling between feature prediction and matching: modifying the features expected to be visible as matching progresses. The algorithms presented here are being developed to perform final verification within a larger Automatic Target Recognition (ATR) system [BHP95]. Thus, upstream color-detection <ref> [BDHR94] </ref> and range boundary matching algorithms [Bev92] provide hypotheses indicating a specific target is at roughly the following position and orientation relative to the sensor platform.
Reference: [Bev92] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The algorithms presented here are being developed to perform final verification within a larger Automatic Target Recognition (ATR) system [BHP95]. Thus, upstream color-detection [BDHR94] and range boundary matching algorithms <ref> [Bev92] </ref> provide hypotheses indicating a specific target is at roughly the following position and orientation relative to the sensor platform. Consequently, the primary aim of the matching algorithm presented here is to reliably refine the pose estimate and match between object model and sensor features. <p> The other shot, Shot 35 from Vehicle Array 9, shows the same M113 APC side-on at approximately 100m with its nose point slightly down relative to the rear of the vehicle. For each shot, matching is initialized using a coregis-tration estimate provided by a range template matching algorithm <ref> [Bev92] </ref>. The estimate provides both an orientation and a translation estimate for the vehicle relative to the two sensors. The template matching algorithm ranks the set of alternative estimates using a confidence factor. These initial hypotheses are needed in order to provide a starting point for our matching algorithm.
Reference: [Bev93] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The parameter ff introduces a non-linear bias which essentially reduces the penalty of small amounts of omission while increasing the penalty for large amounts of omission. A detailed explanation of this relationship may be found in <ref> [Bev93] </ref>. For the optical omission error, E om;o (F ), w is the number of unmatched lines over the total number of lines. For the range data, omission is measured in both directions: model-to-data and data-to-model.
Reference: [BHP94] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University 7 and Alliant Techsystems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The process of determining the error begins by projecting the predicted 3D model edges into the color image. Projection is possible because both the intrinsic sensor parameters and the approximate location of the target are known. The parameters for the color sensor have been determined off-line using calibration targets <ref> [BHP94] </ref>. After each edge is projected, a gradient mask tuned to the precise expected orientation is applied to the pixels lying under each line. The gradient response, ^ G Line (k), is normalized to the range [0; 1]. The derivation of the response is presented in [Mar96].
Reference: [BHP95] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Model-based fusion of flir, </title> <editor> color and ladar. In Paul S. Schenker and Gerard T. McKee, editors, </editor> <booktitle> Proceedings: Sensor Fusion and Networked Robotics VIII, Proc. SPIE 2589, </booktitle> <pages> pages 2 - 11, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: This on-line capability permits us to develop a tight coupling between feature prediction and matching: modifying the features expected to be visible as matching progresses. The algorithms presented here are being developed to perform final verification within a larger Automatic Target Recognition (ATR) system <ref> [BHP95] </ref>. Thus, upstream color-detection [BDHR94] and range boundary matching algorithms [Bev92] provide hypotheses indicating a specific target is at roughly the following position and orientation relative to the sensor platform.
Reference: [BPY94] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Which of the many possible internal features to select is based upon a simple lighting model. To test our algorithms on real data, we have range, color and IR imagery which we and Martin Mari-etta collected at Fort Carson, Colorado in November 1993 <ref> [BPY94] </ref>. The data contains many different image triplets, out of which two pairs of range and color images are used for demonstration here. The first image set, Figure 2, is a simple proof-of-concept image in which the vehicle is roughly 50m away in a fairly open area. <p> over the total number of model points, and q is the number of unmatched data points over the total number of data points. 4 Results Initial testing of the combined feature prediction and matching algorithms is being done on pairs of range and optical imagery from the Fort Carson dataset <ref> [BPY94] </ref>. Results for two shots with varying level of difficulty are presented. A shot is defined as a pair of approximately registered range and optical images. The first shot, Shot 20 from Vehicle Array 5, contains an M113 APC sitting in an open field approximately 50 meters from the sensor.
Reference: [BSS96] <author> J. Ross Beveridge, Mark R. Stevens, and N. A. Schwick-erath. </author> <title> Toward target verification through 3-d model-based sensor fusion. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page (Submitted), </note> <year> 1996. </year>
Reference-contexts: From these simpler models, features to be used in the matching process are then obtained. Currently, we have models for an M113 APC and an M60. This paper deals only with the M113, but work has been done matching the M60 <ref> [BSS96] </ref>. 2.1 Predicting 3D Line Segments The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery [Mar77, Koe84].
Reference: [CA87] <author> C. H. Chien and J. K. Aggarwal. </author> <title> Shape recognition from single silhouettes. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 481-490, </pages> <year> 1987. </year>
Reference-contexts: Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes [WW80, LT90, WMA84], and while work using the 3D edges directly is rare <ref> [CA87] </ref> it is usually concerned with linking 2D image features to 3D model features. Our method approaches the problem from the other direction: we are tying the 3D model edges to the 2D image data.
Reference: [CS94] <author> Jin-Long Chen and George C. Stockman. </author> <title> Determining pose of 3d objects with curved surfaces. </title> <type> Technical Report CPS-93-40, </type> <institution> Michigan State University, </institution> <year> 1994. </year>
Reference-contexts: Our early experiments using only silhouette lines in our domain suggest there is too much ambiguity for the silhouette to adequately constrain the match. Since others have observed improved performance when internal edge structure is added <ref> [CSR93, CS94] </ref>, our feature prediction utilizes simple radiometric and temporal context information in order to predict the internal structure likely to be visible in the optical imagery. 2.1.1 Silhouette Lines To determine which parts of the CAD model produce the silhouette, a unique color is first assigned to each existing face.
Reference: [CSR93] <author> Jin-Long Chen, George C. Stockman, and Kashi Rao. </author> <title> Recovering and tracking pose of curved 3d objects from 2d images. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition, </booktitle> <pages> pages 233-239, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Our early experiments using only silhouette lines in our domain suggest there is too much ambiguity for the silhouette to adequately constrain the match. Since others have observed improved performance when internal edge structure is added <ref> [CSR93, CS94] </ref>, our feature prediction utilizes simple radiometric and temporal context information in order to predict the internal structure likely to be visible in the optical imagery. 2.1.1 Silhouette Lines To determine which parts of the CAD model produce the silhouette, a unique color is first assigned to each existing face.
Reference: [GBSF94] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491-494. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: The maximum range of depth values is approximately 300m. The images show the range data rendered as a set of 3D hollow polygons 6 a. E M b. E M;r (F ) c. E M;o (F ) <ref> [GBSF94, GBSF95] </ref>, from a viewpoint slightly above and to the left of the sensor position. The actual LADAR data is shown in light grey, and the predicted sampled surface features are shown in black 1 .
Reference: [GBSF95] <author> Michael E. Goss, J. Ross Beveridge, Mark R. Stevens, and Aaron D. Fuegi. </author> <title> Three-dimensional visualization environment for multi-sensor data analysis, interpretation, and model-based object recognition. </title> <editor> In Georges G. Grin-stein and Robert F. Erbacher, editors, </editor> <booktitle> Proceedings: Visual Data Exploration and Analysis II, </booktitle> <pages> pages 283-291. </pages> <booktitle> SPIE Vol. </booktitle> <volume> 2410, </volume> <month> feb </month> <year> 1995. </year>
Reference-contexts: The maximum range of depth values is approximately 300m. The images show the range data rendered as a set of 3D hollow polygons 6 a. E M b. E M;r (F ) c. E M;o (F ) <ref> [GBSF94, GBSF95] </ref>, from a viewpoint slightly above and to the left of the sensor position. The actual LADAR data is shown in light grey, and the predicted sampled surface features are shown in black 1 .
Reference: [HH94] <author> Anthony Hoogs and Douglas Hackett. </author> <title> Model-supported exploitation as a framework for image understanding. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 265-268. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: Our method approaches the problem from the other direction: we are tying the 3D model edges to the 2D image data. Hoogs has noted that there exist several forms of contextual information which can be exploited when tackling computer vision problems: geometric, temporal, functional, radiometric, and image context <ref> [HH94] </ref>. In particular, he has developed a statistical framework for estimating the probability that a given edge will be distinctive enough to be found in the sensor imagery.
Reference: [Ike87] <author> Katsushi Ikeuchi. </author> <title> Precompiling a geometrical model into an interpretation tree for object recognition in bin-picking tasks. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 321-330, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Army Research Office. y Appears also in the Proceedings of the 1996 ARPA Image Understanding Workshop. Common approaches to model feature prediction have focused upon developing off-line data structures which capture feature visibility information associated with geometry alone <ref> [PD87, Pla88, Ike87] </ref>. This process usually begins with the division of all possible viewpoints into sets of constant model topology [KvD76, KvD79]. From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching.
Reference: [J. 96] <author> J. Ross Beveridge and Bruce A. Draper and Kris Siejko. </author> <title> Progress on Target and Terrain Recognition Research at Colorado State University. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A detailed justification of this particular parameterization appears in <ref> [J. 96] </ref>. Two different matching strategies emerge depending on whether the search is conducted in the space of correspondences, C, or the space of coregistration parameters: F 2 &lt; 8 .
Reference: [KD87] <author> Matthew R. Korn and Charles R. Dyer. </author> <title> 3D Multiview Object Representations for Model-Based Object Recognition. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 91-103, </pages> <year> 1987. </year>
Reference-contexts: This process usually begins with the division of all possible viewpoints into sets of constant model topology [KvD76, KvD79]. From these regions, silhouette and other model features can be determined and stored <ref> [SD92, KD87] </ref> for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting [PHK91, WI93].
Reference: [Koe84] <author> J.J. Koenderink. </author> <title> What does occluding contour tell us about solid shape? Perception, </title> <booktitle> 13 </booktitle> <pages> 321-330, </pages> <year> 1984. </year>
Reference-contexts: Currently, we have models for an M113 APC and an M60. This paper deals only with the M113, but work has been done matching the M60 [BSS96]. 2.1 Predicting 3D Line Segments The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery <ref> [Mar77, Koe84] </ref>. Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes [WW80, LT90, WMA84], and while work using the 3D edges directly is rare [CA87] it is usually concerned with linking 2D image features to 3D model features.
Reference: [KvD76] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Singularities of Visual Mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 24 </volume> <pages> 51-59, </pages> <year> 1976. </year>
Reference-contexts: Common approaches to model feature prediction have focused upon developing off-line data structures which capture feature visibility information associated with geometry alone [PD87, Pla88, Ike87]. This process usually begins with the division of all possible viewpoints into sets of constant model topology <ref> [KvD76, KvD79] </ref>. From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting [PHK91, WI93].
Reference: [KvD79] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Internal Representation of Shape with Respect to Vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Common approaches to model feature prediction have focused upon developing off-line data structures which capture feature visibility information associated with geometry alone [PD87, Pla88, Ike87]. This process usually begins with the division of all possible viewpoints into sets of constant model topology <ref> [KvD76, KvD79] </ref>. From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting [PHK91, WI93].
Reference: [LT90] <author> Cheng-Hsiung Liu and We-Hsiang Tsai. </author> <title> 3d curved object recognition from multiple 2d camera views. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 50 </volume> <pages> 177-187, </pages> <year> 1990. </year>
Reference-contexts: Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes <ref> [WW80, LT90, WMA84] </ref>, and while work using the 3D edges directly is rare [CA87] it is usually concerned with linking 2D image features to 3D model features. Our method approaches the problem from the other direction: we are tying the 3D model edges to the 2D image data.
Reference: [Mar77] <author> David Marr. </author> <title> Analysis of occluding contour. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B197:441-475, </volume> <year> 1977. </year>
Reference-contexts: Currently, we have models for an M113 APC and an M60. This paper deals only with the M113, but work has been done matching the M60 [BSS96]. 2.1 Predicting 3D Line Segments The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery <ref> [Mar77, Koe84] </ref>. Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes [WW80, LT90, WMA84], and while work using the 3D edges directly is rare [CA87] it is usually concerned with linking 2D image features to 3D model features.
Reference: [Mar96] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: After each edge is projected, a gradient mask tuned to the precise expected orientation is applied to the pixels lying under each line. The gradient response, ^ G Line (k), is normalized to the range [0; 1]. The derivation of the response is presented in <ref> [Mar96] </ref>.
Reference: [PD87] <author> Harry Platinga and Charles Dyer. </author> <title> Visibility, Occlusion, and the Aspect Graph. </title> <type> Technical Report 736, </type> <institution> University of Wisconsin - Madison, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Army Research Office. y Appears also in the Proceedings of the 1996 ARPA Image Understanding Workshop. Common approaches to model feature prediction have focused upon developing off-line data structures which capture feature visibility information associated with geometry alone <ref> [PD87, Pla88, Ike87] </ref>. This process usually begins with the division of all possible viewpoints into sets of constant model topology [KvD76, KvD79]. From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching.
Reference: [PHK91] <author> J. Ponce, A. Hoogs, </author> <title> and D.J. Kriegman. On using cad models to compute the pose of curved 3d objects. </title> <journal> DACBV, </journal> <volume> 91 </volume> <pages> 136-145, </pages> <year> 1991. </year>
Reference-contexts: From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting <ref> [PHK91, WI93] </ref>. In contrast to much of this work, we are promoting an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process.
Reference: [Pla88] <author> William Harry Plantinga. </author> <title> The ASP: A Continuous, Viewer-Centered Object Representation for Computer Vision. </title> <type> PhD thesis, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1988. </year>
Reference-contexts: Army Research Office. y Appears also in the Proceedings of the 1996 ARPA Image Understanding Workshop. Common approaches to model feature prediction have focused upon developing off-line data structures which capture feature visibility information associated with geometry alone <ref> [PD87, Pla88, Ike87] </ref>. This process usually begins with the division of all possible viewpoints into sets of constant model topology [KvD76, KvD79]. From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching.
Reference: [PP76] <author> G.W. Paltridge and C.M.R Platt. </author> <title> Radiative Processes in Meteorology and Climatology. </title> <publisher> Elsevier Scientific Publishing Company, </publisher> <year> 1976. </year>
Reference-contexts: The sun is modelled as an area light source, and the vector to the sun is calculated using a long/lat estimate, time of day, date, and compass orientation <ref> [PP76] </ref>. All of this information is available for our current data set. Once the vector is determined, it provides the direction to the sun for the entire scene, and can be used to predict the internal model edges.
Reference: [SB94] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Searching in correspondence space for a c fl which minimizes equation 1, an optimal coregistration estimate for a given correspondence, c, may be determined using a non-linear least-squares procedure <ref> [SB94] </ref>. Searching in coregistration space, given a coregistration estimate, F, it is possible to determine a best choice of corresponding features c fl . A local assignment procedure based upon proximity of features under transformation, F , is used to determine c fl .
Reference: [SBG95] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In BRL/CAD Symposium 95, </booktitle> <year> 1995. </year>
Reference-contexts: Highly detailed models of the vehicles in our Fort Car-son dataset exist in the CAD model format known as BRL/CAD [U. 91]. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [SBG95, Ste95] </ref>. From these simpler models, features to be used in the matching process are then obtained. Currently, we have models for an M113 APC and an M60.
Reference: [SD92] <author> W. Brent Seales and Charles R. Dyer. </author> <title> Modeling the Rim Appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year>
Reference-contexts: This process usually begins with the division of all possible viewpoints into sets of constant model topology [KvD76, KvD79]. From these regions, silhouette and other model features can be determined and stored <ref> [SD92, KD87] </ref> for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting [PHK91, WI93]. <p> Thus, if the background color appears in a pixel's eight-connected neighborhood, the associated face lies on the silhouette. Subsequent search determines which specific face boundaries (edges) generate the silhouette. An edge is a possible silhouette edge only if one of the two bounding faces is visible <ref> [SD92] </ref>. This step may leave some edges which are actually internal as hypothesized silhouette edges, and it also does not deal with self-occlusion. A clipping algorithm is then used to discover and discard those edges and portions of edges which are not part of the silhouette.
Reference: [Ste95] <author> Mark R. Stevens. </author> <title> Obtaining 3D Silhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State University, </institution> <month> Septem-ber </month> <year> 1995. </year>
Reference-contexts: Highly detailed models of the vehicles in our Fort Car-son dataset exist in the CAD model format known as BRL/CAD [U. 91]. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [SBG95, Ste95] </ref>. From these simpler models, features to be used in the matching process are then obtained. Currently, we have models for an M113 APC and an M60.
Reference: [SWF95] <author> G.D Sullivan, A.D. Worrall, and J.M Ferryman. </author> <title> Visual Object Recognition Using Deformable Models of Vehicles. </title> <booktitle> In Workshop on Context-Based Vision, </booktitle> <pages> pages 75-86, </pages> <month> june </month> <year> 1995. </year>
Reference-contexts: This measurement takes into account both range and optical features, but treats the two cases somewhat differently. For the optical features, the error is a function of the gradient response to a tuned filter for each line segment <ref> [SWF95] </ref>. For range, the error measure is a function of the Euclidean distance from points on the predicted model sampled surface to their nearest neighbor in the range image data. The local search itself samples each of the 8 dimensions of the coregistration space about the current estimate.
Reference: [U. 91] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: The second image set, Figure 3, is considerably more difficulty in that the vehicle is approximately 100m away on a hillside. Highly detailed models of the vehicles in our Fort Car-son dataset exist in the CAD model format known as BRL/CAD <ref> [U. 91] </ref>. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed [SBG95, Ste95]. From these simpler models, features to be used in the matching process are then obtained. Currently, we have models for an M113 APC and an M60.
Reference: [WI93] <author> M.D. Wheeler and K. </author> <title> Ikeuchi. Sensor modeling, markov random fields, and robust localization for recognizing partially occluded objects. </title> <journal> IUW, </journal> <volume> 93 </volume> <pages> 811-818, </pages> <year> 1993. </year>
Reference-contexts: From these regions, silhouette and other model features can be determined and stored [SD92, KD87] for later retrieval during matching. Finally, some promising recent work has used statistical modeling to predict feature visibility based upon both geometry and lighting <ref> [PHK91, WI93] </ref>. In contrast to much of this work, we are promoting an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process.
Reference: [WMA84] <author> Y. F. Wang, M. J. Magee, and J. K. Aggarwal. </author> <title> Matching three-dimensional objects using silhouettes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 513-518, </pages> <year> 1984. </year>
Reference-contexts: Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes <ref> [WW80, LT90, WMA84] </ref>, and while work using the 3D edges directly is rare [CA87] it is usually concerned with linking 2D image features to 3D model features. Our method approaches the problem from the other direction: we are tying the 3D model edges to the 2D image data.
Reference: [WW80] <author> T.P. Wallace and P.A. Wintz. </author> <title> An efficient three-dimensional aircraft recognition algorithm using normalized fourier descriptors. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 13 </volume> <pages> 99-126, </pages> <year> 1980. </year> <month> 8 </month>
Reference-contexts: Many systems have been developed to recognize 3D objects based on their projected 2D silhouettes <ref> [WW80, LT90, WMA84] </ref>, and while work using the 3D edges directly is rare [CA87] it is usually concerned with linking 2D image features to 3D model features. Our method approaches the problem from the other direction: we are tying the 3D model edges to the 2D image data.
References-found: 35

