URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96718-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: ALGORITHMIC REDISTRIBUTION METHODS FOR BLOCK CYCLIC DECOMPOSITIONS  
Author: Antoine Petitet 
Degree: A Dissertation Presented for the Doctor of Philosophy Degree  
Date: December 1996  
Affiliation: The University of Tennessee, Knoxville  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 160 Bibliography </institution>
Reference: [1] <author> M. Aboelaze, N. Chrisochoides, and E. Houstis. </author> <title> The Parallelization of Level 2 and 3 BLAS Operations on Distributed Memory Machines. </title> <type> Technical Report CSD-TR-91-007, </type> <institution> Purdue University, West Lafayette, IN, </institution> <year> 1991. </year>
Reference-contexts: Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project [16, 39] as well as others <ref> [1, 13, 26, 41] </ref>. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy.
Reference: [2] <author> R. Agarwal, F. Gustavson, and M. Zubair. </author> <title> A High Performance Matrix Multiplication Algorithm on a Distributed-Memory Parallel Computer, Using Overlapped Communication. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(6) </volume> <pages> 673-681, </pages> <year> 1994. </year>
Reference-contexts: These source processes change at every iteration of the loop. High performance and efficiency can still be achieved for a wide range of different values of the blocking factors. This has been reported in <ref> [2, 38, 74, 83] </ref>. The use of physical blocking in conjunction with static blocking can lead to a comprehensive and scalable dense linear algebra software library. Existing serial software such as LAPACK [5] can be reused. The ScaLAPACK software library is the result of this reasoning. <p> For a scalable algorithm with n=p held fixed, we expect the performance to be proportional to p. 4.4.1 Physical Blocking In this section, the performance analysis of the physical blocking strategy for aligned operands is presented. A similar analysis can also be found in <ref> [2, 83] </ref>. The reason for reproducing it hereafter is that it considerably simplifies the presentation of the performance analysis for the aggregation and LCM blocking strategies. For the sake of simplicity, the underlying process grid is assumed to be a p p p square mesh of p processes.
Reference: [3] <author> R. Agarwal, F. Gustavson, and M. Zubair. </author> <title> Improving Performance of Linear Algebra Algorithms for Dense Matrices Using Algorithmic Prefetching. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(3) </volume> <pages> 265-275, </pages> <year> 1994. </year>
Reference-contexts: Still, the correctness of these operations and the robustness and reliability of their implementation depend entirely on the material presented in this chapter. 57 Chapter 3 Algorithmic Redistribution 3.1 Introduction In a serial computational environment, transportable efficiency is the essential motivation for developing blocking strategies and block-partitioned algorithms <ref> [3, 5, 33, 60] </ref>. The linear algebra package (LAPACK) [5] is the archetype of such a demarche. The LAPACK software is constructed as much as possible out of calls to the BLAS (Basic Linear Algebra Subprograms).
Reference: [4] <author> T. Agerwala, J. Martin, J. Mirza, D. Sadler, D. Dias, and M. Snir. </author> <title> SP2 System Architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 153-184, </pages> <year> 1995. </year>
Reference-contexts: The clock speed of this processor is 66.7 MHz, giving a peak performance per processor of 266 MFLOPS. There are two types of nodes, known as thin nodes and wide nodes. Thin nodes have a 64 KB data cache. Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. <p> Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. Figure 4.4 shows the thin node performance of the vendor 99 supplied rank-K update library routine for distinct values of M = N and K. In practice, the performance of such an operation was observed to be at most 200 Mflops for those thin nodes. <p> The TB2 switch adapter, which is the interface between the node and the switch, features a Direct Memory Access (DMA) engine. For message passing libraries optimized for the switch, the typical bandwidth is 35 MB/s with a latency of approximately 50 s <ref> [4, 30, 80] </ref>. 100 During our experiments, the performance of the BLACS communication prim-itives implemented on top of the native IBM message passing library (MPL) was measured.
Reference: [5] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du 161 Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide, Second Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1995. </year>
Reference-contexts: The well-known BLAS [36, 35] and LAPACK <ref> [5] </ref> numerical linear algebra software libraries are typical examples of such useful and successful software packages for shared-memory vector and parallel processors. The programming languages used to encode VisiCalc and LAPACK respectively, have ultimately been essential building tools for the existence and success of these software packages. <p> Still, the correctness of these operations and the robustness and reliability of their implementation depend entirely on the material presented in this chapter. 57 Chapter 3 Algorithmic Redistribution 3.1 Introduction In a serial computational environment, transportable efficiency is the essential motivation for developing blocking strategies and block-partitioned algorithms <ref> [3, 5, 33, 60] </ref>. The linear algebra package (LAPACK) [5] is the archetype of such a demarche. The LAPACK software is constructed as much as possible out of calls to the BLAS (Basic Linear Algebra Subprograms). <p> The linear algebra package (LAPACK) <ref> [5] </ref> is the archetype of such a demarche. The LAPACK software is constructed as much as possible out of calls to the BLAS (Basic Linear Algebra Subprograms). These kernels confine the impact of the machine architecture differences within a small number of routines. <p> On RISC microprocessors, however, their performance is limited by the memory access bandwidth bottleneck. The greatest scope for exploiting the highest levels of the memory hierarchy as well as other forms of parallelism is offered by the Level 3 BLAS <ref> [5] </ref>. The previous reasoning applies to distributed memory computational environments in two ways. First, in order to achieve overall high performance, it is necessary to express the bulk of the computation local to each process in terms of Level 3 BLAS operations. <p> This has been reported in [2, 38, 74, 83]. The use of physical blocking in conjunction with static blocking can lead to a comprehensive and scalable dense linear algebra software library. Existing serial software such as LAPACK <ref> [5] </ref> can be reused. The ScaLAPACK software library is the result of this reasoning. As suggested above, if one limits oneself to static 80 and physical blocking, strong alignment restrictions must be met by the matrix operands. <p> From a software portability point of view, one can store these values in a table. At run-time, these values will be retrieved from this table. This is the option that has been selected by the LAPACK <ref> [5] </ref> designers. It is however conceivable to determine such values at run-time by performing a few quick experiments. On a distributed memory concurrent computer, such a method is particularly attractive because the overhead of such trials is in general negligible. <p> First, the linear operator is factorized into a product of two or more matrices featuring suitable properties for the resolution 148 of the problem. Second, the solution of the problem is obtained by solving sim-pler matrix equalities typically involving triangular and/or orthogonal matrices <ref> [5, 49] </ref>. This same framework forms the basis of modern algorithms solving algebraic eigenvalue problems. The matrix representing the linear operator is first reduced to a condensed form. The numerical solution is then obtained by applying an iterative method to this condensed form [5, 49]. <p> equalities typically involving triangular and/or orthogonal matrices <ref> [5, 49] </ref>. This same framework forms the basis of modern algorithms solving algebraic eigenvalue problems. The matrix representing the linear operator is first reduced to a condensed form. The numerical solution is then obtained by applying an iterative method to this condensed form [5, 49]. Block-partitioned algorithms have been developed for most of the matrix factorizations and reductions. These algorithms have been implemented in the LAPACK software library [5] for shared memory systems. The bulk of computation in these algorithms is performed on the matrix representation of the linear operator. <p> The numerical solution is then obtained by applying an iterative method to this condensed form [5, 49]. Block-partitioned algorithms have been developed for most of the matrix factorizations and reductions. These algorithms have been implemented in the LAPACK software library <ref> [5] </ref> for shared memory systems. The bulk of computation in these algorithms is performed on the matrix representation of the linear operator. When such a matrix is distributed onto a process grid according to the block cyclic scheme, the operands of the elementary rank-k updates feature natural alignment characteristics.
Reference: [6] <author> I. Angus, G. Fox, J. Kim, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Software for Concurrent Processors, volume 2. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1990. </year>
Reference-contexts: This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution <ref> [6] </ref>. Second, the target architecture is such that all processes can be treated equally in terms of local performance, and, the communication rate between two processes is independent of the processes considered. Efficiency is the primary consideration justifying any restriction or requirement that an implementation may have.
Reference: [7] <author> C. Ashcraft. </author> <title> The Distributed Solution of Linear Systems Using the Torus-wrap Data mapping. </title> <type> Technical Report ECA-TR-147, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1990. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research <ref> [7, 12, 55] </ref> has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research [21, 46, 48, 55] has aimed at studying different data decompositions <ref> [7, 12, 57] </ref>. As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability [38], load balance and communication [54] properties.
Reference: [8] <author> M. Baber. </author> <title> Hypertasking Support for Dynamically Redistributable and Re-sizeable Arrays on the iPSC. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> pages 59-66, </pages> <year> 1991. </year>
Reference-contexts: Most of the Fortran- and C-based data parallel languages incorporate explicit and implicit redistribution capabilities. The Kali language [75] was one of the first to do so. DINO [79] addresses the implicit redistribution of data at procedure boundaries. The Hypertasking compiler <ref> [8] </ref> for data parallel C programs, Vienna Fortran [14] and Fortran D [50] additionally specify data redistribution primitives. For efficiency purposes as well as simplicity of the compiler, these redistribution operations are often implemented in a library of intrinsics [61].
Reference: [9] <author> R. Bisseling and J. van der Vorst. </author> <title> Parallel LU Decomposition on a Transputer Network. </title> <editor> In G. van Zee and J. van der Vorst, editors, </editor> <booktitle> Lecture Notes in Computer Sciences, </booktitle> <volume> volume 384, </volume> <pages> pages 61-77. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: This constructive definition is more general than the one used in this dissertation. It encompasses the entire family of Cartesian mappings <ref> [9] </ref>. The algorithmic redistributed operations described later in this dissertation can be expressed in terms of locating diagonals of a distributed matrix. The next chapters also illustrate the fundamental role played by LCM tables and the properties presented above in the formulation of these operations.
Reference: [10] <author> R. Bisseling and J. van der Vorst. </author> <title> Parallel Triangular System Solving on a mesh network of Transputers. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 </volume> <pages> 787-799, </pages> <year> 1991. </year> <month> 162 </month>
Reference: [11] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, and B. Mohr. </author> <title> Implementing a Parallel C++ Runtime System for Scalable Parallel Systems. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 588-597, </pages> <year> 1993. </year>
Reference-contexts: Today, the first HPF compilers are slowly becoming available on a wide range of machines, and it is unclear yet if those compilers will fulfill their highly difficult goals in the near future. C-based data parallel extensions have also been proposed, such as Data Parallel C [51] and pC++ <ref> [11] </ref>.
Reference: [12] <author> R. Brent. </author> <title> The LINPACK Benchmark on the AP 1000. </title> <booktitle> In Frontiers, </booktitle> <year> 1992, </year> <pages> pages 128-135, </pages> <address> McLean, VA, </address> <year> 1992. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research <ref> [7, 12, 55] </ref> has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research [21, 46, 48, 55] has aimed at studying different data decompositions <ref> [7, 12, 57] </ref>. As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability [38], load balance and communication [54] properties. <p> With these assumptions, subject to a renumbering of the processes, a D-block is just a diagonal entry and conversely. Since the above properties completely characterize the D-blocks and the D-processes, the problem of interest is solved. These assumptions have been made by some researchers <ref> [12, 13, 54] </ref> to implement the LINPACK benchmark and related dense linear algebra kernels on distributed vector computers. These are also specifications data parallel languages such as HPF are leaning towards. <p> The feasibil 81 ity and performance characteristics of this approach have been illustrated for the numerical resolution of a general linear system of equations and the symmetric eigenproblem in <ref> [12, 13, 53] </ref> for the purely scattered distribution as defined in (2.2.7). Similarly, it is sometimes beneficial to disaggregate a panel into multiple panels in order to overlap communication and computation phases. When applicable, this last strategy also presents the advantage of requiring a smaller amount of workspace.
Reference: [13] <author> R. Brent and P. Strazdins. </author> <title> Implementation of BLAS Level 3 and LINPACK Benchmark on the AP1000. </title> <journal> Fujitsu Scientific and Technical Journal, </journal> <volume> 5(1) </volume> <pages> 61-70, </pages> <year> 1993. </year>
Reference-contexts: With these assumptions, subject to a renumbering of the processes, a D-block is just a diagonal entry and conversely. Since the above properties completely characterize the D-blocks and the D-processes, the problem of interest is solved. These assumptions have been made by some researchers <ref> [12, 13, 54] </ref> to implement the LINPACK benchmark and related dense linear algebra kernels on distributed vector computers. These are also specifications data parallel languages such as HPF are leaning towards. <p> Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project [16, 39] as well as others <ref> [1, 13, 26, 41] </ref>. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy. <p> The feasibil 81 ity and performance characteristics of this approach have been illustrated for the numerical resolution of a general linear system of equations and the symmetric eigenproblem in <ref> [12, 13, 53] </ref> for the purely scattered distribution as defined in (2.2.7). Similarly, it is sometimes beneficial to disaggregate a panel into multiple panels in order to overlap communication and computation phases. When applicable, this last strategy also presents the advantage of requiring a smaller amount of workspace.
Reference: [14] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima. </author> <title> Dynamic Data Redistribution in Vienna Fortran. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 284-293, </pages> <year> 1993. </year>
Reference-contexts: Most of the Fortran- and C-based data parallel languages incorporate explicit and implicit redistribution capabilities. The Kali language [75] was one of the first to do so. DINO [79] addresses the implicit redistribution of data at procedure boundaries. The Hypertasking compiler [8] for data parallel C programs, Vienna Fortran <ref> [14] </ref> and Fortran D [50] additionally specify data redistribution primitives. For efficiency purposes as well as simplicity of the compiler, these redistribution operations are often implemented in a library of intrinsics [61]. Explicit and implicit data redistribution operations are necessary but not quite sufficient.
Reference: [15] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Tseng. </author> <title> Generating Local Adresses and Communication Sets for Data Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26 </volume> <pages> 72-84, </pages> <year> 1995. </year>
Reference-contexts: The approach of solving a set of linear Diophantine equations to determine index sets, process sets and so on is recommended by data parallel compiler designers as one way to proceed <ref> [15, 56, 64, 82] </ref>. Binary algorithms are available [65] to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements.
Reference: [16] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A Portable Linear Algebra Library for Distributed Memory Computers Design Issues and Performance. </title> <journal> Computer Physics Communications, </journal> <volume> 97 </volume> <pages> 1-15, </pages> <year> 1996. </year> <note> (also LA-PACK Working Note #95). 163 </note>
Reference-contexts: Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1. <p> Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project <ref> [16, 39] </ref> as well as others [1, 13, 26, 41]. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy. <p> It may also be necessary to copy back the meaningful part of this buffer when the triangular or symmetric distributed matrix is an output parameter. This strategy is exactly what is done in the current version of the ScaLAPACK software library <ref> [16, 25] </ref>. There is little evidence, however, demonstrating the superiority of this strategy over the second option. The latter avoids the unnecessary floating point operations as well as the data copy, and thus does not require any workspace to store these diagonal LCM blocks.
Reference: [17] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> Installation Guide for ScaLAPACK. </title> <type> Technical Report UT CS-95-280, </type> <note> LAPACK Working Note #93, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: This chapter presents and discusses the results obtained. All experiments were performed in double precision arithmetic, and the matrix operands were randomly generated. The vendor-supplied BLAS library was used on the Intel XP/S Paragon and the IBM SP. The current native version 1.0 of the BLACS <ref> [17] </ref> was used on both systems. The experimental programs were compiled and executed unchanged on both platforms. A testing program was developed for debugging purposes as well as ensuring the validity of the results. This program was adapted from the more general testing software accompanying 116 the PBLAS library [18].
Reference: [18] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Wha-ley. </author> <title> A Proposal for a Set of Parallel Basic Linear Algebra Subprograms. </title> <editor> In J. Dongarra, K. Masden, and J. Wasniewski, editors, </editor> <booktitle> Applied Parallel Computing, </booktitle> <pages> pages 107-114. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year> <note> (also LAPACK Working Note #100). </note>
Reference-contexts: : i + ib 1; :) A (i : i + ib 1; :) T ; A (i : i + ib 1; :) A (i + ib : N; :) T ; This pseudo-code suggests a global index interface for the LCM BLAS similar to the one described in <ref> [18] </ref> for a set of parallel BLAS. Finally, it is possible to reuse existing serial GEMM-based implementations of the Levels 2 and 3 BLAS [33, 60]. 75 3.4.2 Cyclic Ordering The cyclic ordering strategy is distinguished by the fact that the computations are cyclically distributed as opposed to the data. <p> The experimental programs were compiled and executed unchanged on both platforms. A testing program was developed for debugging purposes as well as ensuring the validity of the results. This program was adapted from the more general testing software accompanying 116 the PBLAS library <ref> [18] </ref>. The software passed statistically a large number of tests, that is, for a finite collection of random valid input arguments. Similarly, a timing program was developed and used to obtain the results presented below. Most of the experiments were performed twice or more. Only the best performance is reported. <p> These programs are complicated to write, debug and maintain. These facts have been considered when these experimental programs were designed. First, whenever possible, a "global interface" has been selected as used by the ScaLAPACK library and explained in <ref> [18] </ref>. If such an interface imposes some redundant index 154 computations, it allows for the reuse of sequential data and control structures that are easier to write and debug. Second, the properties shown in Chapter 2 were used to verify and assert the correctness of the experimental programs.
Reference: [19] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley. </author> <title> The Design and Implementation of the Reduction Routines in ScaLAPACK. </title> <editor> In J. J. Dongarra, L. Grandinetti, G. R. Joubert, and J. Kowalik, editors, </editor> <booktitle> High Performance Computing: Technology, Methods and Applications, Advances in Parallel Computing, </booktitle> <volume> 10, </volume> <pages> pages 177-202. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1995. </year>
Reference-contexts: There is considerable evidence that the square block cyclic mapping can lead to very efficient implementations of more complex matrix operations such as the LU, Cholesky or QR factorizations [20, 38] or even the reductions to Hessenberg, tridiagonal and bidiagonal forms <ref> [19, 24] </ref>. The encouraging performance results mentioned above were obtained for particular process grid shapes and empirically chosen distribution parameters on specific hardware platforms.
Reference: [20] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley. </author> <title> The Design and Implementation of the ScaLAPACK LU, QR, and Cholesky Factorization Routines. </title> <journal> Scientific Programming, </journal> <volume> 5 </volume> <pages> 173-184, </pages> <year> 1996. </year> <note> (also LA-PACK Working Note #80). 164 </note>
Reference-contexts: There is considerable evidence that the square block cyclic mapping can lead to very efficient implementations of more complex matrix operations such as the LU, Cholesky or QR factorizations <ref> [20, 38] </ref> or even the reductions to Hessenberg, tridiagonal and bidiagonal forms [19, 24]. The encouraging performance results mentioned above were obtained for particular process grid shapes and empirically chosen distribution parameters on specific hardware platforms.
Reference: [21] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers. </title> <booktitle> In Proceedings of Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, Virginia), </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992. </year> <note> (also LAPACK Working Note #55). </note>
Reference-contexts: These three approaches are presented below and focus on different optimality criteria. 1. optimal data layout and efficiency, 2. data decomposition independence, 3. software reuse via high-level language support for data parallel programming. First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm <ref> [21, 46, 48] </ref>, a great deal of research [7, 12, 55] has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research <ref> [21, 46, 48, 55] </ref> has aimed at studying different data decompositions [7, 12, 57]. <p> Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1.
Reference: [22] <author> J. Choi, J. Dongarra, and D. Walker. </author> <title> Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent Computers. </title> <booktitle> In Proceedings of Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, Virginia), </booktitle> <pages> pages 245-252. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Cal-ifornia, </address> <year> 1993. </year> <note> (also LAPACK Working Note #65). </note>
Reference-contexts: Second, some of this mapping's properties can be expressed in a simpler way if one restricts oneself to the square block cyclic case (see Chapter 2). Some of these simplified corollaries have already been indirectly illustrated and applied in <ref> [22, 23, 25] </ref>. Laborious debugging sessions of code fragments that were in fact relying on these corollaries are at the origin of the development of this more formal approach. <p> This square matrix is called an LCM block. Each process owns exactly lcmb=P fi lcmb=Q entries of this LCM block. This concept has been originally introduced in the restricted context of square block cyclic mappings in <ref> [22, 23, 25] </ref>. The purpose of this section is surely to formally exhibit properties of 28 the block cyclic distribution. <p> Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1.
Reference: [23] <author> J. Choi, J. Dongarra, and D. Walker. PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on Distributed Memory Concurrent Computers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 543-570, </pages> <year> 1994. </year> <note> (also LAPACK Working Note #57). </note>
Reference-contexts: Second, some of this mapping's properties can be expressed in a simpler way if one restricts oneself to the square block cyclic case (see Chapter 2). Some of these simplified corollaries have already been indirectly illustrated and applied in <ref> [22, 23, 25] </ref>. Laborious debugging sessions of code fragments that were in fact relying on these corollaries are at the origin of the development of this more formal approach. <p> This square matrix is called an LCM block. Each process owns exactly lcmb=P fi lcmb=Q entries of this LCM block. This concept has been originally introduced in the restricted context of square block cyclic mappings in <ref> [22, 23, 25] </ref>. The purpose of this section is surely to formally exhibit properties of 28 the block cyclic distribution. <p> Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1.
Reference: [24] <author> J. Choi, J. Dongarra, and D. Walker. </author> <title> The Design of a Parallel, Dense Linear Algebra Software Library: Reduction to Hessenberg, Tridiagonal and Bidig-onal Form. </title> <booktitle> Numerical Algorithms, </booktitle> <volume> 10 </volume> <pages> 379-399, </pages> <year> 1995. </year>
Reference-contexts: There is considerable evidence that the square block cyclic mapping can lead to very efficient implementations of more complex matrix operations such as the LU, Cholesky or QR factorizations [20, 38] or even the reductions to Hessenberg, tridiagonal and bidiagonal forms <ref> [19, 24] </ref>. The encouraging performance results mentioned above were obtained for particular process grid shapes and empirically chosen distribution parameters on specific hardware platforms.
Reference: [25] <author> J. Choi, J. Dongarra, and D. Walker. PB-BLAS: </author> <title> A Set of Parallel Block Basic Linear Algebra Subroutines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(7) </volume> <pages> 517-535, </pages> <year> 1996. </year>
Reference-contexts: Second, some of this mapping's properties can be expressed in a simpler way if one restricts oneself to the square block cyclic case (see Chapter 2). Some of these simplified corollaries have already been indirectly illustrated and applied in <ref> [22, 23, 25] </ref>. Laborious debugging sessions of code fragments that were in fact relying on these corollaries are at the origin of the development of this more formal approach. <p> This square matrix is called an LCM block. Each process owns exactly lcmb=P fi lcmb=Q entries of this LCM block. This concept has been originally introduced in the restricted context of square block cyclic mappings in <ref> [22, 23, 25] </ref>. The purpose of this section is surely to formally exhibit properties of 28 the block cyclic distribution. <p> Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1. <p> One answer is given by the physical blocking approach, where the distribution blocking factors are used as computational blocking units, hence inducing alignment restrictions on the operands. Most of the parallel algorithms proposed in the literature are physically blocked <ref> [25, 26, 74, 83] </ref>. High performance is achievable on a wide range of DMCCs, but usually depends on the distribution blocking factors. The alignment restrictions simplify the expression and implementation of these algorithms, but also limit their application scope in a way that does not satisfy general purpose library requirements. <p> It may also be necessary to copy back the meaningful part of this buffer when the triangular or symmetric distributed matrix is an output parameter. This strategy is exactly what is done in the current version of the ScaLAPACK software library <ref> [16, 25] </ref>. There is little evidence, however, demonstrating the superiority of this strategy over the second option. The latter avoids the unnecessary floating point operations as well as the data copy, and thus does not require any workspace to store these diagonal LCM blocks.
Reference: [26] <author> A. Chtchelkanova, J. Gunnels, G. Morrow, J. Overfelt, and R. van de Geijn. </author> <title> Parallel Implementation of BLAS: General Techniques for Level 3 BLAS. </title> <type> Technical Report TR95-49, </type> <institution> Department of Computer Sciences, UT-Austin, </institution> <year> 1995. </year> <note> Submitted to Concurrency: Practice and Experience. </note>
Reference-contexts: Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project [16, 39] as well as others <ref> [1, 13, 26, 41] </ref>. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy. <p> One answer is given by the physical blocking approach, where the distribution blocking factors are used as computational blocking units, hence inducing alignment restrictions on the operands. Most of the parallel algorithms proposed in the literature are physically blocked <ref> [25, 26, 74, 83] </ref>. High performance is achievable on a wide range of DMCCs, but usually depends on the distribution blocking factors. The alignment restrictions simplify the expression and implementation of these algorithms, but also limit their application scope in a way that does not satisfy general purpose library requirements.
Reference: [27] <author> E. Chu and A. George. </author> <title> QR Factorization of a Dense Matrix on a Hypercube Multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 990-1028, </pages> <year> 1990. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research [7, 12, 55] has aimed at determining optimal data distributions <ref> [27, 54, 57, 72] </ref>. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries <ref> [27, 54, 72] </ref> due to its scalability [38], load balance and communication [54] properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. The contributions of this chapter are two-fold.
Reference: [28] <institution> Mathemetical Committee on Physical and Engineering Sciences, editors. Grand Challenges: High Performance Computing and Communications. </institution> <address> NSF/CISE, 1800 G Street NW, Washington, DC, 20550, </address> <year> 1991. </year>
Reference-contexts: Les deux derniers dispensent assez bien du premier. Charles Maurice de Talleyrand (1754-1838) In the past several years, the emergence of Distributed Memory Concurrent Computers (DMCCs) and their potential for the numerical solution of Grand Challenge problems <ref> [28, 62, 76, 77] </ref> has led to extensive research. As a result, DMCCs have become not only indispensable machines for large-scale engineering and scientific applications, but also common and viable platforms for commercial and financial applications.
Reference: [29] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: By setting p (or q) in Equation (2.3.11) to a constant value, it follows that the distance between two consecutive D-processes in the same process row (or column) is equal to gcd (P; Q). Moreover, the extended Euclid's algorithm <ref> [29] </ref> can be used to solve the linear Diophantine Equation (2.3.12). The solution pairs depend on the local process information (p; q). <p> instead solve m fl Q s l fl P r = gcdb = gcd (P r; Q s): (2.4.17) The solution (l fl ; m fl ) of this equation can be found in O (log (max (P r; Q s))) time and space by using the extended Euclid's algorithm <ref> [29] </ref> for computing gcdb. Then, one computes gcd (r; s) and rewrites fi as a disjoint union of intervals fi h = [h gcd (r; s) : : : (h + 1) gcd (r; s)) with h 2 ZZ.
Reference: [30] <institution> IBM Corporation. IBM RS6000. </institution> <note> (http://www.rs6000.ibm.com/), 1996. </note>
Reference-contexts: The clock speed of this processor is 66.7 MHz, giving a peak performance per processor of 266 MFLOPS. There are two types of nodes, known as thin nodes and wide nodes. Thin nodes have a 64 KB data cache. Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. <p> Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. Figure 4.4 shows the thin node performance of the vendor 99 supplied rank-K update library routine for distinct values of M = N and K. In practice, the performance of such an operation was observed to be at most 200 Mflops for those thin nodes. <p> The TB2 switch adapter, which is the interface between the node and the switch, features a Direct Memory Access (DMA) engine. For message passing libraries optimized for the switch, the typical bandwidth is 35 MB/s with a latency of approximately 50 s <ref> [4, 30, 80] </ref>. 100 During our experiments, the performance of the BLACS communication prim-itives implemented on top of the native IBM message passing library (MPL) was measured.
Reference: [31] <institution> Intel Corporation. Intel Supercomputer Technical Publications Home Page. </institution> <note> (http://www.ssd.intel.com/pubs.html), 1995. </note>
Reference-contexts: General-purpose (GP) nodes are also available. Those nodes have two XP application processors one dedicated to applications and the other to message-passing <ref> [31] </ref>. Figure 4.2 shows the GP node performance of the vendor supplied matrix-matrix multiply library routine for distinct values of M = N and K. In practice, the performance of such an operation was observed to be at most 45 Mflops for those GP nodes. <p> Hardware latency the time to set up the transfer of the first byte of a message is so low (40 ns per MRC traversed) that the physical location of 98 nodes becomes unimportant for performance <ref> [31] </ref>. During our experiments, the performance of the BLACS communication primitives implemented on top of the native Intel XP/S message passing library was measured.
Reference: [32] <editor> M. Cosnard, Y. Robert, P. Quinton, and M. Tchuente, editors. </editor> <booktitle> Parallel Algorithms and Architectures. </booktitle> <publisher> North-Holland, </publisher> <year> 1986. </year> <month> 166 </month>
Reference-contexts: The scheduling policies, however, are not contention free. Developing and/or characterizing such 158 policies for static and dynamic networks is a research area on its own. References on the subject can be found in <ref> [32, 67] </ref>. More recent research work on this topic can be found in [78, 86]. These problems are complex and difficult to address. The experimental results presented in the literature are usually machine dependent. They are often based on empirical trials or heuristics.
Reference: [33] <author> M. Dayde, I. Duff, and A. Petitet. </author> <title> A Parallel Block Implementation of Level 3 BLAS for MIMD Vector Processors. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 20(2) </volume> <pages> 178-193, </pages> <year> 1994. </year>
Reference-contexts: Still, the correctness of these operations and the robustness and reliability of their implementation depend entirely on the material presented in this chapter. 57 Chapter 3 Algorithmic Redistribution 3.1 Introduction In a serial computational environment, transportable efficiency is the essential motivation for developing blocking strategies and block-partitioned algorithms <ref> [3, 5, 33, 60] </ref>. The linear algebra package (LAPACK) [5] is the archetype of such a demarche. The LAPACK software is constructed as much as possible out of calls to the BLAS (Basic Linear Algebra Subprograms). <p> Finally, it is possible to reuse existing serial GEMM-based implementations of the Levels 2 and 3 BLAS <ref> [33, 60] </ref>. 75 3.4.2 Cyclic Ordering The cyclic ordering strategy is distinguished by the fact that the computations are cyclically distributed as opposed to the data. The block cyclic data distribution allocates the data in a cyclic fashion. <p> The algorithm proceeds in an ordered sequence of steps that depend on each other. It is, however, possible to block this algorithm and and express it in terms of triangular solves and matrix multiplies <ref> [33, 60] </ref>. <p> A parallel implementation of these algorithms can take advantage of such distribution properties. Parallel basic linear algebra operations such as the matrix-matrix multiply or the triangular solve operations can also be expressed recursively as a succession of themselves and rank-k updates <ref> [33, 60] </ref>. The algorithms proposed in the literature thus far focus on the naturally aligned cases used in the factorization and reduction operations. This restricted interest prevents one from providing the necessary flexibility that a parallel software library requires to be truly usable.
Reference: [34] <author> G. </author> <title> Lejeune Dirichlet. </title> <journal> Abhandlungen Koniglich Preuss. Akad. Wiss., </journal> <volume> 1849. </volume>
Reference-contexts: The value of this probability can therefore be considered as almost exact or at worst a very accurate lower bound for all possible values of the distribution parameters. Finally, an analogous result known as Dirichlet's theorem <ref> [34] </ref> states that the probability that gcd (u; v) = 1 for u and v integers chosen at random exists and is 6 2 .
Reference: [35] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A Set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: The well-known BLAS <ref> [36, 35] </ref> and LAPACK [5] numerical linear algebra software libraries are typical examples of such useful and successful software packages for shared-memory vector and parallel processors. <p> This classification criterion happens to also correspond 58 to three different kinds of basic linear algebra operations: * Level 1 BLAS [68]: for vector operations, such as y ffx + y, * Level 2 BLAS [36]: for matrix-vector operations, such as y ffAx + fiy, * Level 3 BLAS <ref> [35] </ref>: for matrix-matrix operations, such as C ffAB + fiC. Here, A, B, and C are matrices, x and y are vectors, and ff and fi are scalars.
Reference: [36] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> Algorithm 656: An extended Set of Basic Linear Algebra Subprograms: Model Implementation and Test Programs. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 18-32, </pages> <year> 1988. </year>
Reference-contexts: The well-known BLAS <ref> [36, 35] </ref> and LAPACK [5] numerical linear algebra software libraries are typical examples of such useful and successful software packages for shared-memory vector and parallel processors. <p> This classification criterion happens to also correspond 58 to three different kinds of basic linear algebra operations: * Level 1 BLAS [68]: for vector operations, such as y ffx + y, * Level 2 BLAS <ref> [36] </ref>: for matrix-vector operations, such as y ffAx + fiy, * Level 3 BLAS [35]: for matrix-matrix operations, such as C ffAB + fiC. Here, A, B, and C are matrices, x and y are vectors, and ff and fi are scalars.
Reference: [37] <author> J. Dongarra and R. van de Geijn. </author> <title> Two dimensional Basic Linear Algebra Communication Subprograms. </title> <type> Technical Report UT CS-91-138, </type> <note> LAPACK Working Note #37, </note> <institution> University of Tennessee, </institution> <year> 1991. </year>
Reference-contexts: In our experimental implementation, the local rank-K operation is performed by calling the appropriate subprogram of the vendor-supplied BLAS. The communication operations are implemented by explicit calls to the Basic Linear Algebra Communications Subprograms (BLACS). The BLACS <ref> [37, 40] </ref> are a message passing library specifically designed for distributed linear algebra communication operations. The computational model consists of a one or two-dimensional grid of processes, where each process stores matrices and vectors.
Reference: [38] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> Scalability Issues in the Design of a Library for Dense Linear Algebra. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 523-537, </pages> <year> 1994. </year> <note> (also LAPACK Working Note #43). </note>
Reference-contexts: Efficiency is the primary consideration justifying any restriction or requirement that an implementation may have. As a result, the two-dimensional block cyclic distribution [67] (see Chapter 2) has been suggested as the basic decomposition for parallel dense linear algebra libraries due to its scalability <ref> [38, 67] </ref>, load balance and communication [54] properties. Let us illustrate the implications of this approach on the simple trace computation example. First, one would likely restrict the trace computation to the leading submatrix of the initial distributed matrix 8 to simplify somewhat the index computations. <p> There is considerable evidence that the square block cyclic mapping can lead to very efficient implementations of more complex matrix operations such as the LU, Cholesky or QR factorizations <ref> [20, 38] </ref> or even the reductions to Hessenberg, tridiagonal and bidiagonal forms [19, 24]. The encouraging performance results mentioned above were obtained for particular process grid shapes and empirically chosen distribution parameters on specific hardware platforms. <p> As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability <ref> [38] </ref>, load balance and communication [54] properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. The contributions of this chapter are two-fold. First, elementary results of the theory of integers are systematically brought to the fore. <p> Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly [66]. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library <ref> [16, 21, 22, 23, 25, 38] </ref>. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj. When this is not the case, the properties can easily be adapted as indicated in Ta 37 ble 2.1. <p> These source processes change at every iteration of the loop. High performance and efficiency can still be achieved for a wide range of different values of the blocking factors. This has been reported in <ref> [2, 38, 74, 83] </ref>. The use of physical blocking in conjunction with static blocking can lead to a comprehensive and scalable dense linear algebra software library. Existing serial software such as LAPACK [5] can be reused. The ScaLAPACK software library is the result of this reasoning.
Reference: [39] <author> J. Dongarra and D. Walker. </author> <title> Software Libraries for Linear Algebra Computations on High Performance Computers. </title> <journal> SIAM Review, </journal> <volume> 37(2) </volume> <pages> 151-180, </pages> <month> 167 </month> <year> 1995. </year>
Reference-contexts: Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project <ref> [16, 39] </ref> as well as others [1, 13, 26, 41]. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy.
Reference: [40] <author> J. Dongarra and R. C. Whaley. </author> <title> A User's Guide to the BLACS v1.0. </title> <type> Technical Report UT CS-95-281, </type> <note> LAPACK Working Note #94, </note> <institution> University of Tennessee, </institution> <year> 1995. </year> <note> (http://www.netlib.org/blacs/). </note>
Reference-contexts: In our experimental implementation, the local rank-K operation is performed by calling the appropriate subprogram of the vendor-supplied BLAS. The communication operations are implemented by explicit calls to the Basic Linear Algebra Communications Subprograms (BLACS). The BLACS <ref> [37, 40] </ref> are a message passing library specifically designed for distributed linear algebra communication operations. The computational model consists of a one or two-dimensional grid of processes, where each process stores matrices and vectors.
Reference: [41] <author> R. Falgout, A. Skjellum, S. Smith, and C. </author> <title> Still. The Multicomputer Toolbox Approach to Concurrent BLAS and LACS. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference SHPCC-92. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: These results have motivated this second approach where the user's decomposition is generally not changed but passed as an argument and a suboptimal algorithm is used. This approach is usually referred to as decomposition independent <ref> [41, 84] </ref>. The suboptimality of a routine must be weighted against the possibly large cost of redistributing the input data. Finally, the most potentially ambitious approach attempts to provide high-level language support for data parallel programming. <p> Second, developing a set of BLAS for DMCCs should lead to a straightforward port of the LAPACK software. This is the path followed by the ScaLAPACK research project [16, 39] as well as others <ref> [1, 13, 26, 41] </ref>. 59 Such a design sounds simple and reasonable, even if little is said on the adequate blocking strategies for a distributed memory hierarchy.
Reference: [42] <author> M. Flynn. </author> <title> Some Computer Organizations and Their Effectiveness. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(9) </volume> <pages> 948-960, </pages> <year> 1972. </year>
Reference-contexts: However, when the problem's data or even the tasks to be performed are irregular, the data parallel programming model may not be a viable and/or useful abstraction. In this dissertation, DMCCs will be regarded as MIMD computers from the architectural point of view according to Flynn's classification <ref> [42] </ref>. DMCCs will however be considered as SPMD multicomputers from the programming point of 5 view; that is, the same program is executed by all of the processes simultaneously.
Reference: [43] <author> Message Passing Interface Forum. </author> <title> MPI: A Message Passing Interface Standard. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <pages> 8(3-4), </pages> <year> 1994. </year>
Reference-contexts: Finally, redistribution mechanisms should be as independent as possible from a particular architecture to allow for their portability across a wide range of DMCCs. This last issue has been considerably simplified by the recent development of the Message Passing Interface (MPI) <ref> [43] </ref> and its adoption by a large majority of machine vendors. Most of the Fortran- and C-based data parallel languages incorporate explicit and implicit redistribution capabilities. The Kali language [75] was one of the first to do so. DINO [79] addresses the implicit redistribution of data at procedure boundaries.
Reference: [44] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface (Draft). </title> <note> (http://www.mcs.anl.gov/mpi), 1996. </note>
Reference-contexts: Such a semantic is usually based on the put and get paradigm. The one-sided communication primitives can be implemented in terms of asynchronous send and receive primitives as it is suggested in the current draft of the extensions to the Message-Passing Interface <ref> [44] </ref>. Independently from the semantic of the message passing model, this programming paradigm is tedious, time-consuming, and error-prone for programmers in general, as it is ultimately based on separate name spaces. In the data parallel model, parallelism, i.e., inter-process communication, is explicitly handled by hardware synchronization 4 and flow control.
Reference: [45] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koebel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D Language Specification. </title> <type> Technical Report TR90-141, </type> <institution> Rice University, Department of Computer Science, </institution> <year> 1990. </year> <month> 168 </month>
Reference-contexts: Finally, the most potentially ambitious approach attempts to provide high-level language support for data parallel programming. In the last few years, several data parallel Fortran languages have been proposed, such as Fortran D <ref> [45] </ref> and Vienna Fortran [88]. More recently, the High Performance Fortran (HPF) language [66] has been developed as a community effort to standardize data parallel Fortran programming for DMCCs. HPF includes many of the concepts originally proposed in 10 Fortran D, Vienna Fortran, and other data parallel Fortran languages.
Reference: [46] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1988. </year>
Reference-contexts: These three approaches are presented below and focus on different optimality criteria. 1. optimal data layout and efficiency, 2. data decomposition independence, 3. software reuse via high-level language support for data parallel programming. First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm <ref> [21, 46, 48] </ref>, a great deal of research [7, 12, 55] has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research <ref> [21, 46, 48, 55] </ref> has aimed at studying different data decompositions [7, 12, 57]. <p> An important performance metric is parallel efficiency. Parallel efficiency, 103 E (n; p), for a problem of size n on p processors is defined in the usual way <ref> [46] </ref> as E (n; p) = p T (n; p) where T (n; p) is the runtime of the parallel algorithm, and T seq (n) is the runtime of the best sequential algorithm.
Reference: [47] <author> G. Fox, S. Otto, and A. Hey. </author> <title> Matrix Algorithms on a Hypercube I: Matrix Multiplication. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 17-31, </pages> <year> 1987. </year>
Reference: [48] <author> G. Geist and C. Romine. </author> <title> LU Factorization Algorithms on Distributed Memory Multiprocessor Architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 639-649, </pages> <year> 1988. </year>
Reference-contexts: These three approaches are presented below and focus on different optimality criteria. 1. optimal data layout and efficiency, 2. data decomposition independence, 3. software reuse via high-level language support for data parallel programming. First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm <ref> [21, 46, 48] </ref>, a great deal of research [7, 12, 55] has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research <ref> [21, 46, 48, 55] </ref> has aimed at studying different data decompositions [7, 12, 57].
Reference: [49] <author> G. Golub and C. van Loan. </author> <title> Matrix Computations. </title> <address> Johns-Hopkins, Baltimore, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: The block cyclic data distribution allocates the data in a cyclic fashion. The computation then proceeds in consecutive order just like a conventional serial algorithm. For example, the usual LU factorization algorithm <ref> [49] </ref> handles first the first column of the matrix, then the second and so on. The dual of this framework can be described as follows. First, the data is allocated or distributed in consecutive order, i.e., according to the blocked distribution defined in (2.2.6). <p> First, the linear operator is factorized into a product of two or more matrices featuring suitable properties for the resolution 148 of the problem. Second, the solution of the problem is obtained by solving sim-pler matrix equalities typically involving triangular and/or orthogonal matrices <ref> [5, 49] </ref>. This same framework forms the basis of modern algorithms solving algebraic eigenvalue problems. The matrix representing the linear operator is first reduced to a condensed form. The numerical solution is then obtained by applying an iterative method to this condensed form [5, 49]. <p> equalities typically involving triangular and/or orthogonal matrices <ref> [5, 49] </ref>. This same framework forms the basis of modern algorithms solving algebraic eigenvalue problems. The matrix representing the linear operator is first reduced to a condensed form. The numerical solution is then obtained by applying an iterative method to this condensed form [5, 49]. Block-partitioned algorithms have been developed for most of the matrix factorizations and reductions. These algorithms have been implemented in the LAPACK software library [5] for shared memory systems. The bulk of computation in these algorithms is performed on the matrix representation of the linear operator.
Reference: [50] <author> M. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural Compilation of Fortran D for MIMD Machines. </title> <booktitle> In Proceedings of Supercomputing'92, </booktitle> <pages> pages 522-534, </pages> <year> 1992. </year>
Reference-contexts: The Kali language [75] was one of the first to do so. DINO [79] addresses the implicit redistribution of data at procedure boundaries. The Hypertasking compiler [8] for data parallel C programs, Vienna Fortran [14] and Fortran D <ref> [50] </ref> additionally specify data redistribution primitives. For efficiency purposes as well as simplicity of the compiler, these redistribution operations are often implemented in a library of intrinsics [61]. Explicit and implicit data redistribution operations are necessary but not quite sufficient.
Reference: [51] <author> P. Hatcher and M. Quinn. </author> <title> Data-Parallel Programming On MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Today, the first HPF compilers are slowly becoming available on a wide range of machines, and it is unclear yet if those compilers will fulfill their highly difficult goals in the near future. C-based data parallel extensions have also been proposed, such as Data Parallel C <ref> [51] </ref> and pC++ [11].
Reference: [52] <author> M. Heath and C. Romine. </author> <title> Parallel Solution Triangular Systems on Distributed Memory Multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 558-588, </pages> <year> 1988. </year> <month> 169 </month>
Reference: [53] <author> B. Hendrickson, E. Jessup, and C. Smith. </author> <title> A Parallel Eigensolver for Dense Symmetric Matrices. </title> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: The feasibil 81 ity and performance characteristics of this approach have been illustrated for the numerical resolution of a general linear system of equations and the symmetric eigenproblem in <ref> [12, 13, 53] </ref> for the purely scattered distribution as defined in (2.2.7). Similarly, it is sometimes beneficial to disaggregate a panel into multiple panels in order to overlap communication and computation phases. When applicable, this last strategy also presents the advantage of requiring a smaller amount of workspace.
Reference: [54] <author> B. Hendrickson and D. Womble. </author> <title> The Torus-wrap Mapping for Dense Matrix Calculations on Massively Parallel Computers. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 15(5) </volume> <pages> 1201-1226, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research [7, 12, 55] has aimed at determining optimal data distributions <ref> [27, 54, 57, 72] </ref>. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> As a result, the two-dimensional block cyclic distribution [67] (see Chapter 2) has been suggested as the basic decomposition for parallel dense linear algebra libraries due to its scalability [38, 67], load balance and communication <ref> [54] </ref> properties. Let us illustrate the implications of this approach on the simple trace computation example. First, one would likely restrict the trace computation to the leading submatrix of the initial distributed matrix 8 to simplify somewhat the index computations. <p> As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries <ref> [27, 54, 72] </ref> due to its scalability [38], load balance and communication [54] properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. The contributions of this chapter are two-fold. <p> As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability [38], load balance and communication <ref> [54] </ref> properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. The contributions of this chapter are two-fold. First, elementary results of the theory of integers are systematically brought to the fore. <p> With these assumptions, subject to a renumbering of the processes, a D-block is just a diagonal entry and conversely. Since the above properties completely characterize the D-blocks and the D-processes, the problem of interest is solved. These assumptions have been made by some researchers <ref> [12, 13, 54] </ref> to implement the LINPACK benchmark and related dense linear algebra kernels on distributed vector computers. These are also specifications data parallel languages such as HPF are leaning towards.
Reference: [55] <author> G. Henry and R. van de Geijn. </author> <title> Parallelizing the QR Algorithm for the Unsym-metric Algebraic Eigenvalue problem: Myths and Reality. </title> <type> Technical Report UT CS-94-244, </type> <note> LAPACK Working Note #79, </note> <institution> University of Tennessee, </institution> <year> 1994. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research <ref> [7, 12, 55] </ref> has aimed at determining optimal data distributions [27, 54, 57, 72]. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> A definition of the block Hankel wrapped distribution scheme can be found in <ref> [55] </ref> and references therein. For sufficiently small distribution blocking factors, both of these distribution choices ensure that the diagonal blocks of the submatrix operand will be evenly distributed among all processes, and thus a perfect load balance. With these restrictions, one is guaranteed to produce an optimal trace computation implementation. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research <ref> [21, 46, 48, 55] </ref> has aimed at studying different data decompositions [7, 12, 57].
Reference: [56] <author> S. Hiranandani, K. Kennedy, J. Mellor-Crummey, and A. Sethi. </author> <title> Compilation Techniques for Block-Cyclic Distributions. </title> <type> Technical Report CRPC-TR95521-S, </type> <note> Center for Research on Parallel Computation, </note> <year> 1995. </year>
Reference-contexts: The approach of solving a set of linear Diophantine equations to determine index sets, process sets and so on is recommended by data parallel compiler designers as one way to proceed <ref> [15, 56, 64, 82] </ref>. Binary algorithms are available [65] to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements.
Reference: [57] <author> S. Huss-Lederman, E. Jacobson, A. Tsao, and G. Zhang. </author> <title> Matrix Multiplication on the Intel Touchstone DELTA. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(7) </volume> <pages> 571-594, </pages> <year> 1994. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research [7, 12, 55] has aimed at determining optimal data distributions <ref> [27, 54, 57, 72] </ref>. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research [21, 46, 48, 55] has aimed at studying different data decompositions <ref> [7, 12, 57] </ref>. As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability [38], load balance and communication [54] properties.
Reference: [58] <author> K. Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, </publisher> <year> 1993. </year>
Reference-contexts: In order to alleviate this difficulty, parallel programming models have been specifically designed for DMCCs. A programming model is a collection of program abstractions providing a programmer with a simplified and transparent view of the computer hardware/software system <ref> [58] </ref>. The basic computational units in a running parallel program are processes corresponding to operations performed by related code segments on the process's data set. A running program can then be defined as a collection of processes [58]. Inter-process communication 3 defines what is called a running parallel program. <p> a programmer with a simplified and transparent view of the computer hardware/software system <ref> [58] </ref>. The basic computational units in a running parallel program are processes corresponding to operations performed by related code segments on the process's data set. A running program can then be defined as a collection of processes [58]. Inter-process communication 3 defines what is called a running parallel program. In general there may be several processes executed by one physical processor; therefore, without loss of generality, the underlying DMCC will be regarded henceforth as a set of processes rather than physical processors.
Reference: [59] <author> S. L. Johnsson. </author> <title> Communication Efficient Basic Linear Algebra Computations on Hypercube Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 2 </volume> <pages> 133-172, </pages> <year> 1987. </year>
Reference-contexts: The disaggregation technique however can only be applied efficiently for operations that do not feature any dependence between steps, such as a matrix-multiply. The disaggregated data remains consecutively ordered. Therefore, it cannot improve significantly the load imbalance caused by consecutive allocation and consecutive elimination <ref> [59] </ref>. 3.4.5 LCM Blocking The LCM blocking strategy operates on a panel of rows or columns that are locally contiguous. The size of this panel is also a logical blocking unit factor that depends on the target machine characteristics.
Reference: [60] <author> B. K-agstrom, P. Ling, and C. van Loan. </author> <title> GEMM-Based Level 3 BLAS: High--Performance Model Implementations and Performance Evaluation Benchmark. </title> <type> Technical Report UMINF 95-18, </type> <institution> Department of Computing Science, Ume-a University, </institution> <year> 1995. </year> <note> Submitted to ACM TOMS. </note>
Reference-contexts: Still, the correctness of these operations and the robustness and reliability of their implementation depend entirely on the material presented in this chapter. 57 Chapter 3 Algorithmic Redistribution 3.1 Introduction In a serial computational environment, transportable efficiency is the essential motivation for developing blocking strategies and block-partitioned algorithms <ref> [3, 5, 33, 60] </ref>. The linear algebra package (LAPACK) [5] is the archetype of such a demarche. The LAPACK software is constructed as much as possible out of calls to the BLAS (Basic Linear Algebra Subprograms). <p> Finally, it is possible to reuse existing serial GEMM-based implementations of the Levels 2 and 3 BLAS <ref> [33, 60] </ref>. 75 3.4.2 Cyclic Ordering The cyclic ordering strategy is distinguished by the fact that the computations are cyclically distributed as opposed to the data. The block cyclic data distribution allocates the data in a cyclic fashion. <p> The algorithm proceeds in an ordered sequence of steps that depend on each other. It is, however, possible to block this algorithm and and express it in terms of triangular solves and matrix multiplies <ref> [33, 60] </ref>. <p> A parallel implementation of these algorithms can take advantage of such distribution properties. Parallel basic linear algebra operations such as the matrix-matrix multiply or the triangular solve operations can also be expressed recursively as a succession of themselves and rank-k updates <ref> [33, 60] </ref>. The algorithms proposed in the literature thus far focus on the naturally aligned cases used in the factorization and reduction operations. This restricted interest prevents one from providing the necessary flexibility that a parallel software library requires to be truly usable.
Reference: [61] <author> E. Kalns. </author> <title> Scalable Data Redistribution Services for Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Michigan State University, </institution> <year> 1995. </year>
Reference-contexts: The Hypertasking compiler [8] for data parallel C programs, Vienna Fortran [14] and Fortran D [50] additionally specify data redistribution primitives. For efficiency purposes as well as simplicity of the compiler, these redistribution operations are often implemented in a library of intrinsics <ref> [61] </ref>. Explicit and implicit data redistribution operations are necessary but not quite sufficient. These expensive operations change the distribution of an entire operand at once, where in a number of cases some of it can be delayed.
Reference: [62] <author> W. Kaufmann and L. </author> <title> Smarr. </title> <booktitle> Supercomputing and the Transformation of Science. </booktitle> <publisher> Scientific American Library, </publisher> <year> 1993. </year>
Reference-contexts: Les deux derniers dispensent assez bien du premier. Charles Maurice de Talleyrand (1754-1838) In the past several years, the emergence of Distributed Memory Concurrent Computers (DMCCs) and their potential for the numerical solution of Grand Challenge problems <ref> [28, 62, 76, 77] </ref> has led to extensive research. As a result, DMCCs have become not only indispensable machines for large-scale engineering and scientific applications, but also common and viable platforms for commercial and financial applications.
Reference: [63] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> Efficient Address Generation For Block-Cyclic Distributions. </title> <type> Technical Report CRPC-TR94485-S, </type> <note> Center for Research on Parallel Computation, </note> <year> 1994. </year>
Reference-contexts: Binary algorithms are available [65] to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements. Dynamic storage facilities are needed for the quadruplet solutions <ref> [63] </ref>. It can be adapted to accommodate variations of the block cyclic distributions that are supported by the HPF language. 40 2.5 LCM Tables Definition 2.5.1 The k-diagonal of a matrix is the set of entries a ij such that i j = k. Remark.
Reference: [64] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A Linear-Time Algorithm for Computing the Memory Access Sequence in Data Parallel Programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <year> 1995. </year>
Reference-contexts: The approach of solving a set of linear Diophantine equations to determine index sets, process sets and so on is recommended by data parallel compiler designers as one way to proceed <ref> [15, 56, 64, 82] </ref>. Binary algorithms are available [65] to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements.
Reference: [65] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1973. </year> <title> Volume 1. Fundamental algorithms. Volume 2. Semi-numerical algorithms. Volume 3. Sorting and searching. </title> <type> 171 </type>
Reference-contexts: The approach of solving a set of linear Diophantine equations to determine index sets, process sets and so on is recommended by data parallel compiler designers as one way to proceed [15, 56, 64, 82]. Binary algorithms are available <ref> [65] </ref> to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements. Dynamic storage facilities are needed for the quadruplet solutions [63]. <p> This precision however, does complicate the following argumentation in a useless manner. More information on the random generation of integers for the purpose of a proof can be found in <ref> [65] </ref>. For the sake of simplicity, only the essential arguments are presented below. We first state and prove two useful lemmas. Lemma 2.5.1 Let p be a prime and ff a positive integer.
Reference: [66] <author> C. Koebel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: Finally, the most potentially ambitious approach attempts to provide high-level language support for data parallel programming. In the last few years, several data parallel Fortran languages have been proposed, such as Fortran D [45] and Vienna Fortran [88]. More recently, the High Performance Fortran (HPF) language <ref> [66] </ref> has been developed as a community effort to standardize data parallel Fortran programming for DMCCs. HPF includes many of the concepts originally proposed in 10 Fortran D, Vienna Fortran, and other data parallel Fortran languages. <p> First, it provides a simple general purpose way of distributing a block-partitioned matrix on DMCCs. It encom 14 passes a large number of more specific data distributions such as the blocked or purely scattered cases (see Chapter 2), and it has been incorporated into the High Performance Fortran language <ref> [66] </ref>. Second, some of this mapping's properties can be expressed in a simpler way if one restricts oneself to the square block cyclic case (see Chapter 2). Some of these simplified corollaries have already been indirectly illustrated and applied in [22, 23, 25]. <p> In this case as well, the diagonals of the D-blocks are the D-diagonal entries and conversely. Similarly as above, the problem of locating the diagonals is solved. The square block cyclic data decomposition is a particular case of the distributions HPF supports stan-dardly <ref> [66] </ref>. This approach has also been chosen for the design of the dense routines in the ScaLAPACK software library [16, 21, 22, 23, 25, 38]. The above properties assume that the diagonals a ij of interest are such that s evenly divides ji jj.
Reference: [67] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: In order to avoid global synchronization after each instructions, the same program can be executed by each processor asynchronously. Synchronization takes place only when processors need to exchange data. This programming model is referred to as the Single Program Multiple Data (SPMD) programming model <ref> [67] </ref>. This model is based on distinct name spaces and loosely synchronous parallel computation with a distinct data set for each process. Thus, data parallelism can be exploited on DMCCs by using the data parallel programming model or the SPMD programming model. <p> Efficiency is the primary consideration justifying any restriction or requirement that an implementation may have. As a result, the two-dimensional block cyclic distribution <ref> [67] </ref> (see Chapter 2) has been suggested as the basic decomposition for parallel dense linear algebra libraries due to its scalability [38, 67], load balance and communication [54] properties. Let us illustrate the implications of this approach on the simple trace computation example. <p> Efficiency is the primary consideration justifying any restriction or requirement that an implementation may have. As a result, the two-dimensional block cyclic distribution [67] (see Chapter 2) has been suggested as the basic decomposition for parallel dense linear algebra libraries due to its scalability <ref> [38, 67] </ref>, load balance and communication [54] properties. Let us illustrate the implications of this approach on the simple trace computation example. First, one would likely restrict the trace computation to the leading submatrix of the initial distributed matrix 8 to simplify somewhat the index computations. <p> Since the data decom 19 position largely determines the performance and scalability of a concurrent algo-rithm, a great deal of research [21, 46, 48, 55] has aimed at studying different data decompositions [7, 12, 57]. As a result, the two-dimensional block cyclic distribution <ref> [67] </ref> has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries [27, 54, 72] due to its scalability [38], load balance and communication [54] properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. <p> Strictly speaking, a NUMA architecture differs from a message passing architecture in the sense that it provides hardware support for direct access to other processor's memories, whereas in a message passing architecture, remote access must be explicitly emulated via message passing <ref> [67] </ref>. The interconnection network of our machine model is static, meaning that it consists of point-to-point communication links among processors. This type of network is also referred to as a direct network as opposed to dynamic networks. The latter are constructed from switches and communication links. <p> Each processor in the two-dimensional mesh has four communication ports. However, the model assumes that a processor can send or receive data on only one of its ports at a time. This assumption is also referred to as the one-port communication model <ref> [67] </ref>. The time spent to communicate a message between two processors is called the communication time T c . In our machine model, T c is approximated by a linear function of the number L of items communicated. <p> Similarly, the model does not make any assumption on the amount of physical memory per node. This machine model is a very crude approximation that is designed specifically to illustrate the cost of the dominant factors to our particular case. More realistic models are described for example in <ref> [67] </ref> and the references therein. 94 4.3 Estimation of the Machine Parameters Two DMCCs, namely the Intel XP/S Paragon and the IBM Scalable POWER-parallel System, have been used in the experiments that have been performed for this dissertation. Both of these DMCCs differ in many aspects from the machine model. <p> Thus, we choose to perform this operation on two process columns or rows in order to limit the link contention. The message scheduling policy used in our one-dimensional redistribution operation is the "caterpillar" algorithm, where the messages are exchanged by pairs [78]. Other scheduling policies exist <ref> [67, 86] </ref>. These methods, however, do not feature a contention-free message scheduling policy as well as an optimal communication volume. Due to the simplicity of our machine model, it is not possible to correctly model the link contention of this redistribution operations. <p> The scheduling policies, however, are not contention free. Developing and/or characterizing such 158 policies for static and dynamic networks is a research area on its own. References on the subject can be found in <ref> [32, 67] </ref>. More recent research work on this topic can be found in [78, 86]. These problems are complex and difficult to address. The experimental results presented in the literature are usually machine dependent. They are often based on empirical trials or heuristics.
Reference: [68] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic Linear Algebra Subprograms for Fortran Usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: The BLAS are subdivided in three levels, each of which offering increased scope for exploiting parallelism. This classification criterion happens to also correspond 58 to three different kinds of basic linear algebra operations: * Level 1 BLAS <ref> [68] </ref>: for vector operations, such as y ffx + y, * Level 2 BLAS [36]: for matrix-vector operations, such as y ffAx + fiy, * Level 3 BLAS [35]: for matrix-matrix operations, such as C ffAB + fiC.
Reference: [69] <author> G. Li and T. Coleman. </author> <title> A Parallel Triangular Solver for a Distributed-Memory Multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 485-502, </pages> <year> 1988. </year>
Reference: [70] <author> G. Li and T. Coleman. </author> <title> A New Method for Solving Triangular Systems on Distributed-Memory Message-Passing Multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(2) </volume> <pages> 382-396, </pages> <year> 1989. </year>
Reference: [71] <author> J. Li and M. Chen. </author> <title> The Data Alignment Phase in Compiling Programs for Distributed-Memory Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year> <month> 172 </month>
Reference-contexts: Similarly, the problem of finding a set of alignments for the indices of multiple program arrays that minimizes data movement among processes is also NP-complete <ref> [71] </ref>. A number of heuristics for determining suitable distributions and alignments [71] have been proposed in the literature. In addition to these theoretical results, it is intuitively clear that a unique data distribution or alignment selection for an entire program may not be enough to achieve the best possible performance. <p> Similarly, the problem of finding a set of alignments for the indices of multiple program arrays that minimizes data movement among processes is also NP-complete <ref> [71] </ref>. A number of heuristics for determining suitable distributions and alignments [71] have been proposed in the literature. In addition to these theoretical results, it is intuitively clear that a unique data distribution or alignment selection for an entire program may not be enough to achieve the best possible performance.
Reference: [72] <author> W. Lichtenstein and S. L. Johnsson. </author> <title> Block-Cyclic Dense Linear Algebra. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 14(6) </volume> <pages> 1259-1288, </pages> <year> 1993. </year>
Reference-contexts: First, because the data decomposition largely determines the performance and scalability of a concurrent algorithm [21, 46, 48], a great deal of research [7, 12, 55] has aimed at determining optimal data distributions <ref> [27, 54, 57, 72] </ref>. This approach tacitly makes two assumptions worth reiterating about the user's data and the target architecture. First, the user's data may need to be redistributed to match this optimal distribution [6]. <p> As a result, the two-dimensional block cyclic distribution [67] has been suggested as a possible general purpose basic decomposition for parallel dense linear algebra libraries <ref> [27, 54, 72] </ref> due to its scalability [38], load balance and communication [54] properties. The purpose of this chapter is to present and define the two-dimensional block cyclic data distribution. The contributions of this chapter are two-fold. <p> First, the data is allocated or distributed in consecutive order, i.e., according to the blocked distribution defined in (2.2.6). Second, the computation proceeds in cyclic fashion. This approach is called cyclic ordering. It has been used throughout the CMSSL library [81]. It is shown in <ref> [72] </ref> that block cyclic order elimination can be used effectively on distributed memory architectures to achieve load balance as an alternative to block cyclic data allocation. The Connection Machine system compilers were designed to use consecutive data allocation as a default, or blocked data distribution as defined in (2.2.6). <p> It has been therefore necessary to develop these cyclic elimination algorithms, without being able to reuse much of the existing software. Nevertheless, this approach has been proven to lead to 76 efficient and scalable parallel algorithms <ref> [72] </ref>. The rank-K update operation [74] can easily be expressed within such a framework. The matrices A, B and C are assumed to be allocated in consecutive order over the process grid, that is blocked distributed.
Reference: [73] <author> M. Mace. </author> <title> Memory Storage Patterns in Parallel Processing, </title> <year> 1987. </year>
Reference-contexts: As a matter of fact, the problem of 9 determining an optimal data distribution for one- or two-dimensional arrays has been proven to be NP-complete <ref> [73] </ref>. Similarly, the problem of finding a set of alignments for the indices of multiple program arrays that minimizes data movement among processes is also NP-complete [71]. A number of heuristics for determining suitable distributions and alignments [71] have been proposed in the literature.
Reference: [74] <author> K. Mathur and S. L. Johnsson. </author> <title> Multiplication of Matrices of Arbitrary Shapes on a Data Parallel Computer. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 919-951, </pages> <year> 1994. </year>
Reference-contexts: One answer is given by the physical blocking approach, where the distribution blocking factors are used as computational blocking units, hence inducing alignment restrictions on the operands. Most of the parallel algorithms proposed in the literature are physically blocked <ref> [25, 26, 74, 83] </ref>. High performance is achievable on a wide range of DMCCs, but usually depends on the distribution blocking factors. The alignment restrictions simplify the expression and implementation of these algorithms, but also limit their application scope in a way that does not satisfy general purpose library requirements. <p> It has been therefore necessary to develop these cyclic elimination algorithms, without being able to reuse much of the existing software. Nevertheless, this approach has been proven to lead to 76 efficient and scalable parallel algorithms [72]. The rank-K update operation <ref> [74] </ref> can easily be expressed within such a framework. The matrices A, B and C are assumed to be allocated in consecutive order over the process grid, that is blocked distributed. <p> These source processes change at every iteration of the loop. High performance and efficiency can still be achieved for a wide range of different values of the blocking factors. This has been reported in <ref> [2, 38, 74, 83] </ref>. The use of physical blocking in conjunction with static blocking can lead to a comprehensive and scalable dense linear algebra software library. Existing serial software such as LAPACK [5] can be reused. The ScaLAPACK software library is the result of this reasoning.
Reference: [75] <author> P. Mehrotra and J. Rosendale. </author> <title> Programming Distributed Memory Architectures Using Kali. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: This last issue has been considerably simplified by the recent development of the Message Passing Interface (MPI) [43] and its adoption by a large majority of machine vendors. Most of the Fortran- and C-based data parallel languages incorporate explicit and implicit redistribution capabilities. The Kali language <ref> [75] </ref> was one of the first to do so. DINO [79] addresses the implicit redistribution of data at procedure boundaries. The Hypertasking compiler [8] for data parallel C programs, Vienna Fortran [14] and Fortran D [50] additionally specify data redistribution primitives.
Reference: [76] <institution> Office of Science and Technology Policy, editors. A Research and Development Strategy for High Performance Computing. Executive Office of the President, </institution> <year> 1987. </year>
Reference-contexts: Les deux derniers dispensent assez bien du premier. Charles Maurice de Talleyrand (1754-1838) In the past several years, the emergence of Distributed Memory Concurrent Computers (DMCCs) and their potential for the numerical solution of Grand Challenge problems <ref> [28, 62, 76, 77] </ref> has led to extensive research. As a result, DMCCs have become not only indispensable machines for large-scale engineering and scientific applications, but also common and viable platforms for commercial and financial applications.
Reference: [77] <institution> Office of Science and Technology Policy, editors. The Federal High Performance Computing Program. Executive Office of the President, </institution> <year> 1989. </year>
Reference-contexts: Les deux derniers dispensent assez bien du premier. Charles Maurice de Talleyrand (1754-1838) In the past several years, the emergence of Distributed Memory Concurrent Computers (DMCCs) and their potential for the numerical solution of Grand Challenge problems <ref> [28, 62, 76, 77] </ref> has led to extensive research. As a result, DMCCs have become not only indispensable machines for large-scale engineering and scientific applications, but also common and viable platforms for commercial and financial applications.
Reference: [78] <author> L. Prylli and B. Tourancheau. </author> <title> Efficient Block-Cyclic Data Redistribution. </title> <type> Technical Report 2766, </type> <institution> INRIA, Rhone-Alpes, </institution> <year> 1996. </year>
Reference-contexts: High performance can be maintained across platforms by parameterizing the user's data distribution or across library function calls by using general redistribution packages <ref> [78] </ref>. The purpose of this chapter is to propose alternatives to the physical blocking strategy. The originality of the algorithms presented here is their systematic derivation from the properties of the underlying mapping. These blocking strategies are expressed within a single framework using LCM tables. <p> Thus, we choose to perform this operation on two process columns or rows in order to limit the link contention. The message scheduling policy used in our one-dimensional redistribution operation is the "caterpillar" algorithm, where the messages are exchanged by pairs <ref> [78] </ref>. Other scheduling policies exist [67, 86]. These methods, however, do not feature a contention-free message scheduling policy as well as an optimal communication volume. Due to the simplicity of our machine model, it is not possible to correctly model the link contention of this redistribution operations. <p> However, the number of messages exchanged can be much larger than the minimal number (p). 4.4.5 Two Dimensional Redistribution The last redistribution strategy considered in this dissertation involves the complete redistribution of the matrix operands A and B beforehand. This operation has been implemented in the ScaLAPACK library <ref> [78] </ref>. This two-dimensional redistribution software was used for our experiments. The algorithm implemented features a minimal communication volume. The message scheduling policy is the "caterpillar" algorithm. This scheme is not contention free. Therefore, the estimated execution time given below should be regarded as a lower bound. <p> Finally, four different blocking and redistribution strategies were studied. To perform the complete redistribution (RED) of a two-dimensional block cyclically distributed matrix into another matrix of the same kind, we used the appropriate component of the ScaLAPACK software library <ref> [78] </ref>. The rest of the software used in these experiments has been 144 developed for this dissertation. The results presented in this chapter clearly show that for the aligned experiments on both platforms, it is legitimate to use algorithmic redistribution variants. <p> The scheduling policies, however, are not contention free. Developing and/or characterizing such 158 policies for static and dynamic networks is a research area on its own. References on the subject can be found in [32, 67]. More recent research work on this topic can be found in <ref> [78, 86] </ref>. These problems are complex and difficult to address. The experimental results presented in the literature are usually machine dependent. They are often based on empirical trials or heuristics. A comprehensive and detailed comparative summary of the known scheduling policies could not be found in the literature.
Reference: [79] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO Parallel Programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 30-42, </pages> <year> 1991. </year> <month> 173 </month>
Reference-contexts: Most of the Fortran- and C-based data parallel languages incorporate explicit and implicit redistribution capabilities. The Kali language [75] was one of the first to do so. DINO <ref> [79] </ref> addresses the implicit redistribution of data at procedure boundaries. The Hypertasking compiler [8] for data parallel C programs, Vienna Fortran [14] and Fortran D [50] additionally specify data redistribution primitives.
Reference: [80] <author> C. Stunkel, D. Shea, B. Abali, M. Atkins, C. Bender, D. Grice, P. Hochshild, D. Joseph, B. Nathanson, R. Swetz, R. Stucke, M. Tsao, and P. Varker. </author> <title> The SP2 High-Performance Switch. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: The clock speed of this processor is 66.7 MHz, giving a peak performance per processor of 266 MFLOPS. There are two types of nodes, known as thin nodes and wide nodes. Thin nodes have a 64 KB data cache. Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. <p> Wide nodes have 256 KB data cache <ref> [4, 30, 80] </ref>. This data cache size difference results in slower performance on thin nodes for computationally intensive applications. The machine used for our experiments consisted of thin nodes exclusively having 128 MB of physical memory [4, 30, 80]. Figure 4.4 shows the thin node performance of the vendor 99 supplied rank-K update library routine for distinct values of M = N and K. In practice, the performance of such an operation was observed to be at most 200 Mflops for those thin nodes. <p> The TB2 switch adapter, which is the interface between the node and the switch, features a Direct Memory Access (DMA) engine. For message passing libraries optimized for the switch, the typical bandwidth is 35 MB/s with a latency of approximately 50 s <ref> [4, 30, 80] </ref>. 100 During our experiments, the performance of the BLACS communication prim-itives implemented on top of the native IBM message passing library (MPL) was measured.
Reference: [81] <institution> Thinking Machines Corporation. CMSSL for Fortran, </institution> <year> 1990. </year>
Reference-contexts: First, the data is allocated or distributed in consecutive order, i.e., according to the blocked distribution defined in (2.2.6). Second, the computation proceeds in cyclic fashion. This approach is called cyclic ordering. It has been used throughout the CMSSL library <ref> [81] </ref>. It is shown in [72] that block cyclic order elimination can be used effectively on distributed memory architectures to achieve load balance as an alternative to block cyclic data allocation.
Reference: [82] <author> A. Thirumalai and J. Ramanujam. </author> <title> Fast Address Sequence Generation for Data Parallel Programs Using Integer Lattices. </title> <editor> In P. Sadayappan and al., editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: The approach of solving a set of linear Diophantine equations to determine index sets, process sets and so on is recommended by data parallel compiler designers as one way to proceed <ref> [15, 56, 64, 82] </ref>. Binary algorithms are available [65] to solve these equations. This method is thus very general and relatively inexpensive in terms of time. Still, this approach is the most powerful and expensive method in terms of memory requirements.
Reference: [83] <author> R. van de Geijn and J. Watts. SUMMA: </author> <title> Scalable Universal Matrix Multiplication Algorithm. </title> <type> Technical Report UT CS-95-286, </type> <note> LAPACK Working Note #96, </note> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: One answer is given by the physical blocking approach, where the distribution blocking factors are used as computational blocking units, hence inducing alignment restrictions on the operands. Most of the parallel algorithms proposed in the literature are physically blocked <ref> [25, 26, 74, 83] </ref>. High performance is achievable on a wide range of DMCCs, but usually depends on the distribution blocking factors. The alignment restrictions simplify the expression and implementation of these algorithms, but also limit their application scope in a way that does not satisfy general purpose library requirements. <p> These source processes change at every iteration of the loop. High performance and efficiency can still be achieved for a wide range of different values of the blocking factors. This has been reported in <ref> [2, 38, 74, 83] </ref>. The use of physical blocking in conjunction with static blocking can lead to a comprehensive and scalable dense linear algebra software library. Existing serial software such as LAPACK [5] can be reused. The ScaLAPACK software library is the result of this reasoning. <p> For a scalable algorithm with n=p held fixed, we expect the performance to be proportional to p. 4.4.1 Physical Blocking In this section, the performance analysis of the physical blocking strategy for aligned operands is presented. A similar analysis can also be found in <ref> [2, 83] </ref>. The reason for reproducing it hereafter is that it considerably simplifies the presentation of the performance analysis for the aggregation and LCM blocking strategies. For the sake of simplicity, the underlying process grid is assumed to be a p p p square mesh of p processes.
Reference: [84] <author> E. van de Velde. </author> <title> Data Redistribution and Concurrency. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 125-138, </pages> <year> 1990. </year>
Reference-contexts: These results have motivated this second approach where the user's decomposition is generally not changed but passed as an argument and a suboptimal algorithm is used. This approach is usually referred to as decomposition independent <ref> [41, 84] </ref>. The suboptimality of a routine must be weighted against the possibly large cost of redistributing the input data. Finally, the most potentially ambitious approach attempts to provide high-level language support for data parallel programming.
Reference: [85] <author> E. van de Velde. </author> <title> Experiments with Multicomputer LU-Decomposition. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2 </volume> <pages> 1-26, </pages> <year> 1990. </year>
Reference: [86] <author> D. Walker and S. Otto. </author> <title> Redistribution of Block-Cyclic Data Distributions Using MPI. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(9) </volume> <pages> 707-728, </pages> <year> 1996. </year> <month> 174 </month>
Reference-contexts: Thus, we choose to perform this operation on two process columns or rows in order to limit the link contention. The message scheduling policy used in our one-dimensional redistribution operation is the "caterpillar" algorithm, where the messages are exchanged by pairs [78]. Other scheduling policies exist <ref> [67, 86] </ref>. These methods, however, do not feature a contention-free message scheduling policy as well as an optimal communication volume. Due to the simplicity of our machine model, it is not possible to correctly model the link contention of this redistribution operations. <p> The scheduling policies, however, are not contention free. Developing and/or characterizing such 158 policies for static and dynamic networks is a research area on its own. References on the subject can be found in [32, 67]. More recent research work on this topic can be found in <ref> [78, 86] </ref>. These problems are complex and difficult to address. The experimental results presented in the literature are usually machine dependent. They are often based on empirical trials or heuristics. A comprehensive and detailed comparative summary of the known scheduling policies could not be found in the literature.
Reference: [87] <author> R. C. Whaley. </author> <title> Basic Linear Algebra Communication Subprograms: Analysis and Implementation Across Multiple Parallel Architectures. </title> <type> Technical Report UT CS-94-234, </type> <note> LAPACK Working Note #73, </note> <institution> University of Tennessee, </institution> <year> 1994. </year>
Reference-contexts: The overhead induced by the BLACS primitives on this system compared to the native Intel message passing library is negligible as shown earlier in <ref> [87] </ref>. 4.3.2 The IBM Scalable POWERparallel System The IBM Scalable POWERparallel System, or SP, consists of nodes (processors with associated memory and disk) connected by ethernet and a high-performance switch. <p> Comparing the BLACS performance versus the native IBM message passing library requires a more detailed explanation and can be found in <ref> [87] </ref>. 4.4 Performance Analysis In this section the machine model defined above is applied in turn to each blocking strategy presented in Chapter 3. The three matrix operands A, B and C are considered to be square of order M = N = K.
Reference: [88] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran: A Language Specification (Version 1.1), </title> <booktitle> 1991. </booktitle> <pages> 175 </pages>
Reference-contexts: Finally, the most potentially ambitious approach attempts to provide high-level language support for data parallel programming. In the last few years, several data parallel Fortran languages have been proposed, such as Fortran D [45] and Vienna Fortran <ref> [88] </ref>. More recently, the High Performance Fortran (HPF) language [66] has been developed as a community effort to standardize data parallel Fortran programming for DMCCs. HPF includes many of the concepts originally proposed in 10 Fortran D, Vienna Fortran, and other data parallel Fortran languages.
References-found: 89

