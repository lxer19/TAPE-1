URL: http://www.first.gmd.de/promoter/papers/RWCPS97.ps.gz
Refering-URL: http://www.first.gmd.de/promoter/papers/index.html
Root-URL: 
Email: email: &lt;giloi@first.gmd.de&gt;  
Title: A High-Level, Massively Parallel Programming Environment and Its Realization  
Author: Wolfgang K. Giloi, Matthias Kessler, Andreas Schramm 
Address: Rudower Chaussee 5, 12489 Berlin, Germany,  
Affiliation: RWCP Massively Parallel Computing Laboratory at the GMD Research Institute for Computer Architecture and Software Technology  
Abstract: Massively parallel computers are necessarily distributed memory architectures (DMA), as only DMAs offer unlimited upward scalability. However, the usual program - ming model for DMAs, the message-passing paradigm , is too difficult to make such machines acceptable for a larger community of users. Therefore, easier high-level program - ming models are needed, in conjunction with paralleliz ing compilers. This will allow the user to formulate a parallel program in terms of applica tion-specific concepts, while low-level issues such as optimal data distribution and coordination of the parallel threads is handled by the compiler. Specifically, the model must provide expressive means for defining explicitly all kinds of application-specific data structures, array-like or hierarchical, static or dynamic, regular or irregular. The paper describes such a model and pro gramming system, called PROMOTER, and its under lying concepts. Experiences with a first imple - mentation of PROMOTER are communicated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> High Performance Fortran Forum: </author> <title> High Perform - ance Fortran Language Specifi cation, </title> <type> Version 1.0, </type> <institution> Rice University, </institution> <address> Houston, TX (May 1993) </address>
Reference-contexts: A step in the right direction has been the creation of data parallel languages and compilers for them, the best-known example being High Performance Fortran (HPF) <ref> [1] </ref>. HPF provides the user with control over data alignment yet hides the concrete communication activities from the programmer. Parallel processing takes the form of loop parallelization .
Reference: [2] <author> Giloi W.K., Schramm A.: </author> <title> PROMOTER, An Application-Oriented Programming Model for Massive Parallelism, in Giloi W.K., </title> <editor> Jaehnichen S., Shriver B.D.(eds.): </editor> <title> Programming Models for Massively Parallel Computers, </title> <publisher> IEEE-CS Press order no. </publisher> <month> 4900-02 (Sept. </month> <year> 1993) </year>
Reference: [3] <author> Giloi, W.K., Kessler, M., Schramm, A.: </author> <title> "PROMOTER: A High Level, </title> <booktitle> Object-Paral lel Programming Language, Proc. Internat. Conf. on High Performance Computing, </booktitle> <publisher> McGraw-Hill Publishing Co., </publisher> <address> New Delhi, India 1995 </address>
Reference: [4] <author> Schramm A.: </author> <title> PROMOTER: A Program ming Model for Massive Parallelism, </title> <note> to be published </note>
Reference-contexts: structure objects are not restricted to arrays but may readily be trees or other hierarchic structures. (iii) Parallel computation means to process all the elements of the potentially large data structures in one step. 3.2 The Structured Universe PROMOTER satisfies the requirements postulated above by its novel structured universe approach <ref> [4] </ref>. In this approach, domains of computation are created as finite, possibly irregular and dynamic substructures of a regular, static, possibly infinite universe. A universe is an index domain with group property. There are two ends of the spectrum: Abelian groups for arrays and free groups for trees.
Reference: [5] <author> Enskonatus P., Kessler M., Schramm A.: </author> <title> PROMOTER Language Definition, </title> <type> GMD FIRST Tech. </type> <note> Report 1996 </note>
Reference-contexts: This has been imple mented in the form of extensions to the objectoriented language C++, coded in MPC++ <ref> [5] </ref>, an approach that avoids the acceptance problems usually encount ered with a new language design.
Reference: [6] <author> Garey M., Johnson D., Stockmeyer L.: </author> <title> Some simplified NP-complete graph problems, </title> <booktitle> Theoretical Computer Science, no.1, </booktitle> <pages> 237-267, </pages> <year> 1976 </year>
Reference-contexts: The condition for an optimized mapping is twofold: ( i) the computational load should be approxi mately the same on all nodes and (ii) the data movement needed in the observation of state approach should be minimized. Mapping is an NP-complete optimization problem <ref> [6] </ref>. Consequently, one must resort to appropriate heuristics. Dynamic data structures may have to be remapped during program execution; thus, the speed of the mapping algorithm is very important. Several mapping heuristics were tested as to their suitability, such as the Kernighan-Lin algorithm [7] and Recursive Spectral Bisection [8].
Reference: [7] <author> Kernighan B.W., Lin S.: </author> <title> An Efficient Heuristic Procedure for Partitioning Graphs, </title> <journal> The Bell System Technical Journal, </journal> <month> Feb. </month> <year> 1970, </year> <pages> 291-307 </pages>
Reference-contexts: Mapping is an NP-complete optimization problem [6]. Consequently, one must resort to appropriate heuristics. Dynamic data structures may have to be remapped during program execution; thus, the speed of the mapping algorithm is very important. Several mapping heuristics were tested as to their suitability, such as the Kernighan-Lin algorithm <ref> [7] </ref> and Recursive Spectral Bisection [8]. In addition, a new mapping algorithm has been developed for PROMOTER, called balanced hypersphere tessellation (BHT) [9]. BHT is a generali zation of vector quanti zation and proved to work well for the partitioning of irregular domains of arbitrary dimensions.
Reference: [8] <author> Pothen A., Simon H.D., </author> <title> Liou K.P.: Partitioning Sparse Matrices With Eigenvectors of Graphs, </title> <journal> SIAM J. Matrix Anal. </journal> <volume> 11 (1990), </volume> <pages> 430-452 </pages>
Reference-contexts: Consequently, one must resort to appropriate heuristics. Dynamic data structures may have to be remapped during program execution; thus, the speed of the mapping algorithm is very important. Several mapping heuristics were tested as to their suitability, such as the Kernighan-Lin algorithm [7] and Recursive Spectral Bisection <ref> [8] </ref>. In addition, a new mapping algorithm has been developed for PROMOTER, called balanced hypersphere tessellation (BHT) [9]. BHT is a generali zation of vector quanti zation and proved to work well for the partitioning of irregular domains of arbitrary dimensions.
Reference: [9] <author> Besch M., Pohl H.W.: </author> <title> Topographic Data Mapping by Balanced Hypersphere Tessel lation, </title> <note> to be published </note>
Reference-contexts: Several mapping heuristics were tested as to their suitability, such as the Kernighan-Lin algorithm [7] and Recursive Spectral Bisection [8]. In addition, a new mapping algorithm has been developed for PROMOTER, called balanced hypersphere tessellation (BHT) <ref> [9] </ref>. BHT is a generali zation of vector quanti zation and proved to work well for the partitioning of irregular domains of arbitrary dimensions.
Reference: [10] <author> Giloi W.K., Bruening U., Schroeder-Preikschat W.: MANNA: </author> <title> Prototype of a Distributed Memory Architecture With Maximized Sustained Performance, </title> <booktitle> Proc. Euromicro PDP96 Workshop 1996, </booktitle> <publisher> IEEE-CS Press </publisher>
Reference-contexts: The executing platform is the MANNA super - computer. Note that PVM on MANNA is not a port but a (highly optimized) implementation that takes advantage of the very low communication latency of MANNA <ref> [10] </ref>. The benchmarks are: 128*128 matrix multipli cation, 512*512 Jacobi relaxation, and 1000*1000 conjugate gradient method. Tpro is the time for the PROMOTER version and Tpvm the time for the PVM version (in seconds).
Reference: [11] <author> Anonymous: </author> <title> The Master Plan for the Real-World Computing Program (DRAFT of May 1992) </title>
Reference-contexts: Benchmark Execution Time Tpro/Tpvm (sec.) 4P 8 P 16 P Matrix Mult. 128*128 .291/.246 .155/.137 .088/.083 Relaxation 512*512 .280/.261 .137/.132 .075/.072 Conj. grad. 1000*1000 8.23/8.21 4.12/4.11 2.36/2.34 Table 1 Benchmark comparison Promoter - PVM 6.3 Contributions to the Goals of RWCP The Master Plan for the Real World Computing Program <ref> [11] </ref> postulated in 1992 that flexible execution models, which can be bases of general-purpose architec tures, should have the ability to fill the gaps between the language models and hardware.
References-found: 11

