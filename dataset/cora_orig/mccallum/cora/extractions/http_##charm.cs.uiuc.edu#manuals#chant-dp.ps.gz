URL: http://charm.cs.uiuc.edu/manuals/chant-dp.ps.gz
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Email: haines@meru.uwyo.edu  [pm,cronk]@icase.edu  
Title: Data Parallel Programming in a Multithreaded Environment  
Author: Matthew Haines Piyush Mehrotra David Cronk 
Note: Research supported by the National Aeronautics and Space Administration under NASA Contract No. NASA 19480, while the authors were in residence  
Address: 82071-3682  Mail Stop 132C Hampton, VA 23681-0001  Hampton, VA 23681.  
Affiliation: Computer Science Department University of Wyoming Laramie, WY  Institute for Computer Applications in Science and Engineering NASA Langley Research Center,  at ICASE, NASA Langley Research Center,  
Abstract: Research on programming distributed memory multiprocessors has resulted in a well-understood programming model, namely data parallel programming. However, data parallel programming in a multi-threaded environment is far less understood. For example, if multiple threads within the same process may belong to different data parallel computations, then the architecture, compiler, or runtime system must ensure that relative indexing and collective operations are handled properly and efficiently. We introduce a runtime-based solution for data parallel programming in a distributed memory environment that handles the problems of relative indexing and collective communications among thread groups. As a result, the data parallel programming model can now be executed in a multithreaded environment, such as a system using threads to support both task and data parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Henri E. Bal, M. Frans Kaashoek, and Andrew S. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Lightweight, user-level threads are becoming increasingly useful for supporting parallelism and asynchronous events in applications and language implementations. In particular, many 2 recent languages for parallel and distributed computing employ lightweight threads to represent functional parallelism, to overlap computations with communications, or to simplify resource management <ref> [1, 7, 13, 15] </ref>. In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support [4, 8, 9], and a committee has been formed to establish standard interfaces for such a runtime system [17].
Reference: [2] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <institution> Technical Report Rice COMP TR89-98, Rice University, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Figure 5 depicts the data structure for the local rope list, including the rope translation table. Again, borrowing from earlier work in area of page coherence for distributed shared memory systems <ref> [2] </ref>, we adopt two options for keeping the distributed translation tables consistent: new information is broadcast so that all tables are kept up-to-date at all times (strong consistency), or tables are allowed to remain out-of-date until a reference for a thread is generated, causing the information to be retrieved and stored
Reference: [3] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <institution> Technical Report Rice COMP TR90-109, Rice University, </institution> <month> March </month> <year> 1990. </year> <note> Appears in Proceedings of ISCA 17. </note>
Reference-contexts: Therefore, our initial design for a name server utilizes a two-level approach, as depicted in Figure 4, consisting of a single global name server and a number of local name servers. This design is derived from the idea of two-level page management schemes used in distributed shared memory systems <ref> [3] </ref>. The operation of the rope name server mechanism proceeds in two levels as follows: 7 1 n Rope Entry Consistency Requirement Context List Local Thread List Rope Server Identifier Local Rope List Rope Translation Table 1.
Reference: [4] <author> Raoul Bhoedjang, Tim Ruhl, Rutger Hofman, Koen Langendoen, Henri Bal, and Frans Kaashoek. Panda: </author> <title> A portable platform to support parallel programming languages. </title> <booktitle> In Symposium on Experiences with Distributed and Multiprocessor Systems IV, </booktitle> <pages> pages 213-226, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support <ref> [4, 8, 9] </ref>, and a committee has been formed to establish standard interfaces for such a runtime system [17]. In this paper we describe a runtime solution to the problem of supporting data parallel execution using lightweight threads as the data parallel agents. <p> threads mapped to the same processor, the gap would be zero and thus the execution times the ropes in parallel would be the same as the execution times for the ropes in sequence. 6 Related Research There are several systems which support distributed threads, such as Nexus [8], and Panda <ref> [4] </ref>, though these systems do not currently support the notion of ropes.
Reference: [5] <author> Aswini K. Chowdappa, Anthony Skjellum, and Nathan E. Doss. </author> <title> Thread-safe message passing with P4 and MPI. </title> <type> Technical Report TR-CS-941025, </type> <institution> Computer Science Department and NSF Engineering Research Center, Mississippi State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: MPI does not currently support the notion of threads as addressable entities within a process, nor the ability to group such threads. However, there has been a lot of recent research activity combining MPI and threads. Besides Chant, which was outlined in Section 2, other projects include <ref> [5] </ref>, which addresses the issue of making MPI (using P4) "thread-safe" with respect to internal, worker threads designed to improve the efficiency of MPI, but not intended to be user-accessible entities (i.e., they cannot execute user code); and [19], which addresses many possible extensions to the MPI standard, including the addition
Reference: [6] <author> Message Passing Interface Forum. </author> <title> Document for a Standard Message Passing Interface, </title> <note> version 1.1 edition, June 1994. http://www.mcs.anl.gov/mpi/. </note>
Reference-contexts: Collective operations are typically supported at the process level by the underlying communication system [12], or by standard communication interfaces <ref> [6, 21] </ref>. For example, MPI [6] provides a mechanism for process scoping called process groups. <p> Collective operations are typically supported at the process level by the underlying communication system [12], or by standard communication interfaces [6, 21]. For example, MPI <ref> [6] </ref> provides a mechanism for process scoping called process groups. <p> Thus, the interaction of pthreads in a distributed environment is undefined. Likewise, the Message Passing Interface Forum (MPI) has recently established a standard for communication between processes <ref> [6] </ref>. Although various extensions to the standard have already been proposed [18, 19], communication between lightweight threads within processes has yet to be supported by MPI. Therefore, Chant was designed to provide a simple mechanism for combining lightweight threads with interprocessor communication. <p> A rope is the thread-level analogy to the process-level scoping mechanisms provided by most communication packages, such as process groups in MPI <ref> [6] </ref>. MPI does not currently support the notion of threads as addressable entities within a process, nor the ability to group such threads. However, there has been a lot of recent research activity combining MPI and threads.
Reference: [7] <author> I. T. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <type> Technical Report MCS-P327-0992 Revision 1, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Lightweight, user-level threads are becoming increasingly useful for supporting parallelism and asynchronous events in applications and language implementations. In particular, many 2 recent languages for parallel and distributed computing employ lightweight threads to represent functional parallelism, to overlap computations with communications, or to simplify resource management <ref> [1, 7, 13, 15] </ref>. In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support [4, 8, 9], and a committee has been formed to establish standard interfaces for such a runtime system [17].
Reference: [8] <author> Ian Foster, Carl Kesselman, Robert Olson, and Steven Tuecke. </author> <title> Nexus: An interoperability layer for parallel and distributed computer systems. </title> <type> Technical Report Version 1.3, </type> <institution> Argonne National Labs, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support <ref> [4, 8, 9] </ref>, and a committee has been formed to establish standard interfaces for such a runtime system [17]. In this paper we describe a runtime solution to the problem of supporting data parallel execution using lightweight threads as the data parallel agents. <p> Were both long-running threads mapped to the same processor, the gap would be zero and thus the execution times the ropes in parallel would be the same as the execution times for the ropes in sequence. 6 Related Research There are several systems which support distributed threads, such as Nexus <ref> [8] </ref>, and Panda [4], though these systems do not currently support the notion of ropes.
Reference: [9] <author> Matthew Haines, David Cronk, and Piyush Mehrotra. </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <pages> pages 350-359, </pages> <address> Washington D.C., </address> <month> November </month> <year> 1994. </year> <month> ACM/IEEE. </month>
Reference-contexts: In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support <ref> [4, 8, 9] </ref>, and a committee has been formed to establish standard interfaces for such a runtime system [17]. In this paper we describe a runtime solution to the problem of supporting data parallel execution using lightweight threads as the data parallel agents. <p> We describe our design for data parallel support among lightweight threads in the context of Chant <ref> [9] </ref>, a runtime system which supports both intra- and inter-processor communication between lightweight threads in a distributed system. However, the design issues we present are applicable to any thread-based runtime system that supports some form of communication between threads. <p> Although various extensions to the standard have already been proposed [18, 19], communication between lightweight threads within processes has yet to be supported by MPI. Therefore, Chant was designed to provide a simple mechanism for combining lightweight threads with interprocessor communication. Chant <ref> [9] </ref> is designed as a layered system (as shown in Figure 1), where efficient point-to-point communication provides the basis for implementing remote service requests and, in turn, remote thread operations. Each layer is accessible to the user so that the proper amount of support and performance can be obtained.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> version 1.1 edition, </note> <month> November </month> <year> 1994. </year> <note> http://www.erc.msstate.edu/hpff/home.html. </note>
Reference-contexts: A (N) real X !HPF$ distribute A (block) onto P ... forall (I=1,N-1) A (I) = A (I+1) X = A (1) In this section we address the issues of interfacing with ropes from the perspective of a data parallel compiler, such as a compiler for High Performance Fortran (HPF) <ref> [10] </ref>, targeting a multithreaded system, such as Chant. For reference, Figure 6 gives the interface for the rope calls as currently implemented in Chant.
Reference: [11] <author> IEEE. </author> <title> Standard for Threads Interface to POSIX, </title> <year> 1996. </year>
Reference-contexts: Section 6 outlines related research projects, and we conclude in Section 7. 2 Chant The Portable Operating System Interfaces for Computer Environments (POSIX) committee has recently established a standard for the interface and functionality of lightweight threads within an operating system process, called pthreads <ref> [11] </ref>. Since threads are defined within the context of a process, they share a single address space, and communication among threads is only defined in terms of shared memory primitives, such as events and locks. Thus, the interaction of pthreads in a distributed environment is undefined.
Reference: [12] <institution> Intel Corporation, Beaverton, OR. </institution> <note> Paragon OSF/1 User's Guide, </note> <month> April </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Also, with proper support for mapping threads to processes, and processes to processors, relative indexing can be used to optimize performance by ensuring that an algorithm is correctly mapped onto the underlying communication topology. Collective operations are typically supported at the process level by the underlying communication system <ref> [12] </ref>, or by standard communication interfaces [6, 21]. For example, MPI [6] provides a mechanism for process scoping called process groups.
Reference: [13] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object oriented parallel programming experiments and results. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pages 273-282, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Lightweight, user-level threads are becoming increasingly useful for supporting parallelism and asynchronous events in applications and language implementations. In particular, many 2 recent languages for parallel and distributed computing employ lightweight threads to represent functional parallelism, to overlap computations with communications, or to simplify resource management <ref> [1, 7, 13, 15] </ref>. In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support [4, 8, 9], and a committee has been formed to establish standard interfaces for such a runtime system [17].
Reference: [14] <author> Mamoru Maekawa. </author> <title> A p N algorithm for mutual exclusion in decentralized systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 145-159, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: While distributed algorithms for name servers [16] and atomic operations <ref> [14] </ref> are well known, their added overhead and implementation complexity are often unwarranted in an initial design. However, a completely centralized solution for naming and updating ropes will certainly cause hot-spots.
Reference: [15] <author> Piyush Mehrotra and Matthew Haines. </author> <title> An overview of the Opus language and runtime system. </title> <booktitle> In Languages and Compilers for Parallel Computers, </booktitle> <pages> pages 346-360. </pages> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <volume> Vol 892, </volume> <year> 1995. </year>
Reference-contexts: Lightweight, user-level threads are becoming increasingly useful for supporting parallelism and asynchronous events in applications and language implementations. In particular, many 2 recent languages for parallel and distributed computing employ lightweight threads to represent functional parallelism, to overlap computations with communications, or to simplify resource management <ref> [1, 7, 13, 15] </ref>. In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support [4, 8, 9], and a committee has been formed to establish standard interfaces for such a runtime system [17].
Reference: [16] <author> R.M. Needham. </author> <title> Names. In Sape Mullender, editor, </title> <journal> Distributed systems, </journal> <volume> chapter 5, </volume> <pages> pages 89-101. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: While distributed algorithms for name servers <ref> [16] </ref> and atomic operations [14] are well known, their added overhead and implementation complexity are often unwarranted in an initial design. However, a completely centralized solution for naming and updating ropes will certainly cause hot-spots.
Reference: [17] <institution> Portable runtime systems (ports) consortium. </institution> <note> http://www.cs.uoregon.edu:80/paracomp/ports/. </note>
Reference-contexts: In response to this increasing demand for parallel language support, several projects have emerged with the goal of providing standard lightweight thread support [4, 8, 9], and a committee has been formed to establish standard interfaces for such a runtime system <ref> [17] </ref>. In this paper we describe a runtime solution to the problem of supporting data parallel execution using lightweight threads as the data parallel agents. Thus we describe the ability to provide relative indexing and collective operations among a group of threads, called a rope.
Reference: [18] <author> Anthony Skjellum, Nathan E. Doss, and Kishore Viswanathan. </author> <title> Inter-communicator extensions to MPI in the MPIX (MPI eXtension) library. </title> <type> Technical report, </type> <institution> Computer Science Department and NSF Engineering Research Center, Mississippi State University, </institution> <month> July </month> <year> 1994. </year> <note> Submitted to ICAE Journal Special Issue on Distributed Computing. </note>
Reference-contexts: Thus, the interaction of pthreads in a distributed environment is undefined. Likewise, the Message Passing Interface Forum (MPI) has recently established a standard for communication between processes [6]. Although various extensions to the standard have already been proposed <ref> [18, 19] </ref>, communication between lightweight threads within processes has yet to be supported by MPI. Therefore, Chant was designed to provide a simple mechanism for combining lightweight threads with interprocessor communication.
Reference: [19] <author> Anthony Skjellum, Nathan E. Doss, Kishore Viswanathan, Aswini Chowdappa, and Pu-rushotham V. </author> <title> Bangalore. Extending the message passing interface (MPI). </title> <type> Technical report, </type> <institution> Computer Science Department and NSF Engineering Research Center, Mississippi State University, </institution> <year> 1994. </year>
Reference-contexts: Thus, the interaction of pthreads in a distributed environment is undefined. Likewise, the Message Passing Interface Forum (MPI) has recently established a standard for communication between processes [6]. Although various extensions to the standard have already been proposed <ref> [18, 19] </ref>, communication between lightweight threads within processes has yet to be supported by MPI. Therefore, Chant was designed to provide a simple mechanism for combining lightweight threads with interprocessor communication. <p> Besides Chant, which was outlined in Section 2, other projects include [5], which addresses the issue of making MPI (using P4) "thread-safe" with respect to internal, worker threads designed to improve the efficiency of MPI, but not intended to be user-accessible entities (i.e., they cannot execute user code); and <ref> [19] </ref>, which addresses many possible extensions to the MPI standard, including the addition of long-lived threads capable of executing user code. Suggestions for altering the role and functionality of communicators would allow for multiple threads per communicator, thus permitting collective operations among the threads.
Reference: [20] <author> Neelakantan Sundaresan and Linda Lee. </author> <title> An object-oriented thread model for parallel numerical applicaitons. </title> <booktitle> In Proceedings of the Second Annual Object-Oriented Numerics Conference, </booktitle> <pages> pages 291-308, </pages> <address> Sunriver, OR, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The term "rope" was first coined in the pthreads++ system <ref> [20] </ref>, in which a rope is a C++ class that provides support for data parallel execution of a task in a shared memory environment, and later extended to a distributed memory environment.
Reference: [21] <author> Vaidy Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year> <month> 24 </month>
Reference-contexts: Collective operations are typically supported at the process level by the underlying communication system [12], or by standard communication interfaces <ref> [6, 21] </ref>. For example, MPI [6] provides a mechanism for process scoping called process groups.
References-found: 21

