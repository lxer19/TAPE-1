URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-257.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: ALIVE System: Wireless, Full-body Interaction with Autonomous Agents  
Author: Pattie Maes, Trevor Darrell, Bruce Blumberg, Alex Pentland 
Keyword: Interactive Interface, Virtual Reality, Autonomous Agents, Com puter Vision.  
Address: 20 Ames Street Cambridge, MA 02139  
Affiliation: MIT Media Laboratory  
Note: The  
Email: pattie/trevor/bruce/sandy@media.mit.edu  
Phone: (617) 253-7442 (phone) (617) 258-6264 (fax)  
Date: Revised, November 1995  
Abstract: M.I.T. Media Laboratory Perceptual Computing Technical Report No. 257 (To appear, ACM Multimedia Systems) Abstract The cumbersomenature of wired interfaces often limits the rangeof application of virtual environments. In this paper we discuss the design and implementation of a novel system, called ALIVE, which allows unencumberedfull-body interaction between a human participant and a rich graphical world inhabited by autonomous agents. Based on results obtained with thousands of users, the paper argues that this kind of system can provide more complex and very different experiences than traditional virtual reality systems. The ALIVE system significantly broadens the range of potential applications of virtual reality systems; in particular, the paper discusses novel applications in the area of training and teaching, entertainment, and digital assistants or interface agents. We overview the methods used in the implementation of the exiting ALIVE systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aloimonos Y., ed., </author> <title> (1993) Active Perception, </title> <publisher> Lawrence Erlbaum Associates </publisher>
Reference: [2] <author> Badler N.I., Phillips C.B. and Webber B.L., </author> <title> (1993) Simulating Humans, Computer Graphics Animation and Control, </title> <publisher> Oxford University Press </publisher>
Reference-contexts: Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also [7, 33]). This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler <ref> [2] </ref> has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters. <p> This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler <ref> [2] </ref> has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters. Most recently Tu and Terzopoulos [31] have modeled autonomous animated fish.
Reference: [3] <author> Ballard, D., and Brown, C., </author> <title> (1982) Computer Vision, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood </address>
Reference-contexts: Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [3, 19] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3D points with their 2D projection.
Reference: [4] <author> Bates J., Altucher J., Hauptman A., Kantrowitz M., Loyall A.B., Murakami K., Olbrich P., Popovic Z., Reilly W.S., Sengers P., Welch W., Weyhrauch P. </author> <title> and Witkin A.,(1993) Edge of Intention, </title> <booktitle> SIGGRAPH-93 Visual Proceedings, Machine Culture, ACM SIGGRAPH, </booktitle> <pages> pp. 113-114. </pages>
Reference-contexts: A number of other researchers have focused on building systems in which animated agents interact, in realtime, with the user. Joe Bates' Woggle world combines behavior models with ideas from traditional animation to explore what he calls believable agents <ref> [4] </ref>. Bates' agents can have fairly complex interactions, despite the user interaction being limited to moving a mouse. Fisher's [17] Menagerie system allows a user to interact with animated agents in real-time using goggles.
Reference: [5] <author> Blumberg B., </author> <year> (1994), </year> <title> Action-Selection in Hamsterdam: Lessons from Ethology, </title> <booktitle> Proceedings of the Third International Conference on the Simulation of Adaptive Behavior, </booktitle> <publisher> MIT Press, </publisher> <address> Brighton, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The ALIVE system incorporates a behavior modeling tool kit for developing semi-intelligent autonomous agents and their interaction with one another and with the user [6] <ref> [5] </ref>. <p> Actions may be unavailable or unreliable. To successfully produce competent autonomous action over extended periods of time, the agent's behavior system must provide solutions to these problems, as well as others. See [6] and <ref> [5] </ref> for a detailed discussion of how our archictecture addresses these issues. When using the behavior tool kit to build an agent, the designer specifies: * The virtual sensors of the agent.
Reference: [6] <author> Blumberg, B and Tinsley Galyean. </author> <title> (1995) Multi-level Direction of Autonomous Creatures for Real-Time Virtual Environ ments. </title> <booktitle> Proceedings of SIGGRAPH 95 </booktitle> . 
Reference-contexts: The ALIVE system incorporates a behavior modeling tool kit for developing semi-intelligent autonomous agents and their interaction with one another and with the user <ref> [6] </ref> [5]. <p> Actions may be unavailable or unreliable. To successfully produce competent autonomous action over extended periods of time, the agent's behavior system must provide solutions to these problems, as well as others. See <ref> [6] </ref> and [5] for a detailed discussion of how our archictecture addresses these issues. When using the behavior tool kit to build an agent, the designer specifies: * The virtual sensors of the agent.
Reference: [7] <author> Broadwell, P, Myers, R., and Schaufler, R., </author> <year> (1985) </year> <month> Plasm: </month> <title> A Fish Sample, Sig-graph 85 Art Show, Siggraph Visual Proceedings, </title> <note> ACM Press. Also available as gold, IndyZone2 CDROM, </note> <month> Summer </month> <year> 1994, </year> <institution> Silicon Graphics Inc. </institution>
Reference-contexts: A number of researchers have taken an Artificial Intelligence approach to animation in which the animated agents perform some actions in response to their perceived environment. Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also <ref> [7, 33] </ref>). This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control.
Reference: [8] <author> Burden R. </author> <title> and Faires J.,(1989) Numerical Analysis, </title> <publisher> PWS-Kent Publishing Co. </publisher> <address> Boston. </address>
Reference: [9] <author> Casey, M., Gardner, W., and Basu, S., </author> <title> (1995) Vision Steered Beam-forming and Transaural Rendering for the Artificial Life Interactive Video Environment (ALIVE) . Proc. </title> <journal> Audio Eng. Soc. </journal> <note> Convention 1995. </note>
Reference-contexts: The ALIVE system creates a special 3D agent in the environment to represent the user. The position and state of that agent are based on the information computed by the vision system on the basis of the camera image of the user, as well as auditory cues <ref> [9] </ref>. Thus, the artificial agents can sense the user using the same virtual sensors that they use to detect objects and other agents.
Reference: [10] <author> Cruz-Neira, C., Sandin, D.J., DeFanti, T.A., Kenyon, R., and Hart, </author> <title> J.C., (1992) "The CAVE, Audio Visual Experience Automatic Virtual Environment", </title> <journal> Communications of the ACM, </journal> <month> June </month> <year> 1992, </year> <pages> pp. 64-72 </pages>
Reference-contexts: 1 Introduction Virtual Environments allow a user to be embedded into a computer generated environment. Most interfaces to these environments require the use of gloves, goggles, and/or a helmet, and offer limited interactions including only hand gestures and the ability to change viewpoint. (Notable exceptions are <ref> [21, 10, 15] </ref>, discussed below.) The cumbersome nature of the equipment and the limited nature of the interaction in these interfaces has proven to restrict the range of applications that have used this type of technology. <p> Finally, the user ends up concentrating more on the environment itself, rather than on the complex and unfamiliar equipment being used to interact with that environment. Other systems, such as the Visual Portal [15] and the CAVE <ref> [10] </ref> system have solved many of the limitations of traditional goggle-based environments through the use of wireless batons and other sensors, thus avoiding both the problems of a tethered display and viewpoint estimation (head angle).
Reference: [11] <author> Darrell T. and Pentland A., </author> <title> (1993) Space-Time Gestures, </title> <booktitle> In: IEEE Conference on Vision and Pattern Recognition, </booktitle> <address> NY, NY, </address> <month> June </month> <year> 1993. </year>
Reference: [12] <author> Darrell, T., and Pentland, A., </author> <title> (1995) Attention-driven Expression and Gesture Analysis in an Interactive Environment, </title> <booktitle> Int'l Workshop on Face and Gesture Recognition, </booktitle> <address> Zurich, Switzerland, </address> <month> June 26-28. 12 </month>
Reference-contexts: To recognize dynamic gestures, we use a high-resolution, active camera to provide a foveated 7 image of the hands (or face) of the user. The camera is guided by the computed feature location, and provides images which can be used successfully in a spatio-temporal gesture recognition method <ref> [12] </ref>. 4 ALIVE Environments We have combined the behavior modeling and vision techniques described above to construct a system for video-based interaction with artificial agents. In our system the user moves around in a real-world space of approximately 16 by 16 feet.
Reference: [13] <author> Darrell, T., Essa, I., and Pentland, A., </author> <title> (1994) Correlation and Interpolation Networks for Real-Time Expression Analysis/Synthesis, </title> <booktitle> Advances in Neural Information Processing Systems-7, </booktitle> <editor> Tesauro, Touretzky, and Leen, eds., </editor> <publisher> MIT Press. Conference", </publisher>
Reference-contexts: Other systems have been developed for vision-based interactive graphics but have generally been restricted to off-line analysis of either face or limb motion [14, 34, 29]. (But see <ref> [13] </ref> for a real-time facial analysis system.) We have developed a set of vision routines for perceiving body actions and gestures performed by a human participant in an interactive system.
Reference: [14] <author> Essa, I., and Pentland, A., </author> <title> (1995) Facial Expression Recognition using a Dynamic Model and Motion Energy, </title> <booktitle> In Proc. Fifth Intl. Conf. Computer Vision, IEEE Computer Society, </booktitle> <pages> pp. 76-83. </pages>
Reference-contexts: Other systems have been developed for vision-based interactive graphics but have generally been restricted to off-line analysis of either face or limb motion <ref> [14, 34, 29] </ref>. (But see [13] for a real-time facial analysis system.) We have developed a set of vision routines for perceiving body actions and gestures performed by a human participant in an interactive system.
Reference: [15] <author> Deering. M., </author> <title> (1992) High Resolution Virtual Reality. </title> <journal> Computer Graphics, </journal> <volume> Vol. 26, 2, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 195-201 </pages>
Reference-contexts: 1 Introduction Virtual Environments allow a user to be embedded into a computer generated environment. Most interfaces to these environments require the use of gloves, goggles, and/or a helmet, and offer limited interactions including only hand gestures and the ability to change viewpoint. (Notable exceptions are <ref> [21, 10, 15] </ref>, discussed below.) The cumbersome nature of the equipment and the limited nature of the interaction in these interfaces has proven to restrict the range of applications that have used this type of technology. <p> Finally, the user ends up concentrating more on the environment itself, rather than on the complex and unfamiliar equipment being used to interact with that environment. Other systems, such as the Visual Portal <ref> [15] </ref> and the CAVE [10] system have solved many of the limitations of traditional goggle-based environments through the use of wireless batons and other sensors, thus avoiding both the problems of a tethered display and viewpoint estimation (head angle).
Reference: [16] <author> Featherstone R., </author> <title> (1987) Robot Dynamics Algorithms, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: [17] <author> Fisher S.S., Girard M. and Amkraut S., Menagerie, </author> <booktitle> (1993) Tomorrow's Realities, SIGGRAPH-93 Visual Proceedings, ACM SIGGRAPH 1993, </booktitle> <pages> pp. 212-213. </pages>
Reference-contexts: Joe Bates' Woggle world combines behavior models with ideas from traditional animation to explore what he calls believable agents [4]. Bates' agents can have fairly complex interactions, despite the user interaction being limited to moving a mouse. Fisher's <ref> [17] </ref> Menagerie system allows a user to interact with animated agents in real-time using goggles. In contrast with behaviors in the ALIVE system, these agents typically are engaged in a single high-level behavior, for example, flocking.
Reference: [18] <author> Girard, Michael and A. A. Maciejewski. </author> <title> Computational Modeling for the Computer Animation of Legged Figures. </title> <booktitle> Pro ceedings of SIGGRAPH 85 (San Fran-cisco, </booktitle> <address> CA, </address> <month> July 22-26, </month> <year> 1985). </year> <booktitle> In Computer Graphics 19, </booktitle> <pages> 263-270. </pages>
Reference-contexts: Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also [7, 33]). This work, together with the work of Raibert [26], Girard <ref> [18] </ref>, McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters.
Reference: [19] <author> Horn, B.K.P., </author> <title> (1991) Robot Vision, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [3, 19] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3D points with their 2D projection.
Reference: [20] <author> Hutchins E. L., Hollan J. D. and Norman D. A., </author> <title> Direct Manipulation Interfaces, (1988) in: User-centered system design: new perspectives on human-computer interaction, </title> <editor> D. A. Norman and S.W. Draper (eds), </editor> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> pp 87-124. </pages>
Reference-contexts: An important contribution of the agent system is that behavior and gestures of the user are interpreted differently by agents in the world depending on their current context and past history. Users can use a direct manipulation interaction style <ref> [20] </ref> for objects which have no associated behaviors. For example, in the current implementation the waving gesture elicits a response from the Puppet agent, but not from the Dog agent.
Reference: [21] <author> Krueger M.W., </author> <booktitle> (1990) Artificial Reality II, </booktitle> <publisher> Addison Wesley. </publisher>
Reference-contexts: 1 Introduction Virtual Environments allow a user to be embedded into a computer generated environment. Most interfaces to these environments require the use of gloves, goggles, and/or a helmet, and offer limited interactions including only hand gestures and the ability to change viewpoint. (Notable exceptions are <ref> [21, 10, 15] </ref>, discussed below.) The cumbersome nature of the equipment and the limited nature of the interaction in these interfaces has proven to restrict the range of applications that have used this type of technology. <p> We also 5 adopt a mirror paradigm, where the user explicitly sees a representation of him/herself and his/her relationship to other objects in the world. The novel vision-based interface presented here was inspired by the pioneering work of Myron Krueger's Videoplace system <ref> [21] </ref>. ALIVE and Videoplace differ primarily in three respects. The first is that Videoplace focuses on 2D rather than 3D worlds and interaction. A second difference is our emphasis on modeling agents.
Reference: [22] <author> Maes P. </author> <year> (1991), </year> <title> Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back, </title> <publisher> Bradford Books/MIT Press. </publisher>
Reference-contexts: The ALIVE system incorporates a behavior modeling tool kit for developing semi-intelligent autonomous agents and their interaction with one another and with the user [6] [5]. This tool kit is the result of our long-term research into architectures for adaptive autonomous agents <ref> [22] </ref>; it's goal is to produce agents that choose the set of activities on every time step that "make the most sense" given the agent's internal needs and motivations, past history, and the perceived environment with its attendant opportunities, challenges and changes. 3 Deciding on the right set of actions is
Reference: [23] <author> Maes P., </author> <title> (1994) Agents that Reduce Work and Information Overload, </title> <journal> Communications of the ACM, </journal> <month> July. </month>
Reference-contexts: The ALIVE system can be successfully combined with the concept of a personal digital assistant. These interface agents help a user with daily tasks such as remembering where things were put, filtering electronic news and mail, scheduling meetings, etc <ref> [23] </ref>. A system such as ALIVE can be used to visualize the interface agent's states and activities. In addition, in an enhanced reality setup, the interface agent could point to real objects as well as virtual objects in its interactions with the user.
Reference: [24] <author> McKenna, Michael and David Zeltzer. </author> <title> Dynamic Simulation of Autonomous Legged Locomotion. </title> <booktitle> Proceedings of SIGGRAP H 90 (Dallas, </booktitle> <address> TX, </address> <month> August 6-10, </month> <year> 1990). </year> <note> In Computer Graphics 24, 4 (August 1990), pp.29-38. </note>
Reference-contexts: Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also [7, 33]). This work, together with the work of Raibert [26], Girard [18], McKenna <ref> [24] </ref>, and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters.
Reference: [25] <author> Mountford S.J. </author> <title> and Gaver W.W., Talking and listening to computers, (1990) In: The art of human-computer interface design, </title> <editor> Brenda Laurel (ed), </editor> <publisher> Addison Wesley. </publisher>
Reference: [26] <author> Raibert M. and Hodgins J., </author> <title> (1991) Animation of Dynamic Legged Locomotion, </title> <booktitle> Computer Graphics: Proceedings of SIGGRAPH '91, </booktitle> <volume> 25(4), </volume> <publisher> ACM Press, </publisher> <month> July. </month>
Reference-contexts: Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also [7, 33]). This work, together with the work of Raibert <ref> [26] </ref>, Girard [18], McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control.
Reference: [27] <author> Renault, O., Thalmann, N., Thalmann, D., </author> <title> A vision-based approach to behavioral animation. The Journal o f Visualization and Computer Animation 1(1),1990, </title> <publisher> pp.18-21. </publisher>
Reference: [28] <author> Reynolds C.W., </author> <title> (1987) Flocks, Herds and Schools: A Distributed Behavioral Model, </title> <booktitle> Computer Graphics: Proceedings of SIGGRAPH '87, </booktitle> <volume> 21(4), </volume> <publisher> ACM Press, </publisher> <month> July </month> <year> 1987. </year> <month> 13 </month>
Reference-contexts: A number of researchers have taken an Artificial Intelligence approach to animation in which the animated agents perform some actions in response to their perceived environment. Reynolds <ref> [28] </ref> was among the first to apply behavior-based animation to computer graphics (see also [7, 33]). <p> Virtual sensors are used by the agents to sense other agents in the world including the agent which acts as the virtual world's representation of the user. Agents in ALIVE also use synthetic vision for low level obstacle avoidance and navigation. See Reynolds <ref> [28] </ref>, Renault [27]and Tu [31] for other examples of this approach. * The releasing mechanisms of the agent. Releasing Mechanisms are entities which identify behaviorally significant stimuli from the agent's set of sensors and transduce values which represent the behavior-specific strength of the stimuli.
Reference: [29] <author> Terzopoulus, D., and Waters, K., </author> <title> (1993) Analysis and Synthesis of Facial Image Sequences Using Physical and Anatomical Models, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6), </volume> <pages> pp. 569-579, </pages> <month> June </month>
Reference-contexts: Other systems have been developed for vision-based interactive graphics but have generally been restricted to off-line analysis of either face or limb motion <ref> [14, 34, 29] </ref>. (But see [13] for a real-time facial analysis system.) We have developed a set of vision routines for perceiving body actions and gestures performed by a human participant in an interactive system.
Reference: [30] <author> Thalmann, N., and Thalmann D.,, </author> <title> (1994) Artificial Life and Virtual Reality. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester U.K. </address>
Reference-contexts: This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns <ref> [30] </ref> have begun to propose more general architectures for behavior-based animated characters. Most recently Tu and Terzopoulos [31] have modeled autonomous animated fish.
Reference: [31] <author> Tu, X., and Terzopoulos, D., </author> <title> Artificial Fishes: Physics, Locomotion, Perception, </title> <booktitle> Behavior Computer Graphics: Proceedings of SIGGRAPH 1994, </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: Some researchers, including Badler [2], Zeltzer [36] and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters. Most recently Tu and Terzopoulos <ref> [31] </ref> have modeled autonomous animated fish. Their model incorporates a "physics-based" model of fish locomotion and motor control, a perception system utilizing synthetic vision, and a behavior system which models a number of fish behaviors. <p> Virtual sensors are used by the agents to sense other agents in the world including the agent which acts as the virtual world's representation of the user. Agents in ALIVE also use synthetic vision for low level obstacle avoidance and navigation. See Reynolds [28], Renault [27]and Tu <ref> [31] </ref> for other examples of this approach. * The releasing mechanisms of the agent. Releasing Mechanisms are entities which identify behaviorally significant stimuli from the agent's set of sensors and transduce values which represent the behavior-specific strength of the stimuli.
Reference: [32] <author> Vincent V.J., </author> <year> (1993) </year> <month> Mandala: </month> <title> Virtual Village, </title> <booktitle> SIGGRAPH-93 Visual Proceedings, Tomorrow's Realities, ACM SIGGRAPH 1993, </booktitle> <pages> pp. 207. </pages>
Reference-contexts: Finally, the ALIVE vision system is able to recognize hand and body gestures as patterns in space and time. Another system that bears similarities to ALIVE is the Mandala system <ref> [32] </ref> which composites the user's color image with a virtual world that is sometimes video-based and sometimes computer animated.
Reference: [33] <author> Wilhelms J., R. Skinner. </author> <title> A 'Notion' for Interactive Behavioral Animation Control. </title> <journal> IEEE Computer Graphics and Applications 10(3) May 1990, </journal> <pages> pp. 14-22. </pages>
Reference-contexts: A number of researchers have taken an Artificial Intelligence approach to animation in which the animated agents perform some actions in response to their perceived environment. Reynolds [28] was among the first to apply behavior-based animation to computer graphics (see also <ref> [7, 33] </ref>). This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control.
Reference: [34] <author> Williams, L., </author> <title> Performance-driven facial animation, </title> <booktitle> Computer Graphics: Proceedings of SIGGRAPH 1990, </booktitle> <volume> 24(4), </volume> <publisher> ACM Press, </publisher> <pages> pp. 235-242. </pages>
Reference-contexts: Other systems have been developed for vision-based interactive graphics but have generally been restricted to off-line analysis of either face or limb motion <ref> [14, 34, 29] </ref>. (But see [13] for a real-time facial analysis system.) We have developed a set of vision routines for perceiving body actions and gestures performed by a human participant in an interactive system.
Reference: [35] <author> Wren, C., Darrell, T., Starner, T., Johnston, M., Russell, K., Azarbayejani, A., and Pentland, A. </author> <year> (1995) </year> <month> pfinder: </month> <title> A Real-Time System for Tracking People, </title> <booktitle> SPIE Conference on Real-Time Vision, </booktitle> <editor> M. Bove, Ed., Philadelpia, </editor> <address> PA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: We assume the background to be an arbitrary, but static, pattern. Mean and variance information about the background pattern are computed, and these statistics are used to determine space-variant criteria for pixel class membership. We will omit mathematical details of our segmentation algorithm, for further reference see <ref> [35] </ref>. In general, we use a hierarchical color classification is used to compute figure/ground segmentation, using a Gaussian model of each background pixel's color and an n-class adaptive model of foreground (person) colors. <p> These feature localiztion algorithms are not infallible, but we have found they work well in a wide range of conditions, especially if combined with color space classification to identify the location of flesh tones <ref> [35] </ref>. ALIVE improves on earlier systems in which only the 2D position of the user's hand was used to determine activation of objects such as virtual buttons. The improvements avoid inadvertent manipulation of objects, such as unintended activation of buttons.
Reference: [36] <author> Zeltzer D., </author> <title> (1991) Task-level graphical simulation: abstraction, representation and control, in: N.I. </title> <editor> Badler, B.A. Barsky and D. Zeltser (editors), </editor> <title> Making them move: mechanics, control and animation of articulated figures, </title> <publisher> Morgan Kauffman, </publisher> <pages> pp. </pages> <month> 3-33. </month> <title> of contour. 15 to pointing gesture by sitting. by user. </title> <booktitle> 16 depending on stance of user. </booktitle> <pages> 17 </pages>
Reference-contexts: This work, together with the work of Raibert [26], Girard [18], McKenna [24], and Badler [2] has focused primarily on modeling single behaviors (e.g. flocking, walking etc...), and on the problems of motion planning and motor control. Some researchers, including Badler [2], Zeltzer <ref> [36] </ref> and the Thalmanns [30] have begun to propose more general architectures for behavior-based animated characters. Most recently Tu and Terzopoulos [31] have modeled autonomous animated fish.
References-found: 36

