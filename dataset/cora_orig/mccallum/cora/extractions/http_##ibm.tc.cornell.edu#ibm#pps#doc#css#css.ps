URL: http://ibm.tc.cornell.edu/ibm/pps/doc/css/css.ps
Refering-URL: http://ibm.tc.cornell.edu/ibm/pps/doc/
Root-URL: http://www.tc.cornell.edu
Title: The SP2 Communication Subsystem  
Author: Craig B. Stunkel Dennis G. Shea Bulent Abali Mark Atkins Carl A. Bender Don G. Grice Peter H. Hochschild Douglas J. Joseph Ben J. Nathanson Richard A. Swetz Robert F. Stucke Michael Tsao Philip R. Varker 
Date: August 22, 1994  
Address: P.O. Box 218 Yorktown Heights, NY 10598  Neighborhood Road Kingston, NY 12401  
Affiliation: IBM Thomas J. Watson Research Center  IBM Highly Parallel Supercomputing Systems Laboratory  
Abstract: The IBM Scalable POWERparallel Systems 1 9076 SP2 1 connects RISC System/6000 1 POWER2 1 processors via the SP2 communication subsystem. This subsystem is based upon a low latency, high bandwidth switching network called the High-Performance Switch. The Switch incorporates a number of unusual features to scale aggregate bandwidth, enhance reliability, diagnose faults, and simplify cabling. It is a bidirectional multistage interconnect, and is a unified data and service network driven by a single oscillator. Switching elements contain a dynamically allocated shared buffer for storing blocked packet bytes from any input port. This paper examines the SP2 communication subsystem architecture and details its enhancements over the SP1, and overviews the subsystem support software. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. B. Stunkel, D. G. Shea, D. G. Grice, P. H. Hochschild, and M. Tsao, </author> <title> "The SP1 high-performance switch," </title> <booktitle> in Proc. 1994 Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 150-157, </pages> <month> May </month> <year> 1994. </year> <month> 22 </month>
Reference-contexts: 1 Introduction The IBM Scalable POWERparallel Systems 2 9076 SP2 2 connects RISC System/6000 2 processors via the SP2 communication subsystem. This subsystem is based upon a low latency, high bandwidth switching network called the High-Performance Switch first introduced for the SP1 <ref> [1] </ref>, and enhanced for SP2. The SP2 systems offer Switch connectivity from 4 to 128 POWER2 2 nodes, and systems of up to 512 nodes are available by special request. The primary goals for the SP2 communication subsystem are to be scalable, modular, and easily integrated.
Reference: [2] <author> C. B. Stunkel, D. G. Shea, B. Abali, M. M. Denneau, P. H. Hochschild, D. J. Joseph, B. J. Nathanson, M. Tsao, and P. R. Varker, </author> <booktitle> "Architecture and implementation of Vulcan," in Proc. 8th Int. Parallel Processing Symp., </booktitle> <pages> pp. 268-274, </pages> <month> April </month> <year> 1994. </year> <note> (An extended version of this paper is also available as Research Report RC19492 from the IBM T. J. </note> <institution> Watson Research Center, </institution> <month> Sept. </month> <year> 1993.). </year>
Reference-contexts: The SP2 adapter provides several enhancements that improve the bandwidth of the processor-to-network interface and reduce the message processing overhead for the RS/6000 processor. The Switch|based upon the Vulcan architecture and Vulcan prototype <ref> [2] </ref> developed in the Parallel Systems department at IBM Research|incorporates a number of unusual features to scale aggregate bandwidth, enhance reliability, diagnose faults, and simplify cabling. It is a bidirectional multistage interconnect, and is a unified data and service network driven by a single oscillator. <p> The SP1 communication adapter card connects the processor's Micro Channel to output and input ports of the Switch network via an ASIC chip called the MSMU (Memory & Switch Management Unit) <ref> [2] </ref>. The MSMU provides communication services commonly found in the physical, datalink and network layers of most communication adapters. The MSMU has one output and one input port implementing the link level protocol of the Switch. <p> The opportunity for wireless hardware changes reduced simulation requirements and permitted an accelerated card build. Finally, time was also gained through two design decisions. One was the use of the i860 as a communications coprocessor. Choosing the i860 chip permitted reuse of hardware from the original Vulcan design <ref> [2] </ref> at IBM Research, eliminating the need 16 for a new MSMU design. The second choice was, as for the SP1 adapter, to interface to the RS/6000 via the Micro Channel. While bandwidth was acceptable, we wished latency could be lower.
Reference: [3] <author> G. J. Lipovski and M. Malek, </author> <title> Parallel Computing: Theory and Comparisons. </title> <address> New York, NY: </address> <publisher> Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: Switching elements (also called routers in similar pipelined networks) contain a plurality of input ports and a plurality of output ports, and provide means of passing data arriving at an input port to an appropriate output port. SP2 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN <ref> [3, 4] </ref> each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth [5] linearly with the number of nodes while maintaining a fixed number of communication ports per switching element.
Reference: [4] <author> I. D. Scherson and C.-H. Chien, </author> <title> "Least common ancestor networks," </title> <booktitle> in Proc. 7th Int. Parallel Processing Symp., </booktitle> <pages> pp. 507-513, </pages> <year> 1993. </year>
Reference-contexts: Switching elements (also called routers in similar pipelined networks) contain a plurality of input ports and a plurality of output ports, and provide means of passing data arriving at an input port to an appropriate output port. SP2 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN <ref> [3, 4] </ref> each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth [5] linearly with the number of nodes while maintaining a fixed number of communication ports per switching element.
Reference: [5] <author> W. J. Dally, </author> <title> "Performance analysis of k-ary n-cube interconnection networks," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: SP2 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN [3, 4] each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth <ref> [5] </ref> linearly with the number of nodes while maintaining a fixed number of communication ports per switching element. MIN's with hundreds or thousands of nodes are richly connected and require some long communication links to support large numbers of nodes.
Reference: [6] <author> L. R. Goke and G. J. Lipovski, </author> <title> "Banyan networks for partitioning multiprocessor systems," </title> <booktitle> in Proc. 1st Ann. Symp. on Computer Architecture, </booktitle> <pages> pp. 21-28, </pages> <year> 1973. </year>
Reference-contexts: SP1 systems are similarly connected, but are limited to 64 processors. Figures 2 and 3 illustrate 48-way and 80-way system topologies. SP2 systems supporting 81 nodes to 128 nodes or more scale Switch bandwidth by cascading switch boards analogous to an SW Banyan <ref> [6] </ref>. These larger SP2 Switch topologies are also related to practical implementations of fat-tree networks [7] derived from Leiserson's idealized fat-trees [8]. Figure 4 displays a 128-way SP machine. The intermediate switch boards are identical to the switch board shown in Figure 1, but are drawn differently.
Reference: [7] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S.-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5," </title> <booktitle> in Proc. 1992 Symp. Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Figures 2 and 3 illustrate 48-way and 80-way system topologies. SP2 systems supporting 81 nodes to 128 nodes or more scale Switch bandwidth by cascading switch boards analogous to an SW Banyan [6]. These larger SP2 Switch topologies are also related to practical implementations of fat-tree networks <ref> [7] </ref> derived from Leiserson's idealized fat-trees [8]. Figure 4 displays a 128-way SP machine. The intermediate switch boards are identical to the switch board shown in Figure 1, but are drawn differently.
Reference: [8] <author> C. E. Leiserson, "Fat-trees: </author> <title> Universal networks for hardware-efficient supercomputing," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-34, </volume> <pages> pp. 892-901, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: SP2 systems supporting 81 nodes to 128 nodes or more scale Switch bandwidth by cascading switch boards analogous to an SW Banyan [6]. These larger SP2 Switch topologies are also related to practical implementations of fat-tree networks [7] derived from Leiserson's idealized fat-trees <ref> [8] </ref>. Figure 4 displays a 128-way SP machine. The intermediate switch boards are identical to the switch board shown in Figure 1, but are drawn differently. The top half of the connections correspond to the "left" side of the switch board, while the bottom half correspond to the "right" side.
Reference: [9] <author> W. J. Dally, </author> <title> "Virtual-channel flow control," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Therefore, an SP2 logical frame may be constructed from two physical frames, only one of which contains the switch board. 3 elements to forward the packet correctly to its destination. The smallest unit on which flow control is performed is called a flow-control digit, or flit <ref> [9] </ref>, which is one byte in the SP2 Switch. The width of the data transmitted by an output port is also one byte. The SP2 method of packet transfer is related to wormhole routing [9]. <p> The smallest unit on which flow control is performed is called a flow-control digit, or flit <ref> [9] </ref>, which is one byte in the SP2 Switch. The width of the data transmitted by an output port is also one byte. The SP2 method of packet transfer is related to wormhole routing [9]. In wormhole routing, each flit of a packet is advanced to the appropriate output port as soon as it arrives at a switching element input port. When the head of a packet is blocked, the flits are buffered in place. <p> This claim is based primarily upon the choice of topology and on the design of the individual switching elements of the network. In this section we present simulation results for random message traffic to judge our assertion. Random traffic is the 18 most common traffic pattern used <ref> [9, 14, 15] </ref> to evaluate networks because it provides a relatively stressful load guaranteeing significant contention, is straightforward to generate, and is network independent. The simulations were based upon a model of the Vulcan switch chip which closely mimics its register-level operation.
Reference: [10] <author> L. Lamport, </author> <title> "Time, clocks, and the ordering of events in a distributed system," </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, </volume> <pages> pp. 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: But the results are visible architecturally as well. Each processor node maintains a local time-of-day clock in synchronism with other nodes, without concern that its time-of-day will drift ahead or behind. This notion of global time, trivializing the well-known clock synchronization problem <ref> [10] </ref>, offers advantages in fault detection, performance monitoring, and debugging. Link error checking is performed on a time basis, rather than a per-packet basis, permitting errors to be detected in bounded time. A global event trace of a parallel application can be synthesized from local traces maintained at each node.
Reference: [11] <author> M. M. Denneau, P. H. Hochschild, and G. Schichman, </author> <title> "The switching network of the TF-1 parallel supercomputer," </title> <booktitle> Supercomputing Magazine, </booktitle> <pages> pp. 7-10, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: All ports are one flit (one byte) wide. In the absence of contention, packet flits incur 5 cycles of latency cutting through the chip via the crossbar path (x4.2). The design of this chip was originally targeted for the TF-1 machine <ref> [11] </ref>. 4.1 Receivers The switch chip contains eight identical receiver modules, one associated with each of the eight input ports.
Reference: [12] <author> Y. Tamir and G. L. Frazier, </author> <title> "Hardware support for high-priority traffic in VLSI communication switches," </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 402-416, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: It accepts packet chunks from the receivers, stores them, and eventually passes them to the appropriate transmitters. The Vulcan switch chip central buffering scheme is similar to the centrally-buffered, dynamically-allocated switch described by Tamir and Frazier <ref> [12] </ref>, but the Vulcan chip allocates central queue space on a chunk basis instead of a packet basis. The central queue stores packets until they can be transmitted. The storage, a 128 by 64-bit dual-port RAM, holds up to 128 eight-flit packet chunks.
Reference: [13] <author> W. Gropp, </author> <title> "Early experiences with the IBM SP1 and the high-performance switch," </title> <type> tech. rep. </type> <institution> ANL-93/41, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: This communication library maps the MSMU registers into user memory space. SP1 node to node performance has been benchmarked and compared to several commercial machines by researchers at Argonne <ref> [13] </ref>. 5.2 SP2 communication adapter The increased power of SP2's processing nodes made it essential that SP2 increase communications performance as well. SP1, while performing strongly on many parallel applications, did not use all of the link bandwidth available to it. The origin of this lay in the node-to-network interface.
Reference: [14] <author> J. H. Kim, Z. Liu, and A. A. Chien, </author> <title> "Compressionless routing: A framework for adaptive and fault-tolerant routing," </title> <booktitle> in Proc. 21st Ann. Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 289-300, </pages> <month> April </month> <year> 1994. </year> <month> 23 </month>
Reference-contexts: This claim is based primarily upon the choice of topology and on the design of the individual switching elements of the network. In this section we present simulation results for random message traffic to judge our assertion. Random traffic is the 18 most common traffic pattern used <ref> [9, 14, 15] </ref> to evaluate networks because it provides a relatively stressful load guaranteeing significant contention, is straightforward to generate, and is network independent. The simulations were based upon a model of the Vulcan switch chip which closely mimics its register-level operation.
Reference: [15] <author> S. Konstantinidou, </author> <title> "Segment routing: A novel routing technique for parallel com-puters," </title> <booktitle> in Proc. Parallel Computer Routing and Communication Workshop, </booktitle> <address> (Seattle, WA), </address> <month> May </month> <year> 1994. </year> <note> To appear in Springer-Verlag Lecture Notes in Computer Science. Also available as IBM Research Report RJ9570. </note>
Reference-contexts: This claim is based primarily upon the choice of topology and on the design of the individual switching elements of the network. In this section we present simulation results for random message traffic to judge our assertion. Random traffic is the 18 most common traffic pattern used <ref> [9, 14, 15] </ref> to evaluate networks because it provides a relatively stressful load guaranteeing significant contention, is straightforward to generate, and is network independent. The simulations were based upon a model of the Vulcan switch chip which closely mimics its register-level operation.
Reference: [16] <author> C. B. Stunkel, </author> <title> "The Vulcan Worm," </title> <type> tech. rep., </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1994. </year>
Reference-contexts: This section overviews the Worm and the Route Table Generator. 7.1 The Worm The Worm package is the SP1 and SP2 Switch network service software <ref> [16] </ref>. It executes on an arbitrarily chosen "service" or "primary" node and controls the network during service mode. The Worm package provides initialization, tuning, global time synchronization, fault determination, and diagnostic services to all devices within or attached to the Switch.
Reference: [17] <author> R. D. Rettberg, W. R. Crowther, P. P. Carvey, and R. S. Tomlinson, </author> <title> "The Monarch parallel processor hardware design," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 18-30, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In the SP2 Switch, all tuning is software-controlled, and utilizes delay logic at each network output port. As a contrasting example, the Monarch parallel processor incorporated a hardwired method for tuning a serial link <ref> [17] </ref>. Two service commands enable the service software to synchronize data arrival for a channel. One command (tuning-control) adds an identical amount of delay to each of the Data and Tag outputs, and selects a separate delay adder for the output port's Token input.
Reference: [18] <author> B. Abali and C. Aykanat, </author> <title> "Routing algorithms for IBM SP1," </title> <booktitle> in Proc. Parallel Computer Routing and Communication Workshop, </booktitle> <address> (Seattle, WA), </address> <month> May </month> <year> 1994. </year> <note> To appear in Springer-Verlag Lecture Notes in Computer Science. Also available as IBM Research Report RC19616. 24 </note>
Reference-contexts: The RTG is based on the SP1 routing algorithm that provides a single shortest path between each pair of processor nodes <ref> [18] </ref>. We enhanced the SP1 algorithm for SP2 to provide multiple (4) paths between each node pair. The SP2 Switch network provides a rich set of paths between node pairs. In such networks, selection of the routes is important because of its impact on performance. <p> The usage count of the ports determine the breadth first search order such that from a given switch the RTG algorithm first visits the switches connected to the least frequently used output ports <ref> [18] </ref>. The RTG routes are stored in a route table in each processor's memory.
References-found: 18

