URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/mogul.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/sigops96/index.html
Root-URL: 
Email: mogul@wrl.dec.com  
Title: Hinted caching in the Web  
Author: Jeffrey C. Mogul 
Address: 250 University Ave., Palo Alto, CA 94301  
Affiliation: Digital Equipment Corp. Western Research Lab.  
Abstract: The World Wide Web, like any practical distributed system, benefits greatly from caching. Many studies have shown relatively poor cache performance, because existing caches depend on temporal locality, and reference patterns in the Web do not have particularly high temporal locality. We can improve Web caching by exploiting spatial locality. This requires protocol changes to allow servers to provide hints to caches, both to support prefetching and to improve allocation and replacement policies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Scott Adams. </author> <note> The Dilbert Zone. http://www.unitedmedia.com/comics/dilbert/. </note>
Reference-contexts: We would not want to cache that page in a mistaken belief that the mean interarrival time is always 80 msec. Some resources may be subject to highly periodic references. For example, the Dilbert cartoon <ref> [1] </ref> changes daily, and most of its users will re-retrieve the page once a day.
Reference: [2] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proc. 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212. </pages> <address> Pacific Grove, CA, </address> <month> October, </month> <year> 1991. </year>
Reference-contexts: Least-recently-used and other cache algorithms that depend on observing the reference stream often do not accurately predict future references, because there is relatively little temporal locality of reference in the Web (compared to, say, a distributed file system <ref> [2] </ref>). In this paper, I discuss several techniques that have been proposed by myself and by others for improving caching in the Web by adding features to HTTP.
Reference: [3] <author> R. Fielding, J. Gettys, J. Mogul, H. Frystyk, and T. Berners-Lee. </author> <title> Hypertext Transfer Protocol -- HTTP/1.1. Internet Draft draft-ietf-http-v11-spec-05, </title> <note> HTTP Working Group, June, 1996. This ``work in progress'' is a preliminary draft, and will be updated. </note>
Reference-contexts: Caching in the Web is evolving from a naive model to one that considers the semantics of various kinds of operations and data values. The Hypertext Transfer Protocol (HTTP), especially in its most recent revision <ref> [3] </ref>, includes features that support the efficient validation of cached values, as well as explicit server and client control over what values may be cached, and for how long. HTTP could provide more support for improving the performance of caches. <p> Even if the path between the proxy and origin server is congested, if the path between client and proxy is underutilized then it would be appropriate to prefetch any documents already cached at the proxy. The HTTP/1.1 proposed standard includes an ``only-if-cached'' directive that may support this technique <ref> [3] </ref>. 6. Conclusions Minor changes to the HTTP protocol, along with active participation by both origin servers and caches, should allow Web caches to exploit spatial locality of reference. This should greatly improve the effectiveness of Web caches.
Reference: [4] <author> Steven Glassman. </author> <title> A Caching Relay for the World Wide Web. </title> <booktitle> In Proc. of the First International World-Wide Web Conference, </booktitle> <pages> pages 69-76. </pages> <address> Geneva, </address> <month> May, </month> <year> 1994. </year> <note> Hinted caching in the Web 1996 SIGOPS European Workshop 6 </note>
Reference-contexts: Prefetching Improved allocation and replacement policies can only help if cached values are re-referenced at least once. Numerous studies have suggested that even with an arbitrarily large Web cache, cache hit rates do not get much above 30%-50% <ref> [4, 10, 13] </ref>. Some Web pages, such as those returned in response to queries, are inherently uncachable, but many others are simply not worth reading twice (such as a daily cartoon). Caches that depend on temporal locality probably will never get cache hit rates above a modest value.
Reference: [5] <author> James Griffioen and Randy Appleton. </author> <title> Reducing File System Latency using a Predictive Approach. </title> <booktitle> In Proc. 1994 Summer USENIX Conf., </booktitle> <pages> pages 197-207. </pages> <address> Boston, MA, </address> <month> June, </month> <year> 1994. </year>
Reference-contexts: The latency of the prefetch can be partially hidden by the user's ``think time.'' Hinted caching in the Web 1996 SIGOPS European Workshop 4 Several file system projects have employed this technique. Griffioen and Appleton <ref> [5] </ref> use the most recent reference to predict some number of future references. Kroeger and Long [6] use one or more recent references to predict the next reference. Both projects found improved cache hit ratios. <p> Also, many users connect to the Internet via non-shared, low-bandwidth dialup links, and it is entirely appropriate for a client to use prefetching to fully utilize the bandwidth of such a link. Note that although the previous work on predictive prefetching <ref> [5, 6, 7, 8] </ref> has assumed static values for the prefetch threshold, the client's threshold could vary dynamically, depending on the available bandwidth and the predicted retrieval latency.
Reference: [6] <author> Thomas M. Kroeger and Darrell D. E. </author> <title> Long. Predicting File System Actions from Prior Events. </title> <booktitle> In Proc. USENIX 1996 Technical Conference, </booktitle> <pages> pages 319-328. </pages> <address> San Diego, CA, </address> <month> January, </month> <year> 1996. </year>
Reference-contexts: Griffioen and Appleton [5] use the most recent reference to predict some number of future references. Kroeger and Long <ref> [6] </ref> use one or more recent references to predict the next reference. Both projects found improved cache hit ratios. <p> Also, many users connect to the Internet via non-shared, low-bandwidth dialup links, and it is entirely appropriate for a client to use prefetching to fully utilize the bandwidth of such a link. Note that although the previous work on predictive prefetching <ref> [5, 6, 7, 8] </ref> has assumed static values for the prefetch threshold, the client's threshold could vary dynamically, depending on the available bandwidth and the predicted retrieval latency.
Reference: [7] <author> Venkata N. Padmanabhan. </author> <title> Improving World Wide Web Latency. </title> <type> Master's thesis, </type> <institution> University of California at Berkeley, </institution> <month> May, </month> <year> 1995. </year>
Reference-contexts: Kroeger and Long [6] use one or more recent references to predict the next reference. Both projects found improved cache hit ratios. Can the same approach be applied to the Web? Working with the author, Padmanabhan <ref> [7, 8] </ref> implemented a prediction engine, applied it to traces from an actual Web server, and simulated mean latency improvements of up to about 60%. (The potential improvement may be higher, since this study assumed a relatively low network latency.) While only the server has enough history to make useful predictions, <p> Also, many users connect to the Internet via non-shared, low-bandwidth dialup links, and it is entirely appropriate for a client to use prefetching to fully utilize the bandwidth of such a link. Note that although the previous work on predictive prefetching <ref> [5, 6, 7, 8] </ref> has assumed static values for the prefetch threshold, the client's threshold could vary dynamically, depending on the available bandwidth and the predicted retrieval latency.
Reference: [8] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency. </title> <journal> Computer Communication Review 26(3):??-??, </journal> <month> July, </month> <year> 1996. </year>
Reference-contexts: Kroeger and Long [6] use one or more recent references to predict the next reference. Both projects found improved cache hit ratios. Can the same approach be applied to the Web? Working with the author, Padmanabhan <ref> [7, 8] </ref> implemented a prediction engine, applied it to traces from an actual Web server, and simulated mean latency improvements of up to about 60%. (The potential improvement may be higher, since this study assumed a relatively low network latency.) While only the server has enough history to make useful predictions, <p> Note that simply increasing the available bandwidth by 40%, without using prefetching, does not improve latency nearly as much as prefetching does. threshold. A prefetch threshold of 1.0 corresponds to the non-prefetching case. (The data points are a reformatted subset of those plotted in figures 5 and 6 in <ref> [8] </ref>.) In some cases, this bandwidth increase makes prefetching inappropriate, but many network paths have excess capacity. Judicious use of this capacity for prefetching improves apparent latency without degrading service for anyone. It should be possible to inhibit prefetching over shared paths that are currently congested. <p> Also, many users connect to the Internet via non-shared, low-bandwidth dialup links, and it is entirely appropriate for a client to use prefetching to fully utilize the bandwidth of such a link. Note that although the previous work on predictive prefetching <ref> [5, 6, 7, 8] </ref> has assumed static values for the prefetch threshold, the client's threshold could vary dynamically, depending on the available bandwidth and the predicted retrieval latency.
Reference: [9] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, Jim Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proc. 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95. </pages> <address> Copper Mountain, CO, Decem-ber, </address> <year> 1995. </year>
Reference-contexts: Unlike large cache lines or sequential prefetching, a prefetch instruction captures spatial locality at a higher level of abstraction. Patterson et al. have suggested a similar mechanism for file systems <ref> [9] </ref>. To make prefetching work in the Web, we need an oracle that operates at the right level of abstraction to predict the sequence of future references.
Reference: [10] <author> James E. Pitkow and Margaret M. Recker. </author> <title> A Simple yet Robust Caching Algorithm Based on Dynamic Access Patterns. </title> <booktitle> In Proc. Second WWW Conference '94: Mosaic and the Web, </booktitle> <pages> pages 1039-1046. </pages> <address> Chicago, IL, </address> <month> Oct., </month> <year> 1994. </year>
Reference-contexts: Prefetching Improved allocation and replacement policies can only help if cached values are re-referenced at least once. Numerous studies have suggested that even with an arbitrarily large Web cache, cache hit rates do not get much above 30%-50% <ref> [4, 10, 13] </ref>. Some Web pages, such as those returned in response to queries, are inherently uncachable, but many others are simply not worth reading twice (such as a daily cartoon). Caches that depend on temporal locality probably will never get cache hit rates above a modest value.
Reference: [11] <author> Richard L. Sites and Richard T. Witek. </author> <title> Alpha AXP Architecture Reference Manual, 2nd Ed. </title> <publisher> Digital Press, </publisher> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: Sequential prefetching in a CPU cache or file system also exploits co-location, anticipating that if one cache line is needed, the next one might well be needed soon. Some CPU designs are more sophisticated, allowing a program to issue a prefetch instruction (such as the Alpha's FETCH <ref> [11] </ref>) to hint to the cache what words will be needed soon. Unlike large cache lines or sequential prefetching, a prefetch instruction captures spatial locality at a higher level of abstraction. Patterson et al. have suggested a similar mechanism for file systems [9].
Reference: [12] <author> J. D. Touch and D. J. Farber. </author> <title> An Experiment in Latency Reduction. </title> <booktitle> In Proc. IEEE Infocom, </booktitle> <pages> pages 175-181. </pages> <address> Toronto, </address> <month> June, </month> <year> 1994. </year>
Reference-contexts: One increases cache line size to benefit from the likelihood that two co-located data items will be referenced at about the same time. For the Web, Touch <ref> [12] </ref> proposes an analogous technique called ``source anticipation.'' In this technique, upon receiving a request for an object, the server not only transmits the requested object but also all ``linked items,'' anticipating the client's future requests.
Reference: [13] <author> Stephen Williams, Marc Abrams, Charles R. Standridge, Ghaleb Abdulla, and Edward A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proc. SIGCOMM '96 Symposium on Communications Architectures and Protocols. </booktitle> <address> Stanford, CA, </address> <month> August, </month> <year> 1996. </year>
Reference-contexts: Williams et al. have shown that a cache removal policy based on object size provides a higher hit rate than an LRU policy, on a variety of reference traces <ref> [13] </ref>. Their preferred policy simply removes the largest cache entry, when a new insertion requires a removal. The decision also depends on the cost of servicing a cache miss, which varies dramatically both among different resources and also (because of network load variations) for retrievals of a single resource. <p> Prefetching Improved allocation and replacement policies can only help if cached values are re-referenced at least once. Numerous studies have suggested that even with an arbitrarily large Web cache, cache hit rates do not get much above 30%-50% <ref> [4, 10, 13] </ref>. Some Web pages, such as those returned in response to queries, are inherently uncachable, but many others are simply not worth reading twice (such as a daily cartoon). Caches that depend on temporal locality probably will never get cache hit rates above a modest value.
References-found: 13

