URL: http://vismod.www.media.mit.edu/~jdavis/OldPapers/asilomar.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/~jdavis/
Root-URL: http://www.media.mit.edu
Title: Determining 3-D Hand Motion  
Author: James Davis Mubarak Shah 
Address: Cambridge, MA 02139 Orlando, FL 32826  
Affiliation: Media Lab Computer Vision Lab Massachusetts Institute of Technology University of Central Florida  
Abstract: This paper presents a glove-free method for tracking hand movements using a set of 3-D models. In this approach, the hand is represented by five cylindrical models which are fit to the third phalangeal segments of the fingers. Six 3-D motion parameters for each model are calculated that correspond to the movement of the fingertips in the image plane. Trajectories of the moving models are then established to show the 3-D nature of hand motion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Artwick, B. </author> <title> Applied Concepts in Microcomputer Graphics. </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1984. </year>
Reference-contexts: The locations of the TP models are continually updated in 3-D to match the 2-D fingertip movement. Only visible model nodes can be used in the motion parameter calculation and can be determined by using together two methods (surface normals and depth array) for back-side elimination <ref> [1] </ref>.
Reference: [2] <author> Bauml, B., and Bauml, F. </author> <title> A Dictionary of Gestures. </title> <publisher> The Scarecrow Press, </publisher> <address> New Jersey, </address> <year> 1975. </year>
Reference-contexts: 1 Introduction The importance of human gestures has been greatly underestimated. We use hundreds of expressive movements every day <ref> [2, 9] </ref>, with many of these movements pertaining to hand gestures. These movements may have radically different interpretations from country to country one hand gesture may represent a meaning of "good" in one country, whereas in another country it may be viewed as offensive [9].
Reference: [3] <author> Cipolla, R., Okamoto, Y., and Kuno, Y. </author> <title> Robust structure from motion using motion parallax. </title> <booktitle> In ICCV, </booktitle> <pages> pages 374-382. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Matching is based upon the normalized correlation between the image and the set of 2-D view models. This method requires the use of special-purpose hardware to achieve real-time performance, and uses gray-level correlation which can be highly sensitive to noise. Cipolla, Okamoto, and Kuno <ref> [3] </ref> present a structure from motion (SFM) method in which the 3-D visual interpretation of hand movements is used in a man-machine interface. A glove with colored markers is used as input to the vision system and movement of the hand results in motion between the markers in the images. <p> We then find a set of points which can be used to differentiate the fingers from the rest of the image. Previous approaches for finding feature points involve boundary curvature extrema [8], interest operators to detect specially colored regions <ref> [3] </ref>, and manual selection [11]. Our approach uses the knowledge of the start position and natural design of the hand to automatically determine five fingertip points fT n g 4 n=0 and seven base points fB m g 6 m=0 which are used to segment the fingers.
Reference: [4] <author> E. Costello. </author> <title> Signing: How to Speak With Your Hands. </title> <publisher> Bantam Books, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Finger-spelling, a subset of sign language, permits any letter of the English alphabet to be presented using a distinct hand gesture. Using the finger-spelling gesture set, people can communicate words to one another using only hand movements <ref> [4] </ref>. The media has realized the significance of gestures and was experienced in the final scene of the movie, Close Encounters of the Third Kind (Columbia Pictures, 1977), where a human and alien communicated to each another using hand movements.
Reference: [5] <author> Darrell, T., and Pentland, A. </author> <title> Space-time gestures. </title> <booktitle> In CVPR, </booktitle> <pages> pages 335-340. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Hand features are measured using local image-based trackers within manually selected search windows. Rendered models and state trajectories are given demonstrating the 3-D nature of their results. Darrell and Pentland <ref> [5] </ref> have proposed an approach for gesture recognition using sets of 2-D view models of a hand (one or more example views of a hand).
Reference: [6] <author> Davis, J., and Shah, M. </author> <title> Recognizing hand gestures. </title> <booktitle> In ECCV, </booktitle> <pages> pages 331-340, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Two-dimensional ambiguities which may arise are the 3-D trajectories which, after undergoing perspective projection, have the same corresponding 2-D trajectory. Also, using 3-D models and motion parameters avoids the need for motion correspondence for mapping feature points to their correct 2-D trajectory <ref> [10, 6] </ref>, for each feature point is a member of a distinct model for a particular finger and thus has no ambiguity in which trajectory it belongs. Therefore to remove these uncertainties which may arise in 2-D, we can use 3-D information. <p> The grasp is then identified using a grasp cohesive index. Though this method uses 3-D finger information, it requires both intensity and costly range imagery to produce the finger models. In an earlier paper <ref> [6] </ref>, we presented a method for recognizing hand gestures using a 2-D approach. A finite state machine is used to model four qualitatiely distinct phases of a generic gesture.
Reference: [7] <author> Horn, B.K.P. </author> <title> Robot Vision. </title> <publisher> McGraw-Hill, </publisher> <year> 1986. </year>
Reference-contexts: The integral to be minimized over finger F is E = F where r is the perpendicular distance from point (x; y) to the axis sought after <ref> [7] </ref>. The fingers and axes will be used in generating cylindrical representations of fin ger segments. 3.2 Cylindrical Fitting Cylindrical models can be employed to represent the fingers due to the inherent cylindrical nature of fingers.
Reference: [8] <author> Kang, S.B., and Ikeuchi, K. </author> <title> Toward automatic robot instruction from perception recognizing a grasp from observation. </title> <journal> IEEE Transactions of Robotics and Automation, </journal> <volume> 9 </volume> <pages> 432-443, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Each gesture is determined from the hand's 2-D position, and does not use any motion characteristics or 3-D feature locations. Gest was used to control graphics applications, such as a graphics editor and flight simulator. Kang and Ikeuchi <ref> [8] </ref> describe a framework for determining 3-D hand grasps. An intensity image is used for the identification and localization of the fingers using curvature analysis, and a range image is used for 3-D cylindrical fitting of the fingers. <p> We then find a set of points which can be used to differentiate the fingers from the rest of the image. Previous approaches for finding feature points involve boundary curvature extrema <ref> [8] </ref>, interest operators to detect specially colored regions [3], and manual selection [11].
Reference: [9] <author> Morris, D., Collet, P., Marsh, P., and O'Saughnessy, M. </author> <title> Gestures: Their Origins and Distribution. </title> <editor> Stein and Day, </editor> <year> 1979. </year>
Reference-contexts: 1 Introduction The importance of human gestures has been greatly underestimated. We use hundreds of expressive movements every day <ref> [2, 9] </ref>, with many of these movements pertaining to hand gestures. These movements may have radically different interpretations from country to country one hand gesture may represent a meaning of "good" in one country, whereas in another country it may be viewed as offensive [9]. <p> These movements may have radically different interpretations from country to country one hand gesture may represent a meaning of "good" in one country, whereas in another country it may be viewed as offensive <ref> [9] </ref>. Finger-spelling, a subset of sign language, permits any letter of the English alphabet to be presented using a distinct hand gesture. Using the finger-spelling gesture set, people can communicate words to one another using only hand movements [4].
Reference: [10] <author> Rangarajan, K., and Shah, M. </author> <title> Establishing motion correspondence. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 54 </volume> <pages> 56-73, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Two-dimensional ambiguities which may arise are the 3-D trajectories which, after undergoing perspective projection, have the same corresponding 2-D trajectory. Also, using 3-D models and motion parameters avoids the need for motion correspondence for mapping feature points to their correct 2-D trajectory <ref> [10, 6] </ref>, for each feature point is a member of a distinct model for a particular finger and thus has no ambiguity in which trajectory it belongs. Therefore to remove these uncertainties which may arise in 2-D, we can use 3-D information.
Reference: [11] <author> Rehg, J., and Kanade, T. </author> <title> Visual tracking of high dof articulated structures: an application to human hand tracking. </title> <booktitle> In ECCV, </booktitle> <pages> pages 35-46, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Then six 3-D motion parameters are calculated for each model corresponding to the 2-D movement of the fingers in the image plane (Section 4). Experiments are shown with 3-D hand movements (Section 5). 2 Related Work Regh and Kanade <ref> [11] </ref> describe a model-based hand tracking system called DigitEyes. This system uses stereo cameras and special real-time image processing hardware to recover the state of a hand model with 27 spatial degrees of freedom. <p> We then find a set of points which can be used to differentiate the fingers from the rest of the image. Previous approaches for finding feature points involve boundary curvature extrema [8], interest operators to detect specially colored regions [3], and manual selection <ref> [11] </ref>. Our approach uses the knowledge of the start position and natural design of the hand to automatically determine five fingertip points fT n g 4 n=0 and seven base points fB m g 6 m=0 which are used to segment the fingers.
Reference: [12] <author> Segen, J. Gest: </author> <title> A learning computer vision system that recognizes hand gestures. </title> <booktitle> Machine Learning IV, </booktitle> <year> 1994. </year>
Reference-contexts: The authors use the affine transformation of an arbitrary triangle formed by the markers to determine the projection of the axis of rotation, change in scale, and cyclotorsion. This information is used to alter the position and orientation of an object displayed on a computer graphics system. Segan's <ref> [12] </ref> Gest is a computer vision system that learns to identify non-rigid 2-D hand shapes and computes their pose. The system displays a hand in a fixed position on the screen and the user responds by presenting that same gesture to the camera. The hand's pose is calculated and classified.
Reference: [13] <author> Taylor, C., and Schwarz, R. </author> <title> The anatomy and mechanics of the human hand. </title> <booktitle> Artificial Limbs, </booktitle> <year> 1955. </year>
Reference-contexts: A finger as a whole is a non-rigid object, with the first phalangeal (FP), second phalangeal (SP), and third phalangeal (TP) segments (only FP and TP segments for thumb) <ref> [13] </ref> each exhibiting rigid behavior. We dismiss the concerns for non-rigidness, occlusion, and connectedness, and only model and track the TP segments (fingertip segments) for simplicity. To model the TP segments, we must know where they are located with respect to each finger in the image.
Reference: [14] <author> Zerroug, M., and Nevatia, R. </author> <title> Segmentation and recovery of shgcs from a real intensity image. </title> <booktitle> In ECCV, </booktitle> <year> 1994. </year>
References-found: 14

