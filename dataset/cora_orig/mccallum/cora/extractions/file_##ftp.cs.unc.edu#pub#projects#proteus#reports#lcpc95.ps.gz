URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/lcpc95.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/proteus/proteus-publications.html
Root-URL: http://www.cs.unc.edu
Email: fpalmerd,prins,sc,faithg@cs.unc.edu  
Title: Piecewise Execution of Nested Data-Parallel Programs  
Author: Daniel W. Palmer, Jan F. Prins, Siddhartha Chatterjee, and Rickard E. Faith 
Address: Chapel Hill, NC 27599-3175  
Affiliation: Department of Computer Science The University of North Carolina  
Abstract: The technique of flattening nested data parallelism combines all the independent operations in nested apply-to-all constructs and generates large amounts of potential parallelism for both regular and irregular expressions. However, the resulting data-parallel programs can have enormous memory requirements, limiting their utility. In this paper, we present piecewise execution, an automatic method of partially serializing data-parallel programs so that they achieve maximum parallelism within storage limitations. By computing large intermediate sequences in pieces, our approach requires asymptotically less memory to perform the same amount of work. By using characteristics of the underlying parallel architecture to drive the computation size, we retain effective use of a parallel machine at each step. This dramatically expands the class of nested data-parallel programs that can be executed using the flattening technique. With the addition of piecewise I/O operations, these techniques can be applied to generate out-of-core execution on large datasets.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Abrams. </author> <title> An APL Machine. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1970. </year>
Reference-contexts: Reducing the memory requirements of a program by eliminating storage of temporary aggregate values is not a recent idea. In 1970, Abrams developed an interpreter system that partially compiled and sequentially executed APL programs <ref> [1] </ref>. The system postponed execution of certain operations until they could be optimized based on contextual information gathered during the postponement. These optimizations eliminated storing temporary aggregates, even those resulting from size increasing operations such as distributions and enumerations.
Reference: 2. <author> J. Backus. </author> <title> Can programming be liberated from the von Neumann style? A functional style and its algebra of programs. </title> <journal> Commun. ACM, </journal> <volume> 21(8) </volume> <pages> 613-41, </pages> <month> Aug. </month> <year> 1978. </year>
Reference-contexts: The expressive benefits of nested data parallelism were long ago recognized by high-level languages such as SETL [19], FP <ref> [2] </ref>, and APL2, but practical parallel execution of such expressions was not achieved until Blelloch and Sabot introduced the flattening technique [4]. Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations. <p> For comparison, we also express this computation with a serialized outer iterator. for (i=1;i&lt;=#D;i++)- T = range1 (D [i]); R [i] = mult_reduce (T); - In Table 1, we compare the execution of these two versions. In this example we have four processors and have set D = <ref> [5; 2; 7; 3] </ref>. For the serialized outer iterator code, if the size of T exceeds the number of processors, we must use multiple steps to complete the computation using virtual processors (see Fig. 1b).
Reference: 3. <author> G. Blelloch and G. Narlikar. </author> <title> A comparison of two n-body algorithms. </title> <booktitle> In Proceedings of DIMACS Parallel Implementation Challenge Workshop III, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The generality of the apply-to-all construct can easily lead to programs that have enormous potential parallelism, and hence, excessive memory requirements. Blelloch and Narlikar <ref> [3] </ref> encountered these large temporaries while comparing two algorithms for n-body simulations. They resolved the problem by manually serializing portions of their NESL code reducing the program's memory requirements so it could execute. The following example illustrates that flattening a nested data-parallel expression can generate code with large temporary values. <p> For comparison, we also express this computation with a serialized outer iterator. for (i=1;i&lt;=#D;i++)- T = range1 (D [i]); R [i] = mult_reduce (T); - In Table 1, we compare the execution of these two versions. In this example we have four processors and have set D = <ref> [5; 2; 7; 3] </ref>. For the serialized outer iterator code, if the size of T exceeds the number of processors, we must use multiple steps to complete the computation using virtual processors (see Fig. 1b).
Reference: 4. <author> G. Blelloch and G. Sabot. </author> <title> Compiling collection-oriented languages onto massively parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2), </volume> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The expressive benefits of nested data parallelism were long ago recognized by high-level languages such as SETL [19], FP [2], and APL2, but practical parallel execution of such expressions was not achieved until Blelloch and Sabot introduced the flattening technique <ref> [4] </ref>. Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations.
Reference: 5. <author> G. E. Blelloch. Nesl: </author> <title> A nested data-parallel language. </title> <type> Technical Report CMU-CS-92-129, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations. Both NESL <ref> [5] </ref> and Proteus [13] are high-level, nested data-parallel languages that use this technique to provide architecture-independence by implementing the data-parallel operations with portable vector operations [6]. 1.2 Excessive memory requirements of flattened programs The flattening technique fully parallelizes every apply-to-all construct, providing large amounts of fine-grained potential parallelism, but introduces temporaries <p> For comparison, we also express this computation with a serialized outer iterator. for (i=1;i&lt;=#D;i++)- T = range1 (D [i]); R [i] = mult_reduce (T); - In Table 1, we compare the execution of these two versions. In this example we have four processors and have set D = <ref> [5; 2; 7; 3] </ref>. For the serialized outer iterator code, if the size of T exceeds the number of processors, we must use multiple steps to complete the computation using virtual processors (see Fig. 1b).
Reference: 6. <author> G. E. Blelloch, S. Chatterjee, J. Hardwick, M. Reid-Miller, J. Sipelstein, and M. Zagha. Cvl: </author> <title> a c vector library manual, </title> <type> version 2. Technical Report CMU-CS-93-114, </type> <institution> Carnegie Mel-lon University, </institution> <year> 1993. </year>
Reference-contexts: Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations. Both NESL [5] and Proteus [13] are high-level, nested data-parallel languages that use this technique to provide architecture-independence by implementing the data-parallel operations with portable vector operations <ref> [6] </ref>. 1.2 Excessive memory requirements of flattened programs The flattening technique fully parallelizes every apply-to-all construct, providing large amounts of fine-grained potential parallelism, but introduces temporaries whose sizes are proportional to the potential parallelism.
Reference: 7. <author> T. Budd. </author> <title> An APL Compiler. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For comparison, we also express this computation with a serialized outer iterator. for (i=1;i&lt;=#D;i++)- T = range1 (D [i]); R [i] = mult_reduce (T); - In Table 1, we compare the execution of these two versions. In this example we have four processors and have set D = <ref> [5; 2; 7; 3] </ref>. For the serialized outer iterator code, if the size of T exceeds the number of processors, we must use multiple steps to complete the computation using virtual processors (see Fig. 1b). <p> Budd explored extending these ideas to the vector domain. Instead of single elements at a time, he proposed to stream a vector's worth of elements through a computation <ref> [7] </ref>. This approach not only evaluated APL expressions in space equal to the larger of the inputs and outputs, but could also make effective use of vector hardware.
Reference: 8. <author> S. Chatterjee. </author> <title> Compiling nested data-parallel programs for shared-memory multiprocessors. </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 15(3) </volume> <pages> 400-462, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Furthermore, pairs of generators/accumulators that exhibit the same piecewise structural behavior are conformable and should be placed in the same piecewise execution loop for best performance. Chatterjee's size inference <ref> [8] </ref> can be used to identify conforming operations. Restrict operations require the introduction of additional loops to provide the data-dependent number of input pieces necessary for restrict to generate an output piece. <p> Although Waters speculated on extending his program transformations to handle nested series expressions, he did not implement it. In 1993, Chatterjee compiled nested data-parallel programs to increase code granularity and relax lock-step synchrony so the programs could effectively execute on MIMD machines <ref> [8] </ref>. Although his compiler did not implement the fixed memory evaluation of Abrams, he was the first to apply temporary elimination in the context of nested data-parallel programs.
Reference: 9. <author> H. P. F. Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction 1.1 Flattening nested data parallelism Nested data parallelism is a powerful paradigm for expressing concurrent execution, especially irregular and dynamic computations. Unlike flat data-parallel languages such as C fl [11], High Performance Fortran <ref> [9] </ref>, and APL [12], nested data-parallel languages allow arbitrary functions to appear in apply-to-all constructs and provide nestable, non-rectangular aggregates.
Reference: 10. <author> L. J. Guibas and D. K. Wyatt. </author> <title> Compilation and delayed evaluation in APL. </title> <booktitle> In Conf. Record of the Fifth Annual ACM Symp. on Princ. of Prog. </booktitle> <address> Lang. (Tucson, Arizona), </address> <pages> pages 1-8. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1978. </year>
Reference-contexts: With this approach, he could evaluate expressions composed from a restricted set of APL operations using fixed storage equal to the larger of the expressions inputs and outputs. In 1978, Guibas and Wyatt formalized and extended Abrams' ideas to build a system that fully compiled APL programs <ref> [10] </ref>. Using data-flow analysis techniques, they generated code that statically did the equivalent of Abrams contextual postponement. The compiled code evaluated APL's aggregate operations using fixed storage by streaming single elements of a flat aggregate through a complete computation. Budd explored extending these ideas to the vector domain.
Reference: 11. <author> P. Hatcher and M. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction 1.1 Flattening nested data parallelism Nested data parallelism is a powerful paradigm for expressing concurrent execution, especially irregular and dynamic computations. Unlike flat data-parallel languages such as C fl <ref> [11] </ref>, High Performance Fortran [9], and APL [12], nested data-parallel languages allow arbitrary functions to appear in apply-to-all constructs and provide nestable, non-rectangular aggregates.
Reference: 12. <author> K. Iverson. </author> <title> A Programming Language. </title> <publisher> Wiley, </publisher> <year> 1962. </year>
Reference-contexts: 1 Introduction 1.1 Flattening nested data parallelism Nested data parallelism is a powerful paradigm for expressing concurrent execution, especially irregular and dynamic computations. Unlike flat data-parallel languages such as C fl [11], High Performance Fortran [9], and APL <ref> [12] </ref>, nested data-parallel languages allow arbitrary functions to appear in apply-to-all constructs and provide nestable, non-rectangular aggregates.
Reference: 13. <author> G. Levin and L. Nyland. </author> <title> An introduction to Proteus, version 0.9. </title> <type> Technical report, </type> <institution> University of North Carolina at Chapel Hill, </institution> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations. Both NESL [5] and Proteus <ref> [13] </ref> are high-level, nested data-parallel languages that use this technique to provide architecture-independence by implementing the data-parallel operations with portable vector operations [6]. 1.2 Excessive memory requirements of flattened programs The flattening technique fully parallelizes every apply-to-all construct, providing large amounts of fine-grained potential parallelism, but introduces temporaries whose sizes are
Reference: 14. <author> D. W. Palmer. Dpl: </author> <title> Data-parallel library manual. </title> <type> Technical Report UNC-CS-93-064, </type> <institution> University of North Carolina at Chapel Hill, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: We can write this in Proteus as *./[i in [1..n]:i]. We then successively apply iterator removing transformation rules [18] [15], yielding equivalent data-parallel operations. *./[i in [1..n]:i] = mult_reduce ([i in [1..n]:i]) = mult_reduce ([1..n]) = mult_reduce (range1 (n)) These operations are part of the Data-Parallel Library (DPL) <ref> [14] </ref>, a collection of routines that supports nested sequences as primitive objects and provides data-parallel execution of nested sequence operations. We then compile the functional expression into an imperative, single-assignment form with explicit temporary variables.
Reference: 15. <author> D. W. Palmer, J. F. Prins, and S. Westfold. </author> <title> Work-efficient nested data-parallelism. </title> <booktitle> In Proc. Fifth Symp. on the Frontiers of Massively Parallel Processing (Frontiers 95). IEEE., </booktitle> <year> 1995. </year>
Reference-contexts: We can write this in Proteus as *./[i in [1..n]:i]. We then successively apply iterator removing transformation rules [18] <ref> [15] </ref>, yielding equivalent data-parallel operations. *./[i in [1..n]:i] = mult_reduce ([i in [1..n]:i]) = mult_reduce ([1..n]) = mult_reduce (range1 (n)) These operations are part of the Data-Parallel Library (DPL) [14], a collection of routines that supports nested sequences as primitive objects and provides data-parallel execution of nested sequence operations. <p> When its source sequence does not exceed the piece size, index does not require synchronization and can itself operate in a piecewise manner with respect to its indices. Although this approach counteracts the effects of piecewise execution, Palmer, Prins and Westfold have developed another technique, work-efficient indexing <ref> [15] </ref>, that prevents increasing the size of many source sequences during the flattening process. We expect that this will reduce the impact of index as a synchronization point on piecewise execution. We implement piecewise versions of all primitive operations in C with explicit calls to DPL operations.
Reference: 16. <author> K. Pingali and Arvind. </author> <title> Efficient demand-driven evaluation. Part 1. </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 7(2) </volume> <pages> 311-33, </pages> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: Since engines contain all the inter-invocation information for piecewise operations, we allocate them in the heap. This allows us to achieve co-routine behavior without persistent activation records or altering the management of the stack. 3.3 Demand-driven piecewise interpretation Pingali and Arvind <ref> [16, 17] </ref> use demand-driven interpretation to evaluate their stream-based language with infinite data structures. Unlike data-driven interpretation, this approach prevents non-termination and unbounded amounts of useless work. Although these issues do not impact Proteus, we use a modified form of demand-driven interpretation to support piecewise execution.
Reference: 17. <author> K. Pingali and Arvind. </author> <title> Efficient demand-driven evaluation. Part 2. </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 8(1) </volume> <pages> 109-39, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: Since engines contain all the inter-invocation information for piecewise operations, we allocate them in the heap. This allows us to achieve co-routine behavior without persistent activation records or altering the management of the stack. 3.3 Demand-driven piecewise interpretation Pingali and Arvind <ref> [16, 17] </ref> use demand-driven interpretation to evaluate their stream-based language with infinite data structures. Unlike data-driven interpretation, this approach prevents non-termination and unbounded amounts of useless work. Although these issues do not impact Proteus, we use a modified form of demand-driven interpretation to support piecewise execution.
Reference: 18. <author> J. F. Prins and D. W. Palmer. </author> <title> Transforming high-level data-parallel programs into vector operations. </title> <booktitle> In Proc. 4th PPOPP. </booktitle> <address> (San Diego, CA, </address> <month> 19-22 May </month> <year> 1993). </year> <journal> ACM., 1993. Published in SIGPLAN Notices, </journal> <volume> 28(7) </volume> <pages> 119-28. </pages>
Reference-contexts: We can write this in Proteus as *./[i in [1..n]:i]. We then successively apply iterator removing transformation rules <ref> [18] </ref> [15], yielding equivalent data-parallel operations. *./[i in [1..n]:i] = mult_reduce ([i in [1..n]:i]) = mult_reduce ([1..n]) = mult_reduce (range1 (n)) These operations are part of the Data-Parallel Library (DPL) [14], a collection of routines that supports nested sequences as primitive objects and provides data-parallel execution of nested sequence operations.
Reference: 19. <author> J. Schwartz. </author> <title> Set theory as a language for program specification and programming. </title> <type> Technical report, </type> <institution> Computer Science Department, Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: Unlike flat data-parallel languages such as C fl [11], High Performance Fortran [9], and APL [12], nested data-parallel languages allow arbitrary functions to appear in apply-to-all constructs and provide nestable, non-rectangular aggregates. The expressive benefits of nested data parallelism were long ago recognized by high-level languages such as SETL <ref> [19] </ref>, FP [2], and APL2, but practical parallel execution of such expressions was not achieved until Blelloch and Sabot introduced the flattening technique [4]. Flattening combines all the independent operations in nested apply-to-all constructs into large data-parallel operations.
Reference: 20. <author> R. Sethi. </author> <title> Complete register allocation problems. </title> <journal> SIAM Journal of Computing, </journal> <volume> 4(3), </volume> <year> 1975. </year>
Reference-contexts: The proper piece size lies somewhere between n 1=2 , which provides half the performance of the machine and n exceeds memory which cannot execute because of insufficient storage (See Fig. 5). Sethi <ref> [20] </ref> showed that determining whether a program can successfully execute without external memory using only k registers requires exponential time. Selecting an acceptable piece size is equivalently complex. The number of pieces, n and the piece size, p are inversely related by n fl p = M .
Reference: 21. <author> R. C. Waters. </author> <title> Automatic transformation of series expressions into loops. </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 13(1) </volume> <pages> 52-98, </pages> <month> Jan. </month> <year> 1991. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Waters, generalizing the APL-based work to applicative series expressions for Common Lisp and Pascal, used data-flow analysis and program transformations to generate semantically equivalent imperative loop structures <ref> [21] </ref>. The transformations eliminated temporaries from series expressions and provided efficient single processor execution of functional programs using streaming. Although Waters speculated on extending his program transformations to handle nested series expressions, he did not implement it.
References-found: 21

