URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-40167.ps.Z
Refering-URL: http://www.ing.unlp.edu.ar/cetad/mos/memetic_home.html
Root-URL: 
Phone: 3  
Title: Cost-sensitive feature reduction applied to a hybrid genetic algorithm  
Author: Nada Lavrac Dragan Gamberger Peter Turney 
Address: Jamova 39, 1001 Ljubljana, Slovenia 2 Rudjer Boskovic Institute, Bijenicka 54, 10000 Zagreb, Croatia  M-50 Montreal Road, Ottawa, Ontario, Canada, K1A 0R6  
Affiliation: 1 Jozef Stefan Institute,  Institute for Information Technology, National Research Council Canada,  
Abstract: This study is concerned with whether it is possible to detect what information contained in the training data and background knowledge is relevant for solving the learning problem, and whether irrelevant information can be eliminated in preprocessing before starting the learning process. A case study of data preprocessing for a hybrid genetic algorithm shows that the elimination of irrelevant features can substantially improve the efficiency of learning. In addition, cost-sensitive feature elimination can be effective for reducing costs of induced hypotheses.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Caruana and D. Freitag, </author> <title> Greedy attribute selection, </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann (1994) 28-36. </publisher>
Reference-contexts: 1 Introduction The problem of relevance was addressed in early research on inductive concept learning [10]. Recently, this problem has also attracted much attention in the context of feature selection in attribute-value learning <ref> [1, 4, 12] </ref>. Basically one can say that all learners are concerned with the selection of `good' literals which will be used to construct the hypothesis.
Reference: 2. <author> D. Gamberger, </author> <title> A minimization approach to propositional inductive learning, </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning (ECML-95), </booktitle> <address> Springer (1995) 151-160. </address>
Reference-contexts: Second, the theorem enables us to directly detect useless literals that do not cover any p=n pair. This theorem is the basis of the REDUCE algorithm for literal elimination. 3 A cost-sensitive literal elimination algorithm Algorithm 1 implements the cost-sensitive literal elimination algorithm, initially developed within the ILLM learner <ref> [2] </ref>. This algorithm is the core of REDUCE. Algorithm 1. <p> This study, using a hybrid genetic decision tree induction algorithm RL-ICET on two East-West Challenge problems, together with the previous results in feature reduction <ref> [2, 3] </ref> confirm the usefulness of feature reduction in preprocessing. Some other experiments were performed and further experimentation is planned along these lines.
Reference: 3. <author> D. Gamberger and N. Lavrac, </author> <title> Towards a theory of relevance in inductive concept learning. </title> <type> Technical report IJS-DP-7310, </type> <institution> J. Stefan Institute, Ljubljana (1995). </institution>
Reference-contexts: The matrix is divided in two parts: P corresponds to the positive examples, and N to the negative examples. To enable a formal discussion of the relevance of literals we the following definitions are introduced <ref> [3] </ref>. Definition 1. A p=n pair is a pair of training examples where p 2 P and n 2 N . Definition 2. <p> The proof of this theorem can be found in <ref> [3] </ref>. The importance of the theorem is manifold. First, it points out that when deciding about the relevance of literals it will be significant to detect which p=n pairs are covered by the literal. <p> This study, using a hybrid genetic decision tree induction algorithm RL-ICET on two East-West Challenge problems, together with the previous results in feature reduction <ref> [2, 3] </ref> confirm the usefulness of feature reduction in preprocessing. Some other experiments were performed and further experimentation is planned along these lines.
Reference: 4. <author> G.H. John, R. Kohavi and K. Pfleger, </author> <title> Irrelevant features and the subset selection problem, </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann (1994) 190-198. </publisher>
Reference-contexts: 1 Introduction The problem of relevance was addressed in early research on inductive concept learning [10]. Recently, this problem has also attracted much attention in the context of feature selection in attribute-value learning <ref> [1, 4, 12] </ref>. Basically one can say that all learners are concerned with the selection of `good' literals which will be used to construct the hypothesis.
Reference: 5. <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Appli cations. </title> <publisher> Ellis Horwood (1994). </publisher>
Reference-contexts: For the East-West Challenge, ICET was extended to handle Prolog input. The decision tree output was converted to Prolog manually. This algorithm is called RL-ICET (Relational Learning with ICET) [14]. RL-ICET is similar to the LINUS learning system <ref> [5] </ref>. RL-ICET uses a three-part strategy. First, a preprocessor translates the Prolog relations and predicates into a feature vector format. The preprocessor in RL-ICET was designed specially for the East-West Challenge, whereas LINUS has a general-purpose preprocessor.
Reference: 6. <author> N. Lavrac, D. Gamberger and S. Dzeroski. </author> <title> An approach to dimensionality reduc tion in learning from deductive databases. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming, </booktitle> <pages> 337-354, </pages> <year> (1995). </year>
Reference-contexts: The algorithm can be efficiently implemented using simple bitstring manipulation on the matrix of training examples. Moreover, this algorithm can be easily transformed into an iterative algorithm that can be used during the process of generation of literals (see <ref> [6] </ref>). 4 Hypothesis H is complete if it covers all the positive examples p 2 P .
Reference: 7. <author> N. Lavrac, D. Gamberger, and P. Turney. </author> <title> Reduction of the number of features in the East-West Challenge, </title> <type> Technical Report IJS-DP-7347, </type> <institution> J. Stefan Institute, Ljubljana (1996). </institution>
Reference-contexts: Two experiments were performed separately for the 20 and 24 trains problems <ref> [7, 8] </ref>. In both experiments, the RL-ICET preprocessor was used to generate the appropriate features and to transform the training examples into a feature vector format. This resulted in two training sets of 20 and 24 examples each, described by 1199 features. <p> Results of the experiments. Results of the 20 trains experiment. With the 20 train data, REDUCE cut the original set of 1199 features down to 86 features. In this way, the complexity of the learning problem was reduced to about 7% (86/1199) of the initial learning problem <ref> [7] </ref>. Results of 10 runs of ICET on the 1199 feature set are the results reported in [14], whereas results of 10 runs of ICET on the training examples described with 86 features are new. The results show that the efficiency of learning significantly increased.
Reference: 8. <author> N. Lavrac, D. Gamberger, and P. Turney. </author> <title> Feature reduction in the 24 trains East-West Challenge, </title> <type> Technical Report IJS-DP-7372, </type> <institution> J. Stefan Institute, </institution> <month> Ljubl-jana </month> <year> (1996). </year>
Reference-contexts: Two experiments were performed separately for the 20 and 24 trains problems <ref> [7, 8] </ref>. In both experiments, the RL-ICET preprocessor was used to generate the appropriate features and to transform the training examples into a feature vector format. This resulted in two training sets of 20 and 24 examples each, described by 1199 features. <p> After that, we eliminate the irrelevant literals and, in the third phase, we construct the reduced set of features which includes all the features which have at least one of their literals in the reduced literal set. The reasons for this three-step procedure are explained in <ref> [8] </ref>. The experimental setup, designed to test the utility of REDUCE, was as follows. First, 10 runs of the ICET algorithm were performed on the set of training examples described with 1199 features. <p> Results of the 24 trains experiment. In this experiment, REDUCE decreased the number of features from 1199 to 116. In this way, the complexity of the learning problem was reduced to about 10% (116/1199) of the initial learning problem <ref> [8] </ref>. The results show that the efficiency of learning significantly increased. In the initial problem with 1199 features, the average time per exper iment was nearly two hours, whereas in the reduced problem setting with 116 features the average time per experiment was about 14 minutes. <p> In both experiments, feature reduction (re-duction to 86 and 116 features, respectively) helped ICET to outperform C4.5 when comparing costs of decision trees, both in terms of minimal and average costs <ref> [8] </ref>. On the other hand, the application of REDUCE did not help C4.5 itself to induce a lower cost solution from examples described with fewer features. This study is also a step towards a better understanding of the notion of relevance for inductive concept learning.
Reference: 9. <author> R.S. Michalski and J.B. Larson. </author> <title> Inductive inference of VL decision rules. Paper pre sented at Workshop in Pattern-Directed Inference Systems, </title> <booktitle> Hawaii, </booktitle> <year> 1977. </year> <note> SIGART Newsletter, ACM 63 (1977) 38-44. </note>
Reference-contexts: The challenge was inspired by a problem posed by Ryszard Michalski <ref> [9] </ref>. The original challenge issued by Michie et al. [11] included three separate tasks. Donald Michie later issued a second challenge, involving a fourth task. Our experiments described here involve the first and fourth tasks.
Reference: 10. <author> R.S. Michalski, </author> <title> A theory and methodology of inductive learning, </title> <editor> In: R. Michalski, J. Carbonell and T. Mitchell (eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <address> Tioga (1983) 83-134. </address>
Reference-contexts: 1 Introduction The problem of relevance was addressed in early research on inductive concept learning <ref> [10] </ref>. Recently, this problem has also attracted much attention in the context of feature selection in attribute-value learning [1, 4, 12]. Basically one can say that all learners are concerned with the selection of `good' literals which will be used to construct the hypothesis.
Reference: 11. <author> D. Michie, S. Muggleton, D. Page, and A. Srinivasan. </author> <title> To the international computing community: A new East-West challenge. </title> <institution> Oxford University Computing Laboratory, </institution> <address> Oxford (1994). (URL ftp://ftp.comlab.ox.ac.uk/pub/ Packages/ILP/trains.tar.Z.). </address> <note> URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/trains/ contains information on both the 20 and 24 train challenges. URL http://www.gmd.de/ml-archive/ILP/public/data/east_west/ contains information on the 20 train challenge. </note>
Reference-contexts: Hypothesis H is consistent if it does not cover any negative example n 2 N . 4 Experimental results 4.1 The East-West Challenge and RL-ICET Michie et al. <ref> [11] </ref> issued a "challenge to the international computing community" to discover low size-complexity Prolog programs for classifying trains as Eastbound or Westbound. The challenge was inspired by a problem posed by Ryszard Michalski [9]. The original challenge issued by Michie et al. [11] included three separate tasks. <p> The East-West Challenge and RL-ICET Michie et al. <ref> [11] </ref> issued a "challenge to the international computing community" to discover low size-complexity Prolog programs for classifying trains as Eastbound or Westbound. The challenge was inspired by a problem posed by Ryszard Michalski [9]. The original challenge issued by Michie et al. [11] included three separate tasks. Donald Michie later issued a second challenge, involving a fourth task. Our experiments described here involve the first and fourth tasks.
Reference: 12. <author> D. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms, </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann (1994) 293-301. </publisher>
Reference-contexts: 1 Introduction The problem of relevance was addressed in early research on inductive concept learning [10]. Recently, this problem has also attracted much attention in the context of feature selection in attribute-value learning <ref> [1, 4, 12] </ref>. Basically one can say that all learners are concerned with the selection of `good' literals which will be used to construct the hypothesis.
Reference: 13. <author> P. Turney. </author> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <note> Journal of Artificial Intelligence Research 2 (1995) 369-409. [Available on the Internet at URL http://www.cs.washington.edu/research/jair/home.html.] </note>
Reference-contexts: The size-complexity of the Prolog program was calculated as the sum of the number of clause occurrences, the number of term occurrences, and the number of atom occurrences. A cost-sensitive algorithm ICET was developed for generating low-cost decision trees <ref> [13] </ref>. ICET is a hybrid of a genetic algorithm and a decision tree induction algorithm: it takes feature vectors as input and generates decision trees as output. The algorithm is sensitive to both the cost of features (attributes) and the cost of classification errors.
Reference: 14. <author> P. Turney. </author> <title> Low size-complexity inductive logic programming: The East-West Chal lenge as a problem in cost-sensitive classification. </title> <booktitle> In Advances in Inductive Logic Programming, </booktitle> <publisher> IOS Press (1996) 308-321. </publisher>
Reference-contexts: Section 3 presents the cost-sensitive literal elimination algorithm REDUCE. Section 4 introduces the problem domain, the 20 and the 24 trains East-West Challenges, and presents the results of our experiments that show that the performance of a hybrid genetic algorithm RL-ICET <ref> [14] </ref> can be significantly improved by applying REDUCE in preprocessing of the dataset. 2 Relevance of literals The representation formalism. <p> The algorithm is sensitive to both the cost of features (attributes) and the cost of classification errors. For the East-West Challenge, ICET was extended to handle Prolog input. The decision tree output was converted to Prolog manually. This algorithm is called RL-ICET (Relational Learning with ICET) <ref> [14] </ref>. RL-ICET is similar to the LINUS learning system [5]. RL-ICET uses a three-part strategy. First, a preprocessor translates the Prolog relations and predicates into a feature vector format. The preprocessor in RL-ICET was designed specially for the East-West Challenge, whereas LINUS has a general-purpose preprocessor. <p> Only average results are relevant for the comparison. 5 The performance in previous experiments by RL-ICET was measured by the cost of decision trees induced by ICET, as well as the complexity of Prolog programs after the RL-ICET transformation of decision trees into the Prolog program form <ref> [14] </ref>. <p> In this way, the complexity of the learning problem was reduced to about 7% (86/1199) of the initial learning problem [7]. Results of 10 runs of ICET on the 1199 feature set are the results reported in <ref> [14] </ref>, whereas results of 10 runs of ICET on the training examples described with 86 features are new. The results show that the efficiency of learning significantly increased.
Reference: 15. <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. Morgan Kaufmann (1993). This article was processed using the LT E X macro package with LLNCS style </title>
Reference-contexts: Some other experiments were performed and further experimentation is planned along these lines. In order to evaluate the effects of feature reduction, we have compared the results of ICET (with and without feature reduction) with the results achieved using C4.5 <ref> [15] </ref>. In both experiments, feature reduction (re-duction to 86 and 116 features, respectively) helped ICET to outperform C4.5 when comparing costs of decision trees, both in terms of minimal and average costs [8].
References-found: 15

