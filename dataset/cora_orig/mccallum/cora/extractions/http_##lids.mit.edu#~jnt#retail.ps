URL: http://lids.mit.edu/~jnt/retail.ps
Refering-URL: http://web.mit.edu/dimitrib/www/publ.html
Root-URL: 
Title: A Neuro-Dynamic Programming Approach to Retailer Inventory Management 1  
Author: Benjamin Van Roy yz Dimitri P. Bertsekas Yuchun Lee John N. Tsitsiklis and 
Note: 1 This material is based upon work supported by the National Science Foundation under award number 9561500. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the National Science Foundation.  
Address: Lincoln North Lincoln, MA 01773  Cambridge, MA 02139  
Affiliation: Unica Technologies, Inc.  Laboratory for Information and Decision Systems Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abounadi, J., Bertsekas, </author> <title> D.P., and Borkar, V.S. (1996) "ODE Analysis for Q-Learning Algorithms," Lab for Information and Decision Systems Draft Report, </title> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Also, since performance is measured in terms of average cost, formulating the problem in terms of average-cost (rather than discounted cost) dynamic programming and employing NDP algorithms that directly address such formulations may enhance performance. Such algorithms are discussed in (Bertsekas and Tsitsiklis, 1996) and analyzed in <ref> (Abounadi et al., 1996) </ref> and (Tsitsiklis and Van Roy, 1996). Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters.
Reference: <author> Bertsekas, D. P. </author> <title> (1995) Dynamic Programming and Optimal Control, </title> <publisher> Athena Scientific, </publisher> <address> Bellmont, MA. </address>
Reference-contexts: an exhaustive search of this type would be computationally prohibitive if we allowed the stores to have different order-up-to levels, which would be called for if the stores had independent attributes (e.g., different transportation delays). 5 Dynamic Programming Dynamic programming (DP) offers a very general framework for stochastic control problems <ref> (Bertsekas, 1995) </ref>. In this section, we present a DP framework that is a bit different from the standard. In particular, our setting is somewhat specialized to the retailer inventory problem and leads to more efficient computational approaches in the context of neuro-dynamic programming (NDP).
Reference: <author> Bertsekas, D. P., and Tsitsiklis, J. N. </author> <title> (1996) Neuro-Dynamic Programming, </title> <publisher> Athena Scientific, </publisher> <address> Bellmont, MA. </address>
Reference-contexts: As a result, complex stochastic control problems that arise in the real world are usually addressed using drastically simplified analyses and/or heuristics. An exciting new alternative that is more closely tied to the sound framework of dynamic programming is being developed in the emerging field of neuro-dynamic programming <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. This approach makes use of ideas from artificial intelligence involving simulation-based algorithms and functional approximation techniques such as neural networks. The outcome is a methodology for approximating dynamic programming solutions without demanding the associated computational requirements. <p> We initially selected two neuro-dynamic programming algorithms and specialized them for the purposes of retailer inventory management. The two algorithms were approximate policy iteration and an on-line temporal-difference method <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. In solving a stochastic control problem like that of retailer inventory management, neuro-dynamic programming algorithms approximate the problem's cost-to-go function by tuning parameters of an approximation architecture. Appropriate approximation architectures must be chosen for the problem at hand. <p> The subsequent policy is then generated via i+1 (x) = arg min ~ J (f 2 (x; u); r i ): There have been many methods used for approximating J i in each ith policy iteration. A comprehensive survey is provided in <ref> (Bertsekas and Tsit-siklis, 1996) </ref>. <p> We refer the reader to <ref> (Bertsekas and Tsitsiklis, 1996) </ref> for a detailed discussion of this method. 6.2 An On-Line Temporal-Difference Method Variants of the temporal-difference algorithm (Sutton, 1988; Tsitsiklis and Van Roy, 1996) have been applied successfully to several large scale applications of NDP. <p> In this section, we present the algorithm in its initial form. This algorithm may be viewed as an extreme form of "optimistic approximate policy iteration," as discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. As mentioned in Section 2, this algorithm was not successful until we added active exploration, which is discussed in the next section. The algorithm updates the parameter vector of an approximation architecture during each step of a single endless simulation. <p> Also, since performance is measured in terms of average cost, formulating the problem in terms of average-cost (rather than discounted cost) dynamic programming and employing NDP algorithms that directly address such formulations may enhance performance. Such algorithms are discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref> and analyzed in (Abounadi et al., 1996) and (Tsitsiklis and Van Roy, 1996). Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters.
Reference: <author> Crites, R. H., and Barto, A. </author> <title> G (1996) "Improving Elevator Performance Using Reinforcement Learning," </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <editor> Touretzky, D. S., Mozer, M. C., and Hasselmo, </editor> <publisher> M. </publisher>
Reference-contexts: Over the past few years, neuro-dynamic programming methods have generated several notable success stories. Examples include a program that plays Backgammon at the world champion level (Tesauro, 1992), an elevator dispatcher that is more efficient than several heuristics employed in practice <ref> (Crites and Barto, 1996) </ref>, and an approach to job shop scheduling (Zhang and Dietterich, 1996). Additional case studies reported by Bertsekas and Tsitsiklis (1996) further demonstrate significant promise for neuro-dynamic programming. <p> Examples include a Backgammon player (Tesauro, 1992), an elevator dispatcher <ref> (Crites and Barto, 1996) </ref>, and a job shop scheduling method (Zhang and Dietterich, 1996). The variants used in these applications bear significant differences, and in this research project, we tried to use a simple algorithm that possessed what we felt were the most important properties.
Reference: <editor> E., eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Lee, H. L., and Billington, C. </author> <title> (1993) "Material Management in Decentralized Supply Chains," </title> <journal> Operations Research, </journal> <volume> vol. 41, no. 5, </volume> <pages> pp. 835-847. </pages>
Reference-contexts: This problem can also be viewed as a simple example from the broad class of multi-echelon inventory control problems that has received significant attention in the field of supply-chain management <ref> (Lee and Billington, 1993) </ref>. The remainder of this paper is organized as follows. The next section provides an overview of the research and results obtained.
Reference: <author> Nahmias, S., and Smith, S. A. </author> <title> (1993) "Mathematical Models of Inventory Retailer Systems: A Review," </title> <booktitle> Perspectives on Operations Management, Essays in Honor of Elwood S. </booktitle> <editor> Buffa, Sarin, R., editor, </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <pages> pp. 249-278. </pages>
Reference-contexts: Because of this, application of neuro-dynamic programming often requires trial and error, in a long process of parameter tweaking and experimentation. In this paper, we describe work directed towards developing a streamlined neuro-dynamic programming approach for optimizing performance of 1 retailer inventory systems <ref> (Nahmias and Smith, 1993) </ref>. This is the problem of ordering and positioning retailer inventory at warehouses and stores in order to meet customer demands while simultaneously minimizing storage and transportation costs. <p> The characteristics of this model are largely motivated by the studies of <ref> (Nahmias and Smith, 1993) </ref> and (Nahmias and Smith, 1994). The general structure is illustrated in Figure 1 and involves several stages: 1. Transportation of products from manufacturers 2. Packaging and storage of products at a central warehouse 3. Delivery of products from the warehouse to stores 4. <p> The type of heuristic used is known as an s-type, or "order-up-to," policy and is accepted as a reasonable approach to problem formulations that have independent identically distributed demands, like the one we have proposed. Examples of research where such policies are the focus of study are discussed in <ref> (Nahmias and Smith, 1993) </ref> and include (Nahmias and Smith, 1994). The s-type policy we implemented is parameterized by two values: a warehouse order-up-to level and a store order-up-to level.
Reference: <author> Nahmias, S., and Smith, S. A. </author> <title> (1994) "Optimizing Inventory Levels in a Two Echelon Retailer System with Partial Lost Sales," </title> <journal> Management Science, </journal> <volume> Vol. 40, </volume> <pages> pp. 582-596. </pages>
Reference-contexts: The characteristics of this model are largely motivated by the studies of (Nahmias and Smith, 1993) and <ref> (Nahmias and Smith, 1994) </ref>. The general structure is illustrated in Figure 1 and involves several stages: 1. Transportation of products from manufacturers 2. Packaging and storage of products at a central warehouse 3. Delivery of products from the warehouse to stores 4. <p> Examples of research where such policies are the focus of study are discussed in (Nahmias and Smith, 1993) and include <ref> (Nahmias and Smith, 1994) </ref>. The s-type policy we implemented is parameterized by two values: a warehouse order-up-to level and a store order-up-to level.
Reference: <author> Sutton, R. S. </author> <title> (1988) "Learning to Predict by the Methods of Temporal Differences," </title> <journal> Machine Learning, </journal> <volume> vol. </volume> <pages> 3. </pages>
Reference: <author> Tesauro, G. J. </author> <title> (1992) "Practical Issues in Temporal-Difference Learning," </title> <journal> Machine Learning, </journal> <volume> vol. </volume> <pages> 8. </pages>
Reference-contexts: The outcome is a methodology for approximating dynamic programming solutions without demanding the associated computational requirements. Over the past few years, neuro-dynamic programming methods have generated several notable success stories. Examples include a program that plays Backgammon at the world champion level <ref> (Tesauro, 1992) </ref>, an elevator dispatcher that is more efficient than several heuristics employed in practice (Crites and Barto, 1996), and an approach to job shop scheduling (Zhang and Dietterich, 1996). Additional case studies reported by Bertsekas and Tsitsiklis (1996) further demonstrate significant promise for neuro-dynamic programming. <p> We refer the reader to (Bertsekas and Tsitsiklis, 1996) for a detailed discussion of this method. 6.2 An On-Line Temporal-Difference Method Variants of the temporal-difference algorithm (Sutton, 1988; Tsitsiklis and Van Roy, 1996) have been applied successfully to several large scale applications of NDP. Examples include a Backgammon player <ref> (Tesauro, 1992) </ref>, an elevator dispatcher (Crites and Barto, 1996), and a job shop scheduling method (Zhang and Dietterich, 1996). The variants used in these applications bear significant differences, and in this research project, we tried to use a simple algorithm that possessed what we felt were the most important properties.
Reference: <author> Tsitsiklis, J. N., and Van Roy, B. </author> <title> (1996) "An Analysis of Temporal-Difference Learning with Function Approximation," </title> <note> to appear in IEEE Transactions on Automatic Control. </note>
Reference-contexts: As a result, complex stochastic control problems that arise in the real world are usually addressed using drastically simplified analyses and/or heuristics. An exciting new alternative that is more closely tied to the sound framework of dynamic programming is being developed in the emerging field of neuro-dynamic programming <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. This approach makes use of ideas from artificial intelligence involving simulation-based algorithms and functional approximation techniques such as neural networks. The outcome is a methodology for approximating dynamic programming solutions without demanding the associated computational requirements. <p> We initially selected two neuro-dynamic programming algorithms and specialized them for the purposes of retailer inventory management. The two algorithms were approximate policy iteration and an on-line temporal-difference method <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. In solving a stochastic control problem like that of retailer inventory management, neuro-dynamic programming algorithms approximate the problem's cost-to-go function by tuning parameters of an approximation architecture. Appropriate approximation architectures must be chosen for the problem at hand. <p> We refer the reader to <ref> (Bertsekas and Tsitsiklis, 1996) </ref> for a detailed discussion of this method. 6.2 An On-Line Temporal-Difference Method Variants of the temporal-difference algorithm (Sutton, 1988; Tsitsiklis and Van Roy, 1996) have been applied successfully to several large scale applications of NDP. <p> In this section, we present the algorithm in its initial form. This algorithm may be viewed as an extreme form of "optimistic approximate policy iteration," as discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. As mentioned in Section 2, this algorithm was not successful until we added active exploration, which is discussed in the next section. The algorithm updates the parameter vector of an approximation architecture during each step of a single endless simulation. <p> Also, since performance is measured in terms of average cost, formulating the problem in terms of average-cost (rather than discounted cost) dynamic programming and employing NDP algorithms that directly address such formulations may enhance performance. Such algorithms are discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref> and analyzed in (Abounadi et al., 1996) and (Tsitsiklis and Van Roy, 1996). Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters. <p> Such algorithms are discussed in (Bertsekas and Tsitsiklis, 1996) and analyzed in (Abounadi et al., 1996) and <ref> (Tsitsiklis and Van Roy, 1996) </ref>. Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters. In particular, special thanks go to Ruby Kennedy, Bob Crites, and Steve Patek.
Reference: <author> Tsitsiklis, J. N., and Van Roy, B. </author> <title> (1996) "Average Cost Temporal-Difference Learning," </title> <note> working paper. </note>
Reference-contexts: As a result, complex stochastic control problems that arise in the real world are usually addressed using drastically simplified analyses and/or heuristics. An exciting new alternative that is more closely tied to the sound framework of dynamic programming is being developed in the emerging field of neuro-dynamic programming <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. This approach makes use of ideas from artificial intelligence involving simulation-based algorithms and functional approximation techniques such as neural networks. The outcome is a methodology for approximating dynamic programming solutions without demanding the associated computational requirements. <p> We initially selected two neuro-dynamic programming algorithms and specialized them for the purposes of retailer inventory management. The two algorithms were approximate policy iteration and an on-line temporal-difference method <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. In solving a stochastic control problem like that of retailer inventory management, neuro-dynamic programming algorithms approximate the problem's cost-to-go function by tuning parameters of an approximation architecture. Appropriate approximation architectures must be chosen for the problem at hand. <p> We refer the reader to <ref> (Bertsekas and Tsitsiklis, 1996) </ref> for a detailed discussion of this method. 6.2 An On-Line Temporal-Difference Method Variants of the temporal-difference algorithm (Sutton, 1988; Tsitsiklis and Van Roy, 1996) have been applied successfully to several large scale applications of NDP. <p> In this section, we present the algorithm in its initial form. This algorithm may be viewed as an extreme form of "optimistic approximate policy iteration," as discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref>. As mentioned in Section 2, this algorithm was not successful until we added active exploration, which is discussed in the next section. The algorithm updates the parameter vector of an approximation architecture during each step of a single endless simulation. <p> Also, since performance is measured in terms of average cost, formulating the problem in terms of average-cost (rather than discounted cost) dynamic programming and employing NDP algorithms that directly address such formulations may enhance performance. Such algorithms are discussed in <ref> (Bertsekas and Tsitsiklis, 1996) </ref> and analyzed in (Abounadi et al., 1996) and (Tsitsiklis and Van Roy, 1996). Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters. <p> Such algorithms are discussed in (Bertsekas and Tsitsiklis, 1996) and analyzed in (Abounadi et al., 1996) and <ref> (Tsitsiklis and Van Roy, 1996) </ref>. Acknowledgments We would like to thank colleagues at Unica and the Laboratory for Information and Decision Systems for useful discussions, feedback, proof-reading, and help with various other matters. In particular, special thanks go to Ruby Kennedy, Bob Crites, and Steve Patek.
Reference: <author> Zhang, W., and Dietterich, T. G. </author> <title> (1995) "A Reinforcement Learning Approach to Job Shop Scheduling," </title> <booktitle> Proceedings of the IJCAI. </booktitle> <pages> 33 </pages>
References-found: 13

