URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR42.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: martens@cis.ohio-state.edu  
Phone: (614) 292-1932  
Title: A Hierarchically-Connected Multiprocessor  
Author: Jeffrey D. Martens 
Note: Revision of  This work was presented as a poster at the 1994 Scalable High-Performance Computing Conference.  
Date: May 1, 1993  June 5, 1994  
Address: Columbus, Ohio 43210-1277  
Affiliation: The Ohio State University Department of Computer and Information Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. S. Almasi and A. Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cummings, New York, </address> <year> 1989. </year>
Reference-contexts: Other large-scale SMMs have been designed with processing elements (PEs) connected to one another via a MIN, with each PE containing a portion of the shared memory as its local memory. Representatives of this class include the BBN Butterfly [5], the IBM RP3 <ref> [1] </ref>, and PASM [33].
Reference: [2] <author> Gordon Bell. </author> <title> Ultracomputers: a Teraflop Before its Time. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 26-48, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Small-scale shared-memory multiprocessors have been built so that all addresses are equally accessible from every processor. Indeed, some authors have considered uniform accessibility to be a characteristic of shared memory <ref> [2] </ref>. The problem with this approach is two-fold: (1) all accesses are equally expensive, implying that all memory accesses in a large-scale machine will be expensive; (2) due to contention, access times cannot actually be uniform. <p> A typical requirement for considering a machine to be an SMM is that the machine have a single address space which is accessible by every PE <ref> [2] </ref>. A characteristic typical of SMMs is that the time to access a single nonlocal memory location is much lower than the time to transmit a small message on a DMM; this startup time is typically masked in a DMM by packing many words into a single message.
Reference: [3] <author> Randy Bramley and J. Bordner. </author> <title> Sequential Optimization and Data Distribution for ARC2D on the Cedar Hierarchical Multiprocessor. </title> <type> Technical Report 1128, </type> <institution> Center of Supercomputing Research and Development, University of Illinois, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: These problems have been addressed through various software techniques designed to split a variable among multiple memory units [34, 15] and to keep busy-waiting traffic to a minimum [28, 27, 9], and utilize local memories as much as possible, accessing global memory only when sharing is needed <ref> [3, 8] </ref>. The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23].
Reference: [4] <author> Eugene D. Brooks III. </author> <title> The Butterfly Barrier. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 295-307, </pages> <year> 1986. </year>
Reference-contexts: consumers so that there is little or no contention [15, 17, 25]. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion <ref> [4, 13, 34] </ref>. This tree barrier is illustrated in figure 9. In figure 9, each dark circle represents a synchronization point. Time progresses as we move from the bottom to the top of the figure, and the number of levels in the tree is log 2 p.
Reference: [5] <author> W. J. Downey III. </author> <title> Inside the GP1000. BBN Advanced Computers, </title> <publisher> Inc., </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Other large-scale SMMs have been designed with processing elements (PEs) connected to one another via a MIN, with each PE containing a portion of the shared memory as its local memory. Representatives of this class include the BBN Butterfly <ref> [5] </ref>, the IBM RP3 [1], and PASM [33].
Reference: [6] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring FORTRAN Programs for Cedar. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 57-66, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: the HCM, GALM was chosen. 2.4.2 Performance with Poor or Naive Partitions One advantage that SMMs have over DMMs is that if the program is naively partitioned, data can still be directly accessed, though possibly at a penalty of greater access time than if the data had been partitioned optimally <ref> [6] </ref>. This is true for either Cedar or the THMM, since it is always possible to place all data at the global level.
Reference: [7] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D Language Specification. </title> <type> Technical Report COMP TR90-141, </type> <institution> Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23]. At the same time, there has been a complementary attempt to provide compiler support to allow the programmer to view a DMM as a SMM <ref> [7, 12, 19] </ref>. 2.1 The HCM We propose three logically-equivalent implementations of a hierarchically-connected multiprocessor. These three implementations are multiple networks, fast finishing, and fat trees, all of which are discussed below.
Reference: [8] <author> Kyle Gallivan, William Jalby, and Dennis Gannon. </author> <title> On the Problem of Optimizing Data Transfers for Complex Memory Systems. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: These problems have been addressed through various software techniques designed to split a variable among multiple memory units [34, 15] and to keep busy-waiting traffic to a minimum [28, 27, 9], and utilize local memories as much as possible, accessing global memory only when sharing is needed <ref> [3, 8] </ref>. The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23].
Reference: [9] <author> Lal George. </author> <title> A Scheduling Strategy for Shared Memory Multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 67-71, </pages> <year> 1990. </year>
Reference-contexts: Representatives of this class include the BBN Butterfly [5], the IBM RP3 [1], and PASM [33]. For essentially nonhierarchical designs such as the above, researchers have convincingly shown that globally-accessible local memory (GALM) is a very useful feature in greatly reducing contention <ref> [9, 28, 27] </ref>; furthermore, as will be argued below, latency (in terms of number of switch stages traversed) can be reduced through the use of GALM. <p> These problems have been addressed through various software techniques designed to split a variable among multiple memory units [34, 15] and to keep busy-waiting traffic to a minimum <ref> [28, 27, 9] </ref>, and utilize local memories as much as possible, accessing global memory only when sharing is needed [3, 8]. The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23].
Reference: [10] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry Rudolph, and Marc Snir. </author> <title> The NYU Ultracomputer|Designing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> February </month> <year> 1983. </year> <month> 14 </month>
Reference-contexts: To allow some accesses to be less expensive, shared-memory multiprocessors (SMMs) have been designed with local memories and/or caches; an example is the NYU Ultracomputer <ref> [10] </ref>. Thus on a multiprocessor with local memory, a program that can be written (or restructured) to exploit data locality will exhibit much less traffic within the interconnect, and thus be less adversely impacted by latency and contention. Multiprocessors in this category are termed non-uniform memory access (NUMA) machines. <p> Some large-scale SMMs have been designed as dance-hall machines, with the processors and the shared memories physically separate, and connected by a multistage interconnection network (MIN); the NYU Ultracomputer <ref> [10] </ref> falls into this category. Other large-scale SMMs have been designed with processing elements (PEs) connected to one another via a MIN, with each PE containing a portion of the shared memory as its local memory.
Reference: [11] <author> R. I. Greenberg and C. E. Leiserson. </author> <title> Randomized Routing in Fat-Trees. </title> <booktitle> In IEEE 26th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1985. </year>
Reference-contexts: This suggests that a more explicitly tree-structured network may bear investigation. The fat tree is a network which is tree-structured with processors as leaves and switches as internal nodes <ref> [11, 22] </ref>. The reason the 6 tree is termed "fat" is that the bandwidth between nodes is higher closer to the root than near the PEs|the tree does not necessarily grow narrower as the root is approached.
Reference: [12] <author> Andrew S. Grimshaw. </author> <title> Easy-to-Use Object-Oriented Parallel Processing With Mentat. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23]. At the same time, there has been a complementary attempt to provide compiler support to allow the programmer to view a DMM as a SMM <ref> [7, 12, 19] </ref>. 2.1 The HCM We propose three logically-equivalent implementations of a hierarchically-connected multiprocessor. These three implementations are multiple networks, fast finishing, and fat trees, all of which are discussed below.
Reference: [13] <author> Debra Hensgen, Raphael Finkel, and Udi Manber. </author> <title> Two Algorithms for Barrier Synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: consumers so that there is little or no contention [15, 17, 25]. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion <ref> [4, 13, 34] </ref>. This tree barrier is illustrated in figure 9. In figure 9, each dark circle represents a synchronization point. Time progresses as we move from the bottom to the top of the figure, and the number of levels in the tree is log 2 p.
Reference: [14] <author> D. N. Jayasimha. </author> <title> Parallel Access to Synchronization Variables. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 97-100, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: In a structured case, it is often possible to arrange the producers and consumers so that there is little or no contention [15, 17, 25]. 10 through backoff techniques, in which PEs pause between accesses of the counter <ref> [14] </ref>, or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion [4, 13, 34]. This tree barrier is illustrated in figure 9. In figure 9, each dark circle represents a synchronization point.
Reference: [15] <author> D. N. Jayasimha. </author> <title> Partially Shared Variables and Hierarchical Shared Memory Multiprocessor Architectures. </title> <booktitle> In 11th Annual IEEE International Conference on Computers and Communications, </booktitle> <pages> pages 63-71, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Multiprocessors in this category are termed non-uniform memory access (NUMA) machines. In order to better take advantage of locality, hierarchical designs have arisen in which, for each processor, some memories are more efficiently accessible than others <ref> [15, 20, 21, 24] </ref>. These machines allow efficient sharing among various subsets of processors. In this paper, we consider any machine with three or more logical levels of memory (e.g., local, cluster, and global) to be hierarchical, and any machine with just one or two levels to be nonhierarchical. <p> Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM <ref> [15, 17, 25] </ref>, the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE [30, 35], the result being that the interconnect itself, rather than the memory per se, is hierarchical. <p> On the other hand, it has been commonly argued that SMMs are not scalable due to contention for shared variables and latency of access to global memories. These problems have been addressed through various software techniques designed to split a variable among multiple memory units <ref> [34, 15] </ref> and to keep busy-waiting traffic to a minimum [28, 27, 9], and utilize local memories as much as possible, accessing global memory only when sharing is needed [3, 8]. <p> In a structured case, it is often possible to arrange the producers and consumers so that there is little or no contention <ref> [15, 17, 25] </ref>. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion [4, 13, 34].
Reference: [16] <author> D. N. Jayasimha. </author> <type> Personal communication, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: In the worst case, multiple producer-consumer pairs can cause considerable contention as the consumers spin on their various not-yet-produced items. 3 Jayasimha has pointed out that this situation can be greatly improved by incorporating a signalling mechanism by which the memory unit informs the consumer when the desired item arrives <ref> [16] </ref>. The produced item still must travel from the producer to the memory, and then from the memory to the consumer, rather than directly from the producer to the consumer, increasing the latency relative to what would be expected on the HCM.
Reference: [17] <author> D. N. Jayasimha and J. D. Martens. </author> <title> Some Architectural and Compilation Issues in the Design of Hierarchical Shared Memory Multiprocessors. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 567-572, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM <ref> [15, 17, 25] </ref>, the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE [30, 35], the result being that the interconnect itself, rather than the memory per se, is hierarchical. <p> In a structured case, it is often possible to arrange the producers and consumers so that there is little or no contention <ref> [15, 17, 25] </ref>. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion [4, 13, 34].
Reference: [18] <author> Harry F. Jordan. </author> <title> Shared Versus Distributed Memory Multiprocessors. </title> <type> Technical Report 91-7, </type> <institution> ICASE, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The reasons for this are outlined in this section. 3.1 Producer-Consumer Communication Jordan <ref> [18] </ref> divides producer-consumer communication into two categories: consumer-initiated, and producer-initiated. 3.1.1 Producer-Initiated Communication In producer-initiated communication, the producer "knows" the identity of the consumer, and thus writes into the location where the consumer expects to find the produced item.
Reference: [19] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23]. At the same time, there has been a complementary attempt to provide compiler support to allow the programmer to view a DMM as a SMM <ref> [7, 12, 19] </ref>. 2.1 The HCM We propose three logically-equivalent implementations of a hierarchically-connected multiprocessor. These three implementations are multiple networks, fast finishing, and fat trees, all of which are discussed below.
Reference: [20] <author> J. Konicek et al. </author> <title> The Organization of the Cedar System. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 49-56, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Multiprocessors in this category are termed non-uniform memory access (NUMA) machines. In order to better take advantage of locality, hierarchical designs have arisen in which, for each processor, some memories are more efficiently accessible than others <ref> [15, 20, 21, 24] </ref>. These machines allow efficient sharing among various subsets of processors. In this paper, we consider any machine with three or more logical levels of memory (e.g., local, cluster, and global) to be hierarchical, and any machine with just one or two levels to be nonhierarchical. <p> Among hierarchical SMMs, some machines (e.g., Cedar <ref> [20] </ref>, the THMM [15, 17, 25], the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE [30, 35], the result being that the interconnect itself, rather than the memory per se, is hierarchical. <p> For the sake of the current paper, we assume that the the global memories are not contained within corresponding PEs. 7 2.3 Cedar Cedar <ref> [20] </ref> is a hierarchical SMM with a three-level hierarchy: local cache, cluster, and global. Within a cluster there is hardware for fast synchronization; across clusters, synchronization is handled through global memory. None of the global memory units is local to any PE [20]. <p> contained within corresponding PEs. 7 2.3 Cedar Cedar <ref> [20] </ref> is a hierarchical SMM with a three-level hierarchy: local cache, cluster, and global. Within a cluster there is hardware for fast synchronization; across clusters, synchronization is handled through global memory. None of the global memory units is local to any PE [20]. Although the Cedar prototype has only four clusters of eight PEs each, it was conceived with scalability in mind. In this paper, Cedar is used to refer to a large-scale SMM structured similarly to the Cedar prototype.
Reference: [21] <author> James T. Kuehn and Burton J. Smith. </author> <title> The Horizon Supercomputing System: </title> <booktitle> Architecture and Software. In Supercomputing '88, </booktitle> <pages> pages 28-34, </pages> <year> 1988. </year>
Reference-contexts: Multiprocessors in this category are termed non-uniform memory access (NUMA) machines. In order to better take advantage of locality, hierarchical designs have arisen in which, for each processor, some memories are more efficiently accessible than others <ref> [15, 20, 21, 24] </ref>. These machines allow efficient sharing among various subsets of processors. In this paper, we consider any machine with three or more logical levels of memory (e.g., local, cluster, and global) to be hierarchical, and any machine with just one or two levels to be nonhierarchical.
Reference: [22] <author> Charles E. Leiserson. Fat-Trees: </author> <title> Networks for Hardware-Efficient Supercomputing. </title> <journal> IEEE Transactions on Computing, </journal> <volume> C-34:892-901, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: This suggests that a more explicitly tree-structured network may bear investigation. The fat tree is a network which is tree-structured with processors as leaves and switches as internal nodes <ref> [11, 22] </ref>. The reason the 6 tree is termed "fat" is that the bandwidth between nodes is higher closer to the root than near the PEs|the tree does not necessarily grow narrower as the root is approached.
Reference: [23] <author> Calvin Lin and Larry Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM <ref> [23] </ref>. At the same time, there has been a complementary attempt to provide compiler support to allow the programmer to view a DMM as a SMM [7, 12, 19]. 2.1 The HCM We propose three logically-equivalent implementations of a hierarchically-connected multiprocessor.
Reference: [24] <author> Stephen A. Mabbs and Kevin E. </author> <title> Forward. Optimizing the Communication Architecture of a Hierarchical Parallel Processor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 516-520, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Multiprocessors in this category are termed non-uniform memory access (NUMA) machines. In order to better take advantage of locality, hierarchical designs have arisen in which, for each processor, some memories are more efficiently accessible than others <ref> [15, 20, 21, 24] </ref>. These machines allow efficient sharing among various subsets of processors. In this paper, we consider any machine with three or more logical levels of memory (e.g., local, cluster, and global) to be hierarchical, and any machine with just one or two levels to be nonhierarchical. <p> Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM [15, 17, 25], the HMS <ref> [24] </ref>) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE [30, 35], the result being that the interconnect itself, rather than the memory per se, is hierarchical. <p> The number of clusters and the number of PEs per cluster are not taken to be fixed quantities herein. An architecture that is in many respects similar to Cedar is the three-level Hierarchical Memory Structure (HMS) of Mabbs and Forward <ref> [24] </ref>. The three levels in this HMS are local memory, cluster memory, and global memory. The local memory is private to the associated PE, and none of the global or cluster memory is local to any PE.
Reference: [25] <author> J. D. Martens and D. N. Jayasimha. </author> <title> A Tree-Structured Hierarchical Memory Multiprocessor. </title> <type> Technical Report 61, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> December </month> <year> 1989. </year> <month> Revised May </month> <year> 1990. </year> <booktitle> An extended abstract appeared in International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 561-562, </pages> <month> August </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM <ref> [15, 17, 25] </ref>, the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE [30, 35], the result being that the interconnect itself, rather than the memory per se, is hierarchical. <p> that column of the network; this reduces latency for packets that get to the proper column before the last switch stage (they need pass through fewer stages), and reduces contention in later stages of the interconnect since there are fewer messages traveling through those stages (this was also noted in <ref> [25, 26, 35] </ref>). Padmanabhan calls this use of comparators in the switch elements and connections from them directly to the memory units fast finishing. <p> In a structured case, it is often possible to arrange the producers and consumers so that there is little or no contention <ref> [15, 17, 25] </ref>. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion [4, 13, 34].
Reference: [26] <author> J. D. Martens and D. N. Jayasimha. </author> <title> Performance of a Tree-Structured Hierarchical Memory Multiprocessor. </title> <type> Technical Report 7, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: that column of the network; this reduces latency for packets that get to the proper column before the last switch stage (they need pass through fewer stages), and reduces contention in later stages of the interconnect since there are fewer messages traveling through those stages (this was also noted in <ref> [25, 26, 35] </ref>). Padmanabhan calls this use of comparators in the switch elements and connections from them directly to the memory units fast finishing.
Reference: [27] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Representatives of this class include the BBN Butterfly [5], the IBM RP3 [1], and PASM [33]. For essentially nonhierarchical designs such as the above, researchers have convincingly shown that globally-accessible local memory (GALM) is a very useful feature in greatly reducing contention <ref> [9, 28, 27] </ref>; furthermore, as will be argued below, latency (in terms of number of switch stages traversed) can be reduced through the use of GALM. <p> These problems have been addressed through various software techniques designed to split a variable among multiple memory units [34, 15] and to keep busy-waiting traffic to a minimum <ref> [28, 27, 9] </ref>, and utilize local memories as much as possible, accessing global memory only when sharing is needed [3, 8]. The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23]. <p> These are low-contention spin waiting and reduced latency of naively partitioned data sets. 2.4.1 Spin Waiting Mellor-Crummey and Scott have demonstrated the utility of globally-accessible local memory (GALM) for synchronization and communication <ref> [27, 28] </ref>. The main idea is that if the software can be structured so that waiting PEs check the status of local variables only, then no busy-wait traffic 8 arises. Several algorithms are given for accomplishing this in [27, 28]; these algorithms assume that the multiprocessor supports fetch-and-op instructions and that <p> demonstrated the utility of globally-accessible local memory (GALM) for synchronization and communication <ref> [27, 28] </ref>. The main idea is that if the software can be structured so that waiting PEs check the status of local variables only, then no busy-wait traffic 8 arises. Several algorithms are given for accomplishing this in [27, 28]; these algorithms assume that the multiprocessor supports fetch-and-op instructions and that there is either GALM or coherent caches; for the HCM, GALM was chosen. 2.4.2 Performance with Poor or Naive Partitions One advantage that SMMs have over DMMs is that if the program is naively partitioned, data can still <p> Some aspects of the algorithm in figure 10 bear discussion. Each PE executes procedure HGSS in parallel, with the parameter m indicating the PE number. The call to allocate locks the selected PE iteration set; these locks can be written to avoid nonlocal spinning <ref> [27, 28] </ref>, so lock contention is not expected to be a serious problem. allocate can return an empty iteration range (indicated by lo &gt; hi); assuming Pascal-like for loop semantics, HGSS executes zero iterations in such cases.
Reference: [28] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Synchronization without Contention. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Representatives of this class include the BBN Butterfly [5], the IBM RP3 [1], and PASM [33]. For essentially nonhierarchical designs such as the above, researchers have convincingly shown that globally-accessible local memory (GALM) is a very useful feature in greatly reducing contention <ref> [9, 28, 27] </ref>; furthermore, as will be argued below, latency (in terms of number of switch stages traversed) can be reduced through the use of GALM. <p> These problems have been addressed through various software techniques designed to split a variable among multiple memory units [34, 15] and to keep busy-waiting traffic to a minimum <ref> [28, 27, 9] </ref>, and utilize local memories as much as possible, accessing global memory only when sharing is needed [3, 8]. The result is that, for maximum performance, it is necessary to borrow some distributed-memory techniques in order to program a SMM [23]. <p> These are low-contention spin waiting and reduced latency of naively partitioned data sets. 2.4.1 Spin Waiting Mellor-Crummey and Scott have demonstrated the utility of globally-accessible local memory (GALM) for synchronization and communication <ref> [27, 28] </ref>. The main idea is that if the software can be structured so that waiting PEs check the status of local variables only, then no busy-wait traffic 8 arises. Several algorithms are given for accomplishing this in [27, 28]; these algorithms assume that the multiprocessor supports fetch-and-op instructions and that <p> demonstrated the utility of globally-accessible local memory (GALM) for synchronization and communication <ref> [27, 28] </ref>. The main idea is that if the software can be structured so that waiting PEs check the status of local variables only, then no busy-wait traffic 8 arises. Several algorithms are given for accomplishing this in [27, 28]; these algorithms assume that the multiprocessor supports fetch-and-op instructions and that there is either GALM or coherent caches; for the HCM, GALM was chosen. 2.4.2 Performance with Poor or Naive Partitions One advantage that SMMs have over DMMs is that if the program is naively partitioned, data can still <p> Some aspects of the algorithm in figure 10 bear discussion. Each PE executes procedure HGSS in parallel, with the parameter m indicating the PE number. The call to allocate locks the selected PE iteration set; these locks can be written to avoid nonlocal spinning <ref> [27, 28] </ref>, so lock contention is not expected to be a serious problem. allocate can return an empty iteration range (indicated by lo &gt; hi); assuming Pascal-like for loop semantics, HGSS executes zero iterations in such cases.
Reference: [29] <author> Krishnan Padmanabhan. </author> <title> Cube Structures for Multiprocessors. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 43-52, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Inputs to the network are on the left, and outputs on the right. The fast finishing lines are bold. Note that input i and output i both connect to PE i . The hierarchically-connected multiprocessor (HCM) is based on the indirect binary cube network <ref> [29, 32] </ref>. A sixteen PE indirect cube is shown in figure 6. 1 The indirect cube network mentioned has p inputs and p outputs, for a total of p PEs.
Reference: [30] <author> Krishnan Padmanabhan. </author> <title> Efficient Architectures for Data Access in a Shared Memory Hierarchy. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 314-327, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM [15, 17, 25], the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE <ref> [30, 35] </ref>, the result being that the interconnect itself, rather than the memory per se, is hierarchical. In this paper the advantages of using a hierarchical interconnect|with all memory at the PEs themselves|rather than a hierarchical memory structure are explored. <p> A third level can be added by adding a second set of networks as in figure 2. A four-level system (local, small cluster, large cluster, global) based on multiple networks is shown in figure 3. 2.1.2 Fast Finishing 2 3 Padmanabhan <ref> [30] </ref> points out that an address comparator can be added after one or more stages of the network to pull out packets destined for the memory unit in that column of the network; this reduces latency for packets that get to the proper column before the last switch stage (they need
Reference: [31] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1425-1439, </volume> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: In order to do this efficiently, a scheme that doesn't require each iteration to be dispatched individually from a central location should be used. One method for grouping iterations, guided self-scheduling (GSS), was given by Polychronopoulos and Kuck <ref> [31] </ref>. In this method, a central counter is used, but each PE allocates itself blocks of iterations rather than single iterations.
Reference: [32] <author> Howard J. Siegel, Wayne G. Nation, Clyde P. Kruskal, and Leonard M. Napolitano. </author> <title> Using the Multistage Cube Network Topology in Parallel Supercomputers. In Interconnection Networks for Large-Scale Parallel Processing by Howard J. Siegel, </title> <publisher> McGraw-Hill Publishing Company, </publisher> <address> New York, </address> <year> 1990. </year> <pages> Pages 313-364. </pages>
Reference-contexts: Inputs to the network are on the left, and outputs on the right. The fast finishing lines are bold. Note that input i and output i both connect to PE i . The hierarchically-connected multiprocessor (HCM) is based on the indirect binary cube network <ref> [29, 32] </ref>. A sixteen PE indirect cube is shown in figure 6. 1 The indirect cube network mentioned has p inputs and p outputs, for a total of p PEs.
Reference: [33] <author> Howard Jay Siegel, Wayne G. Nation, and Mark D. Allemang. </author> <title> The Organization of the PASM Reconfigurable Parallel Processing System. </title> <booktitle> In First Annual OSU Workshop on Parallel Computing, </booktitle> <pages> pages 1-12. </pages> <institution> The Department of Computer and Information Science, Ohio State University, </institution> <year> 1990. </year>
Reference-contexts: Other large-scale SMMs have been designed with processing elements (PEs) connected to one another via a MIN, with each PE containing a portion of the shared memory as its local memory. Representatives of this class include the BBN Butterfly [5], the IBM RP3 [1], and PASM <ref> [33] </ref>.
Reference: [34] <author> Peiyi Tang and Pen-Chung Yew. </author> <title> Software Combining Algorithms for Distributing Hot-Spot Addressing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10, </volume> <year> 1990. </year>
Reference-contexts: On the other hand, it has been commonly argued that SMMs are not scalable due to contention for shared variables and latency of access to global memories. These problems have been addressed through various software techniques designed to split a variable among multiple memory units <ref> [34, 15] </ref> and to keep busy-waiting traffic to a minimum [28, 27, 9], and utilize local memories as much as possible, accessing global memory only when sharing is needed [3, 8]. <p> consumers so that there is little or no contention [15, 17, 25]. 10 through backoff techniques, in which PEs pause between accesses of the counter [14], or the amount of contention can be reduced by splitting the counter among different memory units and combining their values in a tree-like fashion <ref> [4, 13, 34] </ref>. This tree barrier is illustrated in figure 9. In figure 9, each dark circle represents a synchronization point. Time progresses as we move from the bottom to the top of the figure, and the number of levels in the tree is log 2 p.
Reference: [35] <author> Sizheng Wei and Saul Levy. </author> <title> Efficient Hierarchical Interconnection for Multiprocessor Systems. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 708-717, </pages> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Among hierarchical SMMs, some machines (e.g., Cedar [20], the THMM [15, 17, 25], the HMS [24]) have been designed without GALM, whereas in contrast some recent designs place a portion of each level of the hierarchical memory local to each PE <ref> [30, 35] </ref>, the result being that the interconnect itself, rather than the memory per se, is hierarchical. In this paper the advantages of using a hierarchical interconnect|with all memory at the PEs themselves|rather than a hierarchical memory structure are explored. <p> that column of the network; this reduces latency for packets that get to the proper column before the last switch stage (they need pass through fewer stages), and reduces contention in later stages of the interconnect since there are fewer messages traveling through those stages (this was also noted in <ref> [25, 26, 35] </ref>). Padmanabhan calls this use of comparators in the switch elements and connections from them directly to the memory units fast finishing. <p> Pad-manabhan has shown that for a three-level system, multiple networks provide better performance. Fast finishing is likely to be less expensive, however. Due to expected traffic patterns, Wei and Levy suggest providing higher bandwidth low in the hierarchy, and decreased bandwidth higher <ref> [35] </ref>; one possible way of accomplishing this is by using multiple networks for the lower levels, but fast finishing for the higher levels. Regardless of what combination of fast finishing and multiple networks is adopted, the resulting multiprocessor has a single, shared address space.
References-found: 35

