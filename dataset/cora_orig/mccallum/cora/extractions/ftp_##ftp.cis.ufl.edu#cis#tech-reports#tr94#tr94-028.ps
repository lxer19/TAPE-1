URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-028.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: A PARALLEL UNSYMMETRIC-PATTERN MULTIFRONTAL METHOD  
Author: STEVEN M. HADFIELD AND TIMOTHY A. DAVIS 
Keyword: Key words. LU factorization, unsymmetric sparse matrices, multifrontal methods, parallel algorithms  
Note: AMS subject classifications. 65F50, 65F05.  
Abstract: The sparse LU factorization algorithm by Davis and Duff [4] is the first multifrontal method that relaxes the assumption of a symmetric pattern matrix. While the algorithm offers significant performance advantages for unsymmetric pattern matrices, the underlying computational structure changes from a tree (or forest) to a directed acyclic graph. This paper discusses some key issues in the parallel implementation of the unsymmetric-pattern multifrontal method when the pivot sequence is known prior to factorization. The algorithm was implemented on the nCUBE 2 distributed memory multiprocessor and achieved performance is reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Al-Bassam and H. El-Rewini, </author> <title> Processor allocation for hypercubes, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 (1992), </volume> <pages> pp. 394-401. </pages>
Reference-contexts: Management of the subcubes is done using a binary buddy management system [13] where subcubes are split by a fixed ordering of the hypercube's dimensions. Alternative binary buddy management systems are available that can split across several alternative dimensions but they were not used here <ref> [2, 1] </ref>. However, the binary buddy subcube manager is augmented to improve the chances of overlapping data assignments. Specifically, when no subcubes of the requested size are available, all available subcubes of the next larger available size are fragmented.
Reference: [2] <author> M.-S. Chen and K. G. Shin, </author> <title> Processor allocation in an n-cube multiprocessor using gray codes, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 1396-1407. </pages>
Reference-contexts: Management of the subcubes is done using a binary buddy management system [13] where subcubes are split by a fixed ordering of the hypercube's dimensions. Alternative binary buddy management systems are available that can split across several alternative dimensions but they were not used here <ref> [2, 1] </ref>. However, the binary buddy subcube manager is augmented to improve the chances of overlapping data assignments. Specifically, when no subcubes of the requested size are available, all available subcubes of the next larger available size are fragmented.
Reference: [3] <author> T. A. Davis, </author> <title> Users' guide for the unsymmetric-pattern multifrontal package (UMFPACK), </title> <type> Tech. Rep. </type> <institution> TR-93-020, Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Mechanisms. Parallelism is limited to the numerical factorization. Definition of the computational DAG structure and the scheduling, allocation, and assignment are done sequentially in a preprocessing step. The computational DAG structure is determined from UMFPACK, Davis and Duff's unsymmetric pattern multifrontal method <ref> [3] </ref>. This structure is called the assembly DAG, which is then used in scheduling, allocation, and assignment. Once the assembly DAG is defined, we determine the number of processors to allocate to each frontal matrix.
Reference: [4] <author> T. A. Davis and I. S. Duff, </author> <title> An unsymmetric-pattern multifrontal method for sparse lu factorization, </title> <note> SIAM J. Matrix Anal. Appl., (submitted March 1993, under revision.). </note>
Reference-contexts: If the sparse matrix has (or assumes) a symmetric pattern then the resulting computational structure is a tree (or forest). The independence of subtrees allows for efficient scheduling. Prior to the work of Davis and Duff <ref> [4] </ref>, all multifrontal algorithms have assumed a symmetric pattern and several parallel distributed memory implementations have resulted [10, 14, 15]. <p> When the sparse matrix has a significantly unsymmetric pattern, a more general multifrontal method can be employed which takes advantage of the unsymmetric pattern to reduce required computations and expose greater inter-node parallelism. This unsymmetric-pattern multifrontal approach was developed by Davis and Duff <ref> [4] </ref> and has proven to be very competitive with significant potential parallelism [12]. The unsymmetric-pattern multifrontal approach does however result in a computational structure that is a directed acyclic graph (DAG) instead of a tree (or forest). Figure 1 illustrates the LU factorization of a 7-by-7 matrix.
Reference: [5] <author> I. S. Duff, </author> <title> Parallel implementation of multifrontal schemes, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 193-204. </pages>
Reference-contexts: Once the assembly DAG is defined, we determine the number of processors to allocate to each frontal matrix. Experiments have shown that use of both inter- and intra-frontal matrix parallelism is necessary for peak performance <ref> [12, 7, 5] </ref>. As inter-frontal matrix parallelism (between independent frontal matrices) is most efficient, it is preferred when available. Intra-frontal matrix parallelism (multiple processors cooperating on a specific frontal matrix) is most useful for larger frontal matrices.
Reference: [6] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> User's guide for the Harwell-Boeing sparse matrix collection (Release I), </title> <type> Tech. Rep. </type> <institution> TR/PA/92/86, Computer Science and Systems Division, Harwell Laboratory, Oxon, U.K., </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The table reports the matrix name, order, number of nonzeros, speedup on 8 and 64 processors, and memory scalability on 8 and 64 processors All these matrices are highly unsymmetric in pattern. The GEMAT11 is an electrical power problem <ref> [6] </ref>; the others are chemical engineering matrices [16]. Speedup was determined by dividing the method's single processor execution time by its parallel execution time.
Reference: [7] <author> I. S. Duff and L. S. Johnsson, </author> <title> Node orderings and concurrency in structurally-symmetric sparse problems, in Parallel Supercomputing: Methods, Algorithms, and Applications, </title> <editor> G. F. Carey, ed., </editor> <publisher> John Wiley and Sons Ltd., </publisher> <address> New York, NY, </address> <year> 1989, </year> <pages> pp. 177-189. </pages>
Reference-contexts: Once the assembly DAG is defined, we determine the number of processors to allocate to each frontal matrix. Experiments have shown that use of both inter- and intra-frontal matrix parallelism is necessary for peak performance <ref> [12, 7, 5] </ref>. As inter-frontal matrix parallelism (between independent frontal matrices) is most efficient, it is preferred when available. Intra-frontal matrix parallelism (multiple processors cooperating on a specific frontal matrix) is most useful for larger frontal matrices.
Reference: [8] <author> I. S. Duff and J. K. Reid, </author> <title> Some design features of a sparse matrix code, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5 (1979), </volume> <pages> pp. 18-35. </pages>
Reference-contexts: EXTR1 2837 11407 4.5 12.2 0.54 0.15 GEMAT11 4929 33108 5.1 16.8 0.50 0.11 RDIST2 3198 56834 4.4 17.3 0.56 0.19 RDIST3A 2398 61896 5.0 17.8 0.67 0.22 The competitiveness of the new parallel algorithm was evaluated by comparing its single processor execution time to that of the MA28B algorithm <ref> [8] </ref> running on a single nCUBE 2 processor. Due to memory limitations, these results could only be obtained for two of the matrices. These results are shown in Table 2 with the parallel unsymmetric pattern multifrontal code called PRF (run times are in seconds).
Reference: [9] <author> G. A. Geist and M. Heath, </author> <title> Matrix factorization on a hypercube, in Hypercube Multiprocessors 1986, </title> <editor> M. Heath, ed., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1986, </year> <pages> pp. 161-180. </pages>
Reference-contexts: The partial factorization process is done with a column-oriented, pipeline fan-out routine similar to that found in <ref> [9] </ref> but generalized to allow any type of column assignment scheme. Entries in the Schur complement of each frontal matrix are forwarded to subsequent frontal matrices. Message typing conventions allow these forwarded contributions to be selectively read by the receiving frontal matrices.
Reference: [10] <author> A. George, M. Heath, J. W.-H. Liu, and E. G.-Y. Ng, </author> <title> Solution of sparse positive definite systems on a hypercube, </title> <journal> J. Comput. Appl. Math., </journal> <volume> 27 (1989), </volume> <pages> pp. 129-156. </pages>
Reference-contexts: The independence of subtrees allows for efficient scheduling. Prior to the work of Davis and Duff [4], all multifrontal algorithms have assumed a symmetric pattern and several parallel distributed memory implementations have resulted <ref> [10, 14, 15] </ref>. When the sparse matrix has a significantly unsymmetric pattern, a more general multifrontal method can be employed which takes advantage of the unsymmetric pattern to reduce required computations and expose greater inter-node parallelism. <p> These results compare favorably with similar results using symmetric pattern multifrontal methods on matrices of like order <ref> [10, 14, 15] </ref>. The memory requirements scale extremely well with additional processors indicating that increasingly larger problems can be solved by using additional processors.
Reference: [11] <author> S. Hadfield, </author> <title> On the LU Factorization of Sequences of Identically Structured Sparse Matrices within a Distributed Memory Environment, </title> <type> PhD thesis, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: As inter-frontal matrix parallelism (between independent frontal matrices) is most efficient, it is preferred when available. Intra-frontal matrix parallelism (multiple processors cooperating on a specific frontal matrix) is most useful for larger frontal matrices. The performance characteristics of intra-frontal parallelism can be accurately modeled via an analytical formula <ref> [11] </ref> which significantly aids both the allocation and scheduling processes. Allocation of processor sets to specific frontal matrix tasks is done via blocks of tasks.
Reference: [12] <author> S. Hadfield and T. Davis, </author> <title> Potential and achievable parallelism in the unsymmetric-pattern multifrontal LU factorization method for sparse matrices, </title> <booktitle> in Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <year> 1994. </year>
Reference-contexts: This unsymmetric-pattern multifrontal approach was developed by Davis and Duff [4] and has proven to be very competitive with significant potential parallelism <ref> [12] </ref>. The unsymmetric-pattern multifrontal approach does however result in a computational structure that is a directed acyclic graph (DAG) instead of a tree (or forest). Figure 1 illustrates the LU factorization of a 7-by-7 matrix. The example matrix is factorized with four frontal matrices. <p> Once the assembly DAG is defined, we determine the number of processors to allocate to each frontal matrix. Experiments have shown that use of both inter- and intra-frontal matrix parallelism is necessary for peak performance <ref> [12, 7, 5] </ref>. As inter-frontal matrix parallelism (between independent frontal matrices) is most efficient, it is preferred when available. Intra-frontal matrix parallelism (multiple processors cooperating on a specific frontal matrix) is most useful for larger frontal matrices.
Reference: [13] <author> K. C. Knowlton, </author> <title> A fast storage allocator, </title> <journal> Communications of the ACM, </journal> <volume> 8 (1965), </volume> <pages> pp. 623-625. </pages>
Reference-contexts: We refer to this as overlapping. Management of the subcubes is done using a binary buddy management system <ref> [13] </ref> where subcubes are split by a fixed ordering of the hypercube's dimensions. Alternative binary buddy management systems are available that can split across several alternative dimensions but they were not used here [2, 1].
Reference: [14] <author> R. Lucas, T. Blank, and J. Tiemann, </author> <title> A parallel solution method for large sparse systems of equations, </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> CAD-6 </month> <year> (1987), </year> <pages> pp. 981-991. </pages> <editor> 6 S. M. HADFIELD AND T. A. </editor> <address> DAVIS </address>
Reference-contexts: The independence of subtrees allows for efficient scheduling. Prior to the work of Davis and Duff [4], all multifrontal algorithms have assumed a symmetric pattern and several parallel distributed memory implementations have resulted <ref> [10, 14, 15] </ref>. When the sparse matrix has a significantly unsymmetric pattern, a more general multifrontal method can be employed which takes advantage of the unsymmetric pattern to reduce required computations and expose greater inter-node parallelism. <p> These results compare favorably with similar results using symmetric pattern multifrontal methods on matrices of like order <ref> [10, 14, 15] </ref>. The memory requirements scale extremely well with additional processors indicating that increasingly larger problems can be solved by using additional processors.
Reference: [15] <author> A. Pothen and C. Sun, </author> <title> A mapping algorithm for parallel sparse cholesky factorization, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1253-1257. </pages>
Reference-contexts: The independence of subtrees allows for efficient scheduling. Prior to the work of Davis and Duff [4], all multifrontal algorithms have assumed a symmetric pattern and several parallel distributed memory implementations have resulted <ref> [10, 14, 15] </ref>. When the sparse matrix has a significantly unsymmetric pattern, a more general multifrontal method can be employed which takes advantage of the unsymmetric pattern to reduce required computations and expose greater inter-node parallelism. <p> These results compare favorably with similar results using symmetric pattern multifrontal methods on matrices of like order <ref> [10, 14, 15] </ref>. The memory requirements scale extremely well with additional processors indicating that increasingly larger problems can be solved by using additional processors.
Reference: [16] <author> S. E. Zitney and M. A. Stadtherr, </author> <title> Supercomputing strategies for the design and analysis of complex separation systems, </title> <institution> Ind. Eng. Chem. Res., </institution> <month> 32 </month> <year> (1993), </year> <pages> pp. 604-612. </pages>
Reference-contexts: The table reports the matrix name, order, number of nonzeros, speedup on 8 and 64 processors, and memory scalability on 8 and 64 processors All these matrices are highly unsymmetric in pattern. The GEMAT11 is an electrical power problem [6]; the others are chemical engineering matrices <ref> [16] </ref>. Speedup was determined by dividing the method's single processor execution time by its parallel execution time.
References-found: 16

