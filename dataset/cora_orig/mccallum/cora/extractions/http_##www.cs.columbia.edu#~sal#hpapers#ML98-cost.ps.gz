URL: http://www.cs.columbia.edu/~sal/hpapers/ML98-cost.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  
Phone: Phone  
Title: The Effects of Training Class Distributions on Performance Using Cost Models Authors with addresses:  
Author: Philip K. Chan Salvatore J. Stolfo 
Keyword: training class distribution, cost model, large amounts of data, integrating multiple learned models, credit card fraud detection  
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Note: Title:  Email address of contact author:  number of contact author: 407-674-7280 Multiple submission statement (if applicable): N/A  
Abstract: 250 word maximum): Many factors influence a learning process and the performance of a learned classifier. In this paper we investigate the effects of class distribution in the training set on performance. We also study different methods of measuring performance based on cost models and the performance effects of training class distribution with respect to the different cost models. Observations from these effects help us devise a multi-classifier meta-learning approach to learn in domains with skewed class distributions, non-uniform cost per error, and large amounts of data. One such domain is credit card fraud detection and our empirical results indicate that our approach can significantly reduce loss due to illegitimate transactions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: We tested this hypothesis. Experiments were performed on four learning algorithms (C4.5, CART, RIPPER, and BAYES) and three bimodal (two-class) data sets (Credit Card Fraud, Adult Census, and Protein Coding Regions). We obtained C4.5 [19] and CART <ref> [1] </ref> as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [9] is a rule learning algorithm and was obtained from W. Cohen.
Reference: [2] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year> <month> 15 </month>
Reference-contexts: We tested this hypothesis. Experiments were performed on four learning algorithms (C4.5, CART, RIPPER, and BAYES) and three bimodal (two-class) data sets (Credit Card Fraud, Adult Census, and Protein Coding Regions). We obtained C4.5 [19] and CART [1] as part of the IND package <ref> [2] </ref> from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [9] is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in [8].
Reference: [3] <author> C. Cardie and N. Howe. </author> <title> Improving minority class prediction using case-specific feature weights. </title> <booktitle> In Proc. 14th Intl. Conf. Mach. Learning, </booktitle> <pages> pages 57-65, </pages> <year> 1997. </year>
Reference-contexts: Kubat and Matwin [14] acknowledged the performance degradation effects of skewed class distribution and investigated techniques for removing unnecessary instances from the majority class. Instances that are in the borderline region, noisy, or redundant are candidates for removal. Cardie and Howie <ref> [3] </ref> stated that skewed class 13 distributions are "the norm for learning problems in natural language process-ing (NLP)." In a case-based learning framework, they studied techniques to extract relevant features from previously built decision trees and customize local feature weights for each case retrieval.
Reference: [4] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In Proc. Eighth Intl. Work. Machine Learning, </booktitle> <pages> pages 596-599, </pages> <year> 1991. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., <ref> [4] </ref>) and parallelism (e.g., [13, 17]) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: For massive amounts of data, substantial improvement in speed can be achieved for non-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classification behavior. Several meta-learning strategies are described in <ref> [5] </ref>. To simplify our discussion, we only describe the class-combiner (or stacking 11 [21]) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label.
Reference: [6] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: This training set is then used to train a meta-classifier. For integrating subsets, class-combiner can be more effective than the voting-based techniques <ref> [6] </ref>. 5.1 Experiments and Results To evaluate our multi-classifier meta-learning approach to skewed class distributions, we performed a set of experiments using the credit card fraud data.
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 2-7, </pages> <year> 1996. </year>
Reference-contexts: In addition, although banks do not share credit card data for fear of losing valuable customers to competitors or violating the customers' privacy, a bank can import "black-box" classifiers from other banks to improve its local performance <ref> [7] </ref>. Acknowledgments The authors benefited from discussions with Dave Fan, Andreas Prodromidis, and Wenke Lee. Part of this work was performed while the first author was visiting the Computer Science Dept. at Brigham Young University last summer.
Reference: [8] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: RIPPER [9] is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in <ref> [8] </ref>. The credit card fraud data set was obtained from the Chase Manhattan Bank and contains 500,000 transactions from 10/95 to 9/96, about 20% of which are fraudulent (the real distribution is much more skewed (fortunately)|the 20:80 distribution is what we were given).
Reference: [9] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: We obtained C4.5 [19] and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER <ref> [9] </ref> is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in [8].
Reference: [10] <author> M. Craven and J. Shavlik. </author> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proc. IJCAI-93, </booktitle> <pages> pages 1319-1324, </pages> <year> 1993. </year>
Reference-contexts: The adult census data set (courtesy of R. Kohavi and B. Becker) contains records from the 1994 Census. We use a subset which has 45,000 records, 25% of which has more than $50K in income. The protein coding region data set (courtesy of Craven and Shavlik <ref> [10] </ref>) contains 20,000 records of DNA nucleotide sequences and their binary classifications (coding (50%) or non-coding (50%)). In this set of experiments the training class distribution varied from 10% to 90% and 10-fold CV was used.
Reference: [11] <author> T. Fawcett. </author> <title> Learning with skewed class distributions-summary of responses. </title> <journal> Machine Learning List, </journal> <volume> Vol. 8, No. 20, </volume> <month> Dec </month> <year> 1996. </year>
Reference-contexts: Our approach attempts to address all three issues. In Dec 96 Fawcett <ref> [11] </ref> summarized the responses to his inquiry on learning with skewed class distributions. The number of responses was amazingly few given skewed distributions are not rare in the real world.
Reference: [12] <author> T. Fawcett and F. Provost. </author> <title> Combining data mining and machine learning for effective user profiling. </title> <booktitle> In Proc. 2nd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 8-13, </pages> <year> 1996. </year>
Reference-contexts: Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain [20] and some medical tests are more expensive in the medical diagnosis domain. Fawcett and Provost <ref> [12] </ref> considered non-uniform cost per error in their cellular phone fraud detection task. However, they only considered non-uniform cost in evaluating different algorithms|the learning process was not cost-sensitive. Without modifying the learning algorithms, our approach handles non-uniform cost per error and is cost-sensitive during the learning process.
Reference: [13] <author> E. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc ACM-SIGMOD-97, </booktitle> <year> 1997. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., [4]) and parallelism (e.g., <ref> [13, 17] </ref>) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [14] <author> M. Kubat and S. Matwin. </author> <title> Addressing the curse of imbalanaced training sets: One sided selection. </title> <booktitle> In Proc. 14th Intl. Conf. Machine Learning, </booktitle> <pages> pages 179-186, </pages> <year> 1997. </year>
Reference-contexts: Our approach attempts to address all three issues. In Dec 96 Fawcett [11] summarized the responses to his inquiry on learning with skewed class distributions. The number of responses was amazingly few given skewed distributions are not rare in the real world. Kubat and Matwin <ref> [14] </ref> acknowledged the performance degradation effects of skewed class distribution and investigated techniques for removing unnecessary instances from the majority class. Instances that are in the borderline region, noisy, or redundant are candidates for removal.
Reference: [15] <author> D. Margineantu and T. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Proc. 14th Intl. Conf. Machine Learning, </booktitle> <pages> pages 211-218, </pages> <year> 1997. </year>
Reference-contexts: Also, more classifiers are generated when the data set since larger or additional learning algorithms are incorporated. Metrics for analyzing an ensemble of classifiers (e.g., diversity, correlated error, and coverage) can be used in pruning unnecessary classifiers <ref> [15] </ref>. More importantly, since thieves also learn and fraud patterns evolve over time, some classifiers are more relevant than others at a particular time. Therefore, an adaptive classifier selection method is essential.
Reference: [16] <author> M. Pazzani, C. Merz, T. Hume P. Murphy, K. Ali, and C. Brunk. </author> <title> Reducing misclassification costs. </title> <booktitle> In Proc. 11th Intl. Conf. Machine Learning, </booktitle> <pages> pages 217-225, </pages> <year> 1994. </year>
Reference-contexts: Error rate (or accuracy) is commonly used in evaluating learning algorithms; cost-sensitive learning has not been well investigated. Assuming the errors can be grouped into a few types and each type incurs the same cost, some studies (for example, <ref> [16] </ref>) proposed algorithms that aim to reduce the total cost. Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain [20] and some medical tests are more expensive in the medical diagnosis domain.
Reference: [17] <author> F. Provost and J. Aronis. </author> <title> Scaling up inductive learning with massive parallelism. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 33-46, </pages> <year> 1996. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., [4]) and parallelism (e.g., <ref> [13, 17] </ref>) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [18] <author> F. Provost and T. Fawcett. </author> <title> Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proc. 3rd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 43-48, </pages> <year> 1997. </year>
Reference-contexts: This function was also defined by Provost and Fawcett <ref> [18] </ref>. We further define the cost ratio as: CostRatio = Cost (F N ) Cost (F P ) (2) Three cases can be derived from Equation 1: 1.
Reference: [19] <author> J. R. Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: We tested this hypothesis. Experiments were performed on four learning algorithms (C4.5, CART, RIPPER, and BAYES) and three bimodal (two-class) data sets (Credit Card Fraud, Adult Census, and Protein Coding Regions). We obtained C4.5 <ref> [19] </ref> and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [9] is a rule learning algorithm and was obtained from W. Cohen.
Reference: [20] <author> M. Tan and J. Schlimmer. </author> <title> Cost-sensitive concept learning of sensor use in approach and recognition. </title> <booktitle> In Proc. 6th Intl. Work. Machine Learning, </booktitle> <pages> pages 392-395, </pages> <year> 1989. </year>
Reference-contexts: Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain <ref> [20] </ref> and some medical tests are more expensive in the medical diagnosis domain. Fawcett and Provost [12] considered non-uniform cost per error in their cellular phone fraud detection task. However, they only considered non-uniform cost in evaluating different algorithms|the learning process was not cost-sensitive.
Reference: [21] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year> <month> 16 </month>
Reference-contexts: For massive amounts of data, substantial improvement in speed can be achieved for non-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classification behavior. Several meta-learning strategies are described in [5]. To simplify our discussion, we only describe the class-combiner (or stacking 11 <ref> [21] </ref>) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label. This training set is then used to train a meta-classifier.
References-found: 21

