URL: http://www.cs.rice.edu/MSCP/papers/sigplan94.ps.gz
Refering-URL: http://www.cs.rice.edu/MSCP/publications.html
Root-URL: 
Title: Effective Partial Redundancy Elimination  
Author: Preston Briggs Keith D. Cooper 
Address: Houston, Texas 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies. As part of the work, we introduce a new algorithm for global reassociation of expressions. It uses global information to reorder expressions, creating opportunities for other optimizations. The new algorithm generalizes earlier work that ordered FORTRAN array address expressions to improve optimization [25]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers, Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23]. The idea of exploiting associativity and distributivity to rearrange expressions is well known <ref> [17, 1] </ref>; however, early work concentrated on simplifying individual expressions. We know of two prior approaches to reasso-ciation with the goal of exposing loop-invariant expressions, both discovered within IBM and published the same year.
Reference: [2] <author> Bowen Alpern, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Conference Record of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: The net result is that decisions made in the design of the front end dictate the effectiveness of partial redundancy elimination. This paper shows how an optimizer can use global reassociation (see Section 3.1) and a form of partition-based global value numbering <ref> [2] </ref> to improve the effectiveness of partial redundancy elimination. We consider these to be enabling transformations. They do not im This work has been supported by ARPA through ONR grant N00014-91-J-1989. <p> It is important to re-sort sums after distribution. 3.2 Global Renaming To address the naming problems, we use a global renaming scheme based on Alpern, Wegman, and Zadeck's algorithm for determining when two variables have the same value <ref> [2] </ref>. We refer to their technique as "partition-based global value numbering". Instead of building up complex equality relationships from simpler ones, as in traditional value numbering, their technique works from the "optimistic" assumption that all variables are equivalent and uses the individual statements in the code to disprove equivalences. <p> Our implementation of global value numbering uses the simplest variation described by Alpern, Wegman, and Zadeck, possibly missing some opportunities discovered by their more powerful approaches <ref> [2, Sections 3 and 4] </ref>. 4.1 Results We ran several versions of the optimizer on a suite of test routines. Each version adds new passes to the previous one. <p> Alpern, Wegman, and Zadeck suggest the following scheme: If a value x is computed at two points, p and q, and p dominates q, then the computation at q is redundant and may be deleted <ref> [2, page 2] </ref>. 2. The classic approach to global common subexpres-sion elimination is to calculate the set of expressions available at each point in a routine. If x is available on every path reaching p, then any computation of x at p is redundant and may be deleted. 3.
Reference: [3] <author> Marc A. Auslander and Martin E. Hopkins. </author> <title> An overview of the PL.8 compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 17(6) </volume> <pages> 22-31, </pages> <month> June </month> <year> 1982. </year> <booktitle> Proceedings of the ACM SIG-PLAN '82 Symposium on Compiler Construction. </booktitle>
Reference-contexts: This naming discipline can be implemented in the compiler's front end by maintaining a hash table of expressions and creating a new name whenever a new expression is discovered <ref> [3] </ref>. Unfortunately, relying on the front end limits the applicability of PRE. It is difficult to maintain the naming rules across other optimizations; thus, PRE must be run first and only once. Furthermore, the ability of PRE to recognize identities is limited by the programmer's choice of variable names.
Reference: [4] <author> Robert L. Bernstein. </author> <title> Multiplication by integer constants. </title> <journal> Software Practice and Experience, </journal> <volume> 16(7) </volume> <pages> 641-652, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: For example, many compilers replace an integer multiply with one constant argument by a series of shifts, adds, and subtracts <ref> [4] </ref>. Since shifts are not associative, this optimization should not be performed until after global reassociation. For example, if ((x fi y) fi 2) fi z is prematurely converted into ((xfiy) t 1)fiz, we lose the opportunity to group z with either x or y.
Reference: [5] <author> Steve Carr and Ken Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <editor> In Jack Dongarra, Paul Messina, Danny C. Sorensen, and Robert G. Voight, editors, </editor> <booktitle> Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 400-405, </pages> <year> 1990. </year>
Reference-contexts: We expect that strength reduction will improve the code beyond the results shown in this paper. Reassoci-ation should let strength reduction introduce fewer distinct induction variables, particularly in code with complex subscripts like that produced by cache and register blocking <ref> [5, 27] </ref>. Of course, some particularly sophisticated approaches to strength reduction include a form of reassociation [20]; we believe that a separate pass of reassociation will significantly simplify the implementation of strength reduction.
Reference: [6] <author> Gregory J. Chaitin, Marc A. Auslander, Ashok K. Chandra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The invariant expressions r 6 and r 7 have been hoisted from the loop and the redundant computations of r 3 ; r 6 , and r 7 have been removed. Finally, the coalescing phase of a Chaitin-style global register allocator will remove unnecessary copy instructions <ref> [6] </ref>. In this example, coalescing is able to remove all the copies (as shown in Figure 10), though this will not always be possible. Taken together, the sequence of transformations reduced the length of the loop by 1 operation without increasing the length of any path through the routine.
Reference: [7] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Conference Record of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <address> Orlando, Florida, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: In practice, we compute ranks on the SSA form of the routine during a reverse-postorder traversal of the control-flow graph; therefore, our first step is to build the pruned SSA form of the routine <ref> [11, 7] </ref>. During the renaming step [11, Figure 12], we remove all copies, effectively folding them into -nodes. This approach simplifies the intermediate code by removing our dependence on the programmer's choice of variable names (recall Section 2.2).
Reference: [8] <author> Fred C. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Partial redundancy elimination is a powerful optimization that has been discussed in the literature for many years (e.g., <ref> [21, 8, 14, 12, 18] </ref>). Unfortunately, partial redundancy elimination has two serious limitations. It can only recognize lexically-identical expressions; this makes effectiveness a function of the choice of names in the front end. <p> We note that Chow also mentions using forward propagation <ref> [8] </ref>; we conjecture that it helped him avoid the same difficulty with PRE.
Reference: [9] <author> John Cocke and Peter W. Markstein. </author> <title> Measurement of program improvement algorithms. </title> <booktitle> In Proceedings of Information Processing 80. </booktitle> <publisher> North Holland Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: In broad terms, it uses commutativity, associativity, and distributivity to expose common subexpressions and loop-invariant expressions. The effects can be substantial; Cocke and Markstein note that as much as 50% of the code in some inner loops can be eliminated as a result of reassocia-tion <ref> [9, page 225] </ref>. Our approach has three steps: 1. Compute a rank for every expression. 2. Propagate expressions forward to their uses. 3. Reassociate expressions, sorting their operands by ranks. <p> Scarborough and Kolsky describe a front-end discipline for generating an array address expression as a sum of products and associating the sum to expose the loop-invariant parts [25]. Cocke and Markstein also mention the idea of reassociation, this time within the optimizer instead of the front end <ref> [9] </ref>. In a chapter for an upcoming book, Markstein et al. describe a sophisticated algorithm for strength reduction that includes a form of reassociation [20]. Their algorithm attacks the problem on a loop-by-loop basis, working from inner to outer loops.
Reference: [10] <author> John Cocke and Jacob T. Schwartz. </author> <title> Programming languages and their compilers: Preliminary notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: PRE cannot discover this fact even though value numbering can eliminate this redundancy <ref> [10] </ref>. Of course, this is a simple example, but its very simplicity should suggest the large number of opportunities missed by PRE when considering an entire routine. 3 Effective PRE To address the limitations of PRE, we propose a set of techniques that reorder and rename expressions.
Reference: [11] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In practice, we compute ranks on the SSA form of the routine during a reverse-postorder traversal of the control-flow graph; therefore, our first step is to build the pruned SSA form of the routine <ref> [11, 7] </ref>. During the renaming step [11, Figure 12], we remove all copies, effectively folding them into -nodes. This approach simplifies the intermediate code by removing our dependence on the programmer's choice of variable names (recall Section 2.2). <p> In practice, we compute ranks on the SSA form of the routine during a reverse-postorder traversal of the control-flow graph; therefore, our first step is to build the pruned SSA form of the routine [11, 7]. During the renaming step <ref> [11, Figure 12] </ref>, we remove all copies, effectively folding them into -nodes. This approach simplifies the intermediate code by removing our dependence on the programmer's choice of variable names (recall Section 2.2). Given the SSA form, we traverse the control-flow graph in reverse postorder, assigning ranks. <p> The results are given in Table 1. We report results for four different levels of optimization: baseline This column provides the dynamic operation count, including branches, for each routine when optimized using a sequence of global constant propagation [26], global peephole optimization, global dead code elimination <ref> [11, Section 7.1] </ref>, coalescing, and a final pass to eliminate empty basic blocks. 1 partial The left column gives the operation counts for routines optimized with PRE, followed by the sequence of optimizations used to establish the baseline.
Reference: [12] <author> Dhananjay M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Ren-voise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Partial redundancy elimination is a powerful optimization that has been discussed in the literature for many years (e.g., <ref> [21, 8, 14, 12, 18] </ref>). Unfortunately, partial redundancy elimination has two serious limitations. It can only recognize lexically-identical expressions; this makes effectiveness a function of the choice of names in the front end. <p> The second method, based on available expressions, will handle this case; it removes all redundancies. PRE is stronger yet it removes all redundancies and many partial redundancies as well. 6 Related Work While there have been many papers discussing partial redundancy elimination (e.g., <ref> [21, 14, 12, 18] </ref>), none mention the deficiencies discussed in Sections 2.2 and 2.3. Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23].
Reference: [13] <author> Dhananjay M. Dhamdhere and Uday P. Khedker. </author> <title> Complexity of bidirectional data flow analysis. </title> <booktitle> In Conference Record of the Twentieth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Our implementation of PRE uses a variation described by Drechsler and Stadel [14]. Their formulation supports edge placement for enhanced optimization and simplifies the data-flow equations that must be solved, avoiding the bidirectional equations typical of some other approaches <ref> [13] </ref>. Our implementation of global value numbering uses the simplest variation described by Alpern, Wegman, and Zadeck, possibly missing some opportunities discovered by their more powerful approaches [2, Sections 3 and 4]. 4.1 Results We ran several versions of the optimizer on a suite of test routines.
Reference: [14] <author> Karl-Heinz Drechsler and Manfred P. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "Global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Partial redundancy elimination is a powerful optimization that has been discussed in the literature for many years (e.g., <ref> [21, 8, 14, 12, 18] </ref>). Unfortunately, partial redundancy elimination has two serious limitations. It can only recognize lexically-identical expressions; this makes effectiveness a function of the choice of names in the front end. <p> Each pass performs a single optimization, including all the required control-flow and data-flow analyses. While this approach is not suitable for production compilers, its flexibility makes it ideal for experimentation. Our implementation of PRE uses a variation described by Drechsler and Stadel <ref> [14] </ref>. Their formulation supports edge placement for enhanced optimization and simplifies the data-flow equations that must be solved, avoiding the bidirectional equations typical of some other approaches [13]. <p> The second method, based on available expressions, will handle this case; it removes all redundancies. PRE is stronger yet it removes all redundancies and many partial redundancies as well. 6 Related Work While there have been many papers discussing partial redundancy elimination (e.g., <ref> [21, 14, 12, 18] </ref>), none mention the deficiencies discussed in Sections 2.2 and 2.3. Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23].
Reference: [15] <author> Lawrence Feigen, David Klappholz, Robert Casazza, and Xing Xue. </author> <title> The revival transformation. </title> <booktitle> In Conference Record of POPL '94: 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 421-434, </pages> <address> Portland, Oregon, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: New blocks were required to hold the copies. Figure 6 shows the effect of forward propagation. It is interesting to note that forward propagation eliminates partially-dead expressions <ref> [15, 19] </ref>. An expression is live at its definition point if its result is used on some path to an exit. Alternatively, an expression is dead if its result will never be used on any path. <p> We also prefer our global approach to loop-by-loop alternatives since it can make improvements in loop-free code and may admit simpler implementation. Recent work by Feigen et al. and by Knoop et al. describe alternative approaches to the problem of eliminating partially-dead expressions <ref> [15, 19] </ref>. While an adequate comparison of the alternatives would require trial implementations and empirical measurements, it is clear that they solve a similar class of problems in radically different ways.
Reference: [16] <author> George E. Forsythe, Michael A. Malcolm, and Cleve B. Moler. </author> <title> Computer Methods for Mathematical Computations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1977. </year>
Reference-contexts: Each version adds new passes to the previous one. Our test suite consists of 50 routines, drawn from the Spec benchmark suite and from Forsythe, Mal-colm, and Moler's book on numerical methods <ref> [16] </ref>. The results are given in Table 1.
Reference: [17] <author> Dennis J. Frailey. </author> <title> Expression optimization using unary complement operators. </title> <journal> SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 67-85, </pages> <month> July </month> <year> 1970. </year> <booktitle> Proceedings of a Symposium on Compiler Optimization. </booktitle>
Reference-contexts: First, though, we rewrite certain operations to expose more opportunities for reassociation. As suggested by Frailey <ref> [17] </ref>, we rewrite expressions of the form x y + z as x + (y) + z, since addition is associative and subtraction is not. We also perform similar transformations for Boolean operations. On the other hand, we avoid rewriting x=y as x fi 1=y to avoid introducing precision problems. <p> Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23]. The idea of exploiting associativity and distributivity to rearrange expressions is well known <ref> [17, 1] </ref>; however, early work concentrated on simplifying individual expressions. We know of two prior approaches to reasso-ciation with the goal of exposing loop-invariant expressions, both discovered within IBM and published the same year.
Reference: [18] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Lazy code motion. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(7) </volume> <pages> 224-234, </pages> <month> July </month> <year> 1992. </year> <booktitle> Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: 1 Introduction Partial redundancy elimination is a powerful optimization that has been discussed in the literature for many years (e.g., <ref> [21, 8, 14, 12, 18] </ref>). Unfortunately, partial redundancy elimination has two serious limitations. It can only recognize lexically-identical expressions; this makes effectiveness a function of the choice of names in the front end. <p> The second method, based on available expressions, will handle this case; it removes all redundancies. PRE is stronger yet it removes all redundancies and many partial redundancies as well. 6 Related Work While there have been many papers discussing partial redundancy elimination (e.g., <ref> [21, 14, 12, 18] </ref>), none mention the deficiencies discussed in Sections 2.2 and 2.3. Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23].
Reference: [19] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Partial dead code elimination. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(6), </volume> <month> June </month> <year> 1994. </year> <booktitle> Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: New blocks were required to hold the copies. Figure 6 shows the effect of forward propagation. It is interesting to note that forward propagation eliminates partially-dead expressions <ref> [15, 19] </ref>. An expression is live at its definition point if its result is used on some path to an exit. Alternatively, an expression is dead if its result will never be used on any path. <p> We also prefer our global approach to loop-by-loop alternatives since it can make improvements in loop-free code and may admit simpler implementation. Recent work by Feigen et al. and by Knoop et al. describe alternative approaches to the problem of eliminating partially-dead expressions <ref> [15, 19] </ref>. While an adequate comparison of the alternatives would require trial implementations and empirical measurements, it is clear that they solve a similar class of problems in radically different ways.
Reference: [20] <author> Peter W. Markstein, Victoria Markstein, and F. Ken-neth Zadeck. </author> <title> Reassociation and strength reduction. In Optimization in Compilers. </title> <note> ACM Press, to appear. </note>
Reference-contexts: Reassoci-ation should let strength reduction introduce fewer distinct induction variables, particularly in code with complex subscripts like that produced by cache and register blocking [5, 27]. Of course, some particularly sophisticated approaches to strength reduction include a form of reassociation <ref> [20] </ref>; we believe that a separate pass of reassociation will significantly simplify the implementation of strength reduction. <p> Cocke and Markstein also mention the idea of reassociation, this time within the optimizer instead of the front end [9]. In a chapter for an upcoming book, Markstein et al. describe a sophisticated algorithm for strength reduction that includes a form of reassociation <ref> [20] </ref>. Their algorithm attacks the problem on a loop-by-loop basis, working from inner to outer loops. In each loop, they perform some forward propagation and sort subexpres-sions into loop-variant and loop-invariant parts, hoisting the invariant parts. We presume their approach is a development of earlier work within IBM.
Reference: [21] <author> Etienne Morel and Claude Renvoise. </author> <title> Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: 1 Introduction Partial redundancy elimination is a powerful optimization that has been discussed in the literature for many years (e.g., <ref> [21, 8, 14, 12, 18] </ref>). Unfortunately, partial redundancy elimination has two serious limitations. It can only recognize lexically-identical expressions; this makes effectiveness a function of the choice of names in the front end. <p> Additionally, we present experimental evidence that demonstrates the effectiveness of partial redundancy elimination, with and without our transformations. 2 Partial Redundancy Elimination Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise <ref> [21] </ref>. It combines and extends two other techniques. common subexpression elimination An expression is redundant at some point p if and only if it is computed along every path leading to p and none of its constituent subexpressions has been redefined. <p> The large number of possible orderings makes an exhaustive search for optimal solutions impractical. 2.2 Naming Another important issue is the selection of names. Our implementation of PRE distinguishes between variable names and expression names. This distinction was introduced by Morel and Renvoise <ref> [21, page 97] </ref>. A variable name is the target of a copy instruction; conceptually, these correspond to source-level assignments. An expression name is the target of a computation in practice, an instruction other than a branch or copy. This gives every expression (and subexpression) a name. <p> The second method, based on available expressions, will handle this case; it removes all redundancies. PRE is stronger yet it removes all redundancies and many partial redundancies as well. 6 Related Work While there have been many papers discussing partial redundancy elimination (e.g., <ref> [21, 14, 12, 18] </ref>), none mention the deficiencies discussed in Sections 2.2 and 2.3. Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions [23].
Reference: [22] <author> Kevin O'Brien, Bill Hay, Joanne Minish, Hartmann Schaffer, Bob Schloss, Arvin Shepherd, and Matthew Zaleski. </author> <title> Advanced compiler technology for the RISC System/6000 architecture. </title> <institution> In IBM RISC System/6000 Technology. IBM Corporation, </institution> <address> Armonk, New York, </address> <year> 1990. </year>
Reference-contexts: We presume their approach is a development of earlier work within IBM. Other work by O'Brien et al. and Santhanam briefly describe what are apparently further developments of the Cocke and Markstein approach <ref> [22, 24] </ref>. It is difficult to compare our approach directly to these earlier methods. We were motivated by a desire to separate concerns. We already had solutions to hoisting loop invariants and strength reduction; therefore, we looked for a way to reassociate expressions.
Reference: [23] <author> Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In Conference Record of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Rosen et al. recognize the naming problem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions <ref> [23] </ref>. The idea of exploiting associativity and distributivity to rearrange expressions is well known [17, 1]; however, early work concentrated on simplifying individual expressions. We know of two prior approaches to reasso-ciation with the goal of exposing loop-invariant expressions, both discovered within IBM and published the same year.
Reference: [24] <author> Vatsa Santhanam. </author> <title> Register reassociation in PA-RISC compilers. </title> <journal> Hewlett-Packard Journal, </journal> <pages> pages 33-38, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: We presume their approach is a development of earlier work within IBM. Other work by O'Brien et al. and Santhanam briefly describe what are apparently further developments of the Cocke and Markstein approach <ref> [22, 24] </ref>. It is difficult to compare our approach directly to these earlier methods. We were motivated by a desire to separate concerns. We already had solutions to hoisting loop invariants and strength reduction; therefore, we looked for a way to reassociate expressions.
Reference: [25] <author> Randolph G. Scarborough and Harwood G. Kolsky. </author> <title> Improved optimization of FORTRAN object programs. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 660-676, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: Scarborough and Kolsky describe a front-end discipline for generating an array address expression as a sum of products and associating the sum to expose the loop-invariant parts <ref> [25] </ref>. Cocke and Markstein also mention the idea of reassociation, this time within the optimizer instead of the front end [9]. In a chapter for an upcoming book, Markstein et al. describe a sophisticated algorithm for strength reduction that includes a form of reassociation [20].
Reference: [26] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The results are given in Table 1. We report results for four different levels of optimization: baseline This column provides the dynamic operation count, including branches, for each routine when optimized using a sequence of global constant propagation <ref> [26] </ref>, global peephole optimization, global dead code elimination [11, Section 7.1], coalescing, and a final pass to eliminate empty basic blocks. 1 partial The left column gives the operation counts for routines optimized with PRE, followed by the sequence of optimizations used to establish the baseline.
Reference: [27] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(6) </volume> <pages> 30-44, </pages> <month> June </month> <year> 1991. </year> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: We expect that strength reduction will improve the code beyond the results shown in this paper. Reassoci-ation should let strength reduction introduce fewer distinct induction variables, particularly in code with complex subscripts like that produced by cache and register blocking <ref> [5, 27] </ref>. Of course, some particularly sophisticated approaches to strength reduction include a form of reassociation [20]; we believe that a separate pass of reassociation will significantly simplify the implementation of strength reduction.
References-found: 27

