URL: http://www.icsi.berkeley.edu/~luby/PAPERS/bayesian.ps
Refering-URL: http://www.icsi.berkeley.edu/~luby/neural.html
Root-URL: http://www.icsi.berkeley.edu
Title: An Optimal Approximation Algorithm for Bayesian Inference some integer d that is a linear function
Author: Paul Dagum Michael Luby 
Address: Berkeley, CA 94704  
Affiliation: on Medical Informatics, Stanford University School of Medicine, Stanford, California 94305-5479. International Computer Science Institute,  
Note: for  
Pubnum: Section  
Abstract: Approximating the inference probability Pr[X = xjE = e] in any sense, even for a single evidence node E, is NP-hard. This result holds for belief networks that are allowed to contain extreme conditional probabilities|that is, conditional probabilities arbitrarily close to 0. Nevertheless, all previous approximation algorithms have failed to approximate efficiently many inferences, even for belief networks without extreme conditional probabilities. We prove that we can approximate efficiently probabilistic inference in belief networks without extreme conditional probabilities. We construct a randomized approximation algorithm|the bounded-variance algorithm|that is a variant of the known likelihood-weighting algorithm. The bounded-variance algorithm is the first algorithm with provably fast inference approximation on all belief networks without extreme conditional probabilities. From the bounded-variance algorithm, we construct a deterministic approximation algorithm using current advances in the theory of pseudorandom generators. In contrast to the exponential worst-case behavior of all previous deterministic approximations, the deterministic bounded-variance algorithm approximates inference probabilities in worst-case time that is subexponential 2 (log n) d 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Special issue on probability forecasting. </institution> <note> International Journal of Forecasting, 11, </note> <year> 1995. </year>
Reference-contexts: 1 Approximation Algorithms Belief networks are powerful graphical representations of probabilistic dependencies among domain variables. Belief networks have been used successfully in many real-world problems in diagnosis, prediction, and forecasting (for example, papers included in <ref> [1, 2] </ref>). Various exact algorithms exist for probabilistic inference in belief networks [19, 21, 27]. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper [6], however, showed that exact probabilistic inference for general belief networks is NP-hard. <p> If a root node E i belongs to the set E, then we instantiate it to e i . Otherwise, for each root node Z i we choose a number u uniformly from the interval <ref> [0; 1] </ref>; we then set Z i = 0 if u Pr [Z i = 0], Z i = 1 otherwise. Because u is chosen uniformly from the interval [0; 1], the probability that u is less than Pr [Z i = 0] is precisely Pr [Z i = 0]. <p> Otherwise, for each root node Z i we choose a number u uniformly from the interval <ref> [0; 1] </ref>; we then set Z i = 0 if u Pr [Z i = 0], Z i = 1 otherwise. Because u is chosen uniformly from the interval [0; 1], the probability that u is less than Pr [Z i = 0] is precisely Pr [Z i = 0]. Thus, with probability Pr [Z i = 0], the algorithm instantiates Z i to 0; with probability Pr [Z i = 1] it instantiates Z i to 1. <p> Again, if any of these nodes belong to the set E , we instantiate them according to e; otherwise, we set them according to the outcome of a random sample from <ref> [0; 1] </ref>. We proceed until we instantiate all nodes in Z. <p> Let the intervals [l i ; u i ] contain each conditional probability Pr [W i = w i j (W i )]j W=w;Z=z for all instantiations Z = z. We form a new random variable (z; w) = Q k ; contained in the interval <ref> [ Q k i=1 l i =u i ; 1] </ref>. <p> Proof Let E denote the mean of the random variable (z; w) in the bounded-variance algorithm. By construction, the interval <ref> [ Q k i=1 l i =u i ; 1] </ref> contains the outcomes of (z; w). Thus, this interval must also contain the mean E, and therefore E k . <p> i ] Initialize t 0, ! 0, Q k Function Generate instantiation z: (Generates random instantiation z 1 ; :::; z nk of Z 1 ; :::; Z nk .) Initialize Z fl fg and z fl fg For i = 1 to n k do Choose ff i 2 <ref> [0; 1] </ref> uniformly from [0; 1]. <p> 0, ! 0, Q k Function Generate instantiation z: (Generates random instantiation z 1 ; :::; z nk of Z 1 ; :::; Z nk .) Initialize Z fl fg and z fl fg For i = 1 to n k do Choose ff i 2 <ref> [0; 1] </ref> uniformly from [0; 1]. <p> We begin with the lowest ordered node Z 1 in Z. Either the node Z 1 is a root node, or its parents (Z 1 ) belong to W and are instantiated. If Z 1 is a root node, then we choose a number u from the interval <ref> [0; 1] </ref> uniformly, and set Z 1 = 0 if u Pr [Z 1 = 0], and Z 1 = 1 otherwise. If Z 1 is not a root node, then we let (Z 1 ) = z 1 denote the instantiation of its parents. <p> If Z 1 is not a root node, then we let (Z 1 ) = z 1 denote the instantiation of its parents. We choose a number u from the interval <ref> [0; 1] </ref> uniformly, and set Z 1 = 0 if u Pr [Z 1 = 0j (Z 1 ) = z 1 ], and Z 1 = 1 otherwise. <p> The order on the nodes Z guarantees that we instantiate all the parents of a node Z i before we instantiate Z i . This process forms the Generate instantiation function for the bounded-variance algorithm shown in Instead of choosing a number u between <ref> [0; 1] </ref> uniformly, we can choose an m-bit string u uniformly from all m-bit strings. <p> Appendix A.1: Stopping Rule Let Z 1 ; Z 2 ; : : : denote independently and identically distributed random variables with values in the interval <ref> [0; 1] </ref> and mean . Intuitively, Z t is the outcome of experiment t. <p> The function Generate instantiation in Figure 1 takes as input n real numbers u, each chosen uniformly from the interval <ref> [0; 1] </ref>. We determine how the length m of the nm-bit string u affects the error that we make in computing an inference probability Pr [W = w] when we use Generate instantiation z from u 2 , instead of Generate instantiation, to generate instantiations of Z.
Reference: [2] <institution> Special issue on real-world applications of uncertain reasoning. Communications of the ACM, </institution> <month> 38, </month> <year> 1995. </year>
Reference-contexts: 1 Approximation Algorithms Belief networks are powerful graphical representations of probabilistic dependencies among domain variables. Belief networks have been used successfully in many real-world problems in diagnosis, prediction, and forecasting (for example, papers included in <ref> [1, 2] </ref>). Various exact algorithms exist for probabilistic inference in belief networks [19, 21, 27]. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper [6], however, showed that exact probabilistic inference for general belief networks is NP-hard.
Reference: [3] <author> R. Chavez and G. Cooper. </author> <title> An empirical evaluation of a randomized algorithm for probabilistic inference. </title> <editor> In M. Henrion, R. Shachter, L. Kanal, and J. Lem-mer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 5, </booktitle> <pages> pages 191-207. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes <ref> [3, 4, 7, 8] </ref>. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation.
Reference: [4] <author> R. Chavez and G. Cooper. </author> <title> A randomized approximation algorithm for probabilistic inference on Bayesian belief networks. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 661-685, </pages> <year> 1990. </year> <month> 37 </month>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes <ref> [3, 4, 7, 8] </ref>. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation.
Reference: [5] <author> G. Cooper. nestor: </author> <title> A computer-based medical diganostic aid that integrates causal and probabilistic knowledge. </title> <type> PhD thesis, </type> <institution> Medical Computer Science Group, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor <ref> [5] </ref>, and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms [11, 17, 18, 30, 32, 34].
Reference: [6] <author> G. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405, </pages> <year> 1990. </year>
Reference-contexts: Various exact algorithms exist for probabilistic inference in belief networks [19, 21, 27]. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper <ref> [6] </ref>, however, showed that exact probabilistic inference for general belief networks is NP-hard. Cooper's result prompted constructions of approximation algorithms for probabilistic inference that trade off complexity in running time for the accuracy of computation. These algorithms comprise simulation-based and search-based approximations.
Reference: [7] <author> P. Dagum and R.M. Chavez. </author> <title> Approximating probabilistic inference in Bayesian belief networks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15 </volume> <pages> 246-255, </pages> <year> 1993. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes <ref> [3, 4, 7, 8] </ref>. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation.
Reference: [8] <author> P. Dagum and E. Horvitz. </author> <title> A Bayesian analysis of simulation algorithms for inference in belief networks. </title> <journal> Networks, </journal> <volume> 23 </volume> <pages> 499-516, </pages> <year> 1993. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes <ref> [3, 4, 7, 8] </ref>. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation.
Reference: [9] <author> P. Dagum, R. Karp, M. Luby, and S. Ross. </author> <title> An optimal algorithm for Monte Carlo estimation. </title> <booktitle> In Proceedings of the Thirtysixth IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 142-149, </pages> <address> Milwaukee, Wisconson, </address> <year> 1995. </year>
Reference-contexts: The bounded-variance algorithm is a simple variant of the known likelihood-weighting algorithm [13, 33], which employs recent results on the design of optimal algorithms for Monte Carlo simulation <ref> [9] </ref>. We consider an n-node belief network without extreme conditional probabilities and an evidence set E of constant size. <p> By not using the random variable (z; e), we reduce substantially the variance of our estimates. In fact, when the inference probability Pr [X = xjE = e] is small, we know, based on a straightforward application of the Generalized Zero|One Estimator Theorem <ref> [9] </ref>, that likelihood weighting requires exponential time to converge to an approximation. In contrast, we prove that the bounded-variance algorithm converges in polynomial time. We now describe formally the bounded-variance algorithm. Let W denote a subset of belief-network nodes, and let w denote some instantiation of these nodes. <p> Appendix A.1: Stopping Rule Let Z 1 ; Z 2 ; : : : denote independently and identically distributed random variables with values in the interval [0; 1] and mean . Intuitively, Z t is the outcome of experiment t. Stopping-Rule Algorithm <ref> [9] </ref> Input: 0 &lt; * 2, ffi &gt; 0 34 Initialize t 0, X 0, and S fl 4 ln (2=ffi)(1 + *)=* 2 While X &lt; S fl do t t + 1 Generate random variable Z t X X + Z t Let T fl t to be the <p> In addition, this theorem gives an upper bound on the expected number of experiments before the algorithm halts. Stopping-Rule Theorem <ref> [9] </ref>: 1. Pr [(1 *) S fl =T fl (1 + *)] &gt; 1 ffi: A.2: Pseudorandom Generator Nisan [24] gives the following construction of the s fi l matrices A s;l of Theorem 3.
Reference: [10] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153, </pages> <year> 1993. </year>
Reference-contexts: Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds. The following inference-problem instance characterizes the two forms of approximation <ref> [10] </ref>: Instance: A real value * between 0 and 1, a belief network with binary valued nodes V; arcs A; conditional probabilities Pr; hypothesis node X and set of evidence nodes 2 E in V instantiated to x and e, respectively Absolute and relative approximations refer to the type of approximation <p> probabilities with values near 0, then any polynomial-time algorithm cannot generate (1) deterministic approximations of the inference probability Pr [X = xjE = e] with absolute error * &lt; 1=2, unless NPP; and (2) randomized approximations with absolute error * &lt; 1=2 and failure probability ffi &lt; 1=2, unless NPRP <ref> [10] </ref>. The complexity of exact or approximate computation of inference probabilities Pr [X = xjE = e] in belief networks without extreme conditional probabilities remained enigmatic. <p> Unfortunately, in most cases, there does not exist a small set of instantiations that captures most of the mass of a probability. If there does exist such a small set of instantiations, then, in general, it is NP-hard to find <ref> [10] </ref>. Nonetheless, researchers have developed various heuristic methods that attempt to find these instantiations when possible. We present a deterministic-approximation algorithm for probabilistic inference. Our approach is to derandomize the randomized bounded-variance algorithm. <p> Proof The proof is similar to the proof of the complexity of approximating probabilistic inference with a single evidence node <ref> [10] </ref>. Let F denote an instance of 3-SAT with variables V = fV 1 ; : : : ; V n g and clauses C = fC 1 ; : : : ; C m g. <p> We can prove an analogous result with respect to randomized algorithms. The proof applies the same methods used in <ref> [10] </ref> to the preceding construction.
Reference: [11] <author> B. D'Ambrosio. </author> <title> Incremental probabilistic inference. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 301-308, </pages> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year> <booktitle> Association for Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [12] <author> G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic. </author> <title> Approximations of general independent distributions. </title> <booktitle> In Proceedings of the 24th IEEE Annual Symposium on Theory of Commputing, </booktitle> <year> 1992. </year>
Reference-contexts: Constructions of deterministic-approximation algorithms for specific problems in RP that do not rely on unproved conjectures, such as the existence of pseudorandom generators, have also achieved subexponential time <ref> [12, 22] </ref>. Thus far, deterministic-approximation algorithms require substantially increased run time, in comparison to a randomized-approximation algorithm for the same problem. Deterministic algorithms, however, have two significant advantages: (1) they do not require random bits, and (2) they do not fail to produce an approximation.
Reference: [13] <author> R. Fung and K.-C. Chang. </author> <title> Weighing and integrating evidence for stochastic simulation in Bayesian networks. </title> <editor> In M. Henrion, R. Shachter, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 5, </booktitle> <pages> pages 209-219. </pages> <address> El-sevier, Amsterdam, The Netherlands, </address> <year> 1990. </year> <month> 38 </month>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting <ref> [13, 33] </ref>, and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. <p> We construct the bounded-variance algorithm that proves that the complexity of approximating inferences in belief networks without extreme conditional probabilities is polynomial-time solvable. The bounded-variance algorithm is a simple variant of the known likelihood-weighting algorithm <ref> [13, 33] </ref>, which employs recent results on the design of optimal algorithms for Monte Carlo simulation [9]. We consider an n-node belief network without extreme conditional probabilities and an evidence set E of constant size.
Reference: [14] <author> R. Fung and B. Del Favero. </author> <title> Backward simulation in Bayesian networks. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 227-234, </pages> <address> Seatle, Washington, </address> <year> 1994. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation <ref> [14] </ref>, exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities.
Reference: [15] <author> M. Henrion. </author> <title> Propagating uncertainty in Bayesian networks by probabilistic logic sampling. </title> <editor> In J. Lemmer and L. Kanal, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 2, </booktitle> <pages> pages 149-163. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, <ref> [15] </ref>, likelihood weighting [13, 33], and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation.
Reference: [16] <author> M. Henrion. </author> <title> Search-based methods to bound diagnostic probabilities in very large belief nets. </title> <booktitle> In Proceedings of the Seventh Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> University of Los Angeles, Los Angeles, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks <ref> [16, 28, 29] </ref>, and other more general algorithms [11, 17, 18, 30, 32, 34]. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [17] <author> E. Horvitz, H. J. Suermondt, and G. F. Cooper. </author> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proceedings of the 1989 Workshop on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 182-193, </pages> <address> Windsor, Ontario, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [18] <author> K. Huang and M. Henrion. </author> <title> Efficient search-based inference for noisy-OR belief networks: </title> <booktitle> TopEpsilon. In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 325-331, </pages> <address> Portland, Oregon, </address> <year> 1996. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [19] <author> F. V. Jensen, S. L. Lauritzen, and K. G. Olesen. </author> <title> Bayesian updating in causal probabilistic networks by local computations. </title> <journal> Computational Statistics Quarterly, </journal> <volume> 4 </volume> <pages> 269-282, </pages> <year> 1990. </year>
Reference-contexts: 1 Approximation Algorithms Belief networks are powerful graphical representations of probabilistic dependencies among domain variables. Belief networks have been used successfully in many real-world problems in diagnosis, prediction, and forecasting (for example, papers included in [1, 2]). Various exact algorithms exist for probabilistic inference in belief networks <ref> [19, 21, 27] </ref>. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper [6], however, showed that exact probabilistic inference for general belief networks is NP-hard.
Reference: [20] <author> R. Karp, M. Luby, and N. </author> <title> Madras. Monte-Carlo approximation algorithms for enumeration problems. </title> <journal> Journal of Algorithms, </journal> <volume> 10 </volume> <pages> 429-448, </pages> <year> 1989. </year>
Reference-contexts: The Zero|One Estimator Theorem <ref> [20] </ref> gives an upper bound on the number N : N = * 2 ln ffi Thus, provided that the probability Pr [X = x; E = e] Pr [E = e] is not too small| for example, it is at least 1=n O (1) |the number of samples N is
Reference: [21] <author> S. Lauritzen and D. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50, </volume> <year> 1988. </year> <month> 39 </month>
Reference-contexts: 1 Approximation Algorithms Belief networks are powerful graphical representations of probabilistic dependencies among domain variables. Belief networks have been used successfully in many real-world problems in diagnosis, prediction, and forecasting (for example, papers included in [1, 2]). Various exact algorithms exist for probabilistic inference in belief networks <ref> [19, 21, 27] </ref>. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper [6], however, showed that exact probabilistic inference for general belief networks is NP-hard.
Reference: [22] <author> M. Luby, B. Velickovic, and A. Wigderson. </author> <title> Deterministic approximate counting of depth-2 circuits. </title> <booktitle> In Proceedings of the Second Israeli Symposium on Theory of Commputing and Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Constructions of deterministic-approximation algorithms for specific problems in RP that do not rely on unproved conjectures, such as the existence of pseudorandom generators, have also achieved subexponential time <ref> [12, 22] </ref>. Thus far, deterministic-approximation algorithms require substantially increased run time, in comparison to a randomized-approximation algorithm for the same problem. Deterministic algorithms, however, have two significant advantages: (1) they do not require random bits, and (2) they do not fail to produce an approximation.
Reference: [23] <author> R. M. Neal. </author> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal <ref> [23] </ref> provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities.
Reference: [24] <author> N. Nisan. </author> <title> Pseudorandom bits for constant depth circuits. </title> <journal> Combinatorica, </journal> <volume> 11 </volume> <pages> 63-70, </pages> <year> 1991. </year>
Reference-contexts: fl [ fz i g Return z z fl Algorithm: Construct samplespace For all u 2 do Generate instantiation z from u ! i=1 Pr [W i = ! i j (W i )]j W=w;Z=z Output: S=jj 20 4.2.1 Boolean Circuits We discuss a model of computation for which Nisan <ref> [24] </ref> proves that we can stretch a short string of truly random bits into a long string of pseudorandom bits that appears random to this model. <p> Nisan gives a method that stretches (log s) 2d+6 truly random bits into s bits that appear random to any family of circuits of size polynomial in s and depth d. Specifically, Nisan proves the following result. Theorem 3 <ref> [24] </ref> Let fC s g denote a family of circuits of depth d and size polynomial in the number of inputs s, and let l = (log s) 2d+6 . <p> In addition, this theorem gives an upper bound on the expected number of experiments before the algorithm halts. Stopping-Rule Theorem [9]: 1. Pr [(1 *) S fl =T fl (1 + *)] &gt; 1 ffi: A.2: Pseudorandom Generator Nisan <ref> [24] </ref> gives the following construction of the s fi l matrices A s;l of Theorem 3. Let p denote a prime number of size approximately (log s) d+3 , and let l = p 2 .
Reference: [25] <author> J. Pearl. </author> <title> Addendum: Evidential reasoning using stochastic simulation of causal models. </title> <journal> Artificial Intelligence, </journal> <volume> 33:131, </volume> <year> 1987. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation <ref> [25, 26] </ref>, forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms.
Reference: [26] <author> J. Pearl. </author> <title> Evidential reasoning using stochastic simulation of causal models. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 245-257, </pages> <year> 1987. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation <ref> [25, 26] </ref>, forward simulation, [15], likelihood weighting [13, 33], and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms.
Reference: [27] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: 1 Approximation Algorithms Belief networks are powerful graphical representations of probabilistic dependencies among domain variables. Belief networks have been used successfully in many real-world problems in diagnosis, prediction, and forecasting (for example, papers included in [1, 2]). Various exact algorithms exist for probabilistic inference in belief networks <ref> [19, 21, 27] </ref>. For a few special classes of belief networks, these algorithms can be shown to compute conditional probabilities efficiently. Cooper [6], however, showed that exact probabilistic inference for general belief networks is NP-hard.
Reference: [28] <author> Y. Peng and J.A. Reggia. </author> <title> A probabilistic causal model for diagnostic problem solving|Part 1: Integrating symbolic causal inference with numeric probabilistic inference. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-17(2):146-162, </volume> <year> 1987. </year>
Reference-contexts: Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks <ref> [16, 28, 29] </ref>, and other more general algorithms [11, 17, 18, 30, 32, 34]. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [29] <author> Y. Peng and J.A. Reggia. </author> <title> A probabilistic causal model for diagnostic problem solving|Part 2: Diagnostic strategy. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics: </journal> <note> Special issue for diagnosis, SMC-17(3):395-406, </note> <year> 1987. </year>
Reference-contexts: Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks <ref> [16, 28, 29] </ref>, and other more general algorithms [11, 17, 18, 30, 32, 34]. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [30] <author> D. Poole. </author> <title> Average-case analysis of a search algorithm for estimating prior and posterior probabilities in Bayesian networks with extreme probabilities. </title> <booktitle> In Proceedings of the Thirteenth IJCAI, </booktitle> <pages> pages 606-612, </pages> <year> 1993. </year>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [31] <author> M. Pradhan and P. Dagum. </author> <title> Optimal Monte Carlo estimation of belief network inference. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial 40 Intelligence, </booktitle> <pages> pages 446-453, </pages> <address> Portland, Oregon, </address> <year> 1996. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Empirical results in real-world applications, where we may observe a large fraction of query nodes and therefore cannot run the bounded-variance algorithm to completion, suggest that the algorithm continues to provide reliable approximations, although we cannot guarantee the error in those approximations <ref> [31] </ref>. Although we may entertain the possibility that another design of a randomized algorithm might lead to polynomial solutions for inference probabilities regardless of the number of observed nodes, we prove the contrary.
Reference: [32] <author> E. Santos and S. Shimony. </author> <title> Belief updating by enumeratiing high-probability independence-based assignments. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 506-513, </pages> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [33] <author> R. Shachter and M. Peot. </author> <title> Simulation approaches to general probabilistic inference on belief networks. </title> <editor> In M. Henrion, R. Shachter, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 5, </booktitle> <pages> pages 221-231. </pages> <publisher> Elsevier, </publisher> <address> Ams-terdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: These algorithms comprise simulation-based and search-based approximations. Simulation-based algorithms use a source of random bits to generate random samples of the solution space. Simulation-based algorithms include straight simulation [25, 26], forward simulation, [15], likelihood weighting <ref> [13, 33] </ref>, and randomized-approximation schemes [3, 4, 7, 8]. Variants of these methods such as backward simulation [14], exist; Neal [23] provides a good overview of the theory of simulation-based algorithms. Search-based algorithms search the space of alternative instantiations to find the most probable instantiation. <p> We construct the bounded-variance algorithm that proves that the complexity of approximating inferences in belief networks without extreme conditional probabilities is polynomial-time solvable. The bounded-variance algorithm is a simple variant of the known likelihood-weighting algorithm <ref> [13, 33] </ref>, which employs recent results on the design of optimal algorithms for Monte Carlo simulation [9]. We consider an n-node belief network without extreme conditional probabilities and an evidence set E of constant size.
Reference: [34] <author> S. E. Shimony and E. Charniak. </author> <title> A new algorithm for finding MAP assignments to belief networks. </title> <booktitle> In Proceedings of Sixth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 98-103, </pages> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: These methods yield upper and lower bounds on the inference probabilities. Search-based algorithms for probabilistic inference include nestor [5], and, more recently, algorithms restricted to two-level (bipartite) noisy-OR belief networks [16, 28, 29], and other more general algorithms <ref> [11, 17, 18, 30, 32, 34] </ref>. Approximation algorithms are categorized by the nature of the bounds on the estimates that they produce and by the reliability with which the exact answer lies within these bounds.
Reference: [35] <author> A. Yao. </author> <title> Separating the polynomial-time hierarchy by oracles. </title> <booktitle> In Proceedings of the 26th IEEE Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 1-10, </pages> <year> 1985. </year> <month> 41 </month>
Reference-contexts: Computer scientists have shown that randomization renders many problems tractable to polynomial-time approximations. These problems constitute the complexity class RP. Whether we can also generate deterministic approximations in polynomial time for problems in RP is a major open problem. Yao <ref> [35] </ref> shows that, 5 if pseudorandom generators exist, then we can generate deterministic approxima-tions for any problem in RP in subexponential time 2 (log n) d for some integer d &gt; 1.
References-found: 35

