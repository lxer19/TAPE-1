URL: http://www.isle.org/~gervasio/pub/ml89.ps
Refering-URL: http://www.isle.org/~gervasio/compleat.html
Root-URL: 
Title: EXPLANATION-BASED LEARNING OF REACTIVE OPERATORS  
Author: Melinda T. Gervasio and Gerald F. DeJong 
Address: Urbana, IL 61801  
Affiliation: Beckman Institute for Advanced Science and Technology University of Illinois,  
Note: Appears in the Proceedings of the Sixth International Workshop on Machine Learning  
Abstract: This research involves the integration of reactivity into a classical planner. Reactivity is necessary if a system is to deal with the dynamic real world, but a priori planning is also necessary for goal-directedness. A system has been implemented which incorporates explanation-based learning strategies in learning reactive operators, enabling the use of current classical planning techniques in creating partially-specified plans, completed during execution when the information necessary for resolving deferred decisions becomes available. The notion of provably correct plans in classical planning is preserved through contingent explanations and associated achievability conditions which guarantee the eventual achievement of deferred goals. 
Abstract-found: 1
Intro-found: 1
Reference: [Agre87] <author> P. Agre and D. Chapman, Pengi: </author> <title> An Implementation of a Theory of Activity, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1987, </year> <pages> pp. 268-272. </pages>
Reference-contexts: It is unable to utilize the wealth of information that becomes available during execution--information invaluable for various tasks such as verifying goal achievement, triggering operator execution, and indicating the need for replanning. REACTIVITY IN PLANNING Reactive planning <ref> [Agre87, Firby87] </ref> is one solution to the problem of dealing with the dynamic real world. However, the stimulus-response nature which provides reactive systems with their ability to deal with dynamic environments also brings about the problem of task-dependence.
Reference: [Chapman87] <author> D. Chapman, </author> <title> Planning for Conjunctive Goals, </title> <booktitle> Artificial Intelligence 32, 3 (1987), </booktitle> <pages> pp. 333-377. </pages>
Reference-contexts: INTRODUCTION The planning task has traditionally been viewed as separable from the execution task, with most of the research concerning intelligent agents' goal-directed interaction with the world having been directed towards planning. In classical planning <ref> [Chapman87] </ref>, a plan is constructed essentially through inferencing, the end result being a logical proof of the plan's achievement of the specified goals. In order to deal with real world domains solely through inference, however, the classical planner requires vast amounts of knowledge and computational resources.
Reference: [Chien89] <author> S. A. Chien, </author> <title> Using and Refining Simplifications: Explanation-Based Learning of Plans in Intractable Domains, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: Whereas reactive planners can neither plan to be reactive nor learn to be reactive--requiring the task solution itself to be constructed to be reactive--machine learning strategies have been successfully applied to classical planners in various domains <ref> [Chien89, Fikes72, Minton85] </ref>. Our approach involves the integration of reactivity into a classical planner in a manner which allows the system to continue using classical planning techniques in constructing reactive plans.
Reference: [DeJong86] <author> G. F. DeJong and R. J. Mooney, </author> <title> Explanation-Based Learning: An Alternative View, </title> <booktitle> Machine Learning 1, </booktitle> <month> 2 (April </month> <year> 1986), </year> <pages> pp. 145-176. </pages>
Reference-contexts: LEARNING REACTIVE OPERATORS This guarantee is provided in our framework with the learning of reactive operators, when explanations as to the achievability of deferred decisions are constructed. In standard explanation-based learning <ref> [DeJong86, Mitchell86] </ref>, a system learns a goal concept by observing an example of the concept, explaining the example in terms of its domain knowledge, and generalizing the explanation into an operational definition for the goal concept.
Reference: [Fikes72] <author> R. E. Fikes, P. E. Hart and N. J. Nilsson, </author> <title> Learning and Executing Generalized Robot Plans, </title> <booktitle> Artificial Intelligence 3, 4 (1972), </booktitle> <pages> pp. 251-288. </pages>
Reference-contexts: Whereas reactive planners can neither plan to be reactive nor learn to be reactive--requiring the task solution itself to be constructed to be reactive--machine learning strategies have been successfully applied to classical planners in various domains <ref> [Chien89, Fikes72, Minton85] </ref>. Our approach involves the integration of reactivity into a classical planner in a manner which allows the system to continue using classical planning techniques in constructing reactive plans.
Reference: [Firby87] <author> R. J. Firby, </author> <title> An Investigation into Reactive Planning in Complex Domains, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1987, </year> <pages> pp. 202-206. </pages>
Reference-contexts: It is unable to utilize the wealth of information that becomes available during execution--information invaluable for various tasks such as verifying goal achievement, triggering operator execution, and indicating the need for replanning. REACTIVITY IN PLANNING Reactive planning <ref> [Agre87, Firby87] </ref> is one solution to the problem of dealing with the dynamic real world. However, the stimulus-response nature which provides reactive systems with their ability to deal with dynamic environments also brings about the problem of task-dependence.
Reference: [Forbus84] <author> K. D. Forbus, </author> <title> Qualitative Process Theory, </title> <booktitle> Artificial Intelligence 24, </booktitle> <year> (1984), </year> <pages> pp. 85-168. </pages>
Reference-contexts: In many cases, however, processes often have a well-defined behavior which allows reasoning about its various qualitative states--a feature exploited by qualitative reasoning, in particular Qualitative Process Theory <ref> [Forbus84] </ref>. The system's knowledge is couched in an adaptation of QPT [Forbus84], allowing the system to reason about influences or rates of change, qualitative proportionalities, and monotonic quantity changes. <p> In many cases, however, processes often have a well-defined behavior which allows reasoning about its various qualitative states--a feature exploited by qualitative reasoning, in particular Qualitative Process Theory <ref> [Forbus84] </ref>. The system's knowledge is couched in an adaptation of QPT [Forbus84], allowing the system to reason about influences or rates of change, qualitative proportionalities, and monotonic quantity changes. The learning task given to the system is to determine the functional specifications for the goal of achieving a certain higher velocity from some initial velocity--i.e. the process of acceleration.
Reference: [Hammond88] <author> K. Hammond, T. Converse and M. Marks, </author> <title> Learning from Opportunities: Storing and Re-using Execution-Time Optimizations, </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <address> St. Paul, MN, </address> <month> August </month> <year> 1988, </year> <pages> pp. 536-540. </pages>
Reference-contexts: Our approach involves the integration of reactivity into a classical planner in a manner which allows the system to continue using classical planning techniques in constructing reactive plans. While reactivity is a notion which has cropped up in various ways in previous planners--opportunistic planning in TRUCKER <ref> [Hammond88] </ref> and failure recovery in SIPE [Wilkins84], for example--these address planning at a level where the division between planning and execution is neither desirable nor even possible since they deal with multiple goals whose existence and execution intertwine over time.
Reference: [Minton85] <author> S. Minton, </author> <title> Selectively Generalizing Plans for Problem-Solving, </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <month> August </month> <year> 1985, </year> <pages> pp. 596-599. </pages>
Reference-contexts: Whereas reactive planners can neither plan to be reactive nor learn to be reactive--requiring the task solution itself to be constructed to be reactive--machine learning strategies have been successfully applied to classical planners in various domains <ref> [Chien89, Fikes72, Minton85] </ref>. Our approach involves the integration of reactivity into a classical planner in a manner which allows the system to continue using classical planning techniques in constructing reactive plans.
Reference: [Mitchell86] <author> T. M. Mitchell, R. Keller and S. Kedar-Cabelli, </author> <title> Explanation-Based Generalization: A Unifying View, </title> <booktitle> Machine Learning 1, </booktitle> <month> 1 (January </month> <year> 1986), </year> <pages> pp. 47-80. </pages>
Reference-contexts: The basic reason a planner might choose to defer planning decisions is an imperfect domain theory <ref> [Mitchell86] </ref>. Incorrect theories caused by accuracy limitations on measurements and inexactness of execution make a priori computations unlikely to be correct. Intractable theories of complex time-spanning processes are likely to exceed available computational resources. And incomplete theories prevent inferencing regarding future states. <p> LEARNING REACTIVE OPERATORS This guarantee is provided in our framework with the learning of reactive operators, when explanations as to the achievability of deferred decisions are constructed. In standard explanation-based learning <ref> [DeJong86, Mitchell86] </ref>, a system learns a goal concept by observing an example of the concept, explaining the example in terms of its domain knowledge, and generalizing the explanation into an operational definition for the goal concept.
Reference: [Schoppers87] <author> M. J. Schoppers, </author> <title> Universal Plans for Reactive Robots in Unpredictable Environments, </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 1039-1046. </pages>
Reference-contexts: For instance, in planning to build a house, or go on a vacation, or even walk to the office down the hall, there is no reason to delay making decisions until the actual execution when one can very well plan one's moves ahead. Universal plans <ref> [Schoppers87] </ref> enables reactivity while allowing the planning/execution distinction to be made, but it does not address the issue of the suitability for deferral of a planning task.
Reference: [Wilkins84] <author> D. E. Wilkins, </author> <title> Domain-Independent Planning: Representation and Plan Generation, </title> <booktitle> Artificial Intelligence 22, </booktitle> <year> (1984), </year> <pages> pp. 269-301. </pages>
Reference-contexts: While reactivity is a notion which has cropped up in various ways in previous planners--opportunistic planning in TRUCKER [Hammond88] and failure recovery in SIPE <ref> [Wilkins84] </ref>, for example--these address planning at a level where the division between planning and execution is neither desirable nor even possible since they deal with multiple goals whose existence and execution intertwine over time.
References-found: 12

