URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/rkm/www/eurosp97.cbvq/cbvq.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/rkm/www/speechbib.html
Root-URL: 
Email: Email: rkm@cs.cmu.edu  
Phone: Tel. +1 412 268 3344, FAX: +1 412 268 5576,  
Title: SUB-VECTOR CLUSTERING TO IMPROVE MEMORY AND SPEED PERFORMANCE OF ACOUSTIC LIKELIHOOD COMPUTATION  
Author: M. Ravishankar, R. Bisiani* and E. Thayer 
Address: Pittsburgh, PA-15213, USA.  Milan, Italy  
Affiliation: School of Computer Science, Carnegie Mellon University,  *Dept. of Computer Science, University of  
Abstract: We describe a sub-vector clustering technique to reduce the memory size and computational cost of continuous density hidden Markov models (CHMMs). Acoustic models in modern large-vocabulary, continuous speech recognition systems are typically CHMMs. Systems with 100,000 Gaussian distributions of 40-60 dimensions are common, needing several tens of MB of memory. Computing HMM state likelihoods is several tens of times slower than real time. We show that by clustering and quantizing the Gaussian distributions a few dimensions at a time, both computation and memory costs can be reduced several fold without significant loss of recognition accuracy. On the 1994 Wall Street Journal 20K test set, this technique reduced the acoustic model size by a factor of 9-10, and HMM state output likelihood computation time by a factor of 4-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Proceedings of the DARPA Speech Recognition Workshop, Chantilly, VA, </institution> <month> Feb 2-5, </month> <year> 1997. </year>
Reference: [2] <author> Seide, F., </author> <title> Fast Likelihood Computation for Continuous-Mixture Densities Using a Tree-Based Nearest Neighbor Search, </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> Vol. II, </volume> <pages> pp. </pages> <address> II-1079-1082, </address> <month> Sep. </month> <year> 1995. </year>
Reference: [3] <author> Beyerlein, P. and Ulrich, M., </author> <title> Hamming Distance Approximation for a Fast Log-Likelihood Computation for Mixture Densities, </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> Vol. II, </volume> <pages> pp. </pages> <address> II-1083-1086, </address> <month> Sep. </month> <year> 1995. </year>
Reference: [4] <author> Komori, Y. et al, </author> <title> An Efficient Output Probability Computation for Continuous HMM Using Rough and Detail Models, </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> Vol. II, </volume> <pages> pp. </pages> <address> II-1087-1090, </address> <month> Sep. </month> <year> 1995. </year>
Reference: [5] <author> Fritsch, J. et al, </author> <title> Speeding up the Score Computation of HMM Speech Recognizers with the Bucket Voronoi Intersection Algorithm, </title> <booktitle> Proc. Eurospeech, </booktitle> <volume> Vol. II, </volume> <pages> pp. </pages> <address> II-1091-1094, </address> <month> Sep. </month> <year> 1995. </year>
Reference: [6] <editor> Placeway, P. et al, </editor> <booktitle> The 1996 Hub-4 Sphinx-3 System, Proc. DARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference: [7] <author> Gray, </author> <title> R.M. Vector Quantization, Readings in Speech Recognition, </title> <editor> Ed. Waibel&Lee, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> CA. </address>
Reference-contexts: Build the cluster-index codebook by replacing each mean and variance sub-vector in the original codebook with an index of its cluster centroid. (the dashed arrows in Figure 1 (c)). For the clustering in step 4 above, a straightforward k-means algorithm was used, see <ref> [7] </ref>. Thus, the procedure is basically vector quantization of concatenated mean and variance sub-vectors. As observed earlier, clustering shorter length sub-vectors keeps quantization errors low. We note that the scalar mean and variance values in the original codebook have a lot of redundancy.
Reference: [8] <author> Kubala, F. </author> <title> Design of the 1994 CSR Benchmark Tests, </title> <booktitle> Proc. DARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 41-46, </pages> <month> Jan. </month> <year> 1995. </year>
References-found: 8

