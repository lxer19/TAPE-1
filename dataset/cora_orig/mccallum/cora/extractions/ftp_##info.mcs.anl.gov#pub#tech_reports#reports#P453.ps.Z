URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P453.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: A Data Transfer Library for Communicating Data-Parallel Tasks  
Author: B. Avalani A. Choudhary I. Foster R. Krishnaiyer and M. Xu 
Abstract: Many computations can be structured as sets of communicating data-parallel tasks. Individual tasks may be coded in HPF, pC++, etc.; periodically, tasks exchange distributed arrays via channel operations, virtual file operations, message passing, etc. The implementation of these operations is complicated by the fact that the processes engaging in the communication may execute on different numbers of processors and may have different distributions for communicated data structures. In addition, they may be connected by different sorts of networks. In this paper, we describe a communicating data-parallel tasks (CDT) library that we are developing for constructing applications of this sort. We outline the techniques used to implement this library, and we describe a range of data transfer strategies and several algorithms based on these strategies. We also present performance results for several algorithms. The CDT library is being used as a compiler target for an HPF compiler augmented with Fortran M extensions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka, </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results, </title> <booktitle> Proc. Supercomputing '93, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: For the purposes of the experimental studies described in the next section, calls to this library were incorporated by hand into code generated by the Syracuse HPF compiler <ref> [1] </ref>. We are developing compiler extensions that will allow communications to be specified using FM-like CHANNEL statements embedded in HPF programs.
Reference: [2] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <title> Vienna Fortran | A Fortran language extension for distributed memory systems, Languages, Compilers, and Run-time Environments for Distributed Memory Machines, </title> <publisher> Elsevier Press, </publisher> <year> 1992. </year>
Reference-contexts: The sender and receiver tasks are assumed to be HPF-like data-parallel computations executing HPF, Vienna Fortran, Fortran D, or pC++ <ref> [2, 8, 10, 11] </ref> programs as S and R "processes" (HPF processors), respectively.
Reference: [3] <author> J. del Rosario and A. Choudhary, </author> <title> High-performance I/O for parallel computers: Problems and prospects, </title> <journal> IEEE Computer, </journal> <volume> 27(3), </volume> <pages> 59-68, </pages> <year> 1994. </year>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF [15, 12], all-to-all communication [4, 5, 14], and parallel I/O <ref> [3] </ref>. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O.
Reference: [4] <author> A. Edelman, </author> <title> Optimal matrix transposition and bit reversal on hypercubes: All-to-all personalized communication, </title> <journal> J. Par. Dist. Comp., </journal> <volume> 11, </volume> <pages> 328-331, </pages> <year> 1991. </year>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF [15, 12], all-to-all communication <ref> [4, 5, 14] </ref>, and parallel I/O [3]. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O.
Reference: [5] <author> J. O. Eklundh, </author> <title> A fast computer method for matrix transposing, </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21, </volume> <pages> 801-803, </pages> <year> 1972. </year>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF [15, 12], all-to-all communication <ref> [4, 5, 14] </ref>, and parallel I/O [3]. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O. <p> Squares represent staging processors, which would typically be mapped to sending processors. 1. All-to-All. Each sender sends data to each receiver. This strategy can be useful if message startups are cheap and the network connecting senders and receivers provides high connectivity. 2. Multistage. As in all-to-all communication <ref> [5] </ref>, the number of messages required for a data transfer can be reduced by combining data destined for each receiver using a multistage communication structure, such as a butterfly. Total data volume is increased, however.
Reference: [6] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> J. </journal> <note> Parallel and Distributed Computing (to appear), and Preprint MCS-P327-0992, </note> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Notice the staging performed in the first stage. This allows the data transfer to proceed in three instead of four stages, as would be required with the all-to-all algorithm. 2.3 Implementation A prototype of the data transfer library has been constructed using the Fortran M (FM) <ref> [6] </ref> extensions to Fortran. For the purposes of the experimental studies described in the next section, calls to this library were incorporated by hand into code generated by the Syracuse HPF compiler [1].
Reference: [7] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M, </title> <booktitle> Proc. Scalable High Performance Computing Conf., IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: In other related work, Hatcher and Quinn [9] describe an implementation of communicating data-parallel C tasks on iWarp; however, communications pass via a single control processor. Subhlok et al. [13] consider communicating HPF tasks, also on iWarp. Foster et al. <ref> [7] </ref> describe the use of Fortran M constructs to coordinate HPF computations, but do not consider data redistribution. <p> an example of the type of program that might be written using these constructs. (This is one of the programs used for benchmarking purposes.) In our implementation, each HPF task is implemented as a collection of FM processes, and communication both within and between tasks is implemented using FM channels <ref> [7] </ref>. Communicating data-parallel tasks may execute on the same or different processors; in the experiments described in this paper, we place them on disjoint sets of processors. Separate channels are established for the transfer of data and control information.
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran D language specification, </title> <type> Technical Report TR90-141, </type> <institution> Department of Computer Science, Rice University, Houston, Texas, </institution> <year> 1990. </year>
Reference-contexts: The sender and receiver tasks are assumed to be HPF-like data-parallel computations executing HPF, Vienna Fortran, Fortran D, or pC++ <ref> [2, 8, 10, 11] </ref> programs as S and R "processes" (HPF processors), respectively.
Reference: [9] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1991. </year>
Reference-contexts: Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O. In other related work, Hatcher and Quinn <ref> [9] </ref> describe an implementation of communicating data-parallel C tasks on iWarp; however, communications pass via a single control processor. Subhlok et al. [13] consider communicating HPF tasks, also on iWarp.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The sender and receiver tasks are assumed to be HPF-like data-parallel computations executing HPF, Vienna Fortran, Fortran D, or pC++ <ref> [2, 8, 10, 11] </ref> programs as S and R "processes" (HPF processors), respectively.
Reference: [11] <author> A. Malony, B. Mohr, P. Beckman, D. Gannon, S. Yang, F. Bodin, and S. Kesavan, </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems, </title> <booktitle> Proc. Supercomputing '93, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: The sender and receiver tasks are assumed to be HPF-like data-parallel computations executing HPF, Vienna Fortran, Fortran D, or pC++ <ref> [2, 8, 10, 11] </ref> programs as S and R "processes" (HPF processors), respectively.
Reference: [12] <author> S. Ramaswamy and P. Banerjee, </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers, </title> <type> Technical Report CRHC-94-09, </type> <institution> University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF <ref> [15, 12] </ref>, all-to-all communication [4, 5, 14], and parallel I/O [3]. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O.
Reference: [13] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross, </author> <title> Exploiting task and data parallelism on a mul-ticomputer, </title> <booktitle> Proc. 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: In other related work, Hatcher and Quinn [9] describe an implementation of communicating data-parallel C tasks on iWarp; however, communications pass via a single control processor. Subhlok et al. <ref> [13] </ref> consider communicating HPF tasks, also on iWarp. Foster et al. [7] describe the use of Fortran M constructs to coordinate HPF computations, but do not consider data redistribution. <p> this graph would be at a lower value of D if a more efficient initialization algorithm were used. 3.4 Other Applications In addition to the synthetic examples described in this section, we have used our library to implement several more substantial programs, some based on a benchmark suite from CMU <ref> [13] </ref>. For example, a two-dimensional (2-D) fast Fourier transform (FFT) may be implemented as a purely data-parallel program or as two communicating data-parallel tasks, each responsible for performing 1-D FFTs in one direction. The second implementation gives superior performance in many circumstances.
Reference: [14] <author> S. S. Takkella and S. R. Seidel, </author> <title> Complete exchange and broadcast algorithms for meshes, </title> <booktitle> Proc. Scalable High Performance Computing Conf., IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF [15, 12], all-to-all communication <ref> [4, 5, 14] </ref>, and parallel I/O [3]. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O.
Reference: [15] <author> R. Thakur, A. Choudhary and G. Fox, </author> <title> Runtime array redistributions in HPF Programs, </title> <booktitle> Proc. Scalable High Performance Computing Conf., IEEE, </booktitle> <pages> 309-316. </pages>
Reference-contexts: We have designed such a library and used it to experiment with several alternative data transfer algorithms. There are intriguing similarities between some aspects of the data transfer problem and the problems of array redistribution in HPF <ref> [15, 12] </ref>, all-to-all communication [4, 5, 14], and parallel I/O [3]. Some techniques developed for those problems can be applied to the data transfer problem; some techniques described in this paper may be applicable to parallel I/O.
Reference: [16] <author> C. J. Turner, D. Mosberger, and L. L. Peterson. Cluster-C*: </author> <title> Understanding the performance limits. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Another interesting direction for further work is the development of specialized network protocols for the types of data transfer considered in this paper. For example, Turner et al. <ref> [16] </ref> describe specialized network protocols for the execution of data-parallel tasks on workstation networks. 8 0.001 0.1 10 100 1000 10000 100000 Time (sec) Array size (words) S=8, R=4 Model S=4, R=2 Model configurations.
References-found: 16

