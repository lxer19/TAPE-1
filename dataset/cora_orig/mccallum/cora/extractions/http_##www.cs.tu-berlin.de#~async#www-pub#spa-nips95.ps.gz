URL: http://www.cs.tu-berlin.de/~async/www-pub/spa-nips95.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: async@cs.tu-berlin.de  
Title: In  Stable Dynamic Parameter Adaptation  
Author: "D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo Stefan M. Ruger 
Address: Sekr. FR 5-9, Franklinstr. 28/29 10 587 Berlin, Germany  
Affiliation: Fachbereich Informatik, Technische Universitat Berlin  
Note: (Eds.), Advances in Neural Information Processing Systems 8, 225-231, MIT Press, Cambridge, 1996."  algorithms is presented and studied, including a convergence proof.  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Battiti. </author> <title> Accelerated backpropagation learning: Two optimization methods. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 331-342, </pages> <year> 1989. </year>
Reference-contexts: There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive -tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional -estimation [4]; or straightforward -adaptation <ref> [1, 7] </ref>.
Reference: [2] <author> S. Becker and Y. le Cun. </author> <title> Improving the convergence of back-propagation learning with second order methods. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 29-37. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: This weaker stability criterion allows for greedy steps in the initial phase of learning. There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods <ref> [2] </ref> as an adaptive -tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional -estimation [4]; or straightforward -adaptation [1, 7].
Reference: [3] <author> R. Jacobs. </author> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 295-307, </pages> <year> 1988. </year>
Reference-contexts: This weaker stability criterion allows for greedy steps in the initial phase of learning. There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive -tensor; individual learning rates for the weights <ref> [3, 8] </ref>; conjugate gradient as a one-dimensional -estimation [4]; or straightforward -adaptation [1, 7].
Reference: [4] <author> A. Kramer and A. Sangiovanni-Vincentelli. </author> <title> Efficient parallel learning algorithms for neural networks. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 40-48. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: This weaker stability criterion allows for greedy steps in the initial phase of learning. There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive -tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional -estimation <ref> [4] </ref>; or straightforward -adaptation [1, 7].
Reference: [5] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: One serious drawback of Euler's method is that it is unstable: each finite step leaves the trajectory of a solution without trying to get back to it. Virtually any other differential-equation solver surpasses Euler's method, and there are even some featuring dynamic parameter adaptation <ref> [5] </ref>. However, in the context of function minimization, this notion of stability ("do not drift away too far from a trajectory") would appear to be too strong. Indeed, differential-equation solvers put much effort into a good estimation of points that are as close as possible to the trajectory under consideration. <p> The momentum term of a modified backpropagation version uses old gradient directions; Newton or quasi-Newton methods explicitly or implicitly exploit second-order derivatives for a change of direction; another choice of direction is given by conjugate gradient methods <ref> [5] </ref>. The algorithms from Section 3 allow almost any direction, as long as it is not nearly perpendicular to the gradient. <p> Problem (c) uses a pure square error function E: w 7! P n i=1 ijw i j p =2 with p = 2 and n = 76. Note that conjugate gradient needs exactly n epochs to arrive at the minimum <ref> [5] </ref>. However, the few additional epochs that are needed by the ff = 1 algorithm to reach a fairly small error (here 118 as opposed to 76) must be compared to the overhead of conjugate gradient (one line search per epoch).
Reference: [6] <author> R. Salomon. Verbesserung konnektionistischer Lernverfahren, </author> <title> die nach der Gra-dientenmethode arbeiten. </title> <type> PhD thesis, </type> <institution> TU Berlin, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: A particularly good example of dynamic parameter adaptation was proposed by Salomon <ref> [6, 7] </ref>: let &gt; 1; at every step t of the backpropagation algorithm test two values for , a somewhat smaller one, t =, and a somewhat larger one, t ; use as t+1 the value with the better performance, i. e., the smaller error: t+1 = t = if E <p> Although this works much better, Figure 1b shows the instability of this algorithm due to the change in the gradient's direction. There is enough evidence that these algorithms converge for a purely quadratic error function <ref> [6, 7] </ref>.
Reference: [7] <author> R. Salomon and J. L. van Hemmen. </author> <title> Accelerating backpropagation through dynamic self-adaptation. Neural Networks, </title> <note> 1996 (in press). </note>
Reference-contexts: There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive -tensor; individual learning rates for the weights [3, 8]; conjugate gradient as a one-dimensional -estimation [4]; or straightforward -adaptation <ref> [1, 7] </ref>. <p> A particularly good example of dynamic parameter adaptation was proposed by Salomon <ref> [6, 7] </ref>: let &gt; 1; at every step t of the backpropagation algorithm test two values for , a somewhat smaller one, t =, and a somewhat larger one, t ; use as t+1 the value with the better performance, i. e., the smaller error: t+1 = t = if E <p> Although this works much better, Figure 1b shows the instability of this algorithm due to the change in the gradient's direction. There is enough evidence that these algorithms converge for a purely quadratic error function <ref> [6, 7] </ref>. <p> The difference lies in the occurrence of some stabilization steps from time to time, which, in general, improve the convergence. Since comparisons of Salomon's algorithm to many other methods have been published <ref> [7] </ref>, this paper confines itself to show that significant improvements are brought about by non-gradient directions, e. g., by Polak-Ribiere directions (ff = 1). <p> expected and found for large powers. 1 Dynamic parameter adaptation as in (1) can cope with the square-root singularity (p = 1=2) in one dimension, because the adaptation rule allows a fast enough decay of the learning rate; the ability to minimize this one-dimensional square-root singularity is somewhat overemphasized in <ref> [7] </ref>. The 8-3-8 encoder (f) was studied, because the error function has global minima at the boundary of the domain (one or more weights with infinite length). These minima, though not covered in Section 4, are quickly found.
Reference: [8] <author> F. M. Silva and L. B. Almeida. </author> <title> Speeding up backpropagation. </title> <booktitle> In Proceedings of NSMS International Symposium on Neural Networks for Sensory and Motor Systems, </booktitle> <address> Amsterdam, 1990. </address> <publisher> Elsevier. </publisher>
Reference-contexts: This weaker stability criterion allows for greedy steps in the initial phase of learning. There are several successful examples of dynamic learning-rate adaptation for backpropagation: Newton and quasi-Newton methods [2] as an adaptive -tensor; individual learning rates for the weights <ref> [3, 8] </ref>; conjugate gradient as a one-dimensional -estimation [4]; or straightforward -adaptation [1, 7].
References-found: 8

