URL: ftp://ftp.cs.utah.edu/techreports/1994/UUCS-94-040.ps.Z
Refering-URL: http://www.cs.utah.edu/projects/avalanche/avalanche-publications.html
Root-URL: 
Title: Evaluating the Potential of Programmable Multiprocessor Cache Controllers  
Author: John B. Carter Mike Hibler Ravindra R. Kuramkote 
Affiliation: Department of Computer Science University of Utah  
Pubnum: UUCS-94-040  
Abstract: The next generation of scalable parallel systems (e.g., machines by KSR, Convex, and others) will have shared memory supported in hardware, unlike most current generation machines (e.g., offerings by Intel, nCube, and Thinking Machines). However, current shared memory architectures are constrained by the fact that their cache controllers are hardwired and inflexible, which limits the range of programs that can achieve scalable performance. This observation has led a number of researchers to propose building programmable multiprocessor cache controllers that can implement a variety of caching protocols, support multiple communication paradigms, or accept guidance from software. To evaluate the potential performance benefits of these designs, we have simulated five SPLASH benchmark programs on a virtual multiprocessor that supports five directory-based caching protocols. When we compared the off-line optimal performance of this design, wherein each cache line was maintained using the protocol that required the least communication, with the performance achieved when using a single protocol for all lines, we found that use of the "optimal" protocol reduced consistency traffic by 10-80%, with a mean improvement of 25-35%. Cache miss rates also dropped by up to 25%. Thus, the combination of programmable (or tunable) hardware and software able to exploit this added flexibility, e.g., via user pragmas or compiler analysis, could dramatically improve the performance of future shared memory multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer 16 Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Every block of physical memory has associated with it a "home node," which is the node at which the main memory copy of the block resides [21]. A directory-based cache is used to support shared memory spanning all of the nodes in the machine <ref> [1, 11, 21] </ref>. <p> Yes Invalidate 1 Sequential Migratory No Invalidate 1 Sequential Dash Yes Invalidate 1 Release Adaptive Yes Invalidate 1 Release Munin Yes Update Multiple Release Table 2 Summary of Consistency Protocols Investigated 6 The Conventional protocol represents a direct extension of a conventional bus-based write-invalidate consistency protocol to a directory-based implementation <ref> [1] </ref>. A node can only write to a shared cache line when it is the owner and has the sole copy of the block in the system.
Reference: [2] <author> G. Astfalk. </author> <title> Past progress and future abilities in high performance computing. </title> <booktitle> In Proceedings of a 1993 meeting of the Max Planck Society in Gemrany, </booktitle> <year> 1993. </year>
Reference-contexts: Spurred by scalable shared memory architectures developed in academia [11, 21, 32], the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex <ref> [2, 3] </ref> and KSR [6]). However, current shared memory multiprocessors all support a single, hardwired consistency protocol and do not provide any reasonable hooks with which the compiler or runtime system can guide the hardware's behavior.
Reference: [3] <author> G. Astfalk, T. Breweh, and G. Palmeh. </author> <title> Cache coherency in the convex mpp. </title> <institution> Convex Computer Corporation, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Spurred by scalable shared memory architectures developed in academia [11, 21, 32], the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex <ref> [2, 3] </ref> and KSR [6]). However, current shared memory multiprocessors all support a single, hardwired consistency protocol and do not provide any reasonable hooks with which the compiler or runtime system can guide the hardware's behavior.
Reference: [4] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [4, 15, 17, 29] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [5] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In COMPCON '93, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Therefore, while message passing is, and will remain, an important communication mechanism, parallel programmers are increasingly demanding support for shared memory. Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware <ref> [5, 8, 9, 12, 23] </ref> or (ii) via hardware implementations of scalable shared memory architectures [6, 11, 22, 32]. Research in both of these areas has been very successful in recent years.
Reference: [6] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9002001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware [5, 8, 9, 12, 23] or (ii) via hardware implementations of scalable shared memory architectures <ref> [6, 11, 22, 32] </ref>. Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication [9, 12]. <p> Spurred by scalable shared memory architectures developed in academia [11, 21, 32], the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex [2, 3] and KSR <ref> [6] </ref>). However, current shared memory multiprocessors all support a single, hardwired consistency protocol and do not provide any reasonable hooks with which the compiler or runtime system can guide the hardware's behavior.
Reference: [7] <author> J.B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consis tency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: This research was supported in part by the National Science Foundation under the grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols <ref> [7, 31] </ref>, support multiple communication paradigms [10, 18], or accept guidance from software [20, 25]. <p> In this way, multiple nodes are able to write to different words in the same block concurrently. With this scheme, write races can be detected easily <ref> [7, 9] </ref>. In the basic scheme, a read miss is handled by requesting a copy of the data from the home node, which always has a usable copy (2 messages).
Reference: [8] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Therefore, while message passing is, and will remain, an important communication mechanism, parallel programmers are increasingly demanding support for shared memory. Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware <ref> [5, 8, 9, 12, 23] </ref> or (ii) via hardware implementations of scalable shared memory architectures [6, 11, 22, 32]. Research in both of these areas has been very successful in recent years. <p> A line drops out of Migratory mode on any miss for a line that has not been dirtied since the last migration. The Munin protocol is similar to the "write-shared" protocol employed in the Munin DSM system <ref> [8, 9] </ref>. Unlike the other three protocols, it uses an update-based consistency mechanism and allows multiple nodes to modify the cache line concurrently. Like Dash, it exploits the power of release consistency to improve performance.
Reference: [9] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Therefore, while message passing is, and will remain, an important communication mechanism, parallel programmers are increasingly demanding support for shared memory. Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware <ref> [5, 8, 9, 12, 23] </ref> or (ii) via hardware implementations of scalable shared memory architectures [6, 11, 22, 32]. Research in both of these areas has been very successful in recent years. <p> Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication <ref> [9, 12] </ref>. Efficient DSM systems can now perform as well as hardware shared memory for large grained programs [12], but the overhead associated with software implementations of DSM limits its value for fine-grained computations. <p> A line drops out of Migratory mode on any miss for a line that has not been dirtied since the last migration. The Munin protocol is similar to the "write-shared" protocol employed in the Munin DSM system <ref> [8, 9] </ref>. Unlike the other three protocols, it uses an update-based consistency mechanism and allows multiple nodes to modify the cache line concurrently. Like Dash, it exploits the power of release consistency to improve performance. <p> In this way, multiple nodes are able to write to different words in the same block concurrently. With this scheme, write races can be detected easily <ref> [7, 9] </ref>. In the basic scheme, a read miss is handled by requesting a copy of the data from the home node, which always has a usable copy (2 messages). <p> These two facts allow multiple updates to be combined into single messages, as was done successfully in the Munin system <ref> [9] </ref>. Except when otherwise noted, we assume that the hardware is smart enough to put several small updates destined to the same remote node into a single message when there is space. <p> As the size of the cache lines is increased, the effects of false sharing cause Munin to perform better and the invalidation-based protocols to perform more poorly, as expected given Munin's good handling of false sharing <ref> [9] </ref>. cache miss rate, despite the fact that our measure of "optimal" minimizes the number of consistency messages, with no attention paid to the effect on cache miss rate. <p> It can both benefit from large message sizes when the cache lines are small, or from cache-line-sized messages when cache lines are large. This experiment also illustrates the power of combining updates, a scheme that worked well in software for Munin <ref> [9] </ref> and seems to translate well to hardware. 4 Related Work There are a number of ongoing academic efforts with the goal of designing a scalable shared memory multiprocessor.
Reference: [10] <author> D. Chaiken and A. Agarwal. </author> <title> Software-extended coherent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: by the National Science Foundation under the grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [7, 31], support multiple communication paradigms <ref> [10, 18] </ref>, or accept guidance from software [20, 25]. <p> The reference system model that we used, illustrated in Figure 1, is a generalization of the combined features of Alewife <ref> [10, 11] </ref>, FLASH [18, 20], and Typhoon [25]. We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes. <p> For the purposes of this study, we do not specify how this is accomplished, since our intent is only to measure the performance impact that correct tuning can provide. Alewife <ref> [10] </ref>, Typhoon [25], and FLASH [20] all provide mechanisms to allow software access to the caching hardware, to varying degrees. 2.2 Mint multiprocessor simulator We used the Mint memory hierarchy simulator [30] running on a collection of Silicon Graphics Indigo2's to perform our simulations. <p> This machine, called FLASH [20] will support both DASH-like shared memory and efficient message passing, although it should have the power to implement other caching protocols such as the ones described above. The MIT Alewife machine <ref> [10, 11] </ref> also uses a directory-based cache design that supports both very low-latency message passing and shared memory based on an invalidation-based consistency protocol. <p> Furthermore, for several of the applications, this tuning process reduced cache miss rates by up to 25% despite being optimized for reducing the number of consistency messages. This indicates that it is worthwhile investigating (i) hardware techniques for supporting software control of multiprocessor caches <ref> [10, 20, 25] </ref> and (ii) software techniques for fully exploiting this flexible hardware [25]. If successful, such systems could allow shared memory multiprocessors to scale to far larger designs than is currently feasible, without requiring prohibitively expensive and complicated controllers and interconnects.
Reference: [11] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coher ence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware [5, 8, 9, 12, 23] or (ii) via hardware implementations of scalable shared memory architectures <ref> [6, 11, 22, 32] </ref>. Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication [9, 12]. <p> Efficient DSM systems can now perform as well as hardware shared memory for large grained programs [12], but the overhead associated with software implementations of DSM limits its value for fine-grained computations. Spurred by scalable shared memory architectures developed in academia <ref> [11, 21, 32] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex [2, 3] and KSR [6]). <p> The reference system model that we used, illustrated in Figure 1, is a generalization of the combined features of Alewife <ref> [10, 11] </ref>, FLASH [18, 20], and Typhoon [25]. We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes. <p> Every block of physical memory has associated with it a "home node," which is the node at which the main memory copy of the block resides [21]. A directory-based cache is used to support shared memory spanning all of the nodes in the machine <ref> [1, 11, 21] </ref>. <p> The number of overhead bits varies from 1-4% of the data bits, depending on the size of the cache line and the complexity of the modeled hardware. The cache directory in our model is used in the same way as the directories in DASH [21] and Alewife <ref> [11] </ref>. The home node for each block of physical memory maintains a directory entry corresponding to the block with information about its global state, such as the set of processors with copies of the block (the "copyset") and the state of the block (read-shared, exclusive, etc.). <p> We conjecture that the poor scalability of the Munin protocol would be eliminated if we limited the number of processors that can simultaneously cache a given line of data, which is required in large scale machines already because bitmaps are not scalable data structures <ref> [11] </ref>. 3.3 Effect of Limiting the Number of Protocols To evaluate how effective a controller could be with limited "smarts," we measured the optimal performance of a programmable controller when only a subset of the protocols was implemented. <p> This machine, called FLASH [20] will support both DASH-like shared memory and efficient message passing, although it should have the power to implement other caching protocols such as the ones described above. The MIT Alewife machine <ref> [10, 11] </ref> also uses a directory-based cache design that supports both very low-latency message passing and shared memory based on an invalidation-based consistency protocol.
Reference: [12] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Therefore, while message passing is, and will remain, an important communication mechanism, parallel programmers are increasingly demanding support for shared memory. Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware <ref> [5, 8, 9, 12, 23] </ref> or (ii) via hardware implementations of scalable shared memory architectures [6, 11, 22, 32]. Research in both of these areas has been very successful in recent years. <p> Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication <ref> [9, 12] </ref>. Efficient DSM systems can now perform as well as hardware shared memory for large grained programs [12], but the overhead associated with software implementations of DSM limits its value for fine-grained computations. <p> The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication [9, 12]. Efficient DSM systems can now perform as well as hardware shared memory for large grained programs <ref> [12] </ref>, but the overhead associated with software implementations of DSM limits its value for fine-grained computations. Spurred by scalable shared memory architectures developed in academia [11, 21, 32], the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex [2, 3] and KSR [6]).
Reference: [13] <author> A.L. Cox and R.J. Fowler. </author> <title> Adaptive cache coherency for detecting migratory shared data. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98-108, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: a no-replicate migratory protocol ("Migratory"), (iii) a release consistent [16] implementation of a conventional multiple reader, single writer, write invalidate protocol ("Dash"), (iv) a protocol that adapts to the way that it is used to dynamically switch between Dash and Migratory, depending on how a cache line is being used <ref> [13, 27] </ref> ("Adaptive") and (v) a release consistent multiple reader, multiple writer, write update protocol ("Munin"). We selected these five protocols because they covered a wide spectrum of options available to system designers. Table 2 summarizes the design parameters of the four protocols. <p> Several researchers have found that the provision of a migratory protocol can improve system performance significantly <ref> [13, 27] </ref>. Dash is identical to the Conventional protocol, except that the new owner of a cache block does not have to stall while it waits for acknowledgements to its invalidation messages. <p> The Adaptive protocol is a version of Dash modified to detect migratory sharing patterns and dynamically switch between the replicate-on-read-miss protocol of Dash and the migrate-on-read-miss protocol of Migratory <ref> [13, 27] </ref>. All cache lines start out in Dash mode and transition to migratory if on a write hit requiring an invalidation there are exactly two copies of the line and the current invalidating processor is not the last processor to perform an invalidation.
Reference: [14] <author> H. Davis, S. Goldschmidt, and J. L. Hennessy. </author> <title> Tango: A multiprocessor simulation and tracing system. </title> <type> Technical Report CSL-TR-90-439, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Mint simulates a collection of processors and provides support for spinlocks, semaphores, barriers, shared memory, and most Unix system calls. It generates multiple streams of memory reference events, which it uses to drive a user-provided memory system simulator. We chose Mint over several similar multiprocessor simulators, such as Tango <ref> [14] </ref> and the Wisconsin Wind Tunnel (WWT) [24], for three reasons. First, it runs on fairly conventional hardware, namely MIPS-based workstations from Silicon Graphics and DEC. This separates it from WWT, which requires the use of a prohibitively expensive Thinking Machines CM-5. Second, it is very efficient.
Reference: [15] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its ap plication to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year> <month> 17 </month>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [4, 15, 17, 29] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [16] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: races in mp3d, barnes and LocusRoute, but most of the them were localized to few words. 2.4 Protocols Investigated We evaluated the performance of five basic consistency protocols: (i) a sequentially consistent multiple reader, singler writer, write invalidate protocol ("Conventional"), (ii) a no-replicate migratory protocol ("Migratory"), (iii) a release consistent <ref> [16] </ref> implementation of a conventional multiple reader, single writer, write invalidate protocol ("Dash"), (iv) a protocol that adapts to the way that it is used to dynamically switch between Dash and Migratory, depending on how a cache line is being used [13, 27] ("Adaptive") and (v) a release consistent multiple reader, <p> This optimization assumes that the program is written using sufficient synchronization to avoid data races, which is most often the case. The details of why this results in correct behavior is beyond the scope of this paper we refer you to the original DASH papers for a detailed explanation <ref> [16, 21] </ref>. For simplicity, we assume that all acknowledgement messages arrive before the processor reaches a release point, and thus do not count invalidation acknowledgements for the Dash protocol. <p> Specifically, a node is allowed to write to any cache block that it has without communicating with any other node, regardless of whether or not the block is currently replicated. To maintain consistency, when a node performs a release operation as defined by release consistency <ref> [16] </ref> (releases a lock, arrives at a barrier, etc.), it is required to update all replicas of blocks that it has modified since the last release.
Reference: [17] <author> A. Gupta and W.-D. Weber. </author> <title> Cache invalidation patterns in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [4, 15, 17, 29] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [18] <author> J. Heinlein, K. Gharachorloo, and A. Gupta. </author> <title> Integrating multiple communication paradigms in high performance multiprocessors. </title> <type> Technical Report CSL-TR-94-604, </type> <institution> Stanford Computer Systems Laboratory, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: by the National Science Foundation under the grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [7, 31], support multiple communication paradigms <ref> [10, 18] </ref>, or accept guidance from software [20, 25]. <p> The reference system model that we used, illustrated in Figure 1, is a generalization of the combined features of Alewife [10, 11], FLASH <ref> [18, 20] </ref>, and Typhoon [25]. We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes.
Reference: [19] <institution> Intel Supercomputers Systems Division. </institution> <type> Paragon technical summary, </type> <year> 1993. </year>
Reference-contexts: 1 Introduction There are two basic ways that parallel processes typically communicate: via message passing and via shared memory. The current generation of massively parallel processors (e.g., machines by Intel <ref> [19] </ref> and Thinking Machines [28]) support message passing as the sole means of communication because of its architectural simplicity and relative scalability. However, message passing has failed to establish itself as being as straightforward and intuitive a programming model as shared memory.
Reference: [20] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [7, 31], support multiple communication paradigms [10, 18], or accept guidance from software <ref> [20, 25] </ref>. <p> The reference system model that we used, illustrated in Figure 1, is a generalization of the combined features of Alewife [10, 11], FLASH <ref> [18, 20] </ref>, and Typhoon [25]. We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes. <p> For the purposes of this study, we do not specify how this is accomplished, since our intent is only to measure the performance impact that correct tuning can provide. Alewife [10], Typhoon [25], and FLASH <ref> [20] </ref> all provide mechanisms to allow software access to the caching hardware, to varying degrees. 2.2 Mint multiprocessor simulator We used the Mint memory hierarchy simulator [30] running on a collection of Silicon Graphics Indigo2's to perform our simulations. <p> DASH's cache consistency protocol was described in Section 2.4. A second generation DASH multiprocessor is being developed that adds a limited amount of processing power and state at the distributed directories to add some flexibility to the consistency implementation. This machine, called FLASH <ref> [20] </ref> will support both DASH-like shared memory and efficient message passing, although it should have the power to implement other caching protocols such as the ones described above. <p> Furthermore, for several of the applications, this tuning process reduced cache miss rates by up to 25% despite being optimized for reducing the number of consistency messages. This indicates that it is worthwhile investigating (i) hardware techniques for supporting software control of multiprocessor caches <ref> [10, 20, 25] </ref> and (ii) software techniques for fully exploiting this flexible hardware [25]. If successful, such systems could allow shared memory multiprocessors to scale to far larger designs than is currently feasible, without requiring prohibitively expensive and complicated controllers and interconnects.
Reference: [21] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Efficient DSM systems can now perform as well as hardware shared memory for large grained programs [12], but the overhead associated with software implementations of DSM limits its value for fine-grained computations. Spurred by scalable shared memory architectures developed in academia <ref> [11, 21, 32] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex [2, 3] and KSR [6]). <p> We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes. Every block of physical memory has associated with it a "home node," which is the node at which the main memory copy of the block resides <ref> [21] </ref>. A directory-based cache is used to support shared memory spanning all of the nodes in the machine [1, 11, 21]. <p> Every block of physical memory has associated with it a "home node," which is the node at which the main memory copy of the block resides [21]. A directory-based cache is used to support shared memory spanning all of the nodes in the machine <ref> [1, 11, 21] </ref>. <p> The number of overhead bits varies from 1-4% of the data bits, depending on the size of the cache line and the complexity of the modeled hardware. The cache directory in our model is used in the same way as the directories in DASH <ref> [21] </ref> and Alewife [11]. The home node for each block of physical memory maintains a directory entry corresponding to the block with information about its global state, such as the set of processors with copies of the block (the "copyset") and the state of the block (read-shared, exclusive, etc.). <p> This optimization assumes that the program is written using sufficient synchronization to avoid data races, which is most often the case. The details of why this results in correct behavior is beyond the scope of this paper we refer you to the original DASH papers for a detailed explanation <ref> [16, 21] </ref>. For simplicity, we assume that all acknowledgement messages arrive before the processor reaches a release point, and thus do not count invalidation acknowledgements for the Dash protocol. <p> Typhoon is a proposed hardware implementation for this interface. Like the Alewife system, the proposed system uses 15 low level software handlers and provides more flexibility. As such, it requires extensive program modifications and user effort to achieve the required performance. The Stanford DASH multiprocessor <ref> [22, 21] </ref> uses a directory-based cache design to interconnect a collection of 4-processor SGI boards based on the MIPS 3000 RISC processor. DASH's cache consistency protocol was described in Section 2.4.
Reference: [22] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware [5, 8, 9, 12, 23] or (ii) via hardware implementations of scalable shared memory architectures <ref> [6, 11, 22, 32] </ref>. Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication [9, 12]. <p> Typhoon is a proposed hardware implementation for this interface. Like the Alewife system, the proposed system uses 15 low level software handlers and provides more flexibility. As such, it requires extensive program modifications and user effort to achieve the required performance. The Stanford DASH multiprocessor <ref> [22, 21] </ref> uses a directory-based cache design to interconnect a collection of 4-processor SGI boards based on the MIPS 3000 RISC processor. DASH's cache consistency protocol was described in Section 2.4.
Reference: [23] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Therefore, while message passing is, and will remain, an important communication mechanism, parallel programmers are increasingly demanding support for shared memory. Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware <ref> [5, 8, 9, 12, 23] </ref> or (ii) via hardware implementations of scalable shared memory architectures [6, 11, 22, 32]. Research in both of these areas has been very successful in recent years.
Reference: [24] <author> S.K. Reinhardt, M.D. Hill, J.R. Larus, A.R. Lebeck, J.C. Lewis, and D.A. Wood. </author> <title> The Wiscon sin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It generates multiple streams of memory reference events, which it uses to drive a user-provided memory system simulator. We chose Mint over several similar multiprocessor simulators, such as Tango [14] and the Wisconsin Wind Tunnel (WWT) <ref> [24] </ref>, for three reasons. First, it runs on fairly conventional hardware, namely MIPS-based workstations from Silicon Graphics and DEC. This separates it from WWT, which requires the use of a prohibitively expensive Thinking Machines CM-5. Second, it is very efficient.
Reference: [25] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared mem ory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [7, 31], support multiple communication paradigms [10, 18], or accept guidance from software <ref> [20, 25] </ref>. <p> The reference system model that we used, illustrated in Figure 1, is a generalization of the combined features of Alewife [10, 11], FLASH [18, 20], and Typhoon <ref> [25] </ref>. We assume a distributed shared memory (DSM) model, whereby the main memory in the machine is equally distributed across the processing nodes. Every block of physical memory has associated with it a "home node," which is the node at which the main memory copy of the block resides [21]. <p> For the purposes of this study, we do not specify how this is accomplished, since our intent is only to measure the performance impact that correct tuning can provide. Alewife [10], Typhoon <ref> [25] </ref>, and FLASH [20] all provide mechanisms to allow software access to the caching hardware, to varying degrees. 2.2 Mint multiprocessor simulator We used the Mint memory hierarchy simulator [30] running on a collection of Silicon Graphics Indigo2's to perform our simulations. <p> All of these projects are currently emphasizing the mechanisms needed to implement a flexible cache controller, rather than the policies and software required to exploit this flexibility. The proposed user level shared memory in Tempest and Typhoon system <ref> [25] </ref> will support cooperation between software and hardware to implement a scalable shared memory and message passing abstraction. Tempest provides an interface allowing user level code to efficiently utilize the underlying low-level communication and shared memory mechanism. Typhoon is a proposed hardware implementation for this interface. <p> Furthermore, for several of the applications, this tuning process reduced cache miss rates by up to 25% despite being optimized for reducing the number of consistency messages. This indicates that it is worthwhile investigating (i) hardware techniques for supporting software control of multiprocessor caches <ref> [10, 20, 25] </ref> and (ii) software techniques for fully exploiting this flexible hardware [25]. If successful, such systems could allow shared memory multiprocessors to scale to far larger designs than is currently feasible, without requiring prohibitively expensive and complicated controllers and interconnects. <p> This indicates that it is worthwhile investigating (i) hardware techniques for supporting software control of multiprocessor caches [10, 20, 25] and (ii) software techniques for fully exploiting this flexible hardware <ref> [25] </ref>. If successful, such systems could allow shared memory multiprocessors to scale to far larger designs than is currently feasible, without requiring prohibitively expensive and complicated controllers and interconnects.
Reference: [26] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We simulated the performance of the SPLASH benchmark suite <ref> [26] </ref> on a directory-based shared memory multiprocessor that supported five different cache consistency protocols. <p> This let us modify our algorithms repeatedly while refining our experiments, and in conjunction with its efficiency, made it possible for us to test a large number of hypotheses. 2.3 SPLASH benchmark programs We used five programs from the SPLASH benchmark suite <ref> [26] </ref> in our study, mp3d, water, barnes, LocusRoute, and cholesky. We were unable to compile the remaining two programs in our test environment 1 . Table 1 contains the inputs for each test program. mp3d is a three-dimensional particle simulator used to simulated rarified hypersonic airflow.
Reference: [27] <author> P. Stenstrom, M. Brorsson, and L. Sandberg. </author> <title> An adaptive cache coherence protocol opti mized for migratory sharing. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 109-118, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: a no-replicate migratory protocol ("Migratory"), (iii) a release consistent [16] implementation of a conventional multiple reader, single writer, write invalidate protocol ("Dash"), (iv) a protocol that adapts to the way that it is used to dynamically switch between Dash and Migratory, depending on how a cache line is being used <ref> [13, 27] </ref> ("Adaptive") and (v) a release consistent multiple reader, multiple writer, write update protocol ("Munin"). We selected these five protocols because they covered a wide spectrum of options available to system designers. Table 2 summarizes the design parameters of the four protocols. <p> Several researchers have found that the provision of a migratory protocol can improve system performance significantly <ref> [13, 27] </ref>. Dash is identical to the Conventional protocol, except that the new owner of a cache block does not have to stall while it waits for acknowledgements to its invalidation messages. <p> The Adaptive protocol is a version of Dash modified to detect migratory sharing patterns and dynamically switch between the replicate-on-read-miss protocol of Dash and the migrate-on-read-miss protocol of Migratory <ref> [13, 27] </ref>. All cache lines start out in Dash mode and transition to migratory if on a write hit requiring an invalidation there are exactly two copies of the line and the current invalidating processor is not the last processor to perform an invalidation.
Reference: [28] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 technical summary, </title> <year> 1991. </year>
Reference-contexts: 1 Introduction There are two basic ways that parallel processes typically communicate: via message passing and via shared memory. The current generation of massively parallel processors (e.g., machines by Intel [19] and Thinking Machines <ref> [28] </ref>) support message passing as the sole means of communication because of its architectural simplicity and relative scalability. However, message passing has failed to establish itself as being as straightforward and intuitive a programming model as shared memory.
Reference: [29] <author> J.E. Veenstra and R.J. Fowler. </author> <title> A performance evaluation of optimal hybrid cache coherency protocols. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 149-160, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [4, 15, 17, 29] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [30] <author> J.E. Veenstra and R.J. Fowler. Mint: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In MASCOTS 1994, </booktitle> <month> January </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Alewife [10], Typhoon [25], and FLASH [20] all provide mechanisms to allow software access to the caching hardware, to varying degrees. 2.2 Mint multiprocessor simulator We used the Mint memory hierarchy simulator <ref> [30] </ref> running on a collection of Silicon Graphics Indigo2's to perform our simulations. Mint simulates a collection of processors and provides support for spinlocks, semaphores, barriers, shared memory, and most Unix system calls. <p> Depending on the number of processors and the complexity of the cache controllers being simulated, our simulation runs took between five minutes and two hours to complete. Mint is approximately 100 times faster than Tango <ref> [30] </ref>, which would have required hours or days per simulation run. Finally, Mint has a clean user interface and is relatively easy to program.
Reference: [31] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This research was supported in part by the National Science Foundation under the grant CCR-9308879 and the Hewlett-Packard Research Grants Program, and the University of Utah. 2 These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols <ref> [7, 31] </ref>, support multiple communication paradigms [10, 18], or accept guidance from software [20, 25].
Reference: [32] <author> L.D. Wittie, G. Hermannsson, and A. Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year> <title> 20 (cholesky) (cholesky) 21 Miss Rate vs # of Processors (mp3d, 128-byte cache lines) Messages vs # of Processors (mp3d, 128-byte cache lines) 22 Optimal(Dash + Migratory) Protocols (mp3d) Optimal(Dash + Migratory) Protocols (cholesky) 23 24 </title>
Reference-contexts: Traditionally, this demand has been satisfied in two ways: (i) via software implementations of distributed shared memory (DSM) on message-passing hardware [5, 8, 9, 12, 23] or (ii) via hardware implementations of scalable shared memory architectures <ref> [6, 11, 22, 32] </ref>. Research in both of these areas has been very successful in recent years. The performance of software DSM systems has improved dramatically by addressing the problems of false sharing and excessive DSM-related communication [9, 12]. <p> Efficient DSM systems can now perform as well as hardware shared memory for large grained programs [12], but the overhead associated with software implementations of DSM limits its value for fine-grained computations. Spurred by scalable shared memory architectures developed in academia <ref> [11, 21, 32] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex [2, 3] and KSR [6]). <p> Sesame is a fast hardware network interface that supports distributed shared memory on a network of high-speed workstations <ref> [32] </ref>. Sesame aggressively attacks the problem of communication latency caused by demand-driven data transfers by selectively sending updates to shared data in advance of when they are requested, so-called eager sharing.
References-found: 32

