URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/finkem/www/ps/icassp97-swb.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/finkem/www/publications.html
Root-URL: 
Title: RECOGNITION OF CONVERSATIONAL TELEPHONE SPEECH USING THE JANUS SPEECH ENGINE  
Author: Torsten Zeppenfeld Michael Finke Klaus Ries Martin Westphal Alex Waibel 
Address: Germany  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University, USA University of Karlsruhe,  
Abstract: Recognition of conversational speech is one of the most challenging speech recognition tasks to-date. While recognition error rates of 10% or lower can now be reached on speech dictation tasks over vocabularies in excess of 60,000 words, recognition of conversational speech has persistently resisted most attempts at improvements by way of the proven techniques to date. Difficulties arise from shorter words, telephone channel degradation, and highly disfluent and coarticulated speech. In this paper, we describe the application, adaptation, and performance evaluation of our JANUS speech recognition engine to the Switchboard conversational speech recognition task. Through a number of algorithmic improvements, we have been able to reduce error rates from more than 50% word error to 38%, measured on the official 1996 NIST evaluation test set. Improvements include vocal tract length normalization, polyphonic modeling, label boosting, speaker adaptation with and without confidence measures, and speaking mode dependent pronunciation modeling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.R. Bahl, P.V. de Souza, P.S. Gopalakrishnan, D. Nah-moo, and M.A. Picheny. </author> <title> Decision Trees for Phonological Rules in Continuous Speech. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Toronto, 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: The most common approach is to use triphones. Recently, several speech recognition groups have started investigating the use of larger phonetic context windows when building acoustic models <ref> [1, 6] </ref>.
Reference: [2] <author> F. Beaufays, Y. Konig, Z. Rivlin, A. Stolcke, and M. Weintraub. </author> <title> Neural Network Based Measures of Confidence. </title> <booktitle> In Proceedings of LVCSR Hub 5 Workshop, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: We have noted consistent improvements of 1-2% using this technique. This is not surprising, when we note the tremendous performance gain that adaptation can bring to the system when used with known transcriptions (table 2). 4. ACOUSTIC STABILITY CONFIDENCE MEASURE Similar to the N-Best confidence measure described in <ref> [2] </ref>, the idea behind our acoustic stability confidence measure algorithm is that we expect regions of high acoustic stability to be regions that are relatively error free, and regions of low acoustic stability to be regions that will frequently contain recognizer errors.
Reference: [3] <author> Ellen Eide and Herbert Gish. </author> <title> A Parametric Approach to Vocal Tract Length Normalization. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Atlanta, 1996. </address> <publisher> IEEE. </publisher>
Reference-contexts: Speakers come in all shapes and sizes, and so does their speech. We use a maximum likelihood based vocal tract length normalization algorithm in order to remove some of the variation due to speakers' differing vocal tract characteristics. A non-linear warping <ref> [3] </ref> in the frequency domain is done based on the second formant according to: ^ f = f K 2F where K is the warping factor for a particular speaker with second formant F 2 , F is the Nyquist frequency, and F 2 is the average second formant for all
Reference: [4] <author> Michael Finke and Ivica Rogina. </author> <title> Wide Context Acoustic Modeling in Read vs. Spontaneous Speech. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: For Switchboard we ended up having 4000 codebooks and 20000 distributions. This clustering approach implements a flexible parameter tying scheme, and gave us significant improvement across many tasks, including WSJ, Switchboard, and the Spontaneous Scheduling Task. It has also proved itself across several languages (German, Spanish, English) <ref> [4] </ref>. For Switchboard, we have observed a WER reduction of 2.4% absolute. 2.3. Language Modeling The Switchboard corpus contains approximately 2 million words of training text. Typically only about 60% of the trigrams in the test text were actually seen in the training text.
Reference: [5] <author> Reinhard Kneser and Herman Ney. </author> <title> Improved clustering techniques for class-based statistical language modeling. </title> <booktitle> In Eurospeech, </booktitle> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: SWB class based LM: sample classes The word classes for our class-based models were built using a procedure that optimizes the bigram perplexity criterion <ref> [5] </ref>. Table 1 shows examples of some of the automatically generated classes for the Switchboard model. In order to derive effective word classes, we classified only words that have more than a minimum number of counts and introduced a prior on the number of classes.
Reference: [6] <author> R. Kuhn, A. Lazadrides, Y. Normandin, and J. Brousseau. </author> <title> Improved Decision Trees for Phonetic Modeling. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 552-555, </pages> <address> De-troit, Michigan, 1995. </address> <publisher> IEEE. </publisher>
Reference-contexts: The most common approach is to use triphones. Recently, several speech recognition groups have started investigating the use of larger phonetic context windows when building acoustic models <ref> [1, 6] </ref>.
Reference: [7] <author> L. Neumeyer, A. Sankar, and V. Digalakis. </author> <title> A Comparative Study of Speaker Adaptation Techniques. </title> <booktitle> In Eurospeech, </booktitle> <year> 1995. </year>
Reference-contexts: For this reason, a form of unsupervised adaptation is used in our evaluation system. It has the advantage of performing an arbitrary linear transformation on the acoustic models. We use a maximum likelihood linear regression (MLLR) unsupervised speaker adaptation algorithm <ref> [7] </ref> to adapt our acoustic models to specific speakers during testing. Given a set of recognition hypotheses for a speaker's utterances, the algorithm transforms the acoustic models in order to maximize the likelihood of these hypotheses.

References-found: 7

