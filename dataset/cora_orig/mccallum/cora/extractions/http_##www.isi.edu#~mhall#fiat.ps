URL: http://www.isi.edu/~mhall/fiat.ps
Refering-URL: http://www.isi.edu/~mhall/mypapers.html
Root-URL: http://www.isi.edu
Phone: 2  
Title: FIAT: A Framework for Interprocedural Analysis and Transformation  
Author: Mary W. Hall John M. Mellor-Crummey Alan Carle Rene G. Rodrguez 
Address: Stanford, CA 94305  Houston, TX 77251  
Affiliation: 1 Center for Integrated Systems, Stanford University,  Center for Research on Parallel Computation, Rice University,  
Abstract: The fiat system is a compiler-building tool that enables rapid prototyping of interprocedural analysis and compilation systems. Fiat is a framework because it provides parameterized templates and common drivers to support interprocedural data-flow analysis and procedure cloning. Further, fiat provides the complex underlying support required to collect and manage information about the procedures in the program. Fiat's reliance on system-independent abstractions makes it suitable for use in systems with distinct intermediate code representations and enables sharing of system software across research platforms. Demand-driven analysis maintains a clean separation between interpro-cedural analysis problems, enabling tools built upon fiat to solve only the data-flow problems of immediate interest. Fiat drives interprocedu-ral optimization in the ParaScope programming tools at Rice University and the SUIF compiler at Stanford University. Fiat has proven to be a valuable aid in development of a large number of interprocedural tools, including a data race detection system, a static performance estimation tool, a distributed-memory compiler for Fortran D, an interactive paral lelizing tool and an automatic parallelizer in the SUIF compiler.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: We will analyze reaching decompositions and perform communication optimization, similar to the techniques used by the Fortran D compiler. However, the SUIF compiler operates on Fortran 77, automatically partitioning data and computation, rather than relying on programmer specifications <ref> [1] </ref>. A purely automatic compiler requires much more precise analysis information. For example, reaching decompositions requires flow-sensitive analysis. 8 Summary and Future Plans Fiat provides a framework for interprocedural analysis and transformation that facilitates incorporating interprocedural techniques into existing systems.
Reference: 2. <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Interprocedural transformations restructure a program to reveal more precise data-flow information or move code across procedure boundaries. Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [2, 30] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. <p> have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [2, 30] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: 3. <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> to appear 1993. </note>
Reference-contexts: This property is true even if the module is used as part of several programs. Using this three phase analysis strategy along with a technique called recompilation analysis, a whole-program compiler could restrict recompilation to only those modules to which interprocedural optimizations that are no longer valid <ref> [3] </ref> were applied. The fiat system supports the interprocedural propagation phase. This phase is independent of the intermediate representation of source code used by the other phases. Fiat's input consists of local information for each procedure represented using a system-independent abstraction that is described in the next section.
Reference: 4. <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The ProcSummary abstraction for P contains its name and, in the callsites field, the call to R with the names and types of the actual parameters. The tuple <ref> [16; 4] </ref> indicates the data of length 4 bytes starting at an offset of 16 bytes from the beginning of array A. The ProcSummary abstraction for R is similar, with field formals listing the formal parameters and their types. <p> An Example To illustrate how the flow-sensitive solver works, we compute the Live sets defined in Section 2. 5 The example program is shown in Figure 5. For 5 At procedure nodes, the solution represents variables live on exit, similar to Calla han's definition <ref> [4] </ref>. the live problem, the meet function ^ is union, and the initial &gt; set value is ;. The following steps are required to converge at a solution.
Reference: 5. <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 152-161. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter passing mechanism [19]. Constants (p): the set of pairs hv; ci representing that variable v has con stant value c across all calls to procedure p <ref> [5] </ref>. We use the following example of a flow-sensitive problem in Section 5, where the node n represents either a procedure or a block of code. Live (n): the set of variables v that may be used prior to any redefinition [27]. 2.3 Interprocedural Transformations invoke procedure D.
Reference: 6. <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [2, 30]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [6, 33, 32] </ref>, interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> For more precise analysis of procedure side effects to arrays, we are developing subsystems that will compute interprocedural Mod, Ref, and Kill information for subportions of arrays (regular sections <ref> [6] </ref>) affected by a call and interprocedural symbolic analysis that will help eliminate dependences by deriving information about loop bounds and subscript expressions. Data race detection. The eraser system was developed to instrument shared-memory parallel fortran programs to automatically detect data races at run time [25].
Reference: 7. <author> K. Cooper, M. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2), </volume> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Procedure cloning. To perform this transformation, the compiler replicates a procedure and tailors the procedure and its copy to distinct calling environments <ref> [7, 10] </ref>. In Figure 2 (c), procedure D is cloned, inheriting distinct calling context information from A. Procedures B and C both invoke the same copy D2. Cloning thus avoids summarization of information inherited from callers. <p> We then briefly discuss the requirements for frameworks to support other interprocedural transformations. 6.1 A Framework for Procedure Cloning The design of the cloning framework is based on previous research on how to apply cloning to expose better data-flow information while avoiding the potential for exponential code growth <ref> [7] </ref>. Two key insights about cloning motivated the implementation: Refines forward data-flow information. Cloning changes the structure of the call graph in a way that allows interprocedural analysis to proceed along distinct paths. <p> However, compilers cannot capitalize on every new data-flow fact that is exposed. For example, it would not be profitable to clone based on constant values of a variable that is not referenced in the procedure or its descendants. Other examples of desirable cloning filters are discussed elsewhere <ref> [7, 15] </ref>. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed [28]. <p> For clarity of presentation, this algorithm assumes that the call graph contains no cycles (signifying recursion). Support for recursion is described elsewhere <ref> [7] </ref>. The algorithm relies on the following two functions: The function SplitNode (n) replicates node n in the call graph, replicates its outgoing edges, reassociates its incoming edges, and copies its annotations and other associated data.
Reference: 8. <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Moreover, fiat's general framework greatly reduces the programming effort needed to incorporate new interprocedural data-flow analyses and transformations. Fiat currently supports interprocedural optimization in two very different host systems | the ParaScope programming tools at Rice University <ref> [8] </ref> and the Stanford SUIF Compiler [31]. This paper describes the following key aspects of fiat. Abstract representations. The representation of programs used within fiat is decoupled from the representation used by any host system in which fiat will be applied. <p> remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interpro-cedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [8, 10, 14] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate in-terprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2.
Reference: 9. <author> K. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In Figure 2 (b), by inlining the calls to D, each version of D may be fully optimized in the context of its caller. However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time <ref> [29, 9] </ref>. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits [9, 11, 24, 29]. <p> However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [29, 9]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [9, 11, 24, 29] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs. Procedure cloning.
Reference: 10. <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interpro-cedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [8, 10, 14] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate in-terprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2. <p> Procedure cloning. To perform this transformation, the compiler replicates a procedure and tailors the procedure and its copy to distinct calling environments <ref> [7, 10] </ref>. In Figure 2 (c), procedure D is cloned, inheriting distinct calling context information from A. Procedures B and C both invoke the same copy D2. Cloning thus avoids summarization of information inherited from callers.
Reference: 11. <author> J. W. Davidson and A. M. Holler. </author> <title> A study of a C function inliner. </title> <journal> Software| Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 775-790, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [29, 9]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [9, 11, 24, 29] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs. Procedure cloning.
Reference: 12. <author> D. Grove. </author> <title> Interprocedural constant propagation: a study of jump function implementations. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [2, 30]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation <ref> [12, 26] </ref>, array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: 13. <author> M. Hall. </author> <title> Comparing fiat's support of flow-sensitive interprocedural data-flow analysis with existing techniques. </title> <note> Technical Note Available from First Author. To be published as a Stanford Dept. of Computer Science Technical Report. </note>
Reference-contexts: Because of its potential for significant space requirements, algorithms for flow-sensitive analysis typically exploit specific aspects of their data-flow problem to reduce costs. Thus, our support for flow-sensitive analysis currently has two limitations. We outline them here and describe them in detail elsewhere <ref> [13] </ref>. After experience implementing a number of flow-sensitive data-flow problems, we will consider general solutions to these problems. Unrealizable paths. In our example, we propagated information along paths in the graph that do not correspond to possible execution paths, or realizable paths [23].
Reference: 14. <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interpro-cedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [8, 10, 14] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate in-terprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2.
Reference: 15. <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: However, compilers cannot capitalize on every new data-flow fact that is exposed. For example, it would not be profitable to clone based on constant values of a variable that is not referenced in the procedure or its descendants. Other examples of desirable cloning filters are discussed elsewhere <ref> [7, 15] </ref>. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed [28]. <p> The compiler must thus propagate decompositions interprocedurally in order to determine how data is partitioned locally. Moreover, an interprocedural approach to communication optimization would enable communication to be extracted out of loops across procedure boundaries <ref> [15] </ref>. The current compiler implementation, based on fiat, contains analysis of decompositions reaching a procedure, and interprocedural analysis of storage requirements for nonlocal data. Static performance estimation.
Reference: 16. <author> M. W. Hall and K. Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(3), </volume> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Fiat then performs call graph analysis to compute, for each procedure-valued formal f , the set Boundto (f ) containing the possible procedure constants that may be passed to f at run time <ref> [16] </ref>. Finally, for each formal f , the system adds an edge from f's representative node to each procedure b in Boundto (f ). Representative nodes serve as a location to summarize, during interprocedural analysis, interprocedural information from all of the procedures potentially invoked through a procedure-valued formal. <p> The ProcSummary abstraction for P contains its name and, in the callsites field, the call to R with the names and types of the actual parameters. The tuple <ref> [16; 4] </ref> indicates the data of length 4 bytes starting at an offset of 16 bytes from the beginning of array A. The ProcSummary abstraction for R is similar, with field formals listing the formal parameters and their types.
Reference: 17. <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization <ref> [17] </ref>. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> Interprocedural code motion. This transformation moves some portion of code across the procedure boundary. In addition to inline substitution, two other forms of code motion have been explored: loop embedding and loop extraction <ref> [17] </ref>. The former moves a loop surrounding a call into the invoked procedure; the latter moves an outer enclosing loop across a call and into the caller. These transformations partially inline the code. <p> A programmer of the framework would need to pro vide the decision procedure, which would be permitted to examine information in the call graph. Loop embedding and loop extraction require support similar to inlining. The system could determine whether embedding and extraction are safe <ref> [17] </ref>. For instances of safe embedding or extraction, a decision procedure could be invoked to determine if the techniques are desirable.
Reference: 18. <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The compiler performs extensive program analysis as well as optimizations such as extracting messages out of loops in order to reduce communication cost <ref> [18] </ref>. The semantics of Fortran D dictate that a procedure inherits the data decompositions of its formal parameters and global variables from its callers. The compiler must thus propagate decompositions interprocedurally in order to determine how data is partitioned locally.
Reference: 19. <author> J.P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: This organization is shown in Figure 1. 2.2 Interprocedural Data-Flow Analysis Interprocedural data-flow analysis can be described as either flow insensitive or flow sensitive. Flow-insensitive analysis calculates data-flow effects at program locations without considering the control flow paths that will be taken through individual procedures <ref> [19] </ref>. In contrast, flow-sensitive analysis derives the effects common to each distinct control-flow path that reaches a location [27]. <p> Graphical comparison of interprocedural optimization techniques. Mod (c): the set of variables that may be modified following procedure call c, either by the called procedure or its descendants in the call graph <ref> [19] </ref>. - Ref (c): the set of variables that may be referenced as a result of a procedure call c [19]. Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter passing mechanism [19]. <p> Mod (c): the set of variables that may be modified following procedure call c, either by the called procedure or its descendants in the call graph <ref> [19] </ref>. - Ref (c): the set of variables that may be referenced as a result of a procedure call c [19]. Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter passing mechanism [19]. <p> in the call graph <ref> [19] </ref>. - Ref (c): the set of variables that may be referenced as a result of a procedure call c [19]. Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter passing mechanism [19]. Constants (p): the set of pairs hv; ci representing that variable v has con stant value c across all calls to procedure p [5]. We use the following example of a flow-sensitive problem in Section 5, where the node n represents either a procedure or a block of code.
Reference: 20. <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley. </author> <title> Static performance estimation in a parallelizing compiler. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Moreover, often when performing loop transformations, the system has a number of possible options that may have substantially different effects on performance. To assist the compiler in making optimization choices, the ParaScope system contains static performance estimation <ref> [20] </ref>. To make decisions about loops containing procedure calls, these performance estimates must be propagated interprocedurally. Interprocedural optimization in SUIF. The above applications characterize uses of fiat within ParaScope.
Reference: 21. <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience, </journal> <note> to appear 1993. </note>
Reference-contexts: Below we describe the several experimental systems developed at Rice and Stanford that rely on fiat's interprocedural analysis. Interactive parallelization. The ParaScope Editor supports interactive restructuring of fortran programs for execution on shared-memory parallel architectures <ref> [21] </ref>. In order to safely restructure a program to execute the iterations of a loop containing a procedure call in parallel, the tool requires knowledge of the side effects of the call.
Reference: 22. <author> G. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1973. </year>
Reference-contexts: effects, but other problems specific to compiling programs for distributed memory multiprocessors or automatically instrumenting shared-memory programs to detect data races are not included when building tools for any other purpose. 5 Parameterized Templates for Interprocedural Analysis Fiat provides a framework for interprocedural data-flow analysis including a Kildall-style iterative solver <ref> [22] </ref>. <p> The programmer provides specifications to be used by the interprocedural data-flow solver to compute values of data-flow sets at nodes and edges in the call graph. The functions provided to fiat are simply the specifications for the Kildall data-flow analysis framework <ref> [22] </ref>: - InitNode (n) and InitEdge (e) provide the initial annotations at node n and edge e, respectively. These functions collect information from ProcSummary or from the solutions to previous data-flow problems.
Reference: 23. <author> W. Landi and B. Ryder. </author> <title> A safe approximate algorithm for interprocedural pointer aliasing. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, SIGPLAN Notices 27(7), </booktitle> <pages> pages 235-248, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: After experience implementing a number of flow-sensitive data-flow problems, we will consider general solutions to these problems. Unrealizable paths. In our example, we propagated information along paths in the graph that do not correspond to possible execution paths, or realizable paths <ref> [23] </ref>. Specifically, Set (b5) = ft; yg is imprecise. It results from propagation of values along the following subpath in the graph: : : :, b4, b3, b10, b9, b8, b5, : : :.
Reference: 24. <author> S. McFarling. </author> <title> Procedure merging with instruction caches. </title> <booktitle> In SIGPLAN '91 Conference on Programming Language Design and Implementation, SIGPLAN Notices 26(6), </booktitle> <pages> pages 71-79, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [29, 9]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [9, 11, 24, 29] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs. Procedure cloning.
Reference: 25. <author> J. M. Mellor-Crummey. </author> <title> Compile-time support for efficient data race detection in shared-memory parallel programs. </title> <booktitle> In Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Data race detection. The eraser system was developed to instrument shared-memory parallel fortran programs to automatically detect data races at run time <ref> [25] </ref>.
Reference: 26. <author> R. Metzger and S. Stroud. </author> <title> Interprocedural constant propagation: an empirical study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <note> 1993. to appear. </note>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [2, 30]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation <ref> [12, 26] </ref>, array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> As evidence of this point, cloning based on constants in the Convex Applications Compiler increased the number of constant values appearing in codes by more than 50% for 14 programs <ref> [26] </ref>. These programs were part of a suite of 29 programs with average size greater than 62,000 lines. 6.2 Other Interprocedural Transformations Similar frameworks are necessary to support other interprocedural transformations such as inline substitution, loop embedding and loop extraction. Fiat could provide a framework to drive inline substitution.
Reference: 27. <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: Flow-insensitive analysis calculates data-flow effects at program locations without considering the control flow paths that will be taken through individual procedures [19]. In contrast, flow-sensitive analysis derives the effects common to each distinct control-flow path that reaches a location <ref> [27] </ref>. We use several flow-insensitive data-flow problems as examples throughout the paper: A B C (a) Interprocedural Analysis A B C (b) Inline Substitution A B C (c) Procedure Cloning A D1 D2 (d) Interprocedural Code Motion Fig. 2. Graphical comparison of interprocedural optimization techniques. <p> We use the following example of a flow-sensitive problem in Section 5, where the node n represents either a procedure or a block of code. Live (n): the set of variables v that may be used prior to any redefinition <ref> [27] </ref>. 2.3 Interprocedural Transformations invoke procedure D. D contains three calls to procedures not shown.
Reference: 28. <author> P. Briggs and K.D. Cooper and M.W. Hall and L. Torczon. </author> <title> Goal-directed inter-procedural optimization. </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Other examples of desirable cloning filters are discussed elsewhere [7, 15]. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed <ref> [28] </ref>. Fiat's cloning framework supports goal-directed cloning by allowing the programmer to specify a filtering function to distinguish between data-flow facts that have an impact on code quality and those that do not, thus avoiding unnecessary code growth.
Reference: 29. <author> S. Richardson and M. Ganapathi. </author> <title> Interprocedural analysis versus procedure integration. </title> <journal> Information Processing Letters, </journal> <volume> 32(3) </volume> <pages> 137-142, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: In Figure 2 (b), by inlining the calls to D, each version of D may be fully optimized in the context of its caller. However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time <ref> [29, 9] </ref>. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits [9, 11, 24, 29]. <p> However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [29, 9]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [9, 11, 24, 29] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs. Procedure cloning.
Reference: 30. <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Interprocedural transformations restructure a program to reveal more precise data-flow information or move code across procedure boundaries. Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [2, 30] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. <p> have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [2, 30] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [6, 33, 32], interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: 31. <author> S. Tjiang, M. E. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrating scalar optimization and parallelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, Aug. 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Moreover, fiat's general framework greatly reduces the programming effort needed to incorporate new interprocedural data-flow analyses and transformations. Fiat currently supports interprocedural optimization in two very different host systems | the ParaScope programming tools at Rice University [8] and the Stanford SUIF Compiler <ref> [31] </ref>. This paper describes the following key aspects of fiat. Abstract representations. The representation of programs used within fiat is decoupled from the representation used by any host system in which fiat will be applied.
Reference: 32. <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [2, 30]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [6, 33, 32] </ref>, interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: 33. <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(9) </volume> <pages> 85-99, </pages> <year> 1988. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [2, 30]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [6, 33, 32] </ref>, interprocedural constant propa-gation [12, 26], array Kill analysis [2, 30], and transformations to expose loop nests to parallelization [17]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
References-found: 33

