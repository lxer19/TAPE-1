URL: ftp://ftp.cs.washington.edu/tr/1993/06/UW-CSE-93-06-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/projects/softbots/www/papers.html
Root-URL: 
Title: Benchmarks, Testbeds, Controlled Experimentation, and the Design of Agent Architectures  
Author: Steve Hanks, Martha Pollack Paul Cohen 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  2 Department of Computer Science, University of Massachusetts  
Abstract: Technical Report 93-06-05 June 17, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Agre and Chapman 1987] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> An Implementation of a Theory of Activity. </title> <booktitle> In Proceedings, AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: Relaxing this assumption makes the process of predicting the effects of plans more difficult [Hanks 1990b] and also introduces the need to react to unplanned events as they occur at execution time <ref> [Agre and Chapman 1987] </ref>, [Firby 1989].
Reference: [Bond and Gasser 1988] <editor> Alan H. Bond and Les Gasser, editors. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Multiple Agents Allowing multiple agents to act in the world introduces new problems: how to coordinate behaviors, how the agents should communicate, how the effects of simultaneous actions differ from the effects of those actions performed serially. Multiple-agent planning is an active research area <ref> [Bond and Gasser 1988] </ref> and a testbed for exploring these research issues must allow coordinated behavior and communication among the agents that inhabit it.
Reference: [Bratman et al. 1988] <author> Michael E. Bratman, David J. Israel, and Martha E. Pollack. </author> <title> Plans and Resource-Bounded Practical Reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 349-355, </pages> <year> 1988. </year>
Reference-contexts: The time cost of planning becomes important in a world that allows unplanned changes: the longer the agent takes to plan, the more likely it is that the world has changed significantly between the time the plan was generated and the time it is executed <ref> [Bratman et al. 1988] </ref>, [Russell and Wefald 1991], [Dean and Boddy 1988]. Complexity of the World A realistic world has many features. Even a simple block has color, mass, texture, smell, and so on, although many of these features will be irrelevant to many tasks. <p> The world's simplicity means that a few parameters define a world instance completely, and these parameters can be varied as experiments are performed. The Tileworld was originally developed to investigate a particular agent architecture (IRMA <ref> [Bratman et al. 1988] </ref>), and, in fact, is distributed to the research community with an embedded IRMA agent. IRMA actually specifies a space of agent architectures; in other words, there is a range of agent architectures within the IRMA framework. <p> concrete results that can be expected from these experimental endeavors, and these two pieces of work are rare examples of systematic experimentation with agent architectures in small, controlled worlds. 5.1.1 The original tileworld experiments The planning agent studied in [Pollack and Ringuette 1990] is an implementation of the IRMA architecture <ref> [Bratman et al. 1988] </ref>.
Reference: [Bratman 1987] <author> Michael E. Bratman. </author> <title> Intention, Plans and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year> <month> 44 </month>
Reference: [Brooks 1991] <author> Rodney A. Brooks. </author> <title> Intelligence without Reasoning. </title> <booktitle> In Proceedings of the Twelth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 569-595, </pages> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference: [Chapman 1990] <author> David Chapman. </author> <title> On Choosing Domains for Agents, June 1990. </title> <booktitle> Proceedings of the NASA/AMES Workshop on Benchmarks and Metrics. </booktitle>
Reference-contexts: But at this point we must remind ourselves of our ultimate goals: to build systems that solve interesting problems and to understand why they do so. Research decisions must be oriented toward solving problems, not toward satisfying methodological goals. The ultimate 7 <ref> [Chapman 1990] </ref> advances an even stronger view: that randomness without structure actually makes planning more difficult. 28 danger of experimentation in the small is that it entices us into solving problems that we understand rather than ones that are interesting.
Reference: [Chrisman and Simmons 1991] <author> L. Chrisman and Reid Simmons. </author> <title> Senseful Planning: Focusing Perceptual Attention. </title> <booktitle> In Proceedings, AAAI, </booktitle> <year> 1991. </year>
Reference-contexts: An agent must therefore incorporate incorrect and noisy sensor reports into its predictive model of the world [Hanks and McDermott 1993] and must plan sensing actions to improve its state of information, taking into account both the benefit of that information and the cost of acquiring it <ref> [Chrisman and Simmons 1991] </ref>. Thus a testbed for studying agent design might be populated with agents having imperfect sensors and effectors. The testbed needs 7 to make a clean distinction between the agent and the simulated world, the agent's sensing and effecting capabilities defining the interface.
Reference: [Chrisman et al. 1991] <author> Lonnie Chrisman, Rich Caruana, and Wayne Carriker. </author> <title> Intelligent Agent Design Issues: Internal Agent State and Incomplete Perception. </title> <booktitle> In AAAI Fall Symposium Series: Sensory Aspects of Robotic Intelligence, </booktitle> <year> 1991. </year>
Reference: [Cohen et al. 1990] <author> Paul R. Cohen, Adele E. Howe, and David M. Hart. </author> <title> Intelligent Real-Time Problem Solving: Issues and Examples. </title> <editor> In L. Erman, editor, </editor> <title> Intelligent Real-Time Problem Solving: </title> <booktitle> Workshop Report, </booktitle> <address> Palo Alto, CA, </address> <year> 1990. </year> <institution> Cimflex Teknowledge Corp. </institution>
Reference: [Cohen 1991] <author> Paul R. Cohen. </author> <booktitle> A survey of the Eighth National Conference on Artificial Intelligence: Pulling together or pulling apart? AI Magazine, </booktitle> <volume> 12 </volume> <pages> 16-41, </pages> <year> 1991. </year>
Reference-contexts: Research into agent design begins with a theory. Of course, the theory, in whole or in part, may be informed by the theorist's previous experiences building large, interesting systems. An experimental research program on agent design includes the following components (cf. Cohen's "MAD" methodology <ref> [Cohen 1991] </ref>). * A theory T , describing some aspect (s) of agent design and the purported effect of those design aspects on agent behavior in certain environments, particularly describing the the agent's architecture, the environment, and the agent's behavior. 10 * An implemented testbed environment E, and a description of
Reference: [Dean and Boddy 1988] <author> Tom Dean and Mark Boddy. </author> <title> An Analysis of Time-Dependent Planning. </title> <booktitle> In Proceedings AAAI. AAAI, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: of planning becomes important in a world that allows unplanned changes: the longer the agent takes to plan, the more likely it is that the world has changed significantly between the time the plan was generated and the time it is executed [Bratman et al. 1988], [Russell and Wefald 1991], <ref> [Dean and Boddy 1988] </ref>. Complexity of the World A realistic world has many features. Even a simple block has color, mass, texture, smell, and so on, although many of these features will be irrelevant to many tasks.
Reference: [Firby and Hanks 1987] <author> R. James Firby and Steve Hanks. </author> <title> A Simulator for Mobile Robot Planning. </title> <booktitle> In Proceedings of the DARPA Knowledge-Based Planning Workshop, </booktitle> <pages> pages 23-1, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The simulator maintains the map and schedules activities, but, like MICE, much of the domain's physics lies in definitions of the individual tasks. 4.3 The Truckworld The Truckworld <ref> [Firby and Hanks 1987] </ref> [Hanks et al. 1992] is a multiagent testbed designed to test theories of reactive execution [Firby 1989] and to provide motivating examples for a theory of reasoning about dynamic and uncertain worlds [Hanks and McDermott 1993], [Hanks and McDermott 1992].
Reference: [Firby 1989] <author> R. James Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> Technical report, </type> <institution> Department of Computer Science, Yale University, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: Relaxing this assumption makes the process of predicting the effects of plans more difficult [Hanks 1990b] and also introduces the need to react to unplanned events as they occur at execution time [Agre and Chapman 1987], <ref> [Firby 1989] </ref>. <p> The simulator maintains the map and schedules activities, but, like MICE, much of the domain's physics lies in definitions of the individual tasks. 4.3 The Truckworld The Truckworld [Firby and Hanks 1987] [Hanks et al. 1992] is a multiagent testbed designed to test theories of reactive execution <ref> [Firby 1989] </ref> and to provide motivating examples for a theory of reasoning about dynamic and uncertain worlds [Hanks and McDermott 1993], [Hanks and McDermott 1992]. The main commitment is to provide a realistic world for its agents, but without physical sensors or effectors 2 .
Reference: [Greenberg and Westbrook 1990] <author> Michael Greenberg and David L. Westbrook. </author> <title> The Phoenix Testbed. </title> <type> Technical Report COINS TR 90-19, </type> <institution> Computer and Information Science, University of Massachusetts, </institution> <year> 1990. </year>
Reference-contexts: See [Montgomery and Durfee 1990], for example.) 4.2 The Phoenix testbed Phoenix [Hart and Cohen 1990] <ref> [Greenberg and Westbrook 1990] </ref> is a framework for implementing and testing multiple autonomous agents in a complex environment. The scenario is fire fighting: the world consists of a map with varying terrain, elevations, and weather. Fires can start at any location and will spread depending on the surrounding terrain.
Reference: [Haddawy and Hanks 1993] <author> P. Haddawy and S. Hanks. </author> <title> Utility Models for Goals-Directed Decision-Theoretic Planners. </title> <journal> Artificial Intelligence, </journal> <note> 1993. Submitted. 45 </note>
Reference-contexts: But simple achievement of a goal state is an inadequate measure of success: it does not take into account the cost of achieving the goal, and it also does not admit the possibility of partial goal satisfaction. <ref> [Haddawy and Hanks 1993] </ref> and [Wellman and Doyle 1991] explore the relationship between goal expressions and utility functions.
Reference: [Hanks and Badr 1991] <author> Steve Hanks and Badr Al Badr. </author> <title> Critiquing the Tileworld: Agent Architectures, Planning Benchmarks, and Experimental Methodology. </title> <type> Technical Report 91-10-31, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The filter-override mechanism at best has no effect on the agent's performance, and in some cases makes it perform worse. <ref> [Hanks and Badr 1991] </ref> analyzes these experiments in detail; here I want to discuss some issues relevant to the question of what this experimental paradigm can be expected to accomplish. <p> The second result, suggesting that the LV deliberator performs better than the HS delib-erator, must also be interpreted in the context of the particular implementation. We note in <ref> [Hanks and Badr 1991] </ref> that one part of the Tileworld agent is the path-planning algorithm that (1) solves the problem optimally, (2) is not subject to experimental variation, and (3) is written in C, presumably for efficiency reasons. <p> To what extent do the experimental results depend on the ability to solve the path-planning subproblem quickly and optimally? <ref> [Hanks and Badr 1991] </ref> shows that the fast path planner has much more of an effect on the system's performance than does variation in the deliberator (which was one of the parameters varied experimentally). <p> The problem with Tileworld is that there is very little to predict. Tiles appear and disappear randomly, and with no pattern. The effects of the agent's actions are localized. On balance there is little to be gained from thinking hard about the world, which we demonstrate in <ref> [Hanks and Badr 1991] </ref> by demonstrating that there is little benefit to be had even by implementing a deliberator that computes the agent's optimal course of action given present information.
Reference: [Hanks and McDermott 1992] <author> Steve Hanks and Drew McDermott. </author> <title> Modeling a Dynamic and Uncertain World II: Projecting Courses of Action. </title> <note> In Preparation, </note> <year> 1992. </year>
Reference-contexts: definitions of the individual tasks. 4.3 The Truckworld The Truckworld [Firby and Hanks 1987] [Hanks et al. 1992] is a multiagent testbed designed to test theories of reactive execution [Firby 1989] and to provide motivating examples for a theory of reasoning about dynamic and uncertain worlds [Hanks and McDermott 1993], <ref> [Hanks and McDermott 1992] </ref>. The main commitment is to provide a realistic world for its agents, but without physical sensors or effectors 2 .
Reference: [Hanks and McDermott 1993] <author> Steve Hanks and Drew McDermott. </author> <title> Modeling a Dynamic and Uncertain World I: Symbolic and Probabilistic Reasoning about Change. </title> <journal> Artificial Intelligence, </journal> <note> 1993. To appear. </note>
Reference-contexts: Quality and Cost of Sensing and Effecting Sensing and effecting, generally ignored by the classical planners, are neither perfect nor cost free. An agent must therefore incorporate incorrect and noisy sensor reports into its predictive model of the world <ref> [Hanks and McDermott 1993] </ref> and must plan sensing actions to improve its state of information, taking into account both the benefit of that information and the cost of acquiring it [Chrisman and Simmons 1991]. <p> domain's physics lies in definitions of the individual tasks. 4.3 The Truckworld The Truckworld [Firby and Hanks 1987] [Hanks et al. 1992] is a multiagent testbed designed to test theories of reactive execution [Firby 1989] and to provide motivating examples for a theory of reasoning about dynamic and uncertain worlds <ref> [Hanks and McDermott 1993] </ref>, [Hanks and McDermott 1992]. The main commitment is to provide a realistic world for its agents, but without physical sensors or effectors 2 .
Reference: [Hanks et al. 1992] <author> Steve Hanks, Dat Nguyen, and Chris Thomas. </author> <title> The New Truckworld Manual. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1992. </year> <month> Forthcoming. </month>
Reference-contexts: The simulator maintains the map and schedules activities, but, like MICE, much of the domain's physics lies in definitions of the individual tasks. 4.3 The Truckworld The Truckworld [Firby and Hanks 1987] <ref> [Hanks et al. 1992] </ref> is a multiagent testbed designed to test theories of reactive execution [Firby 1989] and to provide motivating examples for a theory of reasoning about dynamic and uncertain worlds [Hanks and McDermott 1993], [Hanks and McDermott 1992].
Reference: [Hanks 1990a] <author> Steve Hanks. </author> <title> Practical Temporal Projection. </title> <booktitle> In Proceedings AAAI, </booktitle> <year> 1990. </year>
Reference-contexts: Reasoning about more realistic models of the world requires the ability to represent and make predictions about complex mechanisms [Weld and de Kleer 1989], as well as the ability to recognize and focus attention on those aspects of the world relevant to the problem at hand <ref> [Hanks 1990a] </ref>. A testbed for exploring realistically complex planning problems should itself provide a complexity and diversity of features. Quality and Cost of Sensing and Effecting Sensing and effecting, generally ignored by the classical planners, are neither perfect nor cost free.
Reference: [Hanks 1990b] <author> Steve Hanks. </author> <title> Projecting Plans for Uncertain Worlds. </title> <type> Technical Report 756, </type> <institution> Department of Computer Science, Yale University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Exogenous Events Perhaps the most limiting assumption of the classical planning worlds (most notably the Blocksworld) is that no exogenous, or unplanned, events can occur. Relaxing this assumption makes the process of predicting the effects of plans more difficult <ref> [Hanks 1990b] </ref> and also introduces the need to react to unplanned events as they occur at execution time [Agre and Chapman 1987], [Firby 1989]. <p> Working on a real-world problem has obvious benefits, but to understand the systems that we build we must isolate attributes and carry out systematic experimentation. My own work leads me to believe that it will be quite difficult to isolate particular aspects of a large planning problem. <ref> [Hanks 1990b] </ref>, for example, confronts the problem of reasoning about plans in an uncertain world.
Reference: [Hart and Cohen 1990] <author> David M. Hart and Paul R. Cohen. </author> <title> Phoenix: A Testbed for Shared Planning Research, June 1990. </title> <booktitle> Proceedings of the NASA/AMES Workshop on Benchmarks and Metrics. </booktitle>
Reference-contexts: MICE might 12 be viewed more as a framework for building testbeds rather than a simulator in and of itself. (The MICE designers have built versions of Tileworld and of Phoenix using this platform. See [Montgomery and Durfee 1990], for example.) 4.2 The Phoenix testbed Phoenix <ref> [Hart and Cohen 1990] </ref> [Greenberg and Westbrook 1990] is a framework for implementing and testing multiple autonomous agents in a complex environment. The scenario is fire fighting: the world consists of a map with varying terrain, elevations, and weather.
Reference: [Hendler and Kinny 1992] <author> James Hendler and David Kinny. </author> <title> Empirical Experiments in Selective Sensing with Non-Zero-Cost Sensors. </title> <type> Technical report, </type> <institution> University of Maryland, </institution> <year> 1992. </year>
Reference: [Kinny and Georgeff 1991] <author> David N. Kinny and Michael P. Georgeff. </author> <title> Commitment and Effectiveness of Situated Agents. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 82-88, </pages> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: The probabilities are independent of one another: a single probability governs the appearance of tiles, and it is the same regardless 1 The scoring metric in Tileworld was later revised to make it easier to compare trials of varying length: raw score is replaced with a normalized value called efficiency <ref> [Kinny and Georgeff 1991] </ref>. A number of changes have been made to the Tileworld system since 1990, some of which are discussed in Section 5.2; see also [Pollack et al. 1993]. <p> This section will discuss a particular methodological approach, which I will call "experimentation in the small." [Langley and Drummond 1990] advocate this position in the abstract. [Pollack and Ringuette 1990] and <ref> [Kinny and Georgeff 1991] </ref> explore it concretely using an implemented testbed and suite of experiments. I take the methodological commitments of this approach to be the following: 1. <p> The experiments themselves do not provide guidance in this task, and may even tend to hinder it. I will use the Tileworld testbed and experiments from [Pollack and Ringuette 1990] and <ref> [Kinny and Georgeff 1991] </ref> to make these points. My goal in doing so is not to single out this particular work for criticism. <p> environment. 22 5.1.2 Subsequent tileworld experiments The experiments in [Pollack and Ringuette 1990] tried to establish a relationship between the agent's commitment to its current plan|its willingness to abandon its current goal to consider a new option, or "boldness" as it was called|and the rate at which the world changes. <ref> [Kinny and Georgeff 1991] </ref> try to make this relationship precise and provide additional empirical support. They begin their experimental inquiry by further simplifying the testbed world: [The Tileworld] was considered too rich for the investigative experiments we had planned. <p> They begin their experimental inquiry by further simplifying the testbed world: [The Tileworld] was considered too rich for the investigative experiments we had planned. Therefore, to reduce the complexity of the object-level reasoning required of our agent, we employed a simplified Tileworld with no tiles. <ref> [Kinny and Georgeff 1991, p. 83] </ref> The agent's task in this simplified Tileworld is to move itself to a hole on the board, at which point it is awarded the hole's score. The agent is provided with perfect, immediate, and cost-free information about the world's present state. <p> These general terms are supposed to suggest an agent's willingness to reassess its plan commitments as it executes its plans: a "bold" agent rarely reconsiders its plans, a "cautious" agent does so frequently. 5 The two main results from <ref> [Kinny and Georgeff 1991] </ref> can be stated as follows: First, that it's a good policy for an agent to be more cautious as the world changes more rapidly. <p> An experimenter using these worlds therefore runs the risk of solving problems in a way that cannot be extended to more realistic worlds, and at the same time making his job artificially difficult for having studied the problem in isolation. <ref> [Kinny and Georgeff 1991, p. 82] </ref> states that "[Simulated worlds] should ideally capture the essential features of real-world domains while permitting flexible, accurate, and reproducible control of the world's characteristics." An appealing proposition, but the fact is we don't know what it means to "capture the essential features of real-world domains,"
Reference: [Kinny 1990] <author> David N. Kinny. </author> <title> Measuring the Effectiveness of Situated Agents. </title> <type> Technical Report 11, </type> <institution> Australian AI Institute, </institution> <address> Carlton, Australia, </address> <year> 1990. </year>
Reference: [Langley and Drummond 1990] <author> Pat Langley and Mark Drummond. </author> <title> Toward an Experimental Science of Planning. In Workshop on Innovative Approaches to Planning, Scheduling, and Control. </title> <booktitle> DARPA, </booktitle> <month> November </month> <year> 1990. </year> <month> 46 </month>
Reference-contexts: A testbed should therefore provide a convenient way for the experimenter to vary the behavior of the worlds in which the agent is to be tested. The experimenter must also be able to monitor the agent's behavior in the testbed world <ref> [Langley and Drummond 1990] </ref>. While it is far from clear at this point what statistics should be used in such an assessment, the testbed must allow performance statistics to be gathered. <p> This section will discuss a particular methodological approach, which I will call "experimentation in the small." <ref> [Langley and Drummond 1990] </ref> advocate this position in the abstract. [Pollack and Ringuette 1990] and [Kinny and Georgeff 1991] explore it concretely using an implemented testbed and suite of experiments. I take the methodological commitments of this approach to be the following: 1.
Reference: [Law and Kelton 1981] <editor> Averill Law and W. David Kelton. </editor> <booktitle> Simulation Modeling and Anal--ysis. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: A Well Defined Model of Time Testbeds must present a reasonable model of passing time in order to simulate exogenous events and simultaneous action, and to define 8 clearly the time cost of reasoning and acting. (This is a general problem in simulation and modeling. See <ref> [Law and Kelton 1981] </ref>, for example.) On the other hand the testbed must somehow be able to communicate how much "simulated time" has elapsed. Making sense of experimental results requires a way to reconcile the testbed's measure of time with that used by the agent.
Reference: [McDermott 1981] <author> Drew McDermott. </author> <booktitle> Artificial Intelligence Meets Natural Stupidity. </booktitle> <editor> In John Haugland, editor, </editor> <booktitle> Mind Design: Essays in Philosophy, Psychology, and Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: The real contribution of such an analysis would be to come up with the right way of characterizing the agent, the world, and their relationship|in terms that are not so specific as to be ap 6 Cf. <ref> [McDermott 1981] </ref> 26 plicable only to the experimental domain but not so vague as to be vacuously true.
Reference: [Minton et al. 1990] <author> Steven Minton, Mark D. Johnston, Andrew B. Philips, and Philip Laird. </author> <title> Solving Large-Scale Constraint Satisfaction and Scheduling Problems Using a Heuristic Repair Method. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 17-24, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Likewise, as the potential user of an AI search algorithm we may be impressed with the performance of the min-conflicts heuristic algorithm on the Million Queens problem <ref> [Minton et al. 1990] </ref>. As scientists and engineers, however, our interests are different. In these roles, we want to understand why a system behaves the way it does.
Reference: [Montgomery and Durfee 1990] <author> Thomas A. Montgomery and Edmund H. Durfee. </author> <title> Using MICE to Study Intelligent Dynamic Coordination. </title> <booktitle> In Second International Conference on Tools for Artificial Intelligence, </booktitle> <pages> pages 438-444. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: The agent is given no indication of whether an operator has succeeded or failed, and must explicitly sense the world in order to ascertain the effects of its actions. MICE <ref> [Montgomery and Durfee 1990] </ref>, [Montgomery et al. 1992] is another grid-oriented simulator, designed to support research into coordinating the problem-solving behavior of multiple autonomous agents. <p> MICE might 12 be viewed more as a framework for building testbeds rather than a simulator in and of itself. (The MICE designers have built versions of Tileworld and of Phoenix using this platform. See <ref> [Montgomery and Durfee 1990] </ref>, for example.) 4.2 The Phoenix testbed Phoenix [Hart and Cohen 1990] [Greenberg and Westbrook 1990] is a framework for implementing and testing multiple autonomous agents in a complex environment. The scenario is fire fighting: the world consists of a map with varying terrain, elevations, and weather.
Reference: [Montgomery et al. 1992] <author> Thomas A. Montgomery, Jaeho Lee, et al. </author> <title> MICE Users Guide. </title> <type> Technical report, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: We will first discuss the Tileworld of Pollack and Ringuette [1990], then the independently developed NASA Tileworld (NTW) [Philips and Bresina 1991], and the MICE simulator <ref> [Montgomery et al. 1992] </ref> [Pollack and Ringuette 1990] reports on the Tileworld testbed|a system designed to sup-port controlled experiments with agent architectures situated in dynamic and unpredictable environments. The world consists of a rectangular grid, on which can be placed the agent, some tiles, some obstacles, and some holes. <p> The agent is given no indication of whether an operator has succeeded or failed, and must explicitly sense the world in order to ascertain the effects of its actions. MICE [Montgomery and Durfee 1990], <ref> [Montgomery et al. 1992] </ref> is another grid-oriented simulator, designed to support research into coordinating the problem-solving behavior of multiple autonomous agents. The basic layout of MICE consists only of a grid and various agents, though agents can be used to simulate objects like tiles and forest fires.
Reference: [Moore and McCabe 1989] <author> David S. Moore and George P. McCabe. </author> <title> Introduction to the Practice of Statistics. W.H. </title> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [Oh 1991] <author> John Oh. </author> <title> A Study of Filtering and Deliberation Strategies in Tileworld. </title> <type> Unpublished manuscript., </type> <year> 1991. </year>
Reference-contexts: In other words, commitment to one's plans can be a valuable strategy for managing a changing environment. Experimentation also suggested needed modifications to the theory. For example, Oh observed that the agent's performance is hindered by its inability to immediately adopt certain extremely promising options without deliberation <ref> [Oh 1991] </ref>. The original theory included a mechanism for short-circuiting deliberation to eliminate a new option, but it lacked a mechanism for short-circuiting deliberation to immediately adopt a new option.
Reference: [Philips and Bresina 1991] <author> Andrew B. Philips and John L. Bresina. </author> <title> NASA TileWorld Manual. </title> <type> Technical Report TR-FIA-91-04, </type> <institution> NASA Ames Research Center, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: We will first discuss the Tileworld of Pollack and Ringuette [1990], then the independently developed NASA Tileworld (NTW) <ref> [Philips and Bresina 1991] </ref>, and the MICE simulator [Montgomery et al. 1992] [Pollack and Ringuette 1990] reports on the Tileworld testbed|a system designed to sup-port controlled experiments with agent architectures situated in dynamic and unpredictable environments. <p> Incomplete or noisy sensing can be achieved by manipulating this data before the agent is allowed to use it. Similarly, imprecision in effecting change has to be specified within the agent itself. The NASA Tileworld NTW <ref> [Philips and Bresina 1991] </ref>, [Philips et al. 1991] is an independently developed testbed also organized around the theme of a two-dimensional grid with tiles. Exogenous events in the NTW consist of winds that can blow tiles across the grid. NTW has no obstacles or holes.
Reference: [Philips et al. 1991] <author> Andrew Philips, Keith J. Swanson, Mark E. Drummond, and John L. Bresina. </author> <title> The NASA TileWorld Simulator (Instantiating Key Domain Attributes While Discarding Irrelevant Semantic Baggage), </title> <booktitle> 1991. Position paper for the AAAI-91 Workshop on Real-Time AI. </booktitle>
Reference-contexts: Incomplete or noisy sensing can be achieved by manipulating this data before the agent is allowed to use it. Similarly, imprecision in effecting change has to be specified within the agent itself. The NASA Tileworld NTW [Philips and Bresina 1991], <ref> [Philips et al. 1991] </ref> is an independently developed testbed also organized around the theme of a two-dimensional grid with tiles. Exogenous events in the NTW consist of winds that can blow tiles across the grid. NTW has no obstacles or holes. Two features distinguish the two simulators. <p> This "scaling" assumption is absolutely crucial to the whole experimental paradigm, and I have not seen it defended in the literature. In fact the only explicit mention of the assumption I have found appears in <ref> [Philips et al. 1991] </ref>: We are not suggesting that studies of these attributes in isolation are sufficient to guarantee the obvious goals of good methodology, brilliant architectures, or first-class results; however, we are suggesting that such isolation facilitates the achievement of such goals.
Reference: [Pollack and Ringuette 1990] <author> Martha E. Pollack and Marc Ringuette. </author> <title> Introducing the Tile-world: Experimentally Evaluating Agent Architectures. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-189, </pages> <address> Boston, MA, </address> <year> 1990. </year> <month> 47 </month>
Reference-contexts: We will first discuss the Tileworld of Pollack and Ringuette [1990], then the independently developed NASA Tileworld (NTW) [Philips and Bresina 1991], and the MICE simulator [Montgomery et al. 1992] <ref> [Pollack and Ringuette 1990] </ref> reports on the Tileworld testbed|a system designed to sup-port controlled experiments with agent architectures situated in dynamic and unpredictable environments. The world consists of a rectangular grid, on which can be placed the agent, some tiles, some obstacles, and some holes. <p> This section will discuss a particular methodological approach, which I will call "experimentation in the small." [Langley and Drummond 1990] advocate this position in the abstract. <ref> [Pollack and Ringuette 1990] </ref> and [Kinny and Georgeff 1991] explore it concretely using an implemented testbed and suite of experiments. I take the methodological commitments of this approach to be the following: 1. <p> The experiments themselves do not provide guidance in this task, and may even tend to hinder it. I will use the Tileworld testbed and experiments from <ref> [Pollack and Ringuette 1990] </ref> and [Kinny and Georgeff 1991] to make these points. My goal in doing so is not to single out this particular work for criticism. <p> I do so first because it's important to discuss the concrete results that can be expected from these experimental endeavors, and these two pieces of work are rare examples of systematic experimentation with agent architectures in small, controlled worlds. 5.1.1 The original tileworld experiments The planning agent studied in <ref> [Pollack and Ringuette 1990] </ref> is an implementation of the IRMA architecture [Bratman et al. 1988]. <p> The point of this discussion is to demonstrate the difficulty of interpreting experimental results such as those reported in <ref> [Pollack and Ringuette 1990] </ref>, or more specifically the difficulty associated with applying the results to any circumstances other than those under which the experiments were conducted. <p> Below I will discuss the implications to the general paradigm of experimentation in the small, but first I want to discuss some follow-up experiments in the Tileworld environment. 22 5.1.2 Subsequent tileworld experiments The experiments in <ref> [Pollack and Ringuette 1990] </ref> tried to establish a relationship between the agent's commitment to its current plan|its willingness to abandon its current goal to consider a new option, or "boldness" as it was called|and the rate at which the world changes. [Kinny and Georgeff 1991] try to make this relationship precise <p> These lessons have led us to make a number of changes and extensions to the oritinal system reported on in <ref> [Pollack and Ringuette 1990] </ref>, some of which I will mention below. Our initial goal in building the Tileworld was to study a particular, well-developed theory of resource-limited reasoning, called IRMA (the Intelligent Resource-Limited Machine Architecture), that we had previously developed [Bratman et al. 1988,Bratman 1987,Pol-lack 1991]. <p> The environmental condition that we suspected would be most important was average rate of change in the environment. Details are be found in <ref> [Pollack and Ringuette 1990] </ref>; this brief sketch is meant to highlight the fact that, underlying our attempt to relate S (in this case, conditions on filtering), E (average rate of change), and B (the agent's overall performance), was a larger theory about the role of intentions in resource-limited reasoning. <p> This fact limited the range of experiments we could conduct: there was no way to explore the behavior of agents who had to perform complex (and thus, computationally costly) plan generation. We have, since the publication of <ref> [Pollack and Ringuette 1990] </ref>, increased the complexity of the Tileworld environment, so that we can study situations in which a wider range of options are presented to the agent. * The experiments also suggested needed changes to the agent embedded in the Tile-world environment. <p> Both processes were uniformly inexpensive, and we were thus unable adequately to explore the advantages of the filtering process, whose intent is to cut down on the amount of deliberation and planning needed <ref> [Pollack and Ringuette 1990] </ref>. This limitation led us subsequently to increase the complexity of the deliberation process.
Reference: [Pollack et al. 1993] <author> Martha E. Pollack, David Joslin, Arthur Nunes, and Sigalit Ur. </author> <title> Exper--imental investigation of an agent design strategy. </title> <note> In preparation., </note> <year> 1993. </year>
Reference-contexts: A number of changes have been made to the Tileworld system since 1990, some of which are discussed in Section 5.2; see also <ref> [Pollack et al. 1993] </ref>. Code and documenation for the Tileworld is available by sending mail to tileworld-request@cs.pitt.edu. 10 of the time, of the location, or of any other parameter in the game. The Tileworld has no explicit sensing operators.
Reference: [Pollack 1991] <author> Martha E. Pollack. </author> <title> Overloading Intentions for Efficient Practical Reasoning. </title> <journal> No^us, </journal> <volume> 25(4) </volume> <pages> 513-536, </pages> <year> 1991. </year>
Reference: [Pollack 1992] <author> Martha E. Pollack. </author> <title> The Uses of Plans. </title> <journal> Artificial Intelligence, </journal> <volume> 57(1) </volume> <pages> 43-69, </pages> <year> 1992. </year>
Reference-contexts: For the experimental program to succeed in AI, AI researchers will need to be more scrupulous about careful theory development; as I have claimed elsewhere <ref> [Pollack 1992] </ref>, our field has not always valued theory development as an integral part of our work. Research into agent design begins with a theory. Of course, the theory, in whole or in part, may be informed by the theorist's previous experiences building large, interesting systems.
Reference: [Powers 1991] <author> Richard Powers. </author> <title> The Gold Bug Variations. </title> <publisher> William Morrow and Company, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The success of reduc tionism depends on measuring and reporting only that bit of cloth that can be understood and tested piecemeal. <ref> [Powers 1991, p.355] </ref>." Experimentation mandates simplification. In investigating a complex phenomenon, the experimenter selectively attends to some aspects of it, namely, those that she believes are relevant to her hypotheses. She exerts control over those aspects of the phenomenon, manipulating them as necessary to test her hypotheses.
Reference: [Rosenschein et al. 1990] <author> Stanley J. Rosenschein, Barbara Hayes-Roth, and Lee D. Erman. </author> <title> Notes on Methodologies for Evaluating IRTPS Systems. </title> <editor> In L. Erman, editor, </editor> <title> Intelligent Real-Time Problem Solving: </title> <booktitle> Workshop Report, </booktitle> <address> Palo Alto, CA, </address> <year> 1990. </year> <institution> Cimflex Teknowledge Corp. </institution>
Reference: [Russell and Wefald 1991] <author> Stuart J. Russell and Eric H. Wefald. </author> <title> Do the Right Thing: Studies in Limited Rationality. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The time cost of planning becomes important in a world that allows unplanned changes: the longer the agent takes to plan, the more likely it is that the world has changed significantly between the time the plan was generated and the time it is executed [Bratman et al. 1988], <ref> [Russell and Wefald 1991] </ref>, [Dean and Boddy 1988]. Complexity of the World A realistic world has many features. Even a simple block has color, mass, texture, smell, and so on, although many of these features will be irrelevant to many tasks.
Reference: [Schoppers 1987] <author> Marcel J. Schoppers. </author> <title> Universal Plans for Reactive Robots in Unpredictable Environments. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference: [Sussman 1975] <author> Gerald J. Sussman. </author> <title> A Computer Model of Skill Acquisition. </title> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: An early benchmark task for AI planning programs was the Sussman anomaly (the "Three Block Problem") <ref> [Sussman 1975] </ref>. The Sussman anomaly helped many researchers elucidate how their planners worked. It was popular because, like matrix multiplication, it was representative of an important class of problems, those involving interactions among conjunctive subgoals, and it was very easy to describe.
Reference: [Weld and de Kleer 1989] <author> Daniel S. Weld and Johan de Kleer, </author> <title> editors. Readings in Qualitative Reasoning about Physical Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Reasoning about more realistic models of the world requires the ability to represent and make predictions about complex mechanisms <ref> [Weld and de Kleer 1989] </ref>, as well as the ability to recognize and focus attention on those aspects of the world relevant to the problem at hand [Hanks 1990a]. A testbed for exploring realistically complex planning problems should itself provide a complexity and diversity of features.
Reference: [Wellman and Doyle 1991] <author> Michael P. Wellman and Jon Doyle. </author> <title> Preferential Semantics for Goals. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 698-703, </pages> <address> Anaheim, CA, </address> <year> 1991. </year> <month> 48 </month>
Reference-contexts: But simple achievement of a goal state is an inadequate measure of success: it does not take into account the cost of achieving the goal, and it also does not admit the possibility of partial goal satisfaction. [Haddawy and Hanks 1993] and <ref> [Wellman and Doyle 1991] </ref> explore the relationship between goal expressions and utility functions.
References-found: 46

