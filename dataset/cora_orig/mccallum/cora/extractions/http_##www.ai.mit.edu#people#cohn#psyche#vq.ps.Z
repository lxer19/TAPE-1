URL: http://www.ai.mit.edu/people/cohn/psyche/vq.ps.Z
Refering-URL: http://www.ai.mit.edu/people/cohn/psyche/
Root-URL: 
Title: Theory and Practice of Vector Quantizers Trained on Small Training Sets  
Author: David Cohn Eve A. Riskin Richard Ladner 
Address: Seattle, WA 98195  Seattle, WA 98195  Seattle, WA 98195  
Affiliation: Computer Science Eng. FR-35, Univ. of Washington  Electrical Engineering FT-10, Univ. of Washington  Computer Science Eng. FR-35, Univ. of Washington  
Abstract: We examine how the performance of a memoryless vector quantizer changes as a function of its training set size. Specifically, we study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image(s). Using the Vapnik-Chervonenkis dimension, we derive formal bounds for the difference of test and training distortion of vector quantizer codebooks. We then describe extensive empirical simulations that test these bounds for a variety of bit rates and vector dimensions, and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook. We conclude that, by using training sets comprised of only a small fraction of the available data, one can produce results that are close to the results obtainable when all available data are used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 2 2.1 Pattern classification and the VC-dimension The results of <ref> [1, 16, 17] </ref> concern the asymptotic performance of learning systems. Specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed. <p> The VC-dimension of the class is the size m of the largest set S m that can be shattered by C. For example, the VC-dimension of the class of balls in k-dimensional Euclidean space is 3k <ref> [1] </ref>. Given the empirical error *(c; c fl ; P m ), for c selected from a class with VC-dimension d, the theorems of Vapnik and Chervonenkis bound the probability that the *(c; c fl ; P) will exceed some value. <p> A detailed description of the VC-dimension and its use in formal learning theory is beyond the scope of this paper, but may be found in <ref> [1, 16] </ref>. 3 2.2 Framing VQ as a Classification Problem The bound in the previous subsection is useful because it lets us predict generalized performance based on observed performance. <p> The simplest upper bound we can place on the VC-dimension of a binary VQ codebook class is derived by a combinatorial argument from <ref> [1] </ref>. In order to shatter m blocks, there must be a codebook that encodes correctly each of the 2 m subsets of those blocks. This requires that the class include at least 2 m distinct codebooks.
Reference: [2] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds? Neural Computation, </title> <booktitle> 4(2) </booktitle> <pages> 249-270, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Pollard ([14]) has shown that, under certain strong conditions, the expected value of (test train) will decrease as O (1=m) when the codebook is designed using an optimal k-means clustering algorithm. Empirical work with artificial neural networks (e.g. <ref> [2] </ref>) has observed this behavior in cases where VC-dimension theory predicts a worst case of Equation 3. Below, we examine (test train) empirically as it changes with training set size (m), block size (k), and codebook size (N ) for typical images. <p> can bound its generalization error as: *(c; c fl ; P) m 2m + 1) m If, however, we know that the classifier follows a Bayes-optimal decision rule, then we can show that the expected worst-case behavior is no greater than *(c; c fl ; P) m Empirical studies in <ref> [2] </ref> have shown that even for some very simple zero-training-error learning problems, expected generalization may be close to the latter, Bayes-optimal bound.
Reference: [3] <author> D. Cohn. </author> <title> Separating formal bounds from practical performance in learning systems. </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1992. </year>
Reference-contexts: It is not difficult to find extreme cases where this relationship breaks down, but it holds well for all "typical" cases we have examined, and allows conversion between the non-replacement approach and the theoretically examined approach of sampling with replacement (for a more detailed treatment of this relationship, see <ref> [3] </ref>). 3.2 Relation to worst-case bounds For the single-image learning experiments, we examined both binary and grayscale images from three sources: photographic images from the USC database, MRI brain scans, and computer-generated line drawings.
Reference: [4] <author> P. Cosman, K. Perlmutter, S. Perlmutter, R. A. Olshen, and R. M. Gray. </author> <title> Training sequence size and vector quantizer performance. </title> <booktitle> In Proceedings of 25th Asilomar Conference on Signals, Systems, and Computers, </booktitle> <pages> pages 434-438, </pages> <address> Asilomar, CA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: If the training is for a set of 10 images rather than a single image, then again only 5,000 vectors are needed in the training set, which now represents only 3% of potential training vectors. The problem of training set size has also been studied independently at Stanford University <ref> [4] </ref> with different but consistent results. It has also been considered from an information-theoretic viewpoint by David Pollard [14]. We summarize the remainder of the paper as follows. Section 2 provides a brief introduction to vector quantization and details the derivation of bounds on training set sizes for VQ codebooks.
Reference: [5] <author> J. Crutchfield and K. Young. </author> <booktitle> Computation at the Onset of Chaos, </booktitle> <pages> pages 223-269. </pages> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [6] <author> R. Floyd and L. Steinberg. </author> <title> An adaptive algorithm for spatial grey scale. </title> <booktitle> SID Int. Sym. Digest of Tech. Papers, </booktitle> <pages> pages 36-37, </pages> <year> 1975. </year> <month> 18 </month>
Reference: [7] <author> A. Gersho and R. M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Vector quantization (VQ) <ref> [7, 8] </ref> is a data compression technique that can be used to reduce the storage or transmission costs of binary and grayscale images. It is lossy in that the compressed/uncompressed image is a degraded copy of the original image.
Reference: [8] <author> R. M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 1 </volume> <pages> 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Vector quantization (VQ) <ref> [7, 8] </ref> is a data compression technique that can be used to reduce the storage or transmission costs of binary and grayscale images. It is lossy in that the compressed/uncompressed image is a degraded copy of the original image.
Reference: [9] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Unifying bounds on the sample complexity of Bayesian learning theory using information theory and the VC dimension. </title> <booktitle> In Proceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We then conclude by briefly recapitulating the results of this paper and suggesting directions for future research. 5.1 Implications of empirical results for theory The bounds derived in Section 2 are worst-case bounds, which hold regardless of the input distribution and codebook-design algorithm. It has been shown in <ref> [9] </ref> that under certain circumstances, much tighter bounds may be derived if something is known about the design algorithm. Consider the case of an arbitrary classifier that can achieve zero training error on m training examples.
Reference: [10] <author> F. Itakura and S. Saito. </author> <title> Analysis synthesis telephony based on the maximum likelihood method. </title> <booktitle> In Proceedings of the 6th International Congress on Acoustics, pages c17-c20, </booktitle> <address> Tokyo, Japan, 1968. </address> <publisher> Elsevier Publishing, </publisher> <address> New York. </address>
Reference-contexts: Typical distortion measures are the mean-squared error, the weighted mean-squared error, and the Itakura-Saito distortion (for speech) <ref> [10] </ref>. Below, we first introduce the pattern classification problem and formal bounds that have been derived for it using the Vapnik-Chervonenkis dimension.
Reference: [11] <author> J. Lin and J. Vitter. </author> <title> *-approximations with minimum constraint violation. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 771-782, </pages> <address> Victoria, Canada, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This would provide an exact bound on worst-case learning complexity. 2. We have completely ignored the implications of this work for achieving minimum distortion for a fixed bit rate (or minimum bit rate for fixed distortion), given a fixed allowable training time. Recent work described in <ref> [11] </ref> and elsewhere, addresses the problem of minimizing training distortion. From a bound on training distortion, with the work here bounding the difference (test train), it should be possible to directly bound the test distortion of an image as a function of its codebook training set size. 3.
Reference: [12] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 28 </volume> <pages> 84-95, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: Given an image (or set of images), a fixed block size, and a fixed codebook size, iterative algorithms such as the Generalized Lloyd Algorithm (GLA) select codebook vectors which locally optimize some image degradation measure <ref> [12] </ref>. Typical distortion measures are the mean-squared error, the weighted mean-squared error, and the Itakura-Saito distortion (for speech) [10]. Below, we first introduce the pattern classification problem and formal bounds that have been derived for it using the Vapnik-Chervonenkis dimension. <p> We denote the total number of blocks in the image as M . From these M blocks, we select a training set S m of m blocks by random sampling with replacement. This training set is used as input to a program running the Generalized Lloyd Algorithm (described in <ref> [12] </ref>). The output of the program is a codebook with (locally) minimal distortion on the training set. We denote this codebook as (S m ).
Reference: [13] <author> A. N. Netravali and B. G. </author> <title> Haskell. Digital Pictures Representation and Compression. </title> <publisher> Plenum Press, </publisher> <address> New York and London, </address> <year> 1988. </year>
Reference: [14] <author> D. Pollard. </author> <title> A central limit theorem for k-means clustering. </title> <journal> Annals of Probability, </journal> <volume> 10(4) </volume> <pages> 919-926, </pages> <year> 1982. </year>
Reference-contexts: The problem of training set size has also been studied independently at Stanford University [4] with different but consistent results. It has also been considered from an information-theoretic viewpoint by David Pollard <ref> [14] </ref>. We summarize the remainder of the paper as follows. Section 2 provides a brief introduction to vector quantization and details the derivation of bounds on training set sizes for VQ codebooks.
Reference: [15] <author> R. Ulichney. </author> <title> Digital Halftoning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference: [16] <author> V. Vapnik. </author> <title> Estimation of Dependencies Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 2 2.1 Pattern classification and the VC-dimension The results of <ref> [1, 16, 17] </ref> concern the asymptotic performance of learning systems. Specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed. <p> A detailed description of the VC-dimension and its use in formal learning theory is beyond the scope of this paper, but may be found in <ref> [1, 16] </ref>. 3 2.2 Framing VQ as a Classification Problem The bound in the previous subsection is useful because it lets us predict generalized performance based on observed performance.
Reference: [17] <author> V. Vapnik and A. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: We then show how these bounds may be used to bound the difference in training and test performance of a VQ codebook. 2 2.1 Pattern classification and the VC-dimension The results of <ref> [1, 16, 17] </ref> concern the asymptotic performance of learning systems. Specifically, these results bound the difference between the empirically observed performance of a system and its "true" performance as a function of the number of inputs over which the empirical performance was observed.
Reference: [18] <author> S. Weiss and C. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: We write this as *(c; c fl ; P m ) = m i=1 0 I x2c (x i ) = I x2c fl (x i ) 1 otherwise. (2) Other more involved methods of estimating empirical error are discussed in <ref> [18] </ref>. Typically, our hypothesis is chosen according to some rule, such as "all points that are within Euclidean distance 1 of point z." This rule defines a concept class, C. <p> The only noticeable deviations from the polynomial model are for small training set sizes or when large codebooks are used. This is consistent with observations made in <ref> [18] </ref>, pointing out that when a learner is sufficiently powerful and a training set is sufficiently small, rather than learning to generalize, the learner simply memorizes data to some extent. Memorization is a qualitatively different phenomenon from generalization, and is beyond the scope of this study.
Reference: [19] <author> P. </author> <title> Zehna. </title> <journal> Probability Distributions and Statistics, </journal> <pages> pages 286-289, </pages> <publisher> Allyn and Bacon, </publisher> <address> Boston, </address> <year> 1970. </year> <month> 19 </month>
References-found: 19

