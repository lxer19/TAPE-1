URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/icannga93.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/
Root-URL: http://www.cs.purdue.edu
Email: e-mail: mam@grete.informatik.uni-dortmund.de  
Title: Representation and Evolution of Neural Networks  
Author: Martin Mandischer 
Address: Dortmund Germany  
Affiliation: Department of Computer Science VI University of  
Abstract: An evolutionary approach for developing improved neural network architectures is presented. It is shown that it is possible to use genetic algorithms for the construction of backpropagation networks for real world tasks. Therefore a network representation is developed with certain properties. Results with various application are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Fox, V. Heinze, K. Moller, S. Thrun, and G. Veenker. </author> <title> Learning by error-driven decomposition. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 207-212, </pages> <address> Amsterdam, June 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: It has been proven [13] that feed-forward networks are capable of arbitrary exact function approximation. Despite this theoretical result, Fox showed that it is difficult for a great number of architectures to approximate a Mexican hat function <ref> [1] </ref>. Therefore we used 841 vectors with x, y, z coordinates building a 3-dimensional Mexican hat function. We developed some evaluation criteria for the networks and the evolution process itself.
Reference: [2] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Each individual is assigned an fitness value which reflects its ability to adapt to the given environment. A genetic algorithms is an iterative procedure, where each iteration is called generation <ref> [2] </ref>. During each iteration the two natural inspired principles of selection and reproduction are applied to the population. The selection mechanism determines which individuals are allowed to produce offsprings for the next generation. <p> The fitness is calculated by several weighted factors: fitness := w 1 fi error + w 2 fi connections + w 3 fi units + w 4 fi epochs With the fitness every network has its expected number of offsprings assigned. The roulette wheel selection method <ref> [2] </ref> chooses networks with a probability according to their relative fitness, until the population size is reached. The chosen networks are put in the mating pool and allowed to reproduce themselves.
Reference: [3] <author> S. A. Harp, T. Samad, and A. Guha. </author> <title> Designing application-specific neural networks using the genetic algorithm. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Proceedings of IEEE conference on Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 447-454, </pages> <address> San Mateo, 1990. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The main problem with this approach is the matrix representation of the network. Incorrect network structure, i.e. feedback connections, and very long codes for larger networks are the result. A more sophisticated approach has been presented by Harp et al. <ref> [3] </ref>. They used 'blueprints' for the networks where each network is described by several parameters like, number of layers, layer size, and connections between layers. With this representation it was possible to place constraints on the networks architecture and to reduce the number of incorrect networks. <p> The search space is much smaller and networks can be prestructured with the restriction that some architectures are excluded. Page 2 The works of Harp et al. and Miller et al. presented representations of both types which are not able to insure the correctness of coded networks <ref> [3, 7] </ref>. Our representation of backpropagation networks allows only correct networks which prevents incorrect networks from participating in the evolution cycle and saves useless training. Therefore the population size and the number of iterations can be reduced to a number that makes the problem tractable.
Reference: [4] <author> J. S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press - Bradford Book, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: From a computational and developers viewpoint, this procedure is very expensive and gives no reliable insight into the architecture/performance relation. Recent work of Judd and Lin/Vitter shows that learning in general, as well as choosing an optimal network topology, are NP-complete problems <ref> [4, 5] </ref>. They also have shown that placing constraints on the topology can help to make learning tractable. This motivates the usage of some evolutionary approach as a heuristic for finding optimal architectures and a network representation which places useful constraints on the architecture.
Reference: [5] <author> H. J. Lin and J. S. Vitter. </author> <title> Complexity issues in learning by neural networks. </title> <type> Technical Report TR-CS-90-01, </type> <institution> Department of Computer Science, Brown University, Providence, RI, </institution> <year> 1990. </year>
Reference-contexts: From a computational and developers viewpoint, this procedure is very expensive and gives no reliable insight into the architecture/performance relation. Recent work of Judd and Lin/Vitter shows that learning in general, as well as choosing an optimal network topology, are NP-complete problems <ref> [4, 5] </ref>. They also have shown that placing constraints on the topology can help to make learning tractable. This motivates the usage of some evolutionary approach as a heuristic for finding optimal architectures and a network representation which places useful constraints on the architecture. <p> It can been shown that the network representation is closed under the given operators and therefore only correct networks participate in the evolution <ref> [5] </ref>. 4 Results To evaluate our approach we selected four applications for backpropagation networks. The first task is to discriminate edges and corners. Training data consisted of 70 patterns, where each pattern is given as 9x9 binary matrix, which displays either an edge or a corner.
Reference: [6] <author> M. Mandischer. </author> <title> Genetische Algorithmen zur Op-timierung Konnektionistischer Modelle. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Dortmund, </institution> <address> Dortmund, PO BOX 500500, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Nearly every genetic network showed its superiority over networks with a comparable number of connection, units and learning epochs. Applied to the function approximation task the genetic network needed 12 times less learning time and showed better approximation quality than standard networks <ref> [6] </ref>. These results give strong evidence that the underlying structure is essential for the performance of the network. An emerging question in this context is how the structure influences the learning process and how we can gain knowledge about this structure/performance relation from superior genetic neural networks.
Reference: [7] <author> G. F. Miller, P. M. Todd, and S. U. Hegde. </author> <title> Designing neural networks using genetic algorithms. </title> <editor> In J. D. Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 379-384, </pages> <address> San Mateo, 1989. (Arlington 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The networks which performed best on the given task, are allowed to produce offsprings for the next generation. New offsprings are created from one or two parent networks by combining or manipulating their connection-matrices. After a fixed number of cycles the best network is chosen <ref> [7] </ref>. The main problem with this approach is the matrix representation of the network. Incorrect network structure, i.e. feedback connections, and very long codes for larger networks are the result. A more sophisticated approach has been presented by Harp et al. [3]. <p> The search space is much smaller and networks can be prestructured with the restriction that some architectures are excluded. Page 2 The works of Harp et al. and Miller et al. presented representations of both types which are not able to insure the correctness of coded networks <ref> [3, 7] </ref>. Our representation of backpropagation networks allows only correct networks which prevents incorrect networks from participating in the evolution cycle and saves useless training. Therefore the population size and the number of iterations can be reduced to a number that makes the problem tractable.
Reference: [8] <author> T. Rohne. Kunstliche neuronale netze zur ecken-detektion in digitalisierten grauwertbildern. </author> <type> Master's thesis, </type> <institution> University of Dortmund, Department of Computer Science X, </institution> <address> Dortmund, FRG, </address> <year> 1991. </year>
Reference: [9] <author> N. Schaudolph and R. K. Belew. </author> <title> Dynamic parameter encoding for genetic algorithms. </title> <type> Technical Report CSE-TR#CS90-175, </type> <institution> University of California, </institution> <address> San Diego, CA, </address> <year> 1990. </year>
Reference-contexts: To prohibit that networks with very high fitness values dominate the population in the first generations and to compensate low fitness diversity at the end of the search, we used the sigma-scaling method <ref> [9] </ref> to rescale the fitness of all networks. The fitness is calculated by several weighted factors: fitness := w 1 fi error + w 2 fi connections + w 3 fi units + w 4 fi epochs With the fitness every network has its expected number of offsprings assigned.
Reference: [10] <author> W. Schiffmann, J. Merten, and W. Randolf. </author> <title> Performance evaluation of evolutionary created neural network topologies. </title> <booktitle> In First International Page 6 Workshop on Parallel Problem Solving from Na--ture, pages A-III:1-11, </booktitle> <address> Dortmund, FRG, </address> <year> 1990. </year> <institution> University of Dortmund, Department of Computer Science X. </institution>
Reference-contexts: Another approach which avoids the representation problem is described by Schiffman et al. To produce the offsprings the best networks are selected and some connections are added or removed from networks and the worst networks are replaced by the offsprings of the best <ref> [10] </ref>. The next section introduces Genetic Algorithms as a method to search for better networks. Section 3 focuses on a network representation that fulfills some useful properties and the operators to create new networks.
Reference: [11] <author> A. Ultsch, R. Hannuschka, U. Hartmann, M. Mandischer, and V. Weber. </author> <title> Optimizing symbolic proofs with connectionist models. </title> <editor> In T. Ko-honen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 585-590, </pages> <address> Amsterdam, June 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: A more difficult task is to discriminate 490 handwritten digits from 6 different writers which are given by 8x11 matrices each showing one of 10 possible digits. The third application consists of 279 vectors with 112 real-valued components which encode control-knowledge to improve proofs in PROLOG <ref> [11] </ref>. It has been proven [13] that feed-forward networks are capable of arbitrary exact function approximation. Despite this theoretical result, Fox showed that it is difficult for a great number of architectures to approximate a Mexican hat function [1]. <p> Results in learn ing control-knowledge are comparable to the digit task. A network (112-36-7) with 2103 connections performed better than the standard network with the same number of connections and slightly worse than a network (112-36-7) with twice as much connections. Compared to Ultsch et al. <ref> [11] </ref> where less than 50% of the control knowledge has been learned, our networks achieved more then 90%. The most astonishing result we gained from the function approximation.
Reference: [12] <author> G. Wei. </author> <title> Combining neural and evolutionary learning: Aspects and approaches. </title> <type> Technical Report FKI-132-90, </type> <institution> Technische Universitat Munchen, </institution> <year> 1990. </year>
Reference-contexts: This motivates the usage of some evolutionary approach as a heuristic for finding optimal architectures and a network representation which places useful constraints on the architecture. Several approaches have been made towards the evolutionary design of neural networks <ref> [12] </ref>. A straight-forward method to determine the architec ture of a network has been used Miller et al. The network structure was mapped onto a binary connection matrix where each cell of the matrix determines whether a connection between two units exists or not. <p> This evolution cycle is iterated for a fixed number of times. During evolution the actual population, best networks and some statistical measures to evaluate the process are collected and saved. 3 Networkrepresentation and Genetic Operators Wei distinguished between low- and high-level representations <ref> [12] </ref>. A low-level representation specifies exactly each network parameter (each connection, unit etc.). This causes a large search space for the GA, so that the number of iterations increases dramatically.
Reference: [13] <author> H. White and K. Hornik. </author> <title> Universal approximation using feedforward networks with non-sigmoid hidden layer activation functions. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> (San Diego, CA), </address> <publisher> IEEE, </publisher> <year> 1989. </year> <pages> Page 7 </pages>
Reference-contexts: The third application consists of 279 vectors with 112 real-valued components which encode control-knowledge to improve proofs in PROLOG [11]. It has been proven <ref> [13] </ref> that feed-forward networks are capable of arbitrary exact function approximation. Despite this theoretical result, Fox showed that it is difficult for a great number of architectures to approximate a Mexican hat function [1]. Therefore we used 841 vectors with x, y, z coordinates building a 3-dimensional Mexican hat function.
References-found: 13

