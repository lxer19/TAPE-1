URL: http://www.cs.rutgers.edu/~ellman/sara98/papers/saitta-paper.ps
Refering-URL: http://www.cs.rutgers.edu/~ellman/sara98/papers/
Root-URL: http://www.cs.rutgers.edu
Email: saitta@di.unito.it  Jean-Daniel.Zucker@lip6.fr  
Title: Semantic Abstraction for Concept Representation and Learning  
Author: Lorenza Saitta Jean-Daniel Zucker 
Address: Corso Svizzera 185 10149 Torino (Italy)  4, Place Jussieu 75252 Paris (France)  
Affiliation: Universit di Torino Dipartimento di Informatica  Universit Paris VI CNRS Laboratoire dInformatique de Paris 6  
Abstract: So far, abstraction has been mainly investigated in problem solving tasks. In this paper, we are interested in the role of abstraction in representing and learning concepts (i.e., intensional descriptions of classes of objects). We propose a novel perspective on abstraction, originating from the observation that a conceptualization of a domain involves entities belonging to at least three levels. The fundamental level is the perception of the world, where concrete objects reside. For memorizing objects, some kind of structure, which describes objects and relations perceived in the world, is needed. Finally, to communicate with others, and also to perform reasoning, a language has to be used; the language allows both the world and theories about the world to be described intensionally . In previous approaches, abstraction has been frequently defined as a mapping between languages. Our main departure from this view is that abstraction is, originally, a mapping between views of the world, and that the modifications of the structure and of the language are side-effects, necessary to describe what happens at the level of the perceived world. Within the defined framework, we show how the abstraction process can be realized by means of a set of operators, and we formalize a constraint that abstraction mappings should satisfy in order to be useful for Machine Learning, i.e. preserving the generality relation among hypotheses. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amarel, S. </author> <year> (1983). </year> <title> Representation in problem solving. In Methods of Heuristics (pp. </title> <address> 131-171). Palo Alto, CA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Abstraction has also been studied in relation with change of problem representation (Benjamin et al., 1990). This type of abstraction relies upon the assumption that there are some representations that are more operational than others for certain kinds of problem <ref> ( Amarel, 1983) </ref>. Subramanian (1990) proposed an approach for performing incremental abstraction in reformulating problems for increasing computational efficiency, based on a notion of relevance (Subramanian, Greiner & Pearl, 1997). Korf (1980) views discovering efficient solution strategies as a heuristic search in the space of problem representations.
Reference: <author> Benjamin D.P., Dorst L., Mandhyan I. and Rosar M. </author> <year> (1990). </year> <title> An Algebraic Approach to Abstraction and Representation Change. </title> <booktitle> Proc. AAAI Workshop on Automatic Generation of Approximations and Abstractions (Boston, </booktitle> <address> MA), </address> <pages> pp. 47-52. </pages>
Reference-contexts: Abstraction has also been studied in relation with change of problem representation <ref> (Benjamin et al., 1990) </ref>. This type of abstraction relies upon the assumption that there are some representations that are more operational than others for certain kinds of problem ( Amarel, 1983).
Reference: <author> Bennett, S. </author> <year> (1987). </year> <title> "Approximation in Mathematical Domains", </title> <booktitle> Proc. IJCAI-87 (Milano, Italy), </booktitle> <pages> pp. 239-241. </pages>
Reference-contexts: In particular, for an attribute or a function, w specifies what subset of the attribute or function values can be merged, because they are considered indistinguishable. If the values belong to a function range, then the operator can be seen as an approximation of the function <ref> (Bennett, 1987) </ref>. Finally, the last two operators apply to subsets of either FUNC or REL, whose elements arity is not less than 2.
Reference: <author> Drastal, G., Czako G., & Raatz S. </author> <year> (1989). </year> <title> Induction in an abstraction space. </title> <booktitle> Proc. </booktitle> <address> IJCAI-89 (Detroit, MI), </address> <pages> pp. 708-712. </pages>
Reference: <author> Ellman T. </author> <year> (1993). </year> <title> Synthesis of Abstraction Hierarchies for Constraint Satisfaction by Clustering Approximately Equivalent Objects . Proc. </title> <booktitle> Int. Conf. on Machine Learning (Amherst, </booktitle> <address> MA), </address> <pages> pp. 104-111. </pages>
Reference: <author> Enderton H.B. </author> <year> (1972). </year> <title> Mathematical Introduction to Logic, </title> <publisher> Academic Press. </publisher>
Reference: <author> Giordana A. and Saitta L. </author> <year> (1990). </year> <title> Abstraction: a General Framework for Learning . Working Notes of Workshop on Automated Generation of Approximations and Abstractions (Boston, </title> <address> MA), </address> <pages> pp. 245-256. </pages>
Reference: <author> Giordana A., Lo Bello G. and Saitta L. </author> <year> (1993). </year> <title> Abstraction in Propositional Calculus . Proc. Workshop on Knowledge Compilation and Speed Up Learning (Amherst, </title> <address> MA), </address> <pages> pp. 56-64. </pages>
Reference: <author> Giordana A., Saitta L., Roverso D. </author> <year> (1991). </year> <title> Abstracting Concepts with Inverse Resolution , Proc. </title> <booktitle> 8th Int. Machine Learning Workshop (Evanston, </booktitle> <address> IL), </address> <pages> pp. 142-146. </pages>
Reference-contexts: The generality relations are evaluated on S g and S a , respectively. r Examples of abstraction mappings that preserve generality can be found in <ref> (Giordana, Saitta & Roverso, 1991) </ref>. The above definition does not set any restriction on the GLAMs, except that the abstracted structure S a be used for evaluating the truth of the abstracted hypotheses.
Reference: <author> Giunchiglia, F., & Walsh, T. </author> <year> (1992). </year> <title> A Theory of Abstraction. </title> <booktitle> Artificial Intelligence , 56, </booktitle> <pages> 323-390. </pages>
Reference-contexts: As already mentioned, Nayak and Levy (1995) proposed a twostep semantic theory of abstraction. This semantic theory yields abstractions that are weaker than the base theory, i.e. they are strictly a subset of TD (theorem decreasing) abstractions <ref> ( Giunchiglia & Walsh, 1992) </ref>. In fact, Nayak and Levy introduce the notions of model increasing abstractions that are a strict subset of TD-abstractions. As Giunchiglia and Walsh state it, TD-abstractions are not really interesting for automated reasoning, because TD-abstractions are loosing part of the theorems.
Reference: <author> Hobbs, J. </author> <year> (1985). </year> <title> "Granularity", </title> <booktitle> Proc. </booktitle> <pages> IJCAI-85 , pp. 432-435. </pages>
Reference: <author> Holte R.C., Mkadmi, T., Zimmer, R.M., MacDonald, A.J. </author> <year> (1996). </year> <title> "Speeding Up Problem-Solving by Abstraction: A Graph-Oriented Approach." </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <pages> 321-361. </pages>
Reference: <author> Holte, </author> <title> R.C. and Zimmer, R.M. </title> <booktitle> "A Mathematical Framework for Studying Representation" Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, Segre A.M. </address> <publisher> (eds.). Morgan Kaufmann. </publisher>
Reference: <author> Imielinski, T. </author> <year> (1987). </year> <title> Domain Abstraction and Limited Reasoning. </title> <booktitle> In Proc. </booktitle> <pages> IJCAI , (pp. 997-1003), </pages> <address> Milano, Italy. </address>
Reference-contexts: In particular, w ind specifies a grouping of indistinguishable objects. This operation corresponds to Imielinskis domain abstraction, which groups objects into equivalence classes <ref> (Imielinski, 1987) </ref>. The arguments of the operator are the objects that shall be considered the same. The operator w ta also acts on OBJ : it specifies how a set of ground objects can be grouped to form a new compound object. The primitive objects disappear from the abstract world.
Reference: <author> Iwasaski Y. </author> <year> (1990). </year> <title> Reasoning with Multiple Abstraction Models . Proc. AAAI Workshop on Automatic Generation of Approximations nd Abstractions (Boston, </title> <address> MA), </address> <pages> pp. 122-133. </pages>
Reference: <author> Knoblock, C. </author> <year> (1992). </year> <title> Automatic Generation of Abstraction for Planning. </title> <journal> Artificial Intelligence, </journal> <volume> 68 (2). </volume>
Reference: <author> Korf, R. E. </author> <year> (1980). </year> <title> Towards a Model for Representation Change . Artificial Intelligence, </title> <booktitle> 14, </booktitle> <pages> 41-78. </pages>
Reference: <author> Lowry, M. </author> <year> (1987). </year> <title> The Abstraction/Implementation Model of Problem Reformulation . Proc. </title> <booktitle> IJCAI-87 (Milano, Italy), </booktitle> <pages> pp. 1004-1010. </pages>
Reference: <author> Matheus, C., Rendell, L. </author> <year> (1989). </year> <title> "Constructive Induction in Decision Trees", </title> <booktitle> Proc. IJCAI-89 , (Detroit, USA), </booktitle> <pages> pp. 645-650. </pages>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A Theory and Methodology of Inductive Learning . In R. </title> <type> Michalski, </type> <institution> T. </institution>
Reference: <editor> Mitchell and J. Carbonell (Eds.), </editor> <booktitle> Machine Learning, </booktitle> <volume> Vol. </volume> <publisher> I , Tioga Publ. Co., </publisher> <address> Palo Alto, CA, </address> <pages> pp. 83-134. </pages>
Reference: <author> Mitchell, T. </author> <year> (1982). </year> <title> "Generalization as Search", </title> <booktitle> Artificial Intelligence,18, </booktitle> <pages> 203-226. </pages>
Reference: <author> Muggleton, S., Buntine, W. </author> <year> (1988). </year> <title> "Machine Invention of First-Order Predicates by Inverting Resolution", </title> <booktitle> Proc. Fifth Int. Conf. on Machine Learning, </booktitle> <address> (Ann Arbor, MI), pp.339-352. </address>
Reference: <author> Nayak P.P. and Levy A.Y. </author> <year> (1995). </year> <booktitle> A Semantic Theory of Abstraction . Proc. IJCAI-95 (Montral, Canada), </booktitle> <pages> pp. 196-202. </pages>
Reference-contexts: As mentioned before, both operators l red-arg prop have been proposed by Plaisted (1981) . Let us consider, for instance, the operator l ind . If the planes from Japan and USA should be merged as in an analogous example from <ref> (Nayak & Levy, 1995) </ref>, we can define in our framework the inference rule: l (J APANESE PLANE, A MERICAN PLANE | F OREIGN PLANE) " x [J APANESE PLANE (x) A MERICAN PLANE (x) fi F OREIGN PLANE (x)] More complex is the case of abstracting formulas.
Reference: <author> Pawlak Z. </author> <year> (1991). </year> <title> Rough Sets : Theoretical Aspects of Reasonning about Data. </title> <publisher> Kluwer Academic Publisher., Norwell, </publisher> <address> MA. </address>
Reference-contexts: Imielinski (1987) proposed an approximate reasoning framework for abstraction. He call such kind of reasoning limited because it is we aker than the general predicate logic proof method, but it leads to significant computational advantages. Todays research on granularity includes the rough set approach <ref> (Pawlak, 1991) </ref>. 3. Definition of Abstraction Mapping In this paper we propose a novel perspective on abstraction, originating from the observation that concept representation deals with entities belonging to three different levels.
Reference: <author> Pearl, J. </author> <year> (1978). </year> <title> "On the Connection between the Complexity and Credibility of Inferred Models", </title> <note> Int. J. of General Systems , 4 , 255-264. </note>
Reference: <author> Plaisted, D. </author> <year> (1981). </year> <title> Theorem Proving with Abstraction. </title> <booktitle> Artificial Intelligence , 16, </booktitle> <pages> 47-108. </pages>
Reference: <author> Sacerdoti, E. </author> <year> (1973). </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> In Proceedings of the 3rd International Jointed Conference in Artificial Intelligence (IJCAI) , (pp. </booktitle> <pages> 412-422). </pages>
Reference: <author> Smith J.M. and Smith D. </author> <year> (1977). </year> <title> Database Abstractio ns: </title> <journal> Aggregation and Generalization . ACM Trans. on Database Systems, </journal> <volume> 2, </volume> <pages> 105-133. </pages>
Reference-contexts: an associated operator in a set S , at the structure level: S = -s , s , s hide , s , s , s prop - Each operator s in S is defined on S by means of relational algebras operators, such as projection, selection, join and product <ref> (Smith & Smith, 1977) </ref>. For example, if w red-arg belongs to W and r is a binary relation, then w red-arg (r*(x,y), y) is a projection on x of the table r* associated to r. In an analogous way, s ind , applied to objects, is a selection operation.
Reference: <author> Subramanian, D. </author> <year> (1990). </year> <title> A Theory of Justified Reformulations. </title> <editor> In D. P. Benjamin (Eds.), </editor> <title> Change of Representation and Inductive Bias (pp. </title> <address> 147-167). Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Subramanian D., Greiner R. and Pearl J. </author> ( <note> eds) (1997). Artificial Intelligence , 97 (1-2). Special Issue on Relevance. </note>
Reference-contexts: This type of abstraction relies upon the assumption that there are some representations that are more operational than others for certain kinds of problem ( Amarel, 1983). Subramanian (1990) proposed an approach for performing incremental abstraction in reformulating problems for increasing computational efficiency, based on a notion of relevance <ref> (Subramanian, Greiner & Pearl, 1997) </ref>. Korf (1980) views discovering efficient solution strategies as a heuristic search in the space of problem representations. For Lowry (1987) a good problem representation incorporates important problem constraints while hiding superfluous details. Ellman (1993) is focused on improving performance in solving constraint satisfaction problems.
Reference: <author> Tenenberg J. </author> <year> (1987). </year> <title> Preserving Consistency across Abstraction Mappings", </title> <booktitle> Proc. IJCAI-87 (Milan, Italy), </booktitle> <pages> pp. 1011-1014. </pages>
Reference: <author> Utgoff, P. </author> <year> (1985). </year> <title> "Shift of Bias For Inductive Concept Learning", </title> <editor> in Michalski R., Carbonell J. & Mitchell T.( </editor> <title> Eds) Machine Learning: An AI Approach, </title> <booktitle> Vol. II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann), pp.107-148. </publisher>
Reference-contexts: Giordana and Saitta (1990) provide a discussion about the differences between generalization and abstraction in learning. There are also significant efforts that have been devoted to the shift of bias <ref> (Utgoff, 1985) </ref>, a problem related to abstraction. This problem deals with the modification of the language used to represent the learning examples and the concepts. <p> The operator l eq-val has been analyzed in several works on abstraction, because it corresponds to the case of predicate mapping , when it is applied to atomic predicates ( Plaisted, 1981; Tenenberg, 1987; Giunchiglia & Walsh, 1992; Nayak & Levy, 1995), or to climbing a taxonomy <ref> (Utgoff, 1985) </ref>, when it is applied to an attribute values. As mentioned before, both operators l red-arg prop have been proposed by Plaisted (1981) . Let us consider, for instance, the operator l ind .
Reference: <author> Van Dalen, D. </author> <year> (1983). </year> <title> Logic and Structure , Springer-Verlag, </title> <address> Berlin, Germany. </address>
Reference-contexts: Their reality consists in the physical stimuli produced on the observer. In order to let these stimuli become available over time, they must be, first of all, memorized into an organized structure S. This structure is an extensional representation <ref> (Van Dalen, 1983) </ref> of the perceived world, in which stimuli related one to another are stored together into tables. The set of these tables constitutes a relational database, on which relational algebra operators can be applied.
Reference: <author> Wneck J. and Michalski R. </author> <year> (1994). </year> <title> Hypothesis-Driven Constructive Induction in AQ17-HCI: A Method and Experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 139-168. </pages>
Reference: <author> Yoshida K. and Motoda H. </author> <year> (1990). </year> <title> Towards Automatic Generation of Hierarchical Knowledge Bases. </title> <booktitle> Proc. AAAI Workshop on Automatic Generation of Approximations nd Abstractions (Boston, </booktitle> <address> MA), </address> <pages> pp. 98-109. </pages>
Reference: <author> Zucker, J.-D., and Ganascia, J.-G. </author> <year> 1994. </year> <title> Selective Reformulation of Examples in Concept Learning. </title> <booktitle> Proc. International Conference on Machine Learning , pp. </booktitle> <pages> 352-360, </pages> <address> New-Brunswick, </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zucker, J.-D., and Ganascia, J.-G. </author> <year> 1996. </year> <title> Changes of Representation for Efficient Learning in Structural Domains. </title> <booktitle> Proc. International Conference in Machine Learning , pp. </booktitle> <pages> 543-551, </pages> <address> July 3-6, Bari, Italy. </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 38

