URL: http://www.cs.colostate.edu/~vision/ps/1996/ICPR96.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: stevensm@cs.colostate.edu  ross@cs.colostate.edu  
Title: Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition  
Author: Mark R. Stevens J. Ross Beveridge 
Affiliation: Colorado State University  Colorado State University  
Abstract: The object recognition system presented combines on-line feature prediction with an iterative multisen-sor matching algorithm. Matching begins with an initial object type and pose hypothesis. An iterative generate-and-test procedure then refines the pose as well as the sensor-to-sensor registration for separate range and electro optical sensors. During matching, object features predicted to be visible are updated to reflect changes in hypothesized object pose and sensor registration. The match found is locally optimal in terms of the complete space of possible matches and globally consistent in the sense of preserving the 3D constraints implied by sensor and object geometry. Results on real data are presented which demonstrate the algorithm correcting for up to 30 ffi errors in initial orientation and 5m errors in initial translation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Beveridge, D. P. Panda, and T. Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The coregistration matching algorithm is part of a larger Automatic Target Recognition (ATR) system [16] which refines and verifies target type and pose estimates provided by up-stream processes. Range and color imagery collected at Fort Carson, Colorado in November 1993 <ref> [1] </ref> is used to demonstrate our algorithm. This data is publicly available through our web site: http://www.cs.colostate.edu/~vision. Figure 1 presents a pair of range and color images in which the target is roughly 50m away in a fairly open area. <p> The sun is modelled as an area light source and the vector to the sun is calculated using a long/lat estimate, time of day, date, and compass orientation [11] (we have this collateral information <ref> [1] </ref>). The dot product of the sun direction vector and the normal of each face is determined. For all visible edges, if the sign of these dot products differ for two adjoining faces then the edge is marked as likely to be visible. <p> To compute the error, each 3D model edge is projected into the color image and a gradient mask tuned to the precise expected orientation is applied to the pixels lying under the line. The gradient response, ^ G Line (k), for each line k is normalized to the range <ref> [0; 1] </ref>. The derivation of the response is presented in [9]. <p> The total fitness is summed over the matched points and normalized to lie in the interval <ref> [0; 1] </ref>. Normalization takes into account of the number of matched points p and the maximum allowable distance t : E fit;r (F ) = p t i2 E point (i) (9) 3.2.3 Omission Error: E om;S (F) Omission accounts for weak responses in optical and unmatched points in range.
Reference: [2] <author> C. H. Chien and J. K. Aggarwal. </author> <title> Shape recognition from single silhouettes. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 481-490, </pages> <year> 1987. </year>
Reference-contexts: An object's silhouette is one valuable recognition cue when dealing with two-dimensional optical imagery [10]. Many systems recognize 3D objects using stored 2D silhouettes [8, 18] while 3D edge silhouette representations such as we produce are less common <ref> [2] </ref>. In our imagery, internal features as well as silhouette features are required to overcome ambiguity.
Reference: [3] <author> A. Hoogs and D. Hackett. </author> <title> Model-supported exploitation as a framework for image understanding. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 265-268. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region [14, 7]. Statistical modeling information for predicting feature visibility <ref> [19, 3] </ref> may also be stored. We have developed an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of matching.
Reference: [4] <author> K. </author> <title> Ikeuchi. Precompiling a geometrical model into an interpretation tree for object recognition in bin-picking tasks. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 321-330, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Common approaches to model feature prediction use static data structures to store feature visibility information associated with object geometry alone <ref> [12, 4] </ref>. Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region [14, 7]. Statistical modeling information for predicting feature visibility [19, 3] may also be stored.
Reference: [5] <author> J. Ross Beveridge and Bruce A. Draper and Kris Siejko. </author> <title> Progress on Target and Terrain Recognition Research at Colorado State University. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: The rationale for this planar translation constraint is explained in <ref> [5] </ref>. One can develop algorithms which emphasize search in either the space of possible correspondence mappings C or the space of coregistration estimates F 2 &lt; 8 .
Reference: [6] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Internal Representation of Shape with Respect to Vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction Common approaches to model feature prediction use static data structures to store feature visibility information associated with object geometry alone [12, 4]. Typically, all possible viewpoints are divided into sets of constant model topology <ref> [6] </ref> and visible face and occluding contour information are stored for each topological region [14, 7]. Statistical modeling information for predicting feature visibility [19, 3] may also be stored.
Reference: [7] <author> M. R. Korn and C. R. Dyer. </author> <title> 3D Multiview Object Representations for Model-Based Object Recognition. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 91-103, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Common approaches to model feature prediction use static data structures to store feature visibility information associated with object geometry alone [12, 4]. Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region <ref> [14, 7] </ref>. Statistical modeling information for predicting feature visibility [19, 3] may also be stored. We have developed an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of matching.
Reference: [8] <author> C.-H. Liu and W.-H. Tsai. </author> <title> 3D curved object recognition from multiple 2d camera views. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 50 </volume> <pages> 177-187, </pages> <year> 1990. </year>
Reference-contexts: Final Orientation e. Final Color Pose f. Final LADAR Pose Two sets of features are used in matching: model edges and discrete sampled surfaces. An object's silhouette is one valuable recognition cue when dealing with two-dimensional optical imagery [10]. Many systems recognize 3D objects using stored 2D silhouettes <ref> [8, 18] </ref> while 3D edge silhouette representations such as we produce are less common [2]. In our imagery, internal features as well as silhouette features are required to overcome ambiguity.
Reference: [9] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 695-697, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kauf-man. </publisher>
Reference-contexts: The gradient response, ^ G Line (k), for each line k is normalized to the range [0; 1]. The derivation of the response is presented in <ref> [9] </ref>.
Reference: [10] <author> D. Marr. </author> <title> Analysis of occluding contour. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B197:441-475, </volume> <year> 1977. </year>
Reference-contexts: Initial Orientation b. Initial Color Pose c. Initial LADAR Pose d. Final Orientation e. Final Color Pose f. Final LADAR Pose Two sets of features are used in matching: model edges and discrete sampled surfaces. An object's silhouette is one valuable recognition cue when dealing with two-dimensional optical imagery <ref> [10] </ref>. Many systems recognize 3D objects using stored 2D silhouettes [8, 18] while 3D edge silhouette representations such as we produce are less common [2]. In our imagery, internal features as well as silhouette features are required to overcome ambiguity.
Reference: [11] <author> G. Paltridge and C. Platt. </author> <title> Radiative Processes in Meteorology and Climatology. </title> <publisher> Elsevier Scientific Publishing Company, </publisher> <year> 1976. </year>
Reference-contexts: The sun is modelled as an area light source and the vector to the sun is calculated using a long/lat estimate, time of day, date, and compass orientation <ref> [11] </ref> (we have this collateral information [1]). The dot product of the sun direction vector and the normal of each face is determined. For all visible edges, if the sign of these dot products differ for two adjoining faces then the edge is marked as likely to be visible.
Reference: [12] <author> H. Platinga and C. Dyer. </author> <title> Visibility, Occlusion, and the Aspect Graph. </title> <type> Technical Report 736, </type> <institution> University of Wisconsin - Madison, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Common approaches to model feature prediction use static data structures to store feature visibility information associated with object geometry alone <ref> [12, 4] </ref>. Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region [14, 7]. Statistical modeling information for predicting feature visibility [19, 3] may also be stored.
Reference: [13] <author> A. N. A. Schwickerath and J. R. Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The rationale for this planar translation constraint is explained in [5]. One can develop algorithms which emphasize search in either the space of possible correspondence mappings C or the space of coregistration estimates F 2 &lt; 8 . In other work, we consider search in C <ref> [13] </ref>, while here we address search in F. 3.1 Coregistration Matching Matching uses an iterative generate-and-test procedure (Figure 3) in which the current coregistration hypothesis F is used to predict a set of model features which are in turn used in the error evaluation function.
Reference: [14] <author> W. B. Seales and C. R. Dyer. </author> <title> Modeling the Rim Appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Common approaches to model feature prediction use static data structures to store feature visibility information associated with object geometry alone [12, 4]. Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region <ref> [14, 7] </ref>. Statistical modeling information for predicting feature visibility [19, 3] may also be stored. We have developed an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of matching. <p> Next, it is determined which individual face boundaries (edges) generate the silhouette. An edge is a possible silhouette edge only if one of the two bounding faces is visible <ref> [14] </ref>. This step may leave some edges which are actually internal as hypothesized silhouette edges, so a clipping algorithm discovers and discards edges and portions of edges not on the silhouette. Clipping projects the 3D model edge endpoints onto the image plane.
Reference: [15] <author> M. R. Stevens. </author> <title> Obtaining 3D Shilhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State Univeristy, Fort Collins, Colorado, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Feature prediction takes 3D object models derived from BRL-CAD [17] constructive solid geometry format. The algorithms used to reduce model complexity are presented elsewhere <ref> [15] </ref>. The CAD model for the M113 APC is used for illustration in this paper. Matching has also been done with an M60 model [16]. a. Initial Orientation b. Initial Color Pose c. Initial LADAR Pose d. Final Orientation e. Final Color Pose f. Final LADAR Pose a.
Reference: [16] <author> M. R. Stevens and J. R. Beveridge. </author> <title> Precise matching of 3D models to multisensor data. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page to appear, </note> <year> 1997. </year>
Reference-contexts: We will use the term coregistration to describe the combined object pose and multisensor image registration refinement process. The coregistration matching algorithm is part of a larger Automatic Target Recognition (ATR) system <ref> [16] </ref> which refines and verifies target type and pose estimates provided by up-stream processes. Range and color imagery collected at Fort Carson, Colorado in November 1993 [1] is used to demonstrate our algorithm. This data is publicly available through our web site: http://www.cs.colostate.edu/~vision. <p> Feature prediction takes 3D object models derived from BRL-CAD [17] constructive solid geometry format. The algorithms used to reduce model complexity are presented elsewhere [15]. The CAD model for the M113 APC is used for illustration in this paper. Matching has also been done with an M60 model <ref> [16] </ref>. a. Initial Orientation b. Initial Color Pose c. Initial LADAR Pose d. Final Orientation e. Final Color Pose f. Final LADAR Pose a. Initial Orientation b. Initial Color Pose c. Initial LADAR Pose d. Final Orientation e. Final Color Pose f. <p> The parameter ff introduces a non-linear bias which reduces the penalty of small amounts of omission while increasing the penalty for large amounts of omission <ref> [16] </ref>. For the optical omission error, E om;o (F ), w is the number of unmatched lines over the total number of lines.
Reference: [17] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: The matching shown in the figures is explained below. 2 3D Model Feature Prediction To coregister an object model to optical and range imagery, prediction extracts features suitable for matching from the CAD model. Feature prediction takes 3D object models derived from BRL-CAD <ref> [17] </ref> constructive solid geometry format. The algorithms used to reduce model complexity are presented elsewhere [15]. The CAD model for the M113 APC is used for illustration in this paper. Matching has also been done with an M60 model [16]. a. Initial Orientation b. Initial Color Pose c.
Reference: [18] <author> Y. F. Wang, M. J. Magee, and J. K. Aggarwal. </author> <title> Matching three-dimensional objects using silhouettes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 513-518, </pages> <year> 1984. </year>
Reference-contexts: Final Orientation e. Final Color Pose f. Final LADAR Pose Two sets of features are used in matching: model edges and discrete sampled surfaces. An object's silhouette is one valuable recognition cue when dealing with two-dimensional optical imagery [10]. Many systems recognize 3D objects using stored 2D silhouettes <ref> [8, 18] </ref> while 3D edge silhouette representations such as we produce are less common [2]. In our imagery, internal features as well as silhouette features are required to overcome ambiguity.
Reference: [19] <author> M. Wheeler and K. </author> <title> Ikeuchi. Sensor modeling, markov random fields, and robust localization for recognizing partially occluded objects. </title> <journal> IUW, </journal> <volume> 93 </volume> <pages> 811-818, </pages> <year> 1993. </year>
Reference-contexts: Typically, all possible viewpoints are divided into sets of constant model topology [6] and visible face and occluding contour information are stored for each topological region [14, 7]. Statistical modeling information for predicting feature visibility <ref> [19, 3] </ref> may also be stored. We have developed an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of matching.
References-found: 19

