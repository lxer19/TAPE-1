URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-026.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Email: fritzke@icsi.berkeley.edu  
Title: Growing Cell Structures A Self-organizing Network for Unsupervised and Supervised Learning  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Bernd Fritzke 
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  Berkeley, CA,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute,  
Pubnum: TR-93-026  
Abstract: We present a new self-organizing neural network model having two variants. The first variant performs unsupervised learning and can be used for data visualization, clustering, and vector quantization. The main advantage over existing approaches, e.g., the Kohonen feature map, is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process which also includes occasional removal of units. The second variant of the model is a supervised learning method which results from the combination of the abovementioned self-organizing network with the radial basis function (RBF) approach. In this model it is possible in contrast to earlier approaches toperform the positioning of the RBF units and the supervised training of the weights in parallel. Therefore, the current classification error can be used to determine where to insert new RBF units. This leads to small networks which generalize very well. Results on the two-spirals benchmark and a vowel classification problem are presented which are better than any results previously published. fl submitted for publication
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, E. B. & K. E. </author> <title> Lang (1991), Constructing hidden units using examples and queries, </title> <booktitle> in advances in neural information processing systems 3, </booktitle> <editor> R.P. Lippmann, J.E. Moody & D.S. Touretzky, eds., </editor> <publisher> Morgan Kaufmann Publ., Inc, </publisher> <address> San Mateo, </address> <pages> pp. 904-910. </pages>
Reference: <author> Blackmore, J. & R. </author> <title> Miikkulainen (1992), Incremental grid growing: encoding high-dimensional structure into a two-dimensional feature map, </title> <institution> University of Texas at Austin, TR AI92-192, Austin, TX. </institution>
Reference: <author> Deterding, D. H. </author> <year> (1989), </year> <title> Speaker Normalisation for Automatic Speech Recognition, </title> <institution> University of Cambridge, </institution> <type> Ph.D. thesis. </type>
Reference: <author> Fahlman, S. E. </author> <year> (1993), </year> <title> CMU Benchmark Collection for Neural Net Learning Algorithms, </title> <institution> Carnegie Mellon University, School of Computer Science, [machine-readable data repository], Pittsburgh. </institution>
Reference-contexts: The data used was collected by Deterding (1989), who recorded examples of the eleven steady state vowels of English spoken by fifteen speakers for a speaker normalization study. The vowel data (as well as the two-spiral data) is electronically available from the Carnegie-Mellon University connectionist benchmark collection <ref> (see Fahlman, 1993) </ref>. (An ASCII approximation to) the International Phonetic Association (I.P.A.) symbol and the word in which the eleven vowel sounds were recorded is given in table 3. The word was uttered once by each of the fifteen speakers, 7 of whom were female and 8 male.
Reference: <author> Fahlman, S. E. & C. </author> <booktitle> Lebiere (1990), The Cascade-Correlation Learning Architecture, in Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky, ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pp. 524-532. </pages>
Reference-contexts: 30, resp. 25 a) final network b) decision regions shown in fig. 16 Simulation parameters: = 240, " b = 0:1, " n = 0:006, k = 2, ~ t = 2, ff = 0:005, = 0:15, no removal of cells. a) two spiral problem b) decision regions for Cascade-Correlation <ref> (reprinted with permission from Fahlman & Lebiere, 1990) </ref> 26 network model number of epochs reported in Backpropagation 20000 Lang & Witbrock (1989) Cross Entropy BP 10000 Lang & Witbrock (1989) Cascade-Correlation 1700 Fahlman & Lebiere (1990) Growing Cell Structures 180 (this paper) Table 2: Training epochs necessary for the two spiral
Reference: <author> Favata, F. & R. </author> <title> Walker (1991), A study of the application of Kohonen-type neural networks to the travelling Salesman Problem, </title> <journal> Biological Cybernetics, </journal> <volume> 64, </volume> <pages> pp. 463-468. </pages>
Reference-contexts: This makes them interesting for applications in various areas ranging from speech recognition (Kohonen, 1988) and data compression (Schweizer et al., 1991) to combinatorial optimization <ref> (Favata & Walker, 1991) </ref>. The fact that similar mappings can be found at various places in the brains of humans and animals indicates that preservation of topology is an important principle at least in natural "signal processing systems".
Reference: <author> Fritzke, B. </author> <year> (1993a), </year> <title> Kohonen feature maps and growing cell structures a performance comparison, </title> <booktitle> in Advances in Neural Information Processing 5, </booktitle> <editor> L. Giles, S. Hanson & J. Cowan, eds., </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The network presented in this contribution has a flexible as well as compact structure, a variable number of elements, and a k-dimensional topology whereby k can be arbitrarily chosen. Recently it was demonstrated that the new model improves over Kohonen's feature map with respect to various important criteria <ref> (Fritzke, 1993a) </ref>. We acknowledge, however, that the new model owes several ideas to Kohonen's approach and that it is an extension of his work rather than a completely different formalism. <p> The abovementioned comparative study indicates that the Growing Cell Structures estimate unknown probability distributions significantly better than Kohonen's feature maps <ref> (Fritzke, 1993a) </ref>. In fig. 7 some stages of a simulation are depicted. The cell structure grows, guided by the input vectors, and finally finds a suitable structure to model the cloud-shaped distribution.
Reference: <author> Fritzke, B. </author> <year> (1993b), </year> <title> Vector quantization with a growing and splitting elastic net, </title> <booktitle> (to appear in the proceedings of ICANN-93), </booktitle> <address> Amsterdam. </address>
Reference-contexts: The resulting network structures differ especially for probability distributions with a nonuniform probability density (see, e.g., fig 14). Recently this particular insertion criterion has been used to develop a new method for vector quantization <ref> (see Fritzke, 1993b) </ref>. For this application the consistency requirements for the structures have been loosened by allowing also separate cells (without any neighbors) to exist. The method is able to generate codebooks of exceptionally good quality.
Reference: <author> Jokusch, S. </author> <year> (1990), </year> <title> A neural network which adapts its structure to a given set of patterns, </title> <booktitle> in Parallel Processing in Neural Systems and Computers, </booktitle> <editor> R. Eckmiller, G. Hartmann & G.Hauske, eds., </editor> <publisher> Elsevier Science Publishers B.V., </publisher> <pages> pp. 169-172. </pages>
Reference: <author> Kangas, J. A., T. Kohonen & T. </author> <month> Laaksonen </month> <year> (1990), </year> <title> Variants of Self-Organizing Maps, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1, </volume> <pages> pp. 93-99. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982), </year> <title> Self-Organized Formation of Topologically Correct Feature Maps, </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> pp. 59-69. </pages>
Reference: <author> Kohonen, T. </author> <year> (1988), </year> <title> The Neural Phonetic Typewriter, </title> <journal> IEEE computer, </journal> <volume> 21, </volume> <pages> pp. 11-22. </pages>
Reference-contexts: These mappings are able to preserve neighborhood relations in the input data and have the property to represent regions of high signal density on correspondingly large parts of the topological structure. This makes them interesting for applications in various areas ranging from speech recognition <ref> (Kohonen, 1988) </ref> and data compression (Schweizer et al., 1991) to combinatorial optimization (Favata & Walker, 1991). The fact that similar mappings can be found at various places in the brains of humans and animals indicates that preservation of topology is an important principle at least in natural "signal processing systems".
Reference: <author> Kohonen, T., K. Makisara & T. </author> <title> Saramaki (1984), Phonotopic maps, insightful representation of phonological features for speech recognition, </title> <booktitle> Proc. 7th Int. Conf. on Pattern Recognition, </booktitle> <address> Montreal. </address>
Reference-contexts: This makes a visualization of complex data possible, e.g., speech data <ref> (Kohonen, Makisara & Saramaki, 1984) </ref> or even high-dimensional symbolic descriptions of objects (Ritter & Kohonen, 1989). 12 a) Growing Cell Structures.
Reference: <author> Lang, K. J. & M. J. </author> <title> Witbrock (1989), Learning to tell two spirals apart, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> D. Touretzky, G. </editor> <booktitle> Hinton & T. </booktitle>
Reference: <editor> Sejnowski, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pp. 52-59. </pages> <note> 33 Martinetz, </note> <author> T. M. & K. J. </author> <title> Schulten (1991), A "neural-gas" network learns topologies, in Artificial Neural Networks, </title> <editor> T. Kohonen, K. Makisara, O. Simula & J. Kangas, eds., </editor> <publisher> North-Holland, Amsterdam, </publisher> <pages> pp. 397-402. </pages>
Reference: <author> Mehlhorn, K. & S. </author> <month> Naher </month> <year> (1989), </year> <title> LEDA, a library of efficient data types and algorithms, </title> <institution> Universitat des Saarlandes, Fachbereich Informatik, </institution> <note> TR A 04/89, Saarbrucken. </note>
Reference-contexts: The problem is that the removal of a neuron might require that also other neurons and connections are removed to make the structure consistent again. Simple heuristics as, e.g., 3 Our current implementation of the model is based on LEDA <ref> (see Mehlhorn & Naher, 1989) </ref>, a publicly available library of data types and algorithms. LEDA contains in particular a very elaborated data type "graph". 11 a) Growing Cell Structures. The node d is to be removed.
Reference: <author> Moody, J. & C. </author> <month> Darken </month> <year> (1988), </year> <title> Learning with Localized Receptive Fields, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> D. Touretzky, G. </editor> <booktitle> Hinton & T. </booktitle>
Reference-contexts: The result is a method which resembles the well-known radial basis function network (RBF) but eliminates some serious drawbacks of this approach. 3.2 Radial Basis Functions Radial Basis Function networks <ref> (Moody & Darken, 1988) </ref> consist of a layer L of units with Gaussian activation functions 5 and an output layer of m linear summation units (see fig. 15). We assume again data pairs (~ i 2 R n ; i 2 R m ) of input and desired output.
Reference: <editor> Sejnowski, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pp. 133-143. </pages>
Reference: <author> Rabiner, L. R. & R. W. </author> <title> Schafer (1978), Digital Processing of Speech Signals, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Ritter, H. J. </author> <year> (1991), </year> <title> Learning with the self-organizing map, </title> <booktitle> in Artificial Neural Networks, T. </booktitle>
Reference: <editor> Kohonen, K. Makisara, O. Simula & J. Kangas, eds., </editor> <publisher> North-Holland, Amsterdam, </publisher> <pages> pp. 379-384. </pages>
Reference: <author> Ritter, H. J. & T. </author> <title> Kohonen (1989), Self-Organizing Semantic Maps, </title> <journal> Biological Cybernetics, </journal> <volume> 61, </volume> <pages> pp. 241-254. </pages>
Reference-contexts: This makes a visualization of complex data possible, e.g., speech data (Kohonen, Makisara & Saramaki, 1984) or even high-dimensional symbolic descriptions of objects <ref> (Ritter & Kohonen, 1989) </ref>. 12 a) Growing Cell Structures. The node d is to be removed and consequently also those triangles (two-dimensional hypertetrahedrons) in which d participates. b) Structure after removal of d and the triangles d participated in. <p> 1 1 0 1 1 1 1 0 to fly 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 swim 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 Table 1: Animal names and binary attributes <ref> (after Ritter & Kohonen, 1989) </ref>: If an attribute applies for an animal the corresponding table entry is 1, otherwise 0. still larger benefit can be gained by having the embedding when the input data is so high-dimensional that we can not visualize the network in input vector space anymore. <p> Animals with similar properties are represented in neighboring locations of the map, as is shown by the (manually added) partition into three regions (after <ref> (Ritter & Kohonen, 1989) </ref>). 2.8 Alternative Insertion Criteria One goal of our model, as described so far, is to estimate the unknown probability density of the input signals with the local density of reference vectors in input vector space.
Reference: <author> Robinson, A. J. </author> <year> (1989), </year> <title> Dynamic Error Propagation Networks, </title> <institution> Cambridge University, </institution> <type> PhD Thesis, </type> <address> Cambridge. </address>
Reference-contexts: 12 bits with 27 a) final network with 145 cells b) decision regions removal of cells. vowel word vowel word i: heed O hod I hid C: hoard E head U hood A had u: who'd a: hard 3: heard Y hud Table 3: Words used in recording the vowels <ref> (from Robinson, 1989) </ref> 28 a 10kHz sampling rate. Twelfth order linear predictive analysis was carried out on six 512 sample Hamming windowed segments from the steady part of the vowel. The reflection coefficients were used to calculate 10 log area parameters, giving a 10 dimensional input space. <p> A general introduction to speech processing and an explanation of this technique can be found in e.g. Rabiner & Schafer (1978). Each speaker, thus, yielded six frames of speech from eleven vowels. This gave 990 frames from the fifteen speakers. Robinson used this data in his thesis <ref> (Robinson, 1989) </ref> to investigate several types of neural network algorithms. He used 528 frames from four male and four female speakers to train the networks and used the remaining 462 frames from four male and three female speakers for testing the performance. <p> The table shows the network size, the number of correctly classified test patterns (out of 462), and the corresponding percentage. The upper box shows the results reported by Robinson in his thesis <ref> (Robinson, 1989) </ref>. He got the best classification rate for the nearest neighbor method. The lower box shows the result of several nets generated by the Growing Cell Structures method.
Reference: <author> Robinson, A. J. </author> <year> (1993), </year> <type> (personal communication). </type>
Reference-contexts: Due to the limited computational facilities available to Robinson, he did only one run for each of the different architectures. Every run was continued for about 3000 epochs <ref> (Robinson, 1993) </ref>. To get comparable results, we trained several Growing Cell Structure networks with the same data as Robinson and thereafter used his test data to evaluate the generalization capabilities of the networks.
Reference: <author> Rodrigues, J. S. & L. B. </author> <title> Almeida (1990), Improving the learning speed in topological maps of patterns, </title> <booktitle> Proc. of INNC, </booktitle> <address> Paris. </address>
Reference: <author> Rosenblatt, F. </author> <year> (1958), </year> <title> The perceptron: a probabilistic model for information storage and organization in the brain, </title> <journal> Psychological Review, </journal> <volume> 65, </volume> <pages> pp. 386-408. </pages>
Reference: <author> Schweizer, L., G. Parladori, G. L. Sicuranza & S. </author> <month> Marsi </month> <year> (1991), </year> <title> A fully neural approach to image compression, in Artificial Neural Networks, </title> <editor> T. Kohonen, K. Makisara, </editor> <address> O. </address>
Reference-contexts: These mappings are able to preserve neighborhood relations in the input data and have the property to represent regions of high signal density on correspondingly large parts of the topological structure. This makes them interesting for applications in various areas ranging from speech recognition (Kohonen, 1988) and data compression <ref> (Schweizer et al., 1991) </ref> to combinatorial optimization (Favata & Walker, 1991). The fact that similar mappings can be found at various places in the brains of humans and animals indicates that preservation of topology is an important principle at least in natural "signal processing systems".
Reference: <editor> Simula & J. Kangas, eds., </editor> <publisher> North-Holland, Amsterdam, </publisher> <pages> pp. 815-820. </pages>
Reference: <author> Willshaw, D. J. & C. </author> <title> von der Malsburg (1976), How Patterned Neural Connections Can Be Set Up by Self-Organization, </title> <journal> Proceedings of the Royal Society of London B, </journal> <volume> 194, </volume> <pages> pp. 431-445. </pages>
Reference: <author> Xu, L. </author> <year> (1990), </year> <title> Adding learning expectation into the learning procedure of self-organizing maps, </title> <journal> Int. Journal of Neural Systems, </journal> <volume> 1, </volume> <pages> pp. 269-283. 34 </pages>
References-found: 30

