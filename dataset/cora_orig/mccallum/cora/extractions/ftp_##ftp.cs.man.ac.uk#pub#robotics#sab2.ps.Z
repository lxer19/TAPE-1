URL: ftp://ftp.cs.man.ac.uk/pub/robotics/sab2.ps.Z
Refering-URL: http://www.cs.man.ac.uk/robotics/lopapers.html
Root-URL: 
Email: E-mail:ulrich@castle.ed.ac.uk, B.O.McGonigle@ed.ac.uk  E-mail:tim@arti1.vub.ac.be  
Phone: Telephone +4431 650 8449 Fax +4431 650 6512  Tel +322 641 2965 Fax +322 641 3582  
Title: Increasing Behavioural Repertoire in a Mobile Robot simply adding further so-called instinct-rules without altering the
Author: Ulrich Nehmzow Tim Smithers and Brendan McGonigle 
Note: Experiments with mobile robots are presented that show that this is possible by  
Address: GB Edinburgh EH8 9LE  Pleinlaan 2 B 1050 Brussel  
Affiliation: Laboratory for Cognitive Neuroscience at the Department of Psychology University of Edinburgh Appleton Tower  Artificial Intelligence Laboratory Vrije Universiteit Brussel  
Abstract: This paper presents an investigation of the the suitability of the robot controller presented in [Nehmzow et al. 89] and [Nehmzow et al. 90] for a computationally cheap expansion of the be-havioural repertoire of a mobile robot. 
Abstract-found: 1
Intro-found: 1
Reference: [Brooks 91] <author> Rodney Brooks, </author> <title> Artificial Life and Real Robots, </title> <booktitle> Proceedings of 1st European Conference on Artificial Life, </booktitle> <address> Paris 1991, </address> <publisher> published by MIT Press 1992 </publisher>
Reference-contexts: Here, LISP programs are determined by a genetic algorithm. This approach is promising, however the resulting programs are not yet complex enough to actually control robots (see <ref> [Brooks 91] </ref>). 2 The Controller Architecture For all experiments described in this paper the same controller architecture was used. New behaviours are acquired by the robot by autonomously determining the effective wiring between sensory input and motor actions in a trial and error process.
Reference: [Kaelbling 90] <author> Leslie Kaelbling, </author> <title> Learning in Embedded Systems, </title> <type> PhD Thesis, Stanford Technical Report TR-90-04, </type> <year> 1990 </year>
Reference-contexts: The controller architecture used in the experiments reported here is shown in figure 3. Its central element is a connectionist associative memory (Perceptron-like) 1 Investigations into reinforcement learning architectures that might be used for robot control are presented by many researchers, for example by <ref> [Kaelbling 90, Sutton 91, Prescott & Mayhew] </ref>, but these have not been conducted using real robots. which stores the effective associations between input signals and motor actions. The performance of the robot is assessed through instinct-rules, which are fixed, predefined rules such as "Keep whiskers straight" 2 .
Reference: [Kaelbling 91] <author> Leslie Kaelbling, </author> <title> An Adaptable Mobile Robot, </title> <booktitle> Proceedings of 1st European Conference on Artificial Life, </booktitle> <address> Paris 1991, </address> <publisher> published by MIT Press 1992 </publisher>
Reference-contexts: This increases the robot's flexibility to accomplish new tasks. 1.1 Related Work Some work has been done on the acquisition of single competences in autonomous mobile robots, for example by [Mahadevan & Connell 91], [Maes & Brooks 90] and <ref> [Kaelbling 91] </ref>. Mahadevan and Connell and Kael-bling use reinforcement learning to control robots acquiring the competences of box pushing and phototaxis (moving towards a light source) respectively.
Reference: [Koza 90] <author> John Koza, </author> <title> Evolution and Co-Evolution of Computer Programs to Control Independently Acting Agents, From Animals to Animats, </title> <publisher> MIT Press 1991 </publisher>
Reference: [Maes & Brooks 90] <author> Pattie Maes and Rodney Brooks, </author> <title> Learning to Coordinate Behaviours, </title> <booktitle> National Conference on Artificial Intelligence 1990 </booktitle>
Reference-contexts: This increases the robot's flexibility to accomplish new tasks. 1.1 Related Work Some work has been done on the acquisition of single competences in autonomous mobile robots, for example by [Mahadevan & Connell 91], <ref> [Maes & Brooks 90] </ref> and [Kaelbling 91]. Mahadevan and Connell and Kael-bling use reinforcement learning to control robots acquiring the competences of box pushing and phototaxis (moving towards a light source) respectively.
Reference: [Mahadevan & Connell 91] <author> Sridhar Mahadevan and Jonathan Connell, </author> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> 9th National Conference on Artificial Intelligence 1991 </booktitle>
Reference-contexts: This increases the robot's flexibility to accomplish new tasks. 1.1 Related Work Some work has been done on the acquisition of single competences in autonomous mobile robots, for example by <ref> [Mahadevan & Connell 91] </ref>, [Maes & Brooks 90] and [Kaelbling 91]. Mahadevan and Connell and Kael-bling use reinforcement learning to control robots acquiring the competences of box pushing and phototaxis (moving towards a light source) respectively.
Reference: [Nehmzow et al. 89] <author> UIrich Nehmzow, John Hallam and Tim Smithers, </author> <title> Really Useful Robots, </title> <editor> in T. Kanade, F.C.A. Groen and L.O. Hertzberger (eds.), </editor> <booktitle> Intelligent Autonomous Systems 2, </booktitle> <address> Amsterdam 1989 </address>
Reference: [Nehmzow et al. 90] <author> Ulrich Nehmzow, Tim Smithers and John Hallam, </author> <title> Steps towards Intelligent Robots, DAI Research Paper No. </title> <type> 502, </type> <institution> Department of Artificial Intelligence, Edinburgh University, </institution> <year> 1990 </year>
Reference: [Nehmzow 92] <author> Ulrich Nehmzow, </author> <title> Experiments in Competence Acquisition for Autonomous Mobile Robots, </title> <type> PhD Thesis, </type> <institution> University of Edinburgh 1992 </institution>
Reference-contexts: To deal with these through a priori defined strategies is (in practice) impossible. One possible solution to the problem of coping with such unforeseen situations is the use of self-organising controllers that determine the effective wiring between sensors and actuators autonomously. Such an approach is described in <ref> [Nehmzow 92] </ref>, for example. An additional property of the proposed architecture is discussed in this paper: that it allows an easy expansion of the robot's behavioural repertoire by adding so-called instinct-rules, without necessitating changes to the controller architecture itself. <p> There is no notion of a bent whisker being identical with an obstacle, for instance. 3 For more details see <ref> [Nehmzow 92] </ref>. 4 If more than one instinct-rule is used instinct-rules are tested for violation beginning at the latest (the newest) and ending at the first instinct-rule. 5 This value is mostly dependent on the velocity of the robot. 6 At the beginning of the learning phase, when no associations are <p> guiding helps, but is not required. * Because the controller is self-organising, it can not only determine the effective connections between sensors and actuators in the first place, it can also reestablish them in the event of unforeseen situations (swapped whiskers, swapped motor connections, change of environment etc., see also <ref> [Nehmzow 92] </ref>). 4.3 Comparision to Biological Systems The system described thus far is certainly based on the designer's interpretation of a given niche within which the robot is supposed to work (office or laboratory environment with smooth, level floors, containing walls and box-like obstacles). <p> an easy expansion of the behavioural repertoire of a mobile robot, as described in this paper, the architecture presented gives a higher degree of flexibility in unforeseen circumstances: the robot becomes able to cope with changes in its own morphology, changes of the task and changes in the environment (see <ref> [Nehmzow 92] </ref>). Acknowledgements The work reported here was supported by grant GR/F/5852.3 from the Science and Engineering Research Council. Other facilities and technical support were provided by the Department of Artificial Intelligence at Edinburgh University.
Reference: [Prescott & Mayhew] <author> Tony Prescott and John Mayhew, </author> <title> Obstacle Avoidance through Reinforcement Learning, </title> <note> to appear in J.E. </note> <editor> Moody, S.J. Hanson and R.P. Lipp-mann (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> Sam Mateo, </address> <publisher> Morgan Kaufman, no year </publisher>
Reference-contexts: The controller architecture used in the experiments reported here is shown in figure 3. Its central element is a connectionist associative memory (Perceptron-like) 1 Investigations into reinforcement learning architectures that might be used for robot control are presented by many researchers, for example by <ref> [Kaelbling 90, Sutton 91, Prescott & Mayhew] </ref>, but these have not been conducted using real robots. which stores the effective associations between input signals and motor actions. The performance of the robot is assessed through instinct-rules, which are fixed, predefined rules such as "Keep whiskers straight" 2 .
Reference: [Sherry & Shacter 87] <author> D.F. Sherry and D.L. Shacter, </author> <title> The Development of Multiple Memory Systems, </title> <journal> Psychological Review, </journal> <volume> Vol 94, </volume> <pages> pp. 439-454, </pages> <year> 1987 </year>
Reference-contexts: There is no evidence, moreover, that declarative types of knowledge based on episodic or semantic forms of memory derive from a procedural motor (sub) system. On the contrary, as <ref> [Sherry & Shacter 87] </ref> have argued, separate multiple memory systems have evolved to cater for different types of achievement. It would be somewhat unwise, therefore, to propose a motor (based) learning system to support a general purpose kind of knowledge acquisition.
Reference: [Sutton 91] <author> Richard Sutton, </author> <title> Reinforcement Learning Architectures for Animats, From Animals to Animats, </title> <publisher> MIT Press 1991 </publisher>
Reference-contexts: The controller architecture used in the experiments reported here is shown in figure 3. Its central element is a connectionist associative memory (Perceptron-like) 1 Investigations into reinforcement learning architectures that might be used for robot control are presented by many researchers, for example by <ref> [Kaelbling 90, Sutton 91, Prescott & Mayhew] </ref>, but these have not been conducted using real robots. which stores the effective associations between input signals and motor actions. The performance of the robot is assessed through instinct-rules, which are fixed, predefined rules such as "Keep whiskers straight" 2 .
References-found: 12

