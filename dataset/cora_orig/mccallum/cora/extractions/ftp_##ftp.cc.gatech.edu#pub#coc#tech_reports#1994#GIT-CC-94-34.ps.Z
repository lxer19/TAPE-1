URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1994/GIT-CC-94-34.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.94.html
Root-URL: 
Title: Evaluation of Causal Distributed Shared Memory for Data-race-free Programs  
Author: Ranjit John Mustaque Ahamad 
Note: This research was supported in part by NSF grant CCR-9106627 and ARPA contract N00174-93-K-0150.  
Address: Atlanta, GA 30332-0280 USA GIT-CC-94/34  Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  College of Computing Georgia Institute of Technology  
Abstract: Distributed Shared Memory (DSM) is becoming an accepted abstraction for programming distributed systems. Although DSM could simplify the programming of distributed applications, maintaining a consistent view of shared memory operations across processors in a distributed system can be expensive. The causal consistency model of DSM can allow more efficient implementations of DSM because it requires that only causally ordered memory operations be viewed in the same order at different processors. Also, weakly ordered systems have been proposed which advocate the use of synchronization information to reduce the frequency of communication between processors. We have implemented a system that exploits both the weaker consistency of causal memory and the synchronization information used in weakly ordered systems. Consistency is ensured by locally invalidating data that is suspected to be causally overwritten and this is only done when certain synchronization operations complete at a processor. Data-race-free programs can be developed in this system assuming that the system provided sequentially consistent memory. By implementing applications that have a variety of data sharing patterns, we show that performance close to message passing implementations of the applications can be achieved in the causal DSM system. The improved performance is due to a significant reduction in communication costs compared to a strongly consistent memory system. These results show that causal memory can meet the consistency and performance requirements of many distributed applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Release consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of a critical section (a release operation on a synchronization variable). Such systems guarantee sequentially consistent behavior only for programs that are data-race-free <ref> [1] </ref> or properly labeled [21]. <p> When programs use sufficient synchronization to control access to shared data, they need not be aware of the optimizations carried out by the hardware and can simply assume that it provides sequentially consistent memory. Adve and Hill <ref> [1] </ref> formalized this and introduced the notion of data-race-free programs. <p> Thus, the optimizations made possible by RC in implementing memory consistency do not increase the complexity of programming. Adve and Hill <ref> [1] </ref>, also developed a similar approach and introduced the notion of data-race-free programs. Hybrid consistency [8] and buffered consistency [38] are examples of other memory models that are based on classifying synchronization operations and defining coherence relative to them.
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report 1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year> <month> 41 </month>
Reference-contexts: Boyer [15] describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance over conventional atomic DSM. 40 There have been several hardware implementations of weakly ordered systems and are described in <ref> [21, 2, 39, 14, 19] </ref>. 6 Concluding Remarks We have presented a DSM system based on causal memory and have given two implementations for it. These implementations make use of the optimizations made possible by weakly ordered as well as weakly consistent systems.
Reference: [3] <author> Divyakant Agrawal, Manhoi Choy, Hong Va Leong, and Ambuj K. Singh. </author> <title> Mixed consistency: A model for parallel programming. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1994. </year>
Reference-contexts: In particular, any new 3 Recently, the Maya system has also proposed a causal memory system that includes orderings induced by synchronization operations <ref> [3] </ref>. 7 causal orderings that get established by the reading of the newly cached data value must not cause the existing data to be overwritten in the causal sense.
Reference: [4] <author> Mustaque Ahamad, Rida Bazzi, Ranjit John, Prince Kohli, and Gil Neiger. </author> <title> The power of processor consistency. </title> <booktitle> In Proceedings of the 5th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: The weaker consistency does not guarantee that the execution of memory operations of all processors is equivalent to some sequential execution of these operations, as in a sequentially consistent system. Examples of weakly consistent memory systems included pipelined RAM (PRAM) [34], processor consistency 1 <ref> [4, 21, 22] </ref>, and causal memory [5]. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack of strong consistency. <p> By choosing the set of operations to be included in a processor view and the orders that must be maintained between them, we have developed implementation independent definitions of several memory systems <ref> [4, 29] </ref>. We now present such a characterization of causal memory. 2.2 Defining Causal Memory Causal memory requires that values returned by read operations respect the causal order between memory operations.
Reference: [5] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Examples of weakly consistent memory systems included pipelined RAM (PRAM) [34], processor consistency 1 [4, 21, 22], and causal memory <ref> [5] </ref>. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack of strong consistency. <p> Other memory models have been proposed that make the weaker consistency of a memory system visible to the programmers. Examples of these include pipelined RAM (PRAM) [34], processor consistency (PC) [22, 21], and causal and slow memories <ref> [5, 24] </ref>. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency.
Reference: [6] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: In this paper, we explore efficient implementations of DSM by exploiting ideas from both approaches described above | we make use of synchronization information, as advocated by the weakly ordered memory systems approach, for implementing causal memory <ref> [6] </ref>, which is a weakly consistent memory. The causal memory model requires that a read operation return a value that is consistent with the causal order of the memory operations that are ordered before it. <p> In Section 4 we describe the applications with which we experimented and provide performance results. We discuss related work in Section 5 and conclude in Section 6. 2 Causal Memory 2.1 The Model Causal memory has been defined in <ref> [6] </ref> by characterizing the possible values that could be returned when a read operation is executed by a processor. We use a more general framework here, as it allows us to easily relate causal memory to a range of memory models that have been proposed. <p> One of the processors that caches a current copy of x is also called its owner. The identity of the owner processor for a data item is known to the manager of the data item. Similar to the implementation developed in <ref> [6] </ref>, the algorithm that we develop ensures that data items cached at a processor are mutually consistent from the point of view of causal consistency. In other words, one cannot violate the consistency requirements of 8 causal memory by reading locally cached data. <p> As a result, some of the cached data values can become overwritten (the reading of an overwritten value will violate causal consistency). In the algorithm presented in <ref> [6] </ref>, we invalidated cached data items that could be potentially overwritten when a new data item was added to the cache.
Reference: [7] <author> Mustaque Ahamad, Gil Neiger, Prince Kohli, James E. Burns, and Phillip W. Hutto. </author> <title> Causal memory: Definitions, implementation and programming. </title> <type> Technical Report GIT-CC-93-55, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Thus, programming of this class of programs is not made more complex when we use causal instead of sequentially consistent memory. This is formally proved in <ref> [7] </ref>. Intuitively, it follows from the fact that writes to a location must all be ordered by the happens before relation, hb !, as there can be no conflicting writes.
Reference: [8] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high-performance multiprocessors. Extended Abstract, </title> <institution> Department of Computer Science, The Technion,Haifa, Israel, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Thus, the optimizations made possible by RC in implementing memory consistency do not increase the complexity of programming. Adve and Hill [1], also developed a similar approach and introduced the notion of data-race-free programs. Hybrid consistency <ref> [8] </ref> and buffered consistency [38] are examples of other memory models that are based on classifying synchronization operations and defining coherence relative to them.
Reference: [9] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report Report RNR-91-002, </type> <institution> NAS Systems Division, Applied Research Branch, NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Finally, we present and analyze the experimental results. 4.1 Applications We have implemented a number of applications to evaluate causal memory. The applications include Embarrassingly Parallel (EP), Integer Sort (IS), and Conjugate Gradient Method (CGM) from the NASA Ames NAS kernels <ref> [9] </ref>, and traveling salesperson (TSP), matrix multiplication (MM), and successive over-relaxation (SOR). These applications have been used in the study of several distributed shared memory systems.
Reference: [10] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Experience with distributed programming in Orca. </title> <booktitle> In In Intl. Conf. on Computer Languages, </booktitle> <year> 1990. </year>
Reference-contexts: We ran the program for a 512 fi 512 size grid. * TSP is unique because of the high degree of dynamic behavior of data sharing exhibited by it. Our implementation is similar to the one reported by Bal et al. <ref> [10] </ref> and uses a branch-and-bound method. A set of partial tours are generated and processors evaluate these partial tours in parallel. They all share a work queue that stores the partial tours. The value of the best tour that has been found so far is 24 also shared.
Reference: [11] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 125-135, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: There could be concurrent writes to the page because of false sharing even when synchronization controls access to data stored in the page. The diff mechanism employed in <ref> [11, 26] </ref> can be used to handle the merging problem but it does have copying and processing overheads, which we have found to be quite high [25]. <p> Ivy, the first DSM system, implemented sequentially consistent memory by using a writer-invalidates-readers protocol. In Mirage [20], pinning was implemented to avoid certain problems in the Ivy protocol. Munin <ref> [11] </ref>, implemented a family of protocols, including the first software implementation of RC. Munin showed that performance competitive with hand coded message passing programs can be achieved if data sharing patterns of an application can be identified and appropriate consistency protocols can be associated with shared data items.
Reference: [12] <author> John K. Bennett, John B. Carter, and Willy Zwaenpoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Hardware implementations of weakly ordered systems use optimizations such as instruction reordering, pipelining and write-buffering but still maintain a coherent cache. In contrast, software implementations sacrifice coherence by delaying consistency related operations to certain specific points in the program execution. The Munin system <ref> [12] </ref> implements release consistency in software by delaying propagation of the changes made inside a critical section till the release operation. Munin also identified several data sharing patterns and corresponding annotations that users can provide to reduce the cost of consistency maintenance in a DSM system. <p> Such data, which does not change during the course of program execution, could get unnecessarily invalidated when the consistency maintenance operations are executed. One solution is to use an approach similar to the annotations used in the Munin system <ref> [12] </ref>. We provide language level support, where the user can annotate such data as read-only or write-once. Such data pages are not considered for invalidation. This annotation is used only for improving performance and is not required for correct execution.
Reference: [13] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Munin also identified several data sharing patterns and corresponding annotations that users can provide to reduce the cost of consistency maintenance in a DSM system. More recently, lazy release consistency [26] and entry consistency <ref> [13] </ref> memory models have been proposed, which use synchronization information to further reduce communication by propagating information about changes to shared data to only the processor that next acquires the lock. <p> Hybrid consistency [8] and buffered consistency [38] are examples of other memory models that are based on classifying synchronization operations and defining coherence relative to them. Lazy release consistency (LRC) [26] and entry consistency (EC) <ref> [13] </ref>, both of which were proposed after RC, make even more aggressive use of synchronization in performing coherence actions.
Reference: [14] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A distributed shared memory system. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> June </month> <year> 1990. </year> <month> 42 </month>
Reference-contexts: Boyer [15] describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance over conventional atomic DSM. 40 There have been several hardware implementations of weakly ordered systems and are described in <ref> [21, 2, 39, 14, 19] </ref>. 6 Concluding Remarks We have presented a DSM system based on causal memory and have given two implementations for it. These implementations make use of the optimizations made possible by weakly ordered as well as weakly consistent systems.
Reference: [15] <author> Fabienne Boyer. </author> <title> A causal distributed shared memory based on external pagers. </title> <booktitle> In Proceedings of the 2nd Usenix Mach Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Thus, it could provide better performance but requires that programmers do additional work to specify the associations. Since its performance must be between message passing and causal memory, we believe the differences in these systems will not be significant. Other implementations of DSM systems have also been reported. Boyer <ref> [15] </ref> describes an implementation of causal DSM on Mach using external pagers.
Reference: [16] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating System Principles, </booktitle> <year> 1989. </year>
Reference-contexts: The matrices were of size 256 fi 256. * SOR is an iterative method for solving discretized Laplace equations on a grid. The program is based on the parallel red/black SOR algorithm as described by Chase et. al. <ref> [16] </ref>. The grid is partitioned among the processors and all the communication occurs between neighboring processors. Only the boundary elements of the grid need to be communicated between iterations.
Reference: [17] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <year> 1993. </year>
Reference-contexts: Thus, consistency is guaranteed even when copying is not done. Recently techniques have been proposed that can be used to reduce copying overhead in message passing systems <ref> [33, 17] </ref>. However, they require additional flexibility from the underlying operating system. 3.5.2 Avoiding Page Transfers on Double Faults A double fault occurs when a page that is not cached locally is first read and then written [28].
Reference: [18] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: One way to reduce communication between processors in shared memory system is to guarantee consistency only at certain points in the execution of a program. This approach was first outlined by Dubois et al. <ref> [18] </ref>, who observed that parallel programs define their own consistency requirements through the use of synchronization operations. Dubois et al. define a weakly ordered system, where synchronization operations are made explicit to the memory system and consistency maintenance is done only at synchronization points. <p> We also relate causal memory to other weakly consistent memory systems. 5.1 Memory Models The use of synchronization information in coherence maintenance was first advocated by Dubois and Scheurich <ref> [18] </ref> for multiprocessor systems. They argued that only the execution of synchronization operations needs to be sequentially consistent as long as coherence actions for other memory operations complete by the time a following synchronization operation is executed.
Reference: [19] <author> Michel Dubois, Jin Chin Wang, Luiz A. Barroso, Kangwoo Lee, and Yung-Syau Chen. </author> <title> Delayed consistency and its effects on the miss rate of parallel programs. </title> <type> Technical report, </type> <institution> Dept. of Electrical Engineering Systems, USC, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Boyer [15] describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance over conventional atomic DSM. 40 There have been several hardware implementations of weakly ordered systems and are described in <ref> [21, 2, 39, 14, 19] </ref>. 6 Concluding Remarks We have presented a DSM system based on causal memory and have given two implementations for it. These implementations make use of the optimizations made possible by weakly ordered as well as weakly consistent systems.
Reference: [20] <author> B. D. Fleisch and G. J. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the ACM Symposium on Operating System Principles, </booktitle> <year> 1989. </year>
Reference-contexts: Thus, a page cannot be cached at multiple processors with write access at any time. This does, however, serialize concurrent writes to a page. We pin a page to a processor <ref> [20] </ref> for a certain amount of time to control the situation where concurrent writers move a page between processors continuously (page thrashing). It must be noted that false sharing, where a single writer is concurrent with multiple readers, does not create any problems in our implementation of causal memory. <p> In his pioneering work, Li adapted a multiprocessor cache coherence protocol to develop a software based implementation of shared memory in a distributed system. Although several improvements have been suggested to this approach <ref> [20, 28] </ref>, exploiting synchronization information and weaker consistency for shared memory represent two significant advancements in handling the performance problems associated with DSM systems. In this section, we compare the causal memory system presented in this paper to systems that make use of synchronization information in maintaining coherence. <p> Ivy, the first DSM system, implemented sequentially consistent memory by using a writer-invalidates-readers protocol. In Mirage <ref> [20] </ref>, pinning was implemented to avoid certain problems in the Ivy protocol. Munin [11], implemented a family of protocols, including the first software implementation of RC.
Reference: [21] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Dubois et al. define a weakly ordered system, where synchronization operations are made explicit to the memory system and consistency maintenance is done only at synchronization points. The DASH multiprocessor <ref> [21] </ref> is a weakly ordered system that implements a memory model called release consistency. Release consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of a critical section (a release operation on a synchronization variable). <p> Release consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of a critical section (a release operation on a synchronization variable). Such systems guarantee sequentially consistent behavior only for programs that are data-race-free [1] or properly labeled <ref> [21] </ref>. In other words, when these programs are executed on a sequentially consistent memory system, conflicting accesses to the same shared location (two writes or a read and write to the same location conflict) will always be separated by accesses to synchronization variables in the equivalent serial order. <p> The weaker consistency does not guarantee that the execution of memory operations of all processors is equivalent to some sequential execution of these operations, as in a sequentially consistent system. Examples of weakly consistent memory systems included pipelined RAM (PRAM) [34], processor consistency 1 <ref> [4, 21, 22] </ref>, and causal memory [5]. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack of strong consistency. <p> In other memory models, the ordering between the local operations of a processor could be partial <ref> [21] </ref>. 3 * Causal order: The happens before relation defined by Lamport [30] can be adapted to a shared memory system; this captures the causal relationship between the read and write operations. <p> Figure 2 shows an execution that is permitted by causal memory which is not allowed by processor consistent memory as implemented by the DASH multiprocessor <ref> [21] </ref>. Causal memory allows concurrent writes to a memory location to be read in any order by different processors. The DASH implementation of processor consistency requires memory to be coherent, that is, writes to a single memory location are serialized and observed in the same order by all processors. <p> This approach can provide im-proved performance because execution of a non-synchronization operation is not delayed until communication completes between processors. Release consistency (RC), proposed by Gharachorloo et. al., further refined the weakly ordered approach by dividing synchronization operations in two types: acquire and release <ref> [21] </ref>. It is only necessary that coherence actions are performed before a release operation is completed. <p> Other memory models have been proposed that make the weaker consistency of a memory system visible to the programmers. Examples of these include pipelined RAM (PRAM) [34], processor consistency (PC) <ref> [22, 21] </ref>, and causal and slow memories [5, 24]. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency. <p> Boyer [15] describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance over conventional atomic DSM. 40 There have been several hardware implementations of weakly ordered systems and are described in <ref> [21, 2, 39, 14, 19] </ref>. 6 Concluding Remarks We have presented a DSM system based on causal memory and have given two implementations for it. These implementations make use of the optimizations made possible by weakly ordered as well as weakly consistent systems.
Reference: [22] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 1006, </type> <institution> University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: The weaker consistency does not guarantee that the execution of memory operations of all processors is equivalent to some sequential execution of these operations, as in a sequentially consistent system. Examples of weakly consistent memory systems included pipelined RAM (PRAM) [34], processor consistency 1 <ref> [4, 21, 22] </ref>, and causal memory [5]. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack of strong consistency. <p> Other memory models have been proposed that make the weaker consistency of a memory system visible to the programmers. Examples of these include pipelined RAM (PRAM) [34], processor consistency (PC) <ref> [22, 21] </ref>, and causal and slow memories [5, 24]. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency.
Reference: [23] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: We use a more general framework here, as it allows us to easily relate causal memory to a range of memory models that have been proposed. The model is motivated by the ones used by Misra [36] and by Herlihy and Wing <ref> [23] </ref>. We define the system to be a finite set of processors that interact via a shared memory consisting of a finite set of locations. Processors execute read and write operations. Each such operation acts on a named location and has an associated value.
Reference: [24] <author> Phil W. Hutto and Mustaque Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distrbuted shared memories. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 302-311, </pages> <year> 1990. </year>
Reference-contexts: Other memory models have been proposed that make the weaker consistency of a memory system visible to the programmers. Examples of these include pipelined RAM (PRAM) [34], processor consistency (PC) [22, 21], and causal and slow memories <ref> [5, 24] </ref>. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency.
Reference: [25] <author> Ranjit John, Mustaque Ahamad, Umakishore Ramachandran, R. Ananthanarayan, and Ajay Mohindra. </author> <title> An evaluation of state sharing techniques in distributed operating systems. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, Atlanta, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The diff mechanism employed in [11, 26] can be used to handle the merging problem but it does have copying and processing overheads, which we have found to be quite high <ref> [25] </ref>. We deal with false sharing by making the restriction that a page can only be accessed by a single writer and multiple readers at a given time. Thus, a page cannot be cached at multiple processors with write access at any time. <p> We use synchronization to propagate causal dependencies whereas TreadMarks uses it to identify the processors at which data must be made consistent. TreadMarks handles multiple writers by creating page copies and by using diff operations. We have found that the processing overhead associated with diff operations can be significant <ref> [25] </ref>. The other difference between causal memory implementation and TreadMarks is in the amount of state which is maintained to track pages that have been actually modified. TreadMarks tries to identify exactly the pages which have been modified and invalidates 39 only those pages. <p> A natural question is how it compares with the implementations of RC, LRC or EC. We have also implemented RC and our results show that causal memory performs better than RC <ref> [25] </ref>. This is because RC does not reduce the number of messages; invalidation or update messages have to be sent to all processors caching the modified data items when a release operation is executed. Causal memory does not send any messages on a release operation.
Reference: [26] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium of Computer Architecture, </booktitle> <year> 1992. </year> <month> 43 </month>
Reference-contexts: Munin also identified several data sharing patterns and corresponding annotations that users can provide to reduce the cost of consistency maintenance in a DSM system. More recently, lazy release consistency <ref> [26] </ref> and entry consistency [13] memory models have been proposed, which use synchronization information to further reduce communication by propagating information about changes to shared data to only the processor that next acquires the lock. <p> There could be concurrent writes to the page because of false sharing even when synchronization controls access to data stored in the page. The diff mechanism employed in <ref> [11, 26] </ref> can be used to handle the merging problem but it does have copying and processing overheads, which we have found to be quite high [25]. <p> Adve and Hill [1], also developed a similar approach and introduced the notion of data-race-free programs. Hybrid consistency [8] and buffered consistency [38] are examples of other memory models that are based on classifying synchronization operations and defining coherence relative to them. Lazy release consistency (LRC) <ref> [26] </ref> and entry consistency (EC) [13], both of which were proposed after RC, make even more aggressive use of synchronization in performing coherence actions.
Reference: [27] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: User specified annotations were used in Munin for this purpose. LRC is implemented in the TreadMarks system <ref> [27] </ref> and the Midway system implements EC. In LRC, writes to shared data are propagated when locks are transferred between processors.
Reference: [28] <author> R. E. Kessler and M. Livny. </author> <title> An analysis of distributed shared memory algorithms. </title> <booktitle> In Proceedings of the 9th International Conference on Distributed Computing Systems, </booktitle> <year> 1989. </year>
Reference-contexts: However, they require additional flexibility from the underlying operating system. 3.5.2 Avoiding Page Transfers on Double Faults A double fault occurs when a page that is not cached locally is first read and then written <ref> [28] </ref>. This would cause a page to be transferred once due to the read and again due to the write. The second page transfer will occur because the processor is not the owner of the page when the protection fault is handled. <p> We optimized it in several ways. For example, we use pinning to control thrashing and also use a technique similar to the one described by Kessler and Livny <ref> [28] </ref> to avoid re-sending a page due to a double page fault. All of the DSM protocols are implemented in the Clouds operating system using low level communication mechanisms. <p> In his pioneering work, Li adapted a multiprocessor cache coherence protocol to develop a software based implementation of shared memory in a distributed system. Although several improvements have been suggested to this approach <ref> [20, 28] </ref>, exploiting synchronization information and weaker consistency for shared memory represent two significant advancements in handling the performance problems associated with DSM systems. In this section, we compare the causal memory system presented in this paper to systems that make use of synchronization information in maintaining coherence.
Reference: [29] <author> Prince Kohli, Gil Neiger, and Mustaque Ahamad. </author> <title> A characterization of scalable shared memories. </title> <booktitle> In Proceedings of the 22nd International Conference of Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: By choosing the set of operations to be included in a processor view and the orders that must be maintained between them, we have developed implementation independent definitions of several memory systems <ref> [4, 29] </ref>. We now present such a characterization of causal memory. 2.2 Defining Causal Memory Causal memory requires that values returned by read operations respect the causal order between memory operations.
Reference: [30] <author> Leslie Lamport. </author> <title> Time, clocks and the ordering of events. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: In other memory models, the ordering between the local operations of a processor could be partial [21]. 3 * Causal order: The happens before relation defined by Lamport <ref> [30] </ref> can be adapted to a shared memory system; this captures the causal relationship between the read and write operations.
Reference: [31] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes mul-tiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: A DSM is an interface between the program and the memory system, and the memory model implemented by the DSM system defines what values can be returned when processors read shared data. Ideally, a distributed shared memory should provide all the consistency guarantees of a true shared memory. Lamport <ref> [31] </ref> defined a memory model called sequential consistency which requires that the execution of all processors must be equivalent to some sequential order in which memory operations are executed and read operations return values consistent with this order.
Reference: [32] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The sequential order must maintain the order of memory operations issued by a particular processor (program order). In the first system that implemented DSM <ref> [32] </ref>, a writer-invalidate-readers protocol was used to provide sequentially consistent DSM. Maintaining sequential consistency in a distributed system can be shown to limit performance and does not lend itself to scaling [34] due to high latencies and communication costs. <p> Communication with other processors may be required when the data to be accessed is not locally cached or when it is written. To locate a data item, we introduce the notion of a manager as is used by Li and Hudak <ref> [32] </ref>. A manager is a processor that either caches a data item x assigned to it or knows the identity of the processor that caches a current copy of x. One of the processors that caches a current copy of x is also called its owner. <p> The sequentially consistent DSM protocol implements a fixed manager writer 5 In Section 5, we address how the performance of causal memory compares with other memory systems such as release consistency, lazy release consistency and others. 22 invalidate-readers protocol similar to the one described by Li and Hudak <ref> [32] </ref>. We optimized it in several ways. For example, we use pinning to control thrashing and also use a technique similar to the one described by Kessler and Livny [28] to avoid re-sending a page due to a double page fault.
Reference: [33] <author> Jochen Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <year> 1993. </year>
Reference-contexts: Thus, consistency is guaranteed even when copying is not done. Recently techniques have been proposed that can be used to reduce copying overhead in message passing systems <ref> [33, 17] </ref>. However, they require additional flexibility from the underlying operating system. 3.5.2 Avoiding Page Transfers on Double Faults A double fault occurs when a page that is not cached locally is first read and then written [28].
Reference: [34] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: In the first system that implemented DSM [32], a writer-invalidate-readers protocol was used to provide sequentially consistent DSM. Maintaining sequential consistency in a distributed system can be shown to limit performance and does not lend itself to scaling <ref> [34] </ref> due to high latencies and communication costs. One way to reduce communication between processors in shared memory system is to guarantee consistency only at certain points in the execution of a program. <p> The weaker consistency does not guarantee that the execution of memory operations of all processors is equivalent to some sequential execution of these operations, as in a sequentially consistent system. Examples of weakly consistent memory systems included pipelined RAM (PRAM) <ref> [34] </ref>, processor consistency 1 [4, 21, 22], and causal memory [5]. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack <p> of weakly consistent memory systems included pipelined RAM (PRAM) <ref> [34] </ref>, processor consistency 1 [4, 21, 22], and causal memory [5]. To execute applications in such weakly consistent memory systems, either the applications must have data sharing patterns that are not effected by the weaker consistency (e.g., conservative programs for PRAM [34]), or the program must explicitly deal with the lack of strong consistency. <p> The DASH implementation of processor consistency requires memory to be coherent, that is, writes to a single memory location are serialized and observed in the same order by all processors. For this reason, the execution would not be permitted by processor consistent memory. Pipelined RAM <ref> [34] </ref> is strictly weaker than causal memory because it only requires that processors order two write operations in the same order in their views only if the writes are executed by the same processor. p 1 : w (x)1 p 3 : r (x)1 r (x)2 5 2.3 Synchronization Operations Causal <p> Other memory models have been proposed that make the weaker consistency of a memory system visible to the programmers. Examples of these include pipelined RAM (PRAM) <ref> [34] </ref>, processor consistency (PC) [22, 21], and causal and slow memories [5, 24]. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency. <p> Examples of these include pipelined RAM (PRAM) <ref> [34] </ref>, processor consistency (PC) [22, 21], and causal and slow memories [5, 24]. Programmers must either show that programs are not affected by the weaker consistency (e.g., conservative programs with respect to PRAM [34]) or must include code in their applications that deals with such weak consistency. It is argued that better performance can be achieved in these systems because strong consistency is not provided for any set of memory operations.
Reference: [35] <author> F. Mattern. </author> <title> Time and global states of distributed systems. </title> <booktitle> In Proceedings of the International Workshop on Parallel and Distributed Algorithms, </booktitle> <year> 1989. </year>
Reference-contexts: When a page not present in the local memory is accessed, it generates an access fault. A write on a page that is cached with read access causes a protection fault. We use vector timestamps to track changes in the state of shared data. Vector times-tamps <ref> [35] </ref> precisely capture the causal relationships between memory operations. Each processor maintains a vector clock and timestamps derived from this clock are stored with each copy of a page.
Reference: [36] <author> J. Misra. </author> <title> Axioms for memory access in asynchronous hardware systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 142-153, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: We use a more general framework here, as it allows us to easily relate causal memory to a range of memory models that have been proposed. The model is motivated by the ones used by Misra <ref> [36] </ref> and by Herlihy and Wing [23]. We define the system to be a finite set of processors that interact via a shared memory consisting of a finite set of locations. Processors execute read and write operations. Each such operation acts on a named location and has an associated value.
Reference: [37] <author> D. S. Parker et. al. </author> <title> Detection of mutual consistency in distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> May </month> <year> 1983. </year>
Reference-contexts: The DATA message that contains the requested page also includes the version number of the page. On a write fault, since a new version of 4 Version numbers were used in the Locus file system for detecting concurrent updates to a single file <ref> [37] </ref>.
Reference: [38] <author> Gautam Shah and Umakishore Ramachandran. </author> <title> Towards exploiting the architectural features of beehive. </title> <booktitle> In International Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Thus, the optimizations made possible by RC in implementing memory consistency do not increase the complexity of programming. Adve and Hill [1], also developed a similar approach and introduced the notion of data-race-free programs. Hybrid consistency [8] and buffered consistency <ref> [38] </ref> are examples of other memory models that are based on classifying synchronization operations and defining coherence relative to them. Lazy release consistency (LRC) [26] and entry consistency (EC) [13], both of which were proposed after RC, make even more aggressive use of synchronization in performing coherence actions.
Reference: [39] <author> SUN. </author> <title> The SPARC Architecture Manual. </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: Boyer [15] describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance over conventional atomic DSM. 40 There have been several hardware implementations of weakly ordered systems and are described in <ref> [21, 2, 39, 14, 19] </ref>. 6 Concluding Remarks We have presented a DSM system based on causal memory and have given two implementations for it. These implementations make use of the optimizations made possible by weakly ordered as well as weakly consistent systems.
Reference: [40] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2), </volume> <month> May </month> <year> 1993. </year> <month> 44 </month>
Reference-contexts: This is because on certain synchronization operations, causal memory incurs considerable processing overheads. A faster network will make the difference among the different systems less significant. This is because the memory systems that send large messages (pages) will benefit more from the increased network speed <ref> [40] </ref>. The memory systems will also become more competitive with message passing in architectures that have smaller page sizes or support multiple page sizes. 5 Related Work Many approaches for building DSM systems have been proposed.
Reference: [41] <author> Gene T. J. Wuu and Arthur J. Bernstein. </author> <title> Efficient solutions to the replicated log and dictionary problems. </title> <booktitle> In Proceedings of the 3rd ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1984. </year> <month> 45 </month>
Reference-contexts: Since version vectors only need to be sent with synchronization variables, we believe that this is not a problem for data-race-free programs. Furthermore, there exist techniques that can be used to limit the information carried in timestamps <ref> [41] </ref> by maintaining at each processor how far the clocks at other processors have advanced. 36 The performance benefits due to better scalability of causal memory or message passing are not easily seen for the applications and their sizes that we used in the study.
References-found: 41

