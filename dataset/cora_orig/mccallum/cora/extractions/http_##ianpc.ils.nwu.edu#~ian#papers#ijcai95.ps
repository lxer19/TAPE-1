URL: http://ianpc.ils.nwu.edu/~ian/papers/ijcai95.ps
Refering-URL: http://www.ils.nwu.edu/~ian/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ian@ai.mit.edu  
Title: Visual routines and visual search: a real-time implementation and an automata-theoretic analysis  
Author: Ian Horswill 
Address: 545 Technology Square Cambridge, MA 02139 USA  
Affiliation: MIT Artificial Intelligent Laboratory  
Abstract: I describe a real-time implementation of Ull-man's visual routine processor (VRP) theory of intermediate vision for visual search. The system performs serial self-terminating visual search and computes 2D spatial relations of objects from live color video using low cost hardware. I present a formal model of a VRP with unbounded resources and quantify the amount of external control structure required to solve Horn clauses using the VRP. In discussing the effect of resource limitations I show that contemporary models of biological visual attention are unable to solve surprisingly simple queries. I also describe a novel logic programming system that finds satisfying variable assignments for Horn clause queries using the VRP. The system contains no internal database: all logic variables are directly grounded in the world using VRP queries. Finally, I briefly discuss experiments with natural language interpretation and motor control using the VRP. Experiments on real data are given. 1
Abstract-found: 1
Intro-found: 1
Reference: [ Agre and Chapman, 1987 ] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: Joanna Bryson kindly read drafts of this paper and provided useful comments. properties of an image. These resources are collectively referred to as the visual routine processor or VRP. The visual routines theory have received increasing attention from the AI community in recent years ( <ref> [ Agre and Chapman, 1987 ] </ref>[ Chapman, 1990 ][ Reece and Shafer, 1991 ][ Romanycia, 1987 ][ Whitehead and Ballard, 1990 ] ). <p> To date, VRP systems have run either off of hand-drawn bitmaps [ Romanycia, 1987 ] or have been directly interfaced to the world model of a world simulator, thus bypassing low-level vision entirely <ref> [ Agre and Chapman, 1987 ] </ref>[ Chapman, 1990 ][ Reece and Shafer, 1991 ] . In this paper, I describe Jeeves , a working visual routine processor that runs off of color video at approximately 10Hz using only relatively simple hardware. This is work in progress.
Reference: [ Ahmad and Omohundro, 1991 ] <author> Subutai Ahmad and Stephen Omohundro. </author> <title> Efficient visual search: A connectionist solution. </title> <booktitle> In Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <address> Chicago, IL, </address> <year> 1991. </year>
Reference: [ Chapman, 1990 ] <author> David Chapman. </author> <title> Vision, instruction, and action. </title> <type> Technical Report 1204, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Lab, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: The details of the registers and functional units are unknown. Chapman's system <ref> [ Chapman, 1990 ] </ref> contained four types of state elements: markers, which held image locations; lines and rays; activation planes, which were general registers that could hold arbitrary binary images; and a return inhibition map. All state elements except the return inhibition map could be directly named and manipulated. <p> If either term is zero, the position salience is zero. The marker, ray direction, and rates of decay are inputs to the attention mechanism. 4 Automata-theoretic analysis Most computational work on covert attention and visual search (e.g. <ref> [ Chapman, 1990 ] </ref>[ Tsotsos et al., 1994 ][ Weis-meyer, 1992 ] ) has been designed specifically to fit psychophysical timing data on very specific tasks, such as Triesman's. Relatively little attention has been given to the computational power of these models in the abstract.
Reference: [ Clark and Ferrier, 1988 ] <author> James J. Clark and Nicola Fer-rier. </author> <title> Modal control of an attentive vision system. </title> <booktitle> In Proceedings of the Second International Conference on Computer Vision, </booktitle> <pages> pages 514-523, </pages> <address> Tampa, Florida, De-cember 1988. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: the use of segmentation is logically independent of other design decisions; one could change or remove the segmentation system and still have a usable vision system. 3.3 The saliency map and visual attention The VRP selects task-relevant image regions by computing a pixel-by-pixel weighted sum of the low level maps <ref> [ Clark and Ferrier, 1988 ] </ref>[ Ahmad and Omohundro, 1991 ] . The weights are one of the input parameters of the system and can be changed continuously by the higher levels (see figure 2). Weighted sums may be more powerful than the human system.
Reference: [ Horswill, 1993 ] <author> Ian Horswill. Polly: </author> <title> A vision-based artificial agent. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 824-829. </pages> <publisher> AAAI, MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Although designed for visual search, the visual routine processor already contains much of the machinery necessary for the visual system of the Polly robot <ref> [ Horswill, 1993 ] </ref> . For example, the Polly system performs collision avoidance by steering to avoid texture in the image.
Reference: [ Koch and Ullman, 1985 ] <author> C. Koch and S. Ullman. </author> <title> Shifts in selective visual attention: Towards the underlying neural circuitry. </title> <journal> Human Neurobiology, </journal> <volume> 2 </volume> <pages> 219-227, </pages> <year> 1985. </year>
Reference-contexts: These models assume image regions are selected based on their low level features (see above). Most use a pyramid structure, and so are referred to as "attention-" or "addressing-pyramids". Models differ on what the attention mechanism reports to the next level about the region. Some models such as <ref> [ Koch and Ullman, 1985 ] </ref> route aggregate feature values of the attended region to their outputs, while other models, such as [ Olshausen et al., 1992 ] , route resampled images to their outputs. Jeeves , is patterned after [ Koch and Ullman, 1985 ] , although dynamic resampling would <p> Some models such as <ref> [ Koch and Ullman, 1985 ] </ref> route aggregate feature values of the attended region to their outputs, while other models, such as [ Olshausen et al., 1992 ] , route resampled images to their outputs. Jeeves , is patterned after [ Koch and Ullman, 1985 ] , although dynamic resampling would be easy to add.
Reference: [ Olshausen et al., 1992 ] <author> Bruno Olshausen, Charles An-derson, and David Van Essen. </author> <title> A neural model of visual attention and invariant pattern recognition. CNS Memo 18, CalTech Computation and Neuural Systems Program, </title> <address> Pasadena, CA, </address> <year> 1992. </year>
Reference-contexts: Models differ on what the attention mechanism reports to the next level about the region. Some models such as [ Koch and Ullman, 1985 ] route aggregate feature values of the attended region to their outputs, while other models, such as <ref> [ Olshausen et al., 1992 ] </ref> , route resampled images to their outputs. Jeeves , is patterned after [ Koch and Ullman, 1985 ] , although dynamic resampling would be easy to add.
Reference: [ Reece and Shafer, 1991 ] <author> Douglas A. Reece and Steven Shafer. </author> <title> Using active vision to simplify perception for robot driving. </title> <type> CMU-CS 91-199, </type> <institution> Carnegie-Mellon University Computer Science Department, </institution> <year> 1991. </year>
Reference: [ Romanycia, 1987 ] <author> Marc H. J. Romanycia. </author> <title> The design and control of visual routines for the computation of simple geometric properties and relations. </title> <type> Technical Report 87-34, </type> <institution> University of British Columbia, Van-couver, BC, </institution> <address> Canada V6T 1W5, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Despite the level of interest, there has yet to be a VRP implementation that runs on real camera images. To date, VRP systems have run either off of hand-drawn bitmaps <ref> [ Romanycia, 1987 ] </ref> or have been directly interfaced to the world model of a world simulator, thus bypassing low-level vision entirely [ Agre and Chapman, 1987 ][ Chapman, 1990 ][ Reece and Shafer, 1991 ] .
Reference: [ Treisman and Gelade, 1980 ] <author> Anne M. Treisman and Garry Gelade. </author> <title> A feature integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12 </volume> <pages> 97-136, </pages> <year> 1980. </year>
Reference-contexts: Jeeves , is patterned after [ Koch and Ullman, 1985 ] , although dynamic resampling would be easy to add. For the purposes of this paper, the most important data on covert attention are the experiments of Treisman et al. <ref> [ Treisman and Gelade, 1980 ] </ref> , which suggest the vision system can search all points in the image in parallel for many low level features (called "pop-up" properties) but must handle conjunctions of features by serially enumerating regions satisfying one of the conjuncts and then testing them for the other
Reference: [ Tsotsos et al., 1994 ] <author> John K. Tsotsos, Seam M. Cul-hane, Winky Yan Kei Wai, Yuzhong Lai, Neal Davis, and Fernando Nuflo. </author> <title> Modeling visual attention via selective tuning. </title> <type> Technical report, </type> <institution> University of Toronto Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: These maps are believed to be computed bottom-up and in parallel. In the last decade, covert visual attention, the prob-lem of selecting image-plane regions for processing, has been widely studied in the psychophysical and neurophysiological communities. A wide range of neural models of covert attention have been proposed (see <ref> [ Tsotsos et al., 1994 ] </ref> for a detailed survey). These models assume image regions are selected based on their low level features (see above). Most use a pyramid structure, and so are referred to as "attention-" or "addressing-pyramids". <p> All regions satisfying a given pop-up feature can then be enumerated by repeatedly selecting a region and inhibiting it. It is outside the scope of this paper to debate the validity of Treisman's experiments or the various neural models of covert attention. The interested reader is directed to <ref> [ Tsotsos et al., 1994 ] </ref> . 2.2 Visual routines The visual routine model [ Ullman, 1984 ] claims that certain kinds of visual work are done by selecting relevant regions of the image and applying simple geometric operations to them such as drawing lines to connect them, searching along the <p> In point of fact, more recent psychophysical results do indicate that at least some conjunctions can be learned with sufficient training (see [ Weismeyer, 1992 ] and <ref> [ Tsotsos et al., 1994 ] </ref> for surveys of recent results). The attention system computes the region with the maximum integral of salience.
Reference: [ Tsotsos, 1990 ] <author> John K. Tsotsos. </author> <title> Analyzing vision at the complexity level. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13(3) </volume> <pages> 423-469, </pages> <year> 1990. </year>
Reference-contexts: It also provides a formal study of the VRP's visual search capabilities and the effects of the resource limitations in biological attention theories (for a study of visual search independent of the VRP architecture, see <ref> [ Tsotsos, 1990 ] </ref> ). One of the outcomes of the formal analysis is that some surprisingly simple queries cannot be solved without assuming more machinery than current biological theories provide.
Reference: [ Ullman, 1984 ] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Shimon Ullman proposed the visual routines theory of intermediate vision as a way of explaining how the human visual system might solve certain visual tasks (such as computing spatial relations) that seem to require serial processing <ref> [ Ullman, 1984 ] </ref> . <p> It is outside the scope of this paper to debate the validity of Treisman's experiments or the various neural models of covert attention. The interested reader is directed to [ Tsotsos et al., 1994 ] . 2.2 Visual routines The visual routine model <ref> [ Ullman, 1984 ] </ref> claims that certain kinds of visual work are done by selecting relevant regions of the image and applying simple geometric operations to them such as drawing lines to connect them, searching along the lines for other regions, or checking whether a point lies within a closed curve
Reference: [ Weismeyer, 1992 ] <author> Mark David Weismeyer. </author> <title> An operator-based model of human covert visual attention. </title> <type> CSE-TR 123-92, </type> <institution> University of Michigan Computer Science and Engineering Division, </institution> <address> Ann Arbor, MI, </address> <year> 1992. </year>
Reference-contexts: Since this would require considerable training, the human system might use the weighted combinations and still be unable to do parallel search for conjunctions. In point of fact, more recent psychophysical results do indicate that at least some conjunctions can be learned with sufficient training (see <ref> [ Weismeyer, 1992 ] </ref> and [ Tsotsos et al., 1994 ] for surveys of recent results). The attention system computes the region with the maximum integral of salience.
Reference: [ Whitehead and Ballard, 1990 ] <author> S. Whitehead and D. Ballard. </author> <title> Active perception and reinforcement learning. </title> <journal> Neural Computation, </journal> <volume> 2(4), </volume> <year> 1990. </year>
References-found: 15

