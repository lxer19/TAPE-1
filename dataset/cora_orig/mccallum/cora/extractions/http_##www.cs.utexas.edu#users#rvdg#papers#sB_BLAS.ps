URL: http://www.cs.utexas.edu/users/rvdg/papers/sB_BLAS.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/SL_library/library.html
Root-URL: 
Title: Parallel Implementation of BLAS: General Techniques for Level 3 BLAS  
Author: Almadena Chtchelkanova John Gunnels Greg Morrow James Overfelt Robert A. van de Geijn 
Date: October 11, 1995  
Address: Austin, Texas 78712  
Affiliation: The University of Texas at Austin  
Abstract: In this paper, we present straight forward techniques for a highly efficient, scalable implementation of common matrix-matrix operations generally known as the Level 3 Basic Linear Algebra Subprograms (BLAS). This work builds on our recent discovery of a parallel matrix-matrix multiplication implementation, which has yielded superior performance, and requires little work space. We show that the techniques used for the matrix-matrix multiplication naturally extend to all important level 3 BLAS and thus this approach becomes an enabling technology for efficient parallel implementation of these routines and libraries that use BLAS. Representative performance results on the Intel Paragon system are given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, S. M. Balle, M. Joshi, P. Palkar, </author> <title> "A High Performance Matrix Multiplication Algorithm for MPPs" IBM T.J. </title> <institution> Watson Research Center, </institution> <year> 1995. </year>
Reference: [2] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> "A High Performance Matrix Multiplication Algorithm on a Distributed-Memory Parallel Computer, using Overlapped Communication," </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp. 673-681, </pages> <year> 1994. </year>
Reference-contexts: N 1 broadcast A fli within rows broadcast B ifl within columns in parallel C = C + B B A 0i . . . 1 C A B i0 B i1 B i (N1) endfor It has been pointed out to us that this implementation was already reported in <ref> [2] </ref>. In that paper the benefits of overlapping communication and computation are also studied. 3.1.2 Forming C = AB T One approach to implementing this case is to transpose matrix B followed by the algorithm presented in the previous section.
Reference: [3] <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbau m, S. Hammarling, A. McKenney, and D. Sorensen, </author> <title> "Lapack: A Portable Linear Algebra Library for High Performance Computers," </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <publisher> IEEE Press, </publisher> <year> 1990, </year> <pages> pp. 1-10. </pages>
Reference-contexts: Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found. Perhaps the most successful sequential library, LAPACK <ref> [3, 4] </ref>, is built upon a few compute kernels, known as the Basic Linear Algebra Subprograms (BLAS). These include the level 1 BLAS (vector-vector operations) [32], level 2 BLAS (matrix-vector operations) [17], and level 3 BLAS (matrix-matrix operations) [18].
Reference: [4] <author> Anderson, E., Z. Bai, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found. Perhaps the most successful sequential library, LAPACK <ref> [3, 4] </ref>, is built upon a few compute kernels, known as the Basic Linear Algebra Subprograms (BLAS). These include the level 1 BLAS (vector-vector operations) [32], level 2 BLAS (matrix-vector operations) [17], and level 3 BLAS (matrix-matrix operations) [18].
Reference: [5] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. van de Geijn, </author> <title> "LAPACK for Distributed Memory Architectures: Progress Report." </title> <booktitle> in the Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year> <pages> pp. 625-630. 17 </pages>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [6] <author> Auslander, L., Tsao, A., </author> <title> On Parallelizable Eigensolvers, </title> <journal> Advanced Appl. Math., </journal> <volume> Vol. 13, </volume> <pages> pp. 253-261, </pages> <year> 1992 </year>
Reference-contexts: We have tried to describe the techniques so that they can be easily customized for specific distributions. The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers <ref> [6, 7, 8, 33] </ref>, or out-of-core dense linear solvers [31]. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [7] <author> Bai, Z., Demmel, J., </author> <title> "Design of a Parallel Nonsymmetric Eigenroutine Toolbox, Part I", Parallel Processing for Scientific Computing, </title> <editor> Editors R. Sincovec, D. Keyes, M. Leuze, L. Petzold, and D. </editor> <booktitle> Reed, </booktitle> <pages> pp. 391-398, </pages> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1993 </year>
Reference-contexts: We have tried to describe the techniques so that they can be easily customized for specific distributions. The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers <ref> [6, 7, 8, 33] </ref>, or out-of-core dense linear solvers [31]. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [8] <author> Bai, Z., Demmel, J., Dongarra, J., Petitet, A., Robinson, H., Stanley, K., </author> <title> The Spectral Decomposition of Nonsymmetric Matrices on Distributed Memory Parallel Computers, </title> <note> LAPACK working note 91, </note> <institution> University of Tennessee, </institution> <month> Jan. </month> <year> 1995 </year>
Reference-contexts: We have tried to describe the techniques so that they can be easily customized for specific distributions. The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers <ref> [6, 7, 8, 33] </ref>, or out-of-core dense linear solvers [31]. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [9] <author> M. Barnett, D.G. Payne, R. van de Geijn and J. Watts. </author> <title> "Broadcasting on Meshes with Worm-Hole Routing," </title> <institution> University of Texas, Department of Computer Sciences, </institution> <month> TR-93-24 </month> <year> (1993). </year>
Reference-contexts: among p nodes is given by dlog (p)e (ff + nfi) and the cost for summing vectors of length n among p nodes is given by dlog (p)e (ff + nfi + nfl) There are a number of papers on how to improve upon this method of broadcasting and summation <ref> [28, 38, 9, 40, 42] </ref>, but will analyze only this simple approach, since it is typically the current MPI default implementation. 5.
Reference: [10] <author> Cannon, L.E., </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm , Ph.D. </title> <type> Thesis (1969), </type> <institution> Montana State University. </institution>
Reference: [11] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. </author> <title> Whaley "A Proposal for a Set of Parallel Basic Linear Algebra Subprograms", </title> <note> LAPACK Working Note 100, </note> <institution> University of Tennessee, CS-95-292, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The above observations lead to two related problems: first, how to specify the parallel BLAS and second, how to implement them. The former is left to those working to standardize parallel BLAS interfaces <ref> [11] </ref>. This paper concentrates on the latter issue. Notice that the dense linear algebra community has always depended on fast implementations of the BLAS provided by the vendors.
Reference: [12] <author> Choi J., J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> "Scalapack: A Scalable Linear Algebra Library for Distributed Memory Concurrent Computers," </title> <booktitle> Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [13] <author> Choi, J., J. J. Dongarra, and D. W. Walker, </author> <title> "Level 3 BLAS for distributed memory concurrent computers", </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <address> Saint Hilaire du Touvet, France, </address> <month> Sept. </month> <pages> 7-8, </pages> <address> 1992. </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference: [14] <author> Choi, J., J. J. Dongarra, and D. W. Walker, "PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on distributed memory concurrent computers," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol 6(7), </volume> <pages> 543-570, </pages> <year> 1994. </year>
Reference-contexts: Additional support came from the Intel Research Council. In a recent paper [41], we describe a highly efficient implementation of matrix-matrix multiplication, Scalable Universal Matrix Multiplication Algorithm (SUMMA), that has many benefits over alternative implementations <ref> [14, 29, 30] </ref>. These benefits include better performance, simpler and more flexible implementation, and a lower workspace requirement.
Reference: [15] <author> James Demmel, Jack Dongarra, Robert van de Geijn and David Walker, </author> <title> "LAPACK for Distributed Memory Architectures: The Next Generation," </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Norfolk, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [16] <author> Dongarra, J. J., I. S. Duff, D. C. Sorensen, and H. A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference: [17] <author> Dongarra, J. J., J. Du Croz, S. Hammarling, and R. J. Hanson, </author> <title> "An Extended Set of FORTRAN Basic Linear Algebra Subprograms", </title> <journal> TOMS, </journal> <volume> Vol. 14, No. 1, </volume> <pages> pp. 1-17, </pages> <year> 1988. </year>
Reference-contexts: Perhaps the most successful sequential library, LAPACK [3, 4], is built upon a few compute kernels, known as the Basic Linear Algebra Subprograms (BLAS). These include the level 1 BLAS (vector-vector operations) [32], level 2 BLAS (matrix-vector operations) <ref> [17] </ref>, and level 3 BLAS (matrix-matrix operations) [18]. It is thus natural to try to push most of the effort into parallelizing these kernels, in the hope that good parallel implementations of codes that utilize these kernels will follow automatically.
Reference: [18] <author> Dongarra, J. J., J. Du Croz, S. Hammarling, and I. Duff, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <journal> TOMS, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 1-16, </pages> <year> 1990. </year>
Reference-contexts: Perhaps the most successful sequential library, LAPACK [3, 4], is built upon a few compute kernels, known as the Basic Linear Algebra Subprograms (BLAS). These include the level 1 BLAS (vector-vector operations) [32], level 2 BLAS (matrix-vector operations) [17], and level 3 BLAS (matrix-matrix operations) <ref> [18] </ref>. It is thus natural to try to push most of the effort into parallelizing these kernels, in the hope that good parallel implementations of codes that utilize these kernels will follow automatically.
Reference: [19] <author> Jack J. Dongarra, Robert A. van de Geijn and R. Clint Whaley, </author> <title> "Two Dimensional Basic Linear Algebra Communication Subprograms," </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Norfolk, </address> <month> March </month> <year> 1993. </year>
Reference: [20] <author> Jack Dongarra and Robert van de Geijn, </author> <title> "A Parallel Dense Linear Solve Library Routine," </title> <booktitle> in Proceedings of the 1992 Intel Supercomputer Users' Group Meeting, </booktitle> <address> Dallas, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [21] <author> Jack Dongarra, Robert van de Geijn, and David Walker, </author> <title> "Scalability Issues Affecting the Design of a Dense Linear Algebra Library," Special Issue on Scalability of Parallel Algorithms, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 22, No. 3, </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found. <p> Parameters ff and fi represent the startup and cost per item transfer time, respectively. Performing a floating point computation requires time fl. 2.2 Data decomposition We will assume the classical data decomposition, generally referred to as a two dimensional block-wrapped, or block-cyclic, matrix decomposition <ref> [21] </ref>. <p> We have tried to describe the techniques so that they can be easily customized for specific distributions. The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization <ref> [21] </ref>, the left-looking Cholesky factorization, matrix-multiplication based eigensolvers [6, 7, 8, 33], or out-of-core dense linear solvers [31]. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [22] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn, </author> <title> "Parallel Matrix Distributions: have we been doing it all wrong?" Department of Computer Sciences, UT-Austin, </title> <type> Report TR95-39, </type> <month> Oct. </month> <year> 1995. </year> <month> 18 </month>
Reference-contexts: Unfortunately, the traditional approach to matrix distribution does not allow this operation to be performed efficiently, elegantly, or generally. We believe that the key to solving this problem is to switch to what we call Physically Based Matrix Distributions (PBMD) <ref> [22] </ref>. We are currently pursuing implementation of all BLAS using this new approach to matrix distribution. The above comments may make it appear we are sending mixed signals concerning the usefulness of the described techniques.
Reference: [23] <author> Fox, G. C., M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [24] <author> Fox, G., S. Otto, and A. Hey, </author> <title> "Matrix Algorithms on a Hypercube I: Matrix Multiplication," </title> <booktitle> Parallel Computing 3 (1987), </booktitle> <pages> pp 17-31. </pages>
Reference: [25] <author> Golub, G. H. , and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference: [26] <author> Brian Grayson and Robert van de Geijn "A High Performance Parallel Strassen Implementation," </author> <note> Parallel Processing Letters, to appear. </note>
Reference-contexts: The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers [6, 7, 8, 33], or out-of-core dense linear solvers [31]. Finally, in <ref> [26] </ref> we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication. This suggests that an entire suite of variants of Strassen's algorithm for all the level 3 BLAS could be implemented on top of our standard parallel level 3 BLAS implementations.
Reference: [27] <author> Gropp, W., E. Lusk, A. Skjellum, </author> <title> Using MPI: Portable Programming with the Message-Passing Interface, </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference: [28] <author> C.-T. Ho and S. L. Johnsson, </author> <title> Distributed Routing Algorithms for Broadcasting and Personalized Communication in Hypercubes, </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. 640-648, </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: among p nodes is given by dlog (p)e (ff + nfi) and the cost for summing vectors of length n among p nodes is given by dlog (p)e (ff + nfi + nfl) There are a number of papers on how to improve upon this method of broadcasting and summation <ref> [28, 38, 9, 40, 42] </ref>, but will analyze only this simple approach, since it is typically the current MPI default implementation. 5.
Reference: [29] <author> Huss-Lederman, S., E. Jacobson, A. Tsao, </author> <title> "Comparison of Scalable Parallel Matrix Multiplication Libraries," </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <address> Starksville, MS, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Additional support came from the Intel Research Council. In a recent paper [41], we describe a highly efficient implementation of matrix-matrix multiplication, Scalable Universal Matrix Multiplication Algorithm (SUMMA), that has many benefits over alternative implementations <ref> [14, 29, 30] </ref>. These benefits include better performance, simpler and more flexible implementation, and a lower workspace requirement.
Reference: [30] <author> Huss-Lederman, S., E. Jacobson, A. Tsao, G. Zhang, </author> <title> "Matrix Multiplication on the Intel Touchstone DELTA," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 6 (7), </volume> <month> Oct. </month> <year> 1994, </year> <pages> pp. 571-594. </pages>
Reference-contexts: Additional support came from the Intel Research Council. In a recent paper [41], we describe a highly efficient implementation of matrix-matrix multiplication, Scalable Universal Matrix Multiplication Algorithm (SUMMA), that has many benefits over alternative implementations <ref> [14, 29, 30] </ref>. These benefits include better performance, simpler and more flexible implementation, and a lower workspace requirement.
Reference: [31] <author> Ken Klimkowski and Robert van de Geijn, </author> <title> "Anatomy of an out-of-core dense linear solver", </title> <note> International Conference on Parallel Processing 1995, to appear. </note>
Reference-contexts: The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers [6, 7, 8, 33], or out-of-core dense linear solvers <ref> [31] </ref>. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [32] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh, </author> <title> "Basic Linear Algebra Subprograms for Fortran Usage", </title> <journal> TOMS, </journal> <volume> Vol. 5, No. 3, </volume> <pages> pp. 308-323, </pages> <year> 1979. </year>
Reference-contexts: Perhaps the most successful sequential library, LAPACK [3, 4], is built upon a few compute kernels, known as the Basic Linear Algebra Subprograms (BLAS). These include the level 1 BLAS (vector-vector operations) <ref> [32] </ref>, level 2 BLAS (matrix-vector operations) [17], and level 3 BLAS (matrix-matrix operations) [18]. It is thus natural to try to push most of the effort into parallelizing these kernels, in the hope that good parallel implementations of codes that utilize these kernels will follow automatically.
Reference: [33] <author> Lederman, S., Tsao, A., Turnbull, T., </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center, </institution> <year> 1991 </year>
Reference-contexts: We have tried to describe the techniques so that they can be easily customized for specific distributions. The library as it stands would be useful for implementation of a wide range of algorithms, including the LU factorization [21], the left-looking Cholesky factorization, matrix-multiplication based eigensolvers <ref> [6, 7, 8, 33] </ref>, or out-of-core dense linear solvers [31]. Finally, in [26] we show how this approach can be used for parallel implementation of Strassen's algorithm for matrix-matrix multiplication.
Reference: [34] <author> J. G. Lewis and R. A. van de Geijn, </author> <title> "Implementing Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Multicomputers," </title> <booktitle> Supercomputing '93. </booktitle>
Reference: [35] <author> J. G. Lewis, D. G. Payne, and R. A. van de Geijn, </author> <title> "Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Computers," </title> <booktitle> Scalable High Performance Computing Conference 1994. </booktitle>
Reference: [36] <author> W. Lichtenstein and S. L. Johnsson, </author> <title> "Block-Cyclic Dense Linear Algebra", </title> <institution> Harvard University, Center for Research in Computing Technology, TR-04-92, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [37] <author> Lin, C., and L. Snyder, </author> <title> "A Matrix Product Algorithm and its Comparative Performance on Hypercubes," </title> <booktitle> in Proceedings of Scalable High Performance Computing Conference, </booktitle> <editor> (Stout, Q, and M. Wolfe, eds.), </editor> <publisher> IEEE Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 190-3. </pages>
Reference: [38] <author> Prasenjit Mitra, David Payne, Lance Shuler, Robert van de Geijn, and Jerrell Watts, </author> <title> "Fast Collective Communication Libraries, Please," </title> <booktitle> in the Proceedings of the Intel Supercomputing Users' Group Meeting 1995. </booktitle>
Reference-contexts: In the absense of network conflicts, communicating a message between two nodes requires time ff + nfi, which is reasonable on machines like the Intel Paragon system <ref> [38] </ref> . Parameters ff and fi represent the startup and cost per item transfer time, respectively. Performing a floating point computation requires time fl. 2.2 Data decomposition We will assume the classical data decomposition, generally referred to as a two dimensional block-wrapped, or block-cyclic, matrix decomposition [21]. <p> among p nodes is given by dlog (p)e (ff + nfi) and the cost for summing vectors of length n among p nodes is given by dlog (p)e (ff + nfi + nfl) There are a number of papers on how to improve upon this method of broadcasting and summation <ref> [28, 38, 9, 40, 42] </ref>, but will analyze only this simple approach, since it is typically the current MPI default implementation. 5.
Reference: [39] <author> Robert van de Geijn, </author> <title> "Massively Parallel LINPACK Benchmark on the Intel Touchstone DELTA and iPSC/860 Systems: </title> <type> Preliminary Report," </type> <institution> TR-91-28, Department of Computer Sciences, University of Texas, </institution> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Over the last five years, we have learned a lot about how to parallelize dense linear algebra libraries <ref> [5, 12, 15, 20, 21, 23, 36, 39] </ref>. Since much effort went into implementation of individual algorithms, it became clear that in order to parallelize entire sequential libraries, better approaches had to be found.
Reference: [40] <author> Robert van de Geijn, </author> <title> "On Global Combine Operations," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22, </volume> <pages> pp. </pages> <month> 324-328 </month> <year> (1994). </year>
Reference-contexts: among p nodes is given by dlog (p)e (ff + nfi) and the cost for summing vectors of length n among p nodes is given by dlog (p)e (ff + nfi + nfl) There are a number of papers on how to improve upon this method of broadcasting and summation <ref> [28, 38, 9, 40, 42] </ref>, but will analyze only this simple approach, since it is typically the current MPI default implementation. 5.
Reference: [41] <author> Robert van de Geijn and Jerrell Watts, "SUMMA: </author> <title> Scalable Universal Matrix Multiplication Algorithm," </title> <institution> TR-95-13, Department of Computer Sciences, University of Texas, </institution> <month> April </month> <year> 1995. </year> <note> Also: LAPACK Working Note 96, May 1. Submitted to Concurrency: Practice and Experience. 19 </note>
Reference-contexts: Additional support came from the Intel Research Council. In a recent paper <ref> [41] </ref>, we describe a highly efficient implementation of matrix-matrix multiplication, Scalable Universal Matrix Multiplication Algorithm (SUMMA), that has many benefits over alternative implementations [14, 29, 30]. These benefits include better performance, simpler and more flexible implementation, and a lower workspace requirement. <p> the indexing of the blocks starting at 0, which leads to simpler explanations.) 3 Parallel Algorithms for the Level 3 BLAS In this section, we give high level descriptions of the parallel implementations of all the level 3 BLAS. 3 3.1 Matrix-Matrix Multiplication (xGEMM) 3.1.1 Forming C = AB In <ref> [41] </ref>, we note the following: C = AB = B B A 00 . . . 1 C A B 00 B 01 B 0 (N1) (2) 0 B @ A 11 A (N1)1 C C + B B A 0 (N1) . . . 1 C A where A flj <p> In that paper the benefits of overlapping communication and computation are also studied. 3.1.2 Forming C = AB T One approach to implementing this case is to transpose matrix B followed by the algorithm presented in the previous section. In <ref> [41] </ref>, we show how to avoid this initial communication: 0 B @ C 1i C (N1)i C C = A B i0 B i1 B i (N1) 4 C can thus be computed column panel at a time. <p> multiple right-hand-sides to update according to Eqn. 20 are performed, and i is decreased. 3.6.5 Forming B = A T B, A upper triangular This case can be treated similarly. 3.6.6 Forming B = BA T , A upper and lower triangular These cases follow similarly. 10 4 Analysis In <ref> [41] </ref>, we give an analysis of the matrix-matrix multiplication algorithm. In that paper, we assumed a slightly different matrix distribution, and hence the results were slightly different from those we will derive next. In addition, we show the results of similar analysis of representative other parallel level 3 BLAS. <p> Scalability can be analysed using these costs extimates as was done for matrix-matrix multiply. The resulting conclusions are similar to those for the matrix-matrix multiply. 12 5 Optimizations In <ref> [41] </ref>, we show how the performance of the matrix-matrix multiplication can be improved considerably by pipelining communication and computation. In effect, the broadcast is replaced by one that passes the messages around the ring that forms the nodes involved in the broadcast.

References-found: 41

