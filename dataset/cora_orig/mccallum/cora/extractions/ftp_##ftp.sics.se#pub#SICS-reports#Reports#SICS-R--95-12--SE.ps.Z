URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--95-12--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: Capacity of SDM  
Author: Gunnar Sjodin 
Keyword: Sparse Distributed Memory, SDM, Associative memory, Neural Networks, Efficient information retrieval, Noise reduction.  
Date: December 1995  
Address: Box 1263, S-164 28 Kista, Sweden  
Affiliation: RWCP 1 Neuro SICS 2 Laboratory  
Note: Improving the  
Abstract: Report R95:12 ISRN : SICS-R--95/12-SE ISSN : 0283-3638 Abstract A more efficient way of reading the SDM memory is presented. This is accomplished by using implicit information, hitherto not utilized, to find the information-carrying units and thus removing unnecessary noise when reading the memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard Durrett. </author> <title> Probability: Theory and Examples. </title> <publisher> Duxbury Press, </publisher> <address> second edition, </address> <year> 1995. </year>
Reference-contexts: By counting the number of elements in h, we also get h, which we need for the reading methods in section 2. 39 A Appendix Proof of proposition 1 General references for the following are Durrett <ref> [1] </ref> and Feller [2]. Throughout this appendix the assumptions of the proposition 1 are supposed to hold and ffi and ~ will refer to the constants in its statement. A.1 Large deviation lemmas Lemma 2 Let a &gt; 0. <p> regardless of the value of p P (jX 1 + : : : + X n j na) 2e n a 2 In particular P (jX 1 + : : : + X n j n 2 +fi ) 2e 1 (65) Proof Using the large deviation technique in Durrett <ref> [1] </ref> page 71, we obtain P (X 1 + : : : + X n na) e ng (a) (66) where for 0 a &lt; 2q g (a) = p (1 + 2p a ) + q (1 2q a ) (67) which is obtained by taking the maximum of a <p> Then, P (n 1fi2fl &lt; X 1 + : : : + X n &lt; n 1fi+2fl ) 1 2e 1 (84) Proof Using the large deviation theory in Durrett <ref> [1] </ref> again we get, for a &gt; p P (X 1 + : : : + X n na) e ng (a;p) (86) where g (a; p) = a log p 1 a (87) Let p = cn fi and let a = n fl p = cn flfi , fl
Reference: [2] <author> W. Feller. </author> <title> Generalization of a probability limit theorem of cramer. </title> <journal> Tran-scations of the American Mathematical Society, </journal> <pages> pages 361-372, </pages> <year> 1943. </year>
Reference-contexts: By counting the number of elements in h, we also get h, which we need for the reading methods in section 2. 39 A Appendix Proof of proposition 1 General references for the following are Durrett [1] and Feller <ref> [2] </ref>. Throughout this appendix the assumptions of the proposition 1 are supposed to hold and ffi and ~ will refer to the constants in its statement. A.1 Large deviation lemmas Lemma 2 Let a &gt; 0. <p> P (n 1fi2fl &lt; X 1 + : : : + X n &lt; n 1fi+2fl ) 1 2e 1 (96) where n 0 is chosen such that n 0 n 1 ; n 2 and n fl c n fl , n n 0 Using theorem 1 in Feller <ref> [2] </ref>, we have (by letting 0 &lt; n x &lt; 1 24 in the theorem) Lemma 5 Assume that X 1 ; : : : ; X n are independent stochastic variables with EX i = 0 , 1 i n (97) i = E (X 2 s 2 1 + <p> If s n 1 , 1 i n (100) then P (j X 1 + : : : + X n j &gt; xs n ) 2e 3 for x 0:775 (101) Proof With the notations of Feller Feller <ref> [2] </ref>, let n = 24x 43 Then n s n = 24x 0 &lt; n x = 24 1 (104) Furthermore, 12 n x = 1 2 gives jQ n (x)j n 2 2 ) 1 (105) so that P (j X 1 + : : : + X n j
Reference: [3] <author> Gunnar Sjodin. </author> <title> Convergence and new operations in SDM, </title> <note> 1995. SICS Research Report, to appear. </note>
Reference-contexts: If we read at an address X which is close to an address X 0 , at which we have stored a datum, we would like to move X towards the unknown X 0 in order to get a better reading. This problem is addressed in Sjodin <ref> [3] </ref>. 5 Contents Matrix M fi U counters 1 U1 u m 1 -1 1 1 1 -11 1001011: : : 11001 a 2 C 2;1 6 -2 C 2;u 0-8 6 -4 0 C M;u -22 X ADRESS REGISTER W TRANSFORMED WORD-IN REGISTER Some method for reading based on the
Reference: [4] <author> L. A. Jaeckel. </author> <title> An alternative Design for a Sparse Distributed Memory. </title> <type> Technical Report TR-89.28, </type> <institution> RIACS, </institution> <year> 1989. </year>
Reference: [5] <author> Pentti Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> The MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: -2 C 2;u 0-8 6 -4 0 C M;u -22 X ADRESS REGISTER W TRANSFORMED WORD-IN REGISTER Some method for reading based on the contents of the activated locations Z WORD-OUT REGISTER Some activation mechanism ? ? @ @ @ A A A A A A 6 1.2 Notations Following <ref> [5] </ref> we will use the following notations and definitions. An address is a binary vector of 0:s and 1:s. Its indices are called coordinates. A datum is a vector of 1:s and 1:s. Its indices are called positions. A mask is a subset of the coordinates in the address vectors.
Reference: [6] <author> Pentti Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In Mohamad H. Hassoun, editor, </editor> <title> Associative Neural Memories, </title> <booktitle> chapter 3, </booktitle> <pages> pages 50-76. </pages> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: A picture of the general structure is given in figure 1. In 1.2 a list of commonly used notations is given. It should serve as a general reference when reading this paper. For relevant literature on SDM cf. [4],[5], <ref> [6] </ref>, [7], [8], [10]. When a U -dimensional binary vector W is stored "at" an address X a mechanism activates a set of memory-locations and W is added to the contents of these after first transforming 0:s to 1:s. <p> Usually we will assume that the activation mechanism is such that = (2M 2 ) 1 (cf. e.g. <ref> [6] </ref> and [8]) In particular, in the Jaeckel/Karlsson model, we will have 2 K = = (2M 2 ) 1 1.2.5 Data directly available when reading the memory X = the address at which we read the memory H = H (X) * * = the locations activated by X H
Reference: [7] <author> Roland Karlsson. </author> <title> A Fast Activation Mechanism for the Kanerva SDM Memory, 1995. </title> <institution> SICS Research Report R95:10, Swedish Institute of Computer Science. </institution>
Reference-contexts: A picture of the general structure is given in figure 1. In 1.2 a list of commonly used notations is given. It should serve as a general reference when reading this paper. For relevant literature on SDM cf. [4],[5], [6], <ref> [7] </ref>, [8], [10]. When a U -dimensional binary vector W is stored "at" an address X a mechanism activates a set of memory-locations and W is added to the contents of these after first transforming 0:s to 1:s.
Reference: [8] <author> Jan Kristoferson. </author> <title> Best Probabilities of Activation and Performance Comparisons for Several Designs of Kanerva's SDM (Sparse Distributed Memory), 1995. </title> <institution> SICS Research Report R95:09, Swedish Institute of Computer Science. </institution>
Reference-contexts: A picture of the general structure is given in figure 1. In 1.2 a list of commonly used notations is given. It should serve as a general reference when reading this paper. For relevant literature on SDM cf. [4],[5], [6], [7], <ref> [8] </ref>, [10]. When a U -dimensional binary vector W is stored "at" an address X a mechanism activates a set of memory-locations and W is added to the contents of these after first transforming 0:s to 1:s. <p> Usually we will assume that the activation mechanism is such that = (2M 2 ) 1 (cf. e.g. [6] and <ref> [8] </ref>) In particular, in the Jaeckel/Karlsson model, we will have 2 K = = (2M 2 ) 1 1.2.5 Data directly available when reading the memory X = the address at which we read the memory H = H (X) * * = the locations activated by X H = the
Reference: [9] <author> Jan Kristoferson. </author> <title> Some comments on the Information Stored in Sparse Distributed Memory, </title> <note> 1995. SICS Research Report, to appear. </note>
Reference-contexts: As a matter of fact, and as will be clear by the techniques used in this paper, it would be sufficient, cf. section 3.2, just to have knowledge about T to estimate the p u :s, a m :s etc. In fact, as is demonstrated in <ref> [9] </ref>, even T may be calculated from the memory without the extra location and the extra position.
Reference: [10] <author> David Rogers. </author> <title> Using data tagging to improve the performance of Kan-erva's sparse distributed memory. </title> <type> Technical Report TR-88.1, </type> <institution> RIACS, </institution> <year> 1988. </year>
Reference-contexts: A picture of the general structure is given in figure 1. In 1.2 a list of commonly used notations is given. It should serve as a general reference when reading this paper. For relevant literature on SDM cf. [4],[5], [6], [7], [8], <ref> [10] </ref>. When a U -dimensional binary vector W is stored "at" an address X a mechanism activates a set of memory-locations and W is added to the contents of these after first transforming 0:s to 1:s. <p> The same formula is obtained by minimizing the variance of ff 1 c e s 1 + : : : + ff h c e s h under the the constraint ff 1 + : : : + ff h = 1 cf. <ref> [10] </ref>. From (20) we get the decision rule s 1 + : : : + c h for choosing = 1. <p> Hence we can find the possibly complex values of (51) and then use the closest (non-negative) values as values for h Q1 ; h Q2 . 6 Finding the h-locations It may be argued that the information-carrying locations, the h-locations, can easily be found using the "tagged memories" of <ref> [10] </ref>. This construction stores symbols, in each location, for what has been stored in it.
References-found: 10

