URL: http://www.cs.berkeley.edu/~russell/papers/mini94f-relevance.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: fsdavies,russellg@cs.berkeley.edu  
Title: NP-Completeness of Searches for Smallest Possible Feature Sets a subset of the set of all
Author: Scott Davies and Stuart Russell 
Keyword: Proof of NP-Completeness  
Note: S is  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: In many learning problems, the learning system is presented with values for features that are actually irrelevant to the concept it is trying to learn. The FOCUS algorithm, due to Almuallim and Dietterich, performs an explicit search for the smallest possible input feature set S that permits a consistent mapping from the features in S to the output feature. The FOCUS algorithm can also be seen as an algorithm for learning determinations or functional dependencies, as suggested in [6]. Another algorithm for learning determinations appears in [7]. The FOCUS algorithm has superpolynomial runtime, but Almuallim and Di-etterich leave open the question of tractability of the underlying problem. In this paper, the problem is shown to be NP-complete. We also describe briefly some experiments that demonstrate the benefits of determination learning, and show that finding lowest-cardinality determinations is easier in practice than finding minimal determi Define the MIN-FEATURES problem as follows: given a set X of examples (which are each composed of a a binary value specifying the value of the target feature and a vector of binary values specifying the values of the other features) and a number n, determine whether or not there exists some feature set S such that: We show that MIN-FEATURES is NP-complete by reducing VERTEX-COVER to MIN-FEATURES. 1 The VERTEX-COVER problem may be stated as the question: given a graph G with vertices V and edges E, is there a subset V 0 of V , of size m, such that each edge in E is connected to at least one vertex in V 0 ? We may reduce an instance of VERTEX-COVER to an instance of MIN-FEATURES by mapping each edge in E to an example in X, with one input feature for every vertex in V . 1 In [8], a "proof" is reported for this result by reduction to set covering. The proof therefore fails to show NP-completeness. nations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almuallim, H. and Dietterich, T. </author> <title> (1991) "Learning with Many Irrelevant Features." </title> <booktitle> In Proc. AAAI-91. </booktitle>
Reference: [2] <author> Blum, A. </author> <year> 1990. </year> <title> "Learning Boolean Functions in an Infinite Attribute Space," </title> <booktitle> Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 64-72. </pages> <address> Baltimore, MD. </address>
Reference-contexts: If it is known that the function being learned is of a certain class (e.g., K-CNF or K-DNF), then the problem once again becomes tractable <ref> [2] </ref>.
Reference: [3] <author> Blum, A.; Hellerstein, L.; and Littlestone, N. </author> <year> 1991. </year> <title> "Learning in the Presence of Finitely or Infinitely Many Irrelevant Attributes," </title> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory. </booktitle> <address> Santa Cruz, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: If the learning system is allowed to make queries about the outputs corresponding to arbitrary input vectors, then the determination of smallest possible feature sets for discrete data becomes completely tractable <ref> [3] </ref>. If it is known that the function being learned is of a certain class (e.g., K-CNF or K-DNF), then the problem once again becomes tractable [2].
Reference: [4] <author> John, G. H., Kohavi, R., and Pfleger, K. </author> <year> 1994. </year> <title> "Irrelevant features and the subset selection problem." </title> <booktitle> In Proceedings of ML-94. </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In general, however, their approach to searching for relevant features appears to be intractable. Furthermore, they must be significantly changed if they are to work properly for noisy or floating-point functions. John et al. <ref> [4] </ref> suggest that it may be better to select a feature set that yields the best inductive performance by M 0 (as measured by cross-validation), rather than simply identifying an exactly relevant feature set. This approach seems promising, and more likely to tolerate noise.
Reference: [5] <author> Quinlan, R. </author> <year> 1986. </year> <title> "Induction of Decision Trees," Machine Learning, </title> <publisher> Kluwer Academic, 1:1. </publisher>
Reference-contexts: This takes more time, however, and although the resulting learning curves are more predictable, they often have the same averages. Using a basic implementation of Quinlan's ID3 decision-tree learning algorithm <ref> [5] </ref> for M 0 , we tested the above algorithm on random boolean functions of 16 boolean variables, only 5 of which were relevant.
Reference: [6] <author> Russell, S. J. </author> <title> (1989) The Use of Knowledge in Analogy and Induction. </title> <publisher> London: Pitman Press. </publisher>
Reference: [7] <author> Schlimmer, J. </author> <title> (1993) "Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning." </title> <booktitle> In Proc. </booktitle> <address> ML-93. </address>
Reference: [8] <author> Wong, S. K. M., and Ziarko, W. </author> <title> (1985) "On optimal decision rules in decision tables." </title> <journal> Bull. Polish Acad. Sci. (Math.), </journal> <pages> 33(11-12), 693-696. </pages>
Reference-contexts: 1 In <ref> [8] </ref>, a "proof" is reported for this result by reduction to set covering. The proof therefore fails to show NP-completeness. Start with one example with 0's for all input features| one input for every vertex in V|and a 0 for an output.
References-found: 8

