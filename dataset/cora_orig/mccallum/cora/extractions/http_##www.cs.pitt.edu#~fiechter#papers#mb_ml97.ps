URL: http://www.cs.pitt.edu/~fiechter/papers/mb_ml97.ps
Refering-URL: http://www.cs.pitt.edu/~fiechter/papers/
Root-URL: 
Email: fiechter@cs.pitt.edu  
Title: Expected Mistake Bound Model for On-Line Reinforcement Learning  
Author: Claude-Nicolas Fiechter 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: We propose a model of efficient on-line reinforcement learning based on the expected mistake bound framework introduced by Haussler, Littlestone and Warmuth (1987). The measure of performance we use is the expected difference between the total reward received by the learning agent and that received by an agent behaving optimally from the start. We call this expected difference the cumulative mistake of the agent and we require that it "levels off" at a reasonably fast rate as the learning progresses. We show that this model is polynomially equivalent to the PAC model of off-line reinforcement learning introduced in (Fiechter, 1994). In particular we show how an off-line PAC reinforcement learning algorithm can be transformed into an efficient on-line algorithm in a simple and practical way. An immediate consequence of this result is that the PAC algorithm for the general finite state-space reinforcement learning problem described in (Fiechter, 1994) can be transformed into a polynomial on-line al gorithm with guaranteed performances.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berry, D. and Fristedt, B. </author> <year> (1985). </year> <title> Bandits Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK. </address>
Reference-contexts: We consider the mistake at each trial, called the instantaneous mistake, as well as the cumulative mistake on all the trials from the very beginning of the learning process. This last measure is equivalent to the notion of regret used in Bandits problems <ref> (Berry and Fristedt, 1985) </ref>. Using these measures of performance we can define criteria for efficient reinforcement learning which are, we think, both intuitive and theoretically sound.
Reference: <author> Fiechter, C.-N. </author> <year> (1994). </year> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: On the one hand the agent needs to try a wide variety of actions to explore its environment and acquire knowledge. On the other hand the agent wants to exploit its environment and select only the actions it thinks will result in the highest reward. In a previous work <ref> (Fiechter, 1994) </ref> we proposed a model of efficient (polynomial-time) reinforcement learning based on Valiant's PAC learning framework (Valiant, 1984), and described a PAC algorithm for the general finite state-space reinforcement learning problem. <p> An immediate consequence of this result is that PAC algorithms that have been proposed for the associative reinforcement learning problem (Fiechter, 1995) and for the general infinite-horizon, finite state-space reinforcement learning problem <ref> (Fiechter, 1994) </ref> can be transformed into polynomial on-line algorithms with guaranteed performances. Maybe more importantly, this result suggests that the "harder" problem in reinforcement learning is the problem of efficient exploration, that is the learning per se, rather than the exploration-exploitation trade-off. <p> finite state and action spaces, and are interested in learning state-action mappings, then the size of an environment E can be measured as a combination of its number N of states, of its number K of actions, and of a factor 1=(1 fl) that depends of the discount factor fl <ref> (Fiechter, 1994) </ref>. If we are considering associative reinforcement learn ing and are interested in learning policies represented by restricted classes of functions defined over some attributes of the states, then the size of an environment E can simply be the number n of state attributes (Fiechter, 1995). <p> That is, the cumulative mistake must "level off" as the learning progresses. Figure 1 shows representative learning curve graphs for these two conditions. This definition of efficient on-line reinforcement learn-ing can be related to the model of probably approximately correct (PAC) reinforcement learning proposed in <ref> (Fiechter, 1994) </ref>. The learning protocol (i.e., the interaction between the agent and the environment) is similar in the two models but the performance measures differ. Notably, in the PAC model there is no constraint on the quality (value) of the policies used by the algorithm during the learning process. <p> We have shown this model to be equivalent to the PAC model of reinforcement learning introduced in <ref> (Fiechter, 1994) </ref>. Note that the PAC algorithm for the general finite state-space reinforcement learning problem described in (Fiechter, 1994) is a model-based algorithm: the agent first constructs a model of its environment, and then uses this model to compute an (approximate) optimal policy. <p> We have shown this model to be equivalent to the PAC model of reinforcement learning introduced in <ref> (Fiechter, 1994) </ref>. Note that the PAC algorithm for the general finite state-space reinforcement learning problem described in (Fiechter, 1994) is a model-based algorithm: the agent first constructs a model of its environment, and then uses this model to compute an (approximate) optimal policy.
Reference: <author> Fiechter, C.-N. </author> <year> (1995). </year> <title> PAC associative reinforcement learning. </title> <type> Unpublished manuscript, </type> <institution> Department of Computer Science, University of Pittsburgh, </institution> <address> Pitts-burgh, PA. </address>
Reference-contexts: In particular we show how an off-line PAC algorithm can be transformed into an efficient on-line algorithm in a simple and practical way. An immediate consequence of this result is that PAC algorithms that have been proposed for the associative reinforcement learning problem <ref> (Fiechter, 1995) </ref> and for the general infinite-horizon, finite state-space reinforcement learning problem (Fiechter, 1994) can be transformed into polynomial on-line algorithms with guaranteed performances. <p> If we are considering associative reinforcement learn ing and are interested in learning policies represented by restricted classes of functions defined over some attributes of the states, then the size of an environment E can simply be the number n of state attributes <ref> (Fiechter, 1995) </ref>. Here we only make the following general assumptions regarding the size parameter and the representation used for the policies. First we assume that the rep-resentation for a policy is related to the size of the environment.
Reference: <author> Grefenstette, J. J. and Ramsey, C. L. </author> <year> (1992). </year> <title> An approach to anytime-learning. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 189-195. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haussler, D., Littlestone, N., and Warmuth, M. K. </author> <year> (1987). </year> <title> Expected mistake bounds for on-line learning algorithms. </title> <type> Technical report, </type> <institution> Department of Computer and Information Sciences, University of Califor-nia, </institution> <address> Santa Cruz, CA. </address>
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sum of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30. </pages>
Reference-contexts: We have E [S t ] = i=1 Z t x - dx = (t 1- 1) with = 1=(1 -). Since S t is a sum of independent random variables we can use the Hoeffding inequality <ref> (Hoeffding, 1963) </ref> to bound the probability that it is smaller than some value (=2)t 1- which is less than E [S t ].
Reference: <author> Kaelbling, L. P. </author> <year> (1994). </year> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 279-298. </pages>
Reference-contexts: After a failure the agent can use the reset operation to start a new trial. On the other hand, the (one-step) associative reinforcement learning problem <ref> (Kaelbling, 1994) </ref> corresponds to a special case of the first assumption whith M = 1.
Reference: <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference-contexts: 1 INTRODUCTION Reinforcement learning considers the problem of learning a behavior by trial-and-error <ref> (Kaelbling et al., 1996) </ref>. The learning agent tries to learn a task by interacting with its environment, using rewards and punishments that it receives in response to the actions it takes An important characteristic of reinforcement learning is that it is an on-line learning problem.
Reference: <author> Thrun, S. B. </author> <year> (1992). </year> <title> The role of exploration in learning control. In White, </title> <editor> D. and Sofge, D., editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY. </address>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communication of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference-contexts: On the other hand the agent wants to exploit its environment and select only the actions it thinks will result in the highest reward. In a previous work (Fiechter, 1994) we proposed a model of efficient (polynomial-time) reinforcement learning based on Valiant's PAC learning framework <ref> (Valiant, 1984) </ref>, and described a PAC algorithm for the general finite state-space reinforcement learning problem. Unfortunately, the PAC model separates the learning phase, during which rewards and punishments do not count, from the exploitation phase. It is thus essentially a model of off-line (or batch) learning.
References-found: 10

