URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR487.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: ajcbik@cs.indiana.edu  
Title: javar a prototype Java restructuring compiler  
Author: Aart J.C. Bik and Dennis B. Gannon 
Address: Lindley Hall 215, Bloomington, Indiana 47405-4101, USA  
Affiliation: Computer Science Dept., Indiana University  
Abstract: In this paper we show how implicit parallelism in Java programs can be made explicit by a restructuring compiler using the multi-threading mechanism of the language. In particular, we focus on automatically exploiting implicit parallelism in loops and multi-way recursive methods. Expressing parallelism in Java itself clearly has the advantage that the transformed program remains portable. After compilation of the transformed Java program into bytecode, speedup can be obtained on any platform on which the implementation of the JVM (Java Virtual Machine) supports the true parallel execution of threads. Moreover, we will see that the transformations presented in this paper only induce a slight overhead on uni-processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'.
Reference: [2] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference: [3] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference: [4] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1993. </year> <title> A Book Series on Loop Transformations for Restructuring Compilers. </title>
Reference-contexts: Because the same amount of work is done in each iteration, block-scheduling has been selected (see section 3.3.3, where adding `implements Schedules' to the transformed class enables the compiler to use the symbolic representation of a scheduling policy): void instanceMethod (int N) - LoopWorker_1 [] worker_1 = new LoopWorker_1 <ref> [4] </ref>; for (int i_1 = 0; i_1 &lt; 4; i_1++) worker_1 [i_1] = new LoopWorker_1 (this); LoopWorker.parloop (0, N, worker_1, 0, SCHED_BLOCK); - Now, suppose that a similar loop that operates on class variables appears in a class method classMethod () of a class SimpleLoop2: class SimpleLoop2 - static double [] <p> - - Finally, the original loop is replaced by the construct shown below, in which the value of N and pixels is supplied to every loop-worker (see section 3.3.3): void computePixels (int N) - int [] pixels = new int [N * N]; - LoopWorker_3 [] worker_3 = new LoopWorker_3 <ref> [4] </ref>; for (int i_3 = 0; i_3 &lt; 4; i_3++) worker_3 [i_3] = new LoopWorker_3 (this, N, pixels); LoopWorker.parloop (0, N, worker_3, 0, SCHED_BLOCK); - ... more operations ... - 18 In figure 11, we show the execution time of the serial and parallel version of the previous loop (excluding the
Reference: [5] <author> Utpal Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994. </year> <title> A Book Series on Loop Transformations for Restructuring Compilers. </title>
Reference: [6] <author> Aart J.C. Bik and Dennis B. Gannon. </author> <title> Automatically exploiting implicit parallelism in Java. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 9(6) </volume> <pages> 579-619, </pages> <year> 1997. </year>
Reference-contexts: A Java program MyClass.java is used as input of our source to source Java restructuring compiler javar. fl This project is supported by DARPA under contract ARPA F19628-94-C-0057 through a subcontract from Syracuse University. This paper has been published as journal paper in <ref> [6] </ref>. 1 First, the compiler identifies the loops and the multi-way recursive methods that can exploit implicit parallelism. Parallel loops are either detected automatically by means of data dependence analysis, or are identified explicitly by the programmer by means of annotations.
Reference: [7] <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1987. </year> <month> 40 </month>
Reference: [8] <author> Ron G. Cytron. </author> <title> DOACROSS, beyond vectorization for multiprocessors. </title> <booktitle> In Proceed--ings of the International Conference on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <year> 1986. </year>
Reference-contexts: Although there are several methods to enforce synchronization in a DOACROSS-loop (see e.g. <ref> [8, 9, 23] </ref>[24, 25, 40]), in this paper we focus on random synchronization [39, p75-83][41, p289-295], where synchronization variables are implemented as bit-arrays that provide one bit for each iteration.
Reference: [9] <author> Ron G. Cytron. </author> <title> Limited processor scheduling of DOACROSS loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 226-234, </pages> <year> 1987. </year>
Reference-contexts: Although there are several methods to enforce synchronization in a DOACROSS-loop (see e.g. <ref> [8, 9, 23] </ref>[24, 25, 40]), in this paper we focus on random synchronization [39, p75-83][41, p289-295], where synchronization variables are implemented as bit-arrays that provide one bit for each iteration.
Reference: [10] <author> H.M. Deitel and P.J. Deitel. </author> <title> Java, How to Program. </title> <publisher> Prentice-Hall, </publisher> <year> 1997. </year>
Reference-contexts: available processors time while threads are waiting fails due to the overhead involved. 5.4 Tree Traversals In this section, we illustrate the parallelization of multi-way recursive methods in full detail with two very simple tree traversal methods for trees containing integer data items that are implemented as follows (see e.g. <ref> [10] </ref> for discussion of how some typical data structures can be implemented in Java): class Tree - int val; Tree left, right ...
Reference: [11] <author> C.N. Fischer and R.J. LeBlanc. </author> <title> Crafting a Compiler. </title> <address> Benjamin-Cummings, Menlo Park, California, </address> <year> 1988. </year>
Reference-contexts: Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'.
Reference: [12] <author> David Flanagan. </author> <title> Java in a Nutshell. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1996. </year>
Reference: [13] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction To obtain true portability, a program written in the Java programming language <ref> [13] </ref> is compiled into the architectural neutral instructions (bytecode) of an abstract machine, the JVM (Java Virtual Machine) [21], rather than into native machine code. In this manner, a compiled Java program can run on any platform on which an implementation of the JVM is available. <p> However, because the Java language specification does not guarantee fairness between runnable threads with equal priority, and in Java there is no way to declare the elements of an array as volatile (and since the elements of bits are accessed asynchronously, other processors may fail to see changes <ref> [13, ch.17] </ref>), this approach may be unsuited for particular implementations of the Java Virtual Machine. 3.3 Actual Loop Parallelization Now, we are ready to discuss the steps that can be taken by a compiler to exploit implicit loop parallelism in a stride-1 loop of the following form, where we assume that <p> for a void-method: if (wi_x != null) - try - wi_x.join (); - catch (InterruptedException e_x) -- ri = wi_x.result; - If ri is a local variable, it may be necessary to add a dummy assignment to the declaration of this variable to preserve the definite assignment property of Java <ref> [13] </ref>, because the original assignment has been moved into two conditional statements. After these transformations have been applied to an n-way recursive method, n-way forks will be performed in the top levels 0 : : : c of the method invocation tree in case CUT DEPTH = c.
Reference: [14] <author> C.A.R. Hoare. </author> <title> Quick sort. </title> <journal> Computer Journal, </journal> <year> 1962. </year>
Reference-contexts: In the next section we will see that starting some additional threads can also be useful to overcome less trivial load imbalancing. 5.5 Quick Sorting As an example of a typical divide-and-conquer algorithm, consider the following imple mentation of quick-sorting <ref> [14] </ref> in which, as advocated in [37], small sub-arrays are sorted by insertion sorting to prevent further recursive method invocations for small sub-arrays: public class Sort - ... final static void quicksort (int [] a, int q, int r) - if ((r - q) &lt;= 20) - // Insertion Sorting of
Reference: [15] <author> C.A.R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <year> 1974. </year>
Reference-contexts: illustrated in figure 2: 2 code1; MyThread t = new MyThread (); t.start (); code2; t.join (); // may throw InterruptedException code3; class MyThread extends Thread - public void run () - ... - The Java programming language supports a synchronization mechanism that is based on the concept of monitors <ref> [15] </ref>. Mutual exclusion amongst different threads that call methods with the qualifier `synchronized' on the same object is guaranteed. Once a lock of a particular object has been acquired, a thread can suspends itself by calling the wait ()-method on this object.
Reference: [16] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <booktitle> 1978. </booktitle> <volume> Volume 1. </volume>
Reference: [17] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Programming. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Moreover, because parallelism is expressed in Java itself, the transformed program remains portable and speedup can be obtained on any platform on which the Java bytecode interpreter supports the true parallel execution of threads (typically a shared-address-space architecture <ref> [17] </ref>), whereas we will see that the transformations presented in this paper only induce a slight overhead on uni-processors. In figure 1, we illustrate our approach to automatically exploiting implicit parallelism in Java programs. <p> virtual tree of method invocations is traversed, we will visualize the parallelization of such methods using trees. 10 The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [17, 19, 35, 36] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [18] <author> Doug Lea. </author> <title> Concurrent Programming in Java. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1997. </year>
Reference: [19] <author> Ted G. Lewis. </author> <title> Foundations of Parallel Programming. </title> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, </address> <year> 1994. </year>
Reference-contexts: virtual tree of method invocations is traversed, we will visualize the parallelization of such methods using trees. 10 The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [17, 19, 35, 36] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [20] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> On reducing data synchronization in multi-processed loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:105-109, </volume> <year> 1987. </year>
Reference-contexts: For DOACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [20, 23, 24, 25, 41] </ref>). Setting the bit of iteration i of a synchronization with number k is implemented as `sync x [k].doPost (i)'.
Reference: [21] <author> Tim Lindholm and Frank Yellin. </author> <title> The Java Virtual Machine Specification. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction To obtain true portability, a program written in the Java programming language [13] is compiled into the architectural neutral instructions (bytecode) of an abstract machine, the JVM (Java Virtual Machine) <ref> [21] </ref>, rather than into native machine code. In this manner, a compiled Java program can run on any platform on which an implementation of the JVM is available.
Reference: [22] <author> Jon Meyer and Troy Downing. </author> <title> Java Virtual Machine. </title> <publisher> O'Reilly & Associates, </publisher> <address> Se-bastopol, CA, </address> <year> 1997. </year>
Reference: [23] <author> Samuel P. Midkiff. </author> <title> The Dependence Analysis and Synchronization of Parallel Programs. </title> <type> PhD thesis, </type> <institution> C.S.R.D., </institution> <year> 1993. </year>
Reference-contexts: Although there are several methods to enforce synchronization in a DOACROSS-loop (see e.g. <ref> [8, 9, 23] </ref>[24, 25, 40]), in this paper we focus on random synchronization [39, p75-83][41, p289-295], where synchronization variables are implemented as bit-arrays that provide one bit for each iteration. <p> For DOACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [20, 23, 24, 25, 41] </ref>). Setting the bit of iteration i of a synchronization with number k is implemented as `sync x [k].doPost (i)'.
Reference: [24] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 544-551, </pages> <year> 1986. </year>
Reference-contexts: For DOACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [20, 23, 24, 25, 41] </ref>). Setting the bit of iteration i of a synchronization with number k is implemented as `sync x [k].doPost (i)'.
Reference: [25] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1485-1495, </volume> <year> 1987. </year>
Reference-contexts: For DOACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [20, 23, 24, 25, 41] </ref>). Setting the bit of iteration i of a synchronization with number k is implemented as `sync x [k].doPost (i)'.
Reference: [26] <author> Michael Morrison. </author> <title> Java Unleashed. </title> <address> Samsnet, Indianapolis, Indiana, </address> <year> 1996. </year>
Reference: [27] <author> Patrick Naughton. </author> <title> The Java Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: An efficiency ranging from 75% up to over 90% is obtained once N exceeds 220; 000. 15 16 5.1.2 Double Loops The following double loop is based on an applet example found in <ref> [27] </ref>: class DoubleLoop - void computePixels (int N) - int [] pixels = new int [N * N]; for (int y = 0; y &lt; N; y++) int r = (x^y) & 0xff; int b = (x*4^y*4) & 0xff; pixels [y*N + x] = (255 &lt;< 24) | (r &lt;< 16)
Reference: [28] <author> Patrick Niemeyer and Joshua Peck. </author> <title> Exploring Java. </title> <publisher> O'Reilly & Associates, </publisher> <address> Se-bastopol, CA, </address> <year> 1996. </year> <month> 41 </month>
Reference: [29] <author> Scott Oaks and Henry Wong. </author> <title> Java Threads. </title> <publisher> O'Reilly & Associates, </publisher> <address> Sebastopol, CA, </address> <year> 1997. </year>
Reference: [30] <author> David A. Padua, David J. Kuck, and Duncan H. Lawrie. </author> <title> High speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:763-776, </volume> <year> 1980. </year>
Reference: [31] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <year> 1986. </year>
Reference: [32] <author> Thomas W. Parsons. </author> <title> Introduction to Compiler Construction. </title> <publisher> Computer Science Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'. <p> a [i]; a [i] = a [j]; a [j] = t; while (j != i); if ((a [r] & BIT [b]) == 0) j++; radixsort (a,l,j-1,b-1); radixsort (a,j,r, b-1); - - The static initializer show below can be used to initialize array BIT: static int [] BIT = new int <ref> [32] </ref>; static - int k = 1; BIT [i] = k; - An array of positive 32-bit integers (with a zero most significant bit), for example, can be sorted by calling radixsort () with b = 30.
Reference: [33] <author> Constantine D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism to start a number of threads that will execute the different iterations of the loop in parallel [40, 385-387]. The way in which iterations are assigned to the threads is dependent on the scheduling policy <ref> [33, ch4] </ref>[39, p73-74][40, p387-392][41, 296-298]. In a pre-scheduling policy, iterations are assigned statically to threads, for instance, in a block-wise or cyclic fashion (viz. figure 3). In a self-scheduling policy, threads enter a critical section to obtain a next chunk of iterations dynamically. <p> Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'. <p> i = l_x; i &lt; h_x; i += s_x) - sync_x [0].doPost (i); // post (ASYNC, i) sync_x [0].doWait (i-8); // wait (ASYNC, i-8) for (int j = 0; j &lt; N; j++) - However, another way to obtain parallelism in the loop shown above is to first apply loop-distribution <ref> [33, 39, 40, 41] </ref> to the i-loop, which is valid because all data dependences are lexically forward. Thereafter, the second resulting i-loop can be converted into a DOALL-loop.
Reference: [34] <author> Constantine D. Polychronopoulos, David J. Kuck, and David A. Padua. </author> <title> Execution of parallel loops on parallel processor systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 519-527, </pages> <year> 1986. </year>
Reference: [35] <author> Michael J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: virtual tree of method invocations is traversed, we will visualize the parallelization of such methods using trees. 10 The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [17, 19, 35, 36] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [36] <author> Michael J. Quinn. </author> <title> Parallel Computing: Theory and Practice. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: virtual tree of method invocations is traversed, we will visualize the parallelization of such methods using trees. 10 The most straightforward way to exploit implicit parallelism in a parallel n-way recursive method is to let a running thread assign all but one of the recursive method invocations to other threads <ref> [17, 19, 35, 36] </ref>. Although our method is based on this simple approach, the analysis shown below reveals the limitation on the corresponding speedup, and better ways of parallelizing an algorithm may exist.
Reference: [37] <author> Robert Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1988. </year> <note> Second Edition. </note>
Reference-contexts: In the next section we will see that starting some additional threads can also be useful to overcome less trivial load imbalancing. 5.5 Quick Sorting As an example of a typical divide-and-conquer algorithm, consider the following imple mentation of quick-sorting [14] in which, as advocated in <ref> [37] </ref>, small sub-arrays are sorted by insertion sorting to prevent further recursive method invocations for small sub-arrays: public class Sort - ... final static void quicksort (int [] a, int q, int r) - if ((r - q) &lt;= 20) - // Insertion Sorting of a [q..r] int i, j; int <p> Here we see that some of the load imbalancing due to imbalanced invocation trees can be resolved by starting additional threads. 5.6 Radix-Exchange Sorting An alternative divide-and-conquer sorting algorithm that can better adapt to integers with truly random bits is so-called radix-exchange sorting (see e.g. <ref> [37] </ref>). <p> 31 32 5.7 Merge Sorting In this section, we discuss the results of some experiments that have been conducted with array version and linked-lists version of merge-sorting. 5.7.1 Merge Sorting of Arrays An implementation of merge-sorting for arrays that uses insertion sorting for small sub-arrays is shown below (see e.g. <ref> [37] </ref>). <p> List (int val, next) - this.val = val; this.next = next; - - Under the assumption that each list is terminated with a special node SENTINEL that points to itself and with a data item larger than all elements in the list, merge-sorting can be implemented as follows (see e.g. <ref> [37] </ref>): final static List listMergesort (List l) - if (l.next == SENTINEL) return l; else - List a = l, b = l.next.next.next; while (b != SENTINEL) - l = l.next; b = b.next.next; - b = l.next; l.next = SENTINEL; a = listMergesort (a); b = listMergesort (b); return merge
Reference: [38] <author> Glenn L. Vanderburg et al. </author> <title> Tricks of the Java Programming Gurus. </title> <address> Samsnet, Indi-anapolis, Indiana, </address> <year> 1996. </year>
Reference: [39] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Although there are several methods to enforce synchronization in a DOACROSS-loop (see e.g. [8, 9, 23][24, 25, 40]), in this paper we focus on random synchronization <ref> [39, p75-83] </ref>[41, p289-295], where synchronization variables are implemented as bit-arrays that provide one bit for each iteration. Synchronization is enforced using non-blocking post-statements to set particular bits of synchronization variables and wait-statements to block until certain bits of these synchronization variables become set. <p> Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'. <p> i = l_x; i &lt; h_x; i += s_x) - sync_x [0].doPost (i); // post (ASYNC, i) sync_x [0].doWait (i-8); // wait (ASYNC, i-8) for (int j = 0; j &lt; N; j++) - However, another way to obtain parallelism in the loop shown above is to first apply loop-distribution <ref> [33, 39, 40, 41] </ref> to the i-loop, which is valid because all data dependences are lexically forward. Thereafter, the second resulting i-loop can be converted into a DOALL-loop.
Reference: [40] <author> Michael J. Wolfe. </author> <title> High Performance Compilers for Parallel Computers. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1996. </year>
Reference-contexts: On shared-address space architectures, parallel loops can be executed using fork/join-like parallelism to start a number of threads that will execute the different iterations of the loop in parallel <ref> [40, 385-387] </ref>. The way in which iterations are assigned to the threads is dependent on the scheduling policy [33, ch4][39, p73-74][40, p387-392][41, 296-298]. In a pre-scheduling policy, iterations are assigned statically to threads, for instance, in a block-wise or cyclic fashion (viz. figure 3). <p> Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'. <p> i = l_x; i &lt; h_x; i += s_x) - sync_x [0].doPost (i); // post (ASYNC, i) sync_x [0].doWait (i-8); // wait (ASYNC, i-8) for (int j = 0; j &lt; N; j++) - However, another way to obtain parallelism in the loop shown above is to first apply loop-distribution <ref> [33, 39, 40, 41] </ref> to the i-loop, which is valid because all data dependences are lexically forward. Thereafter, the second resulting i-loop can be converted into a DOALL-loop.
Reference: [41] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1990. </year> <month> 42 </month>
Reference-contexts: Li: for (int i = low; i &lt; high; i++) body (i); ... - Furthermore, we assume that the compiler has ascertained that loop Li can be executed in either a DOALL- or DOACROSS-like manner, possibly after standard compiler optimizations, loop normalization, and possibly other loop transformations (see e.g. <ref> [1, 11, 32, 33, 39, 40, 41] </ref> are used to enhance the opportunities of loop parallelization. The parallelization itself proceeds as follows, where, for simplicity, new identifiers that may conflict with other identifiers are denoted with a suffix ` x'. <p> For DOACROSS-like execution, the compiler adds the appropriate synchronization primitives to the loop-body according to methods described in the literature <ref> [20, 23, 24, 25, 41] </ref>). Setting the bit of iteration i of a synchronization with number k is implemented as `sync x [k].doPost (i)'. <p> i = l_x; i &lt; h_x; i += s_x) - sync_x [0].doPost (i); // post (ASYNC, i) sync_x [0].doWait (i-8); // wait (ASYNC, i-8) for (int j = 0; j &lt; N; j++) - However, another way to obtain parallelism in the loop shown above is to first apply loop-distribution <ref> [33, 39, 40, 41] </ref> to the i-loop, which is valid because all data dependences are lexically forward. Thereafter, the second resulting i-loop can be converted into a DOALL-loop.
References-found: 41

