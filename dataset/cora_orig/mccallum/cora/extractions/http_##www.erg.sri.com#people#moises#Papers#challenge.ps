URL: http://www.erg.sri.com/people/moises/Papers/challenge.ps
Refering-URL: http://www.erg.sri.com/projects/hpkb/
Root-URL: 
Title: Challenge: What is the Impact of Bayesian Networks on Learning?  
Author: Nir Friedman Moises Goldszmidt David Heckerman Stuart Russell 
Address: Berkeley, CA 94720  333 Ravenswood Ave. Menlo Park, CA 94025  One Microsoft Way Redmond, WA 98052  Berkeley, CA 94720  
Affiliation: Computer Science Div. University of California  SRI International  Microsoft Research  Computer Science Div. University of California  
Abstract: In recent years, there has been much interest in learning Bayesian networks from data. Learning such models is desirable simply because there is a wide array of off-the-shelf tools that can apply the learned models as expert systems, diagnosis engines, and decision support systems. Practitioners also claim that adaptive Bayesian networks have advantages in their own right as a non-parametric method for density estimation, data analysis, pattern classification, and modeling. Among the reasons cited we find: their semantic clarity and understandability by humans, the ease of acquisition and incorporation of prior knowledge, the ease of integration with optimal decision-making methods, the possibility of causal interpretation of learned models, and the automatic handling of noisy and missing data. In spite of these claims, and the initial success reported recently, methods that learn Bayesian networks have yet to make the impact that other techniques such as neural networks and hidden Markov models have made in applications such as pattern and speech recognition. In this paper, we challenge the research community to identify and characterize domains where induction of Bayesian networks makes the critical difference, and to quantify the factors that are responsible for that difference. In addition to formalizing the challenge, we identify research problems whose solution is, in our view, crucial for meeting this challenge.
Abstract-found: 1
Intro-found: 1
Reference: [ Buntine, 1994 ] <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225. </pages>
Reference-contexts: Second, we will measure how many causal interactions were correctly and incorrectly identified. 3 Technical Challenges Many researchers are now concentrating on learning in more expressive probabilistic models, including hybrid (discrete and continuous) models [ Lauritzen and Wer muth, 1989 ] , mixed (undirected and directed) models <ref> [ Buntine, 1994; Cooper, 1995; Spirtes et al., 1995 ] </ref> , dynamic Bayesian network models representing stochastic processes [ Russell et al., 1995 ] , and stochastic grammars [ Stolcke and Omohundro, 1993 ] .
Reference: [ Chickering and Pearl, 1996 ] <author> Chickering, D. and Pearl, J. </author> <year> (1996). </year> <title> A clinician's tool for analyzing noncompliance. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), Portland, OR, </booktitle> <volume> volume 2, </volume> <pages> pages 1269-1276. </pages>
Reference: [ Cooper, 1995 ] <author> Cooper, G. </author> <year> (1995). </year> <title> Causal discovery from data in the presence of selection bias. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 140-150, </pages> <address> Fort Lauderdale, FL. </address>
Reference-contexts: Second, we will measure how many causal interactions were correctly and incorrectly identified. 3 Technical Challenges Many researchers are now concentrating on learning in more expressive probabilistic models, including hybrid (discrete and continuous) models [ Lauritzen and Wer muth, 1989 ] , mixed (undirected and directed) models <ref> [ Buntine, 1994; Cooper, 1995; Spirtes et al., 1995 ] </ref> , dynamic Bayesian network models representing stochastic processes [ Russell et al., 1995 ] , and stochastic grammars [ Stolcke and Omohundro, 1993 ] .
Reference: [ Cooper and Herskovits, 1992 ] <author> Cooper, G. and Her-skovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference-contexts: Several groups have worked on learning structure from scratch [ Spirtes et al., 1993; Pearl, 1995; Friedman et al., 1997 ] or with weak constraints such as variable ordering <ref> [ Cooper and Herskovits, 1992, for example ] </ref> , while others have worked on learning structure by refining an initial model [ Heckerman et al., 1994 ] .
Reference: [ Friedman et al., 1997 ] <author> Friedman, N., Geiger, D., and Goldszmidt, M. </author> <year> (1997). </year> <title> Bayesian network classifiers. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [ Friedman and Goldszmidt, 1996 ] <author> Friedman, N. and Goldszmidt, M. </author> <title> (1996) Learning Bayesian networks with local structure. </title> <booktitle> In Proceedings of Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Portlan, OR. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Geiger and Heckerman, 1994 ] <author> Geiger, D. and Hecker-man, D. </author> <year> (1994). </year> <title> Learning Gaussian networks. </title> <booktitle> In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <pages> pages 235-243. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Golmard and Mallet, 1991 ] <author> Golmard, J.-L. and Mallet, A. </author> <year> (1991). </year> <title> Learning probabilities in causal trees from incomplete databases. </title> <journal> Revue d'Intelligence Artifi-cielle, </journal> <volume> 5 </volume> <pages> 93-106. </pages>
Reference-contexts: Learning probabilities, which is non-trivial when the network contains hidden variables or the data set has missing values, can be done by a variety of methods including EM [ Lauritzen, 1991; Lauritzen, 1995; Spiegel-halter et al., 1993; ?; Spiegelhalter and Cowell, 1992; Heckerman, 1996 ] gradient-based methods <ref> [ Laskey, 1990; Golmard and Mallet, 1991; Neal, 1992; ? ] </ref> , and Monte-Carlo techniques [ Neal, 1993 ] . These researchers have cited several benefits of using the Bayesian-network representation, with its causal interpretation, as a tool for learning: 1. Incorporation of prior knowledge.
Reference: [ Heckerman, 1995 ] <author> Heckerman, D. </author> <year> (1995). </year> <title> A Bayesian approach for learning causal networks. </title> <booktitle> In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Montreal, </address> <publisher> QU, </publisher> <pages> pages 285-295. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Heckerman, 1996 ] <author> Heckerman, D. </author> <year> (1996). </year> <title> A Tutorial on learning with Bayesian networks. </title> <note> Microsoft Research Technical Report MSR-TR-95-06. Updated Nov. </note> <year> 1996. </year>
Reference: [ Heckerman et al., 1994 ] <author> Heckerman, D., Geiger, D., and Chickering, M. </author> <year> (1994). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington. </address>
Reference-contexts: Several groups have worked on learning structure from scratch [ Spirtes et al., 1993; Pearl, 1995; Friedman et al., 1997 ] or with weak constraints such as variable ordering [ Cooper and Herskovits, 1992, for example ] , while others have worked on learning structure by refining an initial model <ref> [ Heckerman et al., 1994 ] </ref> .
Reference: [ Heckerman et al., 1995 ] <author> Heckerman, D., Mamdani, A., and Wellman, M. </author> <year> (1995). </year> <title> Real-world applications of Bayesian networks. </title> <journal> Communications of the ACM, </journal> <volume> 38. </volume>
Reference-contexts: The representation was originally designed to encode the uncertain knowledge of an expert [ Wright, 1921; Howard and Matheson, 1981; Pearl, 1988 ] , and indeed today, they play a crucial role in modern expert systems, diagnosis engines, and decision support systems <ref> [ Heckerman et al., 1995 ] </ref> . They also have become the representation of choice among researchers interested in uncertainty in AI. <p> The naturalness of using causal information directly in constructing formally characterizable knowledge structures has made it possible to encode the knowledge of many experts. As a result, Bayesian networks have been incorporated into many expert systems, diagnosis engines, and decision-support systems <ref> [ Heckerman et al., 1995 ] </ref> . Nonetheless, it is often difficult and time-consuming to construct Bayesian networks from expert knowledge alone.
Reference: [ Howard and Matheson, 1981 ] <author> Howard, R. and Mathe-son, J. </author> <year> (1981). </year> <title> Influence diagrams. </title> <editor> In Howard, R. and Matheson, J., editors, </editor> <booktitle> Readings on the Principles and Applications of Decision Analysis, </booktitle> <volume> volume II, </volume> <pages> pages 721-762. </pages> <institution> Strategic Decisions Group, </institution> <address> Menlo Park, CA. </address>
Reference-contexts: 1 Introduction A Bayesian network is a graphical representation of the joint probability distribution for a set of variables. The representation was originally designed to encode the uncertain knowledge of an expert <ref> [ Wright, 1921; Howard and Matheson, 1981; Pearl, 1988 ] </ref> , and indeed today, they play a crucial role in modern expert systems, diagnosis engines, and decision support systems [ Heckerman et al., 1995 ] .
Reference: [ Laskey, 1990 ] <author> Laskey, K. B. </author> <year> (1990). </year> <title> Adapting connectionist learning to Bayes networks. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 4 </volume> <pages> 261-282. </pages>
Reference-contexts: Learning probabilities, which is non-trivial when the network contains hidden variables or the data set has missing values, can be done by a variety of methods including EM [ Lauritzen, 1991; Lauritzen, 1995; Spiegel-halter et al., 1993; ?; Spiegelhalter and Cowell, 1992; Heckerman, 1996 ] gradient-based methods <ref> [ Laskey, 1990; Golmard and Mallet, 1991; Neal, 1992; ? ] </ref> , and Monte-Carlo techniques [ Neal, 1993 ] . These researchers have cited several benefits of using the Bayesian-network representation, with its causal interpretation, as a tool for learning: 1. Incorporation of prior knowledge.
Reference: [ Lauritzen, 1991 ] <author> Lauritzen, S. L. </author> <year> (1991). </year> <title> The EM algorithm for graphical association models with missing data. </title> <type> Technical Report TR-91-05, </type> <institution> Department of Statistics, Aalborg University. </institution>
Reference: [ Lauritzen, 1995 ] <author> Lauritzen, S. L. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 </volume> <pages> 191-201. </pages>
Reference: [ Lauritzen and Wermuth, 1989 ] <author> Lauritzen, S. and Wer-muth, N. </author> <year> (1989). </year> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics, </journal> <volume> 17 </volume> <pages> 31-57. </pages>
Reference: [ MacKay, 1992 ] <author> MacKay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference: [ Neal, 1992 ] <author> Neal, R. M. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference-contexts: Learning probabilities, which is non-trivial when the network contains hidden variables or the data set has missing values, can be done by a variety of methods including EM [ Lauritzen, 1991; Lauritzen, 1995; Spiegel-halter et al., 1993; ?; Spiegelhalter and Cowell, 1992; Heckerman, 1996 ] gradient-based methods <ref> [ Laskey, 1990; Golmard and Mallet, 1991; Neal, 1992; ? ] </ref> , and Monte-Carlo techniques [ Neal, 1993 ] . These researchers have cited several benefits of using the Bayesian-network representation, with its causal interpretation, as a tool for learning: 1. Incorporation of prior knowledge.
Reference: [ Neal, 1993 ] <author> Neal, R. </author> <year> (1993). </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: the data set has missing values, can be done by a variety of methods including EM [ Lauritzen, 1991; Lauritzen, 1995; Spiegel-halter et al., 1993; ?; Spiegelhalter and Cowell, 1992; Heckerman, 1996 ] gradient-based methods [ Laskey, 1990; Golmard and Mallet, 1991; Neal, 1992; ? ] , and Monte-Carlo techniques <ref> [ Neal, 1993 ] </ref> . These researchers have cited several benefits of using the Bayesian-network representation, with its causal interpretation, as a tool for learning: 1. Incorporation of prior knowledge. Bayesian networks facilitate the translation of human knowledge into probabilistic form, making it suitable for refine ment by data. 2.
Reference: [ Olesen et al., 1992 ] <author> Olesen, K. G., Lauritzen, S. L., and Jensen, F. V. </author> <year> (1992). </year> <title> aHUGIN: A system for creating adaptive causal probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence (UAI-92), </booktitle> <address> Stanford, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Pearl, 1988 ] <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: 1 Introduction A Bayesian network is a graphical representation of the joint probability distribution for a set of variables. The representation was originally designed to encode the uncertain knowledge of an expert <ref> [ Wright, 1921; Howard and Matheson, 1981; Pearl, 1988 ] </ref> , and indeed today, they play a crucial role in modern expert systems, diagnosis engines, and decision support systems [ Heckerman et al., 1995 ] . <p> Together, these two components represent a unique joint probability distribution over the complete set of variables X <ref> [ Pearl, 1988 ] </ref> . The joint distribution is given by the following equation: p (X) = i=1 It can be shown that this equation implies the conditional independence semantics of the graphical structure given earlier.
Reference: [ Pearl, 1995 ] <author> Pearl, J. </author> <year> (1995). </year> <title> Causal diagrams for empirical research. </title> <journal> Biometrika, </journal> <volume> 82 </volume> <pages> 669-710. </pages>
Reference: [ Robins, 1986 ] <author> Robins, J. </author> <year> (1986). </year> <title> A new approach to causal inference in mortality studies with sustained exposure results. </title> <journal> Mathematical Modelling, </journal> <volume> 7 </volume> <pages> 1393-1512. </pages>
Reference: [ Rubin, 1978 ] <author> Rubin, D. </author> <year> (1978). </year> <title> Bayesian inference for causal effects: The role of randomization. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 34-58. </pages>
Reference-contexts: The tricky problem arises when the data is missing due to specific values that other variables take. In this case, the failure to observe a variable may in itself be informative about the true state of the world <ref> [ Rubin, 1978 ] </ref> . In principle, a successful induction algorithm would be able to take advantage of a good model about the relationship between the state of the world and what variables are missing. For this challenge we will provide both synthetic data and real-life data.
Reference: [ Russell et al., 1995 ] <author> Russell, S., Binder, J., Koller, D., and Kanazawa, K. </author> <year> (1995). </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1146-52, </pages> <address> Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Challenges Many researchers are now concentrating on learning in more expressive probabilistic models, including hybrid (discrete and continuous) models [ Lauritzen and Wer muth, 1989 ] , mixed (undirected and directed) models [ Buntine, 1994; Cooper, 1995; Spirtes et al., 1995 ] , dynamic Bayesian network models representing stochastic processes <ref> [ Russell et al., 1995 ] </ref> , and stochastic grammars [ Stolcke and Omohundro, 1993 ] . Another important problem is the specification of prior distributions over parameters|most current work makes strong assumptions such as parameter independence and likelihood equivalence.
Reference: [ Saul et al., 1996 ] <author> Saul, L., Jaakkola, T., and Jordan, M. </author> <year> (1996). </year> <title> Mean field theory for sigmoid belief networks. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 61-76. </pages>
Reference-contexts: MacKay [1992] and others are working on hierarchical models that relax the assumption of parameter independence. A third area of active research is the development of efficient approximation algorithms for probabilistic inference|a key component of learning|including Monte-Carlo [ Thomas et al., 1992 ] and variational methods <ref> [ Saul et al., 1996 ] </ref> . There are two technical challenges that we believe are critical to the success of Bayesian networks and for which much work needs to be done. One challenge is the efficient handling of incomplete data.
Reference: [ Spiegelhalter et al., 1993 ] <author> Spiegelhalter, D., Dawid, P., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-282. </pages>
Reference: [ Spiegelhalter and Cowell, 1992 ] <author> Spiegelhalter, D. J. and Cowell, R. G. </author> <year> (1992). </year> <title> Learning in probabilistic expert systems. </title> <editor> In Bernardo, J. M., Berger, J. O., Dawid, A. P., and Smith, A. F. M., editors, </editor> <booktitle> Bayesian Statistics 4, </booktitle> <publisher> Oxford. Oxford University Press. </publisher>
Reference: [ Spirtes et al., 1993 ] <author> Spirtes, P., Glymour, C., and Scheines, R. </author> <year> (1993). </year> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: [ Spirtes et al., 1995 ] <author> Spirtes, P., Meek, C., and Richard-son, T. </author> <year> (1995). </year> <title> Causal inference in the presence of latent variables and selection bias. </title> <booktitle> In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Montreal, </address> <publisher> QU, </publisher> <pages> pages 499-506. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Second, we will measure how many causal interactions were correctly and incorrectly identified. 3 Technical Challenges Many researchers are now concentrating on learning in more expressive probabilistic models, including hybrid (discrete and continuous) models [ Lauritzen and Wer muth, 1989 ] , mixed (undirected and directed) models <ref> [ Buntine, 1994; Cooper, 1995; Spirtes et al., 1995 ] </ref> , dynamic Bayesian network models representing stochastic processes [ Russell et al., 1995 ] , and stochastic grammars [ Stolcke and Omohundro, 1993 ] .
Reference: [ Stolcke and Omohundro, 1993 ] <author> Stolcke, A. and Omo-hundro, S. </author> <year> (1993). </year> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <volume> volume 5, </volume> <pages> pages 11-18, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: expressive probabilistic models, including hybrid (discrete and continuous) models [ Lauritzen and Wer muth, 1989 ] , mixed (undirected and directed) models [ Buntine, 1994; Cooper, 1995; Spirtes et al., 1995 ] , dynamic Bayesian network models representing stochastic processes [ Russell et al., 1995 ] , and stochastic grammars <ref> [ Stolcke and Omohundro, 1993 ] </ref> . Another important problem is the specification of prior distributions over parameters|most current work makes strong assumptions such as parameter independence and likelihood equivalence. MacKay [1992] and others are working on hierarchical models that relax the assumption of parameter independence.
Reference: [ Thiesson, 1995 ] <author> Thiesson, B. </author> <year> (1995). </year> <title> Score and information for recursive exponential models with incomplete data. </title> <type> Technical report, </type> <institution> Institute of Electronic Systems, Aalborg University, Aalborg, Denmark. </institution>
Reference: [ Thomas et al., 1992 ] <author> Thomas, A., Spiegelhalter, D., and Gilks, W. </author> <year> (1992). </year> <title> Bugs: A program to perform Bayesian inference using Gibbs sampling. </title> <editor> In Bernardo, J., Berger, J., Dawid, A., and Smith, A., editors, </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pages 837-842. </pages> <publisher> Oxford University Press. </publisher>
Reference-contexts: MacKay [1992] and others are working on hierarchical models that relax the assumption of parameter independence. A third area of active research is the development of efficient approximation algorithms for probabilistic inference|a key component of learning|including Monte-Carlo <ref> [ Thomas et al., 1992 ] </ref> and variational methods [ Saul et al., 1996 ] . There are two technical challenges that we believe are critical to the success of Bayesian networks and for which much work needs to be done. One challenge is the efficient handling of incomplete data.
Reference: [ Wright, 1921 ] <author> Wright, S. </author> <year> (1921). </year> <title> Correlation and causation. </title> <journal> Journal of Agricultural Research, </journal> <volume> 20 </volume> <pages> 557-585. </pages>
Reference-contexts: 1 Introduction A Bayesian network is a graphical representation of the joint probability distribution for a set of variables. The representation was originally designed to encode the uncertain knowledge of an expert <ref> [ Wright, 1921; Howard and Matheson, 1981; Pearl, 1988 ] </ref> , and indeed today, they play a crucial role in modern expert systems, diagnosis engines, and decision support systems [ Heckerman et al., 1995 ] .
References-found: 35

