URL: ftp://ftp.cs.toronto.edu/pub/tap/plate.ieee95.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~tap/
Root-URL: 
Email: tap@ai.utoronto.ca  
Title: Holographic Reduced Representations  
Author: Tony Plate 
Address: Toronto, Ontario, Canada, M5S 1A4  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive proper ties.
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Milton Abramowitz and Irene A. </author> <title> Stegun, editors. Handbook of mathematical functions with formulas, graphs, and mathematical tables. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: is the standard "error function" and tail (x) is the area under the (normalized) normal probability density function beyond x (in one tail): erfc (x) = 2 Z 1 e t 2 tail (x) = 1 2 x t 2 1 erfc p The following inequality from Abramowitz and Stegun <ref> [ 1 ] </ref> and a simplification are used: erfc (x) &lt; 2 x + x 2 + 4 erfc (x) &lt; 1 p e x 2 The probability of correctly identifying all items in the trace can first simplified by chosing t = 0:5 and then by applying the binomial theorem:
Reference: [ 2 ] <author> N. Benvenuto and F. Piazza. </author> <title> On the Complex Backpropagation Algorithm. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40(4) </volume> <pages> 967-969, </pages> <year> 1992. </year>
Reference-contexts: The rest of the system, including clean-up memories, would have to be able to work with complex vectors. There has been some research on creating adapting neural network architectures to work with units with complex valued activations (e.g., <ref> [ 2 ] </ref> ). One of the problems with convolution memories is the noisy results they give. The noise can be reduced if the encoding vectors have uniform power in the frequency domain. Under this condition the approximate inverse is equal to the exact inverse.
Reference: [ 3 ] <author> A. Borsellino and T. Poggio. </author> <title> Convolution and correlation algebras. </title> <journal> Kybernetik, </journal> <volume> 13 </volume> <pages> 113-122, </pages> <year> 1973. </year>
Reference-contexts: The vectors must have a finite number of non-zero elements in order for the convolution operation to be defined, and these are usually centered about the zero'th element <ref> [ 20, 3, 26 ] </ref> . 3 ~ t = ~ c fl ~ x t 1 = c 1 x 0 + c 0 x 1 t 1 = c 0 x 1 + c 1 x 0 t j = k=(n1)=2 for j = (n 1) to n 1 <p> This makes it very easy to manipulate expressions containing additions, convolutions, and scalar multiplications. This algebra has many of the same properties as the algebra considered by Borsellino and Poggio <ref> [ 3 ] </ref> and Shonemann [ 30 ] , which had aperiodic convolution as a multiplication operation over an infinite dimensional vector space restricted to vectors with a finite number of non-zero elements.
Reference: [ 4 ] <author> E. O. Brigham. </author> <title> The Fast Fourier Transform. </title> <publisher> Pren-tice Hall, Inc., </publisher> <address> New Jersey, </address> <year> 1974. </year>
Reference-contexts: This is particularly relevant to the storage capabilities of HRRs because when recursive frames are stored, convolution products e.g., obj cause ~agt eat , are the storage cues. VIII.B Using FFTs to compute convolution The fastest way to compute convolution is via Fast Fourier transforms (FFT) <ref> [ 4 ] </ref> . The computation involves a transform, an element-wise multiplication of two vectors, and an inverse transform.
Reference: [ 5 ] <author> David Casasent and Brian Telfer. </author> <title> Key and recollection vector effects on heteroassociative memory performance. </title> <journal> Applied Optics, </journal> <volume> 28(2) </volume> <pages> 272-283, </pages> <year> 1989. </year>
Reference: [ 6 ] <author> Phillip J. Davis. </author> <title> Circulant matrices. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: It has elements m a ij = a ij (where the subscripts on ~ a are interpreted modulo n). Such matrices are known as "circulant matrices" <ref> [ 6 ] </ref> . The eigenvalues of M a are the individual (complex valued) elements of the Fourier transform of ~ a. The corresponding eigenvectors are the inverse transforms of the frequency components (i.e., (1; 0; 0; :::), (0; 1; 0; :::), etc. in the frequency domain).
Reference: [ 7 ] <author> Douglas F. Elliot. </author> <title> Handbook of Digital Signal Processing Engineering Applications. </title> <publisher> Academic Press, Inc, </publisher> <address> San Diego, CA, </address> <year> 1986. </year>
Reference-contexts: This is not unexpected, as inverse filter are well known to be sensitive to noise <ref> [ 7 ] </ref> .
Reference: [ 8 ] <author> Arthur D. Fisher, Wendy L. Lippincott, and John N. Lee. </author> <title> Optical implementations of associative networks with versatile adaptive learning capabilities. </title> <journal> Applied Optics, </journal> <volume> 26(23) </volume> <pages> 5039-5054, </pages> <year> 1987. </year>
Reference-contexts: The item memories are necessary to clean up the noisy items extracted from the convolution representations. Convolution/correlation (holographic) memories have been generally regarded as inferior to matrix style associative memories for associating pairs of items, for reasons concerning capacity and constraints (see <ref> [ 37, 8 ] </ref> ). However, matrix style memories have a problem of expanding dimensionality when used for representing compositional structure. Convolution/correlation memories do not have this problem. <p> The vectors are usually distributed representations of discrete items (e.g., (images). Convolution-correlation memories (sometimes referred to as holographic-like memories) and matrix memories have been regarded as alternative methods for implementing hetero-associative memory <ref> [ 37, 19, 23, 30, 8 ] </ref> . Matrix memories have received more interest, due to their relative simplicity, their higher capacity in terms of the dimensionality of the vectors being associated, and their relative lack of constraints on those vectors. <p> VI.B Constraints on vectors Some authors have argued that the constraints on vectors necessary for holographic memories to perform well are too restrictive for holographic memories to be useful, e.g., <ref> [ 8 ] </ref> . This argument is based on the entirely valid observation that most vectors produced by sensory apparatus are unlikely to satisfy these constraints.
Reference: [ 9 ] <author> J. A. Fodor and Z. W. Pylyshyn. </author> <title> Connectionism and cognitive architecture: A critical analysis. </title> <journal> Cognition, </journal> <volume> 28 </volume> <pages> 3-71, </pages> <year> 1988. </year>
Reference: [ 10 ] <author> Robert A. Gabel and Richard A. Roberts. </author> <title> Signals and Linear systems. </title> <publisher> John Wiley & Sons, Inc, </publisher> <year> 1973. </year>
Reference-contexts: Thus the dimensionality of the resulting vectors expands with recursive convolution. 4 The problem of expanding dimensionality can be avoided entirely by the use of circular convolution, an operation well known in signal processing (e.g., see <ref> [ 10 ] </ref> ). The result of the circular convolution of two vectors of n elements has just n elements. Matrix and convolution memories provide different in-stantiations of the abstract associative memory operators set out in Section II.A. However, they are more closely related than might be suggested by this.
Reference: [ 11 ] <author> G. E. Hinton. </author> <title> Implementing semantic networks in parallel hardware. </title> <editor> In G. E. Hinton and J. A. Ander-son, editors, </editor> <booktitle> Parallel Models of Associative Memory. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <year> 1981. </year>
Reference-contexts: V.D Simple frame (slot/filler) structures Simple frame-like structures can be represented using convolution encoding in a manner analogous to cross products of roles and fillers in Hinton <ref> [ 11 ] </ref> or the frames of DUCS [ 34 ] . A frame consists of a frame label and a set of roles, each represented by a vector. An instantiated frame is the sum of the frame label and the roles (slots) convolved with their respective fillers.
Reference: [ 12 ] <author> G. E. Hinton. </author> <title> Mapping part-whole heirarchies into connectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):47-76, </volume> <year> 1990. </year>
Reference-contexts: This makes it difficult to represent relationships with recursive structure in which an association of items may be the subject of another association. Hinton <ref> [ 12 ] </ref> discusses this problem and proposes a framework in which "reduced descriptions" are used to represent parts and objects in a part-whole hierarchy (a frame-like representation). <p> This allows the construction of representa tions of objects with compositional structure. I call these Holographic Reduced Representations (HRRs), since convolution and correlation based memories are closely related to holographic storage, and they provide an implementation of Hinton's <ref> [ 12 ] </ref> reduced descriptions. I describe how HRRs and auto-associative item memories can be used to build distributed connectionist systems which manipulate complex structures. The item memories are necessary to clean up the noisy items extracted from the convolution representations. <p> Whether this causes problems remains to be seen. In any case, there are variants of circular convolution that are not commutative (Section VIII.G). Holographic reduced representations provide a way of realizing of Hinton's <ref> [ 12 ] </ref> hypothetical system that could, in the same physical set of units, either focus attention on constituents or have the whole meaning present at once. <p> VI Constraints on the vectors and the representation of features, types, and tokens In many connectionist systems, the vectors representing items are analyzed in terms of "micro-features". For example, Hinton's family trees network <ref> [ 12 ] </ref> learned micro-features representing concepts such as age and nationality. The requirement of HRRs that elements of vectors be randomly and independently distributed seems at odds with this interpretation. <p> In this machine the trace buffer contains the chunk currently being decoded, and the stack contains the portions of higher level chunks that are yet to be decoded. The chunked sequence readout machine is an example of a system that achieves Hinton's <ref> [ 12 ] </ref> objectives of being able to focus attention on constituents when necessary or have the whole "meaning" of a chunk present at once. VIII Mathematical Properties Circular convolution may be regarded as a multiplication operation over vectors: two vectors multiplied together (convolved) result in another vector. <p> The properties of such a system remain a subject for investigation. XII Conclusion Memory models using circular convolution provide a way of representing compositional structure in distributed representations. They implement Hinton's <ref> [ 12 ] </ref> suggestion that reduced descriptions should have microfeatures that are systematically related to those of their constituents. The operations involved are mostly linear and the properties of the scheme are relatively easy to analyze, especially compared to schemes such Pollack's RAAMs [ 27 ] .
Reference: [ 13 ] <author> G. E. Hinton, J. L. McClelland, and D. E. Rumel-hart. </author> <title> Distributed representations. </title> <editor> In J. L. Mc-Clelland D. E. Rumelhart and the PDP research group, editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition., </booktitle> <volume> volume I, </volume> <pages> pages 77-109. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [ 14 ] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences U.S.A., </booktitle> <volume> 79 </volume> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: The decoding operation will work with this sum of individual traces, but the retrieved items may be noisier. In some models the encoding and decoding operations are bi-linear, e.g., Murdock [ 19 ] , in others the decoding operation is non-linear, e.g., Hopfield <ref> [ 14 ] </ref> , and in others all the operations are non-linear, e.g., Willshaw [ 37 ] . To illustrate this, let I be the space of vectors representing items, and Tbe the space of vectors or matrices representing memory traces.
Reference: [ 15 ] <author> P. Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: The exact method of implementation of the item memory is unimportant. Hopfield networks are probably not a good candidate because of their low capacity in terms of the dimension of the vectors being stored. Kanerva networks <ref> [ 15 ] </ref> have sufficient capacity, but can only store binary vectors. 10 For the simulations reported in Appendix B. I stored vectors in an array and computed all dot-products in order to find the closest match.
Reference: [ 16 ] <author> G. Legendre, Y. Miyata, and P. Smolensky. </author> <title> Principles for an Integrated Connectionist/Symbolic Theory of Higher Cognition. </title> <type> Technical Report CU-CS-600-92, </type> <institution> University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: Smolensky suggests placing a hard limit on the depth of recursion in order to keep the size of the association space tractable (e.g., no structure can be more than 4 levels deep). In a later paper Legen-dre, Miyata, Smolensky <ref> [ 16 ] </ref> describe a scheme which permits a soft limit on the depth of recursion, though its properties as the limit is approached or exceeded are not clear.
Reference: [ 17 ] <author> Stephan Lewandowsky and Bennet B. Murdock. </author> <title> Memory for serial order. </title> <journal> Psychological Review, </journal> <volume> 96(1) </volume> <pages> 25-57, </pages> <year> 1989. </year>
Reference-contexts: Alternatively, chunking can be used to represent a sequence of any length in a number of memory traces. Murdock [ 19, 21 ] , and Lewandowsky and Murdock <ref> [ 17 ] </ref> propose a chaining method of representing sequences in a single memory trace, and model a large number of psychological phenomena with it.
Reference: [ 18 ] <author> Janet Metcalfe Eich. </author> <title> A composite holographic associative recall model. </title> <journal> Psychological Review, </journal> <volume> 89 </volume> <pages> 627-661, </pages> <year> 1982. </year>
Reference-contexts: Vectors that grow arbitrarily in dimension are difficult to use in practical systems. This approach has been used by a number of researchers, and the problem of expanding dimensionality has been tackled in a number of ways. Metcalfe Eich <ref> [ 18 ] </ref> and Murdock [ 20 ] both describe methods based on aperiodic convolution. Metcalfe Eich discards outside elements of convolution products to avoid expanding dimensionality. Murdock uses infinite-dimensional vectors. Smolensky [ 32 ] proposes Tensor product memories, which use a generalized outer product as the associative operator. <p> The outer product of two vectors is illustrated in Figure 1, which is intended to help with the understanding of the four subsequent figures. Figure 2shows standard aperiodic convolution, and Figure 3 shows the truncated aperiodic convolution used by Metcalfe Eich <ref> [ 18 ] </ref> .
Reference: [ 19 ] <author> B. B. Murdock. </author> <title> A distributed memory model for serial-order information. </title> <journal> Psychological Review, </journal> <volume> 90(4) </volume> <pages> 316-338, </pages> <year> 1983. </year>
Reference-contexts: The vectors are usually distributed representations of discrete items (e.g., (images). Convolution-correlation memories (sometimes referred to as holographic-like memories) and matrix memories have been regarded as alternative methods for implementing hetero-associative memory <ref> [ 37, 19, 23, 30, 8 ] </ref> . Matrix memories have received more interest, due to their relative simplicity, their higher capacity in terms of the dimensionality of the vectors being associated, and their relative lack of constraints on those vectors. <p> Memory traces can be composed by addition or the binary-OR operation. The decoding operation will work with this sum of individual traces, but the retrieved items may be noisier. In some models the encoding and decoding operations are bi-linear, e.g., Murdock <ref> [ 19 ] </ref> , in others the decoding operation is non-linear, e.g., Hopfield [ 14 ] , and in others all the operations are non-linear, e.g., Willshaw [ 37 ] . <p> An entire sequence can be represented in one memory trace, with the probability of error increasing with the length of the stored sequence. Alternatively, chunking can be used to represent a sequence of any length in a number of memory traces. Murdock <ref> [ 19, 21 ] </ref> , and Lewandowsky and Murdock [ 17 ] propose a chaining method of representing sequences in a single memory trace, and model a large number of psychological phenomena with it.
Reference: [ 20 ] <author> Bennet B. Murdock. </author> <title> A theory for the storage and retrieval of item and associative information. </title> <journal> Psychological Review, </journal> <volume> 89(6) </volume> <pages> 316-338, </pages> <year> 1982. </year>
Reference-contexts: Vectors that grow arbitrarily in dimension are difficult to use in practical systems. This approach has been used by a number of researchers, and the problem of expanding dimensionality has been tackled in a number of ways. Metcalfe Eich [ 18 ] and Murdock <ref> [ 20 ] </ref> both describe methods based on aperiodic convolution. Metcalfe Eich discards outside elements of convolution products to avoid expanding dimensionality. Murdock uses infinite-dimensional vectors. Smolensky [ 32 ] proposes Tensor product memories, which use a generalized outer product as the associative operator. <p> The vectors must have a finite number of non-zero elements in order for the convolution operation to be defined, and these are usually centered about the zero'th element <ref> [ 20, 3, 26 ] </ref> . 3 ~ t = ~ c fl ~ x t 1 = c 1 x 0 + c 0 x 1 t 1 = c 0 x 1 + c 1 x 0 t j = k=(n1)=2 for j = (n 1) to n 1
Reference: [ 21 ] <author> Bennet B. Murdock. </author> <title> Serial-order effects in a distributed-memory model. </title> <editor> In David S. Gorfein and Robert R. Hoffman, editors, </editor> <booktitle> MEMORY AND LEARNING: The Ebbinghaus Centennial Conference, </booktitle> <pages> pages 277-310. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1987. </year>
Reference-contexts: An entire sequence can be represented in one memory trace, with the probability of error increasing with the length of the stored sequence. Alternatively, chunking can be used to represent a sequence of any length in a number of memory traces. Murdock <ref> [ 19, 21 ] </ref> , and Lewandowsky and Murdock [ 17 ] propose a chaining method of representing sequences in a single memory trace, and model a large number of psychological phenomena with it. <p> It has the disadvantage that some sequences with repeated items cannot be properly represented. Another way to represent sequences is to use the entire previous sequence as context rather than just the previous item <ref> [ 21 ] </ref> . This makes it possible to store sequences with repeated of items. <p> As the sequences get longer the noise in the retrieved items increases until the items are impossible to identify. This limit can be overcome by chunking | creating new "non terminal" items representing subsequences <ref> [ 21 ] </ref> . The second sequence representation method is the more suitable one to do chunking with. Suppose we want to represent the sequence ~ a ~ b ~ c ~ d ~ e ~ f ~ g ~ h.
Reference: [ 22 ] <author> Eung Gi Paek and Demetri Psaltis. </author> <title> Optical associative memory using Fourier transform holograms. </title> <journal> Optical Engineering, </journal> <volume> 26(5) </volume> <pages> 428-433, </pages> <year> 1987. </year>
Reference: [ 23 ] <author> Ray Pike. </author> <title> Comparison of convolution and matrix distributed memory systems for associative recall and recognition. </title> <journal> Psychological Review, </journal> <volume> 91(3) </volume> <pages> 281-294, </pages> <year> 1984. </year>
Reference-contexts: The vectors are usually distributed representations of discrete items (e.g., (images). Convolution-correlation memories (sometimes referred to as holographic-like memories) and matrix memories have been regarded as alternative methods for implementing hetero-associative memory <ref> [ 37, 19, 23, 30, 8 ] </ref> . Matrix memories have received more interest, due to their relative simplicity, their higher capacity in terms of the dimensionality of the vectors being associated, and their relative lack of constraints on those vectors.
Reference: [ 24 ] <author> Tony A. </author> <title> Plate. Holographic Reduced Representations. </title> <type> Technical Report CRG-TR-91-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: The effect of similarity among the vectors on the capacity is considered at greater length in Plate <ref> [ 24 ] </ref> . The size of a structure that can be stored in (and successfully retrieved from) a HRR increases almost linearly with the vector dimension, with similar constants to those above.
Reference: [ 25 ] <author> Tony A. </author> <title> Plate. Holographic recurrent networks. </title> <editor> In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5 (NIPS*92), </booktitle> <pages> pages 34-41, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Integer powers are useful for generating some types of encoding keys (cf. Section V.A) and fractional powers can be used to represent trajectories through continuous space <ref> [ 25 ] </ref> . VIII.F Matrices corresponding to circular convolution The convolution operation can be expressed as a matrix vector multiplication. ~ a~ ~ b = M a ~ b where M a is the matrix corresponding to convolution by ~ a. <p> One reason one might want to do this could be to use a back-propagation network to learn good vector representations for items for some specific task. This is pursued in Plate <ref> [ 25 ] </ref> . IX The capacity of convolution memories and HRRs The number of associations that can be stored in a convolution memory is approximately linear in the dimensionality of the vectors. <p> Convolution can be used as a fixed mapping in a connectionist network to replace one or more of the usual weight-matrix mappings. The forward-propagation of activations and the back-propagation of gradients both can be calculated very quickly using FFTs. This possibility is pursued in <ref> [ 25 ] </ref> . It is possible to do all the calculations of HRRs entirely within the frequency domain. If all vectors were represented in the frequency domain it would not be necessary to do any FFTs and all the operations of HRRs could be done in O (n) time.
Reference: [ 26 ] <author> T. Poggio. </author> <title> On holographic models of memory. </title> <journal> Ky-bernetik, </journal> <volume> 12 </volume> <pages> 237-238, </pages> <year> 1973. </year>
Reference-contexts: The vectors must have a finite number of non-zero elements in order for the convolution operation to be defined, and these are usually centered about the zero'th element <ref> [ 20, 3, 26 ] </ref> . 3 ~ t = ~ c fl ~ x t 1 = c 1 x 0 + c 0 x 1 t 1 = c 0 x 1 + c 1 x 0 t j = k=(n1)=2 for j = (n 1) to n 1
Reference: [ 27 ] <author> J. B. Pollack. </author> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):77-105, </volume> <year> 1990. </year>
Reference-contexts: Unfortunately, Hinton does not suggest any concrete way of performing the reduction and expansion mappings. Some researchers have built models or designed frameworks in which some compositional structure is present in distributed representations. For some examples see the papers of Touretzky [ 33 ] , Pollack <ref> [ 27 ] </ref> , or Smolensky [ 32 ] . In this paper I propose a new method for representing compositional structure in distributed representations. Circular convolutions are used to construct associations of vectors. <p> In a later paper Legen-dre, Miyata, Smolensky [ 16 ] describe a scheme which permits a soft limit on the depth of recursion, though its properties as the limit is approached or exceeded are not clear. In Pollack's <ref> [ 27 ] </ref> Recursive Auto-Associative Memories (RAAMs) items, associations, and recursive associations are all represented in the same vector space. A back-propagation network learns the encoding and de coding mappings. This solves the problem of expanding dimensionality. <p> They implement Hinton's [ 12 ] suggestion that reduced descriptions should have microfeatures that are systematically related to those of their constituents. The operations involved are mostly linear and the properties of the scheme are relatively easy to analyze, especially compared to schemes such Pollack's RAAMs <ref> [ 27 ] </ref> . There is no learning entailed and the scheme works with a wide range of vectors. Systems employing HRRs must have an error-correcting auto associative memory to clean up the noisy results produced by convolution decoding.
Reference: [ 28 ] <author> R. Rosenfeld and D. S. Touretzky. </author> <title> Coarse-coded symbol memories and their properties. </title> <journal> Complex Systems, </journal> <volume> 2(4) </volume> <pages> 463-484, </pages> <year> 1988. </year>
Reference-contexts: Note that it is not necessary for elements of vectors to have continuous values for ad dition memories to work. Furthermore, their capacity can be improved by applying a suitable non-linear (e.g., threshold) function to the trace. Touretzky and Hinton [ 35 ] and Rosenfeld and Touretzky <ref> [ 28 ] </ref> discussed binary-OR memories 9 , which can be viewed as a non-linear version of an addition memories. Binary-OR memories were used in the model of Touretzky and Hinton [ 35 ] .
Reference: [ 29 ] <author> D. E. Rumelhart, G. E. Hinton, and Williams R. J. </author> <title> Learning internal representations by error propagation. </title> <editor> In J. L. McClelland D. E. Rumelhart and the PDP research group, editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, volume I. </booktitle> <publisher> Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In Appendix B it is shown that the number of pairs of vectors that can be stored in a 17 For an introduction to feed-forward networks see <ref> [ 29 ] </ref> . 12 convolution memory trace is at least k &gt; 16 ln m 2 2 where n is the vector dimension, m is the number of candidate vectors, and q is the probability of one or more errors in decoding.
Reference: [ 30 ] <author> P. H. Schonemann. </author> <title> Some algebraic relations between involutions, convolutions, and correlations, with applications to holographic memories. </title> <journal> Biological Cybernetics, </journal> <volume> 56 </volume> <pages> 367-374, </pages> <year> 1987. </year>
Reference-contexts: The vectors are usually distributed representations of discrete items (e.g., (images). Convolution-correlation memories (sometimes referred to as holographic-like memories) and matrix memories have been regarded as alternative methods for implementing hetero-associative memory <ref> [ 37, 19, 23, 30, 8 ] </ref> . Matrix memories have received more interest, due to their relative simplicity, their higher capacity in terms of the dimensionality of the vectors being associated, and their relative lack of constraints on those vectors. <p> This is done for an example in Section X.D. This idea of distributing features over the entire vector representing an item is not new. It is a linear transform and has been suggested by other authors under the name "randomization" or "random maps" (e.g., Schonemann <ref> [ 30 ] </ref> ). Care must be taken that the "ownership" of features is not confused when using this method to represent features (or attributes) of objects. Ambiguity of feature ownership can arise when multiple objects are stored in an addition memory. <p> This makes it very easy to manipulate expressions containing additions, convolutions, and scalar multiplications. This algebra has many of the same properties as the algebra considered by Borsellino and Poggio [ 3 ] and Shonemann <ref> [ 30 ] </ref> , which had aperiodic convolution as a multiplication operation over an infinite dimensional vector space restricted to vectors with a finite number of non-zero elements.
Reference: [ 31 ] <author> J. N. Slack. </author> <title> A parsing architecture based on distributed memory machines. </title> <booktitle> In Proceedings of COLING-86, </booktitle> <pages> pages 476-481. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1986. </year>
Reference-contexts: Using chunking, we could first extract the object, clean it up, and then extract its agent, giving a less noisy result. There is a tradeoff between accuracy and speed | if interme diate chunks are not cleaned up the retrievals are faster but less accurate. 12 Slack <ref> [ 31 ] </ref> suggests a distributed memory representation for trees involving convolution products that is similar to the representation suggested here, except that it uses noncircular convolution, and thus does not work with fixed width vectors. 13 Normalization of Euclidean lengths of the frame becomes an issue, see Section X.E. 7
Reference: [ 32 ] <author> P. Smolensky. </author> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):159-216, </volume> <year> 1990. </year>
Reference-contexts: Some researchers have built models or designed frameworks in which some compositional structure is present in distributed representations. For some examples see the papers of Touretzky [ 33 ] , Pollack [ 27 ] , or Smolensky <ref> [ 32 ] </ref> . In this paper I propose a new method for representing compositional structure in distributed representations. Circular convolutions are used to construct associations of vectors. The representation of an association is a vector of the same dimensionality as the vectors which are associated. <p> Metcalfe Eich [ 18 ] and Murdock [ 20 ] both describe methods based on aperiodic convolution. Metcalfe Eich discards outside elements of convolution products to avoid expanding dimensionality. Murdock uses infinite-dimensional vectors. Smolensky <ref> [ 32 ] </ref> proposes Tensor product memories, which use a generalized outer product as the associative operator. In these memories the dimensionality of the association space is exponential in the depth of recursion involved.
Reference: [ 33 ] <author> D. S. Touretzky. Boltzcons: </author> <title> Dynamic symbol structures in a connectionist network. </title> <journal> Artificial Intelligence, </journal> <volume> 42(1-2):5-46, </volume> <year> 1990. </year>
Reference-contexts: Unfortunately, Hinton does not suggest any concrete way of performing the reduction and expansion mappings. Some researchers have built models or designed frameworks in which some compositional structure is present in distributed representations. For some examples see the papers of Touretzky <ref> [ 33 ] </ref> , Pollack [ 27 ] , or Smolensky [ 32 ] . In this paper I propose a new method for representing compositional structure in distributed representations. Circular convolutions are used to construct associations of vectors. <p> One approach to representing more complex data structures in associative memory is to use three-way associations, as are used in LISP data structures (car, cdr, and address). Touretzky and Hinton [ 35 ] and Touret-zky <ref> [ 33 ] </ref> describe systems based on this idea. A major problem with this approach is that access is slow; many pointers must be followed to determine the constituents of a structure. This removes one of the major advantages of distributed representations; fast determination of similarity.
Reference: [ 34 ] <author> D. S. Touretzky and S. Geva. </author> <title> A distributed connectionist representation for concept structures. </title> <booktitle> In Proceedings of the Ninth Annual Cognitive Science Society Conference, </booktitle> <address> Hillsdale, NJ, 1987. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: Non-recursive variable binding can also be implemented easily in other types of associative memory, e.g., the triple-space of BoltzCONS [ 35 ] , or the outer product of roles and fillers in DUCS <ref> [ 34 ] </ref> . V.D Simple frame (slot/filler) structures Simple frame-like structures can be represented using convolution encoding in a manner analogous to cross products of roles and fillers in Hinton [ 11 ] or the frames of DUCS [ 34 ] . <p> , or the outer product of roles and fillers in DUCS <ref> [ 34 ] </ref> . V.D Simple frame (slot/filler) structures Simple frame-like structures can be represented using convolution encoding in a manner analogous to cross products of roles and fillers in Hinton [ 11 ] or the frames of DUCS [ 34 ] . A frame consists of a frame label and a set of roles, each represented by a vector. An instantiated frame is the sum of the frame label and the roles (slots) convolved with their respective fillers.
Reference: [ 35 ] <author> D. S. Touretzky and G. E. Hinton. </author> <title> A distributed connectionist production system. </title> <journal> Cognitive Science, </journal> <volume> 12(3) </volume> <pages> 423-466, </pages> <year> 1988. </year>
Reference-contexts: One approach to representing more complex data structures in associative memory is to use three-way associations, as are used in LISP data structures (car, cdr, and address). Touretzky and Hinton <ref> [ 35 ] </ref> and Touret-zky [ 33 ] describe systems based on this idea. A major problem with this approach is that access is slow; many pointers must be followed to determine the constituents of a structure. <p> Note that it is not necessary for elements of vectors to have continuous values for ad dition memories to work. Furthermore, their capacity can be improved by applying a suitable non-linear (e.g., threshold) function to the trace. Touretzky and Hinton <ref> [ 35 ] </ref> and Rosenfeld and Touretzky [ 28 ] discussed binary-OR memories 9 , which can be viewed as a non-linear version of an addition memories. Binary-OR memories were used in the model of Touretzky and Hinton [ 35 ] . <p> Touretzky and Hinton <ref> [ 35 ] </ref> and Rosenfeld and Touretzky [ 28 ] discussed binary-OR memories 9 , which can be viewed as a non-linear version of an addition memories. Binary-OR memories were used in the model of Touretzky and Hinton [ 35 ] . IV The need for reconstructive item memories Convolution memories share the inability of addition memories to provide accurate reconstructions. <p> This binding method allows multiple instances of variable in trace to be substituted for in a single-operation (approximately). Non-recursive variable binding can also be implemented easily in other types of associative memory, e.g., the triple-space of BoltzCONS <ref> [ 35 ] </ref> , or the outer product of roles and fillers in DUCS [ 34 ] .
Reference: [ 36 ] <author> Elke U. Weber. </author> <title> Expectation and variance of item resemblance distributions in a convolution-correlation model of distributed memory. </title> <journal> Journal of Mathematical Psychology, </journal> <volume> 32 </volume> <pages> 1-43, </pages> <year> 1988. </year>
Reference-contexts: However, this requires taking into account the co variances of the noise terms in the different elements. Extensive tables of variances for dot products of various convolution products have been compiled by Weber <ref> [ 36 ] </ref> for aperiodic convolution. Unfortunately, these do not apply exactly to circular convolution. The means and variances for dot products of some common circular convolution products are given in Table 1 in Section VIII.A.
Reference: [ 37 ] <author> D. Willshaw. </author> <title> Holography, associative memory, and inductive generalization. </title> <editor> In G. E. Hinton and J. A. Anderson, editors, </editor> <title> Parallel models of associative memory. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: The item memories are necessary to clean up the noisy items extracted from the convolution representations. Convolution/correlation (holographic) memories have been generally regarded as inferior to matrix style associative memories for associating pairs of items, for reasons concerning capacity and constraints (see <ref> [ 37, 8 ] </ref> ). However, matrix style memories have a problem of expanding dimensionality when used for representing compositional structure. Convolution/correlation memories do not have this problem. <p> The vectors are usually distributed representations of discrete items (e.g., (images). Convolution-correlation memories (sometimes referred to as holographic-like memories) and matrix memories have been regarded as alternative methods for implementing hetero-associative memory <ref> [ 37, 19, 23, 30, 8 ] </ref> . Matrix memories have received more interest, due to their relative simplicity, their higher capacity in terms of the dimensionality of the vectors being associated, and their relative lack of constraints on those vectors. <p> In some models the encoding and decoding operations are bi-linear, e.g., Murdock [ 19 ] , in others the decoding operation is non-linear, e.g., Hopfield [ 14 ] , and in others all the operations are non-linear, e.g., Willshaw <ref> [ 37 ] </ref> . To illustrate this, let I be the space of vectors representing items, and Tbe the space of vectors or matrices representing memory traces. There are often constraints on the vectors, e.g., they should be nearly orthogonal. <p> Figure 2shows standard aperiodic convolution, and Figure 3 shows the truncated aperiodic convolution used by Metcalfe Eich [ 18 ] . The 3 The exception is the non-linear correlograph of Willshaw <ref> [ 37 ] </ref> , first published in 1969. 4 For the sake of mathematical elegance, many authors have considered the vectors to have an infinite number of elements centered on the zero'th element, i.e., indexed from 1 through 0 to 1. <p> Representing more complex structure Pairs of items are easy to represent in many types of associative memory, but convolution memory is also suited to the representation of more complex structure. 10 Although most of this paper assumes items are represented as real vectors, convolution memories also work with binary vectors <ref> [ 37 ] </ref> . V.A Sequences Sequences can be represented in a number of ways using convolution encoding. An entire sequence can be represented in one memory trace, with the probability of error increasing with the length of the stored sequence.
Reference: [ 38 ] <author> D. Willshaw and P. Dayan. </author> <title> Optimal plasticity from matrix memories: What goes up must come down, </title> <year> 1990. </year>
References-found: 38

