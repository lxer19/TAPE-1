URL: http://www.cs.ucsd.edu/users/mihir/papers/pcp-survey.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mihir/papers/complexity-papers.html
Root-URL: http://www.cs.ucsd.edu
Email: E-mail: mihir@cs.ucsd.edu  
Date: September 1996  
Address: Mail Code 0114  9500 Gilman Drive La Jolla, CA 92093, USA  
Affiliation: Department of Computer Science Engineering  University of California at San Diego  
Web: http://www-cse.ucsd.edu/users/mihir.  
Note: This is a slightly extended and updated version of a survey that appears in Complexity Theory Column 12, Sigact News, Vol. 27, No. 1, March 1996. The most recent version is available via  
Abstract: Proof Checking and Approximation: Towards Tight Results 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arora. </author> <title> Probabilistic checking of proofs and hardness of approximation problems. </title> <type> Ph. D thesis, </type> <institution> UC Berkeley, </institution> <year> 1994. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai [6], Arora and Lund [3], Sudan [48], Shmoys [46], Arora <ref> [1] </ref> or Sudan [47]. So I won't try to discuss them in full here. Rather, as noted above, this survey will focus on the more novel proof theoretic characterizations of N P and their consequences to the obtaining of tight, or at least very strong, non-approximability results. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions. <p> Obviously this is not a tight result. But it is not so weak either. How far can this approach go? Well, we know that the free bit complexity can't go below one| Proposition 3.5 [10] F PCP 1;s <ref> [ log; 1 ] </ref> = P for any constant s &lt; 1. Since 2 f must be integral, the lowest it could be is three. So the gap we would get via Proposition 3.3 won't exceed 3=2. <p> Note that 1-PCP c;s <ref> [ r; q; 1 ] </ref> = PCP c;s [ r; q ]. <p> Then the Raz parallelization result [45] emerged, and combined with [5, 4] yielded a central MPCP system for N P| Theorem 3.14 [5, 4, 45] There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s <ref> [ k () log; 1; ck () ] </ref>, where s (n) = 2 k (n) .
Reference: [2] <author> S. Arora. </author> <title> Reductions, Codes, </title> <booktitle> PCPs and Inapproximability. Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: How low can the free bit complexity go? The best we know today is the following| Theorem 3.2 [10] N P = F PCP 1;1=2 [ log; 6 ], and N P = F PCP 1;s <ref> [ log; 2 ] </ref> for any s &gt; 173=218. The first characterization above enables a comparison with Theorem 3.1 and shows that the free bit complexity required to get error 1=2 can be much lower than the query complexity required to get the same error. <p> H-astad's improvement (Theorem 3.7 and Corollary 3.8) actually came in two stages [32, 33]. We note that Arora <ref> [2] </ref> and [10] had earlier proved results which said that Corollary 3.8 could not be proved using "existing techniques." There is no contradiction here: H-astad has invented new techniques. It has often been asked whether proof checking is a necessary ingredient in the derivation of hardness of approximation results.
Reference: [3] <author> S. Arora and C. Lund. </author> <title> Hardness of Approximations. In Approximation algorithms for NP-hard problems, </title> <editor> D. Hochbaum, ed., </editor> <publisher> PWS Publishing, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai [6], Arora and Lund <ref> [3] </ref>, Sudan [48], Shmoys [46], Arora [1] or Sudan [47]. So I won't try to discuss them in full here. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions. <p> 2 ) 6= (q 4 ) ; Else it rejects iff (q 3 ) 6= (q 4 ) : Simple as they are, these two tests suffice to capture N P| Lemma 3.12 [10] For any fl &gt; 0 and any L 2 N P there exists a PCP 1;s <ref> [ log; 3 ] </ref> verifier V MaxSNP with soundness error s = (17=20) + fl.
Reference: [4] <author> S. Arora, C. Lund, R. Motwani, M. Sudan and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: Similarly we will use log = f n 7! c log (n) : c 2 N g to represent the class of all logarithmic functions. 6 to a constant. This is the so-called PCP theorem of <ref> [4, 5] </ref>, namely N P = PCP 1;1=2 [ log; q ] for some constant q. <p> This is the so-called PCP theorem of [4, 5], namely N P = PCP 1;1=2 [ log; q ] for some constant q. The minimization of query complexity (while maintaining logarithmic randomness complexity) was motivated by applications of the PCP theorem to gap creation in Max-Clique and other problems <ref> [21, 4] </ref>, where it was observed that the gap size increased as the query complexity decreased. The first reasonable values for the constant q were obtained in [11, 22]. Today we know that 11 bits suffice| Theorem 3.1 [10] N P = PCP 1;1=2 [ log; 11 ]. <p> Thus if we believe that Min-VC is hard to approximate within 2 * for any * &gt; 0, then an approach based on Proposition 3.3 won't take us to a tight result. Still, there is room for improvement. History and notes. Non-approximability of Min-VC within some constant follows from <ref> [4, 44] </ref>, but that constant is (unspecified and) only marginally bigger than one. <p> Proposition 3.9 and Corollary 3.11, as indicated above, are due to [24]. 3.6 Gadgets and Max-SNP Any problem in the Max-SNP class of Papadimitriou and Yannakakis [44] can be approximated within some constant U . Arora et. al. <ref> [4] </ref> have shown that there is a constant L &gt; 0 such that approximating Max3-SAT within 1 + L is N P-hard. <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of <ref> [4, 11, 22, 12] </ref>. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. <p> For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard. Arora et. al. <ref> [4] </ref> had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). <p> Then the Raz parallelization result [45] emerged, and combined with <ref> [5, 4] </ref> yielded a central MPCP system for N P| Theorem 3.14 [5, 4, 45] There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s (n) = 2 k (n) <p> Then the Raz parallelization result [45] emerged, and combined with [5, 4] yielded a central MPCP system for N P| Theorem 3.14 <ref> [5, 4, 45] </ref> There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s (n) = 2 k (n) .
Reference: [5] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy [7]. It was also scaled down by Feige, Goldwasser, Lovasz, Safra and Szegedy [21] and then named PCP by Arora and Safra <ref> [5] </ref>. 3.2 Query complexity The query complexity of V is q if for all R, the total number of bits of the proof that V examines, in its computation on inputs x and coins R, is at most q (n). <p> Similarly we will use log = f n 7! c log (n) : c 2 N g to represent the class of all logarithmic functions. 6 to a constant. This is the so-called PCP theorem of <ref> [4, 5] </ref>, namely N P = PCP 1;1=2 [ log; q ] for some constant q. <p> Then the Raz parallelization result [45] emerged, and combined with <ref> [5, 4] </ref> yielded a central MPCP system for N P| Theorem 3.14 [5, 4, 45] There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s (n) = 2 k (n) <p> Then the Raz parallelization result [45] emerged, and combined with [5, 4] yielded a central MPCP system for N P| Theorem 3.14 <ref> [5, 4, 45] </ref> There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s (n) = 2 k (n) .
Reference: [6] <author> L. Babai. </author> <title> Transparent proofs and limits to approximation. </title> <type> Technical Report TR-94-07, </type> <institution> Dept. of Computer Science, University of Chicago, </institution> <year> 1994. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai <ref> [6] </ref>, Arora and Lund [3], Sudan [48], Shmoys [46], Arora [1] or Sudan [47]. So I won't try to discuss them in full here. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions. <p> Note that F PCP c;s [ r; f ] PCP c;s [ r; f ]. How low can the free bit complexity go? The best we know today is the following| Theorem 3.2 [10] N P = F PCP 1;1=2 <ref> [ log; 6 ] </ref>, and N P = F PCP 1;s [ log; 2 ] for any s &gt; 173=218.
Reference: [7] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in polylog-arithmic time. </title> <booktitle> Proceedings of the 23rd Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: It was conceived as an alternative formulation of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [13], in turn an extension of the interactive proof model of Goldwasser, Micali and Rackoff [31]. It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy <ref> [7] </ref>.
Reference: [8] <author> R. Bar-Yehuda and S. </author> <title> Even. A linear time approximation algorithm for the weighted vertex cover problem. </title> <booktitle> In Jour. of Algorithms Vol. </booktitle> <volume> 2, </volume> <year> 1981, </year> <pages> pages 198-201. </pages>
Reference-contexts: Then F PCP c;s [ log; f ] D Gap-Min-VC c 0 ;s 0 with gap c 0 =s 0 = 1 + (c s)=(2 f c). The best known approximation algorithm for Min-VC achieves a factor of 2 o (1) ([9, 42], improv-ing <ref> [8, 34] </ref> and Gavril [29]). Putting together Theorem 3.2, Proposition 3.3 and Proposition 2.1 says that on the other hand we can't get within 6:8% of the optimum| Corollary 3.4 [10] Assuming P 6= N P, Min-VC has no factor 1:068 approximation algorithm. Obviously this is not a tight result.
Reference: [9] <author> R. Bar-Yehuda and S. </author> <title> Even. A local ratio theorem for approximating the weighted vertex cover problem. In Analysis and Design of Algorithms for Combinatorial Problems Vol. 25 of Annals of Discrete Math, </title> <publisher> Elsevier, </publisher> <year> 1985. </year>
Reference: [10] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free Bits, PCPs and Non-Approximability | Towards Tight Results. </title> <note> Version 3 (January 1996), available at the Electronic Colloquium on Computational Complexity, http://www.eccc.uni-trier.de/eccc/. Preliminary version in Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </note> <year> 1995. </year>
Reference-contexts: of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses [36, 6, 3, 48, 46, 1, 47] or the historical sections of <ref> [10] </ref>. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions. <p> 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P <ref> [10] </ref> Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. Let C 1 ; C 2 be complexity classes. <p> N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P <ref> [49, 10] </ref> hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. Let C 1 ; C 2 be complexity classes. <p> The first reasonable values for the constant q were obtained in [11, 22]. Today we know that 11 bits suffice| Theorem 3.1 <ref> [10] </ref> N P = PCP 1;1=2 [ log; 11 ]. Theorem 3.1 is a good starting point for getting rough estimates of the values of non-approximability factors in results derived from the PCP theorem, and also interesting in its own right as a characterization of N P. <p> Note that F PCP c;s [ r; f ] PCP c;s [ r; f ]. How low can the free bit complexity go? The best we know today is the following| Theorem 3.2 <ref> [10] </ref> N P = F PCP 1;1=2 [ log; 6 ], and N P = F PCP 1;s [ log; 2 ] for any s &gt; 173=218. <p> But this will not matter for the applications. Today the non-approximability result that relies on free bit complexity is that for the Min-VC problem. The connection is the following| Proposition 3.3 <ref> [10] </ref> Let c; f; s be constants. Then F PCP c;s [ log; f ] D Gap-Min-VC c 0 ;s 0 with gap c 0 =s 0 = 1 + (c s)=(2 f c). <p> The best known approximation algorithm for Min-VC achieves a factor of 2 o (1) ([9, 42], improv-ing [8, 34] and Gavril [29]). Putting together Theorem 3.2, Proposition 3.3 and Proposition 2.1 says that on the other hand we can't get within 6:8% of the optimum| Corollary 3.4 <ref> [10] </ref> Assuming P 6= N P, Min-VC has no factor 1:068 approximation algorithm. Obviously this is not a tight result. But it is not so weak either. How far can this approach go? Well, we know that the free bit complexity can't go below one| Proposition 3.5 [10] F PCP 1;s <p> optimum| Corollary 3.4 <ref> [10] </ref> Assuming P 6= N P, Min-VC has no factor 1:068 approximation algorithm. Obviously this is not a tight result. But it is not so weak either. How far can this approach go? Well, we know that the free bit complexity can't go below one| Proposition 3.5 [10] F PCP 1;s [ log; 1 ] = P for any constant s &lt; 1. Since 2 f must be integral, the lowest it could be is three. So the gap we would get via Proposition 3.3 won't exceed 3=2. <p> Free bit complexity is due to Feige and Kilian [22] (the name however is due to [12]). As indicated above, Theorem 3.2 and Proposition 3.5 are due to <ref> [10] </ref>. See Section 3.6 for how one deals with other Max-SNP complete problems. 3.4 Amortized free bit complexity and Max-Clique The most spectacular applications of free bits arise when we consider their amortized complexity. <p> It can be improved via randomization [14, 51], and was observed to extend to free bits, and amortized free bits, in [22, 12], respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from <ref> [10] </ref>). The construction of proof systems of low amortized free bit complexity began in [12] and continued in [10], who showed that N P = F PCP [ log; 2 + ffi ] for any constant ffi &gt; 0, whence approximating Max-Clique within N 1=3fl is hard for any fl &gt; <p> The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from <ref> [10] </ref>). The construction of proof systems of low amortized free bit complexity began in [12] and continued in [10], who showed that N P = F PCP [ log; 2 + ffi ] for any constant ffi &gt; 0, whence approximating Max-Clique within N 1=3fl is hard for any fl &gt; 0. H-astad's improvement (Theorem 3.7 and Corollary 3.8) actually came in two stages [32, 33]. <p> H-astad's improvement (Theorem 3.7 and Corollary 3.8) actually came in two stages [32, 33]. We note that Arora [2] and <ref> [10] </ref> had earlier proved results which said that Corollary 3.8 could not be proved using "existing techniques." There is no contradiction here: H-astad has invented new techniques. It has often been asked whether proof checking is a necessary ingredient in the derivation of hardness of approximation results. <p> It has often been asked whether proof checking is a necessary ingredient in the derivation of hardness of approximation results. The "reverse connection" of Bellare, Goldreich and Sudan <ref> [10] </ref> provides what is essentially a converse to Proposition 3.6 and says that for Max-Clique, at least, making N P-hard gaps requires building proof systems for N P, and moreover the gap size the proof system parameters are related as in Proposition 3.6. 3.5 Covering complexity and Chrom-Num When the input <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> In particular, the best reduction known today, namely that of Furer [27], shows that F PCP [ log; f ] R Gap-Chrom-Num c;s with gap c (N ) = N 1+2 max (f;1) ; (1) and combined with the proof systems of <ref> [10] </ref> this implies a N 1=5 factor hardness for Chrom-Num, which was the best known prior to Corollary 3.11. (Arbitrarily small constants dropped here for simplicity). The reduction (1) above does yield a N 1=3 factor hardness for Chrom-Num via Theorem 3.7, but it can't do better. <p> Current methods 10 "encode" the computation of the verifier via the problem at hand. Thus it turns out that what is important is how "simple" is the computation of the verifier, in terms of the "complexity" of the "gadgets" required to express it. A systematic approach is developed by <ref> [10] </ref>. They consider two simple kinds of tests. <p> exactly how we don't care) and If (q 1 ) = 0 then it rejects iff (q 2 ) 6= (q 4 ) ; Else it rejects iff (q 3 ) 6= (q 4 ) : Simple as they are, these two tests suffice to capture N P| Lemma 3.12 <ref> [10] </ref> For any fl &gt; 0 and any L 2 N P there exists a PCP 1;s [ log; 3 ] verifier V MaxSNP with soundness error s = (17=20) + fl. <p> There exists a (4; 1) 3-SAT parity gadget [11] and a (4; 1) MB gadget <ref> [10] </ref>, and putting all this together we get| Proposition 3.13 [10] Assuming P 6= N P, Max3-SAT has no factor 1:038 approximation algorithm. In contrast, the best known approximation algorithm for Max3-SAT achieves a factor of 1:258 (Trevisan, Sorkin, Sudan, and Williamson [49]). <p> There exists a (4; 1) 3-SAT parity gadget [11] and a (4; 1) MB gadget <ref> [10] </ref>, and putting all this together we get| Proposition 3.13 [10] Assuming P 6= N P, Max3-SAT has no factor 1:038 approximation algorithm. In contrast, the best known approximation algorithm for Max3-SAT achieves a factor of 1:258 (Trevisan, Sorkin, Sudan, and Williamson [49]). <p> However, Sorkin et. al. [49] show that the Max3-SAT and Max2-SAT gadgets of <ref> [10] </ref> are optimal. They also provide optimal gadgets for Max-CUT. The next possibility, which still remains un-investigated, is to provide a verifier making different kinds of tests. Beyond that, things are very open. History and notes. <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of [4, 11, 22, 12]. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while <ref> [10] </ref> have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard. <p> For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while <ref> [10] </ref> have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard. Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). <p> Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). The basic paradigm of their reduction, which consists of "encoding" the computation of the verifier via 3-SAT formulas, has been preserved in the improvements that follow <ref> [11, 12, 22, 10, 49] </ref>. 3.7 Multi-oracle systems and Min-Set-Cover For the application of proof checking to hardness of Min-Set-Cover approximation, we enhance the basic PCP model by allowing the verifier access to not one but several oracles 1 ; : : : ; p , the function p being a
Reference: [11] <author> M. Bellare, S. Goldwasser, C. Lund and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> Proceedings of the 25th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) <ref> [19, 11, 41] </ref> Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. <p> The first reasonable values for the constant q were obtained in <ref> [11, 22] </ref>. Today we know that 11 bits suffice| Theorem 3.1 [10] N P = PCP 1;1=2 [ log; 11 ]. <p> The first reasonable values for the constant q were obtained in [11, 22]. Today we know that 11 bits suffice| Theorem 3.1 [10] N P = PCP 1;1=2 <ref> [ log; 11 ] </ref>. Theorem 3.1 is a good starting point for getting rough estimates of the values of non-approximability factors in results derived from the PCP theorem, and also interesting in its own right as a characterization of N P. <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> There exists a (4; 1) 3-SAT parity gadget <ref> [11] </ref> and a (4; 1) MB gadget [10], and putting all this together we get| Proposition 3.13 [10] Assuming P 6= N P, Max3-SAT has no factor 1:038 approximation algorithm. In contrast, the best known approximation algorithm for Max3-SAT achieves a factor of 1:258 (Trevisan, Sorkin, Sudan, and Williamson [49]). <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of <ref> [4, 11, 22, 12] </ref>. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. <p> Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). The basic paradigm of their reduction, which consists of "encoding" the computation of the verifier via 3-SAT formulas, has been preserved in the improvements that follow <ref> [11, 12, 22, 10, 49] </ref>. 3.7 Multi-oracle systems and Min-Set-Cover For the application of proof checking to hardness of Min-Set-Cover approximation, we enhance the basic PCP model by allowing the verifier access to not one but several oracles 1 ; : : : ; p , the function p being a <p> Bellare, Goldwasser, Lund and Russell <ref> [11] </ref> improved the assumption at the cost of a slight decrease in the factor: building new proof systems, they showed that assuming N P 6 T IME ( ( ) loglog () ), Min-Set-Cover has no factor 0:124 ln (N ) approximation algorithm. <p> A systematic consideration of an expanded range of (M)PCP complexity parameters, including the number of oracles and answer size, began in <ref> [11] </ref>.
Reference: [12] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994. </year> <month> 13 </month>
Reference-contexts: Proposition 3.3 is obtained by combining the Max-Clique reduction of Feige et. al. [21] with Karp's standard reduction of Clique to Vertex Cover. Free bit complexity is due to Feige and Kilian [22] (the name however is due to <ref> [12] </ref>). As indicated above, Theorem 3.2 and Proposition 3.5 are due to [10]. See Section 3.6 for how one deals with other Max-SNP complete problems. 3.4 Amortized free bit complexity and Max-Clique The most spectacular applications of free bits arise when we consider their amortized complexity. <p> The main motivation for the consideration of amortized free bit complexity is the following connection to the Max-Clique problem| Proposition 3.6 <ref> [21, 14, 51, 22, 12] </ref> Let f &gt; 0 be a constant. <p> History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. [21]. It can be improved via randomization [14, 51], and was observed to extend to free bits, and amortized free bits, in <ref> [22, 12] </ref>, respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from [10]). <p> It can be improved via randomization [14, 51], and was observed to extend to free bits, and amortized free bits, in [22, 12], respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan <ref> [12] </ref>. (The definition I use is from [10]). The construction of proof systems of low amortized free bit complexity began in [12] and continued in [10], who showed that N P = F PCP [ log; 2 + ffi ] for any constant ffi &gt; 0, whence approximating Max-Clique within N <p> The notion of amortized free bits is due to Bellare and Sudan <ref> [12] </ref>. (The definition I use is from [10]). The construction of proof systems of low amortized free bit complexity began in [12] and continued in [10], who showed that N P = F PCP [ log; 2 + ffi ] for any constant ffi &gt; 0, whence approximating Max-Clique within N 1=3fl is hard for any fl &gt; 0. <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction <ref> [38, 12, 27] </ref>. <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of <ref> [4, 11, 22, 12] </ref>. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. <p> Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). The basic paradigm of their reduction, which consists of "encoding" the computation of the verifier via 3-SAT formulas, has been preserved in the improvements that follow <ref> [11, 12, 22, 10, 49] </ref>. 3.7 Multi-oracle systems and Min-Set-Cover For the application of proof checking to hardness of Min-Set-Cover approximation, we enhance the basic PCP model by allowing the verifier access to not one but several oracles 1 ; : : : ; p , the function p being a
Reference: [13] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the 20th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: History and notes. The PCP model is essentially the oracle model of Fortnow, Rompel and Sipser [26]. It was conceived as an alternative formulation of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson <ref> [13] </ref>, in turn an extension of the interactive proof model of Goldwasser, Micali and Rackoff [31]. It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy [7]. <p> But the existence of such systems is not ruled out.) Notes. MPCPs are a variant of the multi-prover model of Ben-Or et. al. <ref> [13] </ref>.
Reference: [14] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <booktitle> Information and Computation 96, </booktitle> <month> 77-94 </month> <year> (1992). </year>
Reference-contexts: The main motivation for the consideration of amortized free bit complexity is the following connection to the Max-Clique problem| Proposition 3.6 <ref> [21, 14, 51, 22, 12] </ref> Let f &gt; 0 be a constant. <p> Assuming N P 6= coRP, Max-Clique has no factor N 1* approximation algorithm. History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. [21]. It can be improved via randomization <ref> [14, 51] </ref>, and was observed to extend to free bits, and amortized free bits, in [22, 12], respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from [10]).
Reference: [15] <author> R. Boppana and M. Hald orsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <journal> BIT, </journal> <volume> Vol. 32, No. 2, </volume> <year> 1992. </year>
Reference-contexts: Usually we write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) <ref> [15] </ref> N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P <p> Usually we write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) <ref> [15] </ref> N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P <p> Indeed, very recently, H-astad [33] has provided the following remarkable characterization| 8 Theorem 3.7 [33] For any constant ffi &gt; 0 it is the case that N P = F PCP [ log; ffi ]. The best known approximation algorithm for Max-Clique, due to Boppana and Haldorsson <ref> [15] </ref>, achieves a factor of N 1o (1) . Combining Theorem 3.7, Proposition 3.6, and Proposition 2.1 yields a matching hardness result| Corollary 3.8 [33] Let * &gt; 0 be an arbitrary constant. Assuming N P 6= coRP, Max-Clique has no factor N 1* approximation algorithm. History and notes. <p> The best known approximation algorithm for Chrom-Num achieves a factor of N 1o (1) <ref> [15] </ref>. From the above, we see that this is best possible Corollary 3.11 [24] Let * &gt; 0 be an arbitrary constant. Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes.
Reference: [16] <author> P. Crescenzi and V. Kann. </author> <title> A compendium of NP optimization problems. </title> <address> ftp://www. nada.kth.se/ Theory/Viggo-Kann/compendium.ps.Z. </address>
Reference-contexts: I will discuss only a small core of optimization problems where we have been seeing improvements. A compendium providing the status of over 150 optimization problems has been compiled by Crescenzi and Kann <ref> [16] </ref>. A note on history.
Reference: [17] <author> A. Condon. </author> <title> The complexity of the max word problem, or the power of one-way interactive proof systems. </title> <journal> Computational Complexity Vol. </journal> <volume> 3, </volume> <pages> pp. 292-305, </pages> <year> 1993. </year>
Reference-contexts: The problem was how to create "hard gaps." (Roughly, this meant a collection of instances such that the optimum of each instance was either very large or very small, but deciding which was N P-complete.) The breakthrough came when Condon <ref> [17] </ref> and Feige, Goldwasser, Lovasz, Safra and Szegedy [21] pointed out that hard gaps for optimization problems could be created via characterizations of N P in terms of probabilistically checkable proofs.
Reference: [18] <author> C. Dwork, U. Feige, J. Kilian, M. Naor and M. Safra. </author> <title> Low communication 2-prover zero-knowledge proofs for NP. </title> <booktitle> Advances in Cryptology - Crypto 92 Proceedings, Lecture Notes in Computer Science Vol. </booktitle> <volume> 740, </volume> <editor> E. Brickell ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The reduction (1) above does yield a N 1=3 factor hardness for Chrom-Num via Theorem 3.7, but it can't do better. The covering complexity based approach is more promising. The randomization of PCPs was first considered by Dwork et. al. <ref> [18] </ref> in the context of zero-knowledge. Covering complexity was introduced by Feige and Kilian [24]. (I have amortized the measure).
Reference: [19] <author> U. Feige. </author> <title> A threshold of ln(n) for approximating set cover. </title> <booktitle> Proceedings of the 28th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1996. </year>
Reference-contexts: Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) <ref> [19, 11, 41] </ref> Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. <p> Furthermore, a de-randomization technique of Naor, Schulman and Srinivasan [43] doubles the non-approximability factor. This yields: assuming N P 6 T IME ( ( ) loglog () ), Min-Set-Cover has no factor 0:49 ln (N ) approximation algorithm. But even better is known. Feige <ref> [19] </ref> considers MPCPs with a new notion of "weak acceptance." Given a constant * &gt; 0 he builds a one round, k * -PCP system (k * is a constant depending on *) which has the additional properties he needs, while preserving the randomness and answer size complexities of the systems <p> He then adapts the reduction of [41] to get the following tight result| Theorem 3.15 <ref> [19] </ref> Let * &gt; 0 be an arbitrary constant. Assuming N P 6 T IME ( ( ) loglog () ), Min-Set-Cover has no factor (1 *) ln (N ) approximation algorithm. 12 What remains open for Min-Set-Cover is to improve the assumption to P 6= N P.
Reference: [20] <author> U. Feige and M. Goemans. </author> <title> Approximating the value of two prover proof systems, with application to Max-2SAT and Max-DICUT. </title> <booktitle> Proceedings of the Third Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of [4, 11, 22, 12]. For Max2-SAT, Feige and Goemans <ref> [20] </ref> (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard.
Reference: [21] <author> U. Feige, S. Goldwasser, L. Lov asz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> Proceedings of the 32nd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: As we know, the breakthrough has come by the application of results from probabilistic proof checking. It is an area that seems to continue to surprise: since the connection was discovered in 1991 (Feige et. al. <ref> [21] </ref>), not only have non-approximability results emerged for a wide range of problems, but the factors shown hard steadily increase. Today, tight results are known for central problems like Max-Clique and Min-Set-Cover. (That is, the approximation algorithms we have for these problems can be shown to be the best possible). <p> The problem was how to create "hard gaps." (Roughly, this meant a collection of instances such that the optimum of each instance was either very large or very small, but deciding which was N P-complete.) The breakthrough came when Condon [17] and Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [21] </ref> pointed out that hard gaps for optimization problems could be created via characterizations of N P in terms of probabilistically checkable proofs. Most importantly, Feige et. al. [21] provided such a "reduction" to produce the first hardness of approximation result for the Max-Clique problem. The field advanced rapidly. <p> or very small, but deciding which was N P-complete.) The breakthrough came when Condon [17] and Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [21] </ref> pointed out that hard gaps for optimization problems could be created via characterizations of N P in terms of probabilistically checkable proofs. Most importantly, Feige et. al. [21] provided such a "reduction" to produce the first hardness of approximation result for the Max-Clique problem. The field advanced rapidly. <p> It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy [7]. It was also scaled down by Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [21] </ref> and then named PCP by Arora and Safra [5]. 3.2 Query complexity The query complexity of V is q if for all R, the total number of bits of the proof that V examines, in its computation on inputs x and coins R, is at most q (n). <p> This is the so-called PCP theorem of [4, 5], namely N P = PCP 1;1=2 [ log; q ] for some constant q. The minimization of query complexity (while maintaining logarithmic randomness complexity) was motivated by applications of the PCP theorem to gap creation in Max-Clique and other problems <ref> [21, 4] </ref>, where it was observed that the gap size increased as the query complexity decreased. The first reasonable values for the constant q were obtained in [11, 22]. Today we know that 11 bits suffice| Theorem 3.1 [10] N P = PCP 1;1=2 [ log; 11 ]. <p> Proposition 3.3 is obtained by combining the Max-Clique reduction of Feige et. al. <ref> [21] </ref> with Karp's standard reduction of Clique to Vertex Cover. Free bit complexity is due to Feige and Kilian [22] (the name however is due to [12]). As indicated above, Theorem 3.2 and Proposition 3.5 are due to [10]. <p> The main motivation for the consideration of amortized free bit complexity is the following connection to the Max-Clique problem| Proposition 3.6 <ref> [21, 14, 51, 22, 12] </ref> Let f &gt; 0 be a constant. <p> Assuming N P 6= coRP, Max-Clique has no factor N 1* approximation algorithm. History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. <ref> [21] </ref>. It can be improved via randomization [14, 51], and was observed to extend to free bits, and amortized free bits, in [22, 12], respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from [10]).
Reference: [22] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: The first reasonable values for the constant q were obtained in <ref> [11, 22] </ref>. Today we know that 11 bits suffice| Theorem 3.1 [10] N P = PCP 1;1=2 [ log; 11 ]. <p> Proposition 3.3 is obtained by combining the Max-Clique reduction of Feige et. al. [21] with Karp's standard reduction of Clique to Vertex Cover. Free bit complexity is due to Feige and Kilian <ref> [22] </ref> (the name however is due to [12]). As indicated above, Theorem 3.2 and Proposition 3.5 are due to [10]. <p> The main motivation for the consideration of amortized free bit complexity is the following connection to the Max-Clique problem| Proposition 3.6 <ref> [21, 14, 51, 22, 12] </ref> Let f &gt; 0 be a constant. <p> History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. [21]. It can be improved via randomization [14, 51], and was observed to extend to free bits, and amortized free bits, in <ref> [22, 12] </ref>, respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from [10]). <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of <ref> [4, 11, 22, 12] </ref>. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. <p> Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). The basic paradigm of their reduction, which consists of "encoding" the computation of the verifier via 3-SAT formulas, has been preserved in the improvements that follow <ref> [11, 12, 22, 10, 49] </ref>. 3.7 Multi-oracle systems and Min-Set-Cover For the application of proof checking to hardness of Min-Set-Cover approximation, we enhance the basic PCP model by allowing the verifier access to not one but several oracles 1 ; : : : ; p , the function p being a
Reference: [23] <author> U. Feige and J. Kilian. </author> <title> Impossibility results for recycling random bits in two prover proof systems. </title> <booktitle> Proceedings of the 27th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: It would require the construction of constant oracle MPCPs which can achieve 1= polylog () soundness error with only logarithmic randomness and answer complexity. (There are results saying that certain techniques won't suffice towards this end <ref> [23] </ref>. But the existence of such systems is not ruled out.) Notes. MPCPs are a variant of the multi-prover model of Ben-Or et. al. [13].
Reference: [24] <author> U. Feige and J. Kilian. </author> <title> Zero-knowledge and the chromatic number. </title> <booktitle> Proceedings of the 11th Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: Usually we write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P <ref> [24] </ref> Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial <p> [ r; f; ] denote the class of languages recognized by F PCP [ r; f ] verifiers |ie. verifiers which witness the membership of L in the class in question| having amortized covering complexity . (The "R" stands for "randomized.") The relation to Chrom-Num is the following| Proposition 3.9 <ref> [24] </ref> Let f ; &gt; 0 be constants. <p> Feige and Kilian <ref> [24] </ref> randomized the proof system which underlay the N P FPCP [ log; ffi ] result of [33] in such a way that the amortized covering complexity was not more than the amortized free bit complexity| Theorem 3.10 [24] For any constant ffi &gt; 0 it is the case that N <p> Feige and Kilian <ref> [24] </ref> randomized the proof system which underlay the N P FPCP [ log; ffi ] result of [33] in such a way that the amortized covering complexity was not more than the amortized free bit complexity| Theorem 3.10 [24] For any constant ffi &gt; 0 it is the case that N P RF PCP [ log; ffi; ffi ]. The best known approximation algorithm for Chrom-Num achieves a factor of N 1o (1) [15]. From the above, we see that this is best possible Corollary 3.11 [24] Let * <p> Theorem 3.10 <ref> [24] </ref> For any constant ffi &gt; 0 it is the case that N P RF PCP [ log; ffi; ffi ]. The best known approximation algorithm for Chrom-Num achieves a factor of N 1o (1) [15]. From the above, we see that this is best possible Corollary 3.11 [24] Let * &gt; 0 be an arbitrary constant. Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num [28, 41, 11, 22, 12, 27, 10]. <p> The covering complexity based approach is more promising. The randomization of PCPs was first considered by Dwork et. al. [18] in the context of zero-knowledge. Covering complexity was introduced by Feige and Kilian <ref> [24] </ref>. (I have amortized the measure). Proposition 3.9 and Corollary 3.11, as indicated above, are due to [24]. 3.6 Gadgets and Max-SNP Any problem in the Max-SNP class of Papadimitriou and Yannakakis [44] can be approximated within some constant U . <p> The randomization of PCPs was first considered by Dwork et. al. [18] in the context of zero-knowledge. Covering complexity was introduced by Feige and Kilian <ref> [24] </ref>. (I have amortized the measure). Proposition 3.9 and Corollary 3.11, as indicated above, are due to [24]. 3.6 Gadgets and Max-SNP Any problem in the Max-SNP class of Papadimitriou and Yannakakis [44] can be approximated within some constant U . Arora et. al. [4] have shown that there is a constant L &gt; 0 such that approximating Max3-SAT within 1 + L is N P-hard.
Reference: [25] <author> U. Feige and L. Lov asz. </author> <title> Two-prover one round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the 24th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: Lovasz [40]) achieves a factor of 1 + ln (N ). (Recall that the norm N is the number of elements in the base set.) The first hardness result for this problem was already quite strong: Lund and Yannakakis [41] used the two oracle proof systems of Lapidot-Shamir and Feige-Lovasz <ref> [39, 25] </ref> to show that assuming N P 6 T IME ( ( ) polylog () ), Min-Set-Cover has no factor 0:24 ln (N ) approximation algorithm.
Reference: [26] <author> L. Fortnow, J. Rompel and M. Sipser. </author> <title> On the power of multiprover interactive protocols. </title> <booktitle> Proceedings of the 3rd Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: The randomness complexity is r if the length of V 's random string R is at most r (n). History and notes. The PCP model is essentially the oracle model of Fortnow, Rompel and Sipser <ref> [26] </ref>. It was conceived as an alternative formulation of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [13], in turn an extension of the interactive proof model of Goldwasser, Micali and Rackoff [31]. It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy [7].
Reference: [27] <author> M. F urer. </author> <title> Improved hardness results for approximating the chromatic number. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction <ref> [38, 12, 27] </ref>. <p> Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. In particular, the best reduction known today, namely that of Furer <ref> [27] </ref>, shows that F PCP [ log; f ] R Gap-Chrom-Num c;s with gap c (N ) = N 1+2 max (f;1) ; (1) and combined with the proof systems of [10] this implies a N 1=5 factor hardness for Chrom-Num, which was the best known prior to Corollary 3.11. (Arbitrarily
Reference: [28] <author> M. Garey and D. Johnson. </author> <title> The complexity of near optimal graph coloring. </title> <journal> Journal of the ACM Vol. </journal> <volume> 23, No. 1, </volume> <pages> 43-49, </pages> <year> 1976. </year>
Reference-contexts: Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27].
Reference: [29] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: Then F PCP c;s [ log; f ] D Gap-Min-VC c 0 ;s 0 with gap c 0 =s 0 = 1 + (c s)=(2 f c). The best known approximation algorithm for Min-VC achieves a factor of 2 o (1) ([9, 42], improv-ing [8, 34] and Gavril <ref> [29] </ref>). Putting together Theorem 3.2, Proposition 3.3 and Proposition 2.1 says that on the other hand we can't get within 6:8% of the optimum| Corollary 3.4 [10] Assuming P 6= N P, Min-VC has no factor 1:068 approximation algorithm. Obviously this is not a tight result.
Reference: [30] <author> M. Goemans and D. Williamson. </author> <title> :878 approximation algorithms for Max-CUT and Max-2SAT. </title> <booktitle> Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 <ref> [30] </ref> 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. Let C 1 ; C 2 be complexity classes. <p> For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of [4, 11, 22, 12]. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson <ref> [30] </ref>) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard. <p> For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson <ref> [30] </ref>) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while [49] (improving [10]) have shown that achieving 1:019 is N P-hard. Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1).
Reference: [31] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <journal> SIAM J. </journal> <volume> Computing Vol 18, No. 1, </volume> <pages> 186-208, </pages> <year> 1989. </year> <month> 14 </month>
Reference-contexts: History and notes. The PCP model is essentially the oracle model of Fortnow, Rompel and Sipser [26]. It was conceived as an alternative formulation of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [13], in turn an extension of the interactive proof model of Goldwasser, Micali and Rackoff <ref> [31] </ref>. It was scaled down and called transparent proofs by Babai, Fortnow, Levin and Szegedy [7].
Reference: [32] <author> J. H -astad. </author> <title> Testing of the long code and hardness for clique. </title> <booktitle> Proceedings of the 28th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1996. </year>
Reference-contexts: H-astad's improvement (Theorem 3.7 and Corollary 3.8) actually came in two stages <ref> [32, 33] </ref>. We note that Arora [2] and [10] had earlier proved results which said that Corollary 3.8 could not be proved using "existing techniques." There is no contradiction here: H-astad has invented new techniques.
Reference: [33] <author> J. H -astad. </author> <title> Clique is hard to approximate within n 1* . Proceedings of the 37th Symposium on Foundations of Computer Science, </title> <publisher> IEEE, </publisher> <year> 1996. </year>
Reference-contexts: Usually we write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P <ref> [33] </ref> Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] <p> Indeed, very recently, H-astad <ref> [33] </ref> has provided the following remarkable characterization| 8 Theorem 3.7 [33] For any constant ffi &gt; 0 it is the case that N P = F PCP [ log; ffi ]. <p> Indeed, very recently, H-astad <ref> [33] </ref> has provided the following remarkable characterization| 8 Theorem 3.7 [33] For any constant ffi &gt; 0 it is the case that N P = F PCP [ log; ffi ]. The best known approximation algorithm for Max-Clique, due to Boppana and Haldorsson [15], achieves a factor of N 1o (1) . <p> The best known approximation algorithm for Max-Clique, due to Boppana and Haldorsson [15], achieves a factor of N 1o (1) . Combining Theorem 3.7, Proposition 3.6, and Proposition 2.1 yields a matching hardness result| Corollary 3.8 <ref> [33] </ref> Let * &gt; 0 be an arbitrary constant. Assuming N P 6= coRP, Max-Clique has no factor N 1* approximation algorithm. History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. [21]. <p> H-astad's improvement (Theorem 3.7 and Corollary 3.8) actually came in two stages <ref> [32, 33] </ref>. We note that Arora [2] and [10] had earlier proved results which said that Corollary 3.8 could not be proved using "existing techniques." There is no contradiction here: H-astad has invented new techniques. <p> Feige and Kilian [24] randomized the proof system which underlay the N P FPCP [ log; ffi ] result of <ref> [33] </ref> in such a way that the amortized covering complexity was not more than the amortized free bit complexity| Theorem 3.10 [24] For any constant ffi &gt; 0 it is the case that N P RF PCP [ log; ffi; ffi ].
Reference: [34] <author> D. Hochbaum. </author> <title> Efficient algorithms for the stable set, vertex cover and set packing problems. </title> <journal> Discrete Applied Mathematics, </journal> <volume> Vol 6, </volume> <pages> pages 243-254, </pages> <year> 1983. </year>
Reference-contexts: Then F PCP c;s [ log; f ] D Gap-Min-VC c 0 ;s 0 with gap c 0 =s 0 = 1 + (c s)=(2 f c). The best known approximation algorithm for Min-VC achieves a factor of 2 o (1) ([9, 42], improv-ing <ref> [8, 34] </ref> and Gavril [29]). Putting together Theorem 3.2, Proposition 3.3 and Proposition 2.1 says that on the other hand we can't get within 6:8% of the optimum| Corollary 3.4 [10] Assuming P 6= N P, Min-VC has no factor 1:068 approximation algorithm. Obviously this is not a tight result.
Reference: [35] <author> D. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> J. Comput. System Sci. </journal> <volume> Vol. 9, </volume> <pages> pp. 256-278, </pages> <year> 1974. </year>
Reference-contexts: write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) <ref> [35, 40] </ref> (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. <p> Note that 1-PCP c;s [ r; q; 1 ] = PCP c;s [ r; q ]. The best known approximation algorithm for Min-Set-Cover (Johnson <ref> [35] </ref> and Lovasz [40]) achieves a factor of 1 + ln (N ). (Recall that the norm N is the number of elements in the base set.) The first hardness result for this problem was already quite strong: Lund and Yannakakis [41] used the two oracle proof systems of Lapidot-Shamir and
Reference: [36] <author> D. Johnson. </author> <title> The tale of the second prover. In the NP-completeness column: an on-going guide, </title> <journal> J. of Algorithms, </journal> <volume> Vol. 13, </volume> <pages> pp. 502-524, </pages> <year> 1992. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson <ref> [36] </ref>, Babai [6], Arora and Lund [3], Sudan [48], Shmoys [46], Arora [1] or Sudan [47]. So I won't try to discuss them in full here. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions.
Reference: [37] <author> R. Karp. </author> <title> Reducibility among combinatorial problems. Complexity of Computer Computations, </title> <editor> Miller and Thatcher (eds.), </editor> <publisher> Plenum Press, </publisher> <address> New York (1972). </address>
Reference: [38] <author> S. Khanna, N. Linial and S. Safra. </author> <title> On the hardness of approximating the chromatic number. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction <ref> [38, 12, 27] </ref>.
Reference: [39] <author> D. Lapidot and A. Shamir. </author> <title> Fully Parallelized Multi-prover protocols for NEXP-time. </title> <booktitle> Proceedings of the 32nd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: Lovasz [40]) achieves a factor of 1 + ln (N ). (Recall that the norm N is the number of elements in the base set.) The first hardness result for this problem was already quite strong: Lund and Yannakakis [41] used the two oracle proof systems of Lapidot-Shamir and Feige-Lovasz <ref> [39, 25] </ref> to show that assuming N P 6 T IME ( ( ) polylog () ), Min-Set-Cover has no factor 0:24 ln (N ) approximation algorithm.
Reference: [40] <author> L. Lov asz. </author> <title> On the ratio of optimal integral and fractional covers. </title> <journal> Discrete Math, </journal> <volume> Vol. 13, </volume> <pages> pp. 383-390, </pages> <year> 1975. </year>
Reference-contexts: write R or D to emphasize whether the reduction is randomized or not. 4 Problem Approximability Non-Approximability Factor Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) <ref> [35, 40] </ref> (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. <p> Note that 1-PCP c;s [ r; q; 1 ] = PCP c;s [ r; q ]. The best known approximation algorithm for Min-Set-Cover (Johnson [35] and Lovasz <ref> [40] </ref>) achieves a factor of 1 + ln (N ). (Recall that the norm N is the number of elements in the base set.) The first hardness result for this problem was already quite strong: Lund and Yannakakis [41] used the two oracle proof systems of Lapidot-Shamir and Feige-Lovasz [39, 25]
Reference: [41] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 41, </volume> <pages> pp. 960-981, </pages> <year> 1994. </year>
Reference-contexts: Ref Factor Assumption Ref Max-Clique N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) <ref> [19, 11, 41] </ref> Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num <ref> [28, 41, 11, 22, 12, 27, 10] </ref>. Beginning with Lund and Yannakakis [41] these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> Assuming N P 6= coRP, Chrom-Num has no factor N 1* approximation algorithm. History and notes. Corollary 3.11 improves a long line of hardness of approximation results for Chrom-Num [28, 41, 11, 22, 12, 27, 10]. Beginning with Lund and Yannakakis <ref> [41] </ref> these results have been obtained by reduction from Max-Clique. Improvements in the factor were a result both of improvements in the Max-Clique hardness results (discussed above) and improvements in the reduction [38, 12, 27]. <p> The best known approximation algorithm for Min-Set-Cover (Johnson [35] and Lovasz [40]) achieves a factor of 1 + ln (N ). (Recall that the norm N is the number of elements in the base set.) The first hardness result for this problem was already quite strong: Lund and Yannakakis <ref> [41] </ref> used the two oracle proof systems of Lapidot-Shamir and Feige-Lovasz [39, 25] to show that assuming N P 6 T IME ( ( ) polylog () ), Min-Set-Cover has no factor 0:24 ln (N ) approximation algorithm. <p> The original reduction of <ref> [41] </ref> can be applied to the system underlying Theorem 3.14. Furthermore, a de-randomization technique of Naor, Schulman and Srinivasan [43] doubles the non-approximability factor. This yields: assuming N P 6 T IME ( ( ) loglog () ), Min-Set-Cover has no factor 0:49 ln (N ) approximation algorithm. <p> He then adapts the reduction of <ref> [41] </ref> to get the following tight result| Theorem 3.15 [19] Let * &gt; 0 be an arbitrary constant.
Reference: [42] <author> B. Monien and E. Speckenmeyer. </author> <title> Some further approximation algorithms for the vertex cover problem. </title> <booktitle> Proceedings of CAAP 83, Lecure Notes in Computer Science Vol. </booktitle> <volume> 159, </volume> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference: [43] <author> M. Naor, L. Schulman and A. Srinivasan. </author> <title> Splitters and near optimal derandomization. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: The original reduction of [41] can be applied to the system underlying Theorem 3.14. Furthermore, a de-randomization technique of Naor, Schulman and Srinivasan <ref> [43] </ref> doubles the non-approximability factor. This yields: assuming N P 6 T IME ( ( ) loglog () ), Min-Set-Cover has no factor 0:49 ln (N ) approximation algorithm. But even better is known.
Reference: [44] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <journal> J. Comput. System Sci., </journal> <volume> Vol. 43, </volume> <pages> pp. 425-440, </pages> <year> 1991. </year>
Reference-contexts: Thus if we believe that Min-VC is hard to approximate within 2 * for any * &gt; 0, then an approach based on Proposition 3.3 won't take us to a tight result. Still, there is room for improvement. History and notes. Non-approximability of Min-VC within some constant follows from <ref> [4, 44] </ref>, but that constant is (unspecified and) only marginally bigger than one. <p> Covering complexity was introduced by Feige and Kilian [24]. (I have amortized the measure). Proposition 3.9 and Corollary 3.11, as indicated above, are due to [24]. 3.6 Gadgets and Max-SNP Any problem in the Max-SNP class of Papadimitriou and Yannakakis <ref> [44] </ref> can be approximated within some constant U . Arora et. al. [4] have shown that there is a constant L &gt; 0 such that approximating Max3-SAT within 1 + L is N P-hard.
Reference: [45] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the 27th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Then the Raz parallelization result <ref> [45] </ref> emerged, and combined with [5, 4] yielded a central MPCP system for N P| Theorem 3.14 [5, 4, 45] There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s <p> Then the Raz parallelization result [45] emerged, and combined with [5, 4] yielded a central MPCP system for N P| Theorem 3.14 <ref> [5, 4, 45] </ref> There is a constant c such that for any function k () it is the case that N P 2-PCP 1;s [ k () log; 1; ck () ], where s (n) = 2 k (n) .
Reference: [46] <author> D. Shmoys. </author> <title> Computing near-optimal solutions to combinatorial optimization problems. Chapter in the book Combinatorial Optimization, </title> <booktitle> DIMACS series in Discrete Mathematics and Theoretical Computer Science Vol. </booktitle> <volume> 20, </volume> <editor> W. Cook, L. Lovasz, and P. Seymour editors, </editor> <publisher> AMS, </publisher> <year> 1995. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai [6], Arora and Lund [3], Sudan [48], Shmoys <ref> [46] </ref>, Arora [1] or Sudan [47]. So I won't try to discuss them in full here. Rather, as noted above, this survey will focus on the more novel proof theoretic characterizations of N P and their consequences to the obtaining of tight, or at least very strong, non-approximability results. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions.
Reference: [47] <author> M. Sudan. </author> <title> Efficient checking of polynomials and proofs, and the hardness of approximation problems. </title> <type> Ph. D thesis, </type> <institution> UC Berkeley, </institution> <year> 1992, </year> <note> published in ACM Distinguished Theses series, Lecture Notes in Computer Science Vol. 1001, Springer 1996. </note>
Reference-contexts: of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai [6], Arora and Lund [3], Sudan [48], Shmoys [46], Arora [1] or Sudan <ref> [47] </ref>. So I won't try to discuss them in full here. Rather, as noted above, this survey will focus on the more novel proof theoretic characterizations of N P and their consequences to the obtaining of tight, or at least very strong, non-approximability results. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions.
Reference: [48] <author> M. Sudan. </author> <title> On the role of algebra in the efficient verification of proofs. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: The subject of deriving non-approximability results via proof checking has already received quite a lot of exposure, and, in particular, the above events have been described in full in many surveys and theses| see for example Johnson [36], Babai [6], Arora and Lund [3], Sudan <ref> [48] </ref>, Shmoys [46], Arora [1] or Sudan [47]. So I won't try to discuss them in full here. <p> results, techniques, and notions. (The early work on approximation algorithms, the notions of interactive and probabilistically checkable proofs, and techniques from randomness and program checking to name just a few.) I will not survey this here: for full histories, the reader is referred to the above mentioned surveys and theses <ref> [36, 6, 3, 48, 46, 1, 47] </ref> or the historical sections of [10]. Below I have indicated only works directly related to or preceding the ones from which I have drawn results or definitions.
Reference: [49] <author> L. Trevisan, G. Sorkin, M. Sudan and D. Williamson. </author> <booktitle> Proceedings of the 37th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 <ref> [49, 50] </ref> 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. <p> N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 [49, 50] 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P <ref> [49, 10] </ref> hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. Let C 1 ; C 2 be complexity classes. <p> In contrast, the best known approximation algorithm for Max3-SAT achieves a factor of 1:258 (Trevisan, Sorkin, Sudan, and Williamson <ref> [49] </ref>). Similar gadgets are constructed for other problems and yield comparable hardness results. (Gadgets can be constructed for Min-VC as well, but this approach doesn't do as well as the free bit based one.) How can these results be improved? The obvious approach is to construct better gadgets. <p> However, Sorkin et. al. <ref> [49] </ref> show that the Max3-SAT and Max2-SAT gadgets of [10] are optimal. They also provide optimal gadgets for Max-CUT. The next possibility, which still remains un-investigated, is to provide a verifier making different kinds of tests. Beyond that, things are very open. History and notes. <p> Beyond that, things are very open. History and notes. Max-SNP has being seeing a narrowing of the approximation gap, as algorithmic improvements have provided better approximation guarantees while new proof systems have yielded improved non-approximability results. For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. <ref> [49] </ref> improves that of Yannakakis [50], while Proposition 3.13 improves previous hardness results of [4, 11, 22, 12]. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. <p> For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard. For Max-CUT, [30] have provided a factor 1:139 approximation algorithm, while <ref> [49] </ref> (improving [10]) have shown that achieving 1:019 is N P-hard. Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). <p> Arora et. al. [4] had showed that PCP 1;1=2 [ log; O (1) ] D GapMax3-SAT c;s with c=s = (1). The basic paradigm of their reduction, which consists of "encoding" the computation of the verifier via 3-SAT formulas, has been preserved in the improvements that follow <ref> [11, 12, 22, 10, 49] </ref>. 3.7 Multi-oracle systems and Min-Set-Cover For the application of proof checking to hardness of Min-Set-Cover approximation, we enhance the basic PCP model by allowing the verifier access to not one but several oracles 1 ; : : : ; p , the function p being a
Reference: [50] <author> M. Yannakakis. </author> <title> On the approximation of maximum satisfiability. </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 17, </volume> <pages> pp. 475-502, </pages> <year> 1994. </year>
Reference-contexts: N 1o (1) [15] N 1* coRP 6= N P [33] Chrom-Num N 1o (1) [15] N 1* coRP 6= N P [24] Min-Set-Cover ln (N ) [35, 40] (1 *) ln (N ) N P 6 T IME ( ( ) loglog () ) [19, 11, 41] Max3-SAT 1:258 <ref> [49, 50] </ref> 1:038 P 6= N P [10] Max-CUT 1:139 [30] 1:019 P 6= N P [49, 10] hard to achieve by polynomial time algorithms. Here * stands for an arbitrarily small positive constant and N is the norm of the problem instance. <p> History and notes. Max-SNP has being seeing a narrowing of the approximation gap, as algorithmic improvements have provided better approximation guarantees while new proof systems have yielded improved non-approximability results. For Max3-SAT, the above mentioned algorithm 11 of Sorkin et. al. [49] improves that of Yannakakis <ref> [50] </ref>, while Proposition 3.13 improves previous hardness results of [4, 11, 22, 12]. For Max2-SAT, Feige and Goemans [20] (improving Goemans and Williamson [30]) have provided a factor 1:074 approximation algorithm, while [10] have shown that achieving 1:013 is N P-hard.
Reference: [51] <author> D. Zuckerman. </author> <title> NP-Complete problems have a version that is hard to Approximate. </title> <booktitle> Proceedings of the 8th Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1993. </year> <month> 15 </month>
Reference-contexts: The main motivation for the consideration of amortized free bit complexity is the following connection to the Max-Clique problem| Proposition 3.6 <ref> [21, 14, 51, 22, 12] </ref> Let f &gt; 0 be a constant. <p> Assuming N P 6= coRP, Max-Clique has no factor N 1* approximation algorithm. History and notes. The basic connection of PCP to Max-Clique was made by Feige et. al. [21]. It can be improved via randomization <ref> [14, 51] </ref>, and was observed to extend to free bits, and amortized free bits, in [22, 12], respectively, yielding Proposition 3.6. The notion of amortized free bits is due to Bellare and Sudan [12]. (The definition I use is from [10]).
References-found: 51

