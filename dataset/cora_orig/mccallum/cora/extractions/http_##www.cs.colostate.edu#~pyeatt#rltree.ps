URL: http://www.cs.colostate.edu/~pyeatt/rltree.ps
Refering-URL: http://www.cs.colostate.edu/~pyeatt/
Root-URL: 
Email: email: fpyeatt,howeg@cs.colostate.edu  
Title: Decision Tree Function Approximation in Reinforcement Learning  
Author: Larry D. Pyeatt Adele E. Howe 
Web: URL: http://www.cs.colostate.edu/fpyeatt,howeg  
Address: Fort Collins, CO 80523  
Affiliation: Colorado State University  
Abstract: We present a decision tree based approach to function approximation in reinforcement learning. We compare our approach with table lookup and a neural network function approximator on three problems: the well known mountain car and pole balance problems as well as a simulated automobile race car. We find that the decision tree can provide better learning performance than the neural network function approximation and can solve large problems that are infeasible using table lookup.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.G. Barto, R.S. Sutton, and C.W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: Pole balance is another classic problem where the goal is to balance a pole that is affixed to a cart by a hinge <ref> [1] </ref>. The cart moves in one dimension on a finite track. At each time step, the controller decides whether to push the cart to the left or to the right.
Reference: [2] <author> Andrew G. Barto and Richard S. Sutton. </author> <title> An Introduction to Reinforcement Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: 1 Motivation A popular approach for estimating the value function in reinforcement learning is the table lookup method. This approach is guaranteed to converge, subject to some restrictions on the learning parameters <ref> [2] </ref>. However, table lookup does not scale well with the number of inputs, although some variations of this approach, such as sparse coarse coding and hashing [9], have been used to improve scalability. Another approach is to use a neural network to learn the value function.
Reference: [3] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D.S. Touretsky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Another approach is to use a neural network to learn the value function. That approach scales better, but is not guaranteed to converge and often performs poorly even on relatively simple problems <ref> [3] </ref>. We propose an approach that exploits decision trees for learning to estimate the value function. We started to investigate this problem because we are building a reinforcement learning based agent for two simulated robotic environments: Robot Automobile Racing Simulator (RARS) and Khepera. <p> In practice, the neural network approach often performs poorly even on relatively simple problems <ref> [3] </ref>. 2.3 Our Approach: Decision Tree-Based The straightforward table lookup method subdivides the input space into equal intervals. Each part of the state space has the same resolution. A better approach would allow high resolution only where needed. <p> Each part of the state space has the same resolution. A better approach would allow high resolution only where needed. Some attempts have been made to use variable resolution tables, with limited success <ref> [3] </ref>. Decision trees [6] allow the space to be divided with varying levels of resolution. Figure 1 shows an example of a decision tree that divides the state space into 5 regions. <p> Is the decision tree based approach less prone to the learn/forget cycle than the neu ral network approach? 3.1 Problem Domains Mountain car is a classic reinforcement learning task where the goal is to learn the proper acceleration to get out of a valley and up a mountain <ref> [3] </ref>. The car does not have enough power to simply climb up the mountain, so it has to rock back and forth across the valley until it gains enough momentum to carry it up the mountain.
Reference: [4] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and regression trees. </title> <type> Technical report, </type> <institution> Wadsworth International, </institution> <address> Monterey, CA, </address> <year> 1984. </year>
Reference-contexts: Information Gain This is the classic method used in Quinlan's ID3 [8]. It measures the information gained from a particular split. Gini Index This metric is based on the Gini Criterion by Breiman <ref> [4] </ref>, but modified as in OC1 by Murthy [7]. The Gini Index measures the probability of misclassifying a set of instances. Twoing Rule This metric, also proposed by Breiman and used in Murthy's OC1, compares the number of examples in each category on each side of the proposed split.
Reference: [5] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <editor> In John Mylopoulos and Ray Reiter., editors, </editor> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <pages> pages 726-731, </pages> <address> San Mateo, Ca., 1991. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The tree can be used to map an input vector to one of the leaf nodes, which corresponds to a region in the state space. Reinforcement learning can be used to associate a value with each region. G-learning <ref> [5] </ref> uses a decision tree to learn compact representations of the value function for problems with binary inputs. Our approach extends the method to an algorithm that handles real values. The goal is to provide robust convergence along with scalability. <p> Otherwise, the decision is made by calculating the T statistic for each variable and selecting the variable with the highest T statistic. This approach is similar to that used by Chap man and Kaelbling <ref> [5] </ref>, although we remove the restriction that all inputs be binary. 3 Empirical Performance Study To assess the performance of our decision tree based reinforcement learning algorithms, we compared them to table lookup and neural network reinforcement learning on three problem domains.
Reference: [6] <author> Kolluru Venkata Sreerama Murthy. </author> <title> On Growing Better Decision Trees from Data. </title> <type> PhD thesis, </type> <institution> Johns Hopkins University, Baltimore, Maryland, </institution> <year> 1996. </year>
Reference-contexts: Each part of the state space has the same resolution. A better approach would allow high resolution only where needed. Some attempts have been made to use variable resolution tables, with limited success [3]. Decision trees <ref> [6] </ref> allow the space to be divided with varying levels of resolution. Figure 1 shows an example of a decision tree that divides the state space into 5 regions.
Reference: [7] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> JAIR, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <year> 1994. </year>
Reference-contexts: Information Gain This is the classic method used in Quinlan's ID3 [8]. It measures the information gained from a particular split. Gini Index This metric is based on the Gini Criterion by Breiman [4], but modified as in OC1 by Murthy <ref> [7] </ref>. The Gini Index measures the probability of misclassifying a set of instances. Twoing Rule This metric, also proposed by Breiman and used in Murthy's OC1, compares the number of examples in each category on each side of the proposed split. T-statistic Our approach is based on the T-statistic.
Reference: [8] <author> J R Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Information Gain This is the classic method used in Quinlan's ID3 <ref> [8] </ref>. It measures the information gained from a particular split. Gini Index This metric is based on the Gini Criterion by Breiman [4], but modified as in OC1 by Murthy [7]. The Gini Index measures the probability of misclassifying a set of instances.
Reference: [9] <author> Richard S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1038-1044, </pages> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This approach is guaranteed to converge, subject to some restrictions on the learning parameters [2]. However, table lookup does not scale well with the number of inputs, although some variations of this approach, such as sparse coarse coding and hashing <ref> [9] </ref>, have been used to improve scalability. Another approach is to use a neural network to learn the value function. That approach scales better, but is not guaranteed to converge and often performs poorly even on relatively simple problems [3].
Reference: [10] <author> Mitchell E. Timin. </author> <title> Robot automobile racing simulator (RARS). </title> <note> Anonymous ftp ftp.ijs.com:/rars, </note> <year> 1995. </year>
Reference-contexts: At each time step, the controller decides whether to push the cart to the left or to the right. RARS is an environment where a simulated race car driver is responsible for controlling acceleration and steering as the car races against other cars <ref> [10] </ref>. 3.2 Results For each problem domain, we ran all of the reinforcement learning algorithms and then divided the total run time for each algorithm into four periods. The number of iterations in each domain was determined by how long it took the algorithms to reach a stable policy.
Reference: [11] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: Perform split, if required 8. Save r t , a t and s t so that they can be used for training on the next iteration. 9. Return a t . 2.4 Overview of Algorithm We use a variation of reinforcement learning known as Q-learning <ref> [11, 12] </ref>, which maps state-action pairs instead of states.
Reference: [12] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Perform split, if required 8. Save r t , a t and s t so that they can be used for training on the next iteration. 9. Return a t . 2.4 Overview of Algorithm We use a variation of reinforcement learning known as Q-learning <ref> [11, 12] </ref>, which maps state-action pairs instead of states.
References-found: 12

