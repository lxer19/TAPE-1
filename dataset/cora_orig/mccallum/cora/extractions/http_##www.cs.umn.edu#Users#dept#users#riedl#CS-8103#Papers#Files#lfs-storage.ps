URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Files/lfs-storage.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Files/
Root-URL: http://www.cs.umn.edu
Email: mendel@sprite.berkeley.edu ouster@sprite.berkeley.edu  
Phone: 415-642-9669  
Title: The LFS Storage Manager  
Author: Mendel Rosenblum John K. Ousterhout 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California  
Abstract: Advances in computer system technology in the areas of CPUs, disk subsystems, and volatile RAM memory are combining to create performance problems existing file systems are ill-equipped to solve. This paper identifies the problems of using the existing UNIX file systems on 1990's technology and presents an alternative file system design that can use disks an order-of-magnitude more efficiently for typical UNIX workloads. The design, named LFS for log-structured file system, treats the disk as a segmented append-only log. This allows LFS to write many small changes to disk in a single large I/O while still maintaining the fast file reads of existing file systems. In addition, the log-structured approach allows near instantaneous file system crash recovery without coupling CPU and disk performance with synchronous disk writes. This paper describes and justifies the major data structures and algorithms of the LFS design. We compare an implementation of LFS in the Sprite distributed operating system to SunOS's file system running on the same hardware. For tests that create, destroy, or modify files at a high rate, LFS can achieve an order-of-magnitude speedup over SunOS. In spite of its obvious write-optimization, LFS's read performance matches or exceeds the SunOS file system under most common UNIX workloads. This paper was presented at the Summer '90 USENIX Technical Conference, Anaheim, California, June 1990. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> John K. Ousterhout and Fred Douglis, </author> <title> ``Beating the I/O Bottleneck: A Case for Log-structured File Systems,'' </title> <type> UCB/CSD 88/467, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, Berkeley, </institution> <note> CA (October 1988). </note>
Reference-contexts: This paper describes a disk storage manager designed to use disks as efficiently as possible to support write-dominated workloads. The storage manager, called LFS, uses the concepts of log-structured file systems <ref> [1] </ref> to increase the performance of the UNIX file system. In a log-structured file system, all modifications to the file system including data, directories, and metadata blocks are written to disk in large, sequential transfers that proceed at maximum disk bandwidth.
Reference: 2. <author> John K. Ousterhout , Andrew R. Cherenson, Frederick Douglis, Michael N. Nelson, and Brent B. Welch, </author> <title> ``The Sprite Network Operating System,'' </title> <journal> IEEE Computer 21(2) pp. </journal> <month> 23-36 </month> <year> (1988). </year>
Reference-contexts: The following section, Section 3, describes failures of existing file systems. Section 4 describes the LFS design, and Section 5 presents some performance numbers taken from an implementation in the Sprite distributed operating system <ref> [2] </ref>. The paper concludes with a status report and discussion of future directions for LFS. 2. Technology and storage managers This section examines the effect that the current technology trends in CPU speeds, disk performance, and main memory sizes are having on storage managers.
Reference: 3. <author> David A. Patterson , Garth Gibson , and Randy H. Katz, </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID),'' </title> <booktitle> ACM SIGMOD 88, </booktitle> <pages> pp. </pages> <month> 109-116 (Jun </month> <year> 1988). </year>
Reference-contexts: Disk access times, constrained by mechanics, have only improved by a factor of two in the last decade. Although the the bandwidth and throughput of disk subsystems can be substantially increased by the use of arrays of disks such as RAIDs <ref> [3] </ref>, the access time for small disk accesses is not substantially improved and can even be hurt by this technique. The widening gap between CPU and disk access time suggests that the performance of future systems may be limited by the disk subsystem. 2.2.
Reference: 4. <author> Marshall K. </author> <title> McKusick , ``A Fast File System for Unix,'' </title> <journal> Transactions on Computer Systems 2(3) pp. </journal> <month> 181-197 </month> <year> (1984). </year>
Reference-contexts: Disk head motion (seeks) should be avoided whenever possible. 3. Problems with existing UNIX file systems Before presenting the design of LFS it is useful to outline the failures of existing file systems. Although this section uses the BSD file system <ref> [4] </ref> as an example, the problems are present in most commercial file systems in use today. The major reason that existing file systems will not scale with technology is that they perform too many random and synchronous disk operations.
Reference: 5. <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson, </author> <title> ``A Trace-Driven Analysis of the Unix 4.2 BSD File System,'' </title> <booktitle> Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <pages> pp. 15-24 ACM, </pages> <year> (1985). </year>
Reference-contexts: A disk storage manager design is strongly influenced by the expected workload it must support. LFS is designed to support the workload of the office/engineering environment. This environment has been characterized (see <ref> [5] </ref>) by a large number of relatively small files (less than 8 kilobytes) whose contents are accessed sequentially and in their entirety. The average file life time is short, less than a day before it is overwritten or deleted.
Reference: 6. <author> D. Ungar, </author> <title> ``Generation Scavenging: A Non-Disruptive High Performance Storage Reclamation Algorithm,'' </title> <booktitle> Proceedings of the Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pp. </pages> <month> 157-167 (Apr </month> <year> 1984). </year>
Reference-contexts: LFS generates free segments, called clean segments, from fragmented segments using an operation called segment cleaning. During segment cleaning two or more fragmented segments are read into memory, combined, and appended to the log on disk. Segment cleaning in LFS is simply a form of incremental garbage collection <ref> [6] </ref> where the fragmented segments are compressed together to create space to write new segments. LFS implements cleaning by reading the live blocks of a segment into the file cache and then using the cache write-back code to combine and copy the blocks into a new segment.
Reference: 7. <author> Jim Gray, </author> <booktitle> ``Notes on Data Base Operating Systems,'' </booktitle> <pages> pp. </pages> <booktitle> Springer-Verlag in Operating Systems, An Advanced Course, </booktitle> <year> (1979). </year>
Reference-contexts: System crashes can cause file systems to become inconsistent when disk operations are terminated by the crash or delayed writes are not completed. The log structure of LFS allows techniques commonly used in data base systems <ref> [7] </ref> to be used by LFS for fast recovery from system crashes. Unlike the UNIX file system, which must scan the entire disk after a crash to repair damage, LFS need only examine the tail of the log to find crash damage.
Reference: 8. <author> D. Reed and Liba Svobodova, ``SWALLOW: </author> <title> A Distributed Data Storage System for a Local Network,'' </title> <booktitle> Local Networks for Computer Communications, </booktitle> <pages> pp. 355-373 North-Holland, </pages> <year> (1981). </year>
Reference-contexts: Several other file - 9 - The LFS Storage Manager May 1, 1991 systems, motivated by the advent of write-once media such as optical disks, have used similar mechanisms. Write-once storage managers with random access to files include SWALLOW <ref> [8] </ref>, the Optical File Cabinet [9], and others [10]. These file systems were intended principally for archival storage and not as high-performance file servers.
Reference: 9. <author> Jason Gait, </author> <title> ``The Optical File Cabinet: A Random Access File System for Write-Once Optical Disks,'' </title> <note> IEEE Computer 21(6) pp. 11-22 (1988). - 14 - The LFS Storage Manager May 1, </note> <year> 1991 </year>
Reference-contexts: Several other file - 9 - The LFS Storage Manager May 1, 1991 systems, motivated by the advent of write-once media such as optical disks, have used similar mechanisms. Write-once storage managers with random access to files include SWALLOW [8], the Optical File Cabinet <ref> [9] </ref>, and others [10]. These file systems were intended principally for archival storage and not as high-performance file servers. Another way of viewing the LFS design is to see its roots in file systems like Cedar [11] that use logging to improve write performance and recovery time.
Reference: 10. <author> Ross S. Finlayson and David R. Cheriton, </author> <title> ``Log Files: An Extended File Service Exploiting Write-Once Storage,'' </title> <booktitle> Proceedings of the Eleventh Symposium on Operating System Principles, </booktitle> <pages> pp. </pages> <month> 129-148 (November </month> <year> 1987). </year>
Reference-contexts: Several other file - 9 - The LFS Storage Manager May 1, 1991 systems, motivated by the advent of write-once media such as optical disks, have used similar mechanisms. Write-once storage managers with random access to files include SWALLOW [8], the Optical File Cabinet [9], and others <ref> [10] </ref>. These file systems were intended principally for archival storage and not as high-performance file servers. Another way of viewing the LFS design is to see its roots in file systems like Cedar [11] that use logging to improve write performance and recovery time.
Reference: 11. <author> Robert B. Hagmann, </author> <title> ``Reimplementing the Cedar File System Using Logging and Group Commit,'' </title> <booktitle> Proceedings of the 11th Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <month> 155-162 (November </month> <year> 1987). </year>
Reference-contexts: These file systems were intended principally for archival storage and not as high-performance file servers. Another way of viewing the LFS design is to see its roots in file systems like Cedar <ref> [11] </ref> that use logging to improve write performance and recovery time. The difference between LFS and other logging systems is that a read-optimized copy of the data is not kept and hence a copy does not need to be updated.
Reference: 12. <author> David J. DeWitt, Randy H. Katz, Frank Olken, L. D. Shapiro, Mike R. Stonebraker, and David Wood, </author> <title> ``Implementation Techniques for Main Memory Database Systems,'' </title> <booktitle> Proceedings of SIGMOD 1984, </booktitle> <pages> pp. </pages> <month> 1-8 (June </month> <year> 1984). </year>
Reference-contexts: The read-optimized format is not needed because reads are infrequent and the log's format is already well optimized for most file reads. The writing of several files to disk in one operation is much like the concept of group commit <ref> [12] </ref> found in database literature. Using group commit, database systems such as IBM's FASTPATH [13] delay writing so that several transactions can be committed in a single I/O. An interesting related area of research is main-memory database systems. <p> The metrics used to evaluate these systems are checkpoint overhead (flushing the dirty blocks to disk) and crash recovery speed. These metrics are similar to the design goals of write-optimized file systems. Main-memory database designs use logging and large asynchronous writes <ref> [12, 14] </ref>. 5. Performance of LFS This section presents the performance of the LFS implementation running under the Sprite operating system. For comparison purposes, the same tests were also run under SunOS 4.0.3 using Sun's version of the BSD fast file system.
Reference: 13. <author> IBM, </author> <title> ``IBM IMS Version 1 Release 1.5 Fast Path Feature Description and Design Guide,'' G320-5775, IBM World Trade Systems Centers (1979). </title>
Reference-contexts: The writing of several files to disk in one operation is much like the concept of group commit [12] found in database literature. Using group commit, database systems such as IBM's FASTPATH <ref> [13] </ref> delay writing so that several transactions can be committed in a single I/O. An interesting related area of research is main-memory database systems. The metrics used to evaluate these systems are checkpoint overhead (flushing the dirty blocks to disk) and crash recovery speed.
Reference: 14. <author> Robert B. Hagmann, </author> <title> ``A Crash Recovery Scheme for a Memory-Resident Database System,'' </title> <journal> IEEE Transaction on Computers C-35(9)(September 1986). </journal> - <volume> 15 </volume> - 
Reference-contexts: The metrics used to evaluate these systems are checkpoint overhead (flushing the dirty blocks to disk) and crash recovery speed. These metrics are similar to the design goals of write-optimized file systems. Main-memory database designs use logging and large asynchronous writes <ref> [12, 14] </ref>. 5. Performance of LFS This section presents the performance of the LFS implementation running under the Sprite operating system. For comparison purposes, the same tests were also run under SunOS 4.0.3 using Sun's version of the BSD fast file system.
References-found: 14

