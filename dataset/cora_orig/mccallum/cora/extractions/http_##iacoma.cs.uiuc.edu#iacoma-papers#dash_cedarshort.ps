URL: http://iacoma.cs.uiuc.edu/iacoma-papers/dash_cedarshort.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Comparing the Performance of the DASH and Cedar Multiprocessors for Scientific Applications 1  
Author: Josep Torrellas, David Koufaty, and David Padua 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> Int. Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The simplest programs, microkernels, measure the performance of low-level architectural primitives: arithmetic and synchronization support and the memory system. To examine the impact of cache coherence support and model of parallelism we use two parallelized versions of Perfect Club codes <ref> [1] </ref> (MDG and TRFD) and four parallel kernels. The four parallel kernels, MatInc, SOR, LU, and MatMul were written for DASH and then ported to Cedar. Finally, to examine the impact of clustering we run PPDE, where the processors in a cluster share some locality. 4.
Reference: [2] <author> R. Eigenmann et al. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Parallelizing Serial Codes The second issue that we examine is the parallelization of two large serial applications: MDG and TRFD. We focus on how it is impacted by the cache coherence support and model of parallelism. The Cedar versions used are the best a parallelizing compiler can generate <ref> [2] </ref>. For DASH, the versions have been extensively rewritten and hand-optimized. For instance, MDG is the Splash [7] code tuned for DASH. Programming Effort. The lack of hardware cache coherence in Cedar did not represent a major programming handicap.
Reference: [3] <author> D. Kuck et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proc. of ISCA '93, </booktitle> <pages> pages 213-224, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cache coherence is supported in hardware: a snoopy-based protocol within each cluster and a directory-based one across clusters. We examine a machine configured with 8 clusters. Cedar is a 4-cluster vector multiprocessor developed at the Univ. of Illinois' Center for Supercomputing Research and Development <ref> [3] </ref>. Each cluster is an 8-CPU bus-based Alliant FX/8. All processors in a cluster share a 512 Kbyte direct-mapped cache and 64 Mbytes of memory visible only to the cluster. Fast synchronization is possible via a per-cluster synchronization bus. Each processor has a 4 Kbyte prefetch buffer.
Reference: [4] <author> D. Lenoski et al. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: NSF RIA MIP 93-08098, NSF MIP 93-07910, and NSF MIP 89-20891; a NASA ICLASS grant; and grant 1-1-28028 from the Univ. of Illinois Research Board. DASH is a cluster-based machine developed at Stan-ford <ref> [4] </ref>. Each cluster is a 4-CPU bus-based Silicon Graphics Power System/340, with 32 Mbytes of memory visible to all processors in the machine. The clusters are interconnected with a 2-D mesh. <p> Both DASH and Cedar exploit this "base" aspect of clustering. However, the two machines have different, extra support to exploit clustering further. Indeed, DASH supports local cache to cache or remote access cache (RAC) to cache sharing and remote request combining <ref> [4] </ref>. This effect is called local sharing. Cedar has shared caches and synchronization buses. This extra support speeds up the processors in one cluster if they are using the same memory blocks.
Reference: [5] <author> R. H. Saavedra et al, </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 202-213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Given all these differences, it is important to compare the performance of the different machines. Such comparisons can determine the cost-effectiveness of the different choices. As an example, the effectiveness of the all-cache feature has been studied <ref> [5, 6] </ref>. Comparison studies, however, are challenging for at least two reasons. First, they require porting the same codes to the machines compared. Second, it is difficult to isolate the feature studied from implementation-dependent, issues. <p> The arrays are allocated in cluster memory. For Cedar, we use vector mode. Memory System Performance. We use a methodology introduced by Saavedra et al <ref> [5] </ref>: one processor reads the elements of a 1D array with a given stride in a loop. As we increase the stride and the length of the array, we will be causing cache misses and eventually TLB misses.
Reference: [6] <author> J. P. Singh et al. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> In Proc. of Supercomputing '93, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Given all these differences, it is important to compare the performance of the different machines. Such comparisons can determine the cost-effectiveness of the different choices. As an example, the effectiveness of the all-cache feature has been studied <ref> [5, 6] </ref>. Comparison studies, however, are challenging for at least two reasons. First, they require porting the same codes to the machines compared. Second, it is difficult to isolate the feature studied from implementation-dependent, issues.
Reference: [7] <author> J. P. Singh et al. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We focus on how it is impacted by the cache coherence support and model of parallelism. The Cedar versions used are the best a parallelizing compiler can generate [2]. For DASH, the versions have been extensively rewritten and hand-optimized. For instance, MDG is the Splash <ref> [7] </ref> code tuned for DASH. Programming Effort. The lack of hardware cache coherence in Cedar did not represent a major programming handicap. <p> Indeed, MatMul performs well in DASH after matrix blocks are copied and compacted. Finally, we note that it is time consuming to port some symbolic applications to a machine with non-coherent local memories like Cedar. We have attempted to port Locus-Route from Splash <ref> [7] </ref> and found that it requires a significant effort. 7. Support for Clustering The final issue that we examine is the effectiveness of the hardware support for clustering in the two machines.
Reference: [8] <author> J. Torrellas, D. Koufaty, and D. Padua. </author> <title> Comparing the Performance of the DASH and Cedar Multiprocessors for Scientific Applications. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: This analysis, performed by running very small programs, will help us interpret the results of subsequent sections. Due to space constraints, we present a summary only; see <ref> [8] </ref> for a complete discussion. Arithmetic Performance. Using Arith we find that the sustained performance of DASH for double-precision operations is about twice higher than Cedar on vectors and up to 7 times higher on scalars. <p> Cedar cache (A) Cedar 3.0 DASH 1.6 DASH cluster mem. (C1) vs. Cedar cluster mem. (C1) Cedar 1.3 DASH 1.8 DASH avg. memory (C2) vs. Cedar global mem. (C2) Cedar 7.2 DASH 1.1 Cedar we show vector reads only. See <ref> [8] </ref> for the charts on all the memory layers and on scalar reads for Cedar and a more complete discussion. Each level marked in the figures corresponds to a different level in the memory hierarchy where the request was satisfied. <p> The increased memory access time does not impact programs like SOR because the data is reused before being displaced from the cache. However, for MatInc, caches overflow and DASH becomes 4 to 6 times slower than Cedar. In <ref> [8] </ref>, we show the resulting changes in the 32-processor speedups. For MatInc, as we go from small to large problem size, DASH's speedup decreases from 25 to 12, while Cedar's speedup increases from 13 to 20. <p> In addition, we check that the data set size is small enough to cause few conflicts in the cache that would not appear in a split cache <ref> [8] </ref>. Secondly, to measure the effect of the synchronization bus, we use standard test&set instructions instead of the synchronization bus.
References-found: 8

