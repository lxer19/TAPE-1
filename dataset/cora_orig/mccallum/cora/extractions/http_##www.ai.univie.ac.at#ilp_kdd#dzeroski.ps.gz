URL: http://www.ai.univie.ac.at/ilp_kdd/dzeroski.ps.gz
Refering-URL: http://www.ai.univie.ac.at/ilp_kdd/schedule.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: Saso.Dzeroski@ijs.si  
Phone: 2)  3)  4)  5)  
Title: Applying ILP to Diterpene Structure Elucidation from 13 C NMR Spectra  
Author: Saso Dzeroski (;) Steffen Schulze-Kremer () Karsten R. Heidtke () Karsten Siems () Dietrich Wettschereck () 
Address: FORTH-ICS, P.O.Box 1385, 711 10 Heraklion, Crete, Greece  Jamova 39, 1000 Ljubljana, Slovenia  Ihnestrasse 73, 14195 Berlin, Germany  Gustav-Meyer-Allee 25, 13335 Berlin-Wedding, Germany  FIT.KI, Schloss Birlinghoven, 53745 Sankt Augustin, Germany  
Affiliation: 1)  Department of Intelligent Systems, Jozef Stefan Institute  Max-Planck Institute for Molecular Genetics Otto-Warburg-Laboratorium, Department Lehrach  AnalytiCon GmbH  GMD,  
Abstract: We present a novel application of ILP to the problem of diterpene structure elucidation from 13 C NMR spectra. Diterpenes are organic compounds of low molecular weight that are based on a skeleton of 20 carbon atoms. They are of significant chemical and commercial interest because of their use as lead compounds in the search for new pharmaceutical effectors. The structure elucidation of diterpenes based on 13 C NMR spectra is usually done manually by human experts with specialized background knowledge on peak patterns and chemical structures. In the process, each of the 20 skeletal atoms is assigned an atom number that corresponds to its proper place in the skeleton and the diterpene is classified into one of the possible skeleton types. We address the problem of learning classification rules from a database of peak patterns for diterpenes with known structure. Recently, propositional learning was successfully applied to learn classification rules from spectra with assigned atom numbers. As the assignment of atom numbers is a difficult process in itself (and possibly indistinguishable from the classification process), we apply ILP, i.e., relational learning, to the problem of classifying spectra without assigned atom numbers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abraham, R.J., Loftus, P. </author> <title> Proton and Carbon 13 NMR Spectroscopy, An Integrated Approach. </title> <publisher> Heyden, </publisher> <address> London, </address> <year> 1978. </year>
Reference-contexts: For structure elucidation of secondary natural products (not proteins) only 1 H-NMR- and 13 C-NMR-spectroscopy, including combined methods such as 2D-NMR-spectroscopy, are important because hydrogen and carbon are the most abundant atoms in natural products. In structure elucidation of peptides and proteins 15 N-NMR is sometimes helpful <ref> [1] </ref>. 1 H-NMR- and 13 C-NMR-spectroscopy are quite different: in a 13 C-NMR- spectrum every carbon atom occurs as a separate signal in most cases, while in 1 H-NMR-spectra many signals overlap and are therefore difficult to interpret [15]. 1 H-NMR- spectra are logically decomposable and the specialist could get direct
Reference: [2] <author> Clark, P. and Boswell, R. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proc. Fifth European Working Session on Learning, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: This information is then used when classifying new examples, e.g., when testing the accuracy of the rule set. 8 For classification, we implemented a procedure analogous to that of CN2 <ref> [2] </ref>: when an example is classified, all clauses that cover it are taken into account. The distributions of examples of each class are summed for all rules that match and the majority class is assigned to the example.
Reference: [3] <author> Cover, T.M., and Hart, P.E. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13: </volume> <pages> 21-27, </pages> <year> 1968. </year>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks [17, 16], nearest neighbor classification <ref> [3, 18] </ref>, and decision tree induction [12, 14]. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. Table 4: Classification accuracy on unseen cases when learning from classified 13C-NMR spectra with assigned atom numbers. <p> The classification accuracy on unseen cases as measured by ten-fold cross-validation (same folds as above) is 78.5%, which is almost the same as the accuracy achieved by FOIL using both red and prop. 4.2 Nearest neighbor using multiplicities only We also applied nearest neighbor classification <ref> [3, 18] </ref> to the propositional problem defined by the relation prop. The same experimental setting (cross-validation folds) was used. Training on the entire dataset gives 100% accuracy on training data.
Reference: [4] <author> De Raedt, L., and Van Laer, V. </author> <title> Inductive constraint logic. </title> <booktitle> In Proc. Sixth International Workshop on Algorithmic Learning Theory, </booktitle> <pages> pages 80-94. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: An interesting direction for further work would be to apply mFOIL [8] and use the m-estimate to guide the search towards more general rules (larger m in the estimate of the accuracy prefers rules that cover more examples). ICL <ref> [4] </ref> is also an interesting candidate to apply to this problem. Neural networks can be applied to the prop propositional version and C4.5 to the representation of Section 4.2. Some combinations of the representations used here may deserve further investigation using propositional or ILP methods.
Reference: [5] <author> Dzeroski, S., Schulze-Kremer, S., Heidtke, K., Siems, K., Wettschereck, D. </author> <title> Diterpene structure elucidation from 13 C NMR spectra with machine learning. </title> <booktitle> In Proc. ECAI'96 Workshop on Intelligent Data Analysis in Medicine and Pharmacology, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: We address the problem of learning classification rules from a database of peak patterns for diterpenes with known structure. Recently, propositional methods were successfully applied to classifying spectra with assigned atom numbers <ref> [5] </ref>. As the assignment of atom numbers is a difficult process in itself (and possibly indistinguishable from the classification process), we apply ILP, i.e., relational learning, to the problem of classifying spectra without assigned atom numbers. <p> They leave the measured frequencies unchanged. The reduced frequencies that correspond to those in Table 1 are given in Table 3. Note that the multiplicities of atoms 8, 13, 14, 15, and 17 are changed. 6 2.3 Propositional learning: experiments and results Given the above data, Dzeroski et al. <ref> [5] </ref> formulate several propositional learning problems. Twenty-three different skeleton types are represented in the whole set of 1503 compounds: there are thus 23 possible class values. The attributes are the multiplicities and frequencies of each of the 20 skeleton atoms. <p> We first used FOIL to induce rules on the entire data set, and then performed ten-fold cross validations on the same partitions of training examples as used for the propositional case by Dzeroski et al. <ref> [5] </ref>. For each experiment, FOIL was run 23 times, i.e., once for each target relation. The rules from the 23 runs were then taken together to produce a rule set. <p> The most general rule class52 (A) :- red (A,d,B), B=&lt;73.7, red (A,C,D), D&gt;38.5, D=&lt;44.6, B&gt;73.2 covers 43 examples of class 52. 3.2 Using multiplicities only According to the domain expert (Karsten Siems), the multiplicities should suffice for correct classification, at least when atom numbers are available <ref> [5] </ref>. If we remove the Frequency argument from the relation red, all the information left about a particular molecule is captured by the number of atoms which have multiplicity s, d, t, and q, respectively. We store this information in the relation prop (MoleculeID,SAtoms,DAtoms,TAtoms,QAtoms).
Reference: [6] <author> Emde, W., Wettschereck, D. </author> <title> Relational instance-based learning. </title> <booktitle> In Proc. Thirteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1996. </year>
Reference-contexts: This may be the only route to reliable automated structure prediction procedure. 11 Perhaps the most interesting direction for further work is to apply RIBL (relational instance-based learning) <ref> [6] </ref> to the problem addressed. RIBL generalizes the nearest neighbor method to a relational representation. Given a propositional representation, RIBL becomes the classical nearest neighbor method and has the same performance as the latter. This is a very desirable property for a relational learner.
Reference: [7] <author> Gray, N. A. B. </author> <booktitle> Progress in NMR-spectroscopy, </booktitle> <volume> Vol. 15, </volume> <pages> pp. 201-248, </pages> <year> 1982. </year>
Reference-contexts: connected to the carbon; t stands for a triplet with two protons and q for a quartet with three protons bound to the carbon atom. nature of 13 C-NMR-data as compared to 1 H-NMR-data, the former are easier to handle and therefore remain the preferred basis for automatic structure elucidation <ref> [7] </ref>. 2 1.2 Diterpenes Diterpenes are one of a few fundamental classes of natural products with about 5000 members known [11]. The skeleton of every diterpene contains 20 carbon atoms.
Reference: [8] <author> Lavrac, N., Dzeroski, S. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, </address> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The background relation red is nondeterminate, as there are 20 tuples for each molecule. This prevents the use of ILP systems like GOLEM [10] or DINUS <ref> [8] </ref>. While PROGOL [9] would be applicable, preliminary experiments showed it has prohibitive time complexity if longer rules/clauses are needed. Therefore, we used FOIL [13], in particular FOIL6.4. Except for a variable depth limit of one, all other settings were left in their default state. <p> Expert comments indicate that the rules induced by FOIL from red and prop are still overly specific. An interesting direction for further work would be to apply mFOIL <ref> [8] </ref> and use the m-estimate to guide the search towards more general rules (larger m in the estimate of the accuracy prefers rules that cover more examples). ICL [4] is also an interesting candidate to apply to this problem.
Reference: [9] <author> Muggleton, S. </author> <title> Inverse entailment and PROGOL. </title> <journal> New Generation Computing, </journal> <volume> 13: 245--286, </volume> <year> 1995. </year>
Reference-contexts: The background relation red is nondeterminate, as there are 20 tuples for each molecule. This prevents the use of ILP systems like GOLEM [10] or DINUS [8]. While PROGOL <ref> [9] </ref> would be applicable, preliminary experiments showed it has prohibitive time complexity if longer rules/clauses are needed. Therefore, we used FOIL [13], in particular FOIL6.4. Except for a variable depth limit of one, all other settings were left in their default state.
Reference: [10] <author> Muggleton, S., and Feng, C. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proc. First Conference on Algorithmic Learning Theory, </booktitle> <pages> pages 368-381. </pages> <publisher> Ohmsha, </publisher> <address> Tokyo, </address> <year> 1990. </year>
Reference-contexts: For example, the fact red (v1,t,39.00) states that the 13C-NMR spectrum of molecule v1 has a peak at frequency 39.00 ppm with multiplicity t (a triplet). The background relation red is nondeterminate, as there are 20 tuples for each molecule. This prevents the use of ILP systems like GOLEM <ref> [10] </ref> or DINUS [8]. While PROGOL [9] would be applicable, preliminary experiments showed it has prohibitive time complexity if longer rules/clauses are needed. Therefore, we used FOIL [13], in particular FOIL6.4. Except for a variable depth limit of one, all other settings were left in their default state.
Reference: [11] <editor> Natural products on CD-ROM. </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: bound to the carbon atom. nature of 13 C-NMR-data as compared to 1 H-NMR-data, the former are easier to handle and therefore remain the preferred basis for automatic structure elucidation [7]. 2 1.2 Diterpenes Diterpenes are one of a few fundamental classes of natural products with about 5000 members known <ref> [11] </ref>. The skeleton of every diterpene contains 20 carbon atoms. Sometimes there are additional groups linked to the diterpene skeleton by an oxygen atom with the possible effect of increasing the carbon atom count to more than 20 per diterpene.
Reference: [12] <author> Quinlan, J.R. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1(1): </booktitle> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks [17, 16], nearest neighbor classification [3, 18], and decision tree induction <ref> [12, 14] </ref>. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. Table 4: Classification accuracy on unseen cases when learning from classified 13C-NMR spectra with assigned atom numbers.
Reference: [13] <author> Quinlan, J.R. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3): </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: This prevents the use of ILP systems like GOLEM [10] or DINUS [8]. While PROGOL [9] would be applicable, preliminary experiments showed it has prohibitive time complexity if longer rules/clauses are needed. Therefore, we used FOIL <ref> [13] </ref>, in particular FOIL6.4. Except for a variable depth limit of one, all other settings were left in their default state.
Reference: [14] <author> Quinlan, J.R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks [17, 16], nearest neighbor classification [3, 18], and decision tree induction <ref> [12, 14] </ref>. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. Table 4: Classification accuracy on unseen cases when learning from classified 13C-NMR spectra with assigned atom numbers. <p> Combining the engineered features with the original relational representation thus has a positive effect. 4 Comparison to propositional approaches 4.1 C4.5 using multiplicities only As mentioned above, the relation prop defines a propositional version of our classification problem. We therefore applied a propositional learner, C4.5 <ref> [14] </ref>, to this problem. The same experimental setting was used (tree induced on whole data set first, then ten-fold cross-validation). The default settings of C4.5 were used. The induced tree achieves 80.4% accuracy on the entire data set.
Reference: [15] <author> Schulze-Kremer, S. </author> <title> Molecular Bioinformatics Algorithms and Applications. </title> <publisher> de Gruyter, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: In structure elucidation of peptides and proteins 15 N-NMR is sometimes helpful [1]. 1 H-NMR- and 13 C-NMR-spectroscopy are quite different: in a 13 C-NMR- spectrum every carbon atom occurs as a separate signal in most cases, while in 1 H-NMR-spectra many signals overlap and are therefore difficult to interpret <ref> [15] </ref>. 1 H-NMR- spectra are logically decomposable and the specialist could get direct information about the structure (including stereochemistry) from the resonance frequency and shape of the signals, provided a sufficiently high resolution of resonance signals can be experimentally achieved.
Reference: [16] <institution> Stuttgart Neural Network Simulator. Computer code available from the University of Stuttgart, Germany, </institution> <note> via anonymous ftp ftp://ftp.informatik.uni-stuttgart.de/pub/SNNS, </note> <year> 1995. </year>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks <ref> [17, 16] </ref>, nearest neighbor classification [3, 18], and decision tree induction [12, 14]. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. <p> This was done independently of the above experimental setup, but using the same partitions for cross-validation. A standard backpropagation network <ref> [17, 16] </ref> with 960 input neurons, no hidden units and 23 output units was trained with the same input data as for the ILP experiments. We divided the 960 input neurons into four equally large sets of 240 nodes, one each for singulets, doublets, triplets and quadruplets.
Reference: [17] <author> Tveter, D. R. Fast-Backpropagation. </author> <title> Computer code available from the author. </title> <publisher> Address: </publisher> <address> 5228 N Nashville Ave, Chicago, Illinois, 60656, drt@chinet.chi.il.us, </address> <year> 1995. </year>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks <ref> [17, 16] </ref>, nearest neighbor classification [3, 18], and decision tree induction [12, 14]. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. <p> This was done independently of the above experimental setup, but using the same partitions for cross-validation. A standard backpropagation network <ref> [17, 16] </ref> with 960 input neurons, no hidden units and 23 output units was trained with the same input data as for the ILP experiments. We divided the 960 input neurons into four equally large sets of 240 nodes, one each for singulets, doublets, triplets and quadruplets.
Reference: [18] <author> Wettschereck, D. </author> <title> A study of distance-based machine learning algorithms. </title> <type> PhD Thesis, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR, </address> <year> 1994. </year> <month> 13 </month>
Reference-contexts: Four series of experiments are performed, with the following sets of attributes: observed multiplicities and frequencies, reduced multiplicities and frequencies, observed multiplicities only and reduced multiplicities only. Three different machine learning approaches were used: backpropagation networks [17, 16], nearest neighbor classification <ref> [3, 18] </ref>, and decision tree induction [12, 14]. Table 4 gives a summary of the accuracies on unseen cases for the three approaches and the four different problems as estimated by ten-fold cross-validation. Table 4: Classification accuracy on unseen cases when learning from classified 13C-NMR spectra with assigned atom numbers. <p> The classification accuracy on unseen cases as measured by ten-fold cross-validation (same folds as above) is 78.5%, which is almost the same as the accuracy achieved by FOIL using both red and prop. 4.2 Nearest neighbor using multiplicities only We also applied nearest neighbor classification <ref> [3, 18] </ref> to the propositional problem defined by the relation prop. The same experimental setting (cross-validation folds) was used. Training on the entire dataset gives 100% accuracy on training data.
References-found: 18

