URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:lu.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/research/starfish/papers.html
Root-URL: http://www.cs.dartmouth.edu
Email: dfk@cs.dartmouth.edu  
Title: Disk-directed I/O for an Out-of-core Computation  
Author: David Kotz 
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:lu.ps.Z  
Note: Copyright 1995 by IEEE. Appeared in Symp. on High Performance Distributed Computing, pages 159-166. Available at  
Abstract: New file systems are critical to obtain good I/O performance on large multiprocessors. Several researchers have suggested the use of collective file-system operations, in which all processes in an application cooperate in each I/O request. Others have suggested that the traditional low-level interface (read, write, seek) be augmented with various higher-level requests (e.g., read matrix). Collective, high-level requests permit a technique called disk-directed I/O to significantly improve performance over traditional file systems and interfaces, at least on simple I/O benchmarks. In this paper, we present the results of experiments with an out-of-core LU-decomposition program. Although its collective interface was awkward in some places, and forced additional synchronization, disk-directed I/O was able to obtain much better overall performance than the traditional system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. L. Best, A. Greenberg, C. Stanfill, and L. W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proc. of the 7th IPPS, </booktitle> <pages> pp. 489-495, </pages> <year> 1993. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Intel CFS and PFS [19], IBM Vesta [5], TMC <ref> [1] </ref>, and HFS [15], to name a few. There are also several systems intended for workstation clusters, such as PIOUS [16], and VIP-FS [10]. All of these systems decluster file data across many disks to provide parallel access to the data of any file.
Reference: [2] <author> R. Bordawekar, J. M. del Rosario, and A. Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proc. of Supercomp. </booktitle> <volume> '93, </volume> <pages> pp. 452-461, </pages> <year> 1993. </year> <month> 7 </month>
Reference-contexts: The extraneous synchronization of a collective interface would in general accentuate temporary load imbalances, but it can often allow dramatically better I/O performance. Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [2, 21] </ref>, or generated by a smart compiler [6, 22]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [3] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <publisher> MIT TR LCS/TR-516, </publisher> <year> 1991. </year>
Reference-contexts: In these experiments, we ran the program in Figure 2 with both the traditional caching file system and the disk-directed file system, on top of our parallel file-system simulator called STARFISH [12]. STARFISH is based on the Proteus parallel-architecture simulator <ref> [3] </ref>, which runs on a DEC-5000 workstation. It does not model any particular multiprocessor architecture or operating system, but we configure it to behave like a machine of contemporary technology.
Reference: [4] <author> P. Corbett, D. Feitelson, Y. Hsu, J.-P. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Tech. Rep. </type> <institution> NAS-95-002, NASA Ames Research Center, </institution> <address> 1995. </address> <publisher> v. 0.3. </publisher>
Reference-contexts: This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the 1 CHARISMA project. Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function <ref> [5, 7, 4] </ref>, or using an object-oriented interface [15, 11, 21], provide valuable semantic information to the file system, which can then be used for optimization purposes. <p> Unfortunately, few multiprocessor file systems provide a collective interface. CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time system, operating system, and hardware. The MPI message-passing interface may soon be extended to include I/O <ref> [4] </ref>, including collective I/O. Finally, there are several libraries for collective matrix I/O [23, 11, 21]. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [8]. <p> In our LU-decomposition example the code needed some careful structuring to ensure that all processes participated in all I/O requests. Clearly, a collective interface that supported subsets of processes would reduce the need to structure the code this way (the MPI-IO proposal <ref> [4] </ref> appears to have this support). Otherwise, any of the common collective matrix-I/O interfaces could be adapted for use. The next challenge is to define a specific interface and to experiment with real applications, such as the computational fluid dynamics we encountered in our tracing efforts [14].
Reference: [5] <author> P. F. Corbett and D. G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proc. of the Scalable High-Perf. Comp. Conf., </booktitle> <pages> pp. 63-70, </pages> <year> 1994. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Intel CFS and PFS [19], IBM Vesta <ref> [5] </ref>, TMC [1], and HFS [15], to name a few. There are also several systems intended for workstation clusters, such as PIOUS [16], and VIP-FS [10]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. <p> This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the 1 CHARISMA project. Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function <ref> [5, 7, 4] </ref>, or using an object-oriented interface [15, 11, 21], provide valuable semantic information to the file system, which can then be used for optimization purposes.
Reference: [6] <author> T. H. Cormen and A. Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <institution> Dartmouth College TR PCS-TR94-243. </institution>
Reference-contexts: Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library [2, 21], or generated by a smart compiler <ref> [6, 22] </ref>. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [7] <author> E. DeBenedictis and J. M. del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Proc. of the 11th IPCCC, </booktitle> <pages> pp. 0117-0124, </pages> <year> 1992. </year>
Reference-contexts: This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the 1 CHARISMA project. Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function <ref> [5, 7, 4] </ref>, or using an object-oriented interface [15, 11, 21], provide valuable semantic information to the file system, which can then be used for optimization purposes.
Reference: [8] <author> J. M. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on I/O in Par. Comp. Sys., </booktitle> <pages> pp. 56-70. </pages>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [4], including collective I/O. Finally, there are several libraries for collective matrix I/O [23, 11, 21]. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface <ref> [8] </ref>. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. When reading, the compute processors cooperate to read a matrix in a conforming distribution, chosen for best I/O performance, and then the data is redistributed to its ultimate destination.
Reference: [9] <author> D. G. Feitelson, P. F. Corbett, Y. Hsu, and J.-P. Prost. </author> <title> Parallel I/O systems and interfaces for parallel computers. In C.-L. Wu, editor, Multiprocessor Systems Design and Integration. </title> <publisher> World Scientific, </publisher> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: All of these systems decluster file data across many disks to provide parallel access to the data of any file. A full characterization of these systems is not possible here due to space limitations, but <ref> [9] </ref> presents a reasonable summary. Workload. The CHARISMA project traced production parallel scientific-computing workloads on an Intel iPSC/860 [14] and on a TMC CM-5 [20] to characterize their file-system activity.
Reference: [10] <author> M. Harry, J. M. del Rosario, and A. Choudhary. </author> <title> VIP-FS: A VIrtual, Parallel File System for high performance parallel and distributed computing. </title> <booktitle> In Proc. of the 9th IPPS, </booktitle> <pages> pp. 159-164, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: There are many parallel file systems today, including Intel CFS and PFS [19], IBM Vesta [5], TMC [1], and HFS [15], to name a few. There are also several systems intended for workstation clusters, such as PIOUS [16], and VIP-FS <ref> [10] </ref>. All of these systems decluster file data across many disks to provide parallel access to the data of any file. A full characterization of these systems is not possible here due to space limitations, but [9] presents a reasonable summary. Workload.
Reference: [11] <author> J. F. Karpovich, A. S. Grimshaw, and J. C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file I/O. </title> <booktitle> In Proc. of the 9th OOPSLA, </booktitle> <pages> pp. 191-204, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function [5, 7, 4], or using an object-oriented interface <ref> [15, 11, 21] </ref>, provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system. <p> CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time system, operating system, and hardware. The MPI message-passing interface may soon be extended to include I/O [4], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [23, 11, 21] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [8]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase.
Reference: [12] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proc. of the 1994 Symp. on OS Design and Impl., </booktitle> <pages> pp. 61-74, </pages> <month> Nov. </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on Nov. 8, </note> <year> 1994. </year>
Reference-contexts: Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance [18]. Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O <ref> [12] </ref> to improve performance by orders of magnitude. In [12], however, experiments were limited to simple benchmarks that read or wrote matrices. In this paper we evaluate the performance of disk-directed I/O on a much more complex program, an out-of-core LU-decomposition program. <p> Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O <ref> [12] </ref> to improve performance by orders of magnitude. In [12], however, experiments were limited to simple benchmarks that read or wrote matrices. In this paper we evaluate the performance of disk-directed I/O on a much more complex program, an out-of-core LU-decomposition program. <p> When writing, the data is first redistributed and then written in a conforming distribution. There are no published performance results for an out-of-core application using two-phase I/O. Disk-directed I/O. Disk-directed I/O is a technique for optimizing data transfer given a high-level, collective interface <ref> [12] </ref>. <p> In experiments with reading and writing one- and two-dimensional matrices, disk-directed I/O was as much as 18 times faster than traditional caching in some access patterns, and was never slower <ref> [12] </ref>. One implementation of a similar technique led to excellent I/O performance on an IBM SP-2 multiprocessor [21]. 3 LU decomposition LU decomposition represents the bulk of the effort in one technique for solving linear systems of equations. <p> In these experiments, we ran the program in Figure 2 with both the traditional caching file system and the disk-directed file system, on top of our parallel file-system simulator called STARFISH <ref> [12] </ref>. STARFISH is based on the Proteus parallel-architecture simulator [3], which runs on a DEC-5000 workstation. It does not model any particular multiprocessor architecture or operating system, but we configure it to behave like a machine of contemporary technology. <p> It includes an extremely accurate disk-drive model, and uses Proteus to count instructions while executing the actual system code needed to implement the file system. As such, it accurately accounts for the overhead of system software. We configured Proteus as in <ref> [12] </ref>, except as noted below. Simulation overhead limited our experiments to decomposing a 1024 fi 1024 matrix of single-precision numbers, using eight compute processors (CPs), eight I/O processors (IOPs), and eight disks (one on each IOP). <p> While this cache may seem small, it is consistent with the size of the system and problem, and with our previous experiments <ref> [12] </ref>. In the disk-directed file system, the IOPs allocated two one-block buffers per disk (for double-buffering each disk), or 16 blocks of total buffer space. <p> The former would require a very large cache, and the latter would have the effect of spreading out simultaneous multi-block requests into multiple localities, counteracting the benefits of the contiguous layout <ref> [12] </ref>. The results of experiments with 1 KB blocks support this statement. <p> As we show here and in <ref> [12] </ref>, disk-directed I/O can lead to much better performance than traditional caching. This paper shows that disk-directed I/O, using a collective, high-level interface, could be used effectively for an out-of-core LU-decomposition computation. The additional synchronization of the collective interface appeared not to be a significant factor here.
Reference: [13] <author> D. Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <type> Technical Report PCS-TR95-251, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Furthermore, insights come by comparing the performance of two configurations, rather than from the absolute performance of any one configuration. Thus, we normalize and compare by charting the ratio of a measure between one configuration and another (see <ref> [13] </ref> for the raw data). configurations using 4 KB blocks. Figure 3a focuses on the disk-I/O traffic.
Reference: [14] <author> D. Kotz and N. Nieuwejaar. </author> <title> File-system workload on a scientific multiprocessor. </title> <journal> IEEE Par. and Dist. Tech., </journal> <pages> pp. 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: A full characterization of these systems is not possible here due to space limitations, but [9] presents a reasonable summary. Workload. The CHARISMA project traced production parallel scientific-computing workloads on an Intel iPSC/860 <ref> [14] </ref> and on a TMC CM-5 [20] to characterize their file-system activity. In both cases, applications accessed large files (megabytes or gigabytes in size) using surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes). <p> While this interface is comfortable to parallel programmers familiar with sequential programming, it is inadequate for expressing their needs <ref> [14] </ref>. Given this interface and the amount of interprocessor spatial locality arising from interleaving tiny requests from many processors, caching is 6 of disk traffic and seconds of execution time. Less than 100% indicates that 1 KB was better. <p> Less than 100% indicates that 1 KB was better. For each case there are two bars, one for traditional caching (white) and one for disk-directed I/O (black). essential for reasonable performance <ref> [14] </ref>. A file system based on traditional caching, however, can have terrible performance [18] and, as we show in this paper, can have counter-intuitive performance characteristics (increasing the block size from 4 KB to 8 KB, or increasing the slab size from 16 to 32 columns, sometimes decreased performance). <p> Otherwise, any of the common collective matrix-I/O interfaces could be adapted for use. The next challenge is to define a specific interface and to experiment with real applications, such as the computational fluid dynamics we encountered in our tracing efforts <ref> [14] </ref>.
Reference: [15] <author> O. Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Intel CFS and PFS [19], IBM Vesta [5], TMC [1], and HFS <ref> [15] </ref>, to name a few. There are also several systems intended for workstation clusters, such as PIOUS [16], and VIP-FS [10]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. <p> Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function [5, 7, 4], or using an object-oriented interface <ref> [15, 11, 21] </ref>, provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system.
Reference: [16] <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proc. of the Scalable High-Perf. Comp. Conf., </booktitle> <pages> pp. 71-78, </pages> <year> 1994. </year>
Reference-contexts: There are many parallel file systems today, including Intel CFS and PFS [19], IBM Vesta [5], TMC [1], and HFS [15], to name a few. There are also several systems intended for workstation clusters, such as PIOUS <ref> [16] </ref>, and VIP-FS [10]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. A full characterization of these systems is not possible here due to space limitations, but [9] presents a reasonable summary. Workload.
Reference: [17] <author> N. Nieuwejaar and D. Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In IPPS '95 Workshop on I/O in Par. and Dist. Sys., </booktitle> <pages> pp. 47-62, </pages> <year> 1995. </year>
Reference-contexts: In both cases, applications accessed large files (megabytes or gigabytes in size) using surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes). On further examination, we discovered that most of the files were accessed in complex yet highly regular patterns <ref> [17] </ref>, most likely due to accessing multidimensional matrices. Interfaces. Most parallel file systems present the traditional abstraction of a file as a sequence of bytes with Unix interface semantics, and add a few extensions to control the behavior of an implicit file pointer shared among the processes. <p> This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the 1 CHARISMA project. Higher-level interfaces, such as speci-fying a strided series of requests <ref> [17] </ref>, accessing data through a mapping function [5, 7, 4], or using an object-oriented interface [15, 11, 21], provide valuable semantic information to the file system, which can then be used for optimization purposes.
Reference: [18] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Tech. Rep. </type> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <year> 1992. </year>
Reference-contexts: The use of disk striping to access many disks in parallel has alleviated some of the hardware limitations by providing greater capacity, bandwidth, and throughput. Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance <ref> [18] </ref>. Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O [12] to improve performance by orders of magnitude. In [12], however, experiments were limited to simple benchmarks that read or wrote matrices. <p> Less than 100% indicates that 1 KB was better. For each case there are two bars, one for traditional caching (white) and one for disk-directed I/O (black). essential for reasonable performance [14]. A file system based on traditional caching, however, can have terrible performance <ref> [18] </ref> and, as we show in this paper, can have counter-intuitive performance characteristics (increasing the block size from 4 KB to 8 KB, or increasing the slab size from 16 to 32 columns, sometimes decreased performance).
Reference: [19] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proc. of the Fourth Conf. on Hypercube Concurrent Comp. and Appl., </booktitle> <pages> pp. 155-160. </pages> <year> 1989. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Intel CFS and PFS <ref> [19] </ref>, IBM Vesta [5], TMC [1], and HFS [15], to name a few. There are also several systems intended for workstation clusters, such as PIOUS [16], and VIP-FS [10]. All of these systems decluster file data across many disks to provide parallel access to the data of any file.
Reference: [20] <author> A. Purakayastha, C. S. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best. Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proc. of the 9th IPPS, </booktitle> <pages> pp. 165-172, </pages> <year> 1995. </year>
Reference-contexts: A full characterization of these systems is not possible here due to space limitations, but [9] presents a reasonable summary. Workload. The CHARISMA project traced production parallel scientific-computing workloads on an Intel iPSC/860 [14] and on a TMC CM-5 <ref> [20] </ref> to characterize their file-system activity. In both cases, applications accessed large files (megabytes or gigabytes in size) using surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes).
Reference: [21] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <note> Submitted to Supercomputing '95, </note> <year> 1995. </year>
Reference-contexts: Higher-level interfaces, such as speci-fying a strided series of requests [17], accessing data through a mapping function [5, 7, 4], or using an object-oriented interface <ref> [15, 11, 21] </ref>, provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system. <p> CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time system, operating system, and hardware. The MPI message-passing interface may soon be extended to include I/O [4], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [23, 11, 21] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [8]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. <p> In experiments with reading and writing one- and two-dimensional matrices, disk-directed I/O was as much as 18 times faster than traditional caching in some access patterns, and was never slower [12]. One implementation of a similar technique led to excellent I/O performance on an IBM SP-2 multiprocessor <ref> [21] </ref>. 3 LU decomposition LU decomposition represents the bulk of the effort in one technique for solving linear systems of equations. An N fi N matrix M is decomposed into two matrices, a lower-triangular matrix L and an upper-triangular matrix U , such that LU = M . <p> The extraneous synchronization of a collective interface would in general accentuate temporary load imbalances, but it can often allow dramatically better I/O performance. Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [2, 21] </ref>, or generated by a smart compiler [6, 22]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [22] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs. </title> <booktitle> In Proc. of the 8th ACM Int'l Conf. on Supercomp., </booktitle> <pages> pp. 382-391, </pages> <year> 1994. </year>
Reference-contexts: Note that because of the cyclic distribution any one processor's slab is not contiguous in the file, but that the set of corresponding slabs for all processors collectively represent a contiguous set of bytes in the file. The code for parallel, out-of-core LU-decomposition (based on that in <ref> [22] </ref>) is shown in Figure 2. There are several things to note about this program. First, note the optimization to split the outer loop into two loops, with the I/O pulled out of the second loop. <p> Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library [2, 21], or generated by a smart compiler <ref> [6, 22] </ref>. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [23] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proc. of the Scalable Par. Libraries Conf., </booktitle> <pages> pp. 119-128, </pages> <year> 1994. </year>
Reference-contexts: CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time system, operating system, and hardware. The MPI message-passing interface may soon be extended to include I/O [4], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [23, 11, 21] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [8]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase.
Reference: [24] <author> D. Womble, D. Greenberg, S. Wheat, and R. Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In Proc. of the 1993 DAGS/PC Symposium, </booktitle> <pages> pp. 56-63, </pages> <address> Hanover, NH, </address> <year> 1993. </year> <note> Many of the above references are available via URL http://www.cs.dartmouth.edu/pario.html 8 </note>
Reference-contexts: N for k = i+1 to N M (j,k) -= mult (j) * M (i,k) end end One simple parallelization of this algorithm (although not the best; see <ref> [24] </ref> for a better algorithm) is to distribute responsibility for columns of the matrix among P processors in a cyclic pattern; that is, column k is handled by processor (k mod P ) (see Figure 1).
References-found: 24

