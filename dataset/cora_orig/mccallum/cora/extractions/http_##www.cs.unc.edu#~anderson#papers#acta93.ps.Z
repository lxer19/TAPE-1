URL: http://www.cs.unc.edu/~anderson/papers/acta93.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/papers.html
Root-URL: http://www.cs.unc.edu
Title: A Fine-Grained Solution to the Mutual Exclusion Problem  
Author: James H. Anderson 
Keyword: Busy-waiting, mutual exclusion, nonatomic operations, shared data, synchronization primitives. CR Categories: D.4.1, D.4.2, F.3.1  
Date: December 1991, Revised August 1992  
Address: College Park College Park, Maryland 20742-3255  
Affiliation: Department of Computer Science The University of Maryland at  
Abstract: We present a "fine-grained" solution to the mutual exclusion problem. A program is fine-grained if it uses only single-reader, single-writer boolean variables and if each of its atomic operations has at most one occurrence of at most one shared variable. In contrast to other fine-grained solutions that have appeared in the literature, processes in our solution do not busy-wait, but wait on one another only by executing await statements. Such statements can be implemented in practice either by means of context switching or by means of "local" spinning. We show that our algorithm is correct even if shared variables are accessed nonatomically. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson and M. Gouda, </author> <title> "Atomic Semantics of Nonatomic Programs", </title> <journal> Information Processing Letters, </journal> <volume> Vol. 28, </volume> <month> June 24, </month> <year> 1988, </year> <pages> pp. 99-103. </pages>
Reference-contexts: To keep the ensuing discussion as simple as possible, we follow the approach taken by Anderson and Gouda in <ref> [1] </ref> and assume that each statement that reads or awaits on a shared variable is atomic and that each statement that writes a shared variable is nonatomic; the semantics of each nonatomic write is specified by defining each such write to be equivalent to a particular sequence of atomic statements.
Reference: [2] <author> J. Burns and N. Lynch, </author> <title> "Bounds on Shared Memory for Mutual Exclusion", to appear in Information and Computation. Originally appeared as "Mutual Exclusion Using Indivisible Reads and Writes", </title> <booktitle> Proceedings of the 18th Allerton Conference on Communication, Control, and Computing, </booktitle> <month> October </month> <year> 1980, </year> <pages> pp. 833-842. </pages>
Reference-contexts: We call such a solution fine-grained . Previous fine-grained algorithms include the "One-Bit" algorithm discovered independently by Burns and Lynch <ref> [2] </ref> and by Lamport [7] and also the boolean variable implementation of the algorithm presented by Peterson in [11]. (Strictly speaking, none of these algorithms is fine-grained as presented because each employs multi-reader shared variables. <p> As discussed in Section 6, such statements can be efficiently implemented either by performing a context switch or by means of "local" spinning [9]. The algorithms given in <ref> [2, 7, 11] </ref> all employ busy-waiting loops in which shared variables are repeatedly read and updated and thus do not admit such implementations. The rest of this paper is organized as follows. In Section 2, we present our model of concurrent programs. <p> This stands in sharp contrast to the algorithms of <ref> [2, 7, 11] </ref>, where busy-waiting loops are employed in which shared variables are repeatedly tested and updated. The performance studies of [9] suggest that such busy-waiting may result in an unacceptable degree of memory and interconnect contention.
Reference: [3] <author> E. Dijkstra, </author> <title> "Solution of a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM , Vol. </journal> <volume> 8, No. 9, </volume> <year> 1965, </year> <pages> pp. 569. </pages>
Reference-contexts: its critical section. fl To appear in Acta Informatica. y Work supported, in part, by NSF Contract CCR 9109497 and by the Center of Excellence in Space Data and Information Sciences. 1 The mutual exclusion problem has been studied for many years, dating back to the seminal paper of Dijkstra <ref> [3] </ref>. Since then, many solutions have been proposed, most of which are quite complicated and difficult to understand. A notable exception is an especially simple solution presented by Peterson in [10]. In Peterson's paper, a two-process solution is presented and then generalized to apply to an arbitrary number of processes.
Reference: [4] <author> E. Dijkstra, </author> <title> A Discipline of Programming, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference-contexts: A concurrent program consists of a set of processes and a set of variables. A process is a sequential program consisting of labeled statements, and is specified using guarded commands <ref> [4] </ref> and await statements. An await statement has the form "await B", where B is a predicate over program variables. This statement is enabled for execution only when predicate B is true and is atomically executed (when enabled) by transferring control to the next 2 executable statement. <p> An assertion P is stable for a program iff for each statement k:i of that program, P ^ Enabled (k:i) ) wp (k:i; P ) holds, where wp is the "weakest precondition" predicate transformer <ref> [4] </ref>. The second approach involves proving that the given assertion follows from a set of known invariants.
Reference: [5] <author> J. Kessels, </author> <title> "Arbitration Without Common Modifiable Variables", </title> <journal> Acta Informatica, </journal> <volume> Vol. 17, </volume> <year> 1982, </year> <pages> pp. 135-141. </pages>
Reference-contexts: As such, its correctness is predicated upon the existence of an underlying mechanism for properly linearizing concurrent reads and concurrent writes of the same shared variable. A refinement of Peterson's algorithm in which only single-writer variables are used was later presented by Kessels in <ref> [5] </ref>. Although Kessels' algorithm is more fine-grained than Peterson's, it still employs multi-reader shared variables. <p> We also require that each process in its exit section eventually enters its noncritical section; this requirement is trivially satisfied by our solution (and most others), so we will not consider it further. As in <ref> [5, 10] </ref>, we obtain our solution to the mutual exclusion problem by solving the two-process case and by using two-process mutual exclusion to solve the N -process case. We use a well-known approach to do the latter. <p> is important that the two-process entry sections be executed in "increasing" order; this is similar to a two-phase locking protocol, where deadlock is avoided by locking data items in a fixed linear order.) If the underlying two-process solution is fine-grained, then the N -process program is also fine-grained. 4 (In <ref> [5] </ref>, Kessels describes a different approach for using two-process mutual exclusion to solve N - process mutual exclusion. In this approach, a two-process solution is applied in a binary arbitration tree. Associated with each link in the tree is an entry section and an exit section. <p> the two processes are identical. (Note that the statements of the program are not labeled in increasing linear order; this is done in order to facilitate the Progress proof.) The program is similar to the two-process solution given by Peterson in [10] and also to that given by Kessels in <ref> [5] </ref>, but uses only single-reader, single-writer boolean variables. The two variables T [u] and T [v] together correspond to the variable TURN of Peterson's algorithm, and are used as a tie-breaker in the event that both processes attempt to enter their critical sections at the same time.
Reference: [6] <author> L. Lamport, </author> <title> "On Interprocess Communication, Parts I and II", </title> <journal> Distributed Computing , Vol. </journal> <volume> 1, </volume> <year> 1986, </year> <pages> pp. 77-101. </pages>
Reference-contexts: Thus, in any fair history, each nonatomic write eventually terminates. Observe that our notion of a nonatomic shared variable is consistent with Lamport's definition of a "safe" shared register <ref> [6] </ref>. In particular, if a read of a shared variable is executed concurrently with a write to that variable, then the read may obtain any value from the value domain of the variable.
Reference: [7] <author> L. Lamport, </author> <title> "The Mutual Exclusion Problem II Statement and Solutions", </title> <journal> Journal of the ACM , Vol. </journal> <volume> 33, No. 2, </volume> <month> April </month> <year> 1986, </year> <pages> pp. 327-348. </pages>
Reference-contexts: We call such a solution fine-grained . Previous fine-grained algorithms include the "One-Bit" algorithm discovered independently by Burns and Lynch [2] and by Lamport <ref> [7] </ref> and also the boolean variable implementation of the algorithm presented by Peterson in [11]. (Strictly speaking, none of these algorithms is fine-grained as presented because each employs multi-reader shared variables. <p> As discussed in Section 6, such statements can be efficiently implemented either by performing a context switch or by means of "local" spinning [9]. The algorithms given in <ref> [2, 7, 11] </ref> all employ busy-waiting loops in which shared variables are repeatedly read and updated and thus do not admit such implementations. The rest of this paper is organized as follows. In Section 2, we present our model of concurrent programs. <p> This stands in sharp contrast to the algorithms of <ref> [2, 7, 11] </ref>, where busy-waiting loops are employed in which shared variables are repeatedly tested and updated. The performance studies of [9] suggest that such busy-waiting may result in an unacceptable degree of memory and interconnect contention.
Reference: [8] <author> L. Lamport, </author> <type> personal communication. </type>
Reference-contexts: Also, the One-Bit algorithm allows individual processes to starve, i.e., it fails to satisfy requirement (ii) in the first paragraph of this section. However, by adding an extra boolean variable to one of the processes, it is possible to obtain a two-process version of this algorithm that is starvation-free <ref> [8] </ref>.) Like these previous solutions, our algorithm is correct even if shared variables are accessed nonatomically; thus, no underlying mechanism is required for linearizing statements that access the same shared variable.
Reference: [9] <author> J. Mellor-Crummey and M. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 9, No. 1, </volume> <month> February </month> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: As discussed in Section 6, such statements can be efficiently implemented either by performing a context switch or by means of "local" spinning <ref> [9] </ref>. The algorithms given in [2, 7, 11] all employ busy-waiting loops in which shared variables are repeatedly read and updated and thus do not admit such implementations. The rest of this paper is organized as follows. In Section 2, we present our model of concurrent programs. <p> Such a statement can be implemented either by context switching or by means of a lower level busy-waiting loop such as the following. done := X; do :done ! done := X od As pointed out by Mellor-Crummey and Scott in <ref> [9] </ref>, many of today's multiprocessing systems permit shared variables to be locally accessible; such is the case if coherent caching schemes are employed, or if shared variables can be allocated in a local portion of distributed shared memory. <p> This stands in sharp contrast to the algorithms of [2, 7, 11], where busy-waiting loops are employed in which shared variables are repeatedly tested and updated. The performance studies of <ref> [9] </ref> suggest that such busy-waiting may result in an unacceptable degree of memory and interconnect contention. Acknowledgement: I would like to thank Mohamed Gouda, Leslie Lamport, Udaya Shankar, and Jae-Heon Yang for their comments on earlier drafts of this paper.
Reference: [10] <author> G. Peterson, </author> <title> "Myths About the Mutual Exclusion Problem", </title> <journal> Information Processing Letters, </journal> <volume> Vol. 12, No. 3, </volume> <month> June </month> <year> 1981, </year> <pages> pp. 115-116. </pages>
Reference-contexts: Since then, many solutions have been proposed, most of which are quite complicated and difficult to understand. A notable exception is an especially simple solution presented by Peterson in <ref> [10] </ref>. In Peterson's paper, a two-process solution is presented and then generalized to apply to an arbitrary number of processes. Although Peterson's algorithm requires only read/write atomicity, it employs shared variables that can be read and written by multiple processes. <p> We also require that each process in its exit section eventually enters its noncritical section; this requirement is trivially satisfied by our solution (and most others), so we will not consider it further. As in <ref> [5, 10] </ref>, we obtain our solution to the mutual exclusion problem by solving the two-process case and by using two-process mutual exclusion to solve the N -process case. We use a well-known approach to do the latter. <p> With the exception of statements 0, 8, and 10, the two processes are identical. (Note that the statements of the program are not labeled in increasing linear order; this is done in order to facilitate the Progress proof.) The program is similar to the two-process solution given by Peterson in <ref> [10] </ref> and also to that given by Kessels in [5], but uses only single-reader, single-writer boolean variables.
Reference: [11] <author> G. Peterson, </author> <title> "A New Solution to Lamport's Concurrent Programming Problem Using Small Shared Variables", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 5, No. 1, </volume> <month> January </month> <year> 1983, </year> <pages> pp. 56-65. 18 </pages>
Reference-contexts: We call such a solution fine-grained . Previous fine-grained algorithms include the "One-Bit" algorithm discovered independently by Burns and Lynch [2] and by Lamport [7] and also the boolean variable implementation of the algorithm presented by Peterson in <ref> [11] </ref>. (Strictly speaking, none of these algorithms is fine-grained as presented because each employs multi-reader shared variables. Also, the One-Bit algorithm allows individual processes to starve, i.e., it fails to satisfy requirement (ii) in the first paragraph of this section. <p> As discussed in Section 6, such statements can be efficiently implemented either by performing a context switch or by means of "local" spinning [9]. The algorithms given in <ref> [2, 7, 11] </ref> all employ busy-waiting loops in which shared variables are repeatedly read and updated and thus do not admit such implementations. The rest of this paper is organized as follows. In Section 2, we present our model of concurrent programs. <p> This stands in sharp contrast to the algorithms of <ref> [2, 7, 11] </ref>, where busy-waiting loops are employed in which shared variables are repeatedly tested and updated. The performance studies of [9] suggest that such busy-waiting may result in an unacceptable degree of memory and interconnect contention.
References-found: 11

