URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-95-50.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Email: schapire@research.att.com  singer@research.att.com  manfred@cse.ucsc.edu  
Title: A Comparison of New and Old Algorithms for A Mixture Estimation Problem  
Author: David P. Helmbold Robert E. Schapire Yoram Singer Manfred K. Warmuth 
Address: Santa Cruz, CA 95064 USA  600 Mountain Avenue, Murray Hill, NJ 07974,  600 Mountain Avenue, Murray Hill, NJ 07974,  Santa Cruz, CA 95064  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  AT&T Bell Laboratories,  AT&T Bell Laboratories,  Computer and Information Sciences, University of California,  
Date: October 27, 1995  
Pubnum: UCSC-CRL-95-50  
Abstract: We investigate the problem of estimating the proportion vector which maximizes the likelihood of a given sample for a mixture of given densities. We adapt a framework developed for supervised learning and give simple derivations for many of the standard iterative algorithms like gradient projection and EM. In this framework, the distance between the new and old proportion vectors is used as a penalty term. The square distance leads to the gradient projection update, and the relative entropy to a new update which we call the exponentiated gradient update (EG ). Curiously, when a second order Taylor expansion of the relative entropy is used, we arrive at an update EM which, for = 1, gives the usual EM update. Experimentally, both the EM -update and the EG -update for &gt; 1 outperform the EM algorithm and its variants. We also prove a polynomial bound on the worst-case global rate of convergence of the EG algorithm. fl Computer and Information Sciences, University of California, Santa Cruz, CA 95064, dph@cse.ucsc.edu
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe, J. Takeuchi, and M. K. Warmuth. </author> <title> Polynomial learnability of probablistic concepts with respect to the Kullback-Leibler divergence. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 277-289. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
Reference-contexts: Similar observations were made in the supervised setting [6, 7]. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Baysian averaging was also used in <ref> [1] </ref>) and then use the averaged matrix to run our algorithm. <p> The algorithm then updates w t . The loss suffered by the algorithm at time t is P p=1 while the loss of the (unknown) ML solution u is P p=1 4 This algorithm's performance was analyzed in the PAC model in <ref> [1] </ref>. 4. Convergence and Progress 9 We are interested in bounding the (cumulative) difference between the loss of the algorithm and the loss of the ML solution. We assume that max i x t;i = 1 for all p. <p> Z t = i=1 P Y exp x p;i ! N X w t;i p=1 exp ! x p;i ! 1=P 10 4. Convergence and Progress Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp !! ! 1=P N X P Y w t x p w t;i x p;i We will <p> Z t = i=1 P Y exp x p;i ! N X w t;i p=1 exp ! x p;i ! 1=P 10 4. Convergence and Progress Since x t;i 2 <ref> [0; 1] </ref> and since fi x 1 (1 fi)x for fi &gt; 0 and x 2 [0; 1] we can upper bound the right-hand side by: N X w t;i p=1 1 1 exp !! ! 1=P N X P Y w t x p w t;i x p;i We will need the following fact: For non-negative numbers A i;p , N X P Y A i;p <p> 1 exp !! ! 1=P yields an upper bound on Z t of P Y N X w t x p w t;i x p;i (4.4) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1: For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> x p;i (4.4) P Y w t x p : To further bound ln Z t , we apply the following: Lemma 1: For all ff 2 <ref> [0; 1] </ref> and x 2 R, ln (1 ff (1 e x )) ffx + x 2 =8 : Proof: Fix ff 2 [0; 1], and let f (x) = ffx + x 2 =8 ln (1 ff (1 e x )) : We wish to show that f (x) 0. <p> When some of the components x p;i are zero, or very close to zero, we can use the following algorithm which is parameterized by a real number ff 2 <ref> [0; 1] </ref>. Let ~ x p = (1 ff=N )x p + (ff=N )1 where 1 is the all 1's vector. <p> Since x p;i 2 <ref> [r; 1] </ref>, and assuming that w t;i 0, it follows that this is bounded by P p=1 w t x p + 2r 2 : Thus, summing over all t T , we get 1 ku w T +1 k 2 2 T X P X ln u x p !
Reference: [2] <author> T. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematical Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <year> 1991. </year>
Reference-contexts: We have also applied the on-line version of our algorithms to a portfolio selection problem investigated by Cover <ref> [2] </ref>. Although Cover's analytical bounds appear better than ours, preliminary experimental results indicate that EM and EG outperform Cover's algorithm on historical stock market data. Furthermore, our algorithms are computationally efficient while Cover's algorithm is exponential in the number of possible investments.
Reference: [3] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [9] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data <ref> [3, 14] </ref>. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [6] for supervised on-line learning.
Reference: [4] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction The problem of maximum-likelihood (ML) estimation of a mixture of densities is an important and well known learning problem <ref> [4] </ref>. ML estimators are asymptotically unbiased and are a basic tool for other more complicated problems such as clustering and learning hidden Markov models. We investigate the ML-estimation problem when the densities are given and only the mixture proportions are unknown.
Reference: [5] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns-Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: The EM and EG updates are competitive with the gradient projection update, and in fact there is no clear winner. As discussed in Section 4, we have some theoretical evidence 6 The conjugate gradient search is a method for iteratively searching a quadratic cost function <ref> [9, 5] </ref>. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: [6] <author> J. Kivinen and M. K. Warmuth. </author> <title> Additive versus exponentiated gradient updates. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1995. </year> <note> 20 References </note>
Reference-contexts: We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth <ref> [6] </ref> for supervised on-line learning. Our goal is to maximize the log likelihood of the observations as a function of the mixture vector w, denoted by LogLike (w). This is computationally hard and requires iterative methods. <p> Kivinen and Warmuth <ref> [6] </ref> study the supervised on-line setting where the vector w t summarizes the learning done in previous iterations 1 and that learning can be preserved by choosing a w t+1 that is "close" to w t . <p> Now the equations (3.2) become rL (w t ) i w t+1;i 1 + fl = 0 : 2 A similar update for the case of linear regression was first given by Kivinen and Warmuth <ref> [6] </ref>. 3. <p> More precisely, we use the same distance function that motivates the update as a potential function to obtain worst-case cumulative loss bounds over sequences of updates (similar to the methods applied to the supervised case <ref> [6] </ref>). The natural loss of a mixture vector w t for our problem is LogLike (w t ). Note that this loss is unbounded since the likelihood for w t is zero when there is some x p for which w t x p = 0. <p> In the supervised case, one can obtain firm worst-case loss bounds with respect to the square loss for various updates by analyzing the progress <ref> [6] </ref>. But the square loss is bounded and it is not surprising that it is much harder to obtain strong loss bounds for our (unbounded loss) unsupervised setting. <p> Similar observations were made in the supervised setting <ref> [6, 7] </ref>. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Baysian averaging was also used in [1]) and then use the averaged matrix to run our algorithm. <p> However, when most of the components of the ML solution are very small, the EM and EG updates tend to be better. Thus, as in other settings <ref> [6, 7, 8] </ref>, updates based on the relative entropy tend to be the best when the solution is sparse (see Figure 5.4). We also compared the performance of the various updates with second order methods.
Reference: [7] <author> J. Kivinen and M. K. Warmuth. </author> <title> The perceptron algorithm vs. winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. </title> <booktitle> In Proceedings of the Eighth Annual Workshop on Computational Learning Theory, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Similar observations were made in the supervised setting <ref> [6, 7] </ref>. We also show below how to obtain bounds when the entries in the matrix have zero-valued components. We essentially average the data matrix with a uniform matrix (this *-Baysian averaging was also used in [1]) and then use the averaged matrix to run our algorithm. <p> However, when most of the components of the ML solution are very small, the EM and EG updates tend to be better. Thus, as in other settings <ref> [6, 7, 8] </ref>, updates based on the relative entropy tend to be the best when the solution is sparse (see Figure 5.4). We also compared the performance of the various updates with second order methods.
Reference: [8] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: However, when most of the components of the ML solution are very small, the EM and EG updates tend to be better. Thus, as in other settings <ref> [6, 7, 8] </ref>, updates based on the relative entropy tend to be the best when the solution is sparse (see Figure 5.4). We also compared the performance of the various updates with second order methods.
Reference: [9] <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes <ref> [9] </ref> or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data [3, 14]. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. <p> In particular, the standard EM algorithm (using = 1) has the property that the non-negativity constraints are always preserved. 4 Convergence and Progress In this section we discuss the convergence properties of the algorithms. Using standard methods, as in <ref> [9] </ref>, it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. <p> The EM and EG updates are competitive with the gradient projection update, and in fact there is no clear winner. As discussed in Section 4, we have some theoretical evidence 6 The conjugate gradient search is a method for iteratively searching a quadratic cost function <ref> [9, 5] </ref>. When the cost function is non-quadratic, as is the likelihood function in our case, a variant of the conjugate gradient method can be devised.
Reference: [10] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature <ref> [14, 10] </ref>. 2 Definitions and Problem Statement Let R represent the real numbers. Let log denote the base 2 logarithm and let ln denote the natural logarithm. <p> At first, a high learning rate is used to quickly approach the ML solution. Later iterations use a lower learning rate to aid convergence. The EM algorithm is in fact a limiting case of a more general approach usually called Generalized EM (GEM). Neal and Hinton <ref> [10] </ref> considered one variant of GEM which involves examining only a portion of the observation matrix X on each iteration.
Reference: [11] <author> S. J. Nowlan. </author> <title> Soft Competative Adaption: Neural Network Learning Algorithms Based on Fitting Statistical Mixtures. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: The contour lines for d RE are deformed ellipses that bend towards the vertices of the triangular feasible region. One can also get an update by re-parameterizing the probability vectors and doing unconstrained gradient ascent in the new parameter space. We use the standard exponential parameterization <ref> [11] </ref>: w i = e r i = P N j=1 e r j and maximize the function ParLogLike (r) = LogLike (w (r)): (Note that the w's are probability vectors whereas the corresponding vectors r are uncon strained and lie in R N .) For this parameterization the gradient descent
Reference: [12] <author> B. C. Peters and H. F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach <ref> [12, 13, 14] </ref>. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker <ref> [12, 13] </ref>. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (3.6) also approximates the EG -update (3.5).
Reference: [13] <author> B. C. Peters and H. F. Walker. </author> <title> The numerical evaluation of the maximum-likelihood estimates of a subset of mixture proportions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 447-452, </pages> <year> 1978. </year>
Reference-contexts: Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach <ref> [12, 13, 14] </ref>. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. <p> The EM 1 update can be motivated by the likelihood equations, and the generalization to arbitrary was studied by Peters and Walker <ref> [12, 13] </ref>. Since the 2 distance approximates the relative entropy it may not be surprising that the EM -update (3.6) also approximates the EG -update (3.5). <p> Using standard methods, as in [9], it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. Moreover, using similar techniques, as in <ref> [13, 14] </ref>, it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
Reference: [14] <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> Siam Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Most of the common techniques to solve this problem are based on either gradient ascent iterative schemes [9] or on the Expectation Maximization (EM) algorithm for parameter estimation from incomplete data <ref> [3, 14] </ref>. We derive the standard iterative algorithms for the unsupervised mixture proportions estimation problem by placing them in a common hill-climbing framework. This framework is analogous to the one developed by Kivinen and Warmuth [6] for supervised on-line learning. <p> Our experimental evidence suggests that setting &gt; 1 results in a more effective update. These results agree with the infinitesimal analysis in the limit of n ! 1 based on a stochastic approximation approach <ref> [12, 13, 14] </ref>. For the exponentiated gradient algorithm, we are able to prove rigorous polynomial bounds on the number of iterations needed to get an arbitrarily good ML-estimator. <p> The derivations of the learning rules using the above framework are simple and can readily be applied to other settings. They are similar to previous derivations found in the literature <ref> [14, 10] </ref>. 2 Definitions and Problem Statement Let R represent the real numbers. Let log denote the base 2 logarithm and let ln denote the natural logarithm. <p> Using standard methods, as in [9], it can be shown that, given certain assumptions, all updates described in the previous section converge locally to an optimal ML solution, provided that the current mixture vector w t is close to the ML solution. Moreover, using similar techniques, as in <ref> [13, 14] </ref>, it can be shown that it is better to use a learning rate &gt; 1 rather than the rate = 1. This implies that the EM algorithm is not optimal for this family of update rules.
References-found: 14

