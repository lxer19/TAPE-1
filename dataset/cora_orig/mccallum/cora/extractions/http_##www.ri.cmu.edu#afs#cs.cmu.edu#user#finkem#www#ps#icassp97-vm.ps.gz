URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/ps/icassp97-vm.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/finkem/www/publications.html
Root-URL: 
Title: THE KARLSRUHE-VERBMOBIL SPEECH RECOGNITION ENGINE  
Author: Michael Finke Petra Geutner Hermann Hild Thomas Kemp Klaus Ries Martin Westphal 
Address: USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe, Germany Carnegie Mellon University,  
Abstract: Verbmobil, a German research project, aims at machine translation of spontaneous speech input. The ultimate goal is the development of a portable machine translator that will allow people to negotiate in their native language. Within this project the University of Karlsruhe has developed a speech recognition engine that has been evaluated on a yearly basis during the project and shows very promising speech recognition word accuracy results on large vocabulary spontaneous speech. In this paper we will introduce the Janus Speech Recognition Toolkit underlying the speech recognizer. The main new contributions to the acoustic modeling part of our 1996 evaluation system speaker normalization, channel normalization and polyphonic clustering- will be discussed and evaluated. Besides the acoustic models we delineate the different language models used in our evaluation system: Word trigram models interpolated with class based models and a separate spelling language model were applied. As a result of using the toolkit and integrating all these parts into the recognition engine the word error rate on the German Spontaneous Scheduling Task (GSST) could be decreased from 30% word error rate in 1995 to 13.8% in 1996. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Andreou, T. Kamm, and J. Cohen. </author> <title> Experiments in Vocal Tract Normalization. </title> <booktitle> In Proceedings of the CAIP Workshop: Frontiers in Speech Recognition II, </booktitle> <year> 1994. </year>
Reference-contexts: Based on these spectral features the further preprocessing steps can be summarized as speaker normalization, channel normalization, and speech feature extraction. 3.1.1. Speaker Normalization One major source of interspeaker variability in automatic continuous speech recognition is the variation in vocal tract shape among speakers. Andreou et al <ref> [1] </ref> proposed a set of maximum likelihood speaker normalization procedures to explicitly compensate for these variations.
Reference: [2] <author> Michael Finke and Ivica Rogina. </author> <title> Wide Context Acoustic Modeling in Read vs. Spontaneous Speech. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: For Verbmobil we ended up having 2500 codebooks and 10000 distributions. This clustering approach implementing a flexible parameter tying scheme gave us significant improvement across all tasks WSJ, SWB, and Spontaneous Scheduling Task, and across all languages involved (German i.e. Verbmobil, Spanish, English) <ref> [2] </ref>. 4. LANGUAGE MODELING In terms of language model training material the Verbmo-bil domain is a fairly small spontaneous speech corpus. As baseline we use a trigram backoff model with absolute discounting and non-linear interpolation.
Reference: [3] <author> Michael Finke, Torsten Zeppenfeld, Martin Maier, Laura Mayfield, Klaus Ries, Puming Zhan, John Laf-ferty, and Alex Waibel. </author> <title> Switchboard April 1996 Evaluation Report. </title> <booktitle> In Proceedings of LVCSR Hub 5 Workshop, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The Tk component adds a graphical user interface to the recognition toolkit thereby simplifying setting up and running demos. The toolkit passed its first test with the Janus Switchboard recognizer which was top ranking in DARPA's spring 96 LVCSR evaluation and currently has a state-of-the-art error rate of 36% <ref> [3, 9] </ref>. 3. ACOUSTIC MODELING Currently, approximately 32 hours of labelled spontaneous speech training material is available for training the acoustic models of our speech recognition engine.
Reference: [4] <author> Petra Geutner. </author> <title> Introducing Linguistic Constraints into Statistical Language Modeling. </title> <booktitle> In International Conference on Spoken Language Processing, </booktitle> <pages> pages 402-405, </pages> <address> Philadelphia, USA, </address> <year> 1996. </year>
Reference-contexts: Function and Content Words In order to introduce longer-term dependencies than conventional trigrams, some linguistic constraints were introduced into our language models. The notion of function and content words <ref> [4] </ref> was used in order to predict the next word not only based on the last word pair, but also on the last function/content word pair. An improvement of 0.4% WA abolute was achieved. 4.2.
Reference: [5] <author> Reinhard Kneser and Herman Ney. </author> <title> Improved Clustering Techniques for Class-Based Statistical Language Modeling. </title> <booktitle> In Eurospeech, </booktitle> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: To make up for the lack of training data, word-dependent linear interpolation of the baseline language model with models built on different corpora was used. Also class-based trigram models <ref> [5] </ref> were applied where each word is assigned to exactly one class. We achieved a word error reduction of 0:3% absolute by interpolating the baseline with a class-based Verbmobil model and a model built on a large German newspaper corpus (FAZ). 4.3. <p> The classes were found by an adaptive clustering algorithm, a variant of <ref> [5] </ref> that minimizes the perplexity of the adapted bigram model. We also achieved a word accuracy improvement around 0:5% absolute using phrases of words as the base unit of language modeling [7] on an earlier version of the system without retraining acoustics.
Reference: [6] <author> Li Lee and Richard C. Rose. </author> <title> Speaker Normalization us ing Efficient Frequency Warping Procedures. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 353-356, </pages> <address> Atlanta, 1996. </address> <publisher> IEEE. </publisher>
Reference-contexts: As a consequence a speaker normalization step was introduced into our preprocessing. In Janus we implemented a maximum likelihood approach similar to <ref> [6] </ref>, where the goal is to determine a frequency warping factor ^ff such that the warped speech signal fits best to the acoustic models. Let O ff i be the acoustic observation vectors for utterance i warped by ff based on a piecewise linear warping function as described in [8].
Reference: [7] <author> Klaus Ries, Finn Dag But, and Alex Waibel. </author> <title> Class phrase models for language modelling. </title> <booktitle> In International Conference on Spoken Language Processing, </booktitle> <address> Philadel-phia, USA, </address> <year> 1996. </year>
Reference-contexts: The classes were found by an adaptive clustering algorithm, a variant of [5] that minimizes the perplexity of the adapted bigram model. We also achieved a word accuracy improvement around 0:5% absolute using phrases of words as the base unit of language modeling <ref> [7] </ref> on an earlier version of the system without retraining acoustics.
Reference: [8] <author> Steven Wegmann, Don McAllaster, Jeremy Orloff, and Barbara Peskin. </author> <title> Speaker Normalization on Conversational Telephone Speech. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 339-341, </pages> <address> Atlanta, 1996. </address> <publisher> IEEE. </publisher>
Reference-contexts: Let O ff i be the acoustic observation vectors for utterance i warped by ff based on a piecewise linear warping function as described in <ref> [8] </ref>. <p> With gender-dependent modeling we could achieve a 2% relative decrease in WER compared to the speaker independent non VTLN system assuming perfect gender detection. That means that gender-dependent modeling is outperformed by VTLN as it was observed on SWB, too <ref> [8] </ref>. One explanation is that the speaker clustering and subsequent training of independent acoustic models reduced the training data for each recognizer to about the half. The VTLN approach on the other hand aims at normalizing with respect to the speakers' vocal tract shape.

References-found: 8

