URL: http://www.cs.utexas.edu/users/pclark/papers/phd.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/phd.abs.html
Root-URL: 
Title: A Model of Argumentation and Its Application in a Cooperative Expert System  
Author: Peter Clark Ph.D. 
Date: 1991  
Address: Glasgow  
Affiliation: Turing Institute Department of Computer Science University of Strathclyde  
Abstract-found: 0
Intro-found: 1
Reference: [Aha89] <author> D. W. Aha. </author> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 387-391, </pages> <address> NY, 1989. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: This work contributes to research in case-based reasoning in the following ways. First, our model augments case descriptions (the grounds) with models of reasoning about those descriptions (the arguments). This provides a knowledge-rich way to identify relevant cases, and contrasts with statistical methods sometimes used in case-based reasoning eg. <ref> [Aha89, Aha90] </ref>. A model of opinion can be seen as a personalised domain theory, containing a dynamic component controllable by the user (the warrant strengths). This work thus contributes a practical theory of how dynamic and encoded similarity assessment methods can be combined for case-based reasoning.
Reference: [Aha90] <author> David W. Aha. </author> <title> A study of instance-based algorithms for supervised learning tasks. </title> <type> Tech. Report 90-42, </type> <institution> Univ. California at Irvine, </institution> <address> CA, </address> <month> Nov </month> <year> 1990. </year> <note> (Also available as author's PhD thesis). </note>
Reference-contexts: This is significant: it allows us to distinguish between relevant and incidental features relating to a decision, and hence to know which case features to search for when looking for precedents. We are thus using knowledge-based rather than statistical techniques (eg. <ref> [Aha90] </ref>) to identify feature relevance in precedents. 2. Warrants provide a natural way of expressing the components in a chain of reasoning and how they combine to conclude a claim. <p> As a result, models would not need to be explicitly stored but instead generated at run-time. For efficiency, only the parts of the model required for the current problem would need to be generated (`lazy generalisation' <ref> [Aha90] </ref>, p4). This is a move to a case-based model of a user's opinion. <p> This work contributes to research in case-based reasoning in the following ways. First, our model augments case descriptions (the grounds) with models of reasoning about those descriptions (the arguments). This provides a knowledge-rich way to identify relevant cases, and contrasts with statistical methods sometimes used in case-based reasoning eg. <ref> [Aha89, Aha90] </ref>. A model of opinion can be seen as a personalised domain theory, containing a dynamic component controllable by the user (the warrant strengths). This work thus contributes a practical theory of how dynamic and encoded similarity assessment methods can be combined for case-based reasoning.
Reference: [AKA91] <author> David W. Aha, Dennis Kibler, and Marc K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Cases include not just data (the grounds) and a claim, but also a representation of the reasoning about how the claim was concluded from the grounds (the argument). Aha describes case-based reasoning systems as comprising (i) a similarity assessment component, (ii) a performance component and (iii) a learning component <ref> [AKA91] </ref>. For similarity assessment, there are several case-based reasoning systems which also use domain knowledge to identify important features (eg. Hypo [RVA84], Protos [BPM89] and by Cain et al., [CPS91]). 3.10. RELATED WORK 49 Our model is thus an example within this group. <p> The use of precedent arguments is a form of case-based reasoning, where the case consists not only of a problem description but also a `snap-shot' of a geologist's reasoning about that problem. Using Aha's framework for case-based reasoning <ref> [AKA91] </ref>, the details of the three main components are as follows: Similarity Assessment: A relevant, previous case must be one in which same skeleton warrant 4.9. THE DYNAMICS OF ARGUMENTATION 71 whose strength is under dispute was applied.
Reference: [And89] <author> B. Andrews. </author> <title> Successful Expert Systems. Financial Times Business Info Ltd., </title> <address> Lon-don, </address> <year> 1989. </year>
Reference: [AR87] <author> Kevin D. Ashley and Edwina L. Rissland. </author> <title> Compare and contrast, a test of expertise. </title> <booktitle> In AAAI-87, </booktitle> <pages> pages 273-278, </pages> <year> 1987. </year>
Reference: [AW87] <author> Chidanand V. Apte and Sholom M. Weiss. </author> <title> An expert system methodology for control and interpretation of applications software. </title> <journal> International Journal of Expert Systems, </journal> <volume> 1(1) </volume> <pages> 17-37, </pages> <year> 1987. </year>
Reference: [BA82] <author> R. Banares-Alcantara. </author> <title> Development of a Consultant for Physical Property Predictions. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon Univ., </institution> <address> PA, </address> <year> 1982. </year>
Reference: [Bai86] <author> William M. Bain. </author> <title> A case-based reasoning system for subjective assessment. </title> <booktitle> In AAAI-86, </booktitle> <pages> pages 523-527, </pages> <year> 1986. </year>
Reference-contexts: The inclusion of this justification knowledge is novel for case-based reasoning. Finally, our model assumes cases will be stored in a simple database; this contrasts with case-based approaches which store cases in a more complex, organised structure (eg. Judge <ref> [Bai86] </ref>, Unimem [Leb86] and Cyrus [Kol83]). We thus assume standard database mechanisms will be adequate for search and retrieval. 3.10.4 Conflict Resolution Ideally, an argumentation system should be able to resolve conflicts which it locates in its knowledge.
Reference: [BC88] <author> Michael L. Begeman and Jeff Conklin. </author> <title> The right tool for the job. </title> <journal> Byte, </journal> <volume> 13(10) </volume> <pages> 255-266, </pages> <month> Oct </month> <year> 1988. </year>
Reference-contexts: Introduction gIBIS (graphical Issue-Based Information System) <ref> [CB87, BC88] </ref> and rIBIS (real-time IBIS) [RE91] are hypertext systems designed to support design deliberations among cooperating experts. As mentioned earlier, these systems are hypertext systems for group decision support, and thus span these two categories in this review.
Reference: [BD83] <author> A. Bonnet and C. Dahan. </author> <title> Oil-well interpretation using expert system and pattern recognition technique. </title> <booktitle> In IJCAI-83, </booktitle> <pages> pages 185-189, </pages> <year> 1983. </year>
Reference: [BF81] <author> B. G. Buchanan and E. A. Feigenbaum. </author> <title> Dendral and Meta-Dendral: Their application dimension. </title> <editor> In B. L. Webber and N. J. Nilsson, editors, </editor> <booktitle> Readings in Artificial Intelligence, </booktitle> <pages> pages 313-322. </pages> <publisher> Tioga, </publisher> <address> CA, </address> <year> 1981. </year>
Reference: [BFF91] <author> Ray Bareiss, William Ferguson, and Andre Fano. </author> <title> The story archive: A memory for case-based tutoring. </title> <booktitle> In Proc. 1991 Case-Based Reasoning Workshop, </booktitle> <pages> pages 269-279, </pages> <address> CA, 1991. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: Toulmin's approach alleviates this conflict by regarding rules as argument components which may be modified by case evidence. Our model provides a practical embodiment of Toulmin's ideas. Finally, Optimist can be seen as a mature implementation of a case-based tutoring system (Bareiss et al. discuss this concept in <ref> [BFF91] </ref>). The basic process of selectively informing users about what others have done in similar, previous problems can be used to tutor new users in a domain.
Reference: [Bid89] <author> Hossein Bidgoli. </author> <title> Decision Support Systems: </title> <booktitle> Principles and Practice. </booktitle> <publisher> West Pub., </publisher> <address> St. Paul, MN, </address> <year> 1989. </year>
Reference: [BML89] <author> Ivan Bratko, Igor Mozetic, and Nada Lavrac. Kardio: </author> <title> A Study in Deep and Qualitative Knowledge for Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Ma, </address> <year> 1989. </year> <note> 137 138 BIBLIOGRAPHY </note>
Reference: [BMS88] <author> D. G. Bobrow, S. Mittal, and M. J. Stefik. </author> <title> Expert systems perils and promise. </title> <editor> In D. T. Pham, editor, </editor> <booktitle> Expert Systems in Engineering, </booktitle> <pages> pages 19-42. </pages> <publisher> IFS Publications, Bradford, </publisher> <year> 1988. </year>
Reference-contexts: Other authors have reported similar high figures for user-interface components, eg. 42% for Dipmeter [SB83] and 33%-50% for Pride <ref> [BMS88] </ref>. 3. The argument construction algorithm is small and simple, accounting directly for about 3% of the code (it indirectly uses other code, eg. interfaces to models and data).
Reference: [BPM89] <author> E. R. Bareiss, B. W. Porter, and K. S. Murray. </author> <title> Supporting start-to-finish development of knowledge bases. </title> <journal> Machine Learning, </journal> 4(3/4):259-283, 1989. 
Reference-contexts: Aha describes case-based reasoning systems as comprising (i) a similarity assessment component, (ii) a performance component and (iii) a learning component [AKA91]. For similarity assessment, there are several case-based reasoning systems which also use domain knowledge to identify important features (eg. Hypo [RVA84], Protos <ref> [BPM89] </ref> and by Cain et al., [CPS91]). 3.10. RELATED WORK 49 Our model is thus an example within this group. Second, our model retrieves cases in order to warn users of possible inconsistencies in their reasoning. <p> This simplicity distinguishes it from other knowledge acquisition systems, described earlier in Section 1.6.3. With Protos <ref> [BPM89] </ref>, for example, the user expresses his or her opinions in a formal, Lisp-like syntax, reported to require either programming skills or assistance for its use [Sha91]. <p> In this research we have explored a particular trade-off between generality and practicality of knowledge acquisition, adopting an approach less general but more practical than other more complex systems for knowledge acquisition (eg. Protos <ref> [BPM89] </ref>). Our implementation contributes a `data point' to this trade-off, illustrating a point at which practicality is achieved. It also presents challenges for further improving the knowledge acquisition functions it supports while maintaining usability. Finally, the evaluation supports the view of knowledge acquisition as a modelling rather than extraction process.
Reference: [BPW90] <author> E. R. Bareiss, B. W. Porter, and C. C. Wier. PROTOS: </author> <title> An exemplar-based learning apprentice. </title> <editor> In Yves Kodratoff and Ryszard Michalski, editors, </editor> <booktitle> Machine Learning: An AI Approach (Volume 3), </booktitle> <pages> pages 112-127, </pages> <address> Ca, 1990. </address> <publisher> Kaufmann. </publisher>
Reference: [Bra90] <author> Rafael Bracho, </author> <month> Oct. </month> <year> 1990. </year> <title> (rxb@eng.sun.com, </title> <type> Personal Communication). </type>
Reference: [Buc85] <author> Bruce G. Buchanan. </author> <title> Expert systems. In Knowledge-Based Systems and Their Applications: Texas Instruments AI Satellite Symposium. Texas Instruments, </title> <booktitle> 1985. (Seminar Notes). </booktitle>
Reference: [BWB87] <author> A. Brooks, A. Walker, and C. Boardman. </author> <booktitle> At the interface of shell-built expert systems. In Proc. Third Int. Expert Systems Conference, </booktitle> <pages> pages 223-232, </pages> <address> Oxford, </address> <year> 1987. </year> <note> Learned Information. </note>
Reference-contexts: This helps to isolate the contribution which the decision-aiding system has made. However, there are problems also with this experimental method. Most severely, practical issues may prohibit or limit its application: 1. It may be infeasible to achieve sufficiently large groups to eliminate natural performance variation between users <ref> [BWB87] </ref>. 2. It may be infeasible to perform a sufficiently long experiment to adequately measure a system's impact (again, this depends on the application task). <p> For example, large control group experiments are infeasible, due to the limited availability of sufficient numbers of trained experts. These pragmatic constraints are not uncommon in system evaluation, and must be taken into consideration during evaluation design <ref> [BWB87, O'K89] </ref>. 5.3 Evaluation Criteria Given this evaluation context, we now set out the claims which we wish to establish and the assessment methods which we apply. Our thesis goal is to develop a practical, computational model of argumentation which can usefully support users in decision-making.
Reference: [CAG + 86] <author> John Castner, Chidanand Aptee, James Griesmer, Se June Hong, Maurice Kar-naugh, Eric Mays, and Yoshio Tozawa. </author> <title> A knowledge-based consultant for financial marketing. </title> <journal> AI Magazine, </journal> <volume> 7(5) </volume> <pages> 71-79, </pages> <year> 1986. </year>
Reference: [CB87] <author> Jeff Conklin and Michael L. Begeman. gIBIS: </author> <title> a hypertext tool for team design deliberation. </title> <editor> In John B. Smith and Frank Halasz, editors, </editor> <booktitle> Hypertext '87 Proceedings, </booktitle> <pages> pages 247-251, </pages> <address> NY, 1987. </address> <publisher> Assoc. Computing Machinery. </publisher>
Reference-contexts: Introduction gIBIS (graphical Issue-Based Information System) <ref> [CB87, BC88] </ref> and rIBIS (real-time IBIS) [RE91] are hypertext systems designed to support design deliberations among cooperating experts. As mentioned earlier, these systems are hypertext systems for group decision support, and thus span these two categories in this review.
Reference: [CB91] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Machine Learning - EWSL-91, </booktitle> <pages> pages 151-163, </pages> <address> Berlin, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In this first application, the task is to advise users, interested in applying machine learning techniques to solve some problem, as to whether the particular machine learning tool ID/CN (implementing algorithms ID3 [Qui83] and CN2 <ref> [CN89, CB91] </ref>) is appropriate or not.
Reference: [CCT85] <author> CCT Agency. </author> <title> Expert systems: Some guidelines. </title> <type> Ccta report, </type> <institution> HM Treasury, </institution> <address> London, </address> <month> August </month> <year> 1985. </year>
Reference: [CHDH82] <author> A. N. Campbell, V. F. Hollister, R. O. Duda, and P. E. Hart. </author> <title> Recognition of a hidden mineral deposit by an artificial intelligence program. </title> <journal> Science, </journal> <volume> 217(3) </volume> <pages> 927-929, </pages> <month> Sept. </month> <year> 1982. </year>
Reference: [Che88] <author> Peter Cheeseman. </author> <title> An inquiry into computer understanding. </title> <journal> Computational Intelligence, </journal> <volume> 4(1), </volume> <month> Feb </month> <year> 1988. </year> <note> (followed by 23 commentaries). </note>
Reference-contexts: Consensus Approaches One class of approach to conflict resolution is to combine conflicting evidence or conclusions together using analytic methods. For example, Bayesian techniques combine evidence relating to a conclusion together <ref> [Che88] </ref>, and game-theoretic methods select the best course of action from alternatives given quantified reliabilities, costs and payoffs [Ros85, vM44]. This approach can be taken in domains where probabilistic or decision-theoretic models can be precisely specified.
Reference: [CHQ + 89] <author> P. Compton, K. Horn, J. R. Quinlan, L. Lazarus, and K. Ho. </author> <title> Maintaining an expert system. </title> <editor> In J. Ross Quinlan, editor, </editor> <booktitle> Applications of Expert Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 366-384. </pages> <publisher> Addison-Wesley, </publisher> <address> Sydney, </address> <year> 1989. </year>
Reference-contexts: We also make some final comments concerning the view of recorded appraisals as expressing rather than deriving an answer; this considers appraisal as the two processes of oil probability assessment and the translation of the reasoning in that assessment into a numeric form (six probabilities). Compton and Jansen <ref> [CJ89, CHQ + 89] </ref> have advocated a similar view of knowledge acquisition, regarding it as a post hoc justification of inarticulatable reasoning. <p> Compton et al. suggest experts are engaged in the following processes: "Firstly, he [the expert] identifies the correct interpretation for the case, and secondly he justifies this interpretation. The justification...is what the expert communicates. The insight...is not available." p378 <ref> [CHQ + 89] </ref> Seen from this perspective, the activity of constructing arguments (both with or without a computer) is a post hoc translation process, translating inarticulatable reasoning into a communicable form.
Reference: [CJ89] <author> P. Compton and R. Jansen. </author> <title> A philosophical basis for knowledge acquisition. </title> <type> Tech. Report TR-FD-89-01, </type> <institution> CSIRO IT Division, </institution> <address> Sydney, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: We also make some final comments concerning the view of recorded appraisals as expressing rather than deriving an answer; this considers appraisal as the two processes of oil probability assessment and the translation of the reasoning in that assessment into a numeric form (six probabilities). Compton and Jansen <ref> [CJ89, CHQ + 89] </ref> have advocated a similar view of knowledge acquisition, regarding it as a post hoc justification of inarticulatable reasoning.
Reference: [Cla88a] <author> Peter Clark. </author> <title> Exemplar-based reasoning in geological prospect appraisal. </title> <type> TIRM 034, </type> <institution> Turing Institute, Glasgow, UK, </institution> <year> 1988. </year>
Reference-contexts: Each well's relevance (a kind of similarity) is assessed by constructing a relevance argument, using warrants for the claim that well is relevant. This is described in more detail in <ref> [Cla88a] </ref>. The performance component makes predictions using statistics over the relevant wells, and learning occurs simply by adding new wells to the database. Postmortem Analysis Because Optimist maintains time-stamped arguments for appraisals, it can perform an after-the-event (`post mortem') analysis of appraisals after the outcome of drilling is known.
Reference: [Cla88b] <author> Peter Clark. </author> <title> Representing arguments as background knowledge for constraining generalisation. </title> <editor> In Derek Sleeman, editor, </editor> <booktitle> Proc. Third European Working Session on Learning (EWSL-88), </booktitle> <pages> pages 37-44, </pages> <address> London, </address> <month> October </month> <year> 1988. </year> <note> Pitman. BIBLIOGRAPHY 139 </note>
Reference-contexts: This contrasts with a skeleton warrant, which represents only one possible influence on a claim. Because determinations assume containment and do not represent degrees of uncertainty, their use for argumentation is limited. The possibilities for and limitations of determinations for argumentation are discussed further by Clark <ref> [Cla88b, Cla88c] </ref>. 3.11 Summary We have presented a model of argumentation, specifying structures for representing knowledge, how they relate and how they can be used for cooperative problem-solving. We now describe and evaluate one way in which this model can be applied.
Reference: [Cla88c] <author> Peter Clark. </author> <title> Representing arguments as background knowledge for the justification of case-based inferences. </title> <editor> In E. L. Rissland and J. A. King, editors, </editor> <booktitle> Proc. AAAI-88 Workshop on Case-Based Reasoning, </booktitle> <pages> pages 24-29. </pages> <publisher> AAAI, </publisher> <month> August </month> <year> 1988. </year> <note> (Also available as TIRM-40 from Turing Institute, Glasgow). </note>
Reference-contexts: This contrasts with a skeleton warrant, which represents only one possible influence on a claim. Because determinations assume containment and do not represent degrees of uncertainty, their use for argumentation is limited. The possibilities for and limitations of determinations for argumentation are discussed further by Clark <ref> [Cla88b, Cla88c] </ref>. 3.11 Summary We have presented a model of argumentation, specifying structures for representing knowledge, how they relate and how they can be used for cooperative problem-solving. We now describe and evaluate one way in which this model can be applied.
Reference: [Cla89] <author> William J. Clancey. </author> <title> The knowledge level reinterpreted: Modelling how systems interact. </title> <journal> Machine Learning, </journal> 4(3/4):285-291, 1989. 
Reference-contexts: Clancey writes 136 CHAPTER 7. CONCLUSION "Knowledge acquisition, in particular, is a process of developing computer models, often for the first time, not a process of transferring or accessing statements or diagrams that are already written down and filed away in an expert's mind." p39 <ref> [Cla89] </ref> Others have made similar comments. Compton et al. suggest experts are engaged in the following processes: "Firstly, he [the expert] identifies the correct interpretation for the case, and secondly he justifies this interpretation. The justification...is what the expert communicates.
Reference: [Cla90a] <author> Peter Clark. </author> <title> Induction with nixdorf applications. </title> <institution> Esprit MLT Report TI/P2154/PC/wp8/1.2, Turing Institute, </institution> <year> 1990. </year>
Reference: [Cla90b] <author> Peter Clark. </author> <title> Induction with uci-forth medical applications. </title> <institution> Esprit MLT Report TI/P2154/PC/wp8/1.1, Turing Institute, </institution> <year> 1990. </year>
Reference: [Cla90c] <author> Peter Clark. </author> <title> Representing knowledge as arguments: Applying expert system technology to judgemental problem-solving. </title> <editor> In T. R. Addis and R. M. Muir, editors, </editor> <booktitle> Research and Development in Expert Systems VII, </booktitle> <pages> pages 147-159. </pages> <publisher> Cambridge Univ. Press, </publisher> <year> 1990. </year> <booktitle> (Proc. ES90, the 10th BCS Specialist Group on Expert Systems). </booktitle>
Reference: [CN89] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning Journal, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: Given a set of examples, warrants could be automatically generated from the data, backed as being from statistical evidence in that data set. This is in fact the task that inductive rule learning systems perform <ref> [CN89, Qui83] </ref>. Precedent: Warrants can obtain some support even from a single example of their use, especially when that example is recognised as representative in some way of a class of examples. <p> In this first application, the task is to advise users, interested in applying machine learning techniques to solve some problem, as to whether the particular machine learning tool ID/CN (implementing algorithms ID3 [Qui83] and CN2 <ref> [CN89, CB91] </ref>) is appropriate or not.
Reference: [Coh85] <author> P. R. Cohen. </author> <title> Heuristic reasoning about uncertainty : An AI approach. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: However, such principles are difficult to formulate, and even this simple principle is sometimes violated as he himself illustrates. Cohen has similarly strongly advocated the use of domain knowledge to reason about uncertainty and conflict <ref> [Coh85] </ref>. He uses the term endorsements to describe elements of meta-knowledge supporting facts and rules within a system, similar to Toulmin's backings. Despite his persuasive philosophy, however, Cohen had substantial difficulties in developing a computational model of this theory.
Reference: [CPS91] <author> Timothy Cain, Michael Pazzani, and Glen Silverstein. </author> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proc. 1991 Case-Based Reasoning Workshop, </booktitle> <pages> pages 191-202, </pages> <address> CA, 1991. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: For similarity assessment, there are several case-based reasoning systems which also use domain knowledge to identify important features (eg. Hypo [RVA84], Protos [BPM89] and by Cain et al., <ref> [CPS91] </ref>). 3.10. RELATED WORK 49 Our model is thus an example within this group. Second, our model retrieves cases in order to warn users of possible inconsistencies in their reasoning.
Reference: [DAC + 81] <author> R. Davis, H. Austin, I. Carlbom, B. Frawley, P. Pruchnik, R. Sneiderman, and J. A. Gilreath. </author> <title> The dipmeter advisor: interpretation of geological signals. </title> <journal> In IJCAI-81, </journal> <volume> volume 2, </volume> <pages> pages 846-849, </pages> <year> 1981. </year>
Reference: [Dav77] <author> Randall Davis. </author> <title> Interactive transfer of expertise: Acquisition of new inference rules. </title> <booktitle> In IJCAI-77, </booktitle> <pages> pages 321-328, </pages> <year> 1977. </year>
Reference: [DBG91] <author> Nicholas Duncan, Eli Blevis, and Janice Glasgow. </author> <title> A case-based approach to decision support. </title> <institution> Queen's Univ., Canada. </institution> <note> (Submitted for publication), </note> <year> 1991. </year>
Reference-contexts: Hypo [RVA84], Protos [BPM89] and by Cain et al., [CPS91]). 3.10. RELATED WORK 49 Our model is thus an example within this group. Second, our model retrieves cases in order to warn users of possible inconsistencies in their reasoning. A similiar approach was explicitly used in Ladies <ref> [DBG91] </ref>, and is also the implicit goal of many case-based systems solely performing case retrieval (eg. [Kol88]). Our argumentation model, however, is distinguished by presenting not only the previous, relevant judgement but also the justification why that judgement was made (the backing).
Reference: [deK84] <author> J. deKleer. </author> <title> Choices without backtracking. </title> <booktitle> In AAAI-84, </booktitle> <year> 1984. </year>
Reference-contexts: Truth maintenance techniques are examples of this approach. Given a set of (possibly conflicting) statements, a truth maintenance system searches for a consistent subset of them <ref> [Doy79, deK84] </ref> equivalent to retracting some of the conflicting statements. These systems, though, used little or no knowledge about which statements should be blamed in the case of a conflict.
Reference: [DGH81] <author> Richard Duda, John Gaschnig, and Peter Hart. </author> <title> Model design in the Prospector consultant system for mineral exploration. </title> <editor> In B. L. Webber and N. J. Nilsson, editors, </editor> <booktitle> Readings in Artificial Intelligence, </booktitle> <pages> pages 334-348. </pages> <publisher> Tioga, </publisher> <address> CA, </address> <year> 1981. </year>
Reference: [DH88] <author> A. D'Agapeyeff and C. J. B. Hawkins. </author> <title> Expert systems in UK business: A critical assessment. </title> <journal> Knowledge Engineering Review, </journal> <volume> 2(3) </volume> <pages> 185-201, </pages> <month> Sept. </month> <year> 1988. </year>
Reference: [DKSZ79] <author> Randell Davis, Shigetoko Kaihara, Edward Shortliffe, and Lofti Zadeh. </author> <title> Panel on dealing with uncertainty. </title> <booktitle> In IJCAI-79, </booktitle> <pages> pages 1101-1102, </pages> <year> 1979. </year>
Reference-contexts: Thus in domains where opinions differ, it may often also be the case that an objective statistical approach has limited value. Qualitative Representations of Strength It is well known that users can have difficulty in precisely quantifying degrees of belief (eg. <ref> [Saf87, DKSZ79] </ref>). Although our model will tolerate a degree of imprecision, an alternative would be to ask users to specify belief in a qualitative manner (eg. "this warrant is very strong"). We illustrated a simple use of qualitative measures of warrant strength in the ML Advisor application (Section 6.5).
Reference: [Doy79] <author> J. Doyle. </author> <title> A truth maintenance system. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 231-272, </pages> <year> 1979. </year>
Reference-contexts: Truth maintenance techniques are examples of this approach. Given a set of (possibly conflicting) statements, a truth maintenance system searches for a consistent subset of them <ref> [Doy79, deK84] </ref> equivalent to retracting some of the conflicting statements. These systems, though, used little or no knowledge about which statements should be blamed in the case of a conflict.
Reference: [DR87] <author> Todd R. Davies and Stuart J. Russell. </author> <title> A logical approach to reasoning by analogy. </title> <booktitle> In IJCAI-87, </booktitle> <pages> pages 264-270, </pages> <year> 1987. </year>
Reference-contexts: This is closely related 3.11. SUMMARY 51 to Russell's representation of determinations <ref> [DR87] </ref> and work on representing relevance in AI [SG87]. A determination is a predicate schema which makes a logical statement of relevance between predicates. <p> For example, given nationality (marianne; swiss) and :needs visa (marianne), Equation 3.1 then implies that: nationality (P erson; swiss) ) :needs visa (P erson) (3:2) The formal machinery for this and its use is described in <ref> [DR87, Rus88a, Rus88b] </ref>. We make several points of comparison. First, determinations and skeleton warrants are both representations of relevance.
Reference: [Dud80] <author> Richard O. Duda. </author> <title> The Prospector system for mineral exploration. SRI Project 8172, SRI, </title> <address> CA, </address> <month> April </month> <year> 1980. </year> <note> Final Report. 140 BIBLIOGRAPHY </note>
Reference-contexts: Alternative calculi, eg. Prospector's Bayesian calculus, can present substantial problems for users to understand. Duda, for example, reports: 4.7. REPRESENTING ARGUMENTS 59 "Because the [Bayesian] techniques...are unfamiliar to most geologists, few geologists can even evaluate Prospector's models, let alone contribute to their development" <ref> [Dud80] </ref> p26 Third, the role of strengths and qualifications must be carefully understood. Their primary roles are to organise backings and to permit users to express their opinions adequately. Provided these roles are fulfilled, we do not necessarily require absolute precision in quantifying belief.
Reference: [FHN81] <author> R. Fikes, P. Hart, and N. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <editor> In B. Webber and N. Nilsson, editors, </editor> <booktitle> Readings in AI, </booktitle> <pages> pages 231-249. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1981. </year>
Reference: [Fra86] <author> William Frank. </author> <title> The dipmeter advisor is field tested. </title> <editor> In T. Bernold, editor, </editor> <booktitle> Expert Systems and Knowledge Engineering, </booktitle> <pages> pages 111-120. </pages> <publisher> Elsevier, North-Holland, </publisher> <year> 1986. </year>
Reference: [Gas80] <author> John Gaschnig. </author> <title> Development of uranium exploration models for the Prospector consultant system. SRI Project 7856, </title> <booktitle> SRI International, </booktitle> <year> 1980. </year> <note> (Final report). </note>
Reference: [Gas82] <author> J. Gaschnig. </author> <title> Application of the Prospector system to geological exploration problems. </title> <editor> In J. E. Hayes, D. Michie, and Y-H. Pao, editors, </editor> <booktitle> Machine Intelligence 10, </booktitle> <pages> pages 301-323. </pages> <publisher> Horwood, </publisher> <address> Chichester, </address> <year> 1982. </year>
Reference: [GL88] <author> David V. Gibson and E. Jean Ludl. </author> <title> Group decision support systems and organisa-tional context. </title> <editor> In R. M. Lee, A. M. McCosh, and P. Migliarese, editors, </editor> <booktitle> Organisa-tional Decision Support Systems, </booktitle> <pages> pages 273-285. </pages> <publisher> Elsevier, </publisher> <year> 1988. </year>
Reference: [GN87] <author> Michael R. Genesereth and Nils J. Nilsson. </author> <booktitle> Logical Foundations for Artificial Intelligence. </booktitle> <publisher> Kaufmann, </publisher> <address> CA, </address> <year> 1987. </year>
Reference-contexts: A MODEL OF ARGUMENT STRUCTURE 37 3.3.3 Formal Logic There has also been substantial work on arguments in logic, arising from philosophy (eg. [Rus67]) and in AI (eg. <ref> [GN87] </ref>). Here, the focus has been on systems for establishing the formal validity of a line of reasoning. However, while this work has been valuable for formal analysis of inference, our concern is primarily with argumentation as a social phenomenon.
Reference: [Ham86] <author> P. Hammond. </author> <title> Representation of DHSS regulations as a logic program. </title> <editor> In G. Mi-tra, editor, </editor> <title> Computer-assisted decision-making: expert systems, decision analysis, </title> <booktitle> mathematical programming, </booktitle> <pages> pages 107-116. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference: [Haw83] <author> David Hawkins. </author> <title> An analysis of expert thinking. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 18 </volume> <pages> 1-47, </pages> <year> 1983. </year>
Reference: [Her87] <author> David Bendel Hertz, </author> <title> editor. </title> <journal> Applied AI Reporter, </journal> <volume> volume 4 (4). </volume> <publisher> Univ. </publisher> <address> of Miami, NJ, </address> <month> April </month> <year> 1987. </year>
Reference: [Joh84] <author> Tim Johnson. </author> <title> The commercial application of expert systems technology. </title> <address> Ovum, London, </address> <year> 1984. </year>
Reference: [Kin87] <author> Ross King. </author> <title> An inductive learning approach to the problem of predicting a protein's secondary structure from its amino acid sequence. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning). </booktitle> <address> Sigma, Wilmslow, UK, </address> <year> 1987. </year>
Reference: [KL89] <author> Mark Klein and Stephen C.-Y. Lu. </author> <title> Conflict resolution in co-operative design. </title> <journal> AI in Engineering, </journal> <volume> 4(4) </volume> <pages> 168-180, </pages> <year> 1989. </year>
Reference: [KM90] <author> Michel Klein and Leif B. Methlie. </author> <title> Expert Systems: A Decision Support Approach. </title> <publisher> Addison-Wesley, </publisher> <address> Wokingham, UK, </address> <year> 1990. </year>
Reference: [Kol83] <author> Janet L. Kolodner. </author> <title> Maintaining organization in a dynamic long-term memory. </title> <journal> Cognitive Science, </journal> <volume> 7 </volume> <pages> 281-328, </pages> <year> 1983. </year>
Reference-contexts: The inclusion of this justification knowledge is novel for case-based reasoning. Finally, our model assumes cases will be stored in a simple database; this contrasts with case-based approaches which store cases in a more complex, organised structure (eg. Judge [Bai86], Unimem [Leb86] and Cyrus <ref> [Kol83] </ref>). We thus assume standard database mechanisms will be adequate for search and retrieval. 3.10.4 Conflict Resolution Ideally, an argumentation system should be able to resolve conflicts which it locates in its knowledge. We now summarise approaches to this task and the difficulties encountered with them.
Reference: [Kol88] <author> Janet L. Kolodner. </author> <title> Retrieving events from a case memory: A parallel implementation. </title> <booktitle> In Proc. 1988 Case-Based Reasonong Workshop, </booktitle> <pages> pages 233-249, </pages> <address> CA, 1988. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: Second, our model retrieves cases in order to warn users of possible inconsistencies in their reasoning. A similiar approach was explicitly used in Ladies [DBG91], and is also the implicit goal of many case-based systems solely performing case retrieval (eg. <ref> [Kol88] </ref>). Our argumentation model, however, is distinguished by presenting not only the previous, relevant judgement but also the justification why that judgement was made (the backing).
Reference: [Kon88] <author> Kurt Konolige. </author> <title> Defeasable argumentation in reasoning about events. </title> <editor> In Zbig-niew W. Ras and Lorenza Saitta, editors, </editor> <booktitle> Methods for Intelligent Systems, </booktitle> <volume> 3, </volume> <pages> pages 380-390. </pages> <publisher> Elsevier, </publisher> <address> NY, </address> <year> 1988. </year> <note> BIBLIOGRAPHY 141 </note>
Reference-contexts: A MODEL OF ARGUMENTATION Knowledge-Based Approaches Konolige, in an analysis of conflict resolution, concludes that: "general domain-independent principles [for adjudicating among conflict] will be very weak, and that information from the semantics of the domains will be the most important way of deciding among competing arguments" <ref> [Kon88] </ref> p381 Konolige thus recognises that domain knowledge (eg. that contained in backings) is needed to resolve conflicts. In his system `ArgH' [Kon88], he reflects this by labelling each inference rule as describing either an event or persistence of facts. Conflicts are resolved by event rules dominating persistence rules. <p> [for adjudicating among conflict] will be very weak, and that information from the semantics of the domains will be the most important way of deciding among competing arguments" <ref> [Kon88] </ref> p381 Konolige thus recognises that domain knowledge (eg. that contained in backings) is needed to resolve conflicts. In his system `ArgH' [Kon88], he reflects this by labelling each inference rule as describing either an event or persistence of facts. Conflicts are resolved by event rules dominating persistence rules. This resolution technique is thus a first attempt at formulating general conflict resolution principles using domain-specific information.
Reference: [Lan85] <author> Robert L. Langley. </author> <title> A case study of the dipmeter advisor develpment. </title> <editor> In T. Bernold and G. Albers, editors, </editor> <booktitle> AI: Towards Practical Application, </booktitle> <pages> pages 111-117. </pages> <publisher> Elsevier, North-Holland, </publisher> <year> 1985. </year>
Reference: [Lan86] <author> Norman E. Lane. </author> <title> Global issues in evaluation of expert systems. </title> <booktitle> In Proc. IEEE Conf. on Systems, Man and Cybernetics, </booktitle> <volume> volume 1, </volume> <pages> pages 121-125, </pages> <address> NJ, 1986. </address> <publisher> IEEE. </publisher>
Reference-contexts: User satisfaction has a number of advantages as a metric: perceived benefit is often an indicator of actual benefit, and is also an indicator of the system's suitability for integration into a decision-making context <ref> [Lan86] </ref>. A common method for evaluating user satisfaction is through the use of interviews or questionnaires, eg. employing attitude scales. Despite this, there are also several problems associated with this criterion: 1. User satisfaction is closely related to the users' expectations. <p> Most importantly, it shows that users perceive a benefit in using the system, a strong indicator that the assistance the system provides is of value. Additionally, it indirectly measures acceptance of the system by users, argued for by Lane as essential for achieving practical decision-support <ref> [Lan86] </ref>. However, again we raise some cautionary points: 2.5. EVALUATION ISSUES 33 1. High usage may not necessarily imply success if that usage is mandatory (eg. manual methods removed), or is to use peripheral aspects of the system only (eg. report genera tion). 2. <p> A strength of our application is its simplicity, contributing to its comprehensibility by users. This is a critical concern for meaningful argumentation to occur, and has also been recognised more generally as a fundamental issue in successfully applying AI technology <ref> [Lan86] </ref>. Our evaluation indicated that this level of simplicity was adequate for decision-support in a complex domain, a significant result for the argumentation model. However, in other domains these characteristics may be inappropriate, either due to the domain itself or the way users perform problem-solving.
Reference: [Leb86] <author> M. Lebowitz. </author> <title> Concept learning in a rich input domain : generalization-based memory. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 2. </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1986. </year>
Reference-contexts: The inclusion of this justification knowledge is novel for case-based reasoning. Finally, our model assumes cases will be stored in a simple database; this contrasts with case-based approaches which store cases in a more complex, organised structure (eg. Judge [Bai86], Unimem <ref> [Leb86] </ref> and Cyrus [Kol83]). We thus assume standard database mechanisms will be adequate for search and retrieval. 3.10.4 Conflict Resolution Ideally, an argumentation system should be able to resolve conflicts which it locates in its knowledge. We now summarise approaches to this task and the difficulties encountered with them.
Reference: [LeC85] <author> S. R. LeClair. </author> <title> A Multi-Expert Knowledge System Architecture for Manufacturing Decision Analysis. </title> <type> PhD thesis, </type> <institution> Arizona Univ., </institution> <year> 1985. </year> <note> (described by Alexander and Evans (1988)). </note>
Reference-contexts: This is easily effected by simply labelling represented knowledge with the owner's name (eg. Negoplan [MSK + 89], LeClair's system <ref> [LeC85] </ref> and Young's system [You87]). However, the more difficult task is to compare knowledge from separately encoded knowledge bases, made difficult if differences in language and rule structure exist.
Reference: [Lie89] <author> Jay Liebowitz. </author> <title> Problem selection for expert systems development. </title> <editor> In Jay Liebowitz and Daniel A. DeSalvo, editors, </editor> <booktitle> Structuring Expert systems, </booktitle> <pages> pages 3-23. </pages> <publisher> Prentice-Hall, </publisher> <address> NJ, </address> <year> 1989. </year>
Reference: [LMCP87] <author> J. Lebailly, R. Martin-Clouaire, and A. Prade. </author> <title> Use of fuzzy logic in a rule-based system in petroleum geology. </title> <editor> In E. Sanchez and L. A. Zadah, editors, </editor> <booktitle> Approximate Reasoning In Intelligent Systems, </booktitle> <pages> pages 125-144. </pages> <publisher> Pergamon, </publisher> <year> 1987. </year>
Reference: [Lou89] <author> Ronald Loui. </author> <title> Analogical reasoning, defeasible reasoning and the reference class. </title> <institution> Wucs-89-7, Dept. CS., Univ. of Washington, St. Louis, Missouri, </institution> <year> 1989. </year>
Reference-contexts: However, they would require more knowledge to resolve conflicts in a less arbitrary way. Some authors have looked for domain-independent principles for deciding which statement should dominate given a conflict (eg. <ref> [Lou89, Sim89] </ref>). The only major forthcoming principle was that of specificity: if A 1 ) B, and A 2 ) :B and A 2 is a specialisation of A 1 , then the second rule should dominate. This principle is based on a probabilistic interpretation of the implication relationship.
Reference: [LS83] <author> Curtis P. Langlotz and Edward H. Shortliffe. </author> <title> Adapting a consultation system to critique user plans. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 19(5) </volume> <pages> 479-496, </pages> <year> 1983. </year>
Reference: [Mac89] <author> Alan K. Mackworth. </author> <title> Cooperative systems for perceptual tasks in a remote sensing environment. </title> <type> Tech. Report 89-10, </type> <institution> Dept. CS, Univ. British Columbia, Canada, </institution> <year> 1989. </year>
Reference: [Mar87] <author> Catherine C. Marshall. </author> <title> Exploring representational problems using hypertext. </title> <editor> In John B. Smith and Frank Halasz, editors, </editor> <booktitle> Hypertext '87 Proceedings, </booktitle> <pages> pages 253-268, </pages> <address> NY, </address> <month> November </month> <year> 1987. </year> <note> Assoc. Computing Machinery. </note>
Reference-contexts: Unlike Euclid, it is intended as a general-purpose tool. Marshall has examined its specific application to the representation and organisation of arguments <ref> [Mar87] </ref>. Her representation of argument structure is of relevance to this thesis. The central construct in the NoteCards system is a semantic network consisting of electronic notecards connected by typed links. Each notecard contains information as text, graphics etc.
Reference: [Mas87] <author> R. Mark Maslyn. </author> <title> Gridding advisor: An expert system for selecting gridding algorithms. </title> <booktitle> Geobyte, </booktitle> <pages> pages 42-43, </pages> <month> Nov </month> <year> 1987. </year>
Reference: [MBK87] <author> Charles J. Malmborg, Kenneth W. Brammer, and Bhaskaran Krishnakumar. </author> <title> A decision support system to evaluate maintenance requirements of recoverable populations. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 17(3) </volume> <pages> 465-473, </pages> <year> 1987. </year>
Reference: [MBS88] <author> K. J. MacCallum, C. Brysland, and D. Stevenson. </author> <title> A knowledge-based scheduling assistant for underwater exploration activities. In The Use of Expert Systems in Oil and Gas. </title> <publisher> IBC Ltd., </publisher> <address> UK, </address> <year> 1988. </year>
Reference: [MC80] <author> R. S. Michalski and R. Chilausky. </author> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 125-160, </pages> <year> 1980. </year>
Reference: [McD82] <author> J. McDermott. </author> <title> R1: A rule-based configurer of computer systems. </title> <journal> Artificial Intelligence, </journal> <volume> 19(1) </volume> <pages> 39-88, </pages> <year> 1982. </year>
Reference: [MD85] <author> Sanjay Mittal and Clive L. Dym. </author> <title> Knowledge acquisition from multiple experts. </title> <journal> AI Magazine, </journal> <volume> 6(2) </volume> <pages> 32-36, </pages> <year> 1985. </year> <note> 142 BIBLIOGRAPHY </note>
Reference-contexts: In particular, if the system is to support experts who can already solve the problem themselves, 2.5. EVALUATION ISSUES 31 strong emphasis on the system's own solutions is less appropriate (Mittal and Dym describe encountering this problem when installing the MDX system <ref> [MD85] </ref>). Many expert systems have been shown to be accurate but failed to show evidence of usefully aiding experts. As Sutton notes, the computer's accuracy may matter less than its credibility [Sut89]. 2.
Reference: [Mil84] <author> Perry L. Miller. </author> <title> A Critiquing Approach to Expert Computer Advice: </title> <publisher> ATTENDING. Pitman, </publisher> <address> Boston, </address> <year> 1984. </year>
Reference: [MKKC86] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning Journal, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Many warrants could be generated from a precedent example, and thus extra domain knowledge is needed to identify those where the example offers more support than simply statistics over a sample size of one. Some work in `explanation-based generalisation' in machine learning can be regarded as performing this task <ref> [MKKC86] </ref>. Alternatively, given an existing warrant, the location of precedents can play an important role in increasing support for it (eg. in law, where precedents strongly determine the interpretation of terminology). 3.6.
Reference: [MMS90] <author> Tom M. Mitchell, Sridbar Mahadevan, and Louis I. Steinberg. </author> <title> LEAP: A learning apprentice for VLSI design. </title> <editor> In Yves Kodratoff and Ryszard Michalski, editors, </editor> <booktitle> Machine Learning: An AI Approach (Volume 3), </booktitle> <pages> pages 271-289, </pages> <address> Ca, 1990. </address> <publisher> Kaufmann. </publisher>
Reference: [MPM82] <author> A. M. Miller, H. E. Pople, and J. D. Myers. Internist-I, </author> <title> an experimental computer-based diagnostic consultant for general internal medicine. </title> <journal> New England Journal of Medicine, </journal> <volume> 307 </volume> <pages> 468-476, </pages> <year> 1982. </year> <title> (as described in Expert Systems, </title> <editor> Eds: J.Alty and M.Coombs, Manchester:NCC, </editor> <year> 1984). </year>
Reference: [MS82] <author> L. Thorne McCarty and N. S. Sridharan. </author> <title> A computational theory of legal argument. </title> <type> Technical Report LRP-TR-13, </type> <institution> Rutgers Univ., Lab. for C.S. Research, NJ, </institution> <year> 1982. </year>
Reference: [MSK + 89] <author> Stan Matwin, Stan Szpakowicz, Zbig Koperczak, Gregory E. Kersten, and Wojtek Michalowski. Negoplan: </author> <title> An expert system shell for negotiation support. </title> <journal> IEEE Expert, </journal> <volume> 4(4) </volume> <pages> 50-62, </pages> <year> 1989. </year>
Reference-contexts: This is easily effected by simply labelling represented knowledge with the owner's name (eg. Negoplan <ref> [MSK + 89] </ref>, LeClair's system [LeC85] and Young's system [You87]). However, the more difficult task is to compare knowledge from separately encoded knowledge bases, made difficult if differences in language and rule structure exist.
Reference: [Nib91] <author> Tim Niblett. </author> <title> Cooperative expert systems. </title> <booktitle> In Proc. Technical Workshop on Future Customer Facing Systems, </booktitle> <address> Ipswich, </address> <month> March </month> <year> 1991. </year> <note> BT Research. </note>
Reference-contexts: A MODEL OF ARGUMENTATION 3.4.3 Notation Toulmin's Structure We introduce a notation for Toulmin's argument structure, following suggestions by Niblett <ref> [Nib91] </ref>. Warrants express a relationship between grounds and a claim, which we denote: g 1 ^ . . . ^ g n ! s C where g i are the grounds, C is the claim and s is the strength of the warrant.
Reference: [Niw86] <author> Kiyoshi Niwa. </author> <title> A knowledge-based human-computer cooperative system for ill-structured management domains. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 16(3) </volume> <pages> 335-342, </pages> <year> 1986. </year>
Reference: [Niw89] <author> Kiyoshi Niwa. </author> <title> Knowledge-Based Risk Management in Engineering. </title> <publisher> Wiley, </publisher> <address> NY, </address> <year> 1989. </year>
Reference: [NNSN86] <author> Y. Nishikawa, J. Nomura, K. Sawada, and R. Nakejima. </author> <title> Design of a hierarchical multiobjective decision-support system for inventory planning and control. </title> <editor> In G. Mancini, G. Johannsen, and L. Martensson, editors, </editor> <booktitle> Analysis, Design and Eva-lution of Man-Machine Systems (Proc. 2nd IFAC/IFIP/IFORS/IEA Conf. </booktitle> <year> 1985), </year> <pages> pages 135-140, </pages> <address> Oxford, UK, 1986. </address> <publisher> Pergamon. </publisher>
Reference: [O'K89] <author> Robert M. O'Keefe. </author> <title> The evaluation of decision-aiding systems: Guidelines and methods. </title> <journal> Information and Management, </journal> <volume> 17(4) </volume> <pages> 217-226, </pages> <year> 1989. </year>
Reference-contexts: In many domains the correctness of the system's (or user's) answer is difficult or impossible to assess. We discuss this further in the context of user effectiveness below. 2.5.3 User Effectiveness `Improved user effectiveness' has often been suggested as an overall goal for decision-aiding systems <ref> [O'K89] </ref>. However, `effectiveness' itself cannot be measured directly; instead a more specific definition of the term is needed, suitable for the application domain of interest. Possible indirect measures of effectiveness include users' speed, users' accuracy, and company profitability. <p> In the context of evaluating decision-aiding systems, analysis of the value of information which the system provides has sometimes be used for evaluation <ref> [O'K89] </ref>. Again, however, indicators of this are often indirect and can be difficult to apply. Encoding Subjective Probabilities One class of analysis concerns the evaluation of encoded subjective probability estimates. <p> For example, large control group experiments are infeasible, due to the limited availability of sufficient numbers of trained experts. These pragmatic constraints are not uncommon in system evaluation, and must be taken into consideration during evaluation design <ref> [BWB87, O'K89] </ref>. 5.3 Evaluation Criteria Given this evaluation context, we now set out the claims which we wish to establish and the assessment methods which we apply. Our thesis goal is to develop a practical, computational model of argumentation which can usefully support users in decision-making.
Reference: [Pea88] <author> D. A. Pearce. </author> <title> The induction of fault diagnosis systems from qualitative models. </title> <booktitle> In AAAI-88, </booktitle> <volume> volume 1, </volume> <pages> pages 353-357, </pages> <address> CA, 1988. </address> <publisher> Kaufmann. </publisher>
Reference: [Pre89a] <author> G. Premkumar. </author> <title> A cognitive study of the decision-making process in a business context: Implications for design of expert systems. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31 </volume> <pages> 557-572, </pages> <year> 1989. </year>
Reference: [Pre89b] <author> David S. Prerau. </author> <title> Choosing an expert system domain. </title> <editor> In G. Guida and C. Tasso, editors, </editor> <booktitle> Topics in Expert System Design, </booktitle> <pages> pages 27-43. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference: [Qui83] <author> J. Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess endgames. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 1. </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1983. </year> <note> BIBLIOGRAPHY 143 </note>
Reference-contexts: Given a set of examples, warrants could be automatically generated from the data, backed as being from statistical evidence in that data set. This is in fact the task that inductive rule learning systems perform <ref> [CN89, Qui83] </ref>. Precedent: Warrants can obtain some support even from a single example of their use, especially when that example is recognised as representative in some way of a class of examples. <p> In this first application, the task is to advise users, interested in applying machine learning techniques to solve some problem, as to whether the particular machine learning tool ID/CN (implementing algorithms ID3 <ref> [Qui83] </ref> and CN2 [CN89, CB91]) is appropriate or not.
Reference: [RE91] <author> Gail L. Rein and Clarence A. Ellis. rIBIS: </author> <title> A real-time group hypertext system. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 34(3) </volume> <pages> 349-367, </pages> <month> Mar </month> <year> 1991. </year>
Reference-contexts: Introduction gIBIS (graphical Issue-Based Information System) [CB87, BC88] and rIBIS (real-time IBIS) <ref> [RE91] </ref> are hypertext systems designed to support design deliberations among cooperating experts. As mentioned earlier, these systems are hypertext systems for group decision support, and thus span these two categories in this review. <p> It is claimed to have achieved `wider and more prolonged usage' than other hypertext systems in the authors' environment. rIBIS is described as being used in 16 design sessions (the majority of which concerned improving the design of rIBIS itself) <ref> [RE91] </ref>. Users reactions to rIBIS were described as mixed, ranging from frustating and unproductive to satisfying and productive. Discussion Although gIBIS is a system for supporting group deliberation an activity of argumentation - the computer does not play a participatory role in this process.
Reference: [Reb81] <author> Rene Reboh. </author> <title> Knowledge engineering techniques and tools in the Prospector environment. </title> <type> Technical Note 243, </type> <institution> SRI International, </institution> <month> June </month> <year> 1981. </year> <note> (SRI Projects 5821, 6415 and 8172). </note>
Reference-contexts: Given a description of this kind, the system could then seek to assign strengths to warrants in a way consistent with the expressed relationships. Similar techniques were used during the construction of Prospector's models: the tool KAS <ref> [Reb81] </ref> was used to find appropriate assignments of LS/LN factors in Prospector's models such that correct results on a set of training examples were obtained. 132 CHAPTER 6.
Reference: [Reb83] <author> Rene Reboh. </author> <title> Extracting useful advice from conflicting expertise. </title> <journal> In IJCAI-83, </journal> <volume> volume 1, </volume> <pages> pages 145-150, </pages> <year> 1983. </year>
Reference: [RFTT85] <author> C. A. Reid, R. M. Fung, R. M. Tong, and E. Tse. </author> <title> A knowledge representation for reasoning about petroleum geology. </title> <booktitle> In Procs. Second Conference on Artificial Intelligence Applications, </booktitle> <pages> pages 125-129, </pages> <year> 1985. </year>
Reference: [Ris85] <author> Edwina L. </author> <title> Rissland. </title> <booktitle> Artificial intelligence and legal reasoning. In IJCAI-85, </booktitle> <pages> pages 1254-1260, </pages> <year> 1985. </year> <title> (Summary of panel discussion). </title>
Reference-contexts: We illustrate this schematically as shown in Figure 3.3. There are additional research issues which arise here of how to translate vocabulary, including words whose meanings are not precisely defined (`open textured' <ref> [Ris85] </ref>). As we require this translation to occur outside our argumentation framework, we do not pursue these issues, but note they can be significant in argumentation (eg. in the field of law).
Reference: [Ris89] <author> Edwina Rissland, </author> <title> editor. AI and Law: </title> <booktitle> Proc. Second International Conference, </booktitle> <address> NY, 1989. </address> <publisher> ACM. </publisher>
Reference: [Ros85] <author> J. Rosenschein. </author> <title> Rational interaction: Cooperation among intelligent agents. </title> <type> Tech. Report STAN-CS-85-1081, </type> <institution> Stanford Univ., </institution> <year> 1985. </year> <type> (also PhD thesis). </type>
Reference-contexts: Consensus Approaches One class of approach to conflict resolution is to combine conflicting evidence or conclusions together using analytic methods. For example, Bayesian techniques combine evidence relating to a conclusion together [Che88], and game-theoretic methods select the best course of action from alternatives given quantified reliabilities, costs and payoffs <ref> [Ros85, vM44] </ref>. This approach can be taken in domains where probabilistic or decision-theoretic models can be precisely specified. However, in many domains this is not possible, and thus these methods are of limited utility with our model.
Reference: [RRG82] <author> R. Reboh, J. Reiter, and J. Gaschnig. </author> <title> Development of a knowledge-based interface to a hydrological simulation program. SRI Project 3477, </title> <booktitle> SRI International, </booktitle> <month> May </month> <year> 1982. </year> <note> (Final report). </note>
Reference: [Rud90] <author> Jim Rudolf. </author> <title> Interface development without tears. </title> <journal> Knowledge-Based Systems Mangement Reviews, </journal> <volume> 2(2) </volume> <pages> 14-15, </pages> <year> 1990. </year>
Reference-contexts: THE MODEL APPLIED TO OIL PROSPECT ASSESSMENT 4.10.3 Implementation Statistics Optimist is implemented in approximately 15000 lines of Quintus Prolog, running on a Sun workstation. It uses HyperNeWS <ref> [Rud90] </ref> for its user interface, and the Oracle database for mass data storage. A summary of the percentage of code in different modules of Optimist is shown in Table 4.8. We summarise the most interesting features: 1.
Reference: [Rus67] <author> B. Russell. </author> <title> The Problems of Philosophy. </title> <publisher> Oxford Univ. Press, </publisher> <address> UK, </address> <year> 1967. </year>
Reference-contexts: None provides a representation which meets all our requirements above, and thus we still seek a suitable framework in which these contributions can be integrated. 3.4. A MODEL OF ARGUMENT STRUCTURE 37 3.3.3 Formal Logic There has also been substantial work on arguments in logic, arising from philosophy (eg. <ref> [Rus67] </ref>) and in AI (eg. [GN87]). Here, the focus has been on systems for establishing the formal validity of a line of reasoning. However, while this work has been valuable for formal analysis of inference, our concern is primarily with argumentation as a social phenomenon.
Reference: [Rus88a] <author> Stuart J. Russell. </author> <title> A sketch of automous learning using declarative bias. </title> <editor> In Pavel Brazdil, editor, </editor> <booktitle> International workshop on Machine Learning, Meta-reasoning and Logics, </booktitle> <pages> pages 147-166, </pages> <address> Portugal, </address> <year> 1988. </year> <institution> Faculdade de Economia, Univ. Porto. </institution> <note> (Proceedings to be published in book form in 1989). </note>
Reference-contexts: For example, given nationality (marianne; swiss) and :needs visa (marianne), Equation 3.1 then implies that: nationality (P erson; swiss) ) :needs visa (P erson) (3:2) The formal machinery for this and its use is described in <ref> [DR87, Rus88a, Rus88b] </ref>. We make several points of comparison. First, determinations and skeleton warrants are both representations of relevance.
Reference: [Rus88b] <author> Stuart J. Russell. </author> <title> Tree-structured bias. </title> <booktitle> In AAAI-88, </booktitle> <volume> volume 2, </volume> <pages> pages 641-645, </pages> <address> CA, 1988. </address> <publisher> Kaufman. </publisher>
Reference-contexts: For example, given nationality (marianne; swiss) and :needs visa (marianne), Equation 3.1 then implies that: nationality (P erson; swiss) ) :needs visa (P erson) (3:2) The formal machinery for this and its use is described in <ref> [DR87, Rus88a, Rus88b] </ref>. We make several points of comparison. First, determinations and skeleton warrants are both representations of relevance.
Reference: [RVA84] <author> Edwina L. Rissland, Eduardo M. Valcarce, and Kevin D. Ashley. </author> <title> Explaining and arguing with examples. </title> <booktitle> In AAAI-84, </booktitle> <pages> pages 288-294, </pages> <year> 1984. </year>
Reference-contexts: As we require this translation to occur outside our argumentation framework, we do not pursue these issues, but note they can be significant in argumentation (eg. in the field of law). These issues are discussed further in <ref> [RVA84, vG83] </ref>. 3.5 Formalising Argument Content 3.5.1 Introduction Section 3.4 described the structure of arguments, ie. the domain-independent elements of an argument and their relationship. This section examines the representation of the content of 42 CHAPTER 3. A MODEL OF ARGUMENTATION the argument elements. <p> Aha describes case-based reasoning systems as comprising (i) a similarity assessment component, (ii) a performance component and (iii) a learning component [AKA91]. For similarity assessment, there are several case-based reasoning systems which also use domain knowledge to identify important features (eg. Hypo <ref> [RVA84] </ref>, Protos [BPM89] and by Cain et al., [CPS91]). 3.10. RELATED WORK 49 Our model is thus an example within this group. Second, our model retrieves cases in order to warn users of possible inconsistencies in their reasoning.
Reference: [Saf87] <author> Alessandro Saffiotti. </author> <title> An AI view of the treatment of uncertainty. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 2(2) </volume> <pages> 75-97, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Thus in domains where opinions differ, it may often also be the case that an objective statistical approach has limited value. Qualitative Representations of Strength It is well known that users can have difficulty in precisely quantifying degrees of belief (eg. <ref> [Saf87, DKSZ79] </ref>). Although our model will tolerate a degree of imprecision, an alternative would be to ask users to specify belief in a qualitative manner (eg. "this warrant is very strong"). We illustrated a simple use of qualitative measures of warrant strength in the ML Advisor application (Section 6.5).
Reference: [SB83] <author> Reid. G. Smith and James D. Baker. </author> <title> The dipmeter advisor system: A case study in commercial expert system development. </title> <journal> In IJCAI-83, </journal> <volume> volume 1, </volume> <pages> pages 122-129, </pages> <year> 1983. </year>
Reference-contexts: Furthermore, the low-level graphical commands controlling the appearance and position of items in the interface are coded in a further 6500 lines of PostScript code and are not included in Table 4.8. Other authors have reported similar high figures for user-interface components, eg. 42% for Dipmeter <ref> [SB83] </ref> and 33%-50% for Pride [BMS88]. 3. The argument construction algorithm is small and simple, accounting directly for about 3% of the code (it indirectly uses other code, eg. interfaces to models and data).
Reference: [SBF + 87] <author> Paul Smolensky, Brigham Bell, Barbara Fox, Roger King, and Clayton Lewis. </author> <title> Constraint-based hypertext for argumentation. </title> <editor> In John B. Smith and Frank Ha-lasz, editors, </editor> <booktitle> Hypertext '87 Proceedings, </booktitle> <pages> pages 215-245, </pages> <address> NY, </address> <month> November </month> <year> 1987. </year> <note> Assoc. Computing Machinery. </note>
Reference: [SBM88] <author> Ramesh Sharda, Steve H. Barr, and James C. </author> <title> McDonnell. Decision support system effectiveness: A review and an empirical test. </title> <journal> Management Science, </journal> <volume> 34(2) </volume> <pages> 139-159, </pages> <month> Feb </month> <year> 1988. </year> <note> 144 BIBLIOGRAPHY </note>
Reference-contexts: In Sharda et als' review of twelve laboratory studies of DSS effectiveness, the majority of systems were evaluated after only one or two sessions with the system <ref> [SBM88] </ref>. 3. Users' behaviour in the laboratory setting may be unreflective of normal behaviour within real-world decision-making contexts. Thus laboratory trials are only indirect indicators of the system's utility within those contexts. 2.5.4 User Satisfaction A third criterion for measuring success of a decision-aiding system is user satisfaction.
Reference: [SCM + 91] <author> Ashwin Srinivasan, Paul Compton, Ron Malor, Glenn Edwards, and Leslie Lazarus. </author> <title> Knowledge acquisition in context for a complex domain. </title> <booktitle> In Proc. </booktitle> <address> EKAW-91, </address> <year> 1991. </year> <note> (in press). </note>
Reference-contexts: This, of course does not detract from Protos's value as a knowledge acquisition tool; however it does make its knowledge acquisition method inappropriate to meet our goal of usability by unsupervised, non-computer-skilled users. Srinivasan et al's ripple-down rules methodology <ref> [SCM + 91] </ref> is an alternative whereby users select corrections to a rule from a list of possible corrections. However, this requires an adequate space of possible rules to be identified, thus constraining the corrections the users can express. <p> However, 6.7. FUTURE WORK 131 we should also point out that there are still several limitations with this methodology which need to be overcome, for example an adequate vocabulary must be provided to the system for ripple-down rules to work. The reader is referred to <ref> [SCM + 91] </ref> for a more detailed discussion of this technique. 6.7.3 Development of Models of Opinion Models of opinions are approximate expressions of users' subjective judgements about a domain. There are several ways in which their representation, use, and validation can be further enhanced which we now discuss.
Reference: [SG87] <author> D. Subramanian and M. R. Genesereth. </author> <title> The relevance of irrelevance. </title> <journal> In IJCAI-87, </journal> <volume> volume 1, </volume> <pages> pages 416-422, </pages> <year> 1987. </year>
Reference-contexts: This is closely related 3.11. SUMMARY 51 to Russell's representation of determinations [DR87] and work on representing relevance in AI <ref> [SG87] </ref>. A determination is a predicate schema which makes a logical statement of relevance between predicates.
Reference: [Sha91] <author> Nigel Shadbolt, </author> <year> 1991. </year> <type> (Personal communication). </type>
Reference-contexts: This simplicity distinguishes it from other knowledge acquisition systems, described earlier in Section 1.6.3. With Protos [BPM89], for example, the user expresses his or her opinions in a formal, Lisp-like syntax, reported to require either programming skills or assistance for its use <ref> [Sha91] </ref>. This, of course does not detract from Protos's value as a knowledge acquisition tool; however it does make its knowledge acquisition method inappropriate to meet our goal of usability by unsupervised, non-computer-skilled users.
Reference: [She87] <institution> Shell Labs. Shell takes wraps off its knowledge-based systems. AI Business, 9:2, </institution> <month> Jan </month> <year> 1987. </year>
Reference: [Sho76] <author> E. H. Shortliffe. </author> <title> Computer-Based Medical Consultations: MYCIN. </title> <publisher> American Else-vier, </publisher> <address> NY, </address> <year> 1976. </year>
Reference-contexts: The review in Chapter 2 and other literature suggest several alternative calculi which could be used, for example: * Modified Bayesian inference (eg. Prospector and Explorationist, in Section 2.2). * fuzzy logic (eg. SPII-2, in Section 2.2). * Other expert system calculi eg. Mycin's certainty factors <ref> [Sho76] </ref>. * A modified version of our calculus where risk is assessed in relation to a `typical' rather than `ideal' prospect ie. the initial probability of oil is taken as a prior probability (eg. q o = 0:2) rather than q o = 1.0 (Section 4.6.2). <p> We take medical diagnosis as an example domain, considering replacing a diagnostic expert system such as Mycin (say) <ref> [Sho76] </ref> with an argumentative version (`A-Mycin', say). For conciseness we do not sketch a full implementation but merely highlight the important points of comparison. The process by which an expert system produces diagnostic advice is sketched in Figure 6.3.
Reference: [Sim89] <author> Guillermo R. Simari. </author> <title> A justification finder. </title> <type> Technical Report WUCS-89-24, </type> <institution> Wash-ington Univ. in St. Louis, MO, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: However, they would require more knowledge to resolve conflicts in a less arbitrary way. Some authors have looked for domain-independent principles for deciding which statement should dominate given a conflict (eg. <ref> [Lou89, Sim89] </ref>). The only major forthcoming principle was that of specificity: if A 1 ) B, and A 2 ) :B and A 2 is a specialisation of A 1 , then the second rule should dominate. This principle is based on a probabilistic interpretation of the implication relationship.
Reference: [SM89] <author> Reid Simmons and John Mohammed. </author> <title> Causal modelling of semiconductor fabrication. </title> <journal> AI in Engineering, </journal> <volume> 4(1) </volume> <pages> 2-21, </pages> <year> 1989. </year>
Reference: [SMZ86] <author> S. Slocombe, K. Moore, and M. Zelouf. </author> <title> Engineering expert system applications. </title> <booktitle> In Annual Conference of the BCS Specialist Group on Expert Systems, </booktitle> <month> December </month> <year> 1986. </year>
Reference: [SSK + 86] <author> M. J. Sergot, F. Sadri, R. A. Kowalski, F. Kriwaczek, P. Hammond, and H. T. Cory. </author> <title> The British nationality act as a logic program. </title> <journal> Communications of the ACM, </journal> <volume> 29(5) </volume> <pages> 370-386, </pages> <year> 1986. </year>
Reference: [Stu87] <author> Arthur Stutt. </author> <title> Second generation expert systems, explanations, arguments and archaeology. </title> <type> Tech. Report 25, </type> <institution> Human Cognition Research Lab., Milton Keynes, UK, </institution> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: A backing could itself be a complete argument, claiming that the warrant is valid. This would provide a hierarchical structure of arguments, allowing a more sustained dialogue to occur as advocated by Stutt <ref> [Stu87] </ref> (eg. the backings themselves could be argued about). As discussed in Section 1.5.2, the issue here is not whether the improved functionality would be desirable, but of how it can be constrained so that practicality can be maintained.
Reference: [Sut89] <author> G. C. Sutton. </author> <title> Computer-aided diagnosis: A review. </title> <journal> British Journal of Surgery, </journal> <volume> 76, </volume> <year> 1989. </year>
Reference-contexts: Many expert systems have been shown to be accurate but failed to show evidence of usefully aiding experts. As Sutton notes, the computer's accuracy may matter less than its credibility <ref> [Sut89] </ref>. 2. This metric can only be applied to systems which offer decision advice directly (as opposed to performing data analysis subtasks, for example). 3. In many domains the correctness of the system's (or user's) answer is difficult or impossible to assess. <p> In his study of applied medical expert systems, for example, Sutton argues that a main cause of improved diagnostic accuracy was simply that users had been forced to use more structured data collection methods <ref> [Sut89] </ref>, and that the diagnostic systems were themselves responsible for only a small part of the improvement. 3. Often metrics of effectiveness are long-term (eg. company profit), which may be impractical to measure or simply not available within the time-frame of the system's use.
Reference: [Syk76] <author> J. B. Sykes, </author> <title> editor. The Concise Oxford Dictionary. </title> <publisher> Clarendon, Oxford, </publisher> <year> 1976. </year>
Reference: [Tat77] <author> Austin Tate. </author> <title> Generating project networks. </title> <booktitle> In IJCAI-77, </booktitle> <pages> pages 888-893, </pages> <year> 1977. </year>
Reference: [TF90] <author> Donna M. Thompson and Jerald L. Feinstein. </author> <title> Cost justifying expert systems. </title> <booktitle> In Managing Artificial Intelligence and Expert Systems, </booktitle> <pages> pages 93-121. </pages> <address> Yourdon/Prentice-Hall, NJ, </address> <year> 1990. </year>
Reference-contexts: Often metrics of effectiveness are long-term (eg. company profit), which may be impractical to measure or simply not available within the time-frame of the system's use. For some applications a formal cost-benefit analysis may be possible to quantify financial gain from an application <ref> [TF90] </ref>. These methods serve to translate measures of effectiveness into financial terms, useful for a company's financial planning, but leave open the question of how the changed users' performance can be assessed in the first place. We thus point to these 32 CHAPTER 2.
Reference: [Tou58] <author> Stephen Edelston Toulmin. </author> <title> The Uses of Argument. </title> <publisher> University Press, </publisher> <address> Cambridge, UK, </address> <year> 1958. </year>
Reference-contexts: However, while this work has been valuable for formal analysis of inference, our concern is primarily with argumentation as a social phenomenon. In the social context, reasoning can be inconsistent, imprecise, and ambiguous; the concept of a formally valid argument or belief rarely has a role <ref> [Tou58] </ref>. This makes formal logic an inappropriate model to meet our requirements. 3.3.4 Social Argumentation Models There have been very few models of social argumentation proposed in the literature. We reviewed several applied in the context of hypertext systems in Section 2.4. <p> For the systems Euclid, gIBIS and rIBIS, arguments were represented at a coarse level of detail, allowing the relation between arguments to be represented but providing no internal structure for them. Aside from these approaches, Toulmin's model <ref> [Tou58] </ref> is the only other which has been proposed that also provides a detailed structure of the elements within primitive arguments. We detail this shortly. <p> We now describe Toulmin's structure of arguments, and the elements within it which he identified as independent of the field of discourse (`field-invariant' p15 <ref> [Tou58] </ref>). Following this, we show how we have developed it to construct our own model of arguments. We discuss the representation of elements in this structure and how it can be employed for practical argumentation. 38 CHAPTER 3. <p> Toulmin summarises this: "Once we bring into the open the backing on which (in the last resort) the soundness of our arguments depends, the suggestion that validity is to be explained in terms of `formal properties', in any geometrical sense, loses its plausibility." p120 <ref> [Tou58] </ref> Whereas grounds and warrants can be viewed as a particular model of belief with which we can perform mechanised inference, backings can be viewed as representing evidence of why we should believe that model is correct in the first place.
Reference: [TRJ79] <author> Stephen Toulmin, Richard Rieke, and Allan Janik. </author> <title> An Introduction to Reasoning. </title> <publisher> Macmillan, </publisher> <address> NY, </address> <year> 1979. </year>
Reference-contexts: Finally, we briefly overview the relation of additional work from the literature to our model. 3.2 Basic Definitions Some basic definitions which we use, partly following Toulmin <ref> [TRJ79] </ref>, are as follows: An argument, in the sense of a train of reasoning, is the sequence of interlinked claims and reasons that, between them, establish the content and force of the position for which a particular speaker is arguing. Inference is the process by which arguments are constructed.
Reference: [vG83] <author> Anne v.d.L. Gardner. </author> <title> The design of a legal analysis program. </title> <booktitle> In AAAI-83, </booktitle> <pages> pages 114-118, </pages> <year> 1983. </year>
Reference-contexts: As we require this translation to occur outside our argumentation framework, we do not pursue these issues, but note they can be significant in argumentation (eg. in the field of law). These issues are discussed further in <ref> [RVA84, vG83] </ref>. 3.5 Formalising Argument Content 3.5.1 Introduction Section 3.4 described the structure of arguments, ie. the domain-independent elements of an argument and their relationship. This section examines the representation of the content of 42 CHAPTER 3. A MODEL OF ARGUMENTATION the argument elements.
Reference: [vH86] <author> P. van der Pas and L. J. B. Hoffman. </author> <booktitle> Knowledge engineering research in petrophysics in shell. In Procs. Second International Expert System Conference, </booktitle> <pages> pages 205-209, </pages> <year> 1986. </year>
Reference: [Vid91] <author> Paul T. Vidler, </author> <title> editor. Expert Systems: Can They Aid Geological Interpretation And Evaluation?, </title> <type> Surrey, </type> <month> Feb </month> <year> 1991. </year> <title> (Abstracts only). BIBLIOGRAPHY 145 </title>
Reference: [vM44] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behaviour. </title> <publisher> Princeton Univ., </publisher> <address> NJ, </address> <year> 1944. </year> <editor> (described in Matwin et al. </editor> <year> (1989)). </year>
Reference-contexts: Consensus Approaches One class of approach to conflict resolution is to combine conflicting evidence or conclusions together using analytic methods. For example, Bayesian techniques combine evidence relating to a conclusion together [Che88], and game-theoretic methods select the best course of action from alternatives given quantified reliabilities, costs and payoffs <ref> [Ros85, vM44] </ref>. This approach can be taken in domains where probabilistic or decision-theoretic models can be precisely specified. However, in many domains this is not possible, and thus these methods are of limited utility with our model.
Reference: [VNGD88] <author> Douglas R. Vogel, J. F. Nunamaker, Joey F. George, and Alan R. Dennis. </author> <title> Group decision support systems: Evolution and status at the university of arizona. </title> <editor> In R. M. Lee, A. M. McCosh, and P. Migliarese, editors, </editor> <booktitle> Organisational Decision Support Systems, </booktitle> <pages> pages 287-304. </pages> <publisher> Elsevier, </publisher> <year> 1988. </year>
Reference: [Wal87] <author> Michael G. Walker. </author> <booktitle> How feasible is automated discovery? IEEE Expert, </booktitle> <pages> pages 69-82, </pages> <month> Spring </month> <year> 1987. </year>
Reference: [WB83] <author> Thomas S. Wallsten and David V. Budescu. </author> <title> Encoding subjective probabilities: </title> <journal> A psychological and psychometric review. Management Science, </journal> <volume> 29(2) </volume> <pages> 151-173, </pages> <month> Feb </month> <year> 1983. </year>
Reference-contexts: Encoding Subjective Probabilities One class of analysis concerns the evaluation of encoded subjective probability estimates. We single this out for further discussion as our application task is of oil probability estimation, hence these forms of analysis are directly relevant. In a review of subjective probability encoding <ref> [WB83] </ref>, Wallsten and Budescu identify two aspects of probability estimation which can be assessed: reliability (absence of random error) and validity (its accuracy as a representation of opinion). In their detailed review [WB83], they identify four types of analysis which can be performed: reliability (measures of random variance by comparing probability <p> In a review of subjective probability encoding <ref> [WB83] </ref>, Wallsten and Budescu identify two aspects of probability estimation which can be assessed: reliability (absence of random error) and validity (its accuracy as a representation of opinion). In their detailed review [WB83], they identify four types of analysis which can be performed: reliability (measures of random variance by comparing probability estimates at different times), internal consistency (observation of whether expressed probabilities obey the laws of probability), calibration (correlation of estimates with independently obtainable measures), and inter-response correlation (comparison of estimates expressed in <p> By variability, we refer to fluctuations in the way the `same' risk judgement is expressed in appraisals (also referred to as `reliability' elsewhere in the literature <ref> [WB83] </ref>). Variability reduces the value of recorded appraisals as meaningful representations of decision-making. In the extreme case where the expressed judgements are completely random, their value as information to the user is lost. Variability is particularly difficult to evaluate because of the problem of identifying `similar' risk judgements. <p> Another area for future work would be to investigate whether, with time, users' were able to improve their own precision of strength estimates with time. There is evidence that feedback can help in this <ref> [WB83] </ref>, and the assessment of this in the context of feedback from argumentation would be a valuable test of the approach.
Reference: [WG91] <author> Dave Wolstenholme and Nigel Goodwin. </author> <title> Identification of geological environment taking an explanation-intensive and meta-level approach. </title> <type> Technical report, </type> <institution> BP Research, Sunbury-on-Thames, UK, </institution> <year> 1991. </year>
Reference: [Whi88] <author> Ian White. </author> <title> W(h)ither expert systems? a view from outside. </title> <journal> AI and Society, </journal> <volume> 2(2) </volume> <pages> 161-171, </pages> <year> 1988. </year>
Reference: [Win86] <author> P. H. Winston. </author> <title> Learning by augmenting rules and accumulating censors. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 2, </volume> <pages> pages 45-61. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1986. </year>
Reference-contexts: Response to such a challenge is not normally possible by most rule-based systems, and thus the representation and use of backings is an important distinguishing feature of the model with which we are working. Rebuttals are also normally omitted in rule-based systems (but see <ref> [Win86] </ref> for an example of their use in machine learning). Instead, statements of rebuttal can be included in the conditions of rules (eg. a rebuttal R of the rule if A then B can be accounted for by re-expressing the rule if A and not R then B).
Reference: [WS89] <author> Michael R. Wick and James R. Slagle. </author> <title> The partitioned support network for expert system justification. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 19(3) </volume> <pages> 528-535, </pages> <month> May/June </month> <year> 1989. </year>
Reference-contexts: The grounds are the foundation of the argument. Warrant: A statement of a general relationship between the grounds and the claim. Backing: The knowledge that supports the warrant. The backing is sometimes referred to as the justification for a rule in the AI literature (eg. <ref> [WS89] </ref>), and describes why the warrant itself should be believed. (This is distinct from a typical expert system-like explanation of how the grounds of a warrant were satisfied). Rebuttal: Exceptions that invalidate the claim. The rebuttal can attack any part of an argu ment (except itself).
Reference: [Yan86] <author> J. Yan. </author> <title> Identifying depositional environment structure: an expert system approach using object-oriented programming and model-driven verification. </title> <booktitle> In Procs. Sixth International Workshop on Expert Systems and their Applications, </booktitle> <pages> pages 1081-1092, </pages> <year> 1986. </year>
Reference: [YFB + 84] <author> Victor L. Yu, Lawrence M. Fagan, Sharon Wraith Bennett, William J. Clancey, A. Carlisle Scott, John F. Hannigan, Bruce G. Buchanan, and Stanley N. Cohen. </author> <title> An evaluation of mycin's advice. </title> <editor> In Bruce G. Buchanan and Edward H. Shortliffe, editors, </editor> <booktitle> Rule-Based Expert Systems, </booktitle> <pages> pages 589-596. </pages> <publisher> Addison-Wesley, </publisher> <year> 1984. </year> <title> (This is an edited version of the article appearing in the Journal of the American Medical Association 242: </title> <type> 1279-1282, </type> <year> (1979)). </year>
Reference-contexts: It is also interesting to note that during the evaluation of Mycin, significant differences of opinion among therapists were encountered; for example, on average only 42% of therapies recommended by 9 experts were rated as acceptable by 8 other (independent) experts <ref> [YFB + 84] </ref>. Although Mycin (as a 10th recommender) fared well (in fact best, scoring 52%), this still low score highlights the significance of differences of opinion in this domain.
Reference: [YL86] <author> Yao Yuchuan and Gong Leiguang. </author> <title> Abstraction and interpretation of knowledge for complex tasks. </title> <editor> In John F. Gilmore, editor, </editor> <booktitle> Applications of Artificial Intelligence IV, </booktitle> <volume> volume 657, </volume> <pages> pages 54-57. SPIE, </pages> <year> 1986. </year>
Reference: [You87] <author> Mark Anthony Young. </author> <title> The design and implementation of an evidence oracle for the understanding of arguments. </title> <institution> Research Report CS-87-33, Dept. C.S., Univ. Waterloo, </institution> <address> Ontario, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: This is easily effected by simply labelling represented knowledge with the owner's name (eg. Negoplan [MSK + 89], LeClair's system [LeC85] and Young's system <ref> [You87] </ref>). However, the more difficult task is to compare knowledge from separately encoded knowledge bases, made difficult if differences in language and rule structure exist.

References-found: 143

