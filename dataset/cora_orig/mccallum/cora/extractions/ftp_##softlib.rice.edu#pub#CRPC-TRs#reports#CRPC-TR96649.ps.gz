URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96649.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Characterization of the Smoothness and Curvature of a Marginal Function for a Trust-Region Problem  
Author: David A. Andrews Lus Nunes Vicente 
Date: May 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. A. Andrews, </author> <title> Optimal projections for normal groups, </title> <type> tech. rep., </type> <institution> Department of Statistics, Rice University, </institution> <year> 1996. </year>
Reference-contexts: This application was the main motivation for the work in this paper and is described in detail in Andrews <ref> [1] </ref>. A simple description of the statistical problem is as follows: suppose we have two populations that have p-dimensional normal distributions with (positive definite) covariance matrices 1 and 2 and mean vectors 1 6= 2 . <p> Thus, we can use the results of this paper to calculate the derivative of (5.1) and so solve (5.2) using Newton's method. This approach has been applied to a few statistical data sets with successful numerical results <ref> [1] </ref>. 5.2 Reducing the trust radius in optimization Consider the unconstrained optimization problem (1.3) and assume that a step s k = s ( k ) is computed by approximately solving the trust-region subproblem (1.4).
Reference: [2] <author> J. Gauvin, </author> <title> The generalized gradient of a marginal function in mathematical programming, </title> <journal> Math. Oper. Res., </journal> <volume> 4 (1979), </volume> <pages> pp. 458-463. </pages>
Reference-contexts: See the papers <ref> [2] </ref>, [3], [4], [7], [8], [14], [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> Section 4 characterizes the curvature of the marginal function. In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin <ref> [2] </ref>, Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our <p> studied among others by Gauvin <ref> [2] </ref>, Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints. The nonlinear programming considered in [2], [4] is of the form: minimize f (x) subject to g i (x) y i ; i = 1; : : : ; <p> The work in <ref> [2] </ref>, [4] applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints. The nonlinear programming considered in [2], [4] is of the form: minimize f (x) subject to g i (x) y i ; i = 1; : : : ; n 1 ; h j (x) = y j ; j = n 1 + 1; : : : ; n 1 + n 2 ; where <p> A direct application of Theorem 2 and Corollary 1 in <ref> [2] </ref> yields the following result. We point out that the regularity conditions R1 and R2 considered in [2] are trivially satisfied for problem (1.1). Theorem 2.1 The marginal function v () defined in (1.2) is Lipschitz continuous in (0; +1). <p> A direct application of Theorem 2 and Corollary 1 in <ref> [2] </ref> yields the following result. We point out that the regularity conditions R1 and R2 considered in [2] are trivially satisfied for problem (1.1). Theorem 2.1 The marginal function v () defined in (1.2) is Lipschitz continuous in (0; +1). Furthermore, the marginal function has left and right derivatives in (0; +1).
Reference: [3] <author> J. Gauvin and F. Dubeau, </author> <title> Differential properties of the marginal function in mathematical programming, </title> <journal> Math. Programming Stud., </journal> <volume> 19 (1982), </volume> <pages> pp. 101-119. </pages>
Reference-contexts: See the papers [2], <ref> [3] </ref>, [4], [7], [8], [14], [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> Section 4 characterizes the curvature of the marginal function. In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau <ref> [3] </ref>, Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. <p> 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau <ref> [3] </ref>, Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints.
Reference: [4] <author> J. Gauvin and J. W. Tolle, </author> <title> Differential stability in nonlinear programming, </title> <journal> SIAM J. Control Optim., </journal> <volume> 15 (1977), </volume> <pages> pp. 294-311. </pages>
Reference-contexts: See the papers [2], [3], <ref> [4] </ref>, [7], [8], [14], [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> Section 4 characterizes the curvature of the marginal function. In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle <ref> [4] </ref>, Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our context since the marginal function depends only on <p> among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle <ref> [4] </ref>, Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints. The nonlinear programming considered in [2], [4] is of the form: minimize f (x) subject to g i (x) y i ; i = 1; : : : ; n <p> The work in [2], <ref> [4] </ref> applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints. The nonlinear programming considered in [2], [4] is of the form: minimize f (x) subject to g i (x) y i ; i = 1; : : : ; n 1 ; h j (x) = y j ; j = n 1 + 1; : : : ; n 1 + n 2 ; where the
Reference: [5] <author> D. M. Gay, </author> <title> Computing optimal locally constrained steps, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 2 (1981), </volume> <pages> pp. 186-197. </pages>
Reference-contexts: For a proof of this result see Gay <ref> [5] </ref>, or Sorensen [17]. The necessary part of these conditions is just an application of the first-order and second-order necessary optimality conditions for nonlinear programming. These conditions were independently discovered by Karush [9] and Kuhn and Tucker [10] and are usually called the Karush-Kuhn-Tucker (KKT) conditions.
Reference: [6] <author> J.-B. Hiriart-Urruty and C. </author> <title> Lamar echal, Convex Analysis and Minimization Algo rithms I, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference: [7] <author> W. W. Hogan, </author> <title> Directional derivatives for extreme-value functions with applications to the completely convex case, </title> <journal> Oper. Res., </journal> <volume> 21 (1973), </volume> <pages> pp. 188-209. </pages>
Reference-contexts: See the papers [2], [3], [4], <ref> [7] </ref>, [8], [14], [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan <ref> [7] </ref>, Janin [8], Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function.
Reference: [8] <author> R. Janin, </author> <title> Directional derivative of the marginal function in nonlinear programming, </title> <journal> Math. Programming Stud., </journal> <volume> 21 (1984), </volume> <pages> pp. 110-126. </pages>
Reference-contexts: See the papers [2], [3], [4], [7], <ref> [8] </ref>, [14], [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin <ref> [8] </ref>, Rockafellar [14], Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. <p> Theorem 2.2 Assume for a given , that problem (1.1) as an unique optimal solution s () with an unique Lagrange multiplier (). Then the marginal function v () defined in (1.2) is differentiable at with gradient rv () given by rv () = (): From the results in <ref> [8] </ref>, we know also that the marginal function v () given by (1.2) has a right derivative at = 0. Thus, this marginal function is Lipschitz continuous in [0; +1).
Reference: [9] <author> W. Karush, </author> <title> Minima of Functions of Several Variables with Inequalities as Side Constraints, </title> <type> Master's thesis, </type> <institution> Department of Mathematics, University of Chicago, </institution> <year> 1939. </year>
Reference-contexts: For a proof of this result see Gay [5], or Sorensen [17]. The necessary part of these conditions is just an application of the first-order and second-order necessary optimality conditions for nonlinear programming. These conditions were independently discovered by Karush <ref> [9] </ref> and Kuhn and Tucker [10] and are usually called the Karush-Kuhn-Tucker (KKT) conditions. The parameter () is the Lagrange multiplier associated with the trust-region constraint ksk .
Reference: [10] <author> H. W. Kuhn and A. W. Tucker, </author> <title> Nonlinear programming, </title> <booktitle> in Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <editor> J. Neyman, ed., </editor> <publisher> University of California Press, </publisher> <year> 1951. </year>
Reference-contexts: For a proof of this result see Gay [5], or Sorensen [17]. The necessary part of these conditions is just an application of the first-order and second-order necessary optimality conditions for nonlinear programming. These conditions were independently discovered by Karush [9] and Kuhn and Tucker <ref> [10] </ref> and are usually called the Karush-Kuhn-Tucker (KKT) conditions. The parameter () is the Lagrange multiplier associated with the trust-region constraint ksk .
Reference: [11] <author> J. J. Mor e, </author> <title> Recent developments in algorithms and software for trust regions methods, in Mathematical programming. The state of art, </title> <editor> A. Bachem, M. Grotschel, and B. Korte, eds., </editor> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1983, </year> <pages> pp. </pages> <month> 258-287. </month> <title> [12] , Generalizations of the trust region problem, Optimization Methods and Software, </title> <booktitle> 2 (1993), </booktitle> <pages> pp. 189-209. </pages>
Reference-contexts: To us, these results are quite surprising and confirm the elegance of trust regions. In Nonlinear Optimization the problem (1.1) appears in the globalization of Newton and quasi-Newton algorithms, and it is usually called the trust-region subproblem <ref> [11] </ref>. The real number is called the trust radius. For simplicity consider the unconstrained minimization problem minimize f (x) ; (1.3) where f is a twice continuously differentiable function mapping IR n into IR.
Reference: [13] <author> J. J. Mor e and D. C. Sorensen, </author> <title> Computing a trust region step, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 553-572. </pages>
Reference-contexts: Proposition 3.1 The problem (1.1) has no solutions at the boundary fs : ksk = g if and only if H is positive definite and kH 1 gk &lt; . A proof of this simple fact can be found in <ref> [13] </ref>.
Reference: [14] <author> R. T. Rockafellar, </author> <title> Directional differentiability of the optimal value function in a nonlinear programming problem, </title> <journal> Math. Programming Stud., </journal> <volume> 21 (1984), </volume> <pages> pp. 213-226. </pages>
Reference-contexts: See the papers [2], [3], [4], [7], [8], <ref> [14] </ref>, [15], [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. <p> In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar <ref> [14] </ref>, Seeger [15], and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. <p> Theorem 2.1 The marginal function v () defined in (1.2) is Lipschitz continuous in (0; +1). Furthermore, the marginal function has left and right derivatives in (0; +1). The following result is an application of the Corollary of Theorem 4 in <ref> [14] </ref>. (Again, we point out that the conditions and constraint qualifications required to apply this result are trivially satisfied for problem (1.1).) This results provides a formula for the derivative of the marginal function v () in the case where problem (1.1) has a unique solution and unique corresponding Lagrange multiplier.
Reference: [15] <author> A. Seeger, </author> <title> Second order directional derivatives in parametric optimization problems, </title> <journal> Math. Oper. Res., </journal> <volume> 13 (1988), </volume> <pages> pp. 124-139. </pages>
Reference-contexts: See the papers [2], [3], [4], [7], [8], [14], <ref> [15] </ref>, [16] and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. This work was developed while the author was a graduate student at the Department of Computational and Applied Mathematics of Rice University. <p> In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger <ref> [15] </ref>, and Shapiro [16]. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. <p> Thus, this marginal function is Lipschitz continuous in [0; +1). A general characterization of the second-order directional derivatives of the marginal function in nonlinear programming is given in the papers <ref> [15] </ref>, [16].
Reference: [16] <author> A. Shapiro, </author> <title> Second-order derivatives of extreme-value functions and optimality conditions for semi-infinite programs, </title> <journal> Math. Oper. Res., </journal> <volume> 10 (1985), </volume> <pages> pp. 207-219. </pages>
Reference-contexts: See the papers [2], [3], [4], [7], [8], [14], [15], <ref> [16] </ref> and the references therein. fl Department of Statistics, Rice University, Houston, Texas 77005-1892, USA. E-Mail: andrewsd@stat.rice.edu. y Departamento de Matematica, Universidade de Coimbra, 3000 Coimbra, Portugal. This work was developed while the author was a graduate student at the Department of Computational and Applied Mathematics of Rice University. <p> In Section 5, we discuss the applications in Statistics and Optimization. 2 Applying sensitivity theory for nonlinear programming The marginal function in nonlinear programming has been studied among others by Gauvin [2], Gauvin and Dubeau [3], Gauvin and Tolle [4], Hogan [7], Janin [8], Rockafellar [14], Seeger [15], and Shapiro <ref> [16] </ref>. In [3], the authors consider perturbations in the left and right sides of the constraints as well as in the objective function. The work in [2], [4] applies directly to our context since the marginal function depends only on perturbations of the right side of the constraints. <p> Thus, this marginal function is Lipschitz continuous in [0; +1). A general characterization of the second-order directional derivatives of the marginal function in nonlinear programming is given in the papers [15], <ref> [16] </ref>.
Reference: [17] <author> D. C. Sorensen, </author> <title> Newton's method with a model trust region modification, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 (1982), </volume> <pages> pp. 409-426. </pages>
Reference-contexts: For a proof of this result see Gay [5], or Sorensen <ref> [17] </ref>. The necessary part of these conditions is just an application of the first-order and second-order necessary optimality conditions for nonlinear programming. These conditions were independently discovered by Karush [9] and Kuhn and Tucker [10] and are usually called the Karush-Kuhn-Tucker (KKT) conditions. <p> If &gt; # , then any solution for h () = 2 is such that H + I is not positive definite. As result we cannot calculate () by solving h () = 2 . It is shown in <ref> [17] </ref> that in such case a solution for the problem (1.1) can be calculated by finding the t () such that kp + t ()qk = ; where p solves p = g and q is any nonzero vector in E ( 1 ).
References-found: 16

