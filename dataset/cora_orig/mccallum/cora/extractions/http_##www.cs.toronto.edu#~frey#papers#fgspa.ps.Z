URL: http://www.cs.toronto.edu/~frey/papers/fgspa.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/papers/fgspa.abs.html
Root-URL: http://www.cs.toronto.edu
Email: (frank@comm.utoronto.ca)  (frey@cs.utoronto.ca)  (haloeliger@access.ch)  
Title: Factor Graphs and the Sum-Product Algorithm  
Author: Frank R. Kschischang Brendan J. Frey Hans-Andrea Loeliger 
Keyword: Graphical models, factor graphs, Tanner graphs, sum-product algorithm, marginalization, forward/backward algorithm, Viterbi algorithm, iterative decoding, belief propagation, Kalman filtering, fast Fourier transform.  
Address: Toronto, Toronto, Ontario M5S 3G4, CANADA  405 North Mathews Avenue, Urbana, IL 61801, USA  Gartenstrasse 120, CH-4052 Basel, SWITZERLAND  
Affiliation: Department of Electrical Computer Engineering, University of  The Beckman Institute,  Endora Tech AG,  
Date: July 27, 1998  
Abstract: A factor graph is a bipartite graph that expresses how a "global" function of many variables factors into a product of "local" functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute|either exactly or approximately|various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative "turbo" decoding algorithm, Pearl's belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. Submitted to IEEE Transactions on Information Theory, July, 1998. This paper is available on the web at http://www.comm.utoronto.ca/frank/factor/. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. M. Aji and R. J. </author> <title> McEliece, "A general algorithm for distributing information on a graph," </title> <booktitle> in Proc. 1997 IEEE Int. Symp. on Inform. Theory, </booktitle> <address> (Ulm, Germany), p. 6, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: In parallel with the development of this paper, Aji and McEliece <ref> [1, 2] </ref> develop the closely related "generalized distributive law," an alternative approach based on the properties of junction trees (and not factor graphs). Aji and McEliece also point out the commonalities among a wide variety of algorithms, and furnish an extensive bibliography. <p> We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where <p> Intuitively, x is not marginalized out in the region of T in which x is "involved." To paraphrase the California winemakers Gallo, "we summarize no variable before its time." 6.4 An FFT An important observation due to Aji and McEliece <ref> [1, 2] </ref> is that various fast transform algorithms can be developed using a graph-based approach. In this section, we translate the approach of Aji and McEliece to the language of factor graphs. A factor graph for the DFT kernel was given in Section 2, Example 11.
Reference: [2] <author> S. M. Aji and R. J. </author> <title> McEliece, "The generalized distributive law," </title> <note> preprint available on-line from http://www.systems.caltech.edu/EE/faculty/rjm, 1998. </note>
Reference-contexts: In parallel with the development of this paper, Aji and McEliece <ref> [1, 2] </ref> develop the closely related "generalized distributive law," an alternative approach based on the properties of junction trees (and not factor graphs). Aji and McEliece also point out the commonalities among a wide variety of algorithms, and furnish an extensive bibliography. <p> We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where <p> Intuitively, x is not marginalized out in the region of T in which x is "involved." To paraphrase the California winemakers Gallo, "we summarize no variable before its time." 6.4 An FFT An important observation due to Aji and McEliece <ref> [1, 2] </ref> is that various fast transform algorithms can be developed using a graph-based approach. In this section, we translate the approach of Aji and McEliece to the language of factor graphs. A factor graph for the DFT kernel was given in Section 2, Example 11.
Reference: [3] <author> B. D. O. Anderson and J. B. Moore, </author> <title> Optimal Filtering. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1979. </year>
Reference-contexts: The details of this straightforward extension are omitted. 51 5.5 Kalman Filtering In this section, we derive the Kalman filter <ref> [3] </ref> as the optimal predictor given by the sum-product algorithm in a factor graph for a time-varying discrete-time linear dynamical system (cf., (8)). The input to the system consists of a sequence of unknown K-dimensional real-valued input column vectors u j , j = 1; 2; : : : . <p> j+1 = B j OE j B 0 j (D j j D 0 j ) 1 C j j ]A 0 where K j = A j j C 0 j + C j j C 0 These updates are exactly equal to the updates used by Kalman filtering <ref> [3] </ref>.
Reference: [4] <author> L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, </author> <title> "Optimal decoding of linear codes for minimizing symbol error rate," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 284-287, </pages> <month> Mar. </month> <year> 1974. </year>
Reference-contexts: The optimum (minimum probability of symbol error) detection algorithm for codes, sometimes referred to as the MAP (maximum a posteriori probability) algorithm or the BCJR algo 3 rithm (after the authors of <ref> [4] </ref>) turns out to be a special case of the sum-product algorithm applied to a trellis. This algorithm was developed earlier in the statistics literature [5] and perhaps even earlier in classified work due to L. R. Welch [29]. <p> Our main results will be the derivation of a variety of well known algorithms as special cases of the sum-product algorithm. 5.1 The Forward/Backward Algorithm The forward/backward algorithm, sometimes referred to in coding theory as the BCJR algorithm <ref> [4] </ref> or "MAP" algorithm, is an application of the sum-product algorithm to the hidden Markov model of Example 6, shown in Fig. 9 (d), or to the trellises of examples Examples 3 and 4 (Figs. 6 and 7) in which certain variables are observed at the output of a memoryless channel. <p> In fact, the forward and backward message chains do not interact, so their computation could occur in parallel. ff (s i ) ff (s i+1 ) s i s i+1 u i ffi (u i ) In the literature on the forward/backward algorithm (e.g., <ref> [4] </ref>), the messages sent from the channel input variables are referred to as `fl's, the messages sent from state variables in the forward step are referred to as `ff's, and the messages sent from state variables in the backward step are referred to as `fi's.
Reference: [5] <author> L. E. Baum and T. Petrie, </author> <title> "Statistical inference for probabilistic functions of finite state Markov chains," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 37, </volume> <pages> pp. 1559-1563, </pages> <year> 1966. </year>
Reference-contexts: This algorithm was developed earlier in the statistics literature <ref> [5] </ref> and perhaps even earlier in classified work due to L. R. Welch [29]. In the signal processing literature, and particularly in speech processing, this algorithm is widely known as the forward-backward algorithm [33].
Reference: [6] <author> S. Benedetto and G. Montorsi, </author> <title> "Iterative decoding of serially concatenated convolutional codes," Electr. </title> <journal> Lett., </journal> <volume> vol. 32, </volume> <pages> pp. 1186-1188, </pages> <month> June </month> <year> 1996. </year>
Reference: [7] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> "Near Shannon-limit error-correcting coding and decoding: turbo codes," </title> <booktitle> Proc. ICC'93, </booktitle> <pages> pp. 1064-1070, </pages> <address> Geneva, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Tanner's approach was later generalized to graphs with hidden (state) variables by Wiberg, et al. [39, 38]. In coding theory, much of the current interest in so-called "soft-output" decoding algorithms stems from the near-capacity-achieving performance of turbo codes, introduced by Berrou, et al. <ref> [7] </ref>. <p> More recently, at least two papers [22, 30] develop a view of the "turbo decoding" algorithm <ref> [7] </ref> as an instance of probability propagation in a Bayesian network code model. Each node v in a Bayesian network is associated with a random variable. <p> Extensive simulation results (see, e.g., <ref> [7, 25, 26] </ref>) show that sum-product based decoding algorithms with very long codes can astonishing performance (within a fraction of a decibel of the Shannon limit in some cases), even though the underlying factor graph has cycles. <p> Descriptions of the way in which the sum-product algorithm is applied to a variety of "compound codes" are given in [22]. In this section, we restrict ourselves to two examples: turbo codes <ref> [7] </ref> and low-density parity-check codes [14]. Turbo Codes A "turbo code" or parallel concatenated convolutional code has the encoder structure shown in Fig. 22 (a). A block u of data to be transmitted enters the systematic encoder which produces u, and two parity-check streams p and q at its output.
Reference: [8] <author> U. Bertele and F. Brioschi, </author> <title> Nonserial Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: In Section 6, we will see that such factor graph representations can lead to fast Fourier transform (FFT) algorithms. f y 2 y 1 y 0 Example 12. (Nonserial dynamic programming) In the field of optimization, in their formulation <ref> [8] </ref> of so-called nonserial dynamic pro gramming problems, Bertele and Brioschi consider functions that are the additive composition of local functions. These types of problems fit naturally in the factor graph 23 framework, provided that we view the `multiplication' operation in R as real-valued ad-dition. If, as in [8], we are <p> their formulation <ref> [8] </ref> of so-called nonserial dynamic pro gramming problems, Bertele and Brioschi consider functions that are the additive composition of local functions. These types of problems fit naturally in the factor graph 23 framework, provided that we view the `multiplication' operation in R as real-valued ad-dition. If, as in [8], we are interested in minima and maxima of the global function, we will take as a summary operator the `min' or `max' operation. <p> If, as in [8], we are interested in minima and maxima of the global function, we will take as a summary operator the `min' or `max' operation. It is interesting to observe the close relationship between the "interaction graph" of a nonserial dynamic programming problem <ref> [8, p. 4] </ref> and the graph corresponding to a Markov random field (see example 7): they are really the same object defined with respect to a different binary operation: function addition in the nonserial dynamic programming case and function multiplication in the Markov random field case.
Reference: [9] <author> P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, </author> <title> "The Helmholtz machine," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 889-904, </pages> <year> 1995. </year>
Reference-contexts: further examples of factor graphs that might be used in a variety of fields, including artificial intelligence, neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception <ref> [18, 17, 9, 12] </ref>. (See [11] for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" [34] treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth.
Reference: [10] <author> G. D. Forney, Jr., </author> <title> "On iterative decoding and the two-way algorithm," </title> <booktitle> Proc. Int. Symp. on Turbo Codes and Related Topics, </booktitle> <address> Brest, France, </address> <month> Sept., </month> <year> 1997. </year>
Reference-contexts: Aji and McEliece also point out the commonalities among a wide variety of algorithms, and furnish an extensive bibliography. Forney <ref> [10] </ref> gives a nice overview of the development of many of these algorithms, with an emphasis on applications in coding theory. The first appearance of the sum-product algorithm in the coding theory literature is probably Gallager's decoding algorithm for low-density parity-check codes [14]. <p> Provided that D S = C, i.e., that the subconfigurations of the elements of D with respect to S is equal to C, we will consider a factor graph for D to be a valid factor graph for C. Following Forney <ref> [10] </ref> we will sometimes refer to such a factor graph|and any factor graph for a set membership indicator function having auxiliary 11 variables|as as a TWL (Tanner/Wiberg/Loeliger) graph for C. As mentioned earlier, auxiliary variable nodes are indicated with a double circle in our factor graph diagrams.
Reference: [11] <author> B. J. Frey, </author> <title> Graphical Models for Machine Learning and Digital Communication. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> Neural network formulations of factor graphs have also been used for unsupervised learning and density estimation; see, e.g., <ref> [11] </ref>. In coding theory, Tanner [35] generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. Tanner's approach was later generalized to graphs with hidden (state) variables by Wiberg, et al. [39, 38]. <p> This theorem can be interpreted as saying that, in a sense, a factor graph is the "square root" of a Markov random field. Example 8. (Bayesian networks) Bayesian networks (see, e.g., <ref> [31, 20, 11] </ref>) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). Bayesian networks, combined with Pearl's "belief propagation algorithm" [31], have become an important tool in expert systems over the past decade. <p> that might be used in a variety of fields, including artificial intelligence, neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception [18, 17, 9, 12]. (See <ref> [11] </ref> for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" [34] treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth.
Reference: [12] <author> B. J. Frey, P. Dayan, and G. E. Hinton, </author> <title> "A simple algorithm that discovers efficient perceptual codes," in Computational and Psychophysical Mechanisms of Visual Coding, </title> <editor> (M. Jenkin and L. R. Harris, eds). </editor> <address> New York, NY: </address> <publisher> Cambridge University Press, </publisher> <pages> pp. 296-315, </pages> <year> 1997. </year>
Reference-contexts: further examples of factor graphs that might be used in a variety of fields, including artificial intelligence, neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception <ref> [18, 17, 9, 12] </ref>. (See [11] for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" [34] treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth.
Reference: [13] <author> B. J. Frey and G. E. Hinton, </author> <title> "Variational learning in non-linear Gaussian belief networks," </title> <note> to appear in Neural Computation, </note> <year> 1998. </year>
Reference-contexts: Although the details of these models and learning algorithms fall beyond the scope of this paper, we present here a brief example. See <ref> [13] </ref> for the details of a model and a learning algorithm for real-valued variables. In this example, we discuss a binary version of this problem for clarity. Each image in the pair is one-dimensional and contains 6 binary pixels.
Reference: [14] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes. </title> <address> Cambridge, MA: </address> <publisher> M.I.T. Press, </publisher> <year> 1963. </year>
Reference-contexts: Forney [10] gives a nice overview of the development of many of these algorithms, with an emphasis on applications in coding theory. The first appearance of the sum-product algorithm in the coding theory literature is probably Gallager's decoding algorithm for low-density parity-check codes <ref> [14] </ref>. The optimum (minimum probability of symbol error) detection algorithm for codes, sometimes referred to as the MAP (maximum a posteriori probability) algorithm or the BCJR algo 3 rithm (after the authors of [4]) turns out to be a special case of the sum-product algorithm applied to a trellis. <p> The first to connect Bayesian networks and belief propaga tion with applications in coding theory were MacKay and Neal [25], who independently re-discovered Gallager's earlier work on low-density parity-check codes <ref> [14] </ref> (including Gallager's decoding algorithm). More recently, at least two papers [22, 30] develop a view of the "turbo decoding" algorithm [7] as an instance of probability propagation in a Bayesian network code model. Each node v in a Bayesian network is associated with a random variable. <p> Descriptions of the way in which the sum-product algorithm is applied to a variety of "compound codes" are given in [22]. In this section, we restrict ourselves to two examples: turbo codes [7] and low-density parity-check codes <ref> [14] </ref>. Turbo Codes A "turbo code" or parallel concatenated convolutional code has the encoder structure shown in Fig. 22 (a). A block u of data to be transmitted enters the systematic encoder which produces u, and two parity-check streams p and q at its output. <p> This is then followed by another forward/backward computation over the other constituent code, and propagation of messages back to the first encoder. This schedule of messages is illustrated in [22, Fig. 10]. Low-density Parity-check Codes Low-density parity-check (LDPC) codes were introduced by Gallager <ref> [14] </ref> in the early 1960s. LDPC codes are defined in terms of a regular bipartite graph. In a (j; k) LDPC code, left nodes, representing codeword symbols, all have degree j, while right nodes, representing checks, all have degree k. <p> We treat here only the important case where all variables are binary and all functions (except single-variable functions) are parity checks (as in Fig. 8 (b) and in Fig. 23). This includes, in particular, low-density parity check codes <ref> [14] </ref> of the previous subsection. We give the corresponding simplifications for both the sum-product and the min-sum versions of the algorithm, both of which were known long ago [14]. <p> This includes, in particular, low-density parity check codes <ref> [14] </ref> of the previous subsection. We give the corresponding simplifications for both the sum-product and the min-sum versions of the algorithm, both of which were known long ago [14].
Reference: [15] <author> R. L. Graham, D. E. Knuth and O. Patashnik, </author> <title> Concrete Mathematics. </title> <address> New York, NY: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: joint probability mass or density function of a collection of random variables, and if x i and x j are variables that contribute independently to g, then the corresponding random variables are independent. (The converse is not necessarily true.) We will also need the following useful notation called "Iverson's convention" <ref> [15, p. 24] </ref> for indicating the truth of a logical proposition: if P is a Boolean proposition, then [P ] is the binary function that indicates whether or not P is true, i.e., [P ] = 1 if P ; 0 otherwise: (4) We will use Iverson's convention in formulas only
Reference: [16] <author> J. Hagenauer, E. Offer and L. Papke, </author> <title> "Iterative decoding of binary block and convolutional codes," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 429-445, </pages> <month> March </month> <year> 1996. </year>
Reference: [17] <author> G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, </author> <title> "The wake-sleep algorithm for unsupervised neural networks," </title> <journal> Science, </journal> <volume> vol. 268, </volume> <pages> pp. 1158-1161, </pages> <year> 1995. </year>
Reference-contexts: further examples of factor graphs that might be used in a variety of fields, including artificial intelligence, neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception <ref> [18, 17, 9, 12] </ref>. (See [11] for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" [34] treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth.
Reference: [18] <author> G. E. Hinton and T. J. Sejnowski, </author> <title> "Learning and relearning in Boltzmann machines," in Parallel Distributed Processing: Explorations in the Microstructure of Cognition (D. </title> <editor> E. Rumelhart and J. L. McClelland, eds.), </editor> <volume> vol. I, </volume> <pages> pp. 282-317, </pages> <address> Cambridge MA.: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In other words, G is an MRF if every variable v is independent of non-neighboring variables in the graph, given the values of its immediate neighbors. MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., <ref> [21, 32, 19, 18] </ref>). Kschischang and Frey [22] give a brief discussion of the use of MRFs to describe codes. Recall that a clique in a graph is a collection of vertices which are all pairwise neighbors. <p> further examples of factor graphs that might be used in a variety of fields, including artificial intelligence, neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception <ref> [18, 17, 9, 12] </ref>. (See [11] for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" [34] treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth.
Reference: [19] <author> V. Isham, </author> <title> "An introduction to spatial point processes and Markov random fields," </title> <journal> Int. Stat. Rev., </journal> <volume> vol. 49, </volume> <pages> pp. 21-43, </pages> <year> 1981. </year>
Reference-contexts: and a variable node; the latter is called the variable associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields <ref> [19, 21, 32] </ref>, Bayesian networks [20, 31] and Tanner graphs [35, 38, 39]. <p> In other words, G is an MRF if every variable v is independent of non-neighboring variables in the graph, given the values of its immediate neighbors. MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., <ref> [21, 32, 19, 18] </ref>). Kschischang and Frey [22] give a brief discussion of the use of MRFs to describe codes. Recall that a clique in a graph is a collection of vertices which are all pairwise neighbors.
Reference: [20] <author> F. V. Jensen, </author> <title> An Introduction to Bayesian Networks. </title> <address> New York: </address> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: latter is called the variable associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields [19, 21, 32], Bayesian networks <ref> [20, 31] </ref> and Tanner graphs [35, 38, 39]. <p> Pearl's belief propagation and belief revision algorithms, widely applied in expert systems and in artificial intelligence, turn out to be examples of the sum-product algorithm operating in a Bayesian network; see <ref> [31, 20] </ref> for textbook treatments. Neural network formulations of factor graphs have also been used for unsupervised learning and density estimation; see, e.g., [11]. In coding theory, Tanner [35] generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. <p> In this case, motivated by a similar convention in Bayesian networks <ref> [20, 31] </ref> (see example 8, below), we will sometimes indicate the child, i.e., x i , in this relationship by placing an arrow on the edge leading from the local function f to x i . <p> This theorem can be interpreted as saying that, in a sense, a factor graph is the "square root" of a Markov random field. Example 8. (Bayesian networks) Bayesian networks (see, e.g., <ref> [31, 20, 11] </ref>) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). Bayesian networks, combined with Pearl's "belief propagation algorithm" [31], have become an important tool in expert systems over the past decade. <p> Bayesian networks are widely used in a variety of applications in artificial intelligence and expert systems, and an extensive literature on them exists. See <ref> [31, 20] </ref> for textbook treatments. To convert a Bayesian network into a factor graph is straightforward; we introduce a function node for each factor p (v i ja (v i )) in (12) and draw edges from this node to v i and its parents a (v i ). <p> Nevertheless, such transformations can be quite useful, and we apply them to derive a fast Fourier transform algorithm from the factor graph representing the DFT kernel. In <ref> [20, 24] </ref>, similar general procedures are described for transforming a graphical probability model into cycle-free form. 56 6.1 Grouping Like Nodes, Multiplying by Unity It is always possible to group two nodes of like type|i.e., both variable nodes or both function nodes|without changing the global function being represented by a factor
Reference: [21] <author> R. Kindermann and J. L. Snell, </author> <title> Markov Random Fields and their Applications. </title> <address> Providence, Rhode Island: </address> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference-contexts: and a variable node; the latter is called the variable associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields <ref> [19, 21, 32] </ref>, Bayesian networks [20, 31] and Tanner graphs [35, 38, 39]. <p> In the next two examples, we describe very briefly the close relationship between factor graphs and models based on undirected graphs (Markov random fields) and models based on directed acyclic graphs (Bayesian networks). 16 Example 7. (Markov random fields) A Markov random field (see, e.g., <ref> [21] </ref>) is a graphical model based on an undirected graph G = (V; E) in which each node corresponds to a random variable. <p> In other words, G is an MRF if every variable v is independent of non-neighboring variables in the graph, given the values of its immediate neighbors. MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., <ref> [21, 32, 19, 18] </ref>). Kschischang and Frey [22] give a brief discussion of the use of MRFs to describe codes. Recall that a clique in a graph is a collection of vertices which are all pairwise neighbors. <p> By assigning a unit potential function to all cliques of F 2 S that do not correspond to some E 2 Q, we obtain a collection of Gibbs potential functions over all of the cliques of F 2 S . It is well known (see, e.g., <ref> [21] </ref>) that any such collection of non-negative Gibbs potential functions is linearly proportional to a probability distribution that satisfies the local Markov property (10) and hence defines a Markov random field. 64 B Complexity of the Sum-Product Algorithm in a Finite Tree The complexity of the sum-product algorithm is difficult to
Reference: [22] <author> F. R. Kschischang and B. J. Frey, </author> <title> "Iterative decoding of compound codes by probability propagation in graphical models," </title> <journal> IEEE J. Selected Areas in Commun., </journal> <volume> vol. 16, </volume> <year> 1998. </year>
Reference-contexts: Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> In coding theory, much of the current interest in so-called "soft-output" decoding algorithms stems from the near-capacity-achieving performance of turbo codes, introduced by Berrou, et al. [7]. The turbo decoding algorithm was formulated in a Bayesian network framework by McEliece, et al. [30], and Kschischang and Frey <ref> [22] </ref>. 1.3 A Sum-Product Algorithm Example As we will describe more precisely in Section 4, the sum-product algorithm can be used (in a factor graph that forms a tree) to compute a function summary or marginal. <p> MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., [21, 32, 19, 18]). Kschischang and Frey <ref> [22] </ref> give a brief discussion of the use of MRFs to describe codes. Recall that a clique in a graph is a collection of vertices which are all pairwise neighbors. <p> may be preferable to an MRF in expressing such a factorization, since distinct factorizations, i.e., factorizations with different Qs in (11), may yield precisely the same underlying MRF graph, whereas they will always yield distinct factor graphs. (An example in a coding context of this MRF ambiguity is given in <ref> [22] </ref>.) In the opposite direction, a factor graph F that represents a joint probability distribution can be converted to a Markov random field via a component of the second higher power graph F 2 [22]. <p> distinct factor graphs. (An example in a coding context of this MRF ambiguity is given in <ref> [22] </ref>.) In the opposite direction, a factor graph F that represents a joint probability distribution can be converted to a Markov random field via a component of the second higher power graph F 2 [22]. <p> The first to connect Bayesian networks and belief propaga tion with applications in coding theory were MacKay and Neal [25], who independently re-discovered Gallager's earlier work on low-density parity-check codes [14] (including Gallager's decoding algorithm). More recently, at least two papers <ref> [22, 30] </ref> develop a view of the "turbo decoding" algorithm [7] as an instance of probability propagation in a Bayesian network code model. Each node v in a Bayesian network is associated with a random variable. <p> Obviously a wide variety of message passing schedules are possible. For example, the schedule in which L i = (S fi Q) [ (Q fi S) for all i, is called the flooding schedule <ref> [22] </ref>. The flooding schedule calls for a message to be passed on each edge in each direction at each clock tick. <p> Descriptions of the way in which the sum-product algorithm is applied to a variety of "compound codes" are given in <ref> [22] </ref>. In this section, we restrict ourselves to two examples: turbo codes [7] and low-density parity-check codes [14]. Turbo Codes A "turbo code" or parallel concatenated convolutional code has the encoder structure shown in Fig. 22 (a). <p> This is then followed by another forward/backward computation over the other constituent code, and propagation of messages back to the first encoder. This schedule of messages is illustrated in <ref> [22, Fig. 10] </ref>. Low-density Parity-check Codes Low-density parity-check (LDPC) codes were introduced by Gallager [14] in the early 1960s. LDPC codes are defined in terms of a regular bipartite graph.
Reference: [23] <author> S. L. Lauritzen and F. V. Jensen, </author> <title> "Local computation with valuations from a commutative semigroup," </title> <journal> Annals of Math. and Artificial Intelligence, </journal> <volume> vol. 21, </volume> <pages> pp. 51-69, </pages> <year> 1997. </year>
Reference-contexts: We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where
Reference: [24] <author> S. L. Lauritzen and D. J. Spiegelhalter, </author> <title> "Local computations with probabilities on graphical structures and their application to expert systems," </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> vol. 50, </volume> <pages> pp. 157-224, </pages> <year> 1988. </year>
Reference-contexts: Nevertheless, such transformations can be quite useful, and we apply them to derive a fast Fourier transform algorithm from the factor graph representing the DFT kernel. In <ref> [20, 24] </ref>, similar general procedures are described for transforming a graphical probability model into cycle-free form. 56 6.1 Grouping Like Nodes, Multiplying by Unity It is always possible to group two nodes of like type|i.e., both variable nodes or both function nodes|without changing the global function being represented by a factor
Reference: [25] <author> D. J. C. MacKay and R. M. Neal, </author> <title> "Good codes based on very sparse matrices," in Cryptography and Coding. </title> <booktitle> 5th IMA Conference (C. </booktitle> <editor> Boyd, ed.), </editor> <volume> no. </volume> <booktitle> 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pp. 100-111, </pages> <address> Berlin Germany: </address> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> Bayesian networks, combined with Pearl's "belief propagation algorithm" [31], have become an important tool in expert systems over the past decade. The first to connect Bayesian networks and belief propaga tion with applications in coding theory were MacKay and Neal <ref> [25] </ref>, who independently re-discovered Gallager's earlier work on low-density parity-check codes [14] (including Gallager's decoding algorithm). More recently, at least two papers [22, 30] develop a view of the "turbo decoding" algorithm [7] as an instance of probability propagation in a Bayesian network code model. <p> Extensive simulation results (see, e.g., <ref> [7, 25, 26] </ref>) show that sum-product based decoding algorithms with very long codes can astonishing performance (within a fraction of a decibel of the Shannon limit in some cases), even though the underlying factor graph has cycles. <p> 7 x 6 x 5 x 4 x 3 x 2 x 1 x 0 Low-density parity-check codes, like turbo codes, are very effectively decoded using the sum-product algorithm; for example MacKay and Neal report excellent performance results approaching that of turbo codes using what amounts to a flooding schedule <ref> [25, 26] </ref>.
Reference: [26] <author> D. J. C. MacKay, </author> <title> "Good error-correcting codes based on very sparse matrices", </title> <note> submitted to IEEE Transactions on Information Theory", </note> <year> 1997. </year>
Reference-contexts: Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> Extensive simulation results (see, e.g., <ref> [7, 25, 26] </ref>) show that sum-product based decoding algorithms with very long codes can astonishing performance (within a fraction of a decibel of the Shannon limit in some cases), even though the underlying factor graph has cycles. <p> 7 x 6 x 5 x 4 x 3 x 2 x 1 x 0 Low-density parity-check codes, like turbo codes, are very effectively decoded using the sum-product algorithm; for example MacKay and Neal report excellent performance results approaching that of turbo codes using what amounts to a flooding schedule <ref> [25, 26] </ref>.
Reference: [27] <author> J. L. Massey, </author> <title> Threshold Decoding. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1963. </year>
Reference: [28] <author> R. J. </author> <title> McEliece, "On the BJCR trellis for linear block codes," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 1072-1092, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where
Reference: [29] <author> R. J. </author> <title> McEliece, </title> <type> private communication, </type> <year> 1997. </year> <month> 71 </month>
Reference-contexts: This algorithm was developed earlier in the statistics literature [5] and perhaps even earlier in classified work due to L. R. Welch <ref> [29] </ref>. In the signal processing literature, and particularly in speech processing, this algorithm is widely known as the forward-backward algorithm [33].
Reference: [30] <author> R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng, </author> <title> "Turbo decoding as an instance of Pearl's `belief propagation' algorithm," </title> <journal> IEEE J. on Selected Areas in Commun., </journal> <volume> vol. 16, </volume> <year> 1998. </year>
Reference-contexts: Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> In coding theory, much of the current interest in so-called "soft-output" decoding algorithms stems from the near-capacity-achieving performance of turbo codes, introduced by Berrou, et al. [7]. The turbo decoding algorithm was formulated in a Bayesian network framework by McEliece, et al. <ref> [30] </ref>, and Kschischang and Frey [22]. 1.3 A Sum-Product Algorithm Example As we will describe more precisely in Section 4, the sum-product algorithm can be used (in a factor graph that forms a tree) to compute a function summary or marginal. <p> The first to connect Bayesian networks and belief propaga tion with applications in coding theory were MacKay and Neal [25], who independently re-discovered Gallager's earlier work on low-density parity-check codes [14] (including Gallager's decoding algorithm). More recently, at least two papers <ref> [22, 30] </ref> develop a view of the "turbo decoding" algorithm [7] as an instance of probability propagation in a Bayesian network code model. Each node v in a Bayesian network is associated with a random variable.
Reference: [31] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems, 2nd ed. </title> <address> San Francisco: </address> <publisher> Mor-gan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: latter is called the variable associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields [19, 21, 32], Bayesian networks <ref> [20, 31] </ref> and Tanner graphs [35, 38, 39]. <p> Pearl's belief propagation and belief revision algorithms, widely applied in expert systems and in artificial intelligence, turn out to be examples of the sum-product algorithm operating in a Bayesian network; see <ref> [31, 20] </ref> for textbook treatments. Neural network formulations of factor graphs have also been used for unsupervised learning and density estimation; see, e.g., [11]. In coding theory, Tanner [35] generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. <p> In this case, motivated by a similar convention in Bayesian networks <ref> [20, 31] </ref> (see example 8, below), we will sometimes indicate the child, i.e., x i , in this relationship by placing an arrow on the edge leading from the local function f to x i . <p> This theorem can be interpreted as saying that, in a sense, a factor graph is the "square root" of a Markov random field. Example 8. (Bayesian networks) Bayesian networks (see, e.g., <ref> [31, 20, 11] </ref>) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). Bayesian networks, combined with Pearl's "belief propagation algorithm" [31], have become an important tool in expert systems over the past decade. <p> Example 8. (Bayesian networks) Bayesian networks (see, e.g., [31, 20, 11]) are graphical models for a collection of random variables that are based on directed acyclic graphs (DAGs). Bayesian networks, combined with Pearl's "belief propagation algorithm" <ref> [31] </ref>, have become an important tool in expert systems over the past decade. The first to connect Bayesian networks and belief propaga tion with applications in coding theory were MacKay and Neal [25], who independently re-discovered Gallager's earlier work on low-density parity-check codes [14] (including Gallager's decoding algorithm). <p> The factor graph corresponding to (13) is shown in Fig. 10 (c); cf. Fig. 1. The arrows in a Bayesian network are often useful in modeling the "flow of causality" in practical situations; see, e.g., <ref> [31] </ref>. Provided that it is not required that a child variable take on some particular value, it is straightforward to simulate a Bayesian network, i.e., draw configurations of the variables consistent with the represented distribution. <p> Bayesian networks are widely used in a variety of applications in artificial intelligence and expert systems, and an extensive literature on them exists. See <ref> [31, 20] </ref> for textbook treatments. To convert a Bayesian network into a factor graph is straightforward; we introduce a function node for each factor p (v i ja (v i )) in (12) and draw edges from this node to v i and its parents a (v i ). <p> Often, we will denote the child v i by drawing an arrow on the edge from the function node to v i . An example conversion from a Bayesian network to a factor graph is shown in Fig. 10 (c). It turns out that Pearl's belief propagation algorithm <ref> [31] </ref> operating on a Bayesian network is equivalent to the sum-product algorithm operating on the corresponding factor graph. Equations similar to Pearl's belief updating and bottom-up/top-down propagation rules [31, pp. 182-183] can easily be derived from the general sum-product algorithm update equations (19) and (20) as follows. <p> It turns out that Pearl's belief propagation algorithm [31] operating on a Bayesian network is equivalent to the sum-product algorithm operating on the corresponding factor graph. Equations similar to Pearl's belief updating and bottom-up/top-down propagation rules <ref> [31, pp. 182-183] </ref> can easily be derived from the general sum-product algorithm update equations (19) and (20) as follows. <p> If, in a Bayesian network, an edge is directed from vertex p to vertex c then p is a parent of c and c is a child of p. Messages sent among between variables are always functions of the parent p. In <ref> [31] </ref>, a message sent from p to c is denoted c (p), while a message sent from c to p is denoted as c (p), as shown in Fig. 24 for the specific Bayesian network of Fig. 10 (c). Consider the central variable, x 3 in Fig. 24. <p> a (x), x (a) = @ d2d (x) Y p2a (x)nfag x (p) A # a: (28) and, for every d 2 d (x), d (x) = c2d (x)nfdg c (x) @ f (xja (x)) a2a (x) 1 The termination condition for cycle-free graphs, called the "belief update" equation in <ref> [31] </ref>, is given by the product of the messages received by x in the factor graph: BEL (x) = d2d (x) 0 Y x (a) A # x: (30) Pearl also introduces a scale factor in (29) and (30) so that the resulting messages properly represent probability mass functions. <p> The relative complexity of (28)-(30) compared with the simplicity of the sum-product update rule given in Section 4 provides a strong pedagogical incentive for the introduction of factor graphs. Pearl also presents an algorithm called "belief revision" in <ref> [31] </ref>; in our terms, belief revision is the "max-product" version of the sum-product algorithm, applied to the factor graph corresponding to a Bayesian network.
Reference: [32] <author> C. J. Preston, </author> <title> Gibbs States on Countable Sets. </title> <publisher> Cambridge University Press, </publisher> <year> 1974. </year>
Reference-contexts: and a variable node; the latter is called the variable associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields <ref> [19, 21, 32] </ref>, Bayesian networks [20, 31] and Tanner graphs [35, 38, 39]. <p> In other words, G is an MRF if every variable v is independent of non-neighboring variables in the graph, given the values of its immediate neighbors. MRFs are well developed in statistics, and have been used in a variety of applications (see, e.g., <ref> [21, 32, 19, 18] </ref>). Kschischang and Frey [22] give a brief discussion of the use of MRFs to describe codes. Recall that a clique in a graph is a collection of vertices which are all pairwise neighbors.
Reference: [33] <author> L. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, </volume> <pages> pp. 257-286, </pages> <year> 1989. </year>
Reference-contexts: This algorithm was developed earlier in the statistics literature [5] and perhaps even earlier in classified work due to L. R. Welch [29]. In the signal processing literature, and particularly in speech processing, this algorithm is widely known as the forward-backward algorithm <ref> [33] </ref>. Pearl's belief propagation and belief revision algorithms, widely applied in expert systems and in artificial intelligence, turn out to be examples of the sum-product algorithm operating in a Bayesian network; see [31, 20] for textbook treatments. <p> Hidden Markov models are widely used in a variety of applications; see, e.g., <ref> [33] </ref> for a tutorial emphasizing applications in signal processing. The strong resemblance between the factor graphs of Fig. 9 (c) and (d) and the factor graphs representing trellises (Figs. 6 (b) and 7) is not accidental; trellises can be viewed as Markov models for codes. <p> Note that if we were interested in the APPs for the s vector, or for the x vector, these could also be computed by the forward/backward algorithm. See <ref> [33] </ref> for a tutorial on some of the applications of the forward/backward algorithm to applications in signal processing. 5.2 The Min-Sum Semiring and the Viterbi Algorithm Suppose now, rather than being interested in the APPs for the individual symbols, we are interested in determining which valid codeword has largest APP.
Reference: [34] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning representations by back-propagating errors," </title> <journal> Nature, </journal> <volume> vol. 323, </volume> <pages> pp. 533-536, </pages> <year> 1986. </year>
Reference-contexts: neural networks, signal processing, optimization, and coding theory. 20 Example 10. (Computer vision and neural network models) Graphical models have found an impressive place in the field of neural network models of perception [18, 17, 9, 12]. (See [11] for a textbook treatment.) Traditional artificial neural networks called "multilayer perceptrons" <ref> [34] </ref> treat perceptual inference as a function approximation problem. For example, the perceptron's objective might be to predict the relative shift between two images, providing a way to estimate depth. Initially, the perceptron's parameters are set to random values and the perceptron is very bad at predicting depth.
Reference: [35] <author> R. M. Tanner, </author> <title> "A recursive approach to low complexity codes," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 533-547, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields [19, 21, 32], Bayesian networks [20, 31] and Tanner graphs <ref> [35, 38, 39] </ref>. <p> Neural network formulations of factor graphs have also been used for unsupervised learning and density estimation; see, e.g., [11]. In coding theory, Tanner <ref> [35] </ref> generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. Tanner's approach was later generalized to graphs with hidden (state) variables by Wiberg, et al. [39, 38].
Reference: [36] <author> A. Vardy, </author> <title> "Trellis Structure of Codes," </title> <note> to appear as a chapter in Handbook of Coding Theory, </note> <editor> (V. S. Pless, W. C. Huffman, R. A. Brualdi, eds). </editor> <publisher> Amsterdam: Elsevier Science Publishers, </publisher> <year> 1998. </year>
Reference-contexts: Since every code can be represented by a trellis (see <ref> [36] </ref> for a recent survey of results in the theory of the trellis structure of codes), this shows that a cycle-free factor graph exists for every code (in fact, for every set membership function).
Reference: [37] <author> S. Verdu and H. V. </author> <title> Poor, "Abstract dynamic programming models under commu-tativity conditions," </title> <journal> SIAM J. on Control and Optimization, </journal> <volume> vol. 25, </volume> <pages> pp. 990-1006, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where
Reference: [38] <author> N. Wiberg, </author> <title> Codes and Decoding on General Graphs. </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1996. </year>
Reference-contexts: associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields [19, 21, 32], Bayesian networks [20, 31] and Tanner graphs <ref> [35, 38, 39] </ref>. <p> Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> In coding theory, Tanner [35] generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. Tanner's approach was later generalized to graphs with hidden (state) variables by Wiberg, et al. <ref> [39, 38] </ref>. In coding theory, much of the current interest in so-called "soft-output" decoding algorithms stems from the near-capacity-achieving performance of turbo codes, introduced by Berrou, et al. [7]. <p> We denote this binary operation by `+', thinking of "sum" for "summary." As in <ref> [1, 2, 23, 28, 37, 38] </ref>, we will insist that the summary operator + satisfy the distributive law in R, i.e., for all x; y; z 2 R, x (y + z) = (x y) + (x z); (17) i.e., we will require that (R; +; ) form a semiring, where
Reference: [39] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> "Codes and iterative decoding on general graphs," Europ. </title> <journal> Trans. Telecomm., </journal> <volume> vol. 6, </volume> <pages> pp. 513-525, </pages> <address> Sept/Oct. </address> <year> 1995. </year>
Reference-contexts: associated with the given edge, and is denoted by x fv;wg . 1.2 Prior Art We will see in Section 2 that factor graphs subsume many other graphical models in signal processing, probability theory, and coding, including Markov random fields [19, 21, 32], Bayesian networks [20, 31] and Tanner graphs <ref> [35, 38, 39] </ref>. <p> Our original motivation for introducing factor graphs was to make explicit the commonalities between Bayesian networks (also known as belief networks, causal networks, and influence diagrams) and Tanner graphs, both of which had previously been used to explain the iterative decoding of turbo codes and low-density parity check codes <ref> [11, 22, 25, 26, 30, 38, 39] </ref>. In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. [39]. <p> In that respect, factor graphs and their applications to coding are just a slight reformulation of the approach of Wiberg, et al. <ref> [39] </ref>. However, a main thesis of this paper is that factor graphs may naturally be used in a wide variety of fields other than coding, including signal processing, system theory, expert systems, and artificial neural networks. <p> In coding theory, Tanner [35] generalized Gallager's bipartite graph approach to low-complexity codes and also developed versions of the sum-product algorithm. Tanner's approach was later generalized to graphs with hidden (state) variables by Wiberg, et al. <ref> [39, 38] </ref>. In coding theory, much of the current interest in so-called "soft-output" decoding algorithms stems from the near-capacity-achieving performance of turbo codes, introduced by Berrou, et al. [7].
Reference: [40] <author> J. C. Willems, </author> <title> "Models for Dynamics," in Dynamics Reported, Volume 2 (U. </title> <editor> Kirch-graber and H. O. Walther, eds). </editor> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <pages> pp. 171-269, </pages> <year> 1989. </year>
Reference-contexts: Such functions are often interpreted as models| set theoretic or probabilistic, respectively|of a physical system. For example, Willems' system theory <ref> [40] </ref> starts from the view that a "system" (i.e., a model) is simply a set 8 of allowed trajectories in some configuration space. Factorizations of such functions can give important structural information about the model.
References-found: 40

