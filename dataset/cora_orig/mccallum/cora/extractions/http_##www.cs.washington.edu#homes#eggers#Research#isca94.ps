URL: http://www.cs.washington.edu/homes/eggers/Research/isca94.ps
Refering-URL: http://www.cs.washington.edu/homes/eggers/Research/multithread.html
Root-URL: http://www.cs.washington.edu
Email: fradhika,eggersg@cs.washington.edu  
Title: Impact of Sharing-Based Thread Placement on Multithreaded Architectures  
Author: Radhika Thekkath and Susan J. Eggers 
Address: Seattle 98195  
Affiliation: Dept. of Computer Science Engg., University of Washington,  
Abstract: To test this hypothesis, we compared a variety of thread placement algorithms via trace-driven simulation of fourteen coarse- and medium-grain parallel applications on several multithreaded architectures. Our results contradict the hypothesis. Rather than decreasing, compulsory and invalidation misses remained nearly constant across all placement algorithms, for all processor configurations, even with an infinite cache. That is, sharing-based placement had no (positive) effect on execution time. Instead, load balancing was the critical factor that affected performance. Our results were due to one or both of the following reasons: (1) the sequential and uniform access of shared data by the application's threads and (2) the insignificant number of data references that require interconnect access, relative to the total number of instructions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: In a few rare situations, e.g., Patch with sixteen processors and LOAD-BAL, we observed thrashing when two co-located threads frequently conflicted for the same cache block. This phenomenon has also been reported by Agar-wal <ref> [1] </ref>. The thrashing processor had an order of magnitude more inter-thread conflict misses than other processors, and therefore took longer to complete execution. <p> Weber and Gupta [23] estimated, via simulation, the extent to which a multithreaded architecture can overcome the effects of long access latencies. They measured processor efficiency by varying the number of contexts and found substantial improvement. Agarwal <ref> [1] </ref> presents an analytical performance model that incorporates network traffic, cache interference, context switching overhead and the number of hardware contexts. The paper also has a cache model that takes into account the interference in the cache due to multithreading.
Reference: [2] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under MACH. </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction There are two memory-system related reasons for sub-linear speedup in multiprocessors. First, data sharing among threads scheduled on different processors can lead to excessive data movement and higher network traffic <ref> [2, 8] </ref>. Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete [4, 10, 12, 18].
Reference: [3] <author> A. Agarwal, B-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> Proceedings of 17th ISCA, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We also assume that all instruction references take 1 cycle, i.e., they always hit in a separate I-cache. We assume a multipath network and do not explicitly model network contention. Instead, we use a latency value of 50 cycles, approximating the average memory latency of a moderately-loaded Alewife-style multiprocessor <ref> [3] </ref>. We assume a local memory access time of 8 cycles. We vary the number of processors and hardware contexts to study the impact, if any, on caches that are stressed to different degrees.
Reference: [4] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete <ref> [4, 10, 12, 18] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [5] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> Proceedings of the 17th ISCA, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Since the computation phase for Barnes-Hut is 1.6 million instructions per thread for our input data set, thread length dominates the write-sharing effect. FFT provides another example. A detailed analysis of the FFT application used in our suite <ref> [5] </ref>, shows that 73% of all shared elements are migratory, i.e., they are accessed in long write runs 2 . Other Presto programs have similar sequential access patterns which account for the small amount of runtime coherence traffic. 2 Write runs are sequences of writes by a single thread.
Reference: [6] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Applications and their Characteristics We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks [17], and medium-grain applications that ran under the Presto parallel programming environment <ref> [6, 22] </ref>. We use the length and number of threads in an application as a measure of its granularity. Coarse-grain programs have fewer, but longer threads, 6.4 million instructions on the average, but as high as 100 million instructions (Table 2).
Reference: [7] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> Proceedings of ASPLOS IV, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The simulator simulates a distributed shared-memory multiprocessor in which processors are connected by a simple multipath interconnection network and cache coherency is maintained with a distributed, directory-based cache coherency protocol <ref> [7] </ref>. The simulator comprises three modules: processor, cache and the interconnection network. Each processor models multiple hardware contexts and a round-robin context switch policy. A context switch takes 6 cycles, the time to drain the execution pipeline.
Reference: [8] <author> S. J. Eggers and R. H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> Proceedings of ASPLOS III, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction There are two memory-system related reasons for sub-linear speedup in multiprocessors. First, data sharing among threads scheduled on different processors can lead to excessive data movement and higher network traffic <ref> [2, 8] </ref>. Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete [4, 10, 12, 18]. <p> With an infinite cache, capacity and conflict misses are eliminated and some conflict misses become invalidation misses <ref> [8] </ref>; thus, coherency operations dominate interconnect traffic. We ran a set of experiments to study this issue. We compared the load balanced version (LOAD-BAL) with the best (static) sharing-based algorithm for each application and the (dynamic) coherency traffic algorithm.
Reference: [9] <author> S. J. Eggers, D. R. Keppel, E. J. Koldinger, and H. M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-46, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The simulator modeled a shared-memory multiprocessor whose processors have multiple hardware contexts. Program traces (both data and instruction) from fourteen explicitly parallel applications were generated using the MPtrace <ref> [9] </ref> parallel tracing tool on a Sequent Symmetry [19]. Certain characteristics of the applications were extracted from the trace files and fed to the placement algorithms, which in turn produced maps associating threads with processors. Both maps and program traces were input to the simulator.
Reference: [10] <author> R. H. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> Proceedings of 15th ISCA, </booktitle> <pages> pages 443-451, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete <ref> [4, 10, 12, 18] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [11] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel programs. </title> <note> Submitted for publication. </note>
Reference-contexts: Programs traces were statically analyzed for characteristics that provided the cluster-combining criteria. A sharing-based placement algorithm that uses information gathered in this way can be approximated by a compiler, using summary side-effect analysis that detects per-thread memory accesses <ref> [11] </ref>. We did not study placement algorithms that rely on program runtime behavior, such as the order of shared accesses that cause interconnect operations, which would be practically impossible to determine at compile-time. Table 3 shows values of several of the measured characteristics for both workloads. <p> Applications with thread length imbalances, i.e., higher deviations from the mean, should benefit more from load balanced placements. Considered together, these values indicate that sharing have a significant amount of false sharing. Shared data in Topopt and Pverify were statically restructured <ref> [11] </ref> to eliminate false sharing. After restructuring, false sharing misses were 1.5% (Pverify) and 1.7% (Topopt) of the total data misses with a 32 KByte cache. The remaining programs had very little false sharing in the original source, from as little as 0.2% (Grav) to 5.8% (Water). <p> In either case, the programs have been optimized for data locality. For most of the programs the programmer was responsible for the partitioning; in two of them (Pverify and Topopt) locality-enhancing compiler optimizations automatically restructured the shared data to achieve the same effect <ref> [11] </ref>. For example, Barnes-Hut which does N-body simulation, computes in each time step the net force on a set of particles and updates their position and velocity. The interaction with other particles decreases with distance; hence the algorithm is parallelized by a spatial partitioning of particles into contiguous zones.
Reference: [12] <author> K. Kurihara, D. Chaiken, and A. Agarwal. </author> <title> Latency tolerance through multithreading in large-scale multiprocessing. </title> <booktitle> International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 91-101, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete <ref> [4, 10, 12, 18] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [13] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> Supercomputing `92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [13, 14, 20] </ref>; we use it for performance comparison. (8) Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling in medium-grain, explicitly parallel programs [21], and fine-grain scheduling of loop iterations <ref> [13, 14, 20] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [14] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [13, 14, 20] </ref>; we use it for performance comparison. (8) Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling in medium-grain, explicitly parallel programs [21], and fine-grain scheduling of loop iterations <ref> [13, 14, 20] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [15] <author> E. Rothberg, J.P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes, and node granularity issues for large-scale multiprocessors. </title> <booktitle> Proceedings of the 20th ISCA, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The cache unit models a direct-mapped cache with a hit time of 1 cycle. Restricted by the practical limit on trace lengths, we scaled down both the data set size and the cache size <ref> [15] </ref> to maintain a realistic ratio between the two. Health and the coarse-grain programs use a 32 KByte cache and the other medium-grain programs use a 64 KByte cache. The cache unit maintains separate statistics on the individual cache miss components of compulsory, intra-thread conflict, inter-thread conflict and invalidation misses.
Reference: [16] <author> R. H. Saavedra-Barrera, D. E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> 2nd ACM SPAA, </booktitle> <pages> pages 169-178, </pages> <year> 1990. </year>
Reference-contexts: The paper's main conclusion vindicated multithreading for a wide range of architectural parameters, provided there existed sufficient network bandwidth. It also showed that high cache miss rates can hurt multithreading performance and that applications with active data sharing improves it. Saavedra-Barrera et al. <ref> [16] </ref> developed a Markov chain model for multithreaded processor efficiency that uses the number of contexts, the network latency, context switch times and remote reference rate. They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies.
Reference: [17] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The next two subsections discuss in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Applications and their Characteristics We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks <ref> [17] </ref>, and medium-grain applications that ran under the Presto parallel programming environment [6, 22]. We use the length and number of threads in an application as a measure of its granularity.
Reference: [18] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, Real-Time Signal Processing IV, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies by context switching between threads and executing useful instructions while waiting for memory accesses to complete <ref> [4, 10, 12, 18] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [19] <institution> Symmetry Technical Summary. Sequent Computer Systems. </institution>
Reference-contexts: The simulator modeled a shared-memory multiprocessor whose processors have multiple hardware contexts. Program traces (both data and instruction) from fourteen explicitly parallel applications were generated using the MPtrace [9] parallel tracing tool on a Sequent Symmetry <ref> [19] </ref>. Certain characteristics of the applications were extracted from the trace files and fed to the placement algorithms, which in turn produced maps associating threads with processors. Both maps and program traces were input to the simulator.
Reference: [20] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> Proceedings of ICPP, </booktitle> <pages> pages II:246-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [13, 14, 20] </ref>; we use it for performance comparison. (8) Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling in medium-grain, explicitly parallel programs [21], and fine-grain scheduling of loop iterations <ref> [13, 14, 20] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [21] <author> R. Vaswani and J. Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> Proceedings of the 13th SOSP, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling in medium-grain, explicitly parallel programs <ref> [21] </ref>, and fine-grain scheduling of loop iterations [13, 14, 20]. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [22] <author> D. B. Wagner. </author> <title> Conservative Parallel Discrete-Event Simulation: Principles and Practice. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <address> Seattle, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Applications and their Characteristics We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks [17], and medium-grain applications that ran under the Presto parallel programming environment <ref> [6, 22] </ref>. We use the length and number of threads in an application as a measure of its granularity. Coarse-grain programs have fewer, but longer threads, 6.4 million instructions on the average, but as high as 100 million instructions (Table 2).
Reference: [23] <author> W-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> Proceedings of 16th ISCA, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: As before, these results are due to program characteristics of sequential and/or uniform inter-thread sharing. 5 Related Work Past research in the performance of multithreaded architectures has examined the tradeoffs between the number of contexts, the network latency and context switch times. Weber and Gupta <ref> [23] </ref> estimated, via simulation, the extent to which a multithreaded architecture can overcome the effects of long access latencies. They measured processor efficiency by varying the number of contexts and found substantial improvement.
References-found: 23

