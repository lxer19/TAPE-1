URL: http://www.csi.uottawa.ca/tanka/uploadable/tr9803.ps
Refering-URL: http://www.csi.uottawa.ca/tanka/papers.html
Root-URL: 
Email: -sscott,stan-@csi.uottawa.ca  sscott@csi.uottawa.ca  
Phone: (613) 565-5770  
Title: Page 1 Using Lexical Knowledge in Text Classification  
Author: Sam Scott and Stan Matwin Sam Scott 
Keyword: Key Words text classification, background knowledge, change of representation, WordNet  
Note: Contact Author  (Submitted to ICML-98, 25/02/98)  
Address: 150 Louis Pasteur, Ottawa, Ontario K1N 6N5 Canada  Ottawa TR-98-03)  
Affiliation: Department of Computer Science University of Ottawa  (University of  
Abstract: This paper describes several experiments in text classification using WordNet, a rich source of lexical background knowledge available in the public domain. WordNet is used to map the original words from a text into sets based on synonym and hypernym relationships. This information is used to compute a change of representation from bag of words to hypernym density . Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using each representation. Experiments show that for some of the more difficult tasks the hypernym density representation leads to significantly more accurate and more comprehensible rules. 
Abstract-found: 1
Intro-found: 1
Reference: <author> [Bergadano, Giordana 88]F. Bergadano and A. Giordana. </author> <title> A knowledge-intensive Approach to Concept Induction. </title> <booktitle> In Proc. ICML-88, </booktitle> <pages> 305-317, </pages> <year> 1988. </year>
Reference: [Brill 92] <author> Eric Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing , ACL, </booktitle> <year> 1992. </year>
Reference-contexts: In order to take advantage of the part of speech organization of the synsets, an automatic part of speech tagger can be used. For this study, the public domain supervised tagger by Eric Brill was chosen <ref> [Brill 92] </ref>. 4. Using WordNet to Accomplish a Change of Representation The proposed algorithm for computing the change of representation requires three passes through the corpus.
Reference: [Clark, Matwin 93] <author> P. Clark and S. Matwin. </author> <title> Learning Domain Theories Using Abstract Background Knowledge. </title> <booktitle> In Proc. ECML-93, </booktitle> <pages> 360-365, </pages> <year> 1993. </year>
Reference-contexts: Related Work In general, the idea of using domain knowledge in an inductive system that learns classification rules from example is not new (e.g. see [Bergadano, Giordana 88], <ref> [Clark, Matwin 93] </ref>). However, little work has been done using available NLP resources for text classification. Recent work by [Rodrguez et. al. 97] is an exception. In their study, WordNet was used in conjunction with a neural network to classify texts from the Reuters corpus.
Reference: [Cohen 95] <author> William W. Cohen. </author> <title> Fast Effective Rule Induction. </title> <booktitle> In Proc. </booktitle> <address> ICML-95. Lake Tahoe, California, </address> <year> 1995. </year>
Reference-contexts: In all cases the classes were made completely disjoint by removing any overlapping examples. 2 The machine learning algorithm chosen for this study was Ripper, a rule-based learner developed by William Cohen <ref> [Cohen 95] </ref>. Ripper was specifically designed to handle the high dimensionality of bag of words text classification by being fast and using set-valued features [Cohen 96].
Reference: [Cohen 96] <author> William W. Cohen. </author> <title> Learning Trees and Rules with Set-valued Features. </title> <booktitle> In Proc. AAAI-96, </booktitle> <year> 1996 </year>
Reference-contexts: Ripper was specifically designed to handle the high dimensionality of bag of words text classification by being fast and using set-valued features <ref> [Cohen 96] </ref>. Table 1 shows that our intuitions about the difficulty of the three corpora for bag of words classification are valid in the case of the Ripper algorithm.
Reference: [Greenhaus 96] <author> Dick Greenhaus. </author> <title> About the Digital Tradition . (www.mudcat.org/DigiTrad-blurb.html), 1996. </title> <type> 8 </type>
Reference-contexts: In keeping with previous studies, we used topic headings as the basis for the Reuters classification tasks and newsgroup names as the basis for the USENET tasks. The third corpus, DigiTrad is a public domain collection of 6500 folk song lyrics <ref> [Greenhaus 96] </ref>. To aid searching, the owners of DigiTrad have assigned to each song one or more key words from a fixed list. Some of these key words capture taxonomic information (e.g. "Irish" or "British") while others relate to subject matter (e.g. "murder" or "marriage").
Reference: [Joachims 97] <author> T. Joachims. </author> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. </title> <booktitle> In Proc. ICML-97, </booktitle> <pages> 143-146, </pages> <year> 1997. </year>
Reference-contexts: No linguistic processing (other than a stop list of most frequent words) was applied to the original text. Two learning techniques, tf-idf weighting 1 and MDL, were applied. Similarly <ref> [Joachims 97] </ref> and [Koller, Sahami 97] also used the bag of words approach in text classification systems. In general, the bag of words is the main, if not the only representation used in most text classification experiments and applications.
Reference: [Koller, Sahami 96] <author> D. Koller and M. Sahami. </author> <title> Hierarchically Classifying Documents Using Very Few Words. </title> <booktitle> In Proc. ICML-97, </booktitle> <pages> 170-176, </pages> <year> 1997. </year>
Reference: [Lang 95] <author> K. Lang. NewsWeeder: </author> <title> Learning to Filter News. </title> <booktitle> In Proc. ICML-95, </booktitle> <pages> 331-336, </pages> <year> 1995. </year> <note> 8 The DigiTrad database itself is at www.deltablues.com/folksearch.html Page 16 </note>
Reference-contexts: 1. Introduction Text classification is an important application of Machine Learning that can be used in a variety of contexts, including email and news filtering, personal information agents and assistants, information retrieval, and automatic indexing. A number of authors have experimented with automatic text classification systems. In a typical paper, <ref> [Lang 95] </ref> investigated how text classification could be applied to filtering news to match the interests of a given user. The classification problem was therefore a two-class problem with a very large feature set.
Reference: [Li et. al. 95] <author> Xiaobin Li, Stan Szpakowicz and Stan Matwin. </author> <title> A WordNet-based Algorithm for Word Sense Disambiguation. </title> <booktitle> In Proc. IJCAI-95 , Montral, </booktitle> <address> Canada, </address> <year> 1995. </year>
Reference-contexts: In the current study, the words number in the tens of thousands. Manual disambiguation is not an option. Automatic disambiguation is possible using WordNet, but the procedure is complex and may require the use of a parser <ref> [Li et. al. 95] </ref>. Instead all senses returned by WordNet are judged equally likely to 5 Note that WordNet also maps words to their roots. For instance, the word "guns" would map to the word "gun" and the word "am" maps to the word "be".
Reference: [Michalski 86] <author> R. Michalski. </author> <title> Understanding the nature of Learning: Issues and Research Directions. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell, eds., </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. II, </volume> <publisher> Morgan Kaufmann 1986.. </publisher>
Reference-contexts: If one is to see the approach presented here as an integrated system, which takes texts as input, automatically engineers the new features using WordNet, and then learns classification rules, then the results shown above represent an example of constructive learning <ref> [Michalski 86] </ref>. 5.3 Discussion Clearly the change of representation to hypernym density greatly improves classification accuracy in some cases. Why does it sometimes not work? In the case of the Reuters tasks, the lack of benefit is not a particular worry.
Reference: [Miller 95] <author> G. Miller. </author> <title> WordNet: a lexical database for English. </title> <journal> Communications of the ACM, </journal> <volume> 38(11), </volume> <year> 1995. </year>
Reference-contexts: Page 5 difficulties posed by the text itself. 3. WordNet WordNet is a public domain online lexical reference system that captures a number of important conceptual relations in addition to the synonymy and antonymy found in most thesauri <ref> [Miller 95] </ref>. 3 These relations can be used to engineer a change of representation in text data by transforming vectors of words into vectors of word meanings.
Reference: [NLM 98] <author> National Library of Medicine. </author> <title> Unified Medical Language System Overview. </title> <note> www.nlm.nih.gov/research/umls/UMLSDOC.HTML, February, </note> <year> 1998. </year>
Reference-contexts: More sophisticated word sense disambiguation (but still short of full syntax analysis) could produce more accurate hypernym density features, and the use of other linguistic resources available in the public domain, such as the Unified Medical Language System Metathesaurus <ref> [NLM 98] </ref>, could improve classifier performance in knowledge Page 15 domains which are semantically close and highly expert. Acknowledgments The authors are grateful to William Cohen for making the Ripper system available for this research.
Reference: [Rodrguez et. al. 97] <author> Manuel de Buenaga Rodrguez, Jos Mara Gmez-Hidalgo and Beln Daz-Agudo. </author> <title> Using WordNet to Complement Training Information in Text Categorization. </title> <booktitle> In Proc. </booktitle> <address> RANLP-97. Stanford, March 25-27, </address> <year> 1997 </year>
Reference-contexts: It has been observed that the topic headings in Reuters tend to consist of words that appear frequently in the text, and this observation has been exploited to help improve classification accuracy <ref> [Rodrguez et. al. 97] </ref>. DigiTrad and USENET are good examples of the opposite extreme. The texts in DigiTrad make heavy use of metaphoric, rhyming, unusual and archaic language. Often the lyrics do not explicitly state what a song is about. <p> Ideally, a word sense disambiguation algorithm could be used to resolve some of this latter overlap by mapping each word to its correct meaning. In the only other published application of WordNet to classification, word sense disambiguation was performed by manual inspection <ref> [Rodrguez et. al. 97] </ref>. This approach was feasible in the context of that study because of the small number of words involved. In the current study, the words number in the tens of thousands. Manual disambiguation is not an option. <p> Related Work In general, the idea of using domain knowledge in an inductive system that learns classification rules from example is not new (e.g. see [Bergadano, Giordana 88], [Clark, Matwin 93]). However, little work has been done using available NLP resources for text classification. Recent work by <ref> [Rodrguez et. al. 97] </ref> is an exception. In their study, WordNet was used in conjunction with a neural network to classify texts from the Reuters corpus.
Reference: [Weiss et. al. 96] <author> Scott A. Weiss, Simon Kasif, and Eric Brill. </author> <title> Text Classification in USENET Newsgroups: A Progress Report. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access , Bulgaria, </booktitle> <month> September 11-13, </month> <year> 1996. </year>
Reference-contexts: Both Reuters and USENET have been the subject of previous studies in machine learning (see [Koller, Sahami 97] for a study of Reuters and <ref> [Weiss et. al. 96] </ref> for a study of USENET). In keeping with previous studies, we used topic headings as the basis for the Reuters classification tasks and newsgroup names as the basis for the USENET tasks.
References-found: 15

