URL: http://www.cs.mu.oz.au/tr_db/mu_94_08.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Optimal Clustering of Relations in a Database System  
Author: Evan P. Harris Kotagiri Ramamohanarao 
Address: Parkville, Victoria 3052 Australia  
Affiliation: Department of Computer Science The University of Melbourne  
Pubnum: Technical Report 94/8  
Abstract: The placement of records can significantly affect the performance of query processing in a database system. By making use of the query patterns and their frequencies, we can design multi-attribute hash indexes to optimally cluster records. In this paper, we first describe algorithms for relational operations which exploit this type of clustering and the costs involved. We then present various optimization techniques for finding the optimal clustering. We show that, when the queries are processed using these algorithms, the cost of the average query can be dramatically reduced. Our results show that performance gains with a factor of 30 are feasible when compared with standard schemes. Moreover, the clustering scheme is stable, even when the probability distribution of queries changes over time significantly. Our results show that if the probabilities are changed by up to 80%, the original clustering scheme remains near-optimal. We also show that when a new clustering scheme is required the data can usually be reorganized efficiently. Each relation can be updated independently, in linear time proportional to the size of the relation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: Therefore, the only combinatorial optimization technique we considered was simulated annealing. 5.1.1 Simulated annealing There are a number of variations of algorithms which may be called simulated annealing (SA) <ref> [1] </ref>. The algorithm we used performs a number of trials, T , and returns the bit allocation associated with the best value for the cost function from amongst the trials. <p> That is, each new probability, p 0 , was calculated from the old probability, p, using the formula p 0 = p (1 P + 2P ffi p ): The value of ffi p is a different random number in the range <ref> [0; 1] </ref> for each probability and P is the maximum amount to change by, and is in the range [0; 1]. We also generated new probability distributions for the random distributions. <p> the old probability, p, using the formula p 0 = p (1 P + 2P ffi p ): The value of ffi p is a different random number in the range <ref> [0; 1] </ref> for each probability and P is the maximum amount to change by, and is in the range [0; 1]. We also generated new probability distributions for the random distributions. In the case of rndt2, the probability of each operation was taken from a uniform distribution to contrast with the distribution it originally had which favoured some of its attributes.
Reference: [2] <author> A. V. Aho and J. D. Ullman. </author> <title> Optimal partial-match retrieval when fields are independently specified. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 168-179, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: In the past these problems have been considered independently. In this paper we are generalizing and extending previous work which searched for the optimal buffer allocation for joins, and searched for an optimal multi-attribute hash index for a single relation for partial match queries and join queries <ref> [2, 17, 19] </ref>. Table 2 defines the functions which are used in the algorithms given below. Table 3 defines the variables and symbols to be used in the cost analyses which were not given in Table 1. Consider two relations, R and S.
Reference: [3] <author> R. Bayer and K. Unterauer. </author> <title> Prefix B-trees. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 2(1) </volume> <pages> 11-26, </pages> <month> March </month> <year> 1977. </year> <month> 55 </month>
Reference-contexts: Variations on the grid file include the Freeston's BANG file [13]. Extensions to the linear hash file include the recursive linear hashing of Ramamohanarao and Sacks-Davis [39]. Extensions to B-trees include the prefix B-trees of Bayer and Unterauer <ref> [3] </ref> and the multidimensional B-trees of Ouksel and Scheuermann [36]. Superimposed coding schemes have been extended to a two level scheme by Sacks-Davis and Ramamohanarao [42] and to the multi-organizational scheme by Kent et al. [21].
Reference: [4] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative search-ing. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 509-517, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods. These include linear hash files, multilevel grid files [46], BANG files [13], multidimensional B-trees [36] and k-d trees <ref> [4] </ref>. We use the creation of optimal hash indexes as an example of our approach. We refer to the generation of hash keys from multiple attributes as multi-attribute hashing. A linear hash file indexed using multi-key hashing is referred to as having a multi-attribute hash index. <p> This method of allocating equal resources to each attribute is very common in situ ations in which there is no knowledge of the query distribution. For example, it was suggested as the method of splitting dimensions for the kd-tree <ref> [4] </ref> and grid file [35]. In our domain, we assume that the probability of each attribute occurring in a query is known. Thus, we add a minor optimization to this technique. We 29 assume that the most probable attribute is allocated a bit first.
Reference: [5] <author> M. W. Blasgen and K. P. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4), </volume> <year> 1977. </year>
Reference-contexts: The most popular join algorithms include the nested loop, sort-merge and hash join algorithms. They were surveyed by Mishra and Eich [32]. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments <ref> [5, 6, 11, 23] </ref>. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join [11]. The other relational operations have not received as much attention as either the selection or join.
Reference: [6] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operations. </title> <editor> In U. Dayal, G. Schlageter, and L. H. Seng, editors, </editor> <booktitle> Proceedings of the Tenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The most popular join algorithms include the nested loop, sort-merge and hash join algorithms. They were surveyed by Mishra and Eich [32]. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments <ref> [5, 6, 11, 23] </ref>. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join [11]. The other relational operations have not received as much attention as either the selection or join.
Reference: [7] <author> W. A. Burkhard. </author> <title> Interpolation-based index maintenance. </title> <journal> BIT, </journal> <volume> 23 </volume> <pages> 274-294, </pages> <year> 1983. </year>
Reference-contexts: In other data structures it is very expensive. For example, in a linear hash file a search must be performed on every distinct value within the range unless an order-preserving hash function is used, as suggested by Burkhard <ref> [7] </ref>. Unless the order-preserving hash function is carefully chosen to match the data it is much more likely to result in long overflow chains caused by skewed data distributions. Much research has been done on improving implementations of the join operation because it is both common and expensive. <p> This was the method used in the original papers on linear hashing by Litwin [27] and Larson [25]. This is not the case when the index is required to be order preserving. For example, Burkhard <ref> [7] </ref>, Chen et al. [9], and Harris and Ramamohanarao [18] require order preserving indexes when working with range queries. If an index is required to be ordered preserving, each hash function h i is required to be order preserving. <p> Partial match range queries using hash files indexed with multiple keys are discussed in more detail in <ref> [7, 9, 18] </ref>. Figure 5 shows an algorithm for implementing the select operation when a single value for an attribute is involved.
Reference: [8] <author> A. F. Cardenas. </author> <title> Analysis and performance of inverted database structures. </title> <journal> Communications of the ACM, </journal> <volume> 18(5) </volume> <pages> 253-263, </pages> <month> May </month> <year> 1975. </year>
Reference-contexts: 1 Introduction One of the principle issues in database implementation is the design of suitable data structures and algorithms which access these data structures to implement the operations required by database users. The most common data structures used are B-trees [10], inverted files <ref> [8] </ref>, binary coded (descriptor) schemes [41], grid files [35], R-trees [14] and linear hash files [25, 27]. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries.
Reference: [9] <author> C. Y. Chen, C. C. Chang, and R. C. T. Lee. </author> <title> Optimal MMI file systems for orthogonal range queries. </title> <journal> Information Systems, </journal> <volume> 18(1) </volume> <pages> 37-54, </pages> <year> 1993. </year>
Reference-contexts: These domains include partial match retrieval [28], partial match retrieval on dynamic files [29], and partial match retrieval across multiple files [40]. Recently this work has been extended to include partial match range queries <ref> [9, 18] </ref>. In this paper we generalize these results. Our method supports all of the relational operations described above. We provide algorithms for implementing the operations and derive their costs. <p> This was the method used in the original papers on linear hashing by Litwin [27] and Larson [25]. This is not the case when the index is required to be order preserving. For example, Burkhard [7], Chen et al. <ref> [9] </ref>, and Harris and Ramamohanarao [18] require order preserving indexes when working with range queries. If an index is required to be ordered preserving, each hash function h i is required to be order preserving. <p> Partial match range queries using hash files indexed with multiple keys are discussed in more detail in <ref> [7, 9, 18] </ref>. Figure 5 shows an algorithm for implementing the select operation when a single value for an attribute is involved. <p> Two of the methods used have been minimal marginal increase and simulated annealing. While minimal marginal increase has performed well in some domains, such as determining optimal multi-attribute hash indexes for partial match retrieval for point [29] and range <ref> [9, 18] </ref> queries, in more complex domains it has not.
Reference: [10] <author> D. Comer. </author> <title> The ubiquitous B-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2) </volume> <pages> 121-138, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: 1 Introduction One of the principle issues in database implementation is the design of suitable data structures and algorithms which access these data structures to implement the operations required by database users. The most common data structures used are B-trees <ref> [10] </ref>, inverted files [8], binary coded (descriptor) schemes [41], grid files [35], R-trees [14] and linear hash files [25, 27]. The strengths and weaknesses of a data structure determines the situations in which it is most applicable.
Reference: [11] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <editor> In B. Yormark, editor, </editor> <booktitle> Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, USA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The most popular join algorithms include the nested loop, sort-merge and hash join algorithms. They were surveyed by Mishra and Eich [32]. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments <ref> [5, 6, 11, 23] </ref>. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join [11]. The other relational operations have not received as much attention as either the selection or join. <p> Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments [5, 6, 11, 23]. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join <ref> [11] </ref>. The other relational operations have not received as much attention as either the selection or join. This is primarily because the implementation of the other operations, except projection, is similar to that of the join. Thus, the same basic approach can be taken. <p> In this paper we describe the results of using all these methods in combination with a multi-attribute hash index on the data file. The most significant of the hash join algorithm variations is the hybrid hash join of DeWitt et al. <ref> [11] </ref>. Shapiro [43] showed that it is particularly useful when a large amount of main memory is available, and Kitsuregawa et al. [22] showed that skewed data distributions could be handled after modifications to the algorithm. <p> It takes advantage of the fact that the nested loop algorithm can be performed with a single scan of the input relations if one relation can be completely contained in memory. Here we describe the GRACE hash join [23] and hybrid hash join <ref> [11] </ref>. 3.5.1 GRACE hash join During the first phase of the GRACE hash join, the input relations are recursively hash partitioned into pairs (one per relation) of smaller partitions until one partition in each pair can be contained in memory. <p> If this number varies during the partitioning, then the term P i should be replaced by Q i where P j is the number of new partitions created from a single partition during stage j, and P 0 = 1. 3.5.2 Hybrid hash join The hybrid hash join algorithm <ref> [11] </ref> is similar to the GRACE hash join algorithm. The primary differences is that it has only one partitioning pass, and during the partitioning an area of memory is set aside to perform joins in.
Reference: [12] <author> M. J. Folk and B. Zoellick. </author> <title> File structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, USA, 2 edition, </address> <year> 1992. </year>
Reference-contexts: This is a valid assumption in extent based file systems, which do just that <ref> [12] </ref>. Furthermore, many commercially available file systems, such as that of SunOS [30], now implement features of extent based file systems and provide similar per formance. * The records are uniformly distributed amongst all pages in the data file. Over flow pages are not considered in our cost formulae. <p> For example, if the load factor is 80% with 50 records per block, the multiplying factor is 1.0725 (the unsuccessful search length). The method for calculating these multiplying factors for various load and blocking factors can be found elsewhere <ref> [12, 38] </ref>. We believe that the method of dealing with non-uniform data distributions using actual rather than expected partition sizes can also be applied in this situation [17]. 3.2 Cost preliminaries Table 1 contains the notation used in the cost functions in the remainder of this section.
Reference: [13] <author> M. Freeston. </author> <title> The BANG file: a new kind of grid file. </title> <editor> In U. Dayal and I. Traiger, editors, </editor> <booktitle> Proceedings of the 1987 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 260-269, </pages> <address> San Francisco, California, USA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Hash files have the advantage that they require no additional data structures of indexing information. There have been many variations and extensions proposed for these data structures. Variations on the grid file include the Freeston's BANG file <ref> [13] </ref>. Extensions to the linear hash file include the recursive linear hashing of Ramamohanarao and Sacks-Davis [39]. Extensions to B-trees include the prefix B-trees of Bayer and Unterauer [3] and the multidimensional B-trees of Ouksel and Scheuermann [36]. <p> In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods. These include linear hash files, multilevel grid files [46], BANG files <ref> [13] </ref>, multidimensional B-trees [36] and k-d trees [4]. We use the creation of optimal hash indexes as an example of our approach. We refer to the generation of hash keys from multiple attributes as multi-attribute hashing.
Reference: [14] <author> A. Guttman. R-trees: </author> <title> a dynamic index structure for spatial searching. </title> <editor> In B. Yormark, editor, </editor> <booktitle> Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 47-57, </pages> <address> Boston, MA, USA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The most common data structures used are B-trees [10], inverted files [8], binary coded (descriptor) schemes [41], grid files [35], R-trees <ref> [14] </ref> and linear hash files [25, 27]. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries.
Reference: [15] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <editor> In Y. Kambayashi, editor, </editor> <booktitle> Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Based on the assumption that a disk seek is much more costly than a disk block transfer, a different cost model was described <ref> [15] </ref> which only counted disk seeks. Using this cost model an equal number of blocks should be allocated to each relation.
Reference: [16] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Optimal storage management of relations for join operations. </title> <type> Technical Report 92/2, </type> <institution> Department of Computer Science, The University of Melbourne, </institution> <address> Parkville, Victoria 3052, Australia, </address> <month> February </month> <year> 1992. </year> <note> Also published as CITRI Technical Report 92/15. </note>
Reference-contexts: There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used <ref> [16, 18, 20, 29, 40] </ref>, possibly augmented by heuristics [44]. Two of the methods used have been minimal marginal increase and simulated annealing.
Reference: [17] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Join algorithm costs revisited. </title> <type> Technical Report 93/5, </type> <institution> Department of Computer Science, The University of Melbourne, </institution> <address> Parkville, Victoria 3052, Australia, </address> <month> April </month> <year> 1993. </year> <note> Also published as CITRI Technical Report 93/6. </note>
Reference-contexts: a more accurate cost model, in which the disk seek time, disk transfer time, and CPU cost of the join algorithms are taken into account, we showed that neither of the previous two methods produces the lowest possible cost, and described an algorithm to search for the optimal buffer allocation <ref> [17] </ref>. The cost of the join can be reduced by a factor of two to three by using the more accurate model of the join algorithm. This was further substantiated by experimental results. <p> We use this cost model in this paper. 3.1 Assumptions We make the following assumptions. 5 * The distribution of records to partitions in the hash join algorithms is uni-form. Skewed data distributions can be handled using one of a number of schemes <ref> [17, 22, 26] </ref>. * A small amount of memory is available in addition to that provided for buffering the pages from disk. For example, we allow an algorithm to require a pointer or two for each page of memory. <p> The method for calculating these multiplying factors for various load and blocking factors can be found elsewhere [12, 38]. We believe that the method of dealing with non-uniform data distributions using actual rather than expected partition sizes can also be applied in this situation <ref> [17] </ref>. 3.2 Cost preliminaries Table 1 contains the notation used in the cost functions in the remainder of this section. A join operation consists of taking two relations and producing a result relation, R. <p> = ~ B R T K + V R T T C nb = C read:1 + C create:hashtable +C read:2:initial + C join:initial +C read:2:other + C join:other + C write:R : We describe an effective algorithm for minimizing this cost function with respect to the buffer sizes in <ref> [17] </ref>. 3.4 Sort-merge join The sort-merge join works in two phases, a sort phase and a merge phase. In the sort phase, each relation is sorted based on the join attributes. In the merge phase, each sorted relation is scanned sequentially and records with matching join attributes are joined. <p> One of these is partitioning the relations in place when using the hash join algorithm <ref> [17] </ref>. Partitioning the relations in place incurs an overhead of 2P 1 pages. For example, consider a situation in which P of the spare pages contain x 1 records, where x is the number of records which can be contained within a page. <p> In the past these problems have been considered independently. In this paper we are generalizing and extending previous work which searched for the optimal buffer allocation for joins, and searched for an optimal multi-attribute hash index for a single relation for partial match queries and join queries <ref> [2, 17, 19] </ref>. Table 2 defines the functions which are used in the algorithms given below. Table 3 defines the variables and symbols to be used in the cost analyses which were not given in Table 1. Consider two relations, R and S. <p> We now discuss how to find the optimal buffer allocation for each of the algorithms we will be using. We have previously examined the question of finding optimal buffer allocations for answering join queries, primarily for the nested loop and GRACE hash join algorithms <ref> [17] </ref>. This was done without having the benefit of indexes on any of the relations. We now provide an overview of the method we used to find a low cost buffer allocation when there are indexes on the relations. This method is a variation on the one previously examined. <p> The sum of these buffer sizes should be equal to the amount of main memory available, B. The buffer size of the outer relation is the most important, and should be a divisor of the size of the outer relation, V 1 , <ref> [17] </ref>. It should be as large as possible, providing that sufficient memory is available for reasonable values of B 2 and B R . <p> In the past we stated that the cost of determining the optimal buffer allocation was too high using a minimization algorithm similar to the one that minimizes the GRACE hash algorithm and that simulated annealing should be used instead <ref> [17] </ref>. However, as we will be using simulated annealing to determine the best bit allocation, to use it for each bit allocation tested to determine the best buffer allocation would be prohibitively expensive. <p> The first simplification made is to assume that there will only be one partitioning pass. If the outer relation is greater than several times the size of main memory the GRACE hash algorithm works as well as the hybrid hash algorithm <ref> [17] </ref>. Thus, if more than one partitioning pass is required we should use the GRACE hash minimization algorithm because it is simpler and more likely to produce optimal results. The basis of the hybrid hash minimization algorithm works as follows.
Reference: [18] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Optimal dynamic multi-attribute hashing for range queries. </title> <journal> BIT, </journal> <volume> 33 </volume> <pages> 561-579, </pages> <year> 1993. </year> <month> 56 </month>
Reference-contexts: These domains include partial match retrieval [28], partial match retrieval on dynamic files [29], and partial match retrieval across multiple files [40]. Recently this work has been extended to include partial match range queries <ref> [9, 18] </ref>. In this paper we generalize these results. Our method supports all of the relational operations described above. We provide algorithms for implementing the operations and derive their costs. <p> This was the method used in the original papers on linear hashing by Litwin [27] and Larson [25]. This is not the case when the index is required to be order preserving. For example, Burkhard [7], Chen et al. [9], and Harris and Ramamohanarao <ref> [18] </ref> require order preserving indexes when working with range queries. If an index is required to be ordered preserving, each hash function h i is required to be order preserving. The d i bits are then taken from the most significant bits of the bit string of the ith attribute. <p> Partial match range queries using hash files indexed with multiple keys are discussed in more detail in <ref> [7, 9, 18] </ref>. Figure 5 shows an algorithm for implementing the select operation when a single value for an attribute is involved. <p> There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used <ref> [16, 18, 20, 29, 40] </ref>, possibly augmented by heuristics [44]. Two of the methods used have been minimal marginal increase and simulated annealing. <p> Two of the methods used have been minimal marginal increase and simulated annealing. While minimal marginal increase has performed well in some domains, such as determining optimal multi-attribute hash indexes for partial match retrieval for point [29] and range <ref> [9, 18] </ref> queries, in more complex domains it has not.
Reference: [19] <author> E. P. Harris and K. Ramamohanarao. </author> <title> Using optimized multi-attribute hash indexes for hash joins. </title> <editor> In R. Sacks-Davis, editor, </editor> <booktitle> Proceedings of the Fifth Australasian Database Conference, </booktitle> <pages> pages 92-111, </pages> <address> Christchurch, New Zealand, </address> <month> January </month> <year> 1994. </year> <title> Global Publications Services. </title>
Reference-contexts: In the past these problems have been considered independently. In this paper we are generalizing and extending previous work which searched for the optimal buffer allocation for joins, and searched for an optimal multi-attribute hash index for a single relation for partial match queries and join queries <ref> [2, 17, 19] </ref>. Table 2 defines the functions which are used in the algorithms given below. Table 3 defines the variables and symbols to be used in the cost analyses which were not given in Table 1. Consider two relations, R and S. <p> It did not perform well when determining optimal multi-attribute hash indexes for partial match retrieval for multiple files [40], or for determining optimal multi-attribute hash indexes for hash join operations when only considering one relation and using a limited cost model <ref> [19] </ref>. As the domain we are examining is more complex than either of the latter two, we did not consider that minimal marginal increase would provide acceptable results. <p> While simulated annealing is not ideal for all optimization applications, as shown by Nahar et al. [34], it has proved to be a useful means of obtaining optimal indexes in applications of this type before in which the optimization is performed rarely <ref> [19, 40] </ref>. It has also been suggested for use as a basis for other techniques in query optimization. <p> The values of the constants for the simulated annealing algorithms are shown in Table 5 and are the same, or similar, to those used in <ref> [19, 40] </ref>. All results were generated on a SPARCserver 1000 and the times shown are the sum of the system and user time taken by each algorithm. 6.2 Performance of multi-attribute hash indexes memory sizes from 64 blocks (512 kbyte) to 16384 blocks (128 Mbyte).
Reference: [20] <author> Y. E. Ioannidis and Y. C. Kang. </author> <title> Randomized algorithms for optimizing large join queries. </title> <editor> In H. Garcia-Molina and H. V. Jagadish, editors, </editor> <booktitle> Proceedings of the 1990 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 312-321, </pages> <address> Atlantic City, NJ, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used <ref> [16, 18, 20, 29, 40] </ref>, possibly augmented by heuristics [44]. Two of the methods used have been minimal marginal increase and simulated annealing. <p> best trial then set the best allocation to be the current allocation end if end procedure function cooling value (cost, chain cost, trial) cooling value exp (( chain cost cost )=( cooling const fl ( control const flfl trial ))) end function the two phase optimization of Ioannidis and Kang <ref> [20] </ref>, and optimization in parallel execution spaces by Lanzelotte et al. [24].
Reference: [21] <author> A. J. Kent, R. Sacks-Davis, and K. Ramamohanarao. </author> <title> A signature file scheme based on multiple organizations for indexing very large text databases. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(7) </volume> <pages> 508-534, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Extensions to B-trees include the prefix B-trees of Bayer and Unterauer [3] and the multidimensional B-trees of Ouksel and Scheuermann [36]. Superimposed coding schemes have been extended to a two level scheme by Sacks-Davis and Ramamohanarao [42] and to the multi-organizational scheme by Kent et al. <ref> [21] </ref>. In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods. These include linear hash files, multilevel grid files [46], BANG files [13], multidimensional B-trees [36] and k-d trees [4].
Reference: [22] <author> M. Kitsuregawa, M. Nakayama, and M. Takagi. </author> <title> The effect of bucket size tuning in the dynamic hybrid GRACE hash join method. </title> <editor> In P. M. G. Apers and G. Wiederhold, editors, </editor> <booktitle> Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 257-266, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The most significant of the hash join algorithm variations is the hybrid hash join of DeWitt et al. [11]. Shapiro [43] showed that it is particularly useful when a large amount of main memory is available, and Kitsuregawa et al. <ref> [22] </ref> showed that skewed data distributions could be handled after modifications to the algorithm. Zeller and Gray [47] described a method similar to the hybrid hash join that adapts when the amount of main memory available to perform the join with changes as it is executing. <p> We use this cost model in this paper. 3.1 Assumptions We make the following assumptions. 5 * The distribution of records to partitions in the hash join algorithms is uni-form. Skewed data distributions can be handled using one of a number of schemes <ref> [17, 22, 26] </ref>. * A small amount of memory is available in addition to that provided for buffering the pages from disk. For example, we allow an algorithm to require a pointer or two for each page of memory.
Reference: [23] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1) </volume> <pages> 66-74, </pages> <year> 1983. </year>
Reference-contexts: The most popular join algorithms include the nested loop, sort-merge and hash join algorithms. They were surveyed by Mishra and Eich [32]. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments <ref> [5, 6, 11, 23] </ref>. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join [11]. The other relational operations have not received as much attention as either the selection or join. <p> They were surveyed by Mishra and Eich [32]. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments [5, 6, 11, 23]. The variations of the hash join algorithm considered in this paper are the GRACE hash join <ref> [23] </ref> and the hybrid hash join [11]. The other relational operations have not received as much attention as either the selection or join. This is primarily because the implementation of the other operations, except projection, is similar to that of the join. Thus, the same basic approach can be taken. <p> It was recently surveyed by Mishra and Eich [32]. Initially, many believed that the sort-merge join algorithm gave the best implementation of the join [31]. However, the development of the GRACE hash join algorithm by Kitsuregawa et al. <ref> [23] </ref>, and a number of other similar techniques based on the hash join, has shown that hash based techniques are better under many circumstances. In this paper we describe the results of using all these methods in combination with a multi-attribute hash index on the data file. <p> It takes advantage of the fact that the nested loop algorithm can be performed with a single scan of the input relations if one relation can be completely contained in memory. Here we describe the GRACE hash join <ref> [23] </ref> and hybrid hash join [11]. 3.5.1 GRACE hash join During the first phase of the GRACE hash join, the input relations are recursively hash partitioned into pairs (one per relation) of smaller partitions until one partition in each pair can be contained in memory.
Reference: [24] <author> R. S. G. Lanzelotte, P. Valduriez, and M. </author> <title> Za it. On the effectiveness of optimization search strategies for parallel execution spaces. </title> <editor> In R. Agrawal, S. Baker, and D. Bell, editors, </editor> <booktitle> Proceedings of the Nineteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 493-504, </pages> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: allocation end if end procedure function cooling value (cost, chain cost, trial) cooling value exp (( chain cost cost )=( cooling const fl ( control const flfl trial ))) end function the two phase optimization of Ioannidis and Kang [20], and optimization in parallel execution spaces by Lanzelotte et al. <ref> [24] </ref>. Note that the toured simulated annealing of Lanzelotte et al. [24] is simply simulated annealing with multiple trials using a different, application specific, method of determining the starting point for each trial. 5.1.2 Heuristic bit allocation techniques Simulated annealing is computationally expensive. <p> trial) cooling value exp (( chain cost cost )=( cooling const fl ( control const flfl trial ))) end function the two phase optimization of Ioannidis and Kang [20], and optimization in parallel execution spaces by Lanzelotte et al. <ref> [24] </ref>. Note that the toured simulated annealing of Lanzelotte et al. [24] is simply simulated annealing with multiple trials using a different, application specific, method of determining the starting point for each trial. 5.1.2 Heuristic bit allocation techniques Simulated annealing is computationally expensive.
Reference: [25] <author> P. A. Larson. </author> <title> Linear hashing with partial expansions. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 224-232, </pages> <address> Mon-treal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: The most common data structures used are B-trees [10], inverted files [8], binary coded (descriptor) schemes [41], grid files [35], R-trees [14] and linear hash files <ref> [25, 27] </ref>. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries. <p> vector is: b 2 1 b 4 1 b 4 2 b 3 2.1 Dynamic hashing If a linear hash file is required to be dynamic, that is, the number of blocks in the file changes over time, linear hashing with partial expansions, as described by Litwin [27] and Larson <ref> [25] </ref>, can be used to organize the file. Instead of increasing the file size directly from 2 d to 2 d+1 , the size is increased one block at a time. <p> That is, bit b i 1 is the least significant bit of the bit string of attribute i. This was the method used in the original papers on linear hashing by Litwin [27] and Larson <ref> [25] </ref>. This is not the case when the index is required to be order preserving. For example, Burkhard [7], Chen et al. [9], and Harris and Ramamohanarao [18] require order preserving indexes when working with range queries.
Reference: [26] <author> R. J. Lipton, J. F. Naughton, and D. A. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <editor> In H. Garcia-Molina and H. V. Jagadish, editors, </editor> <booktitle> Proceedings of the 1990 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-11, </pages> <address> Atlantic City, NJ, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: We use this cost model in this paper. 3.1 Assumptions We make the following assumptions. 5 * The distribution of records to partitions in the hash join algorithms is uni-form. Skewed data distributions can be handled using one of a number of schemes <ref> [17, 22, 26] </ref>. * A small amount of memory is available in addition to that provided for buffering the pages from disk. For example, we allow an algorithm to require a pointer or two for each page of memory.
Reference: [27] <author> W. Litwin. </author> <title> Linear hashing: a new tool for file and table addressing. </title> <booktitle> In Proceedings of the Sixth International Conference on Very Large Data Bases, </booktitle> <pages> pages 212-223, </pages> <address> Montreal, Canada, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: The most common data structures used are B-trees [10], inverted files [8], binary coded (descriptor) schemes [41], grid files [35], R-trees [14] and linear hash files <ref> [25, 27] </ref>. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries. <p> a valid choice vector is: b 2 1 b 4 1 b 4 2 b 3 2.1 Dynamic hashing If a linear hash file is required to be dynamic, that is, the number of blocks in the file changes over time, linear hashing with partial expansions, as described by Litwin <ref> [27] </ref> and Larson [25], can be used to organize the file. Instead of increasing the file size directly from 2 d to 2 d+1 , the size is increased one block at a time. <p> That is, bit b i 1 is the least significant bit of the bit string of attribute i. This was the method used in the original papers on linear hashing by Litwin <ref> [27] </ref> and Larson [25]. This is not the case when the index is required to be order preserving. For example, Burkhard [7], Chen et al. [9], and Harris and Ramamohanarao [18] require order preserving indexes when working with range queries.
Reference: [28] <author> J. W. Lloyd. </author> <title> Optimal partial-match retrieval. </title> <journal> BIT, </journal> <volume> 20 </volume> <pages> 406-413, </pages> <year> 1980. </year>
Reference-contexts: Finding optimal multi-attribute hash indexes to minimize the average query cost has been shown to be NP-hard, even for partial match retrieval [33]. However, some heuristic algorithms have been shown to efficiently produce optimal, or near optimal, solutions in some domains. These domains include partial match retrieval <ref> [28] </ref>, partial match retrieval on dynamic files [29], and partial match retrieval across multiple files [40]. Recently this work has been extended to include partial match range queries [9, 18]. In this paper we generalize these results. Our method supports all of the relational operations described above.
Reference: [29] <author> J. W. Lloyd and K. Ramamohanarao. </author> <title> Partial-match retrieval for dynamic files. </title> <journal> BIT, </journal> <volume> 22 </volume> <pages> 150-168, </pages> <year> 1982. </year>
Reference-contexts: However, some heuristic algorithms have been shown to efficiently produce optimal, or near optimal, solutions in some domains. These domains include partial match retrieval [28], partial match retrieval on dynamic files <ref> [29] </ref>, and partial match retrieval across multiple files [40]. Recently this work has been extended to include partial match range queries [9, 18]. In this paper we generalize these results. Our method supports all of the relational operations described above. <p> There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used <ref> [16, 18, 20, 29, 40] </ref>, possibly augmented by heuristics [44]. Two of the methods used have been minimal marginal increase and simulated annealing. <p> Two of the methods used have been minimal marginal increase and simulated annealing. While minimal marginal increase has performed well in some domains, such as determining optimal multi-attribute hash indexes for partial match retrieval for point <ref> [29] </ref> and range [9, 18] queries, in more complex domains it has not.
Reference: [30] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proceedings of the USENIX 1991 Winter Conference, </booktitle> <pages> pages 33-43, </pages> <address> Dallas, Texas, USA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: This is a valid assumption in extent based file systems, which do just that [12]. Furthermore, many commercially available file systems, such as that of SunOS <ref> [30] </ref>, now implement features of extent based file systems and provide similar per formance. * The records are uniformly distributed amongst all pages in the data file. Over flow pages are not considered in our cost formulae. Overflow pages do not affect the relative performance of the algorithms.
Reference: [31] <author> T. H. Merrett. </author> <title> Why sort-merge gives the best implementation of the natural join. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 13(2) </volume> <pages> 39-51, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: It was recently surveyed by Mishra and Eich [32]. Initially, many believed that the sort-merge join algorithm gave the best implementation of the join <ref> [31] </ref>. However, the development of the GRACE hash join algorithm by Kitsuregawa et al. [23], and a number of other similar techniques based on the hash join, has shown that hash based techniques are better under many circumstances.
Reference: [32] <author> P. Mishra and M. H. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year> <month> 57 </month>
Reference-contexts: Algorithms implementing the join attempt to minimize the number of times each relation must be read and written. The most popular join algorithms include the nested loop, sort-merge and hash join algorithms. They were surveyed by Mishra and Eich <ref> [32] </ref>. Various combinations of join algorithms have been compared elsewhere in a variety of different 1 environments [5, 6, 11, 23]. The variations of the hash join algorithm considered in this paper are the GRACE hash join [23] and the hybrid hash join [11]. <p> It was recently surveyed by Mishra and Eich <ref> [32] </ref>. Initially, many believed that the sort-merge join algorithm gave the best implementation of the join [31].
Reference: [33] <author> S. Moran. </author> <title> On the complexity of designing optimal partial-match retrieval systems. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(4) </volume> <pages> 543-551, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: Finding optimal multi-attribute hash indexes to minimize the average query cost has been shown to be NP-hard, even for partial match retrieval <ref> [33] </ref>. However, some heuristic algorithms have been shown to efficiently produce optimal, or near optimal, solutions in some domains. These domains include partial match retrieval [28], partial match retrieval on dynamic files [29], and partial match retrieval across multiple files [40].
Reference: [34] <author> S. Nahar, S. Sahni, and E. Shargowitz. </author> <title> Experiments with simulated annealing. </title> <booktitle> In Proceedings of the 22nd Design Automation Conference, </booktitle> <pages> pages 748-752, </pages> <year> 1985. </year>
Reference-contexts: An algorithm detailing this simulated annealing algorithm is given in Figure 10. While simulated annealing is not ideal for all optimization applications, as shown by Nahar et al. <ref> [34] </ref>, it has proved to be a useful means of obtaining optimal indexes in applications of this type before in which the optimization is performed rarely [19, 40]. It has also been suggested for use as a basis for other techniques in query optimization.
Reference: [35] <author> J. Nievergelt, H. Hinterberger, and K. C. Sevcik. </author> <title> The grid file: An adaptable, symmetric multikey file structure. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(1) </volume> <pages> 38-71, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: The most common data structures used are B-trees [10], inverted files [8], binary coded (descriptor) schemes [41], grid files <ref> [35] </ref>, R-trees [14] and linear hash files [25, 27]. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries. <p> This method of allocating equal resources to each attribute is very common in situ ations in which there is no knowledge of the query distribution. For example, it was suggested as the method of splitting dimensions for the kd-tree [4] and grid file <ref> [35] </ref>. In our domain, we assume that the probability of each attribute occurring in a query is known. Thus, we add a minor optimization to this technique. We 29 assume that the most probable attribute is allocated a bit first.
Reference: [36] <author> M. Ouksel and P. Scheuermann. </author> <title> Multidimensional B-trees: analysis of dynamic behaviour. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 401-418, </pages> <year> 1981. </year>
Reference-contexts: Variations on the grid file include the Freeston's BANG file [13]. Extensions to the linear hash file include the recursive linear hashing of Ramamohanarao and Sacks-Davis [39]. Extensions to B-trees include the prefix B-trees of Bayer and Unterauer [3] and the multidimensional B-trees of Ouksel and Scheuermann <ref> [36] </ref>. Superimposed coding schemes have been extended to a two level scheme by Sacks-Davis and Ramamohanarao [42] and to the multi-organizational scheme by Kent et al. [21]. In this paper we are concerned with constructing indexes to minimize the cost of the average query. <p> In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods. These include linear hash files, multilevel grid files [46], BANG files [13], multidimensional B-trees <ref> [36] </ref> and k-d trees [4]. We use the creation of optimal hash indexes as an example of our approach. We refer to the generation of hash keys from multiple attributes as multi-attribute hashing. A linear hash file indexed using multi-key hashing is referred to as having a multi-attribute hash index.
Reference: [37] <author> H. Pang, M. J. Carey, and M. Livny. </author> <title> Partially preemptible hash joins. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 59-68, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This assumption may be relaxed at query execution time by using a partially preemptible hash join proposed by Pang et al. <ref> [37] </ref>. * Two pages with consecutive index values in a data file are located contiguously on the disk, and can therefore be retrieved with one read operation. This is a valid assumption in extent based file systems, which do just that [12].
Reference: [38] <author> K. Ramamohanarao and J. W. Lloyd. </author> <title> Dynamic hashing schemes. </title> <journal> The Computer Journal, </journal> <volume> 25 </volume> <pages> 478-485, </pages> <year> 1982. </year>
Reference-contexts: For example, if the load factor is 80% with 50 records per block, the multiplying factor is 1.0725 (the unsuccessful search length). The method for calculating these multiplying factors for various load and blocking factors can be found elsewhere <ref> [12, 38] </ref>. We believe that the method of dealing with non-uniform data distributions using actual rather than expected partition sizes can also be applied in this situation [17]. 3.2 Cost preliminaries Table 1 contains the notation used in the cost functions in the remainder of this section.
Reference: [39] <author> K. Ramamohanarao and R. Sacks-Davis. </author> <title> Recursive linear hashing. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(9) </volume> <pages> 369-391, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: There have been many variations and extensions proposed for these data structures. Variations on the grid file include the Freeston's BANG file [13]. Extensions to the linear hash file include the recursive linear hashing of Ramamohanarao and Sacks-Davis <ref> [39] </ref>. Extensions to B-trees include the prefix B-trees of Bayer and Unterauer [3] and the multidimensional B-trees of Ouksel and Scheuermann [36]. Superimposed coding schemes have been extended to a two level scheme by Sacks-Davis and Ramamohanarao [42] and to the multi-organizational scheme by Kent et al. [21]. <p> While some storage space is not used, the amount can be controlled by the user. If a variation on the basic storage technique is used, such as recursive linear hashing by Ramamo-hanarao and Sacks-Davis <ref> [39] </ref>, the storage utilization can be as high as 93% without significantly affecting performance. In linear hash indexing, the block in the data file in which a record is stored is determined by a hash value calculated for that record.
Reference: [40] <author> K. Ramamohanarao, J. Shepherd, and R. Sacks-Davis. </author> <title> Multi-attribute hashing with multiple file copies for high performance partial-match retrieval. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 404-423, </pages> <year> 1990. </year>
Reference-contexts: However, some heuristic algorithms have been shown to efficiently produce optimal, or near optimal, solutions in some domains. These domains include partial match retrieval [28], partial match retrieval on dynamic files [29], and partial match retrieval across multiple files <ref> [40] </ref>. Recently this work has been extended to include partial match range queries [9, 18]. In this paper we generalize these results. Our method supports all of the relational operations described above. We provide algorithms for implementing the operations and derive their costs. <p> There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used <ref> [16, 18, 20, 29, 40] </ref>, possibly augmented by heuristics [44]. Two of the methods used have been minimal marginal increase and simulated annealing. <p> It did not perform well when determining optimal multi-attribute hash indexes for partial match retrieval for multiple files <ref> [40] </ref>, or for determining optimal multi-attribute hash indexes for hash join operations when only considering one relation and using a limited cost model [19]. <p> While simulated annealing is not ideal for all optimization applications, as shown by Nahar et al. [34], it has proved to be a useful means of obtaining optimal indexes in applications of this type before in which the optimization is performed rarely <ref> [19, 40] </ref>. It has also been suggested for use as a basis for other techniques in query optimization. <p> The values of the constants for the simulated annealing algorithms are shown in Table 5 and are the same, or similar, to those used in <ref> [19, 40] </ref>. All results were generated on a SPARCserver 1000 and the times shown are the sum of the system and user time taken by each algorithm. 6.2 Performance of multi-attribute hash indexes memory sizes from 64 blocks (512 kbyte) to 16384 blocks (128 Mbyte).
Reference: [41] <author> C. S. Roberts. </author> <title> Partial-match retrieval via the method of superimposed codes. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 67(12) </volume> <pages> 1624-1642, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: 1 Introduction One of the principle issues in database implementation is the design of suitable data structures and algorithms which access these data structures to implement the operations required by database users. The most common data structures used are B-trees [10], inverted files [8], binary coded (descriptor) schemes <ref> [41] </ref>, grid files [35], R-trees [14] and linear hash files [25, 27]. The strengths and weaknesses of a data structure determines the situations in which it is most applicable. For example, R-trees are used to store multi-dimensional spatial data and can efficiently answer spatial queries.
Reference: [42] <author> R. Sacks-Davis and K. Ramamohanarao. </author> <title> A two level superimposed coding scheme for partial match retrieval. </title> <journal> Information Systems, </journal> <volume> 8(4) </volume> <pages> 273-280, </pages> <year> 1983. </year>
Reference-contexts: Extensions to B-trees include the prefix B-trees of Bayer and Unterauer [3] and the multidimensional B-trees of Ouksel and Scheuermann [36]. Superimposed coding schemes have been extended to a two level scheme by Sacks-Davis and Ramamohanarao <ref> [42] </ref> and to the multi-organizational scheme by Kent et al. [21]. In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods.
Reference: [43] <author> L. D. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: In this paper we describe the results of using all these methods in combination with a multi-attribute hash index on the data file. The most significant of the hash join algorithm variations is the hybrid hash join of DeWitt et al. [11]. Shapiro <ref> [43] </ref> showed that it is particularly useful when a large amount of main memory is available, and Kitsuregawa et al. [22] showed that skewed data distributions could be handled after modifications to the algorithm.
Reference: [44] <author> A. Swami. </author> <title> Optimization of large join queries: combining heuristic and combinatorial techniques. </title> <editor> In J. Clifford, B. Lindsay, and D. Maier, editors, </editor> <booktitle> Proceedings of the 1989 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 367-376, </pages> <address> Portland, Oregon, USA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: There is no known algorithmic solution for minimizing Equation 5 in polynomial time. For a variety of similar problems when the solution cannot be obtained in polynomial time combinatorial approaches to bit allocation have been used [16, 18, 20, 29, 40], possibly augmented by heuristics <ref> [44] </ref>. Two of the methods used have been minimal marginal increase and simulated annealing. While minimal marginal increase has performed well in some domains, such as determining optimal multi-attribute hash indexes for partial match retrieval for point [29] and range [9, 18] queries, in more complex domains it has not. <p> It has also been suggested for use as a basis for other techniques in query optimization. Some of these include the join query optimization of Swami <ref> [44] </ref>, 28 procedure simulated annealing for trial 1 to T do for each relation do generate a random allocation of bits to attributes end for while number of chains at best chain cost &lt; F do for each perturbing operation in the chain do randomly select a relation randomly select two
Reference: [45] <author> J. Vaghani, K. Ramamohanarao, D. B. Kemp, Z. Somogyi, and P. J. Stuckey. </author> <title> Design overview of the Aditi deductive database system. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 240-247, </pages> <address> Kobe, Japan, April 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: By sorting both relations the merge phase is performed in linear time in the number of blocks in the files. The sort-merge algorithm whose cost we present below is the version of used in the Aditi deductive database system <ref> [45] </ref>. During the sorting phase each relation is divided into sorted partitions, the size of which is the size of the memory buffer.
Reference: [46] <author> K.-Y. Whang and R. Krishnamurthy. </author> <title> The multilevel grid file | a dynamic hierarchical multidimensional file structure. </title> <booktitle> In International Symposium on Database Systems for Advanced Applications, </booktitle> <pages> pages 449-459, </pages> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In this paper we are concerned with constructing indexes to minimize the cost of the average query. Our technique can be applied to many multi-attribute indexing methods. These include linear hash files, multilevel grid files <ref> [46] </ref>, BANG files [13], multidimensional B-trees [36] and k-d trees [4]. We use the creation of optimal hash indexes as an example of our approach. We refer to the generation of hash keys from multiple attributes as multi-attribute hashing.
Reference: [47] <author> H. Zeller and J. Gray. </author> <title> An adaptive hash join algorithm for multiuser environments. </title> <editor> In D. McLeod, R. Sacks-Davis, and H. Schek, editors, </editor> <booktitle> Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 186-197, </pages> <address> Brisbane, Australia, </address> <month> August </month> <year> 1990. </year> <month> 58 </month>
Reference-contexts: Shapiro [43] showed that it is particularly useful when a large amount of main memory is available, and Kitsuregawa et al. [22] showed that skewed data distributions could be handled after modifications to the algorithm. Zeller and Gray <ref> [47] </ref> described a method similar to the hybrid hash join that adapts when the amount of main memory available to perform the join with changes as it is executing.
References-found: 47

