URL: http://www.cs.yale.edu/HTML/YALE/CS/Linda/papers/shortp.ps
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/Linda/ap_and_piranha.html
Root-URL: http://www.cs.yale.edu
Title: Adaptive Parallelism and Piranha  
Author: Nicholas Carriero Eric Freeman David Gelernter David Kaminsky 
Keyword: Parallelism, networks, multiprocessors, adaptive parallelism, programming techniques, Linda, Piranha.  
Date: February 25, 1994  
Address: New Haven, Connecticut 06520  
Affiliation: Department of Computer Science Yale University  
Abstract: Under "adaptive parallelism," the set of processors executing a parallel program may grow or shrink as the program runs. Potential gains include the capacity to run a parallel program on the idle workstations in a conventional LAN|processors join the computation when they become idle, and withdraw when their owners need them|and to manage the nodes of a dedicated multiprocessor efficiency. Experience to date with our Piranha system for adaptive parallelism suggests that these possibilities can be achieved in practice on real applications at comparatively modest costs. 
Abstract-found: 1
Intro-found: 1
Reference: [AG92] <author> Shakil Ahmed and David Gelernter. </author> <title> A CASE environment for parallel programming. </title> <booktitle> In Proc. Fifth International Workshop on Computer-Aided Software Engineering, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Retreat procedures execute under a system timer. If they exceed the allotted time they are terminated, and the entire piranha computation aborts. Only piranha processes are subject to interruption and retreat. The Linda Program Builder <ref> [AG92] </ref> supports parallel programming with a series of built-in templates and high-level operations (in addition to other services). The three-part piranha program framework is a template supported by the Program Builder.
Reference: [Agh86] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Processes are dynamically remapped among free processors as needed. When a processor withdraws, its processes are migrated somewhere else. Such an approach was discussed as long ago as the "MuNet" project and the early stages of Actors research <ref> [Agh86] </ref>; a variant of this approach formed the basis of the "Amber" adaptive parallelism system [CAL + 89]. Piranha, in contrast, is an adaptive version of master-worker parallelism (see [CG90]). Programmers specify in effect a single general purpose (but application specific) "worker function" called piranha ().
Reference: [CAL + 89] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: When a processor withdraws, its processes are migrated somewhere else. Such an approach was discussed as long ago as the "MuNet" project and the early stages of Actors research [Agh86]; a variant of this approach formed the basis of the "Amber" adaptive parallelism system <ref> [CAL + 89] </ref>. Piranha, in contrast, is an adaptive version of master-worker parallelism (see [CG90]). Programmers specify in effect a single general purpose (but application specific) "worker function" called piranha (). They do not create processes and their applications do not rely on any particular number of active processes.
Reference: [CFG94] <author> Nicholas Carriero, Eric Freeman, and David Gelernter. </author> <title> Adaptive parallelism on multiprocessors: Preliminary experience with Piranha on the CM-5. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: When a Piranha retreats, it releases its current task and the edges on which it depends. Performance data is given in Table 5.2. 5.3 Experiments with Multiprocessor Piranha In addition to our Network Piranha results, we have obtained results for our multiprocessor version of Piranha <ref> [CFG94] </ref> from runs of Piranha codes on a 64 node partition of a Connection Machine CM-5. 5 Our CM-5 Piranha implementation presents an essentially complete user-level Piranha system, but it does not offer fully adaptive run-time support.
Reference: [CG90] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs: A first course. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: Such an approach was discussed as long ago as the "MuNet" project and the early stages of Actors research [Agh86]; a variant of this approach formed the basis of the "Amber" adaptive parallelism system [CAL + 89]. Piranha, in contrast, is an adaptive version of master-worker parallelism (see <ref> [CG90] </ref>). Programmers specify in effect a single general purpose (but application specific) "worker function" called piranha (). They do not create processes and their applications do not rely on any particular number of active processes. <p> Even using a relative large set of nodes, Piranha performed 94% efficiently. 3 Note, though, that this is "wind assisted"|the computation to communication ratio is much higher for the Sparcstation 1's than for the RS/6000s. 5.2 DNA Sequence Similarity Assessment <ref> [CG90] </ref> discusses DNA sequence similarity assessment using a technique developed by Gotoh 4 . The algorithm requires a program structure that accommodates inter-task dependencies. The program computes the elements of a matrix starting at the top, left-hand corner.
Reference: [Fre94] <author> Eric Freeman. </author> <title> Piranha on the Connection Machine CM-5. </title> <type> Technical Report YALE/DCS/RR-1011, </type> <institution> Yale University, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: More details on these systems can be found in <ref> [Kam94, Fre94] </ref>. 6.1 Network Piranha An executing Network Piranha system comprises the run-time system of Piranha daemons (one per participating node), and a (possibly empty) set of Piranha user applications. Each Piranha daemon controls the behavior of its node with respect to Piranha applications.
Reference: [GK91] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Sixth ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: Our tests used the Atearth code described above and an electrical engineering application that performs dipole localization in biomagnetic imaging, also developed at Yale. The dipole code is a two-phase bag-of-tasks program: the first phase locates minima within a coarse grid, the second phase further refines the positioning <ref> [GK91] </ref>. We present the sequential and Piranha execution times of these programs in figure 5. The sequential version ran on a Sparc processor with the same performance as a CM-5 node. The piranhafied version ran on a 64-node CM-5 partition.
Reference: [Kam94] <author> David L. Kaminsky. </author> <title> Adaptive Parallelism with Piranha. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1994. </year>
Reference-contexts: More details on these systems can be found in <ref> [Kam94, Fre94] </ref>. 6.1 Network Piranha An executing Network Piranha system comprises the run-time system of Piranha daemons (one per participating node), and a (possibly empty) set of Piranha user applications. Each Piranha daemon controls the behavior of its node with respect to Piranha applications. <p> It has addressed, in the process, the questions and concerns raised earlier. AP is certainly appropriate for pure master/worker codes, but it is also applicable to codes involving low to moderate intertask dependencies. <ref> [Kam94] </ref> presents a methodology for structuring codes based on the nature of their intertask dependencies. [Kam94] also presents a detailed description of the underlying implementation. <p> It has addressed, in the process, the questions and concerns raised earlier. AP is certainly appropriate for pure master/worker codes, but it is also applicable to codes involving low to moderate intertask dependencies. <ref> [Kam94] </ref> presents a methodology for structuring codes based on the nature of their intertask dependencies. [Kam94] also presents a detailed description of the underlying implementation. The coding effort required for AP above and beyond what a "static" parallel code requires is a strong function, not surprisingly, of the complexity of the task interdependencies.
Reference: [LL90] <author> M. Litzkow and M. Livny. </author> <title> Experience with the Condor distributed batch system. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: One well-known approach to node recycling centers on job-level parallelism: users o*oad jobs from their workstations onto idle ones. Systems such as Condor <ref> [LL90] </ref>, Butler [Nic90], Sprite [OCD + 88], and others support this approach. Adaptive parallelism takes a different tack: instead of parceling out waiting user jobs to idle workstations, we spread a single user job over many workstations, thereby focusing the available power on one application.
Reference: [LM90] <author> A. K. Lenstra and M. Manasse. </author> <title> Factoring by electronic mail. </title> <booktitle> In Proceedings of Eurocrypt '89, number 173 in Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: present that a broad collection of useful applications can be refitted for effective AP execution and that AP and "fixed" jobs can co-exist peacefully. 3 The Piranha Model Several ad hoc systems have been designed to solve specific computational tasks adaptively| for example testing primality or computing traveling salesman tours <ref> [LM90] </ref>.
Reference: [Nic90] <author> D.A. Nichols. </author> <title> Multiprocessing in a Network of Workstations. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1990. </year>
Reference-contexts: One well-known approach to node recycling centers on job-level parallelism: users o*oad jobs from their workstations onto idle ones. Systems such as Condor [LL90], Butler <ref> [Nic90] </ref>, Sprite [OCD + 88], and others support this approach. Adaptive parallelism takes a different tack: instead of parceling out waiting user jobs to idle workstations, we spread a single user job over many workstations, thereby focusing the available power on one application.
Reference: [OCD + 88] <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(6) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: One well-known approach to node recycling centers on job-level parallelism: users o*oad jobs from their workstations onto idle ones. Systems such as Condor [LL90], Butler [Nic90], Sprite <ref> [OCD + 88] </ref>, and others support this approach. Adaptive parallelism takes a different tack: instead of parceling out waiting user jobs to idle workstations, we spread a single user job over many workstations, thereby focusing the available power on one application.
References-found: 12

