URL: http://www-sal.cs.uiuc.edu/~pitt/Papers/classic.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~pitt/
Root-URL: http://www.cs.uiuc.edu
Title: Classic Learning  
Author: MICHAEL FRAZIER LEONARD PITT Editor: Tom Hancock 
Keyword: Description logic, polynomial-time learning, Classic, subsumption, queries, knowledge acquisition  
Address: Abilene, TX 79699  1304 W. Springfield Avenue, Urbana, IL 61801  
Affiliation: Computer Science Department, Abilene Christian University,  Department of Computer Science, University of Illinois at Urbana-Champaign  
Note: 1-46 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: frazier@cs.acu.edu  pitt@cs.uiuc.edu  
Date: Received May 1, 1991  
Abstract: Description logics, also called terminological logics, are commonly used in knowledge-based systems to describe objects and their relationships. We investigate the learnability of a typical description logic, Classic, and show that Classic sentences are learnable in polynomial time in the exact learning model using equivalence queries and membership queries (which are in essence, "subsumption queries"|we show a prediction hardness result for the more traditional membership queries that convey information about specific individuals). We show that membership queries alone are insufficient for polynomial time learning of Classic sentences. Combined with earlier negative results (Cohen & Hirsh, 1994a) showing that, given standard complexity theoretic assumptions, equivalence queries alone are insufficient (or random examples alone in the PAC setting are insufficient), this shows that both sources of information are necessary for efficient learning in that neither type alone is sufficient. In addition, we show that a modification of the algorithm deals robustly with persistent malicious two-sided classification noise in the membership queries with the probability of a misclassification bounded below 1/2. Other extensions are considered. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75(2), </volume> <pages> 87-106. </pages>
Reference-contexts: Because the size of counterexamples may vary, and are not under the direct control of the learning algorithm, there are technical subtleties in defining the appropriate model for exact learning in polynomial time <ref> (Angluin, 1987) </ref>. We follow the standard convention and require that for efficient learning, at any point during the run of our algorithm, the time used up to that point must be polynomial in the longest counterexample seen so far.
Reference: <author> Angluin, D. </author> <year> (1988a). </year> <title> Learning with hints. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory (pp. </booktitle> <pages> 167-181). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Angluin, D. </author> <year> (1988b). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <pages> 319-342. </pages>
Reference-contexts: Our main focus is this latter viewpoint of learning general descriptions. Below we summarize our results, and describe the learning protocols used. 8 M. FRAZIER AND L. PITT 1.3. Learning Protocols We will employ two standard learning protocols: that of exactly learning from equivalence and membership queries <ref> (Angluin, 1988b) </ref>, and that of PAC learning (Valiant, 1984; Blumer et al., 1989) with membership queries. In both protocols, the learning algorithm may pose a membership query, which is an example x. <p> We consider the PAC learning model where the algorithm may ask membership queries in addition to receiving randomly generated examples. Consider the standard transformation <ref> (Angluin, 1988b) </ref> of Learn into a PAC algorithm: Each equivalence query is replaced by random sampling; each hypothesis is tested to see if it is probably approximately correct. If so, the hypothesis is kept and the algorithm terminates. Otherwise, a counterexample is obtained.
Reference: <author> Angluin, D. </author> <year> (1988c). </year> <title> Requests for hints that return no hints. </title> <type> Technical report YALEU/DCS/RR-647, </type> <institution> Yale University. </institution>
Reference: <author> Angluin, D. </author> <year> (1992). </year> <title> Computational learning theory: survey and selected bibliography. </title> <booktitle> Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 351-369). </pages> <address> Victoria, BC: </address> <publisher> ACM Press. </publisher>
Reference-contexts: CLASSIC LEARNING 11 1.5. Comparison to Previous Results Automating propositional concept discovery has been well studied <ref> (Angluin, 1992) </ref>. In comparison, efficient first-order learnability has been less well studied. Even so, some results are known.
Reference: <author> Angluin, D. </author> <year> (1994). </year> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University. </institution>
Reference: <author> Angluin, D., Frazier, M., & Pitt, L. </author> <year> (1992). </year> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 147-164. </pages>
Reference-contexts: CLASSIC LEARNING 11 1.5. Comparison to Previous Results Automating propositional concept discovery has been well studied <ref> (Angluin, 1992) </ref>. In comparison, efficient first-order learnability has been less well studied. Even so, some results are known.
Reference: <author> Angluin, D. & Kharitonov, M. </author> <year> (1995). </year> <title> When won't membership queries help? Journal of Computer and System Sciences, </title> <type> 50. </type>
Reference-contexts: positive or negative example of the target description, then our reduction still holds, and the problem of predicting Classic descriptions from individuals, using random examples and membership queries, is as hard as predicting polynomially sized Boolean circuits with the same information, and is intractable assuming the existence of one-way functions <ref> (Angluin & Kharitonov, 1995) </ref>. On the other hand, if a membership query is allowed to construct a "new individual", one that is not currently in the database, then we leave open the question of whether learning is possible. <p> Both of these problems are intractable given the existence of 1-way functions <ref> (Angluin & Kharitonov, 1995) </ref>. 7. Unions of Classic Concepts Classic lacks an OR construct. As such, let us comment briefly on a target consisting of a union of Classic concepts. Here we consider a positive example any concept that is subsumed by the union of the concepts in the target.
Reference: <author> Angluin, D. & Kri~kis, M. </author> <year> (1994). </year> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory (pp. </booktitle> <pages> 57-66). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Angluin, D. & Laird, P. </author> <year> (1988). </year> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <pages> 343-370. </pages>
Reference: <author> Angluin, D. & Slonim, D. K. </author> <year> (1994). </year> <title> Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1), </volume> <pages> 7-26. </pages>
Reference: <author> Auer, P. </author> <year> (1993). </year> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory (pp. </booktitle> <pages> 253-261). </pages> <address> New York: </address> <note> ACM Press. CLASSIC LEARNING 45 Beck, </note> <author> H., Gala, H., & Navathe, S. </author> <year> (1989). </year> <title> Classification as a query processing technique in the CANDIDE semantic model. </title> <booktitle> Data Engineering Conference (pp. </booktitle> <pages> 572-581). </pages> <address> Los Angeles, CA. </address>
Reference: <author> Blum, A. & Chalasani, P. </author> <year> (1992). </year> <title> Learning switching concepts. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 231-242). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4), </volume> <pages> 929-965. </pages>
Reference: <author> Bobrow, D. G. & Winograd, T. </author> <year> (1977). </year> <title> An overview of KRL, a knowledge representation language. </title> <journal> Cognitive Science, </journal> <volume> 1(1), </volume> <pages> 3-46. </pages>
Reference: <author> Borgida, A. </author> <year> (1992). </year> <title> Description logics are not just for the flightless-birds: a new look at the utility and foundations of description logics. </title> <type> Preprint. </type>
Reference: <author> Borgida, A., Brachman, R. J., McGuinness, D. L., & Resnick, L. </author> <year> (1989). </year> <title> CLASSIC: A structural data model for objects. </title> <booktitle> Proceedings of SIGMOD-89 Portland, </booktitle> <address> Oregon. </address>
Reference-contexts: Traditionally, computational learning theory has focused on propositional domains. We investigate learning in the first-order domain of description logics or terminological logics. Specifically we consider the learnability of the description logic known as Classic <ref> (Borgida et al., 1989) </ref>. To the extent that Classic is a typical description logic, our results generalize to a variety of other such logics. Description logics are more expressive than the propositional calculus.
Reference: <author> Borgida, A. & Patel-Schneider, P. F. </author> <year> (1992). </year> <title> A semantics and complete algorithm for subsumption in the CLASSIC description logic. </title> <type> Technical report, </type> <institution> AT&T. </institution>
Reference: <author> Brachman, R. J., Fikes, R. E., & Levesque, H. J. </author> <year> (1983). </year> <title> Krypton: A functional approach to knowledge representation. </title> <journal> IEEE Computer, </journal> <volume> 16(10), </volume> <pages> 67-73. </pages>
Reference: <author> Chen, Z. & Maass, W. </author> <year> (1994). </year> <title> On-line learning of rectangles and unions of rectangles. </title> <journal> Machine Learning, </journal> <volume> 17(2/3), </volume> <pages> 23-50. </pages>
Reference-contexts: However, for reasons that will become apparent in Section 8, we would like to avoid asking membership queries involving only changed vertex labels, so we do not present this approach here. As a second alternative, application of clever techniques <ref> (Chen & Maass, 1994) </ref> 3 would probably allow the same binary search on the AT-LEAST and AT-MOST constraint values to be performed, but without relying on membership queries. We leave the details as an exercise for the truly motivated reader.
Reference: <author> Cohen, W. </author> <year> (1993a). </year> <title> Cryptographic limitations on learning one-clause logic programs. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <address> Washington, D.C. </address>
Reference-contexts: In comparison, efficient first-order learnability has been less well studied. Even so, some results are known. Cohen (1993b) gives a PAC learning algorithm for function-free, two clause, closed, linearly recursive, ij-determinant logic programs; he also shows <ref> (Cohen, 1993a) </ref> that when the condition of linear recursiveness is relaxed, the learning problem becomes cryptographically hard. Page and Frisch (1992) show that constrained atoms (a typed logic) are efficiently learnable.
Reference: <author> Cohen, W. </author> <year> (1993b). </year> <title> Pac-learning a restricted class of recursive logic programs. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <address> Washington, D.C. </address>
Reference: <author> Cohen, W. W., Borgida, A., & Hirsh, H. </author> <year> (1992). </year> <title> Computing least common subsumers in description logics. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: that can serve as the actual vertex label lattice L, because ordering tuples hs 1 ; : : : ; s k i ht 1 ; : : : ; t k i exactly when every s i t i is in accordance with the notion of subsumption for Classic <ref> (Cohen et al., 1992) </ref>. We would like to exploit this similarity to labeled equivalence graphs by employing a prediction preserving reduction (Angluin & Kharitonov, 1995, Pitt & 22 M. FRAZIER AND L. PITT Warmuth, 1990) using the labeled equivalence graph learning algorithm developed in the previous section.
Reference: <author> Cohen, W. W. & Hirsh, H. </author> <year> (1994a). </year> <title> Learnability of description logics with equality constraints. </title> <journal> Machine Learning, </journal> <volume> 17(2/3), </volume> <pages> 169-199. </pages> <note> Special issue on Computational Learning Theory: COLT'92. </note>
Reference-contexts: It is known <ref> (Cohen & Hirsh, 1994a) </ref> that a Classic description subsumes a second Classic description iff the equivalence graph of the first subsumes that of the second. Examples will be labeled according to their relationship to the target under this partial ordering. <p> Examples will be labeled according to their relationship to the target under this partial ordering. Positive examples of an equivalence graph G will be exactly those graphs G 0 which are subsumed by G. Definition 3 <ref> (Cohen & Hirsh, 1994a) </ref> Let G 1 = (V 1 ; E 1 ) and G 2 = (V 2 ; E 2 ) be two equivalence graphs. The cross-product of G 1 and G 2 , denoted G 1 fi G 2 , is defined as follows. <p> Then G 1 sub sumes G 2 if the conditions of Definition 2 hold, and if in addition, ` G 2 (w) ` G 1 (w) for every G 1 -supported string w. It is shown in <ref> (Cohen & Hirsh, 1994a) </ref> that a Classic description subsumes a second Classic description iff the labeled equivalence graph of the first subsumes that of the second. Thus, positive examples of a labeled equivalence graph G will be graphs G 0 which are subsumed by G. <p> Thus, positive examples of a labeled equivalence graph G will be graphs G 0 which are subsumed by G. The cross-product operation on equivalence graphs given in Definition 3 is easily modified to incorporate vertex labels from a partial order <ref> (Cohen & Hirsh, 1994a) </ref>.
Reference: <author> Cohen, W. W. & Hirsh, H. </author> <year> (1994b). </year> <title> Learning the CLASSIC description logic: Theoretical and experimental results. </title> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Fourth International Conference (KR94): </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cohen, W. W. & Page, C. D. Jr. </author> <year> (1994). </year> <title> Polynomial learnability and inductive logic programming: Methods and results. </title> <note> Submitted for publication. </note>
Reference: <author> Dalal, M. & Etherington, D. </author> <year> (1992). </year> <title> Tractable approximate deduction using limited vocabulary. </title> <booktitle> In CSCSI-92 Vancouver. </booktitle>
Reference: <author> Decatur, S. E. </author> <year> (1993). </year> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> Proceedings of the Sixth Annual ACM Conference Computational Learning Theory (pp. </booktitle> <pages> 262-268). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Devanbu, P., Brachman, R. J., Selfridge, P., & Ballard, B. </author> <year> (1991). </year> <title> LaSSIE: A knowledge-based software information system. </title> <journal> Communications of the ACM, </journal> <volume> 35(5). </volume>
Reference: <author> Dietterich, T. G., London, B., Clarkson, K., & Dromey, G. </author> <year> (1982). </year> <title> Learning and inductive inference. </title> <booktitle> In The Handbook of Artificial Intelligence, </booktitle> <volume> Volume 3. </volume> <publisher> William Kaufmann. </publisher>
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1992). </year> <title> Pac-learnability of logic programs. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 128-135). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Frazier, M., Goldman, S., Mishra, N., & Pitt, L. </author> <year> (1994). </year> <title> Learning from a consistently ignorant teacher. </title> <booktitle> Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory (pp. </booktitle> <pages> 328-339). </pages> <address> New York: </address> <note> ACM Press. To appear, Information and Computation. </note>
Reference-contexts: We wish to apply Theorem 7 of <ref> (Frazier et al., 1994) </ref>, which states that if the set of concepts defined by unions of concepts from a class C is learnable, and the set of concepts defined by intersections of concepts from C is learnable, then the set of agreements of concepts in C is learnable and C is
Reference: <author> Frazier, M. & Page, C. D. </author> <year> (1993). </year> <title> Learnability of recursive, non-determinate theories: Some basic results and techniques. </title> <booktitle> Third International Workshop on Inductive Logic Programming. </booktitle>
Reference: <author> Frazier, M. & Pitt, L. </author> <year> (1993). </year> <title> Learning from entailment: An application to propositional horn sentences. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 120-127). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haussler, D. </author> <year> (1989). </year> <title> Learning conjunctive concepts in structural domains. </title> <journal> Machine Learning, </journal> <volume> 4(1), </volume> <pages> 7-40. </pages>
Reference: <author> Haussler, D., Littlestone, N., & Warmuth, M. K. </author> <year> (1994). </year> <title> Predicting f0; 1g functions on randomly drawn points. </title> <journal> Information and Computation, </journal> <volume> 115(2), </volume> <pages> 284-293. </pages>
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301), </volume> <pages> 13-30. </pages> <note> 46 M. </note> <author> FRAZIER AND L. PITT Kearns, M. & Li, M. </author> <year> (1993). </year> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22, </volume> <pages> 807-837. </pages>
Reference-contexts: The algorithm determine-label asks a (noisy) membership query for every element of the test set T . Each independently has probability at least 1 2 + 1 r of being answered correctly. By Hoeffding's inequal ity <ref> (Hoeffding, 1963) </ref>, the probability that fewer than half of the queries for graphs in T are answered correctly is at most e 2m=r 2 , which by choice of m is at most ffi.
Reference: <author> Kearns, M. & Valiant, L. G. </author> <year> (1994). </year> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41(1), </volume> <pages> 67-95. </pages>
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Mays, E., Apte, C., Griesmer, J., & Kastner, J. </author> <month> (Fall </month> <year> 1987). </year> <title> Organizing knowledge in a complex financial domain. </title> <journal> IEEE Expert, </journal> <pages> (pp. 61-70). </pages>
Reference: <author> Muggleton, S. </author> <year> (1991). </year> <title> Inductive logic programming. </title> <journal> New Generation Computing, </journal> <volume> 8, </volume> <pages> 295-318. </pages>
Reference: <author> Page, C. D. & Frisch, A. M. </author> <year> (1992). </year> <title> Generalization and learnability: A study of constrained atoms. </title> <editor> In S. H. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming chapter 2. </booktitle> <address> London: </address> <publisher> Academic Press. </publisher>
Reference: <author> Page, C. D. </author> <year> (1993). </year> <title> Anti-unification in constraint logics: Foundations and applications to learn-ability in first-order logic, to speed-up learning, and to deduction. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: We formalize this definition and leave the question of learnability under this definition open. The second possible definition of subsumption, which is interesting in its own right, possesses a property called strong compactness <ref> (Page, 1993) </ref> in other logi 38 M. FRAZIER AND L. PITT cal systems. This strong compactness property leads immediately to two positive learning results. 7.1.
Reference: <author> Patel-Schneider, P. F. </author> <year> (1989). </year> <title> A four-valued semantics for terminological logics. </title> <journal> Artificial Intelligence, </journal> <volume> 38, </volume> <pages> 319-351. </pages>
Reference: <author> Pitt, L. & Warmuth, M. K. </author> <year> (1990). </year> <title> Prediction preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3), </volume> <pages> 430-467. </pages> <booktitle> Special issue for the Third Annual Conference of Structure in Complexity Theory. </booktitle>
Reference-contexts: Moreover, because Boolean circuits are "prediction-complete" for P <ref> (Pitt & Warmuth, 1990) </ref>, if a polynomial-time algorithm existed that could, after seeing a polynomial number of random example individuals of some CLASSIC LEARNING 31 Classic sentence, labeled as positive or negative, achieve classification accuracy 1 2 + * (only slightly greater than a half), then this algorithm could be used
Reference: <author> Rivest, R. L. & Schapire, R. E. </author> <year> (1993). </year> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2), </volume> <pages> 299-347. </pages>
Reference: <author> Ron, D. & Rubinfeld, R. </author> <year> (1993). </year> <title> Learning fallible finite state automata. </title> <booktitle> Proceedings of the Sixth Annual ACM Conference Computational Learning Theory (pp. </booktitle> <pages> 218-227). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2), </volume> <pages> 197-227. </pages>
Reference: <author> Shackelford, G. & Volper, D. </author> <year> (1988). </year> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory (pp. </booktitle> <pages> 97-103). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sloan, R. </author> <year> (1988). </year> <title> Types of noise in data for concept learning. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory (pp. </booktitle> <pages> 91-96). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sloan, R. H. & Turan, G. </author> <year> (1994). </year> <title> Learning with queries but incomplete information. </title> <booktitle> Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory (pp. </booktitle> <pages> 237-245). </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 1134-1142. </pages> <note> Received Date Accepted Date Final Manuscript Date </note>
Reference-contexts: CLASSIC LEARNING 9 Standard transformations may be employed to obtain an algorithm that learns in the "PAC" learning model <ref> (Valiant, 1984) </ref> augmented with membership queries (described below), or in the on-line mistake bounded learning model (Angluin, 1988b; Littlestone, 1988) with membership queries. <p> PAC learning requires that a learning algorithm output in polynomial time a concept H that, with probability at least 1ffi, has error at most * in classifying random examples as the target C fl does, where the error is measured with respect to the unknown distribution <ref> (Valiant, 1984) </ref>. We consider the PAC learning model where the algorithm may ask membership queries in addition to receiving randomly generated examples. More formally, a learning algorithm A is said to PAC-learn Classic in polynomial time from random examples and membership queries if the following holds: 1.
References-found: 52

