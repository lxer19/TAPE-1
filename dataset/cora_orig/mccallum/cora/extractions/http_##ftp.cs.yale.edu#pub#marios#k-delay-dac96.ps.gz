URL: http://ftp.cs.yale.edu/pub/marios/k-delay-dac96.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/marios/
Root-URL: http://www.cs.yale.edu
Title: Optimizing Systems for Effective Block-Processing: The k-Delay Problem  
Author: Kumar N. Lalgudi Marios C. Papaefthymiou Miodrag Potkonjak 
Date: October 18, 1995  
Address: New Haven, CT 06520  Los Angeles, CA 90095  
Affiliation: Department of Electrical Engineering Yale University  Department of Computer Science University of California  
Abstract: Block-processing is a powerful and popular technique for increasing computation speed by simultaneously processing several samples of data. In comparison with conventional computations, block-processed computations have reduced hardware and power dissipation requirements and result in less expensive implementations on parallel and uniprocessor programmable platforms. The effectiveness of block-processing is often reduced, however, due to suboptimal placement of delays in the dataflow graph of a computation. In this paper we investigate a new application of the retiming transformation for improving the effectiveness of block-processing in computation structures. Specifically, we consider the k-delay problem in which we wish to restructure any given computation by relocating its delays so that for any given k, the resulting computation can process k data samples simultaneously in a fully regular manner. We formulate the k-delay problem as an integer linear program. We also give an efficient algorithm for the k-delay problem, whose running time is polynomial in the size of the computation dataflow graph. Our algorithm relies on an integer monotonic programming formulation of the problem on an appropriately generated constraints graph. We demonstrate the effectiveness of our optimization on several examples.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Blahut. </author> <title> Fast Algorithms for Digital Signal Processing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [11]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes <ref> [1] </ref>. In general, block-processing is beneficial in all cases where the net cost of processing n samples individually is higher than the net cost of processing n samples simultaneously. The typical cost measures include processing time, memory requirements, and energy dissipation per sample.
Reference: [2] <author> D. Eppstein. </author> <title> Finding the k shortest paths. </title> <type> Technical Report 94-26, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It is straightforward to determine all PD vertex pairs for a given CDFG. Algorithm IdentifyPD in Figure 4 runs a 2-shortest paths algorithm <ref> [2] </ref> on the given CDFG which computes the two shortest paths between all vertex pairs. We can show that it is sufficient to check the delay count on the two shortest paths between a vertex pair u; v 2 V to determine if it is potentially deficient. <p> The time required for the all pairs 2-shortest paths algorithm for a graph G = hV; Ei is O (V E + V 2 lg V ) <ref> [2] </ref>.
Reference: [3] <author> W. W. Gibbs. </author> <title> Software's chronic crisis. </title> <publisher> Scientific American, </publisher> <pages> pages 86-95, </pages> <year> 1994. </year>
Reference-contexts: Recent studies show that while computational requirements per sample of state-of-the-art communication has been doubling every year, the processing power of hardware is doubling only every three years <ref> [3] </ref>.
Reference: [4] <author> L. M. Guerra, M. Potkonjak, and J. Rabaey. </author> <title> System-level design guidance using algorithm properties. </title> <booktitle> In Proc. of the VLSI Signal Processing Workshop, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This approach, known as block-processing or vectorization, is widely used to satisfy throughput requirements through the use of parallelism and pipelining. Block-processing enhances both regularity and locality in computations which greatly facilitates efficient implementation on many hardware platforms <ref> [4, 8] </ref>. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [11]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [1].
Reference: [5] <author> D. S. Hochbaum and J. Naor. </author> <title> Simple and fast algorithms for linear and integer programs with two variables per inequality. </title> <journal> SIAM J. Computing, </journal> <volume> 23(6):1179 - 1192, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: We can solve this program in pseudo-polynomial time by extending the Hochbaum-Naor technique for solving monotonic programs with two variables per inequality <ref> [5] </ref>. Our extension involves the construction of a lattice in which each vertex corresponds to a value of each variable and each edge corresponds to a constraint. Additional edges are placed with associated costs that denote the coefficients of the variables in the objective.
Reference: [6] <author> A. T. Ishii, C. E. Leiserson, and M. C. Papaefthymiou. </author> <title> Optimizing two-phase, </title> <booktitle> level-clocked circuitry. In Advanced Research in VLSI and Parallel Systems: Proc. of the 1992 Brown/MIT Conference. </booktitle> <publisher> MIT Press, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: This technique does not always result in full block-processing, however. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis [7, 15], retiming has been used traditionally for clock period minimization <ref> [6, 9, 10] </ref> and for logic synthesis [12, 13]. The example in Figure 1 illustrates how retiming can be used to improve block-processing. The computation dataflow graph (CDFG) in this figure has three computation blocks A, B, and C and three delays.
Reference: [7] <author> D. C. Ku and G. De Micheli. </author> <title> Relative scheduling under timing constraints: Algorithms for high-level synthesis of digital circuits. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 11(6) </volume> <pages> 696-717, </pages> <year> 1992. </year>
Reference-contexts: This technique does not always result in full block-processing, however. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis <ref> [7, 15] </ref>, retiming has been used traditionally for clock period minimization [6, 9, 10] and for logic synthesis [12, 13]. The example in Figure 1 illustrates how retiming can be used to improve block-processing.
Reference: [8] <author> S. Y. Kung. </author> <title> VLSI Array Processors. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1988. </year>
Reference-contexts: This approach, known as block-processing or vectorization, is widely used to satisfy throughput requirements through the use of parallelism and pipelining. Block-processing enhances both regularity and locality in computations which greatly facilitates efficient implementation on many hardware platforms <ref> [4, 8] </ref>. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [11]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [1].
Reference: [9] <author> K. N. Lalgudi and M. C. Papaefthymiou. </author> <title> DelaY: An efficient tool for retiming with realistic delay modeling. </title> <booktitle> In Proceedings of the 32th ACM/IEEE Design Automation Conference, </booktitle> <pages> pages 304-309, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This technique does not always result in full block-processing, however. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis [7, 15], retiming has been used traditionally for clock period minimization <ref> [6, 9, 10] </ref> and for logic synthesis [12, 13]. The example in Figure 1 illustrates how retiming can be used to improve block-processing. The computation dataflow graph (CDFG) in this figure has three computation blocks A, B, and C and three delays. <p> In this section we express Problem KDP as an Integer Linear Program (ILP) by utilizing the companion graph described in <ref> [9] </ref>. The companion graph G 0 of a CDFG G is constructed by segmenting every edge u e v 2 E into two edges u e 1 e 2 ! v, where x uv is a dummy vertex.
Reference: [10] <author> C. E. Leiserson and J. B. Saxe. </author> <title> Retiming synchronous circuitry. </title> <journal> Algorithmica, </journal> <volume> 6(1), </volume> <year> 1991. </year>
Reference-contexts: This technique does not always result in full block-processing, however. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis [7, 15], retiming has been used traditionally for clock period minimization <ref> [6, 9, 10] </ref> and for logic synthesis [12, 13]. The example in Figure 1 illustrates how retiming can be used to improve block-processing. The computation dataflow graph (CDFG) in this figure has three computation blocks A, B, and C and three delays.
Reference: [11] <author> S. Liao, S. Devadas, K. Keutzer, S. Tjiang, and A. Wang. </author> <title> Storage assignment to decrease code size. </title> <booktitle> In Proc. of the ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 186-195, </pages> <year> 1995. </year>
Reference-contexts: Block-processing enhances both regularity and locality in computations which greatly facilitates efficient implementation on many hardware platforms [4, 8]. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods <ref> [11] </ref>. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [1].
Reference: [12] <author> S. Malik, E. Sentovich, R. K. Brayton, and A. Sangiovanni-Vincentelli. </author> <title> Retiming and resynthesis: Optimizing sequential networks with combinational techniques. </title> <booktitle> In Proc. of the Hawaii International Conference on System Sciences, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis [7, 15], retiming has been used traditionally for clock period minimization [6, 9, 10] and for logic synthesis <ref> [12, 13] </ref>. The example in Figure 1 illustrates how retiming can be used to improve block-processing. The computation dataflow graph (CDFG) in this figure has three computation blocks A, B, and C and three delays.
Reference: [13] <author> G. De Micheli. </author> <title> Synchronous logic synthesis: Algorithms for cycle-time minimization. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 10(1) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis [7, 15], retiming has been used traditionally for clock period minimization [6, 9, 10] and for logic synthesis <ref> [12, 13] </ref>. The example in Figure 1 illustrates how retiming can be used to improve block-processing. The computation dataflow graph (CDFG) in this figure has three computation blocks A, B, and C and three delays.
Reference: [14] <author> S. Ritz, M. Pankert, V. Zivojnovic, and H. Meyr. </author> <title> Optimum vectorization of scalable synchronous dataflow graphs. </title> <booktitle> In Proc. of the International Conference on Application-Specific Array Processors, </booktitle> <pages> pages 285-296, </pages> <month> October </month> <year> 1993. </year> <month> 14 </month>
Reference-contexts: Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs <ref> [14, 16] </ref>. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in [16]. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that results in maximum concentration of delays on the edges.
Reference: [15] <author> R. A. Walker and D. E. Thomas. </author> <title> Behavioral transformation for algorithmic level ic design. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 8(10) </volume> <pages> 1115-1127, </pages> <year> 1989. </year>
Reference-contexts: This technique does not always result in full block-processing, however. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other transformation techniques that have targeted high-level synthesis <ref> [7, 15] </ref>, retiming has been used traditionally for clock period minimization [6, 9, 10] and for logic synthesis [12, 13]. The example in Figure 1 illustrates how retiming can be used to improve block-processing.
Reference: [16] <author> V. Zivojnovic, S. Ritz, and H. Meyr. </author> <title> Retiming of DSP programs for optimum vec-torization. </title> <booktitle> In Proc. of the International Conference on Acoustic, Speech, and Signal Processing, </booktitle> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs <ref> [14, 16] </ref>. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in [16]. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that results in maximum concentration of delays on the edges. <p> Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs [14, 16]. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in <ref> [16] </ref>. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that results in maximum concentration of delays on the edges. Fully regular vectorization, however, cannot be achieved using the linear vectorization approach in that paper. <p> Our experimental results are shown in the table of Figure 8. The examples are adaptive voice echo canceler, adaptive video coder, and two examples from <ref> [16] </ref>. The results shown are from uniprocessor implementations with initiation and computation times obtained using measurements on typical DSP general purpose processors, such as TMS32020 and Motorola 56000. The improvement for each CDFG is given by the fraction 1-(# cycles with optimized CDFG)/(# cycles with original CDFG).
References-found: 16

