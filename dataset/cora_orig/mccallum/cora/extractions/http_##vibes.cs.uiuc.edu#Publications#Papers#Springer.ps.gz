URL: http://vibes.cs.uiuc.edu/Publications/Papers/Springer.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: Performance Instrumentation Techniques for Parallel Systems  
Author: Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Although the nascent state of parallel systems makes empirical performance measurement, analysis and tuning critical, rapid technological evolution, coupled with short product life cycles, has often made it difficult to isolate fundamental experimental principles from implementation artifacts. By definition, the apparatus for experimental performance analysis (i.e., instrumentation specification, data buffering, timestamp generation, and data extraction) is shaped by the intended experiment and the object of study. In some environments, certain experiments are not feasible. Balancing the volume of captured performance data against its accuracy and timeliness requires both appropriate tools and an understanding of instrumentation costs, implementation alternatives, and support infrastructure.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arlauskas, R. </author> <title> iPSC/2 System: A Second Generation Hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, Volume I (Pasadena, </booktitle> <address> CA, </address> <month> Jan. </month> <year> 1988), </year> <journal> Association for Computing Machinery, </journal> <pages> pp. 38-42. </pages>
Reference-contexts: Crystal: Operating System Instrumentation. The Intel iPSC/2 hypercube typified second generation distributed memory systems. The iPSC/2 hypercube nodes were based on an Intel 80386/80387 pair, each node contained up to sixteen megabytes of memory, and the nodes sent messages via fixed path circuit-switching <ref> [1] </ref>. In addition, a subset of the nodes supported a parallel input/output system [15] with on commodity disks.
Reference: 2. <editor> Dongarra, J. J., and Tourancheau, B., Eds. </editor> <booktitle> Environments and Tools for Parallel Scientific Computing. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: Given the enormous breadth of possible performance analysis hypotheses, as well as space limitations, techniques for performance instrumentation and data capture are the primary focus of this survey. For lucid introductions to the broader issues of hypothesis testing and performance data analysis, see <ref> [2, 23, 24] </ref>. 2 In practice, conducting this experiment on two parallel systems is a formidable task. Programming models and system configurations differ so greatly that simply porting a code to multiple architectures is problematic.
Reference: 3. <author> Dunigan, T. H. </author> <title> Hypercube Clock Synchronization. </title> <journal> Concurrency: Practice and Experience 4, </journal> <month> 3 (May </month> <year> 1992), </year> <pages> 258-268. </pages>
Reference-contexts: The existence of unsynchronized local clocks motivated the creation of an international time base, Universal Time (UT). clock distribution network. Even for a system with hundreds or thousands of processors, the cost of such a network is low. 8 Software clock synchronization <ref> [3, 17] </ref> is the alternative to a global time base. Intuitively, one chooses one processor's clock as the master and synchronizes all other clocks to that master. <p> The cost of buffer dumping is recorded in the trace data, allowing buffering dumping overheads to be removed from the trace data during post-processing. For parallel systems that lack a global time base, the Pablo instrumentation periodically synchronizes the processors using an implementation of Dunigan's distributed synchronization algorithm <ref> [3] </ref>. The architecture-independent instrumentation interface supports counting, interval timing, and event tracing. Counts can be accumulated or periodically flushed to trace buffers.
Reference: 4. <author> Geist, G. A., Heath, M. T., Peyton, B. W., and Worley, P. H. </author> <title> A User's Guide to PICL A Portable Instrumented Communication Library. </title> <type> Tech. Rep. </type> <institution> ORNL/TM-11616, Oak Ridge National Laboratory, </institution> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: After the receiver acknowledged the receipt and its willingness to accept additional data, the sender transmitted the remainder of the message. 10 Pablo is a trademark of the Board of Trustees of the University of Illinois. 11 PICL, the portable, instrumented communication library <ref> [4] </ref>, developed by the Oak Ridge National Laboratory, shares these attributes, though its primary focus is on portable message passing. model is similar to that for the Crystal instrumentation; there is a separate trace buffer for each processor or thread of control, and performance data are written to these buffers.
Reference: 5. <author> Graham, S., Kessler, P., and McKusick, M. </author> <title> gprof: A Call Graph Execution Profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction (Boston, </booktitle> <address> MA, </address> <month> June </month> <year> 1982), </year> <journal> Association for Computing Machinery, </journal> <pages> pp. 120-126. </pages>
Reference-contexts: Sampling is an approximation to counting, obtained by periodically observing the system state and incrementing a counter that corresponds to the observed state. Standard profiles (e.g., Unix gprof <ref> [5] </ref>), sample the program counter at fixed time intervals, use the program counter as the index to a bin, and increment the associated counter. After program execution, the counter value in each bin is proportional to the total time spent executing code in the associated address range.
Reference: 6. <author> HPFF. </author> <title> High-Performance Fortran Language Specfication, version 1.0. </title> <type> Tech. rep., </type> <institution> High Performance Fortran Forum, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Historically, most distributed memory parallel systems were programmed in single program multiple data (SPMD) mode using an explicit message passing style, and standard workstation compilers were used to generate code. Data parallel languages like High-Performance Fortran (HPF) <ref> [6] </ref> express parallelism by specifying parallel operations on arrays that have been distributed across the memories of the system. Compilers for data parallel languages then create code that reads and writes the distributed arrays using compiler-synthesized message passing.
Reference: 7. <author> Kohr, D. R., Zhang, X., Reed, D. A., and Rahman, M. </author> <title> Object-Oriented, Parallel Operating Systems: A Performance Study. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: As an example of its importance, Fig. 4 shows the cost to record an event on an Encore Multimax using two different operating systems, one with a memory mapped hardware clock and the other with the same clock accessible only via an operating system call <ref> [7] </ref>. With Encore's Unix implementation, accessing the clock requires only a memory read, and events can be recorded in as little as fifteen microseconds. Under the experimental Choices operating system, the system call not only increases the event recording time ten-fold, it also increases the access time variance.
Reference: 8. <author> Kuck, D. J., Davidson, E. S., Lawrie, D. H., and Sameh, A. H. </author> <title> Parallel Supercomputing Today and the Cedar Approach. </title> <booktitle> Science 231 (February 28 1986), </booktitle> <pages> 967-974. </pages>
Reference-contexts: The strengthes of the Pablo instrumentation library's approach are its portability and extensibility. However, this emphasis does limit the library's ability to exploit system-specific features and to easily capture system-level performance data. CTrace: Shared Memory Instrumentation. CTrace [10] is an event tracing system for the experimental Cedar multiprocessor. Cedar <ref> [8] </ref> consists of multiple processor clusters connected via a multistage Omega network to a global, shared memory. In turn, the individual clusters are modified Alliant FX/8 systems, each with eight vector processors, a shared cache, and a shared cluster memory.
Reference: 9. <author> Larson, J. </author> <title> Cray X-MP Hardware Performance Monitor. Cray Channels (1985). </title>
Reference-contexts: Capturing hardware performance data without 4 The Cray Hardware Performance Monitor (HPM) <ref> [9] </ref>, together with application trac ing, provides precisely this capability.
Reference: 10. <author> Malony, A. D. </author> <title> Multiprocessor Instrumentation: Approaches for Cedar. In Instrumentation for Future Parallel Computing Systems, </title> <editor> M. Simmons, R. Koskela, and I. Bucher, Eds. </editor> <publisher> Addison-Wesley, </publisher> <year> 1989, </year> <pages> pp. 1-33. </pages>
Reference-contexts: Hence, we describe three different software implementations of event tracing, Crystal, Pablo, and CTrace, each intended for a different environment. Crystal [20] supports operating system and application performance data capture on the Intel iPSC/2 hypercube, the Pablo instrumentation library [19] supports portable application event tracing, and the CTrace library <ref> [10] </ref> supports application and operating system tracing on a hierarchical, shared memory parallel system. Crystal: Operating System Instrumentation. The Intel iPSC/2 hypercube typified second generation distributed memory systems. <p> The strengthes of the Pablo instrumentation library's approach are its portability and extensibility. However, this emphasis does limit the library's ability to exploit system-specific features and to easily capture system-level performance data. CTrace: Shared Memory Instrumentation. CTrace <ref> [10] </ref> is an event tracing system for the experimental Cedar multiprocessor. Cedar [8] consists of multiple processor clusters connected via a multistage Omega network to a global, shared memory.
Reference: 11. <author> Malony, A. D. </author> <title> Performance Observability. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: If the total program execution time is too low, the sampling error may be high. The operating system dependence of standard profiling techniques makes profiling operating system activity difficult. On parallel systems, the existence of multiple processors can skew profiling statistics <ref> [16, 11] </ref>. Consider a code fragment that achieves linear speedup on P processors. <p> Ideally, the balance between hardware and software implementations is determined during system design. Unfortunately, many instrumentation systems are added late in the design process, necessitating accommodation with existing design features. Below, we describe two examples of hardware support for software performance data capture, Hypermon <ref> [11, 12] </ref>, a retrofit to the Intel iPSC/2 hypercube, and Multikron [14], a performance data recording chip. Hypermon: An Instrumentation Hardware Retrofit. <p> In an attempt to remedy these limitations and to explore the feasibility of retrofitting an existing system with hardware support for performance data capture, Malony developed Hypermon <ref> [11, 12] </ref>, a board set for hardware data buffering and timestamp generation. Hypermon exploited a little-known feature of the Intel iPSC/2, a five bit interface from each node board to a spare node slot in the system cabinet.
Reference: 12. <author> Malony, A. D., and Reed, D. A. </author> <title> A Hardware-Based Performance Monitor for the Intel iPSC/2 Hypercube. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing (June 1990), Association for Computing Machinery, </booktitle> <pages> pp. 213-226. </pages>
Reference-contexts: Ideally, the balance between hardware and software implementations is determined during system design. Unfortunately, many instrumentation systems are added late in the design process, necessitating accommodation with existing design features. Below, we describe two examples of hardware support for software performance data capture, Hypermon <ref> [11, 12] </ref>, a retrofit to the Intel iPSC/2 hypercube, and Multikron [14], a performance data recording chip. Hypermon: An Instrumentation Hardware Retrofit. <p> In an attempt to remedy these limitations and to explore the feasibility of retrofitting an existing system with hardware support for performance data capture, Malony developed Hypermon <ref> [11, 12] </ref>, a board set for hardware data buffering and timestamp generation. Hypermon exploited a little-known feature of the Intel iPSC/2, a five bit interface from each node board to a spare node slot in the system cabinet. <p> Second, event data rates were bursty; these bursts can lead to hardware buffer data overruns. Moreover, the total event data volume increased superlin-early with the number of nodes. As an example, Fig. 7, from <ref> [12] </ref>, shows the Crystal event data rate, in one millisecond windows, for a standard cell placement code run on the Intel iPSC/2. The single processor trace includes only the context switch events that occur each fifty milliseconds.
Reference: 13. <author> Malony, A. D., Reed, D. A., and Rudolph, D. C. </author> <title> Integrating Performance Data Collection, Analysis, and Visualization. In Parallel Computer Systems: Performance Instrumentation and Visualization, </title> <editor> M. Simmons and R. Koskela, Eds. </editor> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990, </year> <pages> pp. 73-97. </pages>
Reference-contexts: Because the iPSC/2's salient features are an integral part of current systems (e.g., the Intel Paragon XP/S and Thinking Machines CM-5), most of the performance instrumentation issues are directly transferable to newer architectures. Crystal <ref> [13, 20, 22, 21] </ref>, based on a modified version of the Intel NX/2 operating system, was an event tracing facility designed to capture both application and operating system events. Application instrumentation could be inserted either manually by users or automatically by a compiler.
Reference: 14. <author> Mink, A., and Carpenter, R. J. </author> <title> Operating Principles of MULTIKRON Performance Instrumentation for MIMD Computer. </title> <type> Tech. Rep. NISTIR 4737, </type> <institution> National Institute of Standards and Technology, </institution> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Unfortunately, many instrumentation systems are added late in the design process, necessitating accommodation with existing design features. Below, we describe two examples of hardware support for software performance data capture, Hypermon [11, 12], a retrofit to the Intel iPSC/2 hypercube, and Multikron <ref> [14] </ref>, a performance data recording chip. Hypermon: An Instrumentation Hardware Retrofit. Users of Crystal's software instrumentation on the Intel iPSC/2 often struggled to overcome its two major limitations: insufficient event data storage capacity and the lack of an accurate, global time base. <p> Multikron: A Performance Monitoring Chip. The NIST Multikron <ref> [14] </ref> integrates support for counting, event trace buffering, timestamp generation, and data extraction on a single chip. In its intended operational mode, each node or processor of a parallel system would include a Multikron chip for unobtrusive data recording. <p> If not, less invasive instrumentation (e.g., counting rather than tracing) is appropriate. Otherwise, hardware support for data buffering and extraction (e.g., like that provided by the NIST Multikron chip <ref> [14] </ref>) may be necessary. 12 This temporal dependence is a cogent argument that vendors should include support for performance instrumentation early in their system designs. Finally, quantifying instrumentation perturbation is difficult.
Reference: 15. <author> Pierce, P. </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers and Applications (Monterey, </booktitle> <address> CA, </address> <month> Mar. </month> <year> 1989), </year> <journal> Association for Computing Machinery, </journal> <pages> pp. 155-160. </pages>
Reference-contexts: The iPSC/2 hypercube nodes were based on an Intel 80386/80387 pair, each node contained up to sixteen megabytes of memory, and the nodes sent messages via fixed path circuit-switching [1]. In addition, a subset of the nodes supported a parallel input/output system <ref> [15] </ref> with on commodity disks. Because the iPSC/2's salient features are an integral part of current systems (e.g., the Intel Paragon XP/S and Thinking Machines CM-5), most of the performance instrumentation issues are directly transferable to newer architectures.
Reference: 16. <author> Ponder, C., and Fateman, R. </author> <title> Inaccuracies in Program Profiling. </title> <journal> Software: Practice and Experience 18, </journal> <month> 5 (May </month> <year> 1988), </year> <pages> 459-467. </pages>
Reference-contexts: If the total program execution time is too low, the sampling error may be high. The operating system dependence of standard profiling techniques makes profiling operating system activity difficult. On parallel systems, the existence of multiple processors can skew profiling statistics <ref> [16, 11] </ref>. Consider a code fragment that achieves linear speedup on P processors.
Reference: 17. <author> Ramanathan, P., Shin, K. G., and Butler, R. W. </author> <title> Fault-tolerant Clock Synchronization in Distributed System. </title> <booktitle> IEEE Computer 23 (1990), </booktitle> <pages> 33-42. </pages>
Reference-contexts: The existence of unsynchronized local clocks motivated the creation of an international time base, Universal Time (UT). clock distribution network. Even for a system with hundreds or thousands of processors, the cost of such a network is low. 8 Software clock synchronization <ref> [3, 17] </ref> is the alternative to a global time base. Intuitively, one chooses one processor's clock as the master and synchronizes all other clocks to that master.
Reference: 18. <author> Reed, D. A., Aydt, R. A., Madhyastha, T. M., Noe, R. J., Shields, K. A., and Schwartz, B. W. </author> <title> The Pablo Performance Analysis Environment. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Unless the operating system source code widely available (e.g., Mach or OSF/1), operating system instrumentation is best supported by a parallel systems vendor. Pablo: Application Instrumentation. The Pablo Performance Analysis Environment 10 <ref> [19, 18] </ref> is a portable performance instrumentation and data analysis environment designed for large-scale parallel systems, with primary emphasis on the Intel Paragon XP/S and Thinking Machines CM-5.
Reference: 19. <author> Reed, D. A., Olson, R. D., Aydt, R. A., Madhyastha, T. M., Birkett, T., Jensen, D. W., Nazief, B. A. A., and Totty, B. K. </author> <title> Scalable Performance Environments for Parallel Systems. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference (1991), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 562-569. </pages>
Reference-contexts: Hence, we describe three different software implementations of event tracing, Crystal, Pablo, and CTrace, each intended for a different environment. Crystal [20] supports operating system and application performance data capture on the Intel iPSC/2 hypercube, the Pablo instrumentation library <ref> [19] </ref> supports portable application event tracing, and the CTrace library [10] supports application and operating system tracing on a hierarchical, shared memory parallel system. Crystal: Operating System Instrumentation. The Intel iPSC/2 hypercube typified second generation distributed memory systems. <p> Unless the operating system source code widely available (e.g., Mach or OSF/1), operating system instrumentation is best supported by a parallel systems vendor. Pablo: Application Instrumentation. The Pablo Performance Analysis Environment 10 <ref> [19, 18] </ref> is a portable performance instrumentation and data analysis environment designed for large-scale parallel systems, with primary emphasis on the Intel Paragon XP/S and Thinking Machines CM-5.
Reference: 20. <author> Reed, D. A., and Rudolph, D. C. </author> <title> Experiences with Hypercube Operating Sys--tem Instrumentation. </title> <journal> International Journal of High-Speed Computing 1, </journal> <month> 4 (Dec. </month> <year> 1989), </year> <pages> 517-542. </pages>
Reference-contexts: Hence, we describe three different software implementations of event tracing, Crystal, Pablo, and CTrace, each intended for a different environment. Crystal <ref> [20] </ref> supports operating system and application performance data capture on the Intel iPSC/2 hypercube, the Pablo instrumentation library [19] supports portable application event tracing, and the CTrace library [10] supports application and operating system tracing on a hierarchical, shared memory parallel system. Crystal: Operating System Instrumentation. <p> Because the iPSC/2's salient features are an integral part of current systems (e.g., the Intel Paragon XP/S and Thinking Machines CM-5), most of the performance instrumentation issues are directly transferable to newer architectures. Crystal <ref> [13, 20, 22, 21] </ref>, based on a modified version of the Intel NX/2 operating system, was an event tracing facility designed to capture both application and operating system events. Application instrumentation could be inserted either manually by users or automatically by a compiler.
Reference: 21. <author> Rudolph, D. C. </author> <title> Performance Instrumentation for the Intel iPSC/2. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Because the iPSC/2's salient features are an integral part of current systems (e.g., the Intel Paragon XP/S and Thinking Machines CM-5), most of the performance instrumentation issues are directly transferable to newer architectures. Crystal <ref> [13, 20, 22, 21] </ref>, based on a modified version of the Intel NX/2 operating system, was an event tracing facility designed to capture both application and operating system events. Application instrumentation could be inserted either manually by users or automatically by a compiler.
Reference: 22. <author> Rudolph, D. C., and Reed, D. A. </author> <title> CRYSTAL: Operating System Instrumentation for the Intel iPSC/2. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications (Monterey, </booktitle> <address> CA, </address> <month> Mar. </month> <year> 1989), </year> <pages> pp. 249-252. </pages>
Reference-contexts: Because the iPSC/2's salient features are an integral part of current systems (e.g., the Intel Paragon XP/S and Thinking Machines CM-5), most of the performance instrumentation issues are directly transferable to newer architectures. Crystal <ref> [13, 20, 22, 21] </ref>, based on a modified version of the Intel NX/2 operating system, was an event tracing facility designed to capture both application and operating system events. Application instrumentation could be inserted either manually by users or automatically by a compiler.
Reference: 23. <author> Simmons, M., and Koskela, R., Eds. </author> <title> Parallel Computing Systems: Performance Instrumentation and Visualization. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1990. </year>
Reference-contexts: Given the enormous breadth of possible performance analysis hypotheses, as well as space limitations, techniques for performance instrumentation and data capture are the primary focus of this survey. For lucid introductions to the broader issues of hypothesis testing and performance data analysis, see <ref> [2, 23, 24] </ref>. 2 In practice, conducting this experiment on two parallel systems is a formidable task. Programming models and system configurations differ so greatly that simply porting a code to multiple architectures is problematic. <p> Because we believe event tracing is the most powerful tool for system understanding, and because the implementation issues for event tracing are a superset of those for counting and timing, tracing is the focus of the remainder of this survey. For a discussion of counting and timing facilities, see <ref> [23, 24] </ref>. 4 Event Tracing Event tracing is possible at either the hardware or software level. As discussed in x2.2, some components of hardware performance instrumentation (e.g., probe points) are inherently system dependent, and these dependences have profound implications for the other components.
Reference: 24. <author> Simmons, M., Koskela, R., and Bucher, I., Eds. </author> <title> Instrumentation for Future Parallel Computing Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Given the enormous breadth of possible performance analysis hypotheses, as well as space limitations, techniques for performance instrumentation and data capture are the primary focus of this survey. For lucid introductions to the broader issues of hypothesis testing and performance data analysis, see <ref> [2, 23, 24] </ref>. 2 In practice, conducting this experiment on two parallel systems is a formidable task. Programming models and system configurations differ so greatly that simply porting a code to multiple architectures is problematic. <p> Because we believe event tracing is the most powerful tool for system understanding, and because the implementation issues for event tracing are a superset of those for counting and timing, tracing is the focus of the remainder of this survey. For a discussion of counting and timing facilities, see <ref> [23, 24] </ref>. 4 Event Tracing Event tracing is possible at either the hardware or software level. As discussed in x2.2, some components of hardware performance instrumentation (e.g., probe points) are inherently system dependent, and these dependences have profound implications for the other components.
Reference: 25. <author> Stunkel, C. B., Fuchs, W. K., Rudolph, D. C., and Reed, D. A. </author> <title> Linear Optimization: A Case Study in Performance Analysis. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications (Monterey, </booktitle> <address> CA, </address> <month> Mar. </month> <year> 1989), </year> <pages> pp. </pages> <month> 265-268. </month> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Finally, the context switch and system call data exposed the coupling of application requests for services with the operating system responses, as well as idle time due to load imbalances. data captured by Crystal. The event trace is from an eight processor execution of a parallel linear optimization code <ref> [25] </ref>. Notice the message send, highlighted on processor 4 and the corresponding receive, highlighted on processor 1. The series of parallel horizontal lines following the "s" are the hardware message transmission of a fixed size message header.
References-found: 25

