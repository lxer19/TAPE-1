URL: http://www.cs.cmu.edu/~ECO/eco.ps
Refering-URL: http://www.cs.cmu.edu/~ECO/
Root-URL: 
Email: Email: flowekamp,adambg@cs.cmu.edu  
Phone: Phone: 412-268-5295 Fax: 412-268-5576  
Title: ECO: Efficient Collective Operations for Communication on Heterogeneous Networks  
Author: Bruce B. Lowekamp and Adam Beguelin 
Date: February 26, 1996  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: PVM and other distributed computing systems have enabled the use of networks of workstations for parallel computation, but their approach of treating all networks as collections of point-to-point connections does not promote efficient communication| particularly collective communication. The Efficient Collective Operations package (ECO) contains programs which solve this problem by analyzing the network and establishing efficient communication patterns. These patterns are used by ECO's library of collective operations. The analysis is done off-line, so that, after paying the one-time cost of analyzing the network, the execution of application programs is not delayed. This paper describes ECO and gives performance results of using ECO to implement the collective communication in CHARMM, a widely used macromolecular dynamics package. ECO substantially improves the performance of CHARMM on a heterogeneous network. ECO facilitates the development of data parallel applications by providing a simple interface to routines which use the available heterogeneous networks efficiently. This approach gives a programmer the ability to use the available networks to their full potential without acquiring any knowledge of the network structure. fl Partially supported by an NSF Graduate Research Fellowship y Joint appointment with Pittsburgh Supercomputing Center 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jose Nagib Cotrim Arabe, Adam Beguelin, Bruce Lowekamp, Erik Seligman, Michael Starkey, and Peter Stephan. Dome: </author> <title> Parallel programming in a heterogeneous multiuser environment. </title> <type> Technical Report CMU-CS-95-137, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Its design makes it possible to utilize more efficient communication techniques, such as those provided by MPP libraries, while maintaining the flexibility to run on an arbitrary collection of machines and networks. ECO is used for collective communication by Dome <ref> [1] </ref>, an object-oriented distributed programming environment currently under development. It has also been used to provide the collective communication required by CHARMM [4], a macromolecular dynamics program extensively used by chemists. The original communication routines provided with CHARMM are highly optimized for a switched or MPP network. <p> Using wider trees is important on bus-based networks, since it reduces the number of communications which are attempted in parallel, but it degrades the performance on a switched network which can handle the aggregate bandwidth. ECO has also been used to implement the collective communication for Dome <ref> [1] </ref>. A molecular dynamics program written in Dome has been run on a network of 20 machines, consisting of six DEC Alphas attached to two ethernets, five IBM Power PCs attached to an ethernet, two DEC Alphas attached to switched FDDI, and seven SGI INDYs attached to switched ethernet.
Reference: [2] <author> Vasanth Bala, Jehoshua Bruck, Robert Cypher, Pablo Elustondo, Alex Ho, Ching-Tien Ho, Shlomo Kipnis, and Marc Snir. </author> <title> CCL: A portable and tunable collective communication library for scalable parallel computers. </title> <booktitle> In Proceedings of 8th International Parallel Processing Symposium, </booktitle> <pages> pages 835-844. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and other packages and approaches to collective communication [14]. Bala, et al. describe a collective communication library originally designed for the IBM SP1 <ref> [2] </ref>. They discuss performance tuning issues and provide a detailed discussion of the semantics of collective communication and group membership, including the correctness of collective operations.
Reference: [3] <author> Mike Barnett, Satya Gupta, David G. Payne, Lance Shuler, Robert van de Geijn, and Jerrell Watts. </author> <title> Building a high-performance collective communication library. </title> <booktitle> In Proceedings of IEEE Scalable High Performance Computing, </booktitle> <pages> pages 835-834. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: The MPI standard [7] acknowledges the importance of collective communication by including it as a chapter in the specification. Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures <ref> [3] </ref>, and other packages and approaches to collective communication [14]. Bala, et al. describe a collective communication library originally designed for the IBM SP1 [2]. They discuss performance tuning issues and provide a detailed discussion of the semantics of collective communication and group membership, including the correctness of collective operations.
Reference: [4] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, and S. Swaminathan M. Karplus. CHARMM: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4 </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: ECO is used for collective communication by Dome [1], an object-oriented distributed programming environment currently under development. It has also been used to provide the collective communication required by CHARMM <ref> [4] </ref>, a macromolecular dynamics program extensively used by chemists. The original communication routines provided with CHARMM are highly optimized for a switched or MPP network. <p> In this case, the butterfly pattern would produce the same results as the binary tree. The measurement was repeated 1000 times with relatively little ambient traffic. 6.2 Application Programs ECO has been used to provide collective communication for CHARMM <ref> [4] </ref>, a macromolec-ular dynamics program used by many chemists.
Reference: [5] <author> K. Efe. </author> <title> Heuristic models of task assignment scheduling in distributed systems. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 897-916, </pages> <year> 1982. </year>
Reference-contexts: In their approach, full load balancing information and communication occurs within subnets, while communication between sub-nets is more carefully controlled. Also related to this subject is the work of Efe on grouping related tasks together in a subnet for a system with deterministic task dependencies <ref> [5] </ref>. Both of these systems share ECO's principle of limiting communication between subnets. However, a major difference between these systems and ECO is that they attempt to avoid global communication whereas ECO tries to optimize it.
Reference: [6] <author> D.J. Evans and W.U.N. Butt. </author> <title> Load balancing with network partitioning using host groups. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 325-345, </pages> <year> 1994. </year>
Reference-contexts: This technique has been used in two areas. Evans and Butt make use of this technique to facilitate load balancing <ref> [6] </ref>. In their approach, full load balancing information and communication occurs within subnets, while communication between sub-nets is more carefully controlled. Also related to this subject is the work of Efe on grouping related tasks together in a subnet for a system with deterministic task dependencies [5].
Reference: [7] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year> <note> http://www.mcs.anl.gov/mpi/index.html. 20 </note>
Reference-contexts: Optimization of both types of communication should be addressed by a complete communications package. Collective operations have long been a component of vendor supplied communication libraries for MPPs, and the supplied routines are optimized for performance on that vendor's hardware. PVM has several collective communication routines and MPI <ref> [7, 9] </ref> provides a more complete set of collective communication routines. <p> ECO addresses these concerns by analyzing the characteristics of the networks to which the machines are attached and developing optimized communication patterns which are used by collective communication routines. These routines provide the same functionality as the collective communication suite in the MPI standard <ref> [7] </ref>. Efficient nearest neighbor communication is provided by routines that map common communication topologies to the network topology. ECO requires no user input to determine the characteristics of the network and has almost no application run-time overhead. <p> Although this gives application developers the opportunity to write code specific to their target architecture, if they have one, it is an undue burden. There has already been substantial work on collective communication packages. The MPI standard <ref> [7] </ref> acknowledges the importance of collective communication by including it as a chapter in the specification. Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and other packages and approaches to collective communication [14]. <p> Developers have expressed a desire for a basic set of topologies [16] for both programming ease and efficiency reasons. PVM currently does not support topologies. The MPI standard dedicates a chapter to the discussion of a mechanism for describing arbitrary topology needs to the message passing system <ref> [7] </ref>. 4 Of particular interest to ECO development is research done on grouping hosts on the basis of network topology. This technique has been used in two areas. Evans and Butt make use of this technique to facilitate load balancing [6]. <p> As mentioned in Section 1, ECO provides the same functionality as the MPI collective communication standard <ref> [7] </ref>. Table 2 lists the operations supported by ECO. The same communication pattern described in Section 4 is used for all operations, with appropriate modifications. For operations involving a single receiver, that receiver is always the root of the tree.
Reference: [8] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine | A Users' Guide and Tutorial for Net-worked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The availability of networks of high-performance workstations and software packages such as PVM <ref> [8] </ref> has made networks of workstations (NOWs) a legitimate alternative to traditional high-performance machines such as supercomputers and massively parallel processors (MPPs). Furthermore, networks of supercomputers can be utilized when even more computational power is needed.
Reference: [9] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Optimization of both types of communication should be addressed by a complete communications package. Collective operations have long been a component of vendor supplied communication libraries for MPPs, and the supplied routines are optimized for performance on that vendor's hardware. PVM has several collective communication routines and MPI <ref> [7, 9] </ref> provides a more complete set of collective communication routines.
Reference: [10] <author> Ken Hardwick. </author> <title> HIPPI world|the switch is the network. </title> <booktitle> In COMPCON Spring 1992, </booktitle> <pages> pages 234-238. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet [15], FDDI, ATM [17], and HIPPI <ref> [10] </ref> networks, has begun to reduce this limitation. Unfortunately few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of the available network bandwidth.
Reference: [11] <author> Chengchang Huang and Philip K. McKinley. </author> <title> Design and implementation of global reduction operations across ATM networks. </title> <booktitle> In Proceedings of 3rd IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 43-50. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1994. </year>
Reference-contexts: McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks [13], wormhole routed MPPs [12], and ATM networks <ref> [11] </ref>. Many applications require the notion of communication topology, such as a mesh or a ring, which is used by the application for nearest neighbor communication. Developers have expressed a desire for a basic set of topologies [16] for both programming ease and efficiency reasons.
Reference: [12] <author> Philip K. McKinley, Yih jia Tsai, and David F. Robinson. </author> <title> A survey of collective communication in wormhole-routed massively parallel computers. </title> <type> Technical Report MSU-CPS-94-35, </type> <institution> Michigan State University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks [13], wormhole routed MPPs <ref> [12] </ref>, and ATM networks [11]. Many applications require the notion of communication topology, such as a mesh or a ring, which is used by the application for nearest neighbor communication. Developers have expressed a desire for a basic set of topologies [16] for both programming ease and efficiency reasons.
Reference: [13] <author> Philip K. McKinley and Jane W. S. Liu. </author> <title> Multicast tree construction in bus-based networks. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 29-41, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Considerable work has been done on collective operations, and multicast communication in general, which can be used as the underpinnings for collective communication, on a variety of specific physical networks. McKinley, et al. have written several papers on issues involved in implementing such operations on bus-based networks <ref> [13] </ref>, wormhole routed MPPs [12], and ATM networks [11]. Many applications require the notion of communication topology, such as a mesh or a ring, which is used by the application for nearest neighbor communication.
Reference: [14] <author> Prasenjit Mitra, David G. Payne, Lance Shuler, Robert van de Geijn, and Jerrell Watts. </author> <title> Fast collective communication libraries, please. </title> <type> Technical Report TR-95-22, </type> <institution> The University of Texas, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Efficient implementations of core collective operations is crucial for achieving maximum performance of applications on message-passing systems <ref> [14] </ref>. 3 PVM provides only a limited set of collective communication routines. Moreover, a powerful implementation presents difficulties because of the portable, heterogeneous nature of PVM. When writing a library for a specific MPP, the developer knows the topology and characteristics of the interconnection network. <p> The MPI standard [7] acknowledges the importance of collective communication by including it as a chapter in the specification. Papers from the InterCom project discuss general techniques for building high-performance collective communication libraries, implementation of their library on several architectures [3], and other packages and approaches to collective communication <ref> [14] </ref>. Bala, et al. describe a collective communication library originally designed for the IBM SP1 [2]. They discuss performance tuning issues and provide a detailed discussion of the semantics of collective communication and group membership, including the correctness of collective operations.
Reference: [15] <author> R.A. Quinnell. </author> <title> Emerging 100-Mbit ethernet standards ease system bottlenecks. EDN (European edition), </title> <type> 39(1) </type> <institution> 35-36,40, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet <ref> [15] </ref>, FDDI, ATM [17], and HIPPI [10] networks, has begun to reduce this limitation. Unfortunately few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of the available network bandwidth.
Reference: [16] <author> Andreas Stathopoulos, Anders Ynnerman, and Charlotte F. Fischer. </author> <title> A PVM implementation of the MCHF atomic structure package. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <note> to appear, 1995. http://www.vuse.vanderbilt.edu/~andreas/publications/jsa.ps. 21 </note>
Reference-contexts: The developer of such a library for PVM has no such assumptions to work with. Because PVM does not provide a large set of collective communication routines, developers of PVM applications have been forced to design their own <ref> [16] </ref>. Although this gives application developers the opportunity to write code specific to their target architecture, if they have one, it is an undue burden. There has already been substantial work on collective communication packages. <p> Many applications require the notion of communication topology, such as a mesh or a ring, which is used by the application for nearest neighbor communication. Developers have expressed a desire for a basic set of topologies <ref> [16] </ref> for both programming ease and efficiency reasons. PVM currently does not support topologies.
Reference: [17] <author> Ronald J. Vetter. </author> <title> ATM concepts, architectures, and protocols. </title> <journal> Communications of the ACM, </journal> <volume> 38(2) </volume> <pages> 30-38, </pages> <month> February </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: However, the networks forming NOWs are almost never as powerful as the networks within MPPs, so most applications run on NOWs have been coarse-grain computations which require relatively little communication. The advent of high-performance and gigabit networks, such as 100Mb ethernet [15], FDDI, ATM <ref> [17] </ref>, and HIPPI [10] networks, has begun to reduce this limitation. Unfortunately few users are lucky enough to have any or all of their machines on such networks, therefore it is critical that efficient use is made of the available network bandwidth.
References-found: 17

