URL: ftp://ftp.cc.gatech.edu/pub/ai/students/carlos/papers/er-94-03.ps.Z
Refering-URL: http://www.cc.gatech.edu/people/home/carlos/jcs-publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: fcarlos,ashwing@cc.gatech.edu  
Title: On-line Knowledge Compilation and Speedup Learning in Continuous Task Domains  
Author: Juan Carlos Santamara and Ashwin Ram 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Note: To appear in the Proceedings of the IV Iberoamerican Congress on Artificial Intelligence (IBERAMIA-94), Caracas, Venezuela, Octubre 1994.  
Abstract: Many techniques for speedup learning and knowledge compilation focus on the learning and optimization of macro-operators or control rules in task domains that can be characterized using a problem-space search paradigm. However, such a characterization does not fit well the class of task domains in which the problem solver is required to perform in a continuous manner. For example, in many robotic domains, the problem solver is required to monitor real-valued perceptual inputs and vary its motor control parameters in a continuous, on-line manner to successfully accomplish its task. In such domains, discrete symbolic states and operators are difficult to define. To improve its performance in continuous problem domains, a problem solver must learn, modify, and use continuous operators that continuously map input sensory information to appropriate control outputs. Additionally, the problem solver must learn the contexts in which those continuous operators are applicable. We propose a learning method that can compile sensorimotor experiences into continuous operators, which can then be used to improve performance of the problem solver. The method speeds up the task performance as well as results in improvements in the quality of the resulting solutions. The method is implemented in a robotic navigation system, which is evaluated through extensive experimentation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arkin, R.C. </author> <year> (1989). </year> <title> Motor schema-based mobile robot navigation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 8(4) </volume> <pages> 92-112. </pages>
Reference: [2] <author> DeJong, </author> <title> G.F. & Mooney, </title> <address> R.J. </address> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 145-176. </pages>
Reference-contexts: 1 Introduction Speedup learning deals with the issue of improving the performance of a problem solver with its experience. Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., <ref> [3, 8, 2] </ref>), and learning some form of control knowledge that can be used to select which operator to try next (e.g., [9, 6, 7]).
Reference: [3] <author> Fikes, R.E., Hart, P.E., & Nilsson, N.J. </author> <year> (1972). </year> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 251-288. </pages>
Reference-contexts: 1 Introduction Speedup learning deals with the issue of improving the performance of a problem solver with its experience. Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., <ref> [3, 8, 2] </ref>), and learning some form of control knowledge that can be used to select which operator to try next (e.g., [9, 6, 7]).
Reference: [4] <author> Hammond, K.J. </author> <year> (1989). </year> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <publisher> Academic Press, </publisher> <address> Boston, MA. </address>
Reference-contexts: In order to learn such continuous operators, the learning and adaptation module uses a combination of ideas from case-based reasoning and learning, which deals with the issue of using past experiences to deal with and learn from novel situations (e.g., see <ref> [5, 4] </ref>), and from reinforcement learning, which deals with the issue of updating the content of system's knowledge based on feedback from the environment (e.g., see [14]).
Reference: [5] <author> Kolodner, J.L. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> MorganKaufmann, San Mateo, CA. </address>
Reference-contexts: In order to learn such continuous operators, the learning and adaptation module uses a combination of ideas from case-based reasoning and learning, which deals with the issue of using past experiences to deal with and learn from novel situations (e.g., see <ref> [5, 4] </ref>), and from reinforcement learning, which deals with the issue of updating the content of system's knowledge based on feedback from the environment (e.g., see [14]).
Reference: [6] <author> Laird, J.E., Rosenbloom, P.S., Newell, A. </author> <year> (1986). </year> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 11-46. </pages>
Reference-contexts: Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., [3, 8, 2]), and learning some form of control knowledge that can be used to select which operator to try next (e.g., <ref> [9, 6, 7] </ref>). These techniques were designed under the assumption that the problem solver conducts some form of problem-space search (e.g., applying operators to transform the current state into the goal state). However there are some domains in which the standard problem-space search paradigm does not fit well.
Reference: [7] <author> Minton, S.N. </author> <year> (1990). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 363-392. </pages>
Reference-contexts: Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., [3, 8, 2]), and learning some form of control knowledge that can be used to select which operator to try next (e.g., <ref> [9, 6, 7] </ref>). These techniques were designed under the assumption that the problem solver conducts some form of problem-space search (e.g., applying operators to transform the current state into the goal state). However there are some domains in which the standard problem-space search paradigm does not fit well.
Reference: [8] <author> Mitchell, T.M., Keller, R.M., & Kedar-Cabelli, S.T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 47-80. </pages>
Reference-contexts: 1 Introduction Speedup learning deals with the issue of improving the performance of a problem solver with its experience. Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., <ref> [3, 8, 2] </ref>), and learning some form of control knowledge that can be used to select which operator to try next (e.g., [9, 6, 7]).
Reference: [9] <author> Mitchell, T. M., Utgoff, P. E. & Banerji, R. B. </author> <year> (1983). </year> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. In R.S. </title> <editor> Michalski, J.G. Carbonell, & T.M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. I), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Most of the techniques that have been developed for speedup learning fall in two broad categories: learning new macro-operators by composing and generalizing sequences of original operators (e.g., [3, 8, 2]), and learning some form of control knowledge that can be used to select which operator to try next (e.g., <ref> [9, 6, 7] </ref>). These techniques were designed under the assumption that the problem solver conducts some form of problem-space search (e.g., applying operators to transform the current state into the goal state). However there are some domains in which the standard problem-space search paradigm does not fit well.
Reference: [10] <author> Ram, A. </author> <title> Creative Conceptual Change. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 17-26, </pages> <address> Boulder, CO, </address> <year> 1993. </year>
Reference: [11] <author> Ram, A., Arkin, R.C., Moorman, K. & Clark, R.J. </author> <year> (1992). </year> <title> Case-based reactive navigation: A case-based method for on-line selection and adaptation of reactive control parameters in autonomous robotics systems. </title> <type> Technical Report GIT-CC-92/57, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA. </address>
Reference: [12] <author> Ram, A. & Santamara, J.C. </author> <year> (1993a). </year> <title> Multistrategy Learning in Reactive Control Systems for Autonomous Robotic Navigation. </title> <journal> Informatica, </journal> <volume> Vol. 17(4) </volume> <pages> 347-369. </pages>
Reference: [13] <author> Ram, A. & Santamara, J.C. </author> <year> (1993b). </year> <title> Continuous case-based reasoning. </title> <booktitle> In Proceedings of the AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> pages 86-93, </pages> <address> Washington, DC. </address>
Reference-contexts: However, the combination, and the nature of the continuous task domain, required us to deviate from the standard algorithms for case-based reasoning and reinforcement learning (see <ref> [13] </ref>, for a discussion of our method for continuous case-based reasoning). The main responsibility of the case-based reasoning method is to (continuously) tune the navigation module using control outputs inputs and schema parameters.
Reference: [14] <author> Sutton, R.S. </author> <year> (1992), </year> <title> editor. </title> <journal> Machine Learning, </journal> <note> 8(3/4), special issue on Reinforcement Learning, </note> <year> 1992. </year>
Reference-contexts: from case-based reasoning and learning, which deals with the issue of using past experiences to deal with and learn from novel situations (e.g., see [5, 4]), and from reinforcement learning, which deals with the issue of updating the content of system's knowledge based on feedback from the environment (e.g., see <ref> [14] </ref>). SINS starts out with no knowledge about how and when to modify the schema parameter values, relying purely on reactive control to navigate through the terrain. <p> Technical details of the learning methods can be found in Ram and Santamara ([12]). One difference between our methods and traditional reinforcement learning is that the latter assumes that the outcomes of the system's actions are known (cf. <ref> [15, 16, 14] </ref>); it learns which actions to execute to maximize a reward.
Reference: [15] <author> C.J.C.H. Watkins, </author> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> University of Cam-bridge, </institution> <address> England, </address> <year> (1989). </year>
Reference-contexts: Technical details of the learning methods can be found in Ram and Santamara ([12]). One difference between our methods and traditional reinforcement learning is that the latter assumes that the outcomes of the system's actions are known (cf. <ref> [15, 16, 14] </ref>); it learns which actions to execute to maximize a reward.
Reference: [16] <author> S.D. Whitehead and D.H. </author> <title> Ballard (1990), Active Perception and Reinforcement Learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 179-188, </pages> <address> Austin, TX, </address> <year> 1990. </year>
Reference-contexts: Technical details of the learning methods can be found in Ram and Santamara ([12]). One difference between our methods and traditional reinforcement learning is that the latter assumes that the outcomes of the system's actions are known (cf. <ref> [15, 16, 14] </ref>); it learns which actions to execute to maximize a reward.
References-found: 16

