URL: ftp://ftp.cs.washington.edu/tr/1993/07/UW-CSE-93-07-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/arch/data-pre-hp.html
Root-URL: 
Title: Data Prefetching for High-Performance Processors  
Author: Tien-Fu Chen 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: 1993  
Note: July  
Abstract: Technical Report 93-07-01 Department of Computer Science and Engineering University of Washington Seattle, WA 98195 
Abstract-found: 1
Intro-found: 1
Reference: [Adve & Hill 90] <author> Adve, S. and Hill, M. </author> <year> (1990). </year> <title> Weak ordering anew definition. </title> <booktitle> In Proc. of the 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 2--14. </pages>
Reference: [Agarwal et al. 90] <author> Agarwal, A., Lim, B.-H., Kranz, D., and Kubiatowicz, J. </author> <year> (1990). </year> <month> APRIL: </month> <title> A processor architecture for multithreading. </title> <booktitle> In Proc. of the 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 104--114. </pages>
Reference: [Alverson et al. 90] <author> Alverson, R., Callahan, D., Cummings, D., Koblenz, D., Porterfield, B., and Smith, B. </author> <year> (1990). </year> <title> The Tera computer system. </title> <booktitle> In Proc. 1990 Intl. Conf. on Supercomputing, </booktitle> <pages> pages 1--6. </pages>
Reference-contexts: A fast context switch mechanism is an essential requirement in those architectures. Caches can be used in these architectures (note that not all threaded architectures use caches, e.g., the Tera machine <ref> [Alverson et al. 90] </ref>) but it is important to realize their vulnerability. The problem stems from the dual functions of the cache: hiding memory latency and preserving locality for all the threads that can be activated. As too many threads share the cache, capacity and conflict misses will increase.
Reference: [Archibald & Baer 86] <author> Archibald, J. and Baer, J. L. </author> <year> (1986). </year> <title> Cache coherence protocols: evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, 4(4):273--298. </journal>
Reference-contexts: The two key issues for implementing multiple-context processors are: when is context switching performed [Boothe & Ranade 92, Laudon et al. 92], 4 and what defines a context [Hum & Gao 91]. Variations on these issues include conditional-switch, switch-on-cache-miss, and switch-every-cycle. The cache coherence, or cache consistency, problem <ref> [Archibald & Baer 86] </ref> arises in shared-memory multiprocessors where several copies of the same block can be present in the local caches of the individual processors.
Reference: [Baer & Chen 91] <author> Baer, J.-L. and Chen, T.-F. </author> <year> (1991). </year> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proc. of Supercomputing '91, </booktitle> <pages> pages 176--186. </pages> <note> Also TR 91-03-07, </note> <institution> Department of Computer Science and Engineering, University of Washington. </institution>
Reference-contexts: Depending on how prefetches are determined and initiated, prefetching can be either hardware-controlled <ref> [Baer & Chen 91, Fu & Patel 92] </ref> or software-directed [Porterfield 89, Klaiber & Levy 91, Mowry et al. 92]. The hardware approach detects accesses with regular patterns and issues prefetches at run time, whereas the software approach relies on the compiler to analyze programs and to insert prefetch instructions. <p> The same general idea of a hardware assist is presented by Sklenar [92], but without any performance or cost evaluation. These two schemes post-date our earlier study of a hardware-assist function for prefetching <ref> [Baer & Chen 91] </ref>. Our approach will be described in Chapter 3. Temporal schemes By lookahead decoding of the instruction stream, temporal mechanisms attempt to have data be in the cache ``just in time'' to be used. <p> Sklenar [92] gives a design for a prefetch unit, which behaves like a co-processor. The prefetch processor calculates the effective addresses and generates prefetch requests. Although these two approaches were proposed later than our original scheme <ref> [Baer & Chen 91] </ref>, they apparently lack two key ingredients: (1) the prefetch for the next iteration is generated on current access, and therefore cannot prefetch more than one iteration ahead, and (2) they do not have a lookahead mechanism to approximately control the arrival time of prefetched blocks.
Reference: [Baer & Wang 89] <author> Baer, J.-L. and Wang, W.-H. </author> <year> (1989). </year> <title> Multi-level cache hierarchies: Organizations, protocols and performance. </title> <journal> Journal of Parallel and Distributed Computing, 6(3):451--476. </journal>
Reference-contexts: With current VLSI developments, several functional units, instruction and data caches, and hardware assists can be included on the processor chip. Therefore, a first obvious method for reducing the average memory access time is to implement multi-level cache hierarchies <ref> [Baer & Wang 89] </ref> with an on-chip first level cache.
Reference: [Ball & Larus 93] <author> Ball, T. and Larus, J. R. </author> <year> (1993). </year> <title> Branch prediction for free. </title> <type> Technical Report #1137, </type> <institution> Computer Science Department, University of Wisconsin - Madison. </institution> <month> 122 </month>
Reference-contexts: The compiler could easily perform a transformation resulting in backward branches for these programs in which forwards conditional branches are used to control the loop iterations <ref> [Ball & Larus 93] </ref>. 30 3.5 Summary In this chapter we have proposed a design for a hardware-based prefetching scheme. The goal of this support unit is to reduce the CPI contribution associated with data cache misses.
Reference: [Boothe & Ranade 92] <author> Boothe, B. and Ranade, A. </author> <year> (1992). </year> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <booktitle> In Proc. of the 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 214--223. </pages>
Reference-contexts: If several threads are assigned to a processor, memory latencies can be masked by rapidly context switching to a different thread rather than waiting for a memory reference to complete. The two key issues for implementing multiple-context processors are: when is context switching performed <ref> [Boothe & Ranade 92, Laudon et al. 92] </ref>, 4 and what defines a context [Hum & Gao 91]. Variations on these issues include conditional-switch, switch-on-cache-miss, and switch-every-cycle.
Reference: [Brantley et al. 85] <author> Brantley, W. C., McAuliffe, K. P., and Weiss, J. </author> <year> (1985). </year> <title> RP3 processor-memory element. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <pages> pages 782--789. </pages>
Reference-contexts: The lockup-free cache in RP3 <ref> [Brantley et al. 85] </ref> supports non-blocking prefetches and multiple outstanding stores. However, reads are blocking until the missed datum returns. <p> For write-back caches (with write-allocate), write buffers are used to temporarily store the written value until the missing data line is returned. Another example is the write-back buffer used for temporary storage of replaced dirty blocks in a write-back cache. As a further example, the lockup-free cache in RP3 <ref> [Brantley et al. 85] </ref> supports non-blocking prefetches and multiple outstanding writes.
Reference: [Bray & Flynn 91] <author> Bray, B. K. and Flynn, M. J. </author> <year> (1991). </year> <title> Writes caches as an alternative to write buffers. </title> <type> Technical Report CSL-TR-91-470, </type> <institution> Stanford University. </institution>
Reference-contexts: The system uses spare interconnect and memory cycles to write buffered values to the next level of the memory hierarchy. Under write-through, the buffering of write requests releases the CPU from waiting for the writes to complete. An extension to the write buffer is a write cache <ref> [Bray & Flynn 91] </ref>, organized like a small regular cache, that uses an allocate strategy on write misses and write backs to reduce the number of writes. <p> Extensions to write buffers have been proposed. For example, write caches <ref> [Bray & Flynn 91] </ref>, organized like regular caches, hold partial written data lines and allow multiple writes on the same line to be combined, thus reducing the total number of writes to the next level of the memory hierarchy.
Reference: [Brewer et al. 91] <author> Brewer, E. A., Dellarocas, C. N., Colbrook, A., and Weihl, W. E. </author> <year> (1991). </year> <title> PROTEUS: A parallel-architecture simulator. </title> <type> Technical Report LCS/TR-516, </type> <institution> MIT. </institution>
Reference-contexts: The idea of a simulation where the simulator and user programs execute concurrently appears in several simulators [Davis et al. 91, Brewer et al. 91, Grunwald et al. 91]. Specifically, our implementation is similar to Tango <ref> [Brewer et al. 91] </ref>. However, the differences are: 1. User synchronizations are handled in the simulator kernel, instead of UNIX processes and semaphores. This can significantly reduce the simulation overhead. 2. Each simulation module is running independently as an internal thread in the process.
Reference: [Censier & Feautrier 78] <author> Censier, L. and Feautrier, P. </author> <year> (1978). </year> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, C-27(12):1112--1118. </journal>
Reference-contexts: Each processor has a local memory for private data and instructions, and primary caches for shared data. We assume that private or stack data are allocated in the local memory. Cache coherence is maintained using a full directory protocol <ref> [Censier & Feautrier 78] </ref>. The directory is distributed among the memory modules and dynamically maintains the states of the data blocks. Prefetched data are put into the caches so that the data still remain visible to the cache coherence protocol. Prefetch requests are generated by the processor.
Reference: [Chen et al. 91] <author> Chen, W. Y., Mahlke, S. A., Chang, P. P., and Hwu, W.-M. </author> <year> (1991). </year> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of the 24th International Symposium on Mircoarchitecture. </booktitle>
Reference-contexts: We also contrast the prefetch buffer solution with a victim cache [Jouppi 90]. The separate prefetch buffer has a block size the same as that of the primary data cache <ref> [Klaiber & Levy 91, Chen et al. 91] </ref>. The rationale is to nullify data pollution effects.
Reference: [Chen et al. 92] <author> Chen, W. Y., Mahlke, S. A., and Hwu, W.-M. </author> <year> (1992). </year> <title> Tolerating data access latency with register preloading. </title> <booktitle> In Proc. 1992 Intl. Conf. on Supercomputing. </booktitle>
Reference-contexts: The freedom of instruction scheduling in most compiler algorithms is limited by the data and control dependencies in the programs. Better performance is 3 generally achieved by moving loads far enough ahead of their uses. Like ``regular'' non-blocking loads, speculative loads <ref> [Chen et al. 92, Rogers & Li 92] </ref> fetch memory values into registers directly bypassing the cache if need be, i.e., without blocking the pipeline on a miss, and allow other instructions to be executed simultaneously.
Reference: [Chow & Hennessy 90] <author> Chow, F. C. and Hennessy, J. </author> <year> (1990). </year> <title> The priority-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, 12(4):501--536. </journal>
Reference-contexts: Although the results of our register renaming procedure are optimistic since we do 111 not limit the number of registers, the approach is still feasible if the compiler identifies the unused registers or performs a priority-based register allocation <ref> [Chow & Hennessy 90] </ref> by taking into account the cost of the data access penalty. 6.5.3 Effect of Instruction Scheduling Table 6.3 shows the effect of the reordering of data accesses.
Reference: [Davis et al. 91] <author> Davis, H., Goldschmidt, S., and Hennessy, J. </author> <year> (1991). </year> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <volume> pages II 99 -- 107. </volume> <pages> 123 </pages>
Reference: [DEC 92] <author> DEC (1992). </author> <title> Alpha Architecture Handbook. </title> <publisher> Digital Press. </publisher>
Reference-contexts: A processor has to explicitly execute the prefetch instruction to initiate a prefetch request. Such prefetch instructions, which are just hints to the memory subsystem for reducing memory latency, are found in contemporary processors, such as ALPHA <ref> [DEC 92] </ref>. Porterfield [89] examines the effect of prefetching all array references in the most inner loops of programs by inserting prefetches one iteration ahead.
Reference: [Dubois et al. 86] <author> Dubois, M., Scheurich, C., and Briggs, F. </author> <year> (1986). </year> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proc. of the 13th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 434--442. </pages>
Reference-contexts: Although write misses or writes on clean write-shared data can be helped by the use of an exclusive-prefetch, the prefetching overhead may be still substantial because it increases invalidation misses in other processors that are still using the data. Instead, our default consistency model is weak consistency <ref> [Dubois et al. 86] </ref>, under which the write latency can be mostly hidden. The baseline cache hierarchy without prefetching was described in Section 5.4.1.
Reference: [Dubois et al. 88] <author> Dubois, M., Scheurich, C., and Briggs, F. A. </author> <year> (1988). </year> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> Computer, </journal> <volume> 21(2). </volume>
Reference: [Dubois et al. 91] <author> Dubois, M., Wang, J.-C., Barroso, L., Lee, K., and Chen, Y.-S. </author> <year> (1991). </year> <title> Delayed consistency and its effects on miss rate of parallel programs. </title> <booktitle> In Proc. of Supercomputing '91, </booktitle> <pages> pages 197--206. </pages>
Reference: [Fu & Patel 91] <author> Fu, J. W. C. and Patel, J. H. </author> <year> (1991). </year> <title> Data prefetching in multiprocessor vector cache memories. </title> <booktitle> In Proc. of the 18th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 54--63. </pages>
Reference: [Fu & Patel 92] <author> Fu, J. W. C. and Patel, J. H. </author> <year> (1992). </year> <title> Stride directed prefetching in scalar processors. </title> <booktitle> In Proc. of the 25th Int'l Symp. on Microarchitecture, </booktitle> <pages> pages 102--110. </pages>
Reference-contexts: Depending on how prefetches are determined and initiated, prefetching can be either hardware-controlled <ref> [Baer & Chen 91, Fu & Patel 92] </ref> or software-directed [Porterfield 89, Klaiber & Levy 91, Mowry et al. 92]. The hardware approach detects accesses with regular patterns and issues prefetches at run time, whereas the software approach relies on the compiler to analyze programs and to insert prefetch instructions. <p> For example, it can be a support unit for an on-chip cache. It is worthwhile to close this chapter with a brief qualitative comparison of our design with the two closest approaches <ref> [Fu & Patel 92, Sklenar 92] </ref> that were reviewed in Section 2.1.1. Fu and Patel [92] present a mechanism which will generate a prefetch request based on two consecutive accesses by adding the current effective address with the difference between the current and previous addresses.
Reference: [Gharachorloo et al. 91a] <author> Gharachorloo, K., Gupta, A., and Hennessy, J. </author> <year> (1991a). </year> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proc. of the 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245--259. </pages>
Reference-contexts: The lockup-free cache in RP3 [Brantley et al. 85] supports non-blocking prefetches and multiple outstanding stores. However, reads are blocking until the missed datum returns. The lockup-free cache appearing in the studies of the DASH multiprocessor <ref> [Gharachorloo et al. 91a, Mowry & Gupta 91] </ref> allows multiple outstanding write and prefetch requests, while the processor still stalls on read misses. In principle, the capability of handling multiple pending requests is essential for prefetching or any other ``buffering'' and ``pipelining'' techniques that we have reviewed.
Reference: [Gharachorloo et al. 91b] <author> Gharachorloo, K., Gupta, A., and Hennessy, J. </author> <year> (1991b). </year> <title> Two techniques to enchance the performance of memory consistency models. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <pages> pages I:355--I:364. </pages>
Reference-contexts: In principle, the capability of handling multiple pending requests is essential for prefetching or any other ``buffering'' and ``pipelining'' techniques that we have reviewed. Most studies on prefetching [Lee et al. 87b, Porterfield 89, Jouppi 90, Fu & Patel 91] and relaxed memory consistency models <ref> [Gharachorloo et al. 91b] </ref> simply assume pipelined caches, which also can be thought of as lockup-free caches. 13 2.2.2 Non-blocking Reads/Writes In the second category, where out-of-order execution is allowed, the processor essentially requires extra hardware complexity to perform dynamic scheduling and support non-blocking loads.
Reference: [Gharachorloo et al. 92] <author> Gharachorloo, K., Gupta, A., and Hennessy, J. </author> <year> (1992). </year> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proc. of the 19th Annual Intl. Symp. on Computer Architecture. </booktitle> <pages> 124 </pages>
Reference-contexts: Dynamic instruction scheduling (out of order execution) obtained at a significant increased cost in hardware complexity, can provide a larger non-blocking distance. However, the effectiveness is still subject to data dependence effects, branch prediction, and the size of the lookahead window provided by the architecture <ref> [Gharachorloo et al. 92] </ref>. By comparison, non-blocking writes have more chances to fully hide the write miss latency because the non-blocking distance is usually equal to the memory access time 1 .
Reference: [Gibbons & Muchnick 86] <author> Gibbons, P. B. and Muchnick, S. S. </author> <year> (1986). </year> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proc. of SIGPLAN Symp. on Compiler Construction. </booktitle>
Reference-contexts: To achieve additional overlap, the compiler may perform instruction scheduling to avoid unnecessary stalls by keeping the CPU busy <ref> [Gibbons & Muchnick 86, Krishnamurthy 90, Kerns & Eggers 92] </ref>. The freedom of instruction scheduling in most compiler algorithms is limited by the data and control dependencies in the programs. Better performance is 3 generally achieved by moving loads far enough ahead of their uses. <p> The schemes discussed in the above require fairly complex hardware. Performance can be improved by compiler assistance. Compiler optimizations for non-blocking loads mainly consist of instruction reordering and the insertion of independent instructions after non-blocking loads to keep the processor as busy as possible. Traditional instruction list schedulers <ref> [Gibbons & Muchnick 86, Krishnamurthy 90] </ref> can be employed to perform the code scheduling. More recently, Kerns and Eggers [92] proposed balanced scheduling, which is particularly suitable to non-blocking loads since the latency of a load is unknown until run time.
Reference: [Gornish et al. 90] <author> Gornish, E., Granston, E., and Veidenbaum, A. </author> <year> (1990). </year> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proc. 1990 Intl. Conf. on Supercomputing, </booktitle> <pages> pages 354--368. </pages>
Reference-contexts: In a shared-memory multiprocessor environment, where consistency requirements are taken into account, prefetching can be either binding (at compile time) <ref> [Lee et al. 87b, Gornish et al. 90] </ref> or non-binding (supported by hardware coherence) [Mowry & Gupta 91, Tullsen & Eggers 93]. <p> In this case, the first situation is equivalent to the attempt at controlling data that arrive at the cache just in time for its use. The second situation occurs when there is high contention for some shared writable data. Approaches, such as binding prefetch <ref> [Gornish et al. 90] </ref>, can reduce the problem by conservatively suppressing prefetches which may have data and control dependencies of accesses in other processors.
Reference: [Grunwald et al. 91] <author> Grunwald, D., Nutt, G. J., Wagner, D., and Zorn, B. </author> <year> (1991). </year> <title> A parallel execution evaluation testbed. </title> <type> Technical report, </type> <institution> University of Colorado. </institution>
Reference: [Hennessy & Patterson 90] <author> Hennessy, J. L. and Patterson, D. A. </author> <year> (1990). </year> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: One of the techniques to alleviate the processor's stalls on observing writes is load bypassing, that is, a memory load can bypass memory stores that are buffered and thus overlap between accesses can be exploited. Load bypassing is essentially required for processors with dynamic scheduling <ref> [Hennessy & Patterson 90] </ref>. A second technique is to relax the memory consistency model. A consistency model is an agreement between the parallel programs and the multiprocessor architecture on the ordering that shared references must observe. <p> In the case of dynamic instruction scheduling, introducing out-of-order execution, a more complicated scoreboarding mechanism is required. In addition, both static and dynamic instruction scheduling strategies need interrupt handling routines that can deal with interrupts generated by the non-blocking operations <ref> [Hennessy & Patterson 90] </ref>. A consistency problem can arise when the processor allows non-blocking writes since a later (in program order) read may be needed before a previous buffered write is performed.
Reference: [Hum & Gao 91] <author> Hum, H. J. and Gao, G. R. </author> <year> (1991). </year> <title> Efficient support of concurrent threads in a hybrid dataflow/von neumann architecture. </title> <type> Technical Report 35, </type> <institution> McGill University. </institution>
Reference-contexts: The two key issues for implementing multiple-context processors are: when is context switching performed [Boothe & Ranade 92, Laudon et al. 92], 4 and what defines a context <ref> [Hum & Gao 91] </ref>. Variations on these issues include conditional-switch, switch-on-cache-miss, and switch-every-cycle. The cache coherence, or cache consistency, problem [Archibald & Baer 86] arises in shared-memory multiprocessors where several copies of the same block can be present in the local caches of the individual processors.
Reference: [Jain 91a] <author> Jain, R. </author> <year> (1991a). </year> <title> The Art of Computer System Performance Anaylsis. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: When an average reduction of MCPI is summarized, a geometric mean is used to average the percentages of the penalty reduction for the benchmarks <ref> [Jain 91a] </ref>.
Reference: [Jain 91b] <author> Jain, S. </author> <year> (1991b). </year> <title> Circular scheduling: a new technique to perform software pipelining. </title> <booktitle> In Proc. SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 219--228. </pages>
Reference-contexts: Register renaming at compile time has been used in conjunction with software pipelining <ref> [Jain 91b] </ref>. The advantage of register renaming at compile time over dynamically renaming at run time is that the compiler can take more advantage of increased instruction parallelism (as a result of renaming) by distributing the parallelism more effectively in the code.
Reference: [Jeremiassen & Eggers 92] <author> Jeremiassen, T. E. and Eggers, S. J. </author> <year> (1992). </year> <title> Computing per-processor summary side-effect information. </title> <booktitle> In Proc. of workshop on Language and Compilers for Parallel Computing. </booktitle>
Reference-contexts: Hardware approaches should be able to fetch back the data which were invalidated, if the state information mandates the prefetching. In case that most invalidation misses are attributed to false sharing, those misses can be minimized by reorganizing the shared data. As a result, an algorithm <ref> [Jeremiassen & Eggers 92] </ref> that restructures shared data to reduce false sharing can be incorporated in the software prefetching schemes. Task scheduling and task migration make prefetching in multiprocessors more complicated, because processor assignments may change before the prefetched data in the cache has been used.
Reference: [Jouppi 90] <author> Jouppi, N. P. </author> <year> (1990). </year> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. of the 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 364--373. 125 </pages>
Reference-contexts: In this section, we investigate an alternative to prefetching in the primary cache, namely prefetching in a separate prefetch buffer. We also contrast the prefetch buffer solution with a victim cache <ref> [Jouppi 90] </ref>. The separate prefetch buffer has a block size the same as that of the primary data cache [Klaiber & Levy 91, Chen et al. 91]. The rationale is to nullify data pollution effects.
Reference: [Kerns & Eggers 92] <author> Kerns, D. R. and Eggers, S. </author> <year> (1992). </year> <title> Balanced scheduling: instruction scheduling when memory latency is uncertain. </title> <type> Technical Report 92-11-03, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle WA. </address>
Reference-contexts: To achieve additional overlap, the compiler may perform instruction scheduling to avoid unnecessary stalls by keeping the CPU busy <ref> [Gibbons & Muchnick 86, Krishnamurthy 90, Kerns & Eggers 92] </ref>. The freedom of instruction scheduling in most compiler algorithms is limited by the data and control dependencies in the programs. Better performance is 3 generally achieved by moving loads far enough ahead of their uses. <p> For instance, a load of an array element with a large stride is likely to be a cache miss while accesses to the stack area will most often result in cache hits. An intelligent compiler could take this into account when assigning edge latencies. Balanced scheduling <ref> [Kerns & Eggers 92] </ref> can also be incorporated in the compiler for distributing instruction parallelism only to the accesses which have been determined as misses. Before instruction scheduling, we may perform software register renaming on the code so that the instruction scheduler would have more freedom to move instructions.
Reference: [Klaiber & Levy 91] <author> Klaiber, A. C. and Levy, H. M. </author> <year> (1991). </year> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proc. of the 18th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 43--53. </pages>
Reference-contexts: Depending on how prefetches are determined and initiated, prefetching can be either hardware-controlled [Baer & Chen 91, Fu & Patel 92] or software-directed <ref> [Porterfield 89, Klaiber & Levy 91, Mowry et al. 92] </ref>. The hardware approach detects accesses with regular patterns and issues prefetches at run time, whereas the software approach relies on the compiler to analyze programs and to insert prefetch instructions. <p> We also contrast the prefetch buffer solution with a victim cache [Jouppi 90]. The separate prefetch buffer has a block size the same as that of the primary data cache <ref> [Klaiber & Levy 91, Chen et al. 91] </ref>. The rationale is to nullify data pollution effects. <p> Most software approaches proposed in the past mainly focus on the loop domain for uniprocessors and most of them study prefetching based on codes with manually inserted prefetches <ref> [Porterfield 89, Klaiber & Levy 91, Mowry & Gupta 91] </ref>. Because Mowry et. al.'s [92] 56 scheme is the only one, to our knowledge, that has been automated in an experimental compiler, we will basically use their framework as the basis of our comparison.
Reference: [Kowalik 85] <author> Kowalik, J. S. </author> <year> (1985). </year> <title> Parallel MIMD Computation: the HEP Supercomputer and its application. </title> <publisher> MIT Press. </publisher>
Reference: [Krishnamurthy 90] <author> Krishnamurthy, S. M. </author> <year> (1990). </year> <title> A brief survey of papers on scheduling for pipelined processors. </title> <journal> SIGPLAN Notices, 25(7):97--106. </journal>
Reference-contexts: To achieve additional overlap, the compiler may perform instruction scheduling to avoid unnecessary stalls by keeping the CPU busy <ref> [Gibbons & Muchnick 86, Krishnamurthy 90, Kerns & Eggers 92] </ref>. The freedom of instruction scheduling in most compiler algorithms is limited by the data and control dependencies in the programs. Better performance is 3 generally achieved by moving loads far enough ahead of their uses. <p> The schemes discussed in the above require fairly complex hardware. Performance can be improved by compiler assistance. Compiler optimizations for non-blocking loads mainly consist of instruction reordering and the insertion of independent instructions after non-blocking loads to keep the processor as busy as possible. Traditional instruction list schedulers <ref> [Gibbons & Muchnick 86, Krishnamurthy 90] </ref> can be employed to perform the code scheduling. More recently, Kerns and Eggers [92] proposed balanced scheduling, which is particularly suitable to non-blocking loads since the latency of a load is unknown until run time. <p> Instruction scheduling is a compiler optimization phase which schedules as many operations as possible in parallel on separate functional units. Several instruction scheduling techniques based on the architecture of a specific target machine have been proposed in the literature <ref> [Krishnamurthy 90] </ref>. Those traditional schedulers focus on instruction scheduling subject to machine resource constraints. More recently, Kerns and Eggers [92] have proposed ``balanced scheduling,'' where instructions are scheduled based on an estimate of the amount of instruction-level parallelism in the code.
Reference: [Kroft 81] <author> Kroft, D. </author> <year> (1981). </year> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proc. of the 8th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 81--87. </pages>
Reference-contexts: The next solution is a design that allows the processor to continue execution on unsatisfied memory references through the use of non-blocking caches (also called lockup-free caches <ref> [Kroft 81] </ref>). The non-blocking caches (see Section 2.2 for more detail) hide the latency of data misses through the overlap of data accesses and computations to the extent allowed by the data dependencies and consistency requirements. <p> When the buffer is full, incoming prefetches are just discarded. Each processor has a 64K-byte data cache, which is direct-mapped and copy-back with a cache line size of 16 bytes. The caches are lockup-free <ref> [Kroft 81] </ref>, thus allowing multiple outstanding data requests. A 16-entry outstanding request list (ORL) is used to keep track of pending requests, some of which might then become hit-wait accesses. As mentioned above, the cache hierarchy is used only for storing shared data.
Reference: [Kurihara et al. 91] <author> Kurihara, K., Chaiken, D., and Agarwal, A. </author> <year> (1991). </year> <title> Latency tolerance through multithreading in large-scale multiprocessing. </title> <booktitle> In Proc. of Int. Symp. on Shared Memory Multiprocessing, </booktitle> <pages> pages 91--101. </pages>
Reference: [Lam 88] <author> Lam, M. S. </author> <year> (1988). </year> <title> Sotfware pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proc. ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318--328. </pages>
Reference-contexts: However, as shown in the example, the loop has been increased roughly by two times (at most the code is within three times the original code <ref> [Lam 88] </ref>), the expansion of outer loops would be more significant and the problem in an instruction cache would be even more difficult to avoid. The other side effect as a result of code expansion is an increase in register pressure, which may introduce extra spilling store/load instructions.
Reference: [Laudon et al. 92] <author> Laudon, J., Gupta, A., and Horowitz, M. </author> <year> (1992). </year> <title> Architectural and implementations tradeoffs in the design of multiple-context processors. </title> <type> Technical Report CSL-TR-92-523, </type> <institution> Stanford University. </institution>
Reference-contexts: If several threads are assigned to a processor, memory latencies can be masked by rapidly context switching to a different thread rather than waiting for a memory reference to complete. The two key issues for implementing multiple-context processors are: when is context switching performed <ref> [Boothe & Ranade 92, Laudon et al. 92] </ref>, 4 and what defines a context [Hum & Gao 91]. Variations on these issues include conditional-switch, switch-on-cache-miss, and switch-every-cycle.
Reference: [Lee & Smith 84] <author> Lee, J. K. F. and Smith, A. J. </author> <year> (1984). </year> <title> Branch prediction strategies and branch target buffer design. </title> <booktitle> Computer, </booktitle> <pages> pages 6--22. </pages>
Reference-contexts: As shown in Figure 3.4, the LA-PC is maintained with the help of a branch prediction mechanism BPT. BPT designs have been thoroughly investigated <ref> [Lee & Smith 84, Perleberg & Smith 89] </ref> and we will not repeat these studies here. <p> BPT designs have been thoroughly investigated [Lee & Smith 84, Perleberg & Smith 89] and we will not repeat these studies here. In our experiments we use the Branch Target Buffer (BTB) with two-bit state transition design described in <ref> [Lee & Smith 84] </ref> and we assume that the BTB has been implemented in the core processor for other purposes. As the LA-distance increases, the data prefetch can be issued early enough so that the memory latency can be completely hidden. <p> The RPT we use is a 512-entry table organized as shown in Figure 3.2. When the schemes with lookahead are evaluated, branch predictions are performed by a Branch Target Buffer with a two-bit state transition design <ref> [Lee & Smith 84] </ref>. Both baseline and prefetching caches will cause the processor to stall on a cache miss. <p> Constant stride references, which may substantially contribute to cache misses, should be helped by the RPT schemes. Prefetching should be avoided for unpredictable irregular references. The column branch prediction miss ratio shows the outcome of branch predictions with a 512-entry BPT, which functions like a 2-state-bit Branch Target Buffer <ref> [Lee & Smith 84] </ref>. This is a second indication of the reference predictability, illustrating the possible benefits exploited by the lookahead approach. We experimented with the three architectural choices, and varying architectural parameters, described previously.
Reference: [Lee et al. 87a] <author> Lee, R. L., Yew, P.-C., and Lawrie, D. H. </author> <year> (1987a). </year> <title> Data prefetching in shared memory multiprocessors. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <pages> pages 28--31. 126 </pages>
Reference-contexts: When the LA-PC finds an entry in the BPT, it indicates that the LA-PC points to a branch instruction. In that case, the prediction result of the branch entry in the BPT is provided to modify the LA-PC. Note that, unlike the instruction prefetch structure in <ref> [Lee et al. 87a] </ref> or decoupled architectures [Smith 82b], the system does not need to decode the predicted instruction stream.
Reference: [Lee et al. 87b] <author> Lee, R. L., Yew, P.-C., and Lawrie, D. H. </author> <year> (1987b). </year> <title> Multprocessor cache design considerations. </title> <booktitle> In Proc. of the 14th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 253--262. </pages>
Reference-contexts: In a shared-memory multiprocessor environment, where consistency requirements are taken into account, prefetching can be either binding (at compile time) <ref> [Lee et al. 87b, Gornish et al. 90] </ref> or non-binding (supported by hardware coherence) [Mowry & Gupta 91, Tullsen & Eggers 93].
Reference: [Motorola 90] <author> Motorola (1990). </author> <title> MC88100 RISC Microprocessor User's Manual. </title> <publisher> Pren-tice Hall. </publisher>
Reference-contexts: If static instruction scheduling in pipelines is used in the processor, some form of register interlock (like a full/empty bit for each register) is needed for preserving correct data dependencies. For instance, the register file in the MC88100 <ref> [Motorola 90] </ref> includes a scoreboard register, which contains one such bit for each of the general-purpose registers. In the case of dynamic instruction scheduling, introducing out-of-order execution, a more complicated scoreboarding mechanism is required.
Reference: [Mowry & Gupta 91] <author> Mowry, T. and Gupta, A. </author> <year> (1991). </year> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, 12(2):87--106. </journal>
Reference-contexts: In a shared-memory multiprocessor environment, where consistency requirements are taken into account, prefetching can be either binding (at compile time) [Lee et al. 87b, Gornish et al. 90] or non-binding (supported by hardware coherence) <ref> [Mowry & Gupta 91, Tullsen & Eggers 93] </ref>. In the former case, a data block location is bound to the value of prefetched data in caches at the time that the prefetch completes, whereas in the latter case, data is kept coherent by the cache coherency mechanism. <p> The lockup-free cache in RP3 [Brantley et al. 85] supports non-blocking prefetches and multiple outstanding stores. However, reads are blocking until the missed datum returns. The lockup-free cache appearing in the studies of the DASH multiprocessor <ref> [Gharachorloo et al. 91a, Mowry & Gupta 91] </ref> allows multiple outstanding write and prefetch requests, while the processor still stalls on read misses. In principle, the capability of handling multiple pending requests is essential for prefetching or any other ``buffering'' and ``pipelining'' techniques that we have reviewed. <p> Mowry and Gupta <ref> [Mowry & Gupta 91] </ref> study software prefetching in a secondary-level remote access cache (RAC) in the context of the DASH cluster architecture. Prefetching in the RAC is the default but their results show that prefetching in the primary cache would have been more effective. <p> Most software approaches proposed in the past mainly focus on the loop domain for uniprocessors and most of them study prefetching based on codes with manually inserted prefetches <ref> [Porterfield 89, Klaiber & Levy 91, Mowry & Gupta 91] </ref>. Because Mowry et. al.'s [92] 56 scheme is the only one, to our knowledge, that has been automated in an experimental compiler, we will basically use their framework as the basis of our comparison.
Reference: [Mowry et al. 92] <author> Mowry, T., Lam, M. S., and Gupta, A. </author> <year> (1992). </year> <title> Design and evaluation of a compiler algoritm for prefetching. </title> <booktitle> In Proc. of the 5th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62--73. </pages>
Reference-contexts: Depending on how prefetches are determined and initiated, prefetching can be either hardware-controlled [Baer & Chen 91, Fu & Patel 92] or software-directed <ref> [Porterfield 89, Klaiber & Levy 91, Mowry et al. 92] </ref>. The hardware approach detects accesses with regular patterns and issues prefetches at run time, whereas the software approach relies on the compiler to analyze programs and to insert prefetch instructions.
Reference: [Murakami et al. 89] <author> Murakami, K., Irie, N., and Tomita, S. </author> <year> (1989). </year> <title> SIMP (single instruction stream / multiple instruction pipelining): A novel high-speed single-processor architecture. </title> <booktitle> In Proc. of the 16th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 78--85. </pages>
Reference-contexts: The memory requirements include a write buffer allowing load bypassing and a cache capable of servicing multiple requests. The SIMP architecture <ref> [Murakami et al. 89] </ref>, which provides dynamic dependency resolution with speculative branch prediction, can take more advantage of lockup-free caches than a processor with MSHR's. Gharachorloo et al. [92] explore the advantages of relaxed consistency models in dynamically scheduled processors.
Reference: [Nikhi et al. 91] <author> Nikhi, R. S., Papadopoulos, G. M., </author> <title> and Arvind (1991). *T: A multi-threaded massively parallel architecture. </title> <type> Technical report, </type> <institution> MIT Computer Science. </institution>
Reference: [Oehler & Groves 90] <author> Oehler, R. R. and Groves, R. D. </author> <year> (1990). </year> <title> IBM RISC System/6000 processor architecture. </title> <institution> IBM J. Res. Development, 34(1):23--36. </institution>
Reference-contexts: Stenstrom et al. [91] formulate access order information from programs, so that a lockup-free cache can exploit this information to achieve performance improvements. They present an implementation which can support and control pipelining among multiple accesses. In the IBM RS/6000 processor <ref> [Oehler & Groves 90] </ref>, register tagging is implemented to allow data cache accesses to overlap with the execution of subsequent independent register-to-register instructions. The schemes discussed in the above require fairly complex hardware. Performance can be improved by compiler assistance.
Reference: [Pan et al. 92] <author> Pan, S.-T., So, K., and Rahmeh, J. T. </author> <year> (1992). </year> <title> Improving the accuracy of dynamic branch prediction using branch correlation. </title> <booktitle> In Proc. of the 5th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 76--84. </pages>
Reference-contexts: We can observe though that there is a correlation between the accesses due to the termination of the inner loop (i.e., B [1,0], B [2,0], B [3,0] etc. have a stride of 400). Correlation has led to the design of more accurate branch prediction <ref> [Pan et al. 92, Yeh & Patt 92] </ref> and can be equally applied to data reference prediction. <p> This assumption could be removed by letting the shift register be modified on every branch as in <ref> [Pan et al. 92] </ref>. While some predictable patterns might emerge, it is not evident that the complexity of implementation is warranted. Second, we assume that the loop iterations are controlled by backward branches 1 .
Reference: [Perleberg & Smith 89] <author> Perleberg, C. H. and Smith, A. J. </author> <year> (1989). </year> <title> Branch target buffer design and optimization. </title> <type> Technical Report UCB/CSD 89/552, </type> <institution> University of California, Berkeley. </institution> <month> 127 </month>
Reference-contexts: As shown in Figure 3.4, the LA-PC is maintained with the help of a branch prediction mechanism BPT. BPT designs have been thoroughly investigated <ref> [Lee & Smith 84, Perleberg & Smith 89] </ref> and we will not repeat these studies here.
Reference: [Porterfield 89] <author> Porterfield, A. K. </author> <year> (1989). </year> <title> Software methods for improvement of cache performance on supercomputer applications. </title> <type> Technical Report COMP TR 89-93, </type> <institution> Rice University. </institution>
Reference-contexts: Depending on how prefetches are determined and initiated, prefetching can be either hardware-controlled [Baer & Chen 91, Fu & Patel 92] or software-directed <ref> [Porterfield 89, Klaiber & Levy 91, Mowry et al. 92] </ref>. The hardware approach detects accesses with regular patterns and issues prefetches at run time, whereas the software approach relies on the compiler to analyze programs and to insert prefetch instructions. <p> Most software approaches proposed in the past mainly focus on the loop domain for uniprocessors and most of them study prefetching based on codes with manually inserted prefetches <ref> [Porterfield 89, Klaiber & Levy 91, Mowry & Gupta 91] </ref>. Because Mowry et. al.'s [92] 56 scheme is the only one, to our knowledge, that has been automated in an experimental compiler, we will basically use their framework as the basis of our comparison.
Reference: [Przybylski 90] <author> Przybylski, S. </author> <year> (1990). </year> <title> The performance impact of block sizes and fetch strategies. </title> <booktitle> In Proc. of the 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 160--169. </pages>
Reference-contexts: the Non-overlapped model. 4.3.2 Effect of Block Size It is well known that for a cache of given capacity and associativity, the block size that leads to the best hit ratio is a compromise between very large sizes to increase the spatial locality and small sizes to reduce conflict misses <ref> [Przybylski 90] </ref>. Given that a prefetching scheme will increase the spatial locality, we can predict that the best block size for a prefetching scheme should be smaller than or equal to that of the pure cache. block size. The baseline is a 32K-byte direct-mapped cache.
Reference: [Rogers & Li 92] <author> Rogers, A. and Li, K. </author> <year> (1992). </year> <title> Software support for speculative loads. </title> <booktitle> In Proc. of the 5th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38--50. </pages>
Reference-contexts: The freedom of instruction scheduling in most compiler algorithms is limited by the data and control dependencies in the programs. Better performance is 3 generally achieved by moving loads far enough ahead of their uses. Like ``regular'' non-blocking loads, speculative loads <ref> [Chen et al. 92, Rogers & Li 92] </ref> fetch memory values into registers directly bypassing the cache if need be, i.e., without blocking the pipeline on a miss, and allow other instructions to be executed simultaneously.
Reference: [Scheurich & Dubois 91] <author> Scheurich, C. and Dubois, M. </author> <year> (1991). </year> <title> Lockup-free caches in high-performance multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, 11(1):25--36. </journal>
Reference: [Singh et al. 92] <author> Singh, J. P., Weber, W.-D., and Gupta, A. </author> <year> (1992). </year> <title> SPLASH: Stanford parallel applications for shared-memory. Computer Architecture News, </title> <publisher> 20(1):5--44. </publisher>
Reference-contexts: Applications executed (K) reads (K) writes (K) Locks Barriers size (K bytes) Matmat 8,723 1,355 421 0 82 2109 Mp3d 7,231 1,334 426 10 60 3673 Water 21,173 1,033 72 8,737 25 156 Cholesky 38,233 6,809 524 5,671 81 6403 The benchmarks we used are Matmat and three SPLASH benchmarks <ref> [Singh et al. 92] </ref>. To study the architectures with a moderate cache size, we run the benchmark programs with larger data sets than what are provided in the benchmark. Table 5.1 summarizes the statistics collected on these benchmarks once their parallel sections are started up until the program is completed.
Reference: [Sklenar 92] <author> Sklenar, I. </author> <year> (1992). </year> <title> Prefetch unit for vector operations on scalar computers. Computer Architecture News, </title> <publisher> 20(4):31--37. </publisher>
Reference-contexts: For example, it can be a support unit for an on-chip cache. It is worthwhile to close this chapter with a brief qualitative comparison of our design with the two closest approaches <ref> [Fu & Patel 92, Sklenar 92] </ref> that were reviewed in Section 2.1.1. Fu and Patel [92] present a mechanism which will generate a prefetch request based on two consecutive accesses by adding the current effective address with the difference between the current and previous addresses.
Reference: [Smith 82a] <author> Smith, A. J. </author> <year> (1982a). </year> <title> Cache memories. </title> <journal> ACM Computing Surveys, 14(3):473--530. </journal>
Reference-contexts: In this introductory chapter, we briefly list some of them; those most germane to our work will be described in more detail in the next chapter. The simplest technique is a write buffer, a FIFO queue which is used to hide the write latency <ref> [Smith 82a] </ref>. Buffering is a performance enhancement for caches both with write-back and write-through strategies. Under write-back, the write buffer is used to hold replaced dirty blocks while ``normal'' execution proceeds. <p> An ORL with N entries is associated with each module. - Pipelined (N) : A request can be issued at every cycle. This model is representative of processor-cache pairs being linked to memory modules through a pipelined packet-switched interconnection network. We assume a load through 35 mechanism <ref> [Smith 82a] </ref>, i.e., the desired word is available as soon as the first data response arrives. An N-entry ORL is associated with the cache. The configurations of the ORLs used in our experiment are Non-overlapped (1), Overlapped (8,2), and Pipelined (8) respectively.
Reference: [Smith 82b] <author> Smith, J. E. </author> <year> (1982b). </year> <title> Decoupled access/execute computer architectures. </title> <booktitle> In Proc. of the 9th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 112--119. </pages>
Reference-contexts: When an incorrect branch prediction is detected, the execution will stall waiting for the buffer to be flushed. Implicit prefetching is present in decoupled architectures <ref> [Smith 82b] </ref>, where two instruction streams operate concurrently, communicate via queues and drive two execution units: one for data access and one for functional operations. The data access stream can be ``ahead'' of the functional stream and hence can prefetch operands most likely needed in the near future. <p> In that case, the prediction result of the branch entry in the BPT is provided to modify the LA-PC. Note that, unlike the instruction prefetch structure in [Lee et al. 87a] or decoupled architectures <ref> [Smith 82b] </ref>, the system does not need to decode the predicted instruction stream.
Reference: [Smith et al. 90] <author> Smith, M. D., Lam, M., and Horowitz, M. A. </author> <year> (1990). </year> <title> Boosting beyond static scheduling in a superscalar processor. </title> <booktitle> In Proc. of the 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 344--254. 128 </pages>
Reference: [Smith et al. 91] <author> Smith, R., Archibald, J., and Nelson, B. </author> <year> (1991). </year> <title> A timing based simulation study of prefetching in a second level cache. </title> <type> Technical Report TR-A105-91.3, </type> <institution> Brigham Young University. </institution>
Reference: [Sohi & Franklin 91] <author> Sohi, G. S. and Franklin, M. </author> <year> (1991). </year> <title> High-bandwidth data memory systems for superscalar processor. </title> <booktitle> In Proc. of the 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53--62. </pages>
Reference-contexts: The following is a brief qualitative view of the expected benefits for both types of overlap. 93 Non-blocking loads delay processor's stalls until the necessary data dependence is encountered. Non-blocking loads will become necessary for processors, such as superscalar processors, capable of issuing multiple instructions per cycle <ref> [Sohi & Franklin 91] </ref>. However, the non-blocking distance, which is the number of instructions that can be overlapped with the memory access (e.g., instructions between the reference and the first dependent instruction), is likely to be small in the case of static scheduling.
Reference: [Stenstrom et al. 91] <author> Stenstrom, P., Dahlgren, F., and Lundberg, L. </author> <year> (1991). </year> <title> A lockup-free multiprocesso cache design. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <address> pages I--246 -- I--250. </address>
Reference: [Tullsen & Eggers 93] <author> Tullsen, D. M. and Eggers, S. J. </author> <year> (1993). </year> <title> Limitation of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proc. of the 20th Annual Intl. Symp. on Computer Architecture. </booktitle>
Reference-contexts: In a shared-memory multiprocessor environment, where consistency requirements are taken into account, prefetching can be either binding (at compile time) [Lee et al. 87b, Gornish et al. 90] or non-binding (supported by hardware coherence) <ref> [Mowry & Gupta 91, Tullsen & Eggers 93] </ref>. In the former case, a data block location is bound to the value of prefetched data in caches at the time that the prefetch completes, whereas in the latter case, data is kept coherent by the cache coherency mechanism.
Reference: [Weber & Gupta 89] <author> Weber, W.-D. and Gupta, A. </author> <year> (1989). </year> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proc. of the 16th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 273--280. </pages>
Reference: [Wolf & Lam 91] <author> Wolf, M. E. and Lam, M. </author> <year> (1991). </year> <title> A data locality optimizing algorithm. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30--44. </pages>
Reference-contexts: Since reuses do not guarantee locality <ref> [Wolf & Lam 91] </ref>, These reuses are mapped to data locality by taking into account the loop iteration count and the cache size. Let us take a typical inner loop as an example (as shown in Figure 5.1). The accesses of A [j][i] have spatial reuse in the loop.
Reference: [Yeh & Patt 92] <author> Yeh, T. and Patt, Y. N. </author> <year> (1992). </year> <title> Alternative implementation of two-level adaptive branch prediction. </title> <booktitle> In Proc. of the 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 124--134. </pages>
Reference-contexts: We can observe though that there is a correlation between the accesses due to the termination of the inner loop (i.e., B [1,0], B [2,0], B [3,0] etc. have a stride of 400). Correlation has led to the design of more accurate branch prediction <ref> [Pan et al. 92, Yeh & Patt 92] </ref> and can be equally applied to data reference prediction.
Reference: [Zucker 92] <author> Zucker, R. N. </author> <year> (1992). </year> <title> Relaxed Consistency and Synchronization in Parallel Processors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of Washington. </institution> <month> 129 </month>
Reference: [Zucker & Baer 92] <author> Zucker, R. N. and Baer, J.-L. </author> <year> (1992). </year> <title> A performance study of memory consistency models. </title> <booktitle> In Proc. of the 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 2--12. </pages>
References-found: 71

