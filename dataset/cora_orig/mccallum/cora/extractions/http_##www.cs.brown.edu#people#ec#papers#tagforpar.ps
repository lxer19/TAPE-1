URL: http://www.cs.brown.edu/people/ec/papers/tagforpar.ps
Refering-URL: http://www.cs.brown.edu/people/ec/
Root-URL: 
Title: Taggers for Parsers  
Author: Eugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael Littman, and John McCann 
Date: 1910  
Note: Correspondence should be addressed to  
Address: Providence RI 02912  Box  Providence RI 02912.  
Affiliation: Department of Computer Science and Division of Engineering Brown University,  Eugene Charniak, Department of Computer Science,  Brown University,  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Boggess, L., Agarwal, R. and Davis, R. </author> <title> Disambiguation of prepositional phrases in automatically labeled technical text. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1991, </year> <pages> 155-159. </pages>
Reference: 2. <author> Brill, E. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. </booktitle> <year> 1992. </year>
Reference: 3. <author> Brill, E. </author> <title> Some advances in transformation-based part of speech tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1994, </year> <pages> 722-727. </pages>
Reference: 4. <author> Charniak, E. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Naturally, in the computation we need associated with each lexical rule its probability P (t ! w). With a little thought one can show the following equality: P (t ! w) = P (w j t) (14) See, for example, <ref> [4] </ref>. Now when we talk of attaching a parser to a tagger we are assuming that the parser has rules that break constituents down to the part-of-speech level, what we called the phrase-marker rules, but that the tagger replaces the lexical rules. <p> This is surely true, but to what degree? To suggest an answer to this question, note that there is an independent measure of the quality of tag-context models, their per-tag cross entropy. We do not go into detail here (see <ref> [4] </ref>) but simply note that, given a well-behaved corpus of n words, the per-tag cross entropy is well approximated by n The lower the cross entropy the better the model. So let us look at how tagging improves as the cross entropy decreases.
Reference: 5. <author> Charniak, E. and Carroll, G. </author> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1994. </year>
Reference-contexts: Second, several would-be edges are collapsed into one when they are identical except for their predictions of subsequent constituents. This significantly reduces the number of edges. The grammar used with the parser is the product of some related work on PCFG induction <ref> [5] </ref>. It consists of about 3500 rewrite rules (brevity was sacrificed for ease of learning) with 19 tags.
Reference: 6. <author> Charniak, E., Carroll, G., Adcock, J., Cassandra, A., Gotoh, Y., Katz, J., Littman, M. and McCann, J. </author> <title> Taggers for Parsers. </title> <institution> Department of Computer Science, Brown University, CS-94-06, </institution> <year> 1994. </year> <month> 16 </month>
Reference-contexts: Our intuition was that an average of two tags per word would lead to an explosion in the number of parses, particularly 10 for sentences of length 10 or more (a little more than half the corpus). In a technical report version of this paper <ref> [6] </ref> we show that adding a single tag could lead to a cubic increase in the number of edges. Figure 2 indicates that we are far from such a worst case.
Reference: 7. <author> Charniak, E., Hendrickson, C., Jacobson, N. and Perkowitz, M. </author> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1993, </year> <pages> 784-789. </pages>
Reference-contexts: The second term, P (t i j t i1 ), is called the "tag-context model" as it tends to make the tagger prefer tags that are likely to come after the tag for the previous word. For more on such equations, see <ref> [7] </ref>. It is the responsibility of the training phase to collect these two kinds of probabilities. However, a common problem for statistical taggers is that the set of examples found in the training data is not exhaustive, so that in the test 3 data the tagger encounters unforeseen situations. <p> Note that this is somewhat analogous to using P (t j w) rather than P (w j t) in Equation 1; for some time there was confusion on this point in the tagging literature, but this leads to a suboptimal result (see <ref> [7] </ref> ) 4 The Performance Measures We use five performance measures in this study, four of which are straightforward. First, we measure the parser's computational effort in terms of the average number of edges generated in the course of parsing a sentence. <p> For our tag set, the cross entropy of this model is 3.61 bits/tag. It is common knowledge that such models give about 90% tagging accuracy. A result of 91.5% is given in <ref> [7] </ref> and that is the figure we use here. 1 1 This figure was for a large tag set, and thus it would probably be higher for our smaller tag set.
Reference: 8. <author> Church, K. W. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing. ACL, </booktitle> <year> 1988, </year> <pages> 136-143. </pages>
Reference: 9. <author> DeRose, S. J. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <booktitle> Computational Linguistics 14 (1988), </booktitle> <pages> 31-39. </pages>
Reference: 10. <author> Francis, W. N. and Ku cera, H. </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: The rest of this section describes how this is done in our tagger. It is included for completeness | nothing in the rest of the paper depends on it | and can be skipped without penalty. We used a 300,000-word subset of the tagged Brown Corpus <ref> [10] </ref> for training. We assume that, even with this relative small training corpus, it is not necessary to smooth the estimated P (t i j t i1 ). <p> parser could get the tags correct while completely botching the parses, one would nevertheless expect, barring some "conspiracy," that the two measures would go up hand in hand. 5 Results Training and testing of taggers and the PCFG were done on a 307885 word subset of the tagged Brown Corpus <ref> [10] </ref>. This includes all sentences of length greater than 1 (i.e., sentences having a symbol other than the final punctuation mark) and less than 23 that do not include foreign words, titles, or certain symbols, most notably parentheses.
Reference: 11. <author> Jelinek, F. </author> <title> Markov source modeling of text generation. In The Impact of Processing Techniques on Communications, </title> <editor> J. K. Skwirzinski, Ed. </editor> <publisher> Nijhoff, </publisher> <address> Dordrecht, </address> <year> 1985. </year>
Reference: 12. <author> Kupiec, J. and Maxwell, J. </author> <title> Training stochastic grammars from unlabeled text corpora. </title> <booktitle> In Workshop Notes, Statistically-Based NLP Techniques. AAAI, </booktitle> <year> 1992, </year> <pages> 14-19. 13. </pages> <editor> deMarcken, C. G. </editor> <booktitle> Parsing the LOB corpus. In Proceedings of the 1990 Conference of the Association for Computational Linguistics. </booktitle> <year> 1990, </year> <pages> 243-251. </pages>
Reference: 14. <author> Marcus, M. P., Santorini, B. and Marcinkiewicz, M. A. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 313-330. </pages>
Reference-contexts: However, to measure this one needs a source of agreed-upon parses for the sentences. While there are now tree-banks of some size <ref> [14] </ref>, they of necessity make assumptions about grammatical formalism. As these assumptions do not fit the grammar we use, we cannot exploit these resources. On the other hand, using tagging accuracy for our performance measure has some advantages.
Reference: 15. <author> Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. and Palmucci, J. </author> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 359-382. </pages>
Reference-contexts: return a single best tag for each word (we call these "single taggers"), some work has been done on taggers that return a list of possible tags in those cases where a second (or even third) 1 best choice might be close to the best according to the tagger's metric <ref> [3,13, 15] </ref> (we call these "multiple taggers"). One obvious reason to do this would be to let the parser make the final decision. <p> One obvious reason to do this would be to let the parser make the final decision. For example, the section on multiple taggers in <ref> [15] </ref> starts by observing that even with a rather low error rate of 3.7%, there are cases in which the system returns the wrong tag, which can be fatal for a pars ing system trying to deal with sentences averaging more than 20 words in length.(p. 366) In this paper we <p> So while the first tagger returns what it considers the best overall tag sequence, the second tagger can identify alternative tags at a position with tag probabilities close to the best. (More formally, it computes the tag probabilities using the standard Markov-model forward-backward algorithm (as in <ref> [15] </ref>) and returns all tags t j such that P (t j j w 1;n ) P (t b j w 1;n ), where t b is the best (most probable) tag for that position and is a system parameter such that 0 1.
Reference: 16. <author> Zernik, U. Shipping departments vs. shipping pacemakers: </author> <title> using thematic analysis to improve tagging accuracy. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <year> 1992, </year> <pages> 335-342. 17 </pages>
Reference-contexts: First, it does not preclude further improvements in tagging. Work that looks for such improvements from collecting finer statistics based upon more lexical information still seems promising (e.g., <ref> [16] </ref>). Second, our result certainly does not imply that parsers are useless. One does not parse to get tags, one parses to find phrase markers. We may have ruled out multiple taggers as a route to improved parsing accuracy, but the need for parsers remains.
References-found: 15

