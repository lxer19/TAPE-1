URL: ftp://ftp.speech.sri.com/pub/people/francois/speechtrans.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Title: Robust Text-Independent Speaker Identification over Telephone Channels  
Author: Hema A. Murthy, Fran~coise Beaufays, Larry P. Heck, Mitchel Weintraub 
Address: Address: Room EJ  333 Ravenswood Ave, Menlo Park, CA 94025, USA  
Affiliation: Beaufays  SRI International,  
Note: EDICS Number: SA 1.8.1, manuscript number SA-594 (Revised version  Corresponding Author: Fran~coise  
Email: e-mail: francois@speech.sri.com  
Phone: 129,  Phone: (650) 859 5698 Fax: (650) 859 5984  
Date: Dec. 1997)  
Abstract: This paper addresses the issue of closed-set text-independent speaker identification from samples of speech recorded over the telephone. It focuses on the effects of acoustic mismatches between training and testing data, and concentrates on two approaches: extracting features that are robust against channel variations, and transforming the speaker models to compensate for channel effects. First, an experimental study shows that optimizing the front end processing of the speech signal can significantly improve speaker recognition performance. A new filterbank design is introduced to improve the robustness of the speech spectrum computation in the front-end unit. Next, a new feature based on spectral slopes is described. Its ability to discriminate between speakers is shown to be superior to that of the traditional cepstrum. This feature can be used alone or combined with the cepstrum. The second part of the paper presents two model transformation methods that further reduce channel effects. These methods make use of a locally collected stereo database to estimate a speaker-independent variance transformation for each speech feature used by the classifier. The transformations constructed on this stereo database can then be applied to speaker models derived from other databases. Combined, the methods developed in this paper resulted in a 38% relative improvement on the closed-set 30-second training 5-second testing condition of the NIST'95 Evaluation task, after cepstral mean removal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. A. Reynolds, </author> <title> "MIT Lincoln Laboratory site presentation", NIST Speaker Recognition Workshop, </title> <institution> Linthicum Heights, Maryland, </institution> <month> 27 March </month> <year> 1996. </year>
Reference-contexts: Recently, acoustic mismatches have also addressed at the classification level, by estimating which type of telephone unit is used in the communication and by modifying the speaker classification algorithm accordingly <ref> [1] </ref>, [2]. A. The Feature Extraction Problem The exact factors in a speech signal that are responsible for speaker characteristics are not exactly known, but it is a fact that humans are able to distinguish among speakers based on their voices. <p> Specifically, we implemented the bilinear transformation proposed by Acero [20], ! new = ! + 2 atan F w sin! ; where the constant F w 2 <ref> [0; 1] </ref> controls the amount of warping. The frequency scale is then multiplied by a bank of N f filters whose center frequencies are uniformly distributed in the interval [Min f ; Max f ], along the warped frequency axis.
Reference: [2] <author> L. P. Heck and M. W. Weintraub, </author> <title> "Handset-dependent background models for robust text-independent speaker recognition", </title> <note> to appear in Proc. </note> <institution> ICASSP-97, Munich, Germany, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Recently, acoustic mismatches have also addressed at the classification level, by estimating which type of telephone unit is used in the communication and by modifying the speaker classification algorithm accordingly [1], <ref> [2] </ref>. A. The Feature Extraction Problem The exact factors in a speech signal that are responsible for speaker characteristics are not exactly known, but it is a fact that humans are able to distinguish among speakers based on their voices.
Reference: [3] <author> G. Doddington, </author> <title> "Speaker recognition-identifying people by their voices," </title> <journal> Proc. IEEE, </journal> <volume> vol. 73, no. 11, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Studies on inter-speaker variations and factors affecting voice quality have revealed that there are various parameters at both the segmental and suprasegmental levels that contribute to speaker variability <ref> [3] </ref>, [4], [5], [6], [7]. Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8].
Reference: [4] <author> G. Fant, A. Kruckenberg and L. Nord, </author> <title> "Prosodic and segmental speaker variations," </title> <journal> Speech Communication, </journal> <volume> vol. 10, </volume> <pages> pp. 521-531, </pages> <year> 1991. </year>
Reference-contexts: Studies on inter-speaker variations and factors affecting voice quality have revealed that there are various parameters at both the segmental and suprasegmental levels that contribute to speaker variability [3], <ref> [4] </ref>, [5], [6], [7]. Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8].
Reference: [5] <author> D. H. Klatt and L. C. Klatt, </author> <title> "Analysis, synthesis and perception of voice quality variations among female and male talkers," </title> <journal> J. Acoust. Soc. Amer., </journal> <volume> vol. 87, no. 2, </volume> <pages> pp. 820-857, </pages> <year> 1990. </year>
Reference-contexts: Studies on inter-speaker variations and factors affecting voice quality have revealed that there are various parameters at both the segmental and suprasegmental levels that contribute to speaker variability [3], [4], <ref> [5] </ref>, [6], [7]. Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8].
Reference: [6] <author> D. G. Childers and C. K. Lee, </author> <title> "Voice quality factors, analysis, synthesis and perception," </title> <journal> J. Acoust. Soc. Amer., </journal> <volume> vol. 90, no. 5, </volume> <pages> pp. 2394-2410, </pages> <year> 1991. </year>
Reference-contexts: Studies on inter-speaker variations and factors affecting voice quality have revealed that there are various parameters at both the segmental and suprasegmental levels that contribute to speaker variability [3], [4], [5], <ref> [6] </ref>, [7]. Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8].
Reference: [7] <author> M. Narendranath, H. A. Murthy, B. Yegnanarayana and S. Rajendran, </author> <title> "Transformation of formants for voice conversion using artificial neural networks," </title> <journal> Speech Communication, </journal> <volume> vol. 16, </volume> <pages> pp. 207-216, </pages> <year> 1995. </year>
Reference-contexts: Studies on inter-speaker variations and factors affecting voice quality have revealed that there are various parameters at both the segmental and suprasegmental levels that contribute to speaker variability [3], [4], [5], [6], <ref> [7] </ref>. Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8].
Reference: [8] <author> D. A. Reynolds, </author> <title> "Speaker identification and verification using Gaussian mixture speaker models," </title> <booktitle> Proc. ESCA Workshop on Automatic Speaker Recognition, </booktitle> <pages> pp. 27-30, </pages> <year> 1994. </year>
Reference-contexts: Despite the fact that one cannot exactly quantify inter-speaker variability in terms of features, current speaker identification systems perform very well with clean speech. However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines <ref> [8] </ref>. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], [10], [11], [12], [13], [14]), and significant performance improvements have been reported from developing sophisticated speaker models.
Reference: [9] <author> T. Matsui and S. Furui, </author> <title> "A text-independent speaker recognition method robust against utterance variations", </title> <booktitle> Proc. ICASSP-91, </booktitle> <pages> pp. 377-390, </pages> <year> 1991. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. <ref> [9] </ref>, [10], [11], [12], [13], [14]), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum.
Reference: [10] <author> M. Savic and J. Sorenson, </author> <title> "Phoneme based speaker verification", </title> <booktitle> Proc. ICASSP-92, </booktitle> <pages> pp. 165-168, </pages> <year> 1992. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], <ref> [10] </ref>, [11], [12], [13], [14]), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum.
Reference: [11] <author> D. A. Reynolds and R. C. Rose, </author> <title> "Robust text-independent speaker identification using Gaussian mixture speaker models", </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 72-83, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], [10], <ref> [11] </ref>, [12], [13], [14]), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum. <p> B. The Model Transformation Problem The second part of this work aims at developing transformation algorithms that render the speaker models more robust to acoustic mismatches. Many ASR systems rely on cepstral mean subtraction (CMS) [16] to compensate for channel effects [17], <ref> [11] </ref>. It is well-known, however, that channel mismatches can still be a significant source of errors after CMS. Preliminary experiments reported in Section III-B confirm this point. For this reason, more sophisticated cepstrum transformation methods have been proposed in the literature.
Reference: [12] <author> H. Gish, M. Schmidt and A. Mielke, </author> <title> "A robust, segmental method for text-independent speaker identifca-tion," </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. 145-148, </pages> <year> 1994. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], [10], [11], <ref> [12] </ref>, [13], [14]), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum.
Reference: [13] <author> J. P. Eatoch and J. S. Mason, </author> <title> "A quantitative assessment of the relative speaker discriminating properties of phonemes," </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. 133-136, </pages> <year> 1994. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], [10], [11], [12], <ref> [13] </ref>, [14]), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum.
Reference: [14] <author> K. T. Assaleh and R. J. Mammone, </author> <title> "Robust cepstral features for speaker identification," </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. 129-132, </pages> <year> 1994. </year>
Reference-contexts: However, the performance of these systems can decrease significantly under certain acoustic conditions, such as noisy telephone lines [8]. In the last few years, much of the speaker identification research has been devoted to modeling issues (e.g. [9], [10], [11], [12], [13], <ref> [14] </ref>), and significant performance improvements have been reported from developing sophisticated speaker models. Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum.
Reference: [15] <author> C. R. Janowski Jr., T. F. Quatieri and D. A. Reynolds, </author> <title> "Measuring fine structure in speech: Application to speaker identification," </title> <booktitle> in Proc. ICASSP-95, </booktitle> <pages> pp. 325-328, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Comparatively fewer papers have addressed the equally important issue of robust feature extraction for the purpose of speaker identification. Many current speaker recognition systems rely on spectral-based features, in particular the mel-cepstrum. A notable exception is the work by Janowski, Quatieri and Reynolds <ref> [15] </ref>, where a new set of features based on amplitude and frequency modulation of speech formants and high-resolution measurement of fundamental frequency is used in addition to the standard filterbank-based cepstrum to perform speaker identification over a degraded channel.
Reference: [16] <author> S. Furui, </author> <title> "Cepstral Analysis Technique for Automatic Speaker Verification," </title> <journal> IEEE Trans. ASSP, </journal> <volume> vol. ASSP-29, </volume> <pages> pp. 254-272, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Numerical results are provided to illustrate the performance gain brought by these algorithms. B. The Model Transformation Problem The second part of this work aims at developing transformation algorithms that render the speaker models more robust to acoustic mismatches. Many ASR systems rely on cepstral mean subtraction (CMS) <ref> [16] </ref> to compensate for channel effects [17], [11]. It is well-known, however, that channel mismatches can still be a significant source of errors after CMS. Preliminary experiments reported in Section III-B confirm this point. For this reason, more sophisticated cepstrum transformation methods have been proposed in the literature.
Reference: [17] <author> H. Gish and M. Schmidt, </author> <title> "Text-independent speaker identification," </title> <journal> IEEE Signal Proc. Magazine, </journal> <pages> pp. 18-32, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: B. The Model Transformation Problem The second part of this work aims at developing transformation algorithms that render the speaker models more robust to acoustic mismatches. Many ASR systems rely on cepstral mean subtraction (CMS) [16] to compensate for channel effects <ref> [17] </ref>, [11]. It is well-known, however, that channel mismatches can still be a significant source of errors after CMS. Preliminary experiments reported in Section III-B confirm this point. For this reason, more sophisticated cepstrum transformation methods have been proposed in the literature.
Reference: [18] <author> R. M. Stern, F.-H. Liu, P. J. Moreno and A. Acero, </author> <title> "Signal processing for robust speech recognition", </title> <journal> Proc. ICSLP-94, </journal> <volume> vol. 3, </volume> <pages> pp. 1027-1030, </pages> <address> 18-22 September 1994, Yokohama, Japan. </address>
Reference-contexts: It is well-known, however, that channel mismatches can still be a significant source of errors after CMS. Preliminary experiments reported in Section III-B confirm this point. For this reason, more sophisticated cepstrum transformation methods have been proposed in the literature. In <ref> [18] </ref>, [19], cepstral compensation vectors are derived from a stereo database and applied to the training data to adjust for environmental changes. The compensation vectors depend either on the SNR or on the phonetic identity of the frames.
Reference: [19] <author> F.-H. Liu, R. M. Stern, A. Acero and P. J. Moreno, </author> <title> "Environment normalization for robust speech recognition using direct cepstral comparison," </title> <journal> Proc. ICASSP-94, </journal> <volume> vol. 2, </volume> <pages> pp. </pages> <address> II/61-64, 19-22 April 1994, Adelaide, SA, Australia. </address>
Reference-contexts: It is well-known, however, that channel mismatches can still be a significant source of errors after CMS. Preliminary experiments reported in Section III-B confirm this point. For this reason, more sophisticated cepstrum transformation methods have been proposed in the literature. In [18], <ref> [19] </ref>, cepstral compensation vectors are derived from a stereo database and applied to the training data to adjust for environmental changes. The compensation vectors depend either on the SNR or on the phonetic identity of the frames.
Reference: [20] <author> A. Acero, </author> <title> Acoustical and Environmental Robustness in Automatic Speech Recognition, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: The frequency scale is then warped according to the mel-scale to give a higher resolution at low frequencies and a lower resolution at high frequencies. Specifically, we implemented the bilinear transformation proposed by Acero <ref> [20] </ref>, ! new = ! + 2 atan F w sin! ; where the constant F w 2 [0; 1] controls the amount of warping.
Reference: [21] <author> R. J. Mammone, X. Zhang and R. P. Ramachandran, </author> <title> "Robust speaker recognition", </title> <journal> IEEE Signal Proc. Magazine, </journal> <volume> vol. 13, no. 5, </volume> <pages> pp. 58-71, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: In [18], [19], cepstral compensation vectors are derived from a stereo database and applied to the training data to adjust for environmental changes. The compensation vectors depend either on the SNR or on the phonetic identity of the frames. In <ref> [21] </ref>, an affine transformation of the cepstral vectors is estimated from a stereo portion of the database under study, and then applied to the training data.
Reference: [22] <author> J.L. Gauvain and C.-H. Lee, </author> <title> "Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains", </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> vol. 2, no. 2, </volume> <pages> pp 291-298, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: However, few of the algorithms developed for speech recognition can be readily applied to the problem of speaker recognition. For example, adaptation algorithms that adjust the features or the models to better represent the test data (e.g. <ref> [22] </ref>, [23], [24], [25]) are hard to use in speaker recognition: if the speaker models are adapted with the test data, they all eventually converge toward the same model, and the speaker discrimination capability is lost.
Reference: [23] <author> C. J. Legetter and P. C. Woodland, </author> <title> "Flexible speaker adaptation using maximum likelihood linear regression", </title> <booktitle> Proc. Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 110-115, </pages> <year> 1995. </year>
Reference-contexts: However, few of the algorithms developed for speech recognition can be readily applied to the problem of speaker recognition. For example, adaptation algorithms that adjust the features or the models to better represent the test data (e.g. [22], <ref> [23] </ref>, [24], [25]) are hard to use in speaker recognition: if the speaker models are adapted with the test data, they all eventually converge toward the same model, and the speaker discrimination capability is lost.
Reference: [24] <author> V. Digalakis, D. Rtischev and L. Neumeyer, </author> <title> "Speaker adaptation using constrained reestimation of Gaussian mixtures", </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 357-366, </pages> <year> 1995. </year>
Reference-contexts: However, few of the algorithms developed for speech recognition can be readily applied to the problem of speaker recognition. For example, adaptation algorithms that adjust the features or the models to better represent the test data (e.g. [22], [23], <ref> [24] </ref>, [25]) are hard to use in speaker recognition: if the speaker models are adapted with the test data, they all eventually converge toward the same model, and the speaker discrimination capability is lost.
Reference: [25] <author> A. Sankar and C.-H. Lee, </author> <title> "A maximum-likelihood approach to stochastic matching for robust speech recognition", </title> <journal> IEEE Trans. on Speech and Audio Proc., </journal> <pages> pp. 190-202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, few of the algorithms developed for speech recognition can be readily applied to the problem of speaker recognition. For example, adaptation algorithms that adjust the features or the models to better represent the test data (e.g. [22], [23], [24], <ref> [25] </ref>) are hard to use in speaker recognition: if the speaker models are adapted with the test data, they all eventually converge toward the same model, and the speaker discrimination capability is lost.
Reference: [26] <author> L. Neumeyer and M. Weintraub, </author> <title> "Probabilistic optimum filtering for robust speech recognition," </title> <journal> Proc. ICASSP-94, </journal> <volume> vol. 1, </volume> <pages> pp. 417-420, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Other speech recognition algorithms have addressed the mismatch issue by assuming that a priori knowledge about the mismatch is available: some algorithms require stereo data representing both conditions (e.g. <ref> [26] </ref>, [27]), others need samples of similar sounds across different channels (e.g. [28]). These approaches are hard to implement in speaker recognition because of the practical difficulty of requiring each speaker to record large amounts of speech over multiple channels.
Reference: [27] <author> M. Weintraub and L. Neumeyer, </author> <title> "Constructing telephone acoustic models from a high-quality speech corpus," </title> <journal> Proc. ICASSP-94, </journal> <volume> vol. 1, </volume> <pages> pp. 85-88, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Other speech recognition algorithms have addressed the mismatch issue by assuming that a priori knowledge about the mismatch is available: some algorithms require stereo data representing both conditions (e.g. [26], <ref> [27] </ref>), others need samples of similar sounds across different channels (e.g. [28]). These approaches are hard to implement in speaker recognition because of the practical difficulty of requiring each speaker to record large amounts of speech over multiple channels.
Reference: [28] <author> S. Lerner and B. Mazor, </author> <title> "Telephone channel normalization for automatic speech recognition," </title> <booktitle> Proc. ICASSP-92, </booktitle> <pages> pp. 261-264. </pages>
Reference-contexts: Other speech recognition algorithms have addressed the mismatch issue by assuming that a priori knowledge about the mismatch is available: some algorithms require stereo data representing both conditions (e.g. [26], [27]), others need samples of similar sounds across different channels (e.g. <ref> [28] </ref>). These approaches are hard to implement in speaker recognition because of the practical difficulty of requiring each speaker to record large amounts of speech over multiple channels. In the case of telephone speech, this problem could be alleviated by clustering the channels in two or three categories.
Reference: [29] <author> A. E. Rosenberg, J. DeLong, B.-H. Juang and F. K. Soong. </author> <title> "The use of cohort normalized scores for speaker verification," </title> <booktitle> Proc. ICSLP 92, </booktitle> <pages> pp. 599-602, </pages> <month> Oct. </month> <pages> 12-16, </pages> <address> 1992, Banff, Alberta, Canada. </address>
Reference-contexts: In the case of telephone speech, this problem could be alleviated by clustering the channels in two or three categories. For example, a natural choice would be carbon button versus electret handsets <ref> [29] </ref>. The ASR system would then require a handset detector in order to select one or the other transformation.
Reference: [30] <author> J. J. Godfrey, E. C. Holliman and J. McDaniel, </author> <title> "SWITCHBOARD: Telephone speech corpus for research and development", </title> <booktitle> Proc ICASSP-92, </booktitle> <pages> pp. </pages> <address> I-517-I-520, </address> <year> 1992. </year>
Reference-contexts: These corpora span different mismatch conditions, contain different amounts of training and testing data, were collected by different institutions, and illustrate different kinds of speech, from read digits to unconstrained conversational speech. A. The Switchboard NIST'95 Evaluation Database (NIST95) This database is a subset of the Switchboard corpus <ref> [30] </ref>, collected by the Linguistic Data Consortium. It consists of conversational telephone speech and was used as a benchmark for the NIST'95 (National Institute of Standards and Technology) Evaluations [31]. The database consists of 26 speakers, 13 male and 13 female.
Reference: [31] <institution> NIST Speaker Recognition Workshop, Johns Hopkins University, Baltimore, Maryland, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: A. The Switchboard NIST'95 Evaluation Database (NIST95) This database is a subset of the Switchboard corpus [30], collected by the Linguistic Data Consortium. It consists of conversational telephone speech and was used as a benchmark for the NIST'95 (National Institute of Standards and Technology) Evaluations <ref> [31] </ref>. The database consists of 26 speakers, 13 male and 13 female. We experimented only with the 30-second training, 5-second testing condition. The training data consists of three 10-second segments taken from different conversations. The test data consists of 36 5-second segments per speaker.
Reference: [32] <author> L. Rabiner and B.-H. Juang, </author> <title> "Fundamentals of speech recognition", </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: The generation of a GMM starts with the choice of a random vector quantization (VQ) codebook. The codebook is then iterated on, using the Lloyd algorithm <ref> [32] </ref>. At each VQ iteration, the Gaussians containing the largest number of data points are split, and the Gaussians containing the fewest data points are eliminated. After convergence of the VQ codebook, Gaussians are fitted to the codewords, and their parameters are adjusted using a few expectation-maximization (EM) iterations [32]. <p> algorithm <ref> [32] </ref>. At each VQ iteration, the Gaussians containing the largest number of data points are split, and the Gaussians containing the fewest data points are eliminated. After convergence of the VQ codebook, Gaussians are fitted to the codewords, and their parameters are adjusted using a few expectation-maximization (EM) iterations [32]. When presented with a speech segment from an unknown speaker, the classifier scores all the sufficiently high energy frames of the segment against all speaker models, accumulating the log-likelihoods of the speech frames for each model.
Reference: [33] <author> H. Hermansky and N. Morgan, </author> <title> "RASTA processing of speech," </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 578-589, </pages> <year> 1994. </year>
Reference-contexts: The results reported in the table show that although CMS reduces the error rate significantly in the mismatched system (from 33% to 16%), its error rate with CMS remains more than three times higher than that of the corresponding matched Sennheiser system (16% vs. 5%). Other researchers (e.g. <ref> [33] </ref>, [34]) have reported similar results: CMS eliminates convolutional effects but it does not eliminate additive noise and does not take into account channel nonlinearities and nonstationarities. In addition, CMS can eliminate some of the speaker characteristics as exemplified by the matched Sennheiser experiments (lines 3 and 4). <p> This result, which may be attributed to the cancellation of vocal-tract information by the high-pass filtering in CMS, is to be expected also from techniques such as RASTA preprocessing <ref> [33] </ref>. Finally, comparing lines 3 and 5 in Table I confirms that, more than the presence of a telephone unit between the speaker and the ASR system, it is the possible mismatch between training and testing conditions that results in poor speaker-ID performance.
Reference: [34] <author> M. Schmidt, H. Gish and A. Mielke, </author> <title> "Covariance estimation methods for channel robust text-independent speaker identification," </title> <booktitle> Proc. </booktitle> <address> ICASSP-95, pp.333-336, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The results reported in the table show that although CMS reduces the error rate significantly in the mismatched system (from 33% to 16%), its error rate with CMS remains more than three times higher than that of the corresponding matched Sennheiser system (16% vs. 5%). Other researchers (e.g. [33], <ref> [34] </ref>) have reported similar results: CMS eliminates convolutional effects but it does not eliminate additive noise and does not take into account channel nonlinearities and nonstationarities. In addition, CMS can eliminate some of the speaker characteristics as exemplified by the matched Sennheiser experiments (lines 3 and 4).
Reference: [35] <author> D. H. </author> <title> Klatt,"A digital filterbank for spectral matching," </title> <booktitle> Proc. ICASSP-76, </booktitle> <pages> pp. 573-576, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: Alternatively, one can make the bandwidth of each filter a function of frequency (as opposed to a function of the distance between center frequencies of adjacent filters as in the baseline design). The question is how this function should be chosen. In <ref> [35] </ref>, Klatt proposed a bank of 30 filters based on auditory perception criteria. Because Klatt's parameters are derived for a fixed filterbank size, his design could not be ported directly to our system. Instead, we approximated Klatt's coefficients with the function given in Table II.
Reference: [36] <author> B. A. Dautrich, L. R. Rabiner and T. B. Martin, </author> <title> "On the effects of varying filter bank parameters on isolated word recognition," </title> <journal> IEEE Trans. Acoust. Speech Sig. Proc., </journal> <volume> vol. ASSP-31, no. 4, </volume> <pages> pp. 793-897, </pages> <year> 1983. </year>
Reference-contexts: For example, varying the filter shape while keeping the other parameters constant would make the error rate vary between 26% and 29%. Other researchers observed similar results (e.g. <ref> [36] </ref> describes an experimental study on the influence of filterbank design and parameters on isolated word recognition). Since it is not clear how each individual front-end parameter affects the classification error rate, we performed an extensive optimization of all the front-end parameters, using the NIST95 database.
Reference: [37] <author> D. H. Klatt, </author> <title> "Prediction of perceived phonetic distance from critical-band spectra: A first step," </title> <booktitle> Proc. ICASSP-82, </booktitle> <pages> pp. 1278-1281, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: It is likely to be dominated by the first formant since the first formant has the highest energy, due to the effect of the glottal roll-off. It is well known that formants and their transitions are very important for the perception of speech. In psychophysical studies performed by Klatt <ref> [37] </ref>, it was observed that when formant locations are changed, the sounds perceived by listeners are different from what was intended. The same study shows that humans perceive the same sound when the relative amplitudes of the formants are modified in different instantiations of the sound. <p> Even if the peak locations are the same for different speakers, the slope information can give information about the tilt in the spectrum, the spectral tilt being related to the shape of the glottal pulse. A spectral slope metric was suggested by Klatt <ref> [37] </ref> and used by Hanson and Wakita [41] for isolated word recognition. In the latter study, the slope is computed indirectly, using the relationship between the derivative of the spectrum and the weighted cepstrum. <p> This principle can be applied to the filterbank-based cepstra only when the number of filters is infinite (and nonoverlapped) and the number of cepstral coefficients is infinite. Neither of these conditions is true in practice. We therefore propose a technique based on the metric suggested by Klatt <ref> [37] </ref> but where the slopes are computed differently. As with the cepstrum feature, the speech signal is transformed to the frequency domain via an FFT, and the frequency scale is warped. The spectrum is then multiplied by a bank of filters similar to that used for the baseline cepstrum.
Reference: [38] <author> B. Yegnanarayana, </author> <title> "Formant extraction from linear prediction phase spectra," </title> <journal> J. Acoust. Soc. Amer., </journal> <volume> vol. 63, no. 5, </volume> <pages> pp. 1638-1640, </pages> <year> 1978. </year>
Reference-contexts: The same study shows that humans perceive the same sound when the relative amplitudes of the formants are modified in different instantiations of the sound. Although various algorithms have been developed to estimate formant frequency locations in running speech (e.g. <ref> [38] </ref>, [39], [40]), the formant-extraction problem is nontrivial. Machines tend to make gross errors in estimating formant locations: spurious peaks are introduced and true peaks are often missed. We therefore looked for a new measure that would emphasize the locations of formants without actually estimating them.
Reference: [39] <author> H. A. Murthy and B. Yegnanarayana, </author> <title> "Formant extraction from group delay function," </title> <journal> Speech Communication, </journal> <volume> vol. 10, </volume> <pages> pp. 209-221, </pages> <year> 1991. </year>
Reference-contexts: The same study shows that humans perceive the same sound when the relative amplitudes of the formants are modified in different instantiations of the sound. Although various algorithms have been developed to estimate formant frequency locations in running speech (e.g. [38], <ref> [39] </ref>, [40]), the formant-extraction problem is nontrivial. Machines tend to make gross errors in estimating formant locations: spurious peaks are introduced and true peaks are often missed. We therefore looked for a new measure that would emphasize the locations of formants without actually estimating them.
Reference: [40] <author> H. A. Murthy and B. Yegnanarayana, </author> <title> "Speech processing using group delay functions," </title> <booktitle> Signal Proc., </booktitle> <volume> vol. 22, </volume> <pages> pp. 259-267, </pages> <year> 1991. </year>
Reference-contexts: The same study shows that humans perceive the same sound when the relative amplitudes of the formants are modified in different instantiations of the sound. Although various algorithms have been developed to estimate formant frequency locations in running speech (e.g. [38], [39], <ref> [40] </ref>), the formant-extraction problem is nontrivial. Machines tend to make gross errors in estimating formant locations: spurious peaks are introduced and true peaks are often missed. We therefore looked for a new measure that would emphasize the locations of formants without actually estimating them.
Reference: [41] <author> B. A. Hanson and H. Wakita, </author> <title> "Spectral slope distance measures with linear prediction analysis for word recognition in noise," </title> <journal> IEEE Trans. Acoust. Speech, Signal Proc., </journal> <volume> vol. ASSP-35, no. 7, </volume> <pages> pp. 968-973, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: A spectral slope metric was suggested by Klatt [37] and used by Hanson and Wakita <ref> [41] </ref> for isolated word recognition. In the latter study, the slope is computed indirectly, using the relationship between the derivative of the spectrum and the weighted cepstrum.

References-found: 41

