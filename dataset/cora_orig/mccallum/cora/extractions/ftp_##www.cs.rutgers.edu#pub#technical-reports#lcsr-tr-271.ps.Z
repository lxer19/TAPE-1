URL: ftp://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-271.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: SOFTWARE SUPPORT FOR PARALLEL PROCESSING OF IRREGULAR AND DYNAMIC COMPUTATIONS  Written under the direction of  
Author: BY JIA JIAO 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Professor Apostolos Gerasoulis and approved by  
Date: October, 1996  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> V. Adve, A. Carle, E. Granston, S. Hiranandani, K. Kennedy, C Koelbel, U. Kremer, J. Mellor-Crummey, C-W. Tseng and S. Warren. </author> <title> Requirements for data-Parallel programming environments. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 48-58, </pages> <year> 1994. </year>
Reference-contexts: This data mapping determines the work mapping the data access pattern. Research efforts concentrate on finding good data partitioning in order to reduce the parallel time. Many compilers are built to handle regular problems, such as PARADIGM [4], Fortran D <ref> [1] </ref>, and SUIF [2]. For irregular code, the CHAOS [34] system uses inspector-executor approach to produce code for input-dependent problems. In the functional parallelism model, the basic units of the program are functional components. <p> This is called a Block mapping of array a. After the data mapping, each processor works on its own data set. For a general overview of data parallel programming environment, see <ref> [1] </ref>. Another basic paradigm in parallel computing is functional parallelism, where functional components, such as loop iterations or task, are the units of partitioning. A functional parallel implementation of the simple loop above may be assigning to each processor 10 iterations of the loop. <p> The Inspector/Executor approach is used to capture the input-dependent nature of irregular problems, and is similar in spirit to our data structures used for the adaptive FMM algorithm. The Fortran D compiler <ref> [1] </ref> has also been extended in an attempt to deal with irregular problems, in the thesis of Reinhard von Hanxleden [30]. The Inspector/Executor model is used for generating input-dependent data mapping. There are many other heuristics for task graph scheduling in the literature. <p> On the topic of automatic parallelizing compiler, many systems have been developed, including, but not limited to, the Fortran D compiler <ref> [1] </ref> at Rice, which is based on data parallelism model and carries out optimizations on data partitioning and layout. The SUIF compiler [2] at Stanford, which involves fine grain dependence analysis for regular loops and certain simple scheduling techniques for processor mapping.
Reference: [2] <author> S.P. Amarasinghe, J. M. Anderson, M.S. Lam and C. W. Tseng, </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> Proceedings of the seventh SIAM conference on parallel processing for scientific computing, </booktitle> <year> 1995. </year>
Reference-contexts: This data mapping determines the work mapping the data access pattern. Research efforts concentrate on finding good data partitioning in order to reduce the parallel time. Many compilers are built to handle regular problems, such as PARADIGM [4], Fortran D [1], and SUIF <ref> [2] </ref>. For irregular code, the CHAOS [34] system uses inspector-executor approach to produce code for input-dependent problems. In the functional parallelism model, the basic units of the program are functional components. <p> Current research effort, therefore, is starting from regular task graphs, hoping to extend the results to general task graphs in the long term future. Such regular codes usually have affine loop bounds. See e.g. <ref> [2, 43] </ref>. Even when we narrow down to only regular task graphs, the effort needed is still great. As far as we know no existing system can accomplish completely automatic parallelization even for basic regular task graphs. <p> We also know that this set of value is within the bounds, which are 2 v 0 4 and v 0 + 1 y 0 n + 1. Finally, we discover that the data content for this instance of communication is a <ref> [x 0 ; 2] </ref>, with bounds x 0 2 1 0 and x 0 + n 0, which is equivalent to 3 x 0 5. <p> Finally, we discover that the data content for this instance of communication is a [x 0 ; 2], with bounds x 0 2 1 0 and x 0 + n 0, which is equivalent to 3 x 0 5. So we know the matrix entries sent are a <ref> [3; 2] </ref>, a [4; 2] and a [5; 2], i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of <p> So we know the matrix entries sent are a [3; 2], a <ref> [4; 2] </ref> and a [5; 2], i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of communication. <p> So we know the matrix entries sent are a [3; 2], a [4; 2] and a <ref> [5; 2] </ref>, i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of communication. <p> We first define data fragments. A data fragment is a piece of data extracted by the matching of a specific task graph edge/communication rule pair. For example, the partial column of length 3, a <ref> [3; 2] </ref> to a [5; 2] generated by the matching in the Gauss-Jordan example is considered one data fragment. <p> We first define data fragments. A data fragment is a piece of data extracted by the matching of a specific task graph edge/communication rule pair. For example, the partial column of length 3, a [3; 2] to a <ref> [5; 2] </ref> generated by the matching in the Gauss-Jordan example is considered one data fragment. <p> Tiling is a useful loop transformation technique to expose more parallelism. It can also be used to increase the granularity of the tasks. Tiling is extensively employed in the SUIF compiler. See <ref> [2, 54] </ref> for detailed discussions about titling transformation. Here we just give a simple example to illustrate how the granularity may be increased. <p> On the topic of automatic parallelizing compiler, many systems have been developed, including, but not limited to, the Fortran D compiler [1] at Rice, which is based on data parallelism model and carries out optimizations on data partitioning and layout. The SUIF compiler <ref> [2] </ref> at Stanford, which involves fine grain dependence analysis for regular loops and certain simple scheduling techniques for processor mapping. For coarse-grain level, they have suggested inter-procedural analysis [31], which is still an open research area.
Reference: [3] <author> V. Balasundaram, </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 9, No. 2, </volume> <pages> pp 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Traditional dependence analysis methods focus on the statement level, whereas in the task graph model, edge dependences are coarser grain. Although there are works using other notions of data dependence analysis, such as Data Access Descriptors <ref> [3] </ref> and Regular Section Descriptor [32], few systems are available for task level dependence generation. <p> Finally, we discover that the data content for this instance of communication is a [x 0 ; 2], with bounds x 0 2 1 0 and x 0 + n 0, which is equivalent to 3 x 0 5. So we know the matrix entries sent are a <ref> [3; 2] </ref>, a [4; 2] and a [5; 2], i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of <p> We first define data fragments. A data fragment is a piece of data extracted by the matching of a specific task graph edge/communication rule pair. For example, the partial column of length 3, a <ref> [3; 2] </ref> to a [5; 2] generated by the matching in the Gauss-Jordan example is considered one data fragment.
Reference: [4] <author> P. Banerjee, J. A. Candy, M. Gupta, E. W. Hodges IV, J. G. Holm, A. Lain, D. J. Palermo, S. Ramaswamy and E. Su, </author> <title> The PARADIGM compiler for distributed-Memory multicomputers. </title> <journal> IEEE Computer, </journal> <volume> Vol 28, No. 10, </volume> <pages> pp 37-47, </pages> <year> 1995. </year>
Reference-contexts: This data mapping determines the work mapping the data access pattern. Research efforts concentrate on finding good data partitioning in order to reduce the parallel time. Many compilers are built to handle regular problems, such as PARADIGM <ref> [4] </ref>, Fortran D [1], and SUIF [2]. For irregular code, the CHAOS [34] system uses inspector-executor approach to produce code for input-dependent problems. In the functional parallelism model, the basic units of the program are functional components. <p> So we know the matrix entries sent are a [3; 2], a <ref> [4; 2] </ref> and a [5; 2], i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of communication. <p> The SUIF compiler [2] at Stanford, which involves fine grain dependence analysis for regular loops and certain simple scheduling techniques for processor mapping. For coarse-grain level, they have suggested inter-procedural analysis [31], which is still an open research area. The Paradigm system <ref> [4] </ref> developed at Illinois is also able to perform dependence analysis and data partitioning. Recently the model of hierarchical task graph has been proposed in [41], which allows data parallelism within tasks, in an attempt to combine data and task parallelism.
Reference: [5] <author> J. Barnes and P. Hut, </author> <title> A hierarchical O(N log N ) force calculation algorithm, </title> <booktitle> Nature Vol. </booktitle> <address> 324 P. </address> <month> 446 , </month> <year> 1986. </year>
Reference-contexts: The direct summation for the above expression costs O (N 2 ). Two fast sequential algorithms are available: the Barnes-Hut Algorithm <ref> [5] </ref>, and The Fast Multipole Method (FMM)[28]. 31 The method we consider here is the FMM algorithm, because of the following rea sons: * Theoretically, the FMM algorithm has lower complexity than the Barnes-Hut method. * More importantly, the task graph dependence structure of the Barnes- Hut algorithm is relatively simple, <p> So we know the matrix entries sent are a [3; 2], a [4; 2] and a <ref> [5; 2] </ref>, i.e, the part of column 2 in the matrix a from row 3 to row 5. 8.4.2 Generation of message id and message content The above discussion demonstrated the matching of an edge in the task graph and a single rule of communication. <p> We first define data fragments. A data fragment is a piece of data extracted by the matching of a specific task graph edge/communication rule pair. For example, the partial column of length 3, a [3; 2] to a <ref> [5; 2] </ref> generated by the matching in the Gauss-Jordan example is considered one data fragment. <p> The N-body simulation has also been of long-standing research interest. Most of the parallel implementations has been for the Barnes-Hut algorithm <ref> [5] </ref> due to its relative simplicity and reasonable complexity, but efforts are also been made on the Fast Multipole Algorithms. Singh [48] has proposed a very problem-specific technique called "cost-zone", which can be applied to both Barnes-Hut and FMM algorithms.
Reference: [6] <author> S. Bhatt, P. Liu, V. Fernandez, and N. Zabusky, </author> <title> Tree codes for vortex dynamics: Application of a programming framework. </title> <booktitle> Proc of the 1st Workshop on Solving Irregular Problems on Parallel Machines, International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA 1995. </address>
Reference-contexts: Such a smoothing approach is not only employed in [37] but many other CFD simula tions, such as in <ref> [6] </ref>. 91 Next, we need to discretize the equation to enable numerical solution. The dis-cretization is quite straightforward. <p> This practice has been adopted by other researchers as well for vortex simulations. See <ref> [6] </ref>. There are other issues as well, most notably, the need of dynamic point insertion. The number of points required to discretize the vortex sheet is related to the smoothing factor ffi.
Reference: [7] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K.H. Randall and Y. Zhou, Cilk: </author> <title> An efficient multithreaded runtime System. </title> <booktitle> Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pp. 207-216, </pages> <address> Santa Barbara, CA, </address> <year> 1995. </year>
Reference-contexts: In fact, we can call our schedule readjustment algorithm for the changes in task weights "Generalized Load Balancing", since it attempts load balancing while taken into account the communication edges. A related work at MIT by Leiserson's group on the run-time system Cilk <ref> [7] </ref> also studies "dynamic spawning", but their model is the multithreaded computation, and the spawning is referring to the dynamic creation of threads, which is a realistic description of shared memory multiprocessing adopted by many SMP workstation platforms, but different from our basic model.
Reference: [8] <author> S. Bokhari, </author> <title> On the mapping problem, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-30 3(1981), </volume> <pages> 207-214. </pages>
Reference-contexts: Determining an optimal physical mapping is difficult and we modify the Bokhari's heuristic algorithm <ref> [8] </ref> to be used for PYRROS. The Bokhari's algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that the F (CC; P ) decreases monotonically.
Reference: [9] <author> D. A. </author> <title> Case, Computer simulation of protein dynamics and thermodynamics, </title> <booktitle> Computer, </booktitle> <pages> pp. 47-57, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In a variation of the classic N-body problem, the protein dynamics simulation, time step size has to be as little as 10 15 seconds, and it takes 10 6 to 10 7 steps to complete a folding process in the order of nanoseconds. See <ref> [9] </ref>. Furthermore, the PYRROS scheduler does not need precise weight information, we expect that small weight perturbations caused by particle movement should not lead to significant deterioration in scheduling performance, which is intuitive and analytically shown in [20].
Reference: [10] <author> S. Chakravarti, M. Gupta, J-D Choi, </author> <title> Global communication analysis and optimization, </title> <booktitle> 1996 ACM conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 68-78, </pages> <address> Philadelphia, PA. </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Other works on generalized task graph models include scheduling for iterative task graphs, as presented in [64], where cross-iteration dependences exist and the scheduling is done symbolically without unwinding of the iterations. Most recently in <ref> [10] </ref>, Chakrabarti et. al. consider global communication analysis in the High Performance Fortran compiler, which can reduce message passing overhead. Their work may also be useful for coarse grain dependence analysis required by our functional parallelism model. 156 9.2 Future Directions The following are some conceivable directions for future work.
Reference: [11] <author> F. T. Chong, Shamik D. Sharma, Eric A. Brewer and Joel Saltz, </author> <title> Multiprocessor runtime support for fine-grained irregular DAGs, </title> <type> Draft, </type> <year> 1994. </year>
Reference-contexts: Many problems, including all three iterative application studied in this thesis, are both irregular and dynamic, although there are problems that are irregular but static, such as the iterative sparse triangular solver studied in a paper by Chong et al. See <ref> [11] </ref>. Scheduling for a static task graph is totally dependent on the initial graph, since the schedule for the first iteration may be reused for all time, although sometimes pipelining technique can be employed to overlap consecutive iterations in order to improve overall performance. <p> Many methods has been proposed. See for example, work done by Venugopal [51], Rothberg [44]. One of the noteworthy works, which is very closely related to our approach, is done by Chong et al <ref> [11] </ref>, where the DSC clustering algorithm from PYRROS was applied to the sparse triangular solution iterative problem. In that paper, it was shown that the DSC algorithm can improve the parallel time per iteration by almost 75% over cyclic mapping.
Reference: [12] <author> F. T. Chong and R. Schreiber, </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs, </title> <type> Tech Report, </type> <year> 1994. </year> <booktitle> Appeared in Proc. of 1st IPPS Workshop on Solving Irregular Problems, </booktitle> <address> Santa Barbara, </address> <year> 1995. </year> <month> 160 </month>
Reference-contexts: In experimental comparisons we find that ETF could yield better schedule than PYRROS but for moderate to large number of processors the cost of ETF is prohibitive. See [21]. We should mention that similar performance-overhead trade-off was observed by 45 Chong et al. ([11], <ref> [12] </ref>) using DSC clustering method on irregular sparse matrix task graphs. 3.4.2 Performance of scheduling in the iterative execution of FMM We use PYRROS to schedule one iteration and then execute the same schedule for many iterations.
Reference: [13] <author> Ph. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference: [14] <author> E.G. Coffman and R.L. Graham, </author> <title> Optimal scheduling for two-processors systems, </title> <journal> Acta Informatica, </journal> <volume> 3 (1972), </volume> <pages> 200-213. </pages>
Reference: [15] <author> M. Cosnard and M. Loi, </author> <title> Automatic task graph generation techniques. </title> <booktitle> Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <volume> Vol II, </volume> <pages> pp 113-122, </pages> <year> 1995. </year>
Reference-contexts: Although there are works using other notions of data dependence analysis, such as Data Access Descriptors [3] and Regular Section Descriptor [32], few systems are available for task level dependence generation. PlusPyr, <ref> [15] </ref>, an automatic task graph generation tool developed in France, with the aim of providing a front-end user interface for PYRROS, is the first such attempt, and for dense linear algebra problems it works well. <p> The tool has undergone extensive evolution since its inception. At present time, it is capable of 18 taking user defined task graphs and manually specified task definition code, i.e, the code fragments containing the portion of sequential operations within tasks, and producing executable parallel code. The PlusPyr tool <ref> [15] </ref> was aimed at providing an automatic task graph generation front-end, and our recent work involved PlusPyr/PYRROS integration, but we will postpone the discussion to chapter 8 of the thesis. For now we focus on PYRROS itself. <p> (k,j) / a (k,k) a (i1,j) = a (i1,j) - a (k,j) * a (i1,k) endfor for i2 = k + 1 to n do endfor ENDTASK endfor endfor In this example, all loop bounds are affine, and there is a single generic task, or "parametric task" as called in <ref> [15] </ref>, defined to enclose the two innermost loops with indices i1 and i2. This generic task, delimited by the TASK and ENDTASK annotations, is in turn enclosed by the k and j loops at higher level of the loop nest. <p> Once the task definitions are provided, the system uses the following procedure, as illustrated in figure 8.2, for task graph generation: This is only an outline, details can be found in the original paper <ref> [15] </ref> written by the French group. 1. Statement level dependence analysis. 136 This is done at the individual statement level, without regard to the task definitions. It's a classic problem which has been extensively studied in the literature, and many algorithms have been proposed, such as [57, 54, 42]. <p> Communication weights are calculated similarly, by the volume of the polytope in the communication rule. The details are very mathematically involved, and will not be described here. Interested reader may refer to <ref> [15] </ref>. 137 Such information regarding task, edge weights and communication rules can be described symbolically, without the actual parameters of the task graph, such as the dimension of matrix for linear algebra task graphs in Gauss-Jordan elimination.
Reference: [16] <author> J. J. Dongarra, J. D. Croz, I. Duff and S. Hammarling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 16(1), </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference: [17] <author> C. I. Dragchicescu, </author> <title> An efficient implementation of particles methods for the incompressible Euler equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> Vol. 31, No. 4, </volume> <pages> pp. 1090-1108, </pages> <month> August </month> <year> 1994. </year>
Reference: [18] <author> D. G. Dritschel, </author> <title> A fast contour dynamics method for many-vortex calculations in two-dimensional flows. Physics of Fluids A 5(1), </title> <year> 1993. </year>
Reference-contexts: The fluid is viewed as interactions between distinct closed contours where there are vorticity jumps across the boundaries. Contours can be imagined as floating in the homogeneous background and exerting influence upon each one, which is an analogous situation to that of the classical N-body problem. Dritschel <ref> [18] </ref> has developed a fast algorithm using expansions (He named them "moments" instead). The algorithm was inspired partly by the FMM algorithm of Greengard described in [28]. On the other hand there are significant differences because the Dritschel algorithm does not use space partitions. <p> Let R = maxjz Zj; z 2 C and R 0 = maxjz 0 Z 0 j; z 0 2 C 0 , and * be the max error tolerance. In <ref> [18] </ref>, a detailed analysis is given, and the well-separate condition is chosen as: 2j~!j ( jZ 0 Zj R 0 R where ! max is the peak vorticity magnitude in the fluid.
Reference: [19] <author> A. George, , M.T. Heath, and J. Liu, </author> <title> Parallel Cholesky factorization on a shared memory processor, </title> <journal> Lin. Algebra Appl., </journal> <volume> 77(1986), </volume> <pages> pp. 165-187. </pages>
Reference-contexts: The same problem is actually inherent in the PYRROS cluster merging step, since it's difficult to know whether such merge based on cluster load is good. On the other hand, such load-based merge has low complexity and performs well on average cases, so it's widely adopted. See <ref> [19] </ref>. It will be very exciting if any of these problem can be overcome by some improved algorithm, even if it's still of heuristic nature.
Reference: [20] <author> A. Gerasoulis, J. Jiao and T. Yang, </author> <title> Scheduling of structured and unstructured computation. </title> <booktitle> DIMACS book series Vol. </booktitle> <volume> 21, </volume> <pages> pp. 139-172. </pages>
Reference-contexts: See [9]. Furthermore, the PYRROS scheduler does not need precise weight information, we expect that small weight perturbations caused by particle movement should not lead to significant deterioration in scheduling performance, which is intuitive and analytically shown in <ref> [20] </ref>. Thus a schedule can be re-used for many steps, until its performance deteriorates to the point where rescheduling becomes necessary. The interesting question is how frequently we need to reschedule, since this will determine the overall performance of PYRROS for the N-body problem. <p> Then we make random perturbations to the task and communication weights, so the schedule is produced using inaccurate weights. Afterward we rerun the computation with the new schedule and measure the new parallel time. According to our run time analysis in a related paper <ref> [20] </ref>, the performance should not be affected by much, provided the weight perturbation is not excessive. We bound the weight perturbation to a random amount between -20% and +20%. Then we observe the increase in parallel time shown in Table 3.4.
Reference: [21] <author> A. Gerasoulis, J. Jiao and T. Yang, </author> <title> A multistage approach for scheduling task graphs on parallel machines. </title> <booktitle> DIMACS book series Vol. </booktitle> <volume> 22, </volume> <month> pp.81-103. </month>
Reference-contexts: They are called one-stage method. It's not clear which class of methods is better. For example, ETF often gives shorter schedule on coarse-grain graphs but the running time is much higher than PYRROS, making the overhead excessively high for real applications. See <ref> [21] </ref>. 3. Physical Mapping: Map the p virtual processors to p physical processors. The reason is that virtual processors are assumed to be completely connected while the real architecture may be not. For example, the Ncube machine where our experiments are conducted has a hypercube architecture. 4. <p> Other scheduling algorithms such as the ETF algorithm [33] has higher complexity (O (v 2 )). In experimental comparisons we find that ETF could yield better schedule than PYRROS but for moderate to large number of processors the cost of ETF is prohibitive. See <ref> [21] </ref>.
Reference: [22] <author> A. Gerasoulis and I. Nelken, </author> <title> Static scheduling for linear algebra DAGs. </title> <booktitle> Proc. of 4th Conf. on Hypercubes, Monterey, </booktitle> <volume> Vol. 1, </volume> <year> 1989, </year> <pages> 671-674. </pages>
Reference-contexts: Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging. This method is simple and has been shown to work well in practice, e.g. makespan [45], Ortega [40], Gerasoulis and Nelken <ref> [22] </ref>. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). It is as follows: 1. Compute the arithmetic load LM j for each cluster. 2. Sort the clusters in an increasing order of their loads. 3.
Reference: [23] <author> A. Gerasoulis, J. Jiao and T. Yang, </author> <title> Scheduling of structured and unstructured computation. Book Series on Interconnection Networks and Mapping and Scheduling Parallel Computations, </title> <journal> American Mathematics Society,1995. </journal>
Reference: [24] <author> A. Gerasoulis, J. Jiao and T. Yang, </author> <title> A multistage approach to scheduling task graphs. Book Series on Parallel Processing of Discrete Optimization Problems, </title> <journal> American Mathematics Society,1995. </journal>
Reference-contexts: Some other methods are one-stage, in the sense that the schedule is produced directly for the given number of processors. The ETF algorithm [33] is one of the most efficient among one-stage methods. One of our papers <ref> [24] </ref> shows that ETF can be very efficient with small or moderate number of processors but its complexity can be prohibitive for large number of processors.
Reference: [25] <author> A. Gerasoulis, J. Jiao, W. Lin, </author> <title> Parallelization of a ship motion simulation system, Under preparation. </title> <booktitle> Results presented at 1995 International Mechanical Engineering Congress, </booktitle> <address> San Francisco, CA. </address>
Reference-contexts: It should be noted that, although the existing parallelization systems are far from adequate to handle real world code, they can still be used to model the program execution and guide our parallelization effort. One example is a large scale industrial code called Lamp, described in <ref> [25] </ref>, which is designed to simulate the motion of ships in sea waves. It's impossible for the PlusPyr/PYRROS integrated system to take the real code as input.
Reference: [26] <author> A. Gerasoulis and T. Yang, </author> <title> A comparison of clustering heuristics for scheduling DAGs on multiprocessors, </title> <journal> J. of Distributed and Parallel Computing, special issue on scheduling and load balancing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> pp. </pages> <month> 276-291 (Dec. </month> <year> 1992). </year>
Reference: [27] <author> A. Gerasoulis and T. Yang, </author> <title> Performance bounds for parallelizing Gaussian-Elimination and Gauss-Jordan on message-passing machines, </title> <journal> Applied Numerical Mathematics Journal, </journal> <volume> 16(1994) 283-297. </volume> <pages> 161 </pages>
Reference: [28] <author> Leslie Greengard, </author> <title> The rapid evaluation of potential fields in particle systems, </title> <type> Ph.D thesis, </type> <institution> Yale University, </institution> <year> 1987. </year>
Reference-contexts: The number of boxes in the list of any single box at any level is always less or equal to 27 in 2D space. At the root of the tree the entire 2D space particle evaluation has been covered. For more details see Greengard's thesis <ref> [28] </ref>. 33 of box O. Any box has most 27 boxes in its interaction list and at most 8 neighbors For such kind of applications, hand-optimization can be extremely difficult, and automatic scheduling method can be a good approach. <p> Because of the regular subdivisions, each is a complete quad-tree. The leaves of the tree correspond to the boxes at the finest level, and the internal nodes correspond to the boxes at higher levels. Following the thesis of Greengard <ref> [28] </ref>, we call the in-tree "upward pass" and the out-tree "downward pass". There are additional edges between the two parts of the graph, which represent interactions between boxes. Figure 3.5 is an example of the task graph for a level 2 division. <p> A non-leaf node in the downward pass Computation weight is O (r 2 ). (a) Receives the multipole expansions from the boxes in its interaction list in the upward pass, then converts these multipole expansions into local expansions. See Greengard thesis <ref> [28] </ref>. (b) Receives the local expansion from its parent, shifts it to its own center, then adds it to the local expansion computed in (a) above to obtain the total local expansion at this box. (c) Sends the local expansion to each of its children. 4. <p> So when we schedule the adaptive FMM algorithm onto parallel machines, we must use certain data structure generated from the input distribution to provide such information to the tasks. The data structures includes different lists of interactions. For the name of list we use the notion introduced by Greengard <ref> [28] </ref>. <p> For a non-leaf box, we still have V and X lists as above, but there are no U and W lists for such boxes. To illustrate these lists, we adopt a complex irregular partitioning from <ref> [28] </ref>, and show the interaction lists of a particular box B in figure 3.7. In the figure boxes marked U are those in U list (B), and so forth. With these lists in mind, the functionalities of node in the task graph can be briefly summarized as follows. <p> A non-leaf node in the downward pass: (a) Receives the multipole expansions from the boxes in its V list in the upward pass. Then converts these multipole expansions into local expansions, see Greengard thesis <ref> [28] </ref>. (b) Also receives the particle positions from X list boxes in the upward pass, but instead of direct summation, converts the field each particle in the X list box into a local expansion about the center of the receiving box, and adds the local expansion to the total expansion. (c) <p> Dritschel [18] has developed a fast algorithm using expansions (He named them "moments" instead). The algorithm was inspired partly by the FMM algorithm of Greengard described in <ref> [28] </ref>. On the other hand there are significant differences because the Dritschel algorithm does not use space partitions. Instead the expansions are centered at each of contours.
Reference: [29] <author> M. Girkar and C. Polychronopoulos, </author> <title> Automatic extraction of functional parallelism from ordinary programs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems,. </journal> <volume> Vol. 3, No. 2, </volume> <pages> pp. 166-178, </pages> <year> 1992. </year>
Reference-contexts: It will not be discussed in this thesis, rather it's left as a future research direction. We therefore assume manual partitioning and focuses on steps 2 and 3. For step 2, there have been some studies of automatic extraction of functional parallelism, such as <ref> [29] </ref>. The PlusPyr ([15]) system, developed in France, is a significant attempt toward the goal of automatic parallelization. It was indeed inspired by PYRROS, with the purpose of providing an automatic task graph generation front-end to PYRROS.
Reference: [30] <author> Reinhard von Hanxleden, </author> <title> Compiler support for machine-independent paralleliza-tion of irregular problems, </title> <type> Ph.D. Thesis, </type> <institution> Rice University, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: The Fortran D compiler [1] has also been extended in an attempt to deal with irregular problems, in the thesis of Reinhard von Hanxleden <ref> [30] </ref>. The Inspector/Executor model is used for generating input-dependent data mapping. There are many other heuristics for task graph scheduling in the literature. PYRROS belongs to the class of multistage methods, which also includes the Sarkar's algorithm [47], among many others.
Reference: [31] <author> M. W. Hall, B. R. Murphy, S. P. Amarasinghe, S. Liao, and M. S. Lam, </author> <title> Interpro-cedural analysis for parallelization, </title> <booktitle> Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing (LCPC95), </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The SUIF compiler [2] at Stanford, which involves fine grain dependence analysis for regular loops and certain simple scheduling techniques for processor mapping. For coarse-grain level, they have suggested inter-procedural analysis <ref> [31] </ref>, which is still an open research area. The Paradigm system [4] developed at Illinois is also able to perform dependence analysis and data partitioning.
Reference: [32] <author> P. Havlak and K. Kennedy, </author> <title> An implementation of interprocedural bounded regular section analysis, </title> <journal> IEEE Transaction on Parallel and Distributed Systems, </journal> <volume> Vol 2, No.3, </volume> <pages> pp. 350-360, </pages> <year> 1991. </year>
Reference-contexts: Traditional dependence analysis methods focus on the statement level, whereas in the task graph model, edge dependences are coarser grain. Although there are works using other notions of data dependence analysis, such as Data Access Descriptors [3] and Regular Section Descriptor <ref> [32] </ref>, few systems are available for task level dependence generation. PlusPyr, [15], an automatic task graph generation tool developed in France, with the aim of providing a front-end user interface for PYRROS, is the first such attempt, and for dense linear algebra problems it works well.
Reference: [33] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interprocessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: A good scheduling algorithm should try to minimize the parallel time. In general such minimization problems are intractable, and many heuristics are proposed for good, albeit non-optimal results. See, for example, work done by Sarkar, Hwang et al. <ref> [47, 33] </ref>, and our own heuristic. 2.2 The Multistage Approach and a Sketch of Each Stage PYRROS is an automatic task graph scheduling tool developed at Rutgers. The tool has undergone extensive evolution since its inception. <p> One advantage of PYRROS is the ability to produce good, albeit non-optimal, schedule at reasonable cost. Other scheduling algorithms such as the ETF algorithm <ref> [33] </ref> has higher complexity (O (v 2 )). In experimental comparisons we find that ETF could yield better schedule than PYRROS but for moderate to large number of processors the cost of ETF is prohibitive. See [21]. <p> PYRROS belongs to the class of multistage methods, which also includes the Sarkar's algorithm [47], among many others. Some other methods are one-stage, in the sense that the schedule is produced directly for the given number of processors. The ETF algorithm <ref> [33] </ref> is one of the most efficient among one-stage methods. One of our papers [24] shows that ETF can be very efficient with small or moderate number of processors but its complexity can be prohibitive for large number of processors.
Reference: [34] <author> Y.S. Hwang, B. Moon, S. Sharma, R. Das, J. Saltz, </author> <title> Runtime support to parallelize adaptive irregular programs, </title> <booktitle> Proceedings of the Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <year> 1994. </year>
Reference-contexts: This data mapping determines the work mapping the data access pattern. Research efforts concentrate on finding good data partitioning in order to reduce the parallel time. Many compilers are built to handle regular problems, such as PARADIGM [4], Fortran D [1], and SUIF [2]. For irregular code, the CHAOS <ref> [34] </ref> system uses inspector-executor approach to produce code for input-dependent problems. In the functional parallelism model, the basic units of the program are functional components. The PYRROS scheduling tool developed here at Rutgers is based upon the Dataflow Task Graph model, a kind of functional parallelism. <p> Alternatively, if we focus on the real code instead of algorithm, a regular problem has affine loop bounds which are also input independent, while an irregular problem has indirect, thus maybe input dependent, loop bounds. This issue is discussed in papers concerning the CHAOS system. See e.g. <ref> [34] </ref>. <p> On the other hand, the "cost-zone" partition is essentially a more involved load balancing, and to our knowledge our thesis work is the first one to model the FMM algorithm as a task graph and solve the problem in an automatic scheduling/rescheduling framework. The CHAOS/PARTI system <ref> [34] </ref> deals with irregular DOALL loop parallelism. It adheres to the data parallelism model and tries to optimize the array partitioning in order to reduce execution time.
Reference: [35] <author> J. D. Keyser and D. Roose, </author> <title> Load balancing data-parallel programs on distributed memory computers, </title> <journal> Parallel Computing, </journal> <volume> Vol. 19, No. 11, </volume> <pages> pp. 1199-1219, </pages> <month> Nov. </month> <year> 1993 </year>
Reference-contexts: The process carries out local adjustments without traversal of the whole graph, thus having a low complexity. Dynamic processor load balancing has been extensively studied in the literature, such as <ref> [35] </ref>. The common principle behind the many algorithms is that the workload of each processor is sampled and works are transferred between processors in the balancing phase. <p> Runtime support for dynamic problems has been traditional studied in a framework of load balancing strategies, where the works assigned to processors are not assumed to be interdependent as in a task graph. The amount of literature for load balancing is very large, such as <ref> [35, 53] </ref>, among many others. But these are not directly applicable to our model. We are the first to propose dynamic support algorithms for task graphs.
Reference: [36] <author> S.J. Kim and J.C Browne, </author> <title> A general approach to mapping of parallel computation upon multiprocessor architectures, </title> <booktitle> Proc. of Int'l Conf. on Parallel Processing, </booktitle> <volume> vol 3, </volume> <pages> pp. 1-8, </pages> <year> 1988. </year>
Reference-contexts: Most of the algorithms in the literature have complexity higher than O (v 2 ) which makes them impractical for large task graphs. * Experimental results using random graphs and numerical computation DAGs show that DSC is comparable or even better than several higher complexity algorithms from the literature <ref> [36, 47, 58] </ref> in terms of the makespan of the schedule produced. Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging.
Reference: [37] <author> R. Krasny, </author> <title> Computation of vortex sheet roll-up in the trefftz plane, </title> <journal> Journal of Fluid Mechanics, </journal> <volume> Vol 184, </volume> <pages> pp. 123-155, </pages> <year> 1987. </year>
Reference-contexts: This problem has been extensively investigated by Robert Krasny <ref> [37] </ref>, using simulations by sequential code, and the behavior of typical vortex sheet configurations is well-known. The analysis of the physical implications can be found in Krasny's original paper [37]. Recently Draghicescu ([17]) used a fast algorithm for sequential simulation. The algorithm is somewhat similar to the FMM algorithm. <p> This problem has been extensively investigated by Robert Krasny <ref> [37] </ref>, using simulations by sequential code, and the behavior of typical vortex sheet configurations is well-known. The analysis of the physical implications can be found in Krasny's original paper [37]. Recently Draghicescu ([17]) used a fast algorithm for sequential simulation. The algorithm is somewhat similar to the FMM algorithm. <p> This new problem, closely related to the classical N-body simulation in the sense of underlying physics laws, could provide us with deeper insight into the dynamic support problem due to the predictable roll-up patterns. The following definitions are adopted from <ref> [37] </ref>. 89 Definition 6.1 A vortex sheet is a curve in two dimensional space: z (; t) = x (; t) + iy (; t) where measures the circulation at a point on the sheet and t is time. <p> So fast algorithms, such as the Fast Multipole Method, can be applied to this problem with certain modifications. For initial condition, we give z ((ff); 0) = c fl cosff; 0 &lt; c 1, which makes the sheet a straight line segment from [-c,0] to [c,0]. (The paper <ref> [37] </ref> makes c=1 but we give more freedom to the initial condition, which makes no fundamental difference. Also the y coordinate does not have to be zero. ) We still have to specify the circulation function . There are two typical circulation distributions: 90 1. <p> Such a smoothing approach is not only employed in <ref> [37] </ref> but many other CFD simula tions, such as in [6]. 91 Next, we need to discretize the equation to enable numerical solution. The dis-cretization is quite straightforward. <p> The curve could even intersect itself, which is physically impossible. This requires dynamic insertion of new points (vortex blobs) in order to preserve the smoothness and resolution of the curve. We follow the insertion scheme used by Krasny in <ref> [37] </ref>: Let * be a parameter chosen to control the maximum distance allowed between successive points. The condition jz j+1 z j j * is enforced at every iteration.
Reference: [38] <author> C. McGreary and H. Gill, </author> <title> Automatic determination of grain size for efficient parallel processing, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, </volume> <pages> pp. 1073-1078, </pages> <month> Sept. </month> <year> 1989, </year>
Reference-contexts: This involves choosing a suitable granularity and defining task bodies. Currently there is no system available, although there have been some research effort, such as <ref> [38] </ref>. We believe that although optimal partitioning is difficult to achieve, there can be methods that choose a reasonable task granularity and find the appropriate loop levels to define tasks.
Reference: [39] <author> K. A. Murthy, Y. Li and P.M. Pardalos. </author> <title> A local search algorithm for quadratic assignment problem. </title> <journal> Informatica, </journal> <volume> Vol 3, No. 4, </volume> <pages> pp 524-538, </pages> <year> 1992. </year>
Reference-contexts: Next we focus on the physical mapping part of the system, where improvement can be made. 2.3 A Physical Mapping Algorithm Developed for PYRROS The physical mapping problem is a special case of the Quadratic Assignment Problem <ref> [39] </ref>. Determining an optimal physical mapping is difficult and we modify the Bokhari's heuristic algorithm [8] to be used for PYRROS. The Bokhari's algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that the F (CC; P ) decreases monotonically.
Reference: [40] <author> J.M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <address> New York:Plenum, </address> <year> 1988. </year>
Reference-contexts: Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging. This method is simple and has been shown to work well in practice, e.g. makespan [45], Ortega <ref> [40] </ref>, Gerasoulis and Nelken [22]. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). It is as follows: 1. Compute the arithmetic load LM j for each cluster. 2.
Reference: [41] <author> G.N.S. Prasanna, A. Agarwal and B. R. Musicus, </author> <title> Hierarchical compilation of macro dataflow graphs for multiprocessors with local memory. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> July </month> <year> 1994. </year>
Reference-contexts: For coarse-grain level, they have suggested inter-procedural analysis [31], which is still an open research area. The Paradigm system [4] developed at Illinois is also able to perform dependence analysis and data partitioning. Recently the model of hierarchical task graph has been proposed in <ref> [41] </ref>, which allows data parallelism within tasks, in an attempt to combine data and task parallelism. Other works on generalized task graph models include scheduling for iterative task graphs, as presented in [64], where cross-iteration dependences exist and the scheduling is done symbolically without unwinding of the iterations.
Reference: [42] <author> K. Psarris, </author> <title> Linear time exact methods for data dependence analysis in practice,Proc. </title> <booktitle> 1995 International Conference on Parallel Processing, </booktitle> <volume> Vol. II, pp.1-8, </volume> <year> 1995. </year> <month> 162 </month>
Reference-contexts: Statement level dependence analysis. 136 This is done at the individual statement level, without regard to the task definitions. It's a classic problem which has been extensively studied in the literature, and many algorithms have been proposed, such as <ref> [57, 54, 42] </ref>. The system cur rently uses the Omega test ([43]), which is an exact algorithm. 2. Task level communication formation. After the fine grain dependences are extracted, they are then grouped according to the task definitions to produce task level coarse grain dependences, or communication rules. <p> The performance is also comparable to very good hand-made parallel code. Future research directions in this area includes: * Incorporating other dependence analysis algorithms into the PlusPyr component of the software tool. Conceivably, some low complexity algorithms, such as the 152 one proposed by Psarris in <ref> [42] </ref>, can be used. * Adding loop transformations, especially automatic tiling into the PlusPyr front-end. Although block operations can increase the granularity, this approach does not apply to all problems.
Reference: [43] <author> W. Pugh and D. Wonnacott, </author> <title> An exact method for analysis of value-based array data dependences, </title> <booktitle> Proc. of the Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1992. </year>
Reference-contexts: This will pose serious obstacle for any user who is not specialized in parallel programming, and limit the usage of parallel machines. 4 Automatic parallelism detection has been researched in the literature, mainly based upon dependence analysis of serial code, see for example <ref> [43] </ref>. Much progress has been made to detect parallelism in affine loops. But again the general problem of determining precise dependences is intractable. <p> PlusPyr, [15], an automatic task graph generation tool developed in France, with the aim of providing a front-end user interface for PYRROS, is the first such attempt, and for dense linear algebra problems it works well. PlusPyr incorporates the Omega test <ref> [43] </ref> as its dependence analysis engine, and creates coarse grain communication rules between tasks by merging fine grain dependences. <p> Current research effort, therefore, is starting from regular task graphs, hoping to extend the results to general task graphs in the long term future. Such regular codes usually have affine loop bounds. See e.g. <ref> [2, 43] </ref>. Even when we narrow down to only regular task graphs, the effort needed is still great. As far as we know no existing system can accomplish completely automatic parallelization even for basic regular task graphs.
Reference: [44] <author> E. Rothberg and A. Gupta, </author> <title> An efficient block-oriented approach to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The Sparse Matrix Computation is one of the most widely investigated problems, and it's notorious difficult to obtain performance for this class of problems. Many methods has been proposed. See for example, work done by Venugopal [51], Rothberg <ref> [44] </ref>. One of the noteworthy works, which is very closely related to our approach, is done by Chong et al [11], where the DSC clustering algorithm from PYRROS was applied to the sparse triangular solution iterative problem.
Reference: [45] <author> Y. Saad, </author> <title> Gaussian elimination on hypercubes, in Parallel Algorithms and Architectures, </title> <editor> Cosnard, M. et al. Eds., </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging. This method is simple and has been shown to work well in practice, e.g. makespan <ref> [45] </ref>, Ortega [40], Gerasoulis and Nelken [22]. The complexity of this algorithm is O (u log u + v), which is less than O (v log v). It is as follows: 1. Compute the arithmetic load LM j for each cluster. 2.
Reference: [46] <author> J. K. Salmon, </author> <title> Parallel Hierarchical N-body Methods. </title> <type> Ph.D thesis, </type> <institution> California Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Parallel N-body simulation is an active area of research in its own right, and is the topic of a number of Ph.D dissertations, such as <ref> [46] </ref>, [48]. We are the first to study this parallelization problem from the perspective of task graph scheduling. We developed a task graph generator for the 2-dimensional adaptive FMM algorithm, implemented the algorithm on the Ncube2s machine, and achieved good performance.
Reference: [47] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: This model, although idealizes the send and receive operations, is widely accepted in the works of many other researchers such as Sarkar <ref> [47] </ref>. It also should be noted that the execution is asynchronous and the send operations are non-blocking, meaning that the sending processor continues its execution immediately after the send operation without waiting for an acknowledgment of receipt. This model is free of communication deadlock. <p> A good scheduling algorithm should try to minimize the parallel time. In general such minimization problems are intractable, and many heuristics are proposed for good, albeit non-optimal results. See, for example, work done by Sarkar, Hwang et al. <ref> [47, 33] </ref>, and our own heuristic. 2.2 The Multistage Approach and a Sketch of Each Stage PYRROS is an automatic task graph scheduling tool developed at Rutgers. The tool has undergone extensive evolution since its inception. <p> Most of the algorithms in the literature have complexity higher than O (v 2 ) which makes them impractical for large task graphs. * Experimental results using random graphs and numerical computation DAGs show that DSC is comparable or even better than several higher complexity algorithms from the literature <ref> [36, 47, 58] </ref> in terms of the makespan of the schedule produced. Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging. <p> The Inspector/Executor model is used for generating input-dependent data mapping. There are many other heuristics for task graph scheduling in the literature. PYRROS belongs to the class of multistage methods, which also includes the Sarkar's algorithm <ref> [47] </ref>, among many others. Some other methods are one-stage, in the sense that the schedule is produced directly for the given number of processors. The ETF algorithm [33] is one of the most efficient among one-stage methods.
Reference: [48] <author> J. P. Singh, </author> <title> Parallel hierarchical N-body methods and their implications for multiprocessors, </title> <type> Ph.D thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: Parallel N-body simulation is an active area of research in its own right, and is the topic of a number of Ph.D dissertations, such as [46], <ref> [48] </ref>. We are the first to study this parallelization problem from the perspective of task graph scheduling. We developed a task graph generator for the 2-dimensional adaptive FMM algorithm, implemented the algorithm on the Ncube2s machine, and achieved good performance. <p> For the same reason, a number of other works ,such as Singh's parallelization of the FMM algorithm <ref> [48] </ref>, also focused on the 2D instead of the 3D version. The basic idea of the algorithm is to subdivide the space into boxes, and the evaluation of particle positions between far-away boxes can be approximated by series of expansions. <p> The N-body simulation has also been of long-standing research interest. Most of the parallel implementations has been for the Barnes-Hut algorithm [5] due to its relative simplicity and reasonable complexity, but efforts are also been made on the Fast Multipole Algorithms. Singh <ref> [48] </ref> has proposed a very problem-specific technique called "cost-zone", which can be applied to both Barnes-Hut and FMM algorithms. He also employed the same argument about the slow and gradual movement of the particles to justify his approaches.
Reference: [49] <author> J. P. Singh, C. Holt, J. L. Hennessy and A. Gupta, </author> <title> A parallel adaptive fast multi-pole method. </title> <booktitle> IEEE Supercomputing 93, </booktitle> <pages> pp. 54-65, </pages> <year> 1993. </year>
Reference-contexts: Since time step size ffit has to be chosen small enough to ensure numerical accuracy and stability. This insight is crucial to almost all the previous works relating to the N-body problem, since it allows the partitioning obtained from the initial distribution to be reused. See for example, <ref> [49] </ref>. Computational scientists had a general rule of allow only very tiny movements per time step, even if the cumulative effect during the whole simulation process is dramatic. This is the only way to follow the fastest motion in the system with reasonable accuracy.
Reference: [50] <author> V. S. Sunderam, G. A. Geist, J. Dongarra and R. Manchek, </author> <title> The PVM concurrent computing system: evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> Vol 20, No. 4, </volume> <pages> pp. 531-547, </pages> <month> April </month> <year> 1994. </year>
Reference: [51] <author> S. Venugopal and V. K. Naik, </author> <title> SHAPE: A parallelization tool for sparse matrix computations. </title> <type> Technical Report DSC-TR-290, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ. </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The Sparse Matrix Computation is one of the most widely investigated problems, and it's notorious difficult to obtain performance for this class of problems. Many methods has been proposed. See for example, work done by Venugopal <ref> [51] </ref>, Rothberg [44]. One of the noteworthy works, which is very closely related to our approach, is done by Chong et al [11], where the DSC clustering algorithm from PYRROS was applied to the sparse triangular solution iterative problem.
Reference: [52] <author> D. W. Walker, </author> <title> The design of a standard message passing interface for distributed memory concurrent computers. </title> <journal> Parallel Computing, </journal> <volume> Vol 20, No. 4, pp.657-675, </volume> <month> April </month> <year> 1994. </year>
Reference: [53] <author> M. Willebeek-LeMair and A. Reeves, </author> <title> Strategies for dynamic load balancing on highly parallel computers, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 9, </volume> <pages> pp. 979-993, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Runtime support for dynamic problems has been traditional studied in a framework of load balancing strategies, where the works assigned to processors are not assumed to be interdependent as in a task graph. The amount of literature for load balancing is very large, such as <ref> [35, 53] </ref>, among many others. But these are not directly applicable to our model. We are the first to propose dynamic support algorithms for task graphs.
Reference: [54] <author> M. E. Wolf and M.S. Lam, </author> <title> A loop transformation theory and an algorithm to maximize parallelism, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <month> Oct. </month> <year> 1991, </year> <pages> pp. 452-471. </pages>
Reference-contexts: Statement level dependence analysis. 136 This is done at the individual statement level, without regard to the task definitions. It's a classic problem which has been extensively studied in the literature, and many algorithms have been proposed, such as <ref> [57, 54, 42] </ref>. The system cur rently uses the Omega test ([43]), which is an exact algorithm. 2. Task level communication formation. After the fine grain dependences are extracted, they are then grouped according to the task definitions to produce task level coarse grain dependences, or communication rules. <p> Tiling is a useful loop transformation technique to expose more parallelism. It can also be used to increase the granularity of the tasks. Tiling is extensively employed in the SUIF compiler. See <ref> [2, 54] </ref> for detailed discussions about titling transformation. Here we just give a simple example to illustrate how the granularity may be increased.
Reference: [55] <author> M. Wolfe, </author> <title> The Tiny loop restructuring research tool, </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference: [56] <author> M. Wolfe, </author> <title> Optimizing supercompilers for supercomputers, </title> <publisher> Pitman, London and The MIT press, </publisher> <year> 1989. </year>
Reference: [57] <author> M. Wolfe and U. Banerjee, </author> <title> Data dependence and its application to parallel processing, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 16, No. 2, </volume> <pages> pp. 137-178, </pages> <year> 1987. </year> <month> 163 </month>
Reference-contexts: Statement level dependence analysis. 136 This is done at the individual statement level, without regard to the task definitions. It's a classic problem which has been extensively studied in the literature, and many algorithms have been proposed, such as <ref> [57, 54, 42] </ref>. The system cur rently uses the Omega test ([43]), which is an exact algorithm. 2. Task level communication formation. After the fine grain dependences are extracted, they are then grouped according to the task definitions to produce task level coarse grain dependences, or communication rules.
Reference: [58] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference-contexts: Most of the algorithms in the literature have complexity higher than O (v 2 ) which makes them impractical for large task graphs. * Experimental results using random graphs and numerical computation DAGs show that DSC is comparable or even better than several higher complexity algorithms from the literature <ref> [36, 47, 58] </ref> in terms of the makespan of the schedule produced. Cluster Merging PYRROS uses a variation of work profiling method suggested by George et. al.[19] for cluster merging.
Reference: [59] <author> T. Yang, </author> <title> Scheduling and code generation for parallel architecture, </title> <type> Ph.D Thesis, </type> <institution> DCS-TR 299, Rutgers University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The tasks, together with the dataflow dependences, forms a Dataflow Task Graph. We shall discuss this model and the concept of scheduling in more details in chapter 2. Previous work has demonstrated that PYRROS can achieve good performance on both random graphs and many matrix algebra problems <ref> [59] </ref>. <p> See [11]. Scheduling for a static task graph is totally dependent on the initial graph, since the schedule for the first iteration may be reused for all time, although sometimes pipelining technique can be employed to overlap consecutive iterations in order to improve overall performance. For instance <ref> [59] </ref> uses iteration overlapping for the Laplace iterative problem. <p> If this is an unbounded number of processors, we simply assign the chain into a new 60 processor, or cluster. If we compare what we did with the procedure used in the DSC clustering algorithm described in <ref> [59] </ref>, we shall see that we are indeed carrying out cluster linearization: The original DSC scheduling step has created non-linearized processor assignment by putting T and the T h chain in the same processor. <p> See <ref> [59] </ref>. * For irregular task graphs that can't be parameterized, the user needs to generate the complete task graph from the particular problem and save it in a data file readable by PYRROS.
Reference: [60] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static task scheduling and code generation for message-passing multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: We repeat this merge process until all new clusters has been accommodated. This has the same underlying mechanism as the cluster merge process in PYRROS. See <ref> [60] </ref>.
Reference: [61] <author> T. Yang and A. Gerasoulis, </author> <title> A Fast Scheduling Algorithm for DAGs on an Unbounded Number of Processors, </title> <booktitle> Proc. of IEEE Supercomputing 91, </booktitle> <pages> pp. 633-642. </pages>
Reference-contexts: The overall complexity of this method is O ((v + e) log v + p 3 ). We sketch the algorithms employed for each stage below. 19 Clustering PYRROS uses the Dominant Sequence Algorithm (DSC) to automatically determine the clustering for task graphs, see <ref> [61] </ref> for a detailed description. It has been shown to be superior to several other clustering algorithms in the literature in the following sense: * DSC has an "almost linear" complexity, O ((v + e) log v)) time complexity and O (v+e) space complexity.
Reference: [62] <author> T. Yang and A. Gerasoulis, </author> <title> List scheduling with and without communication delay, </title> <journal> Parallel Computing, </journal> <volume> Vol 19, </volume> <year> 1993, </year> <pages> pp. 1321-1344. </pages>
Reference-contexts: A task becomes free first but it may still be waiting for some data to arrive from predecessors.) A detailed analysis of the task ordering problem is given in <ref> [62] </ref>. It provided some evidence that using ready list has some advantages over 21 using free list. Code Generation All of the above stages are components of the scheduler. PYRROS contains one other major component which is the code generator.
Reference: [63] <author> T. Yang and A. Gerasoulis, </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 9, </volume> <pages> 951-967, </pages> <year> 1994. </year>
Reference: [64] <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines. </title> <booktitle> Proc. of 24th Inter. Conference on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 151-158, </pages> <month> Aug. </month> <year> 1995. </year> <month> 164 </month>
Reference-contexts: Recently the model of hierarchical task graph has been proposed in [41], which allows data parallelism within tasks, in an attempt to combine data and task parallelism. Other works on generalized task graph models include scheduling for iterative task graphs, as presented in <ref> [64] </ref>, where cross-iteration dependences exist and the scheduling is done symbolically without unwinding of the iterations. Most recently in [10], Chakrabarti et. al. consider global communication analysis in the High Performance Fortran compiler, which can reduce message passing overhead.
References-found: 64

