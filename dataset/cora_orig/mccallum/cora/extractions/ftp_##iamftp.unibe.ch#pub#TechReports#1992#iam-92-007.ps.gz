URL: ftp://iamftp.unibe.ch/pub/TechReports/1992/iam-92-007.ps.gz
Refering-URL: 
Root-URL: 
Title: Parametric string matching and its application to pattern recognition  
Author: H. Bunke J. Csirik 
Keyword: clustering, symbolic nearest-neighbor classification.  
Abstract: String matching is a useful concept in pattern recognition that is constantly receiving attention from both theoretical and practical points of view. In this paper we propose a generalized version of the string matching algorithm by Wagner and Fischer [1]. It is based on a parametrization of the edit cost. We assume constant cost for any delete and insert operation, but the cost for replacing a symbol is given as a parameter r. For any two given strings A and B, our algorithm computes the edit distance of A and B in terms of the parameter r. We give the new algorithm and study some of its properties. Its time complexity is O(n 2 m), where n and m are the lengths of the two strings to be compared and n m. We also discuss potential applications of the new string distance to pattern recognition. Finally, we present some experimental results. key words: string matching, inference of edit cost, dynamic programming, symbolic
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Wagner, R. A./Fischer, M. J.: </author> <title> The string-to-string correction problem. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 21, No. 1, </volume> <year> 1974, </year> <pages> 168-173. </pages>
Reference-contexts: For measuring the similarity of patterns that are represented by strings, one needs the notion of string similarity. The first algorithms for computing the distance, or similarity, of two strings was proposed long ago [2]. Subsequently, other authors have studied the string distance computation problem <ref> [1] </ref>. Generally, one introduces a set of basic edit operations with costs associated with each edit operation and defines the distance of two strings as the minimum cost sequence of edit operations needed to transform one string into the other. The algorithm of Wagner and Fischer [1] is usually referred to <p> string distance computation problem <ref> [1] </ref>. Generally, one introduces a set of basic edit operations with costs associated with each edit operation and defines the distance of two strings as the minimum cost sequence of edit operations needed to transform one string into the other. The algorithm of Wagner and Fischer [1] is usually referred to as the standard solution to the problem. It is based on dynamic programming and has a time complexity of O (n m), where n and m give the lengths of the two strings to be compared. <p> k by ffi (E) = i=1 Now, the edit distance ffi of strings A and B can be defined by ffi (A; B) = minfffi (E)jB is derivable f rom A via Eg: 3 An algorithm for computing the edit distance ffi (A; B) was given by Wagner and Fischer <ref> [1] </ref>; their solution uses a simple dynamic programming approach. Next, we briefly review this algorithm. Let A (i; j) = a i a i+1 : : : a j and B (i; j) = b i b i+1 : : : b j . <p> together with a deletion over a replacement, independently of the concrete value of r as long as r 2. 3 The parametric string matching algorithm As data structures, we use a cost matrix of dimension (n + 1) fi (m + 1), similar to the algorithm by Wagner and Fischer <ref> [1] </ref>. In each position of this matrix, we store a list consisting of elements [k; (x; y)]. <p> A pseudocode description of the algorithm is given below. The algorithm consists of three procedures. The main procedure is parametric distance. This procedure calls stringmatch, and stringmatch calls make-entry. The backpointers in lines 18, 21 and 26 have the same meaning as in the algorithm by Wagner and Fischer <ref> [1] </ref>. <p> At D (5; 4) we can join the interval (2=3; 1) with (1; 2) and get the list element [3; (2=3; 2)]. From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b <ref> [1; (0; 2)] </ref> [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; <p> From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b <ref> [1; (0; 2)] </ref> [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; <p> Therefore, the determination of the minimum in lines 13-16 corresponds to the determination of the minimum in (0) and the correctness of the algorithm immediately follows from the correctness of Wagner and Fischer's algorithm <ref> [1] </ref>. 2 From this theorem, we immediately get the following corollary. <p> It is based on a generalization of the well-known algorithm by Wagner and Fischer <ref> [1] </ref>. For our new 19 string distance, we assume constant cost for any delete and insert operation. However, the replacement cost is given as a parameter. Hence, for any two given strings A and B our algorithm computes their edit distance in terms of the parameter r. <p> It has a time complexity of O (n 2 m), where n and m give the lengths of the two strings under consideration and n = min (n; m). In [16], it has been shown that the space complexity of the algorithm by Wagner and Fischer <ref> [1] </ref> can be reduced from O (n m) to O (n) as only one predecessor row is needed when computing the elements of the cost matrix in a row-by-row fashion.
Reference: [2] <author> Levenshtein, V. I.: </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Cybernetics and Control Theory, </journal> <volume> Vol. 10, No. 8, </volume> <year> 1966, </year> <pages> 707-710. </pages>
Reference-contexts: For measuring the similarity of patterns that are represented by strings, one needs the notion of string similarity. The first algorithms for computing the distance, or similarity, of two strings was proposed long ago <ref> [2] </ref>. Subsequently, other authors have studied the string distance computation problem [1]. <p> However, a number of adjacent intervals having identical costs will be joined. For example, at D (2; 4) we get for r 2 (1; 2) f 1 = 2 = min (f 1 ; f 2 ; f 3 ) and so we replace the last two list elements <ref> [2; (0; 1)] </ref> and [2; (1; 2)] by [2; (0; 2)]. Similar join operations occur at D (3; 1); D (3; 2); D (4; 1); D (4; 2); D (5; 1); D (5; 2) and D (5; 3). <p> For example, at D (2; 4) we get for r 2 (1; 2) f 1 = 2 = min (f 1 ; f 2 ; f 3 ) and so we replace the last two list elements [2; (0; 1)] and <ref> [2; (1; 2)] </ref> by [2; (0; 2)]. Similar join operations occur at D (3; 1); D (3; 2); D (4; 1); D (4; 2); D (5; 1); D (5; 2) and D (5; 3). <p> For example, at D (2; 4) we get for r 2 (1; 2) f 1 = 2 = min (f 1 ; f 2 ; f 3 ) and so we replace the last two list elements [2; (0; 1)] and [2; (1; 2)] by <ref> [2; (0; 2)] </ref>. Similar join operations occur at D (3; 1); D (3; 2); D (4; 1); D (4; 2); D (5; 1); D (5; 2) and D (5; 3). <p> From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] <ref> [2; (0; 2)] </ref> [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; <p> From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] <ref> [2; (0; 2)] </ref> [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] <p> From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] <ref> [2; (1; 2)] </ref> [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; <p> we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] <ref> [2; (1; 2)] </ref> [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; <p> + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] <ref> [2; (1; 2)] </ref> [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An <p> 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] <ref> [2; (1; 2)] </ref> [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of <p> 2)] <ref> [2; (1; 2)] </ref> a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost.
Reference: [3] <author> Hunt, J.W./Szymanski, T.G.: </author> <title> A fast algorithm for computing longest common subsequences. </title> <journal> CACM, </journal> <volume> Vol. 20, No. 5, </volume> <year> 1977, </year> <pages> 350-353. </pages>
Reference-contexts: Similar join operations occur at D (3; 1); D (3; 2); D (4; 1); D (4; 2); D (5; 1); D (5; 2) and D (5; 3). At D (5; 4) we can join the interval (2=3; 1) with (1; 2) and get the list element <ref> [3; (2=3; 2)] </ref>. <p> From D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] <ref> [3; (0; 2)] </ref> a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; <p> D (5; 4), we conclude d (A; B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] <ref> [3; (1; 2)] </ref> [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; <p> B) = 1 + 3r if r 2 (0; 2=3) 7 a c b a b [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] <ref> [3; (0; 2)] </ref> a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] <p> [1; (0; 2)] [r; (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] <ref> [3; (1; 2)] </ref> [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for <p> + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] <ref> [3; (1; 2)] </ref> [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the <p> 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] <ref> [3; (1; 2)] </ref> [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost. <p> 2)] <ref> [3; (1; 2)] </ref> [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost. <p> 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] <ref> [3; (2=3; 1)] </ref> Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost.
Reference: [4] <author> Myers, </author> <title> E.W.: An O(N D) difference algorithm and its variations. </title> <journal> Algorithmica, </journal> <volume> Vol. 1, </volume> <year> 1986, </year> <pages> 251-266. </pages>
Reference-contexts: (0; 2)] [1 + r; (0; 2)] [2; (0; 2)] [3; (0; 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c <ref> [4; (0; 2)] </ref> [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is <p> 2)] a [2; (0; 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] <ref> [4; (1; 2)] </ref> b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost. <p> 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] <ref> [4; (1; 2)] </ref> b [5; (0; 2)] [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost.
Reference: [5] <author> Ukkonen, E.: </author> <title> Algorithms for approximate string matching. </title> <journal> Inform. and Control, </journal> <volume> Vol. 64, </volume> <year> 1985, </year> <pages> 100-118. </pages>
Reference-contexts: 2)] [1; (0; 2)] [2; (1; 2)] [3; (1; 2)] [2; (1; 2)] a [3; (0; 2)] [2; (1; 2)] [1 + r; (1; 2)] [2 + r; (1; 2)] [3; (1; 2)] c [4; (0; 2)] [3; (1; 2)] [2; (1; 2)] [3; (1; 2)] [4; (1; 2)] b <ref> [5; (0; 2)] </ref> [4; (1; 2)] [3; (1; 2)] [2; (1; 2)] [3; (2=3; 1)] Fig. 1: An example 5 Some properties of the algorithm Theorem 1 a) The algorithm for parametric distance calculation is complete over the interval (0,2) of replacement cost.
Reference: [6] <author> Masek, W. J./Paterson, </author> <title> M.S.: A faster algorithm for computing string-edit distances. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 20, No 1, </volume> <year> 1980, </year> <pages> 18-31. </pages>
Reference-contexts: For this problem, a number of algorithms with a lower computational complexity than that of Wagner and Fischer's algorithm have been published [3]-[5]. The asymptotically fastest algorithm for the original string distance problem is that of Masek and Patterson <ref> [6] </ref>, having a time complexity of O (n 2 = log n), where again n and m are the lengths of the two strings to be compared, and n m. A recent survey on algorithms for string matching has been given in [7].
Reference: [7] <author> Aho, </author> <title> A.V.: Algorithms for finding patterns in strings. </title> <editor> In J. van Leeuwen (ed.): </editor> <booktitle> Handbook of theoretical computer science. </booktitle> <publisher> Elsevier Science Publishers B. V., </publisher> <year> 1990, </year> <pages> 255-300. </pages>
Reference-contexts: A recent survey on algorithms for string matching has been given in <ref> [7] </ref>. Another article that reviews and compares a number of different string matching algorithms is [8]. There are a number of interesting applications of string matching in the discipline of pattern recognition. One class of problems that has received much attention is the recognition of two-dimensional shapes [9]-[11].
Reference: [8] <author> Abe, K., Sugita, N.: </author> <title> Distances between strings of symbols-Review and remarks. </title> <booktitle> Proc. 6th ICPR, </booktitle> <address> Munich, </address> <year> 1982, </year> <pages> 172-174. </pages>
Reference-contexts: A recent survey on algorithms for string matching has been given in [7]. Another article that reviews and compares a number of different string matching algorithms is <ref> [8] </ref>. There are a number of interesting applications of string matching in the discipline of pattern recognition. One class of problems that has received much attention is the recognition of two-dimensional shapes [9]-[11]. Character recognition has been addressed in [12].
Reference: [9] <author> Tsai, W.H./Yu, S.S.: </author> <title> Attributed string matching with merging for shape recognition, </title> <journal> IEEE Trans. </journal> <volume> PAMI 7, </volume> <year> 1985, </year> <pages> 453-462. </pages>
Reference: [10] <author> Gorman, J.W/Mitchell, O.R./Kuhl, F.: </author> <title> Partial shape recognition using dynamic programming, </title> <journal> IEEE Trans. </journal> <volume> PAMI 10, </volume> <year> 1988, </year> <pages> 257-266. </pages>
Reference: [11] <author> Maes, M.: </author> <title> Polygonal shape recognition using string matching techniques. </title> <journal> Pattern Recognition, </journal> <volume> Vol. 24, No. 5, </volume> <year> 1991, </year> <pages> 433-440. </pages>
Reference: [12] <author> Fu, K.S./Lu, S.-Y.: </author> <title> A clustering procedure for syntactic patterns, </title> <journal> IEEE Trans. </journal> <volume> SMC 7, </volume> <year> 1977, </year> <pages> 734-742. </pages>
Reference-contexts: There are a number of interesting applications of string matching in the discipline of pattern recognition. One class of problems that has received much attention is the recognition of two-dimensional shapes [9]-[11]. Character recognition has been addressed in <ref> [12] </ref>. Recently it was shown that string matching is useful for a number of medical applications [13]. A bar code reading procedure based on string matching has been described in [14]. A reference covering applications of string matching in speech recognition, molecular biology and other fields is [15].
Reference: [13] <author> Gregor, J.: </author> <title> Aspects of data-driven inference and dynamic programming analysis of pattern structure in strings, </title> <type> PhD Thesis, </type> <institution> Laboratory of Image Analysis, Institute of Electronic Systems, University of Aalborg, Denmark, </institution> <year> 1991. </year>
Reference-contexts: One class of problems that has received much attention is the recognition of two-dimensional shapes [9]-[11]. Character recognition has been addressed in [12]. Recently it was shown that string matching is useful for a number of medical applications <ref> [13] </ref>. A bar code reading procedure based on string matching has been described in [14]. A reference covering applications of string matching in speech recognition, molecular biology and other fields is [15].
Reference: [14] <author> Wang, Y.P./Pavlidis, T.: </author> <title> Optimal correspondences of string subsequences. </title> <editor> In Baird, H. (ed.): </editor> <booktitle> SSPR 90, Preproceedings International Association for Pattern Recognition Workshop on Syntactic and Structural Pattern Recognition, </booktitle> <address> Murray Hill, New Jersey, </address> <year> 1990, </year> <pages> 460-479. </pages>
Reference-contexts: Character recognition has been addressed in [12]. Recently it was shown that string matching is useful for a number of medical applications [13]. A bar code reading procedure based on string matching has been described in <ref> [14] </ref>. A reference covering applications of string matching in speech recognition, molecular biology and other fields is [15]. One advantage of the use of string matching in pattern recognition is that there is no inference or learning procedure of the models or prototype patterns required.
Reference: [15] <editor> Sankoff, D./Kruskal, J.B. (eds.): </editor> <title> Time warps, string edits, and macromolecules; the theory and practice of sequence comparsion. </title> <publisher> Addison Wesley Publ. Co., </publisher> <address> Reading, Ma., </address> <year> 1983. </year>
Reference-contexts: Recently it was shown that string matching is useful for a number of medical applications [13]. A bar code reading procedure based on string matching has been described in [14]. A reference covering applications of string matching in speech recognition, molecular biology and other fields is <ref> [15] </ref>. One advantage of the use of string matching in pattern recognition is that there is no inference or learning procedure of the models or prototype patterns required.
Reference: [16] <author> Hirschberg, D. S.: </author> <title> A linear space algorithm for computing maximal common subsequences, </title> <journal> Comm. ACM, </journal> <volume> Vol. 18, No. 6, </volume> <year> 1975, </year> <pages> 341-343. 21 </pages>
Reference-contexts: The new algorithm can be easily implemented. It has a time complexity of O (n 2 m), where n and m give the lengths of the two strings under consideration and n = min (n; m). In <ref> [16] </ref>, it has been shown that the space complexity of the algorithm by Wagner and Fischer [1] can be reduced from O (n m) to O (n) as only one predecessor row is needed when computing the elements of the cost matrix in a row-by-row fashion.
References-found: 16

