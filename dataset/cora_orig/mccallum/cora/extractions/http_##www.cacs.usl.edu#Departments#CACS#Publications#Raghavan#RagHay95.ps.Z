URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/RagHay95.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Title: The State of Rough Sets for Database Mining Applications  
Author: Vijay V. Raghavan Hayri Sever 
Address: Lafayette, LA 70504, USA  Lafayette, LA 70504, USA  
Affiliation: The Center for Advanced Computer Studies University of Southwestern Louisiana  The Department of Computer Science University of Southwestern Louisiana  
Abstract: The database mining problem is often cited as one of the most promising research topics in the fields of database systems and machine learning. Although many available machine learning algorithms are potentially applicable, real-world databases pose additional difficulties partly due to the nature of their contents. In this article, we describe the characteristic features of the database mining problem, a subset of data mining queries, and the approaches for designing a database mining environment. Then, in that context, we summarize the state of rough sets and present future directions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and B. Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proceedings of the 18th VLDB Conf., </booktitle> <pages> pages 560-573, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: Then we consider the case where a KDD system has a DBMS interface. Second, we present a taxonomy of database mining queries, which is not exhaustive; however, it constitutes an interesting subset of the ones cited in the literature <ref> [1, 4, 19, 25, 36] </ref>. 2 In the literature, the database mining problem is also known as data mining or the knowledge discovery in databases. 2 2.1 Characteristic features of the database mining problem 1.
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans. Knowledge and Data Eng., </journal> <volume> 5(6) </volume> <pages> 914-924, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, the database technology of today offers little functionality to explore such data. At the same time, knowledge discovery 1 techniques for intelligent data analysis are not yet mature for large data sets <ref> [23, 2, 49, 10, 25] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources.
Reference: [3] <author> H. Almuallim and T. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pages 547-552, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A plausible approach to tackle ultra large data is to reduce the data set horizontally by merging identical tuples following the substitution of an attribute value by its higher level value in a generalization hierarchy of categorical attributes [13, 49, 37] or vertically by applying some feature selection methods <ref> [17, 3] </ref>. One could also use random sampling methods along with the horizontal/vertical pruning methods, but this method has not received much attention yet. 2. Noisy Data: Non-systematic errors, which can occur during data-entry or collection of data, are usually referred as noise.
Reference: [4] <author> Y. Cai, N. Cercone, and J. Han. </author> <title> Attribute-oriented induction in relational databases. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 213-228. </pages> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Then we consider the case where a KDD system has a DBMS interface. Second, we present a taxonomy of database mining queries, which is not exhaustive; however, it constitutes an interesting subset of the ones cited in the literature <ref> [1, 4, 19, 25, 36] </ref>. 2 In the literature, the database mining problem is also known as data mining or the knowledge discovery in databases. 2 2.1 Characteristic features of the database mining problem 1. <p> This is one of the strategies used in the attribute oriented approach for inductive concept learning <ref> [4, 13, 15] </ref>. Since an attribute-oriented learning technique operates on relations, its strategies can be easily adapted to rough classifiers to reduce the size of some categorical attributes.
Reference: [5] <author> B. Cestnik, I. Kononenko, and I. Bratko. </author> <title> ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> pages 31-45. </pages> <address> Wilmslow, Sigma, </address> <year> 1987. </year>
Reference-contexts: Hence, erroneous data can be a significant problem in real-world databases. This implies that a knowledge discovery method should be less sensitive to noise in the data set. This problem has been extensively investigated for variations of inductive decision trees, depending on where and how much the noise occurs <ref> [33, 5] </ref>. 3. Null Values: In DBMSs, a null value (also known as missing value) may appear as the value of any attribute that is not a part of the primary key and is treated as a symbol distinct from any other symbol, including other occurrences of null values [42].
Reference: [6] <author> C. J. Date. </author> <title> Relational Database Writings 1985-1989. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Unfortunately there is little support by commercial DBMSs to eliminate/reduce errors that occur during data entry, though the potential exists for providing the capability, in relational data models, to force consistency among attribute values with respect to predefined functional dependencies <ref> [6] </ref>. There is virtually nothing that a DBMS can do to catch the errors introduced by the error-prone collection of data. Hence, erroneous data can be a significant problem in real-world databases. This implies that a knowledge discovery method should be less sensitive to noise in the data set.
Reference: [7] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever. Rough set based classification methods and extended decision tables. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 302-309, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Similarly, a further refinement of antecedent parts of rules in a decision algorithm is a part of the summary if the decision algorithm is persistent in the system and the background knowledge from which the decision algorithm has been induced is dynamic. In Deogun et al. <ref> [7] </ref>, we presented an upper classification method that we believe would be a good starting point to develop an incremental rough classifier. In the algebraic space, rough set theory approximates given concept (s) using lower and upper sets of the concept (s). <p> This subject is, however, is not novel to the rough set methodology because there have been numerous works on developing rough approximation methods based on different definitions positive 3 We assume that the reader is acquainted with the basic notions of the rough set theory [29]. 5 (and boundary) regions <ref> [7, 18, 44, 14] </ref>. For example, in the elementary set approximation of an unknown concept [7], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> For example, in the elementary set approximation of an unknown concept <ref> [7] </ref>, an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> The 'inconsistency' is attributed to the result of a classification method while the 'nondetermism' is attributed to the interpretation of that result. As shown in <ref> [7] </ref>, inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterministically. This is an important result, particularly when the background knowledge is incomplete and dynamic. Redundant data can be eliminated by pruning insignificant attributes with respect to a certain problem at hand. <p> One of the claims in <ref> [7] </ref> is that evolving rough classifier schemes can be developed, if the decision table is accommodated with a composite increment field that contains frequencies of rows. * Closeness of two rules: Slowinski & Stefonowski's study on determining the nearest rule, in the case that the description of a given object does
Reference: [8] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever. Rough set model for database mining applications. </title> <type> Technical Report TR-94-6-10, </type> <institution> The University of Southwestern Louisiana, The Center for Advanced Computer Studies, </institution> <year> 1994. </year>
Reference-contexts: When we inspect the database mining queries with respect to the rough set methodology, we see that attribute dependency analysis and classification are well investigated subjects among others. The hypothesis testing and association between values of an attribute can easily be solved by the rough set methodology <ref> [8] </ref>. A recent theoretical paper by Kent [20] extends the notions of approximation and rough equality to formal concept analysis.
Reference: [9] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: This subject has extensively been investigated in the literature <ref> [9, 17, 33, 38, 29] </ref> and is the primary task of inductive learning. 3. <p> Clustering Query: We call unsupervised partitioning of tuples of a relational table a clustering query (also known as unsupervised learning in the context of inductive learning). 4 There are numerous clustering algorithms ranging from the traditional methods of pattern recognition <ref> [9, 16] </ref> to clustering techniques in machine learning [35]. User-defined parameters such as the maximum number of tuples within a cluster or the number of clusters can influence the result of a clustering query. Clustering queries may be helpful for the following two cases.
Reference: [10] <author> W. J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus. </author> <title> Knowledge discovery databases: An overview. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 1-27. </pages> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction It is estimated that the amount of information in the world doubles every 20 months <ref> [10] </ref>; that is, some scientific, government and corporate information systems are being overwhelmed by a flood of data that are generated and stored, routinely. These massive amounts of data are beyond human experts' ability to be analyzed, though they contain potential gold mine of valuable information. <p> Unfortunately, the database technology of today offers little functionality to explore such data. At the same time, knowledge discovery 1 techniques for intelligent data analysis are not yet mature for large data sets <ref> [23, 2, 49, 10, 25] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources. <p> It comes from the idea that large databases can be viewed as data mines which can be discovered by efficient knowledge discovery techniques. Database mining is a promising area of interest shared by database systems and AI research communities <ref> [10, 39] </ref>.
Reference: [11] <author> J. W. Grzymala-Busse. </author> <title> On the unknown attribute values in learning from examples. </title> <editor> In Z. W. Ras and M. Zemankowa, editors, </editor> <booktitle> Proceedings of Methodologies for Intelligent Systems, Lecture Notes in AI, </booktitle> <volume> 542, </volume> <pages> pages 368-377. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: For example, in the list of personal computers, the attribute that contains the model type of the sound cards would be null for some model of computers. We have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [24, 11, 34] </ref>. 4. Incomplete or Redundant Data: The fact that data has been organized and collected around the needs of organizational activities causes incomplete or redundant data from the view point of the knowledge discovery task. <p> A less restrictive version of the problem, which is known as unknown attribute values, has been studied by Grzymala-Busse and implemented in the LERS, a machine learning system <ref> [11, 12] </ref>. * Characterization query: Even though data dependency analysis within the rough set methodology can be applied to characterize concepts, it lacks an explicit structure such as hierarchy of persistent concepts to exploit concept dependencies.
Reference: [12] <author> J. W. Grzymala-Busse. </author> <title> The rule induction system LERS Q: a version for personal computers. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> page 509, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on offline data; KDD-R, an experimental open tool box [48]; LERS, a machine learning system from examples <ref> [12] </ref>; and DataLogic/R [41], a commercial product for database mining and decision support. <p> A less restrictive version of the problem, which is known as unknown attribute values, has been studied by Grzymala-Busse and implemented in the LERS, a machine learning system <ref> [11, 12] </ref>. * Characterization query: Even though data dependency analysis within the rough set methodology can be applied to characterize concepts, it lacks an explicit structure such as hierarchy of persistent concepts to exploit concept dependencies.
Reference: [13] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in databases: An attribute-oriented approach. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 547-559, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: A plausible approach to tackle ultra large data is to reduce the data set horizontally by merging identical tuples following the substitution of an attribute value by its higher level value in a generalization hierarchy of categorical attributes <ref> [13, 49, 37] </ref> or vertically by applying some feature selection methods [17, 3]. One could also use random sampling methods along with the horizontal/vertical pruning methods, but this method has not received much attention yet. 2. <p> Characterization Query: A classification query emphasizes the finding of features that distinguish different classes. On the other hand, the characterization query describes common features of a class regardless of the characteristics of other classes. Typical examples of characterization methods can be found in <ref> [19, 13] </ref>. 3 The state of rough set computation The important question is not "What are approximation spaces ?"; the important question is "How approximation spaces are used ?", R. E. Kent [20]. <p> This is one of the strategies used in the attribute oriented approach for inductive concept learning <ref> [4, 13, 15] </ref>. Since an attribute-oriented learning technique operates on relations, its strategies can be easily adapted to rough classifiers to reduce the size of some categorical attributes.
Reference: [14] <author> R. R. Hashemi, B. A. Pearce, W. G. Hinson, M. G. Paule, and J. F. Young. </author> <title> IQ estimation of monkeys based on human data using rough sets. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 400-407, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: This subject is, however, is not novel to the rough set methodology because there have been numerous works on developing rough approximation methods based on different definitions positive 3 We assume that the reader is acquainted with the basic notions of the rough set theory [29]. 5 (and boundary) regions <ref> [7, 18, 44, 14] </ref>. For example, in the elementary set approximation of an unknown concept [7], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value.
Reference: [15] <author> X. Hu, N. Cercone, and J. Han. </author> <title> An attribute-oriented rough set approach for knowledge discovery in databases. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> pages 79-94, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: This is one of the strategies used in the attribute oriented approach for inductive concept learning <ref> [4, 13, 15] </ref>. Since an attribute-oriented learning technique operates on relations, its strategies can be easily adapted to rough classifiers to reduce the size of some categorical attributes.
Reference: [16] <author> M. A. Ismail and M. S. Kamel. </author> <title> Multidimensional data clustering: Utilizing hybrid search strategies. </title> <journal> Pattern Recognition, </journal> <volume> 22 </volume> <pages> 75-89, </pages> <year> 1989. </year>
Reference-contexts: Clustering Query: We call unsupervised partitioning of tuples of a relational table a clustering query (also known as unsupervised learning in the context of inductive learning). 4 There are numerous clustering algorithms ranging from the traditional methods of pattern recognition <ref> [9, 16] </ref> to clustering techniques in machine learning [35]. User-defined parameters such as the maximum number of tuples within a cluster or the number of clusters can influence the result of a clustering query. Clustering queries may be helpful for the following two cases.
Reference: [17] <author> M. James. </author> <title> Classification Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: A plausible approach to tackle ultra large data is to reduce the data set horizontally by merging identical tuples following the substitution of an attribute value by its higher level value in a generalization hierarchy of categorical attributes [13, 49, 37] or vertically by applying some feature selection methods <ref> [17, 3] </ref>. One could also use random sampling methods along with the horizontal/vertical pruning methods, but this method has not received much attention yet. 2. Noisy Data: Non-systematic errors, which can occur during data-entry or collection of data, are usually referred as noise. <p> This subject has extensively been investigated in the literature <ref> [9, 17, 33, 38, 29] </ref> and is the primary task of inductive learning. 3.
Reference: [18] <author> J. D. Katzberg and W. Ziarko. </author> <title> Variable precision rough sets with asymmetric bounds. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> pages 163-190, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: This subject is, however, is not novel to the rough set methodology because there have been numerous works on developing rough approximation methods based on different definitions positive 3 We assume that the reader is acquainted with the basic notions of the rough set theory [29]. 5 (and boundary) regions <ref> [7, 18, 44, 14] </ref>. For example, in the elementary set approximation of an unknown concept [7], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> It is the process of reducing an information system such that the set of attributes of the reduced information system is independent and no attribute can be eliminated further without losing some information from the system, the result of which is called reduct <ref> [27, 18] </ref>. Given the fact that exhaustive search over the attribute space is exponential in the number of attributes it might not always be computationally feasible to search for the minimum size reduct of attributes.
Reference: [19] <author> K. A. Kaufman, R. S. Michalski, and L. Kerschberg. </author> <title> Mining for knowledge in databases: Goals and general description of the INLEN system. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 449-461. </pages> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Then we consider the case where a KDD system has a DBMS interface. Second, we present a taxonomy of database mining queries, which is not exhaustive; however, it constitutes an interesting subset of the ones cited in the literature <ref> [1, 4, 19, 25, 36] </ref>. 2 In the literature, the database mining problem is also known as data mining or the knowledge discovery in databases. 2 2.1 Characteristic features of the database mining problem 1. <p> Characterization Query: A classification query emphasizes the finding of features that distinguish different classes. On the other hand, the characterization query describes common features of a class regardless of the characteristics of other classes. Typical examples of characterization methods can be found in <ref> [19, 13] </ref>. 3 The state of rough set computation The important question is not "What are approximation spaces ?"; the important question is "How approximation spaces are used ?", R. E. Kent [20].
Reference: [20] <author> R. E. Kent. </author> <title> Rough concept analysis. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> pages 245-253, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: Typical examples of characterization methods can be found in [19, 13]. 3 The state of rough set computation The important question is not "What are approximation spaces ?"; the important question is "How approximation spaces are used ?", R. E. Kent <ref> [20] </ref>. The rough set theory 3 , based on either algebraic or probabilistic approximation spaces, is used to reason about data that may contain uncertain information for a particular knowledge discovery task. In the following, we review the state of rough sets with respect to the database mining problem. <p> The hypothesis testing and association between values of an attribute can easily be solved by the rough set methodology [8]. A recent theoretical paper by Kent <ref> [20] </ref> extends the notions of approximation and rough equality to formal concept analysis. An immediate result of this study, in our database mining context, is to be able to use the rough set methodology for 6 the characterization of a concept (or more generally for concept exploration). <p> This subject has been 7 formally studied by Wille [43] and used for concept modeling. In previous section, we acknowledged Kent's study <ref> [20] </ref> on extending the rough set methodology to the area of formal concept analysis.
Reference: [21] <author> K. Kira and L. Rendell. </author> <title> The feature selection problem: Tradational methods and a new algorithm. </title> <booktitle> In Proocedings of AAAI-92, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year> <month> 9 </month>
Reference-contexts: The usual remedy for this problem is to map non-quantitative values into a numerical scale and use a distance function for the evaluation. For example, Kira & Rendell suggested a binary scale and the authors used it in their Relief algorithm for feature selection <ref> [21] </ref>.
Reference: [22] <author> R. Kohavi and B. Frasca. </author> <title> Useful feature subsets and rough set reducts. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 310-317, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Furthermore, finding just a single reduct of the attributes may be too restrictive for some data analysis problems, which is one of the arguments stated in Kohavi & Frasca's paper <ref> [22] </ref>. One plausible approach is to utilize the idea of fl-reduct given in [30]. Actually, Modrzejewski instead of using the entropy theory applied a variation of fl-reduct algorithm to select more significant features for constructing decision trees [26].
Reference: [23] <author> R. Krishnamurty and T. Imielinski. </author> <title> Research directions in knowledge discovery. </title> <booktitle> SIGMOD RECORD, </booktitle> <volume> 20 </volume> <pages> 76-78, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, the database technology of today offers little functionality to explore such data. At the same time, knowledge discovery 1 techniques for intelligent data analysis are not yet mature for large data sets <ref> [23, 2, 49, 10, 25] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources.
Reference: [24] <author> T. Luba and R. Lasocki. </author> <title> On unknown attribute values in functional dependencies. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 490-497, </pages> <address> San Jose, CA, </address> <year> 1994. </year>
Reference-contexts: For example, in the list of personal computers, the attribute that contains the model type of the sound cards would be null for some model of computers. We have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [24, 11, 34] </ref>. 4. Incomplete or Redundant Data: The fact that data has been organized and collected around the needs of organizational activities causes incomplete or redundant data from the view point of the knowledge discovery task.
Reference: [25] <author> C. J. Matheus, P. K. Chan, and G. Piatetsky-Shapiro. </author> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 903-912, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, the database technology of today offers little functionality to explore such data. At the same time, knowledge discovery 1 techniques for intelligent data analysis are not yet mature for large data sets <ref> [23, 2, 49, 10, 25] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources. <p> Then we consider the case where a KDD system has a DBMS interface. Second, we present a taxonomy of database mining queries, which is not exhaustive; however, it constitutes an interesting subset of the ones cited in the literature <ref> [1, 4, 19, 25, 36] </ref>. 2 In the literature, the database mining problem is also known as data mining or the knowledge discovery in databases. 2 2.1 Characteristic features of the database mining problem 1. <p> Unfortunately, to our best knowledge, there is virtually no comprehensive study on this sub ject. Matheus, Chan, & Piatetsky used the tradeoff between 'versatility' and 'autonomy' for evaluating a KDD system <ref> [25] </ref>. They argued that an ideal KDD system would handle knowledge discovery tasks autonomously while being applicable across many domains. It is a rather rigorous statement, though it rightfully points out the difference between a KDD system and an expert system.
Reference: [26] <author> M. Modrzejewski. </author> <title> Feature selection using rough sets theory. </title> <editor> In P. B. Brazdil, editor, </editor> <booktitle> Machine Learning: Proc. of ECML-93, </booktitle> <pages> pages 213-226. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: One plausible approach is to utilize the idea of fl-reduct given in [30]. Actually, Modrzejewski instead of using the entropy theory applied a variation of fl-reduct algorithm to select more significant features for constructing decision trees <ref> [26] </ref>. The reasoning about ultra large data set is a novel area for the rough set methodology. As stated earlier, one of the plausible approaches to tackle ultra large data is to reduce the data set horizontally, which is not strange to the rough set community.
Reference: [27] <author> Z. Pawlak. </author> <title> Rough classification. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 20 </volume> <pages> 469-483, </pages> <year> 1984. </year>
Reference-contexts: It is the process of reducing an information system such that the set of attributes of the reduced information system is independent and no attribute can be eliminated further without losing some information from the system, the result of which is called reduct <ref> [27, 18] </ref>. Given the fact that exhaustive search over the attribute space is exponential in the number of attributes it might not always be computationally feasible to search for the minimum size reduct of attributes.
Reference: [28] <author> Z. Pawlak. </author> <title> On learning- a rough set approach. </title> <booktitle> In Lecture Notes, </booktitle> <volume> volume 208, </volume> <pages> pages 197-227. </pages> <publisher> Springer Verlak, </publisher> <year> 1986. </year>
Reference-contexts: In rough set based classification, inconsistent rough classifiers (or decision algorithms) have not received as much attention as consistent rough classifiers. In the rough set literature, the terms 'inconsistent' and 'nondeterministic' decision algorithms (or rules) are used interchangeably <ref> [40, 28, 46] </ref>, though they are different concepts. The 'inconsistency' is attributed to the result of a classification method while the 'nondetermism' is attributed to the interpretation of that result. As shown in [7], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterministically.
Reference: [29] <author> Z. Pawlak. </author> <title> Rough Sets: Theoretical Aspects of Reasoning about Data. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: This subject has extensively been investigated in the literature <ref> [9, 17, 33, 38, 29] </ref> and is the primary task of inductive learning. 3. <p> This subject is, however, is not novel to the rough set methodology because there have been numerous works on developing rough approximation methods based on different definitions positive 3 We assume that the reader is acquainted with the basic notions of the rough set theory <ref> [29] </ref>. 5 (and boundary) regions [7, 18, 44, 14]. For example, in the elementary set approximation of an unknown concept [7], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value.
Reference: [30] <author> Z. Pawlak, K. Slowinski, and R. Slowinski. </author> <title> Rough classification of patients after highly selective vagotomy for duodenal ulcer. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 24 </volume> <pages> 413-433, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, finding just a single reduct of the attributes may be too restrictive for some data analysis problems, which is one of the arguments stated in Kohavi & Frasca's paper [22]. One plausible approach is to utilize the idea of fl-reduct given in <ref> [30] </ref>. Actually, Modrzejewski instead of using the entropy theory applied a variation of fl-reduct algorithm to select more significant features for constructing decision trees [26]. The reasoning about ultra large data set is a novel area for the rough set methodology.
Reference: [31] <author> Z. Pawlak, S. K. M. Wong, and W. Ziarko. </author> <title> Rough sets: Probabilistic versus deterministic approach. </title> <journal> Int. J. of Man-Machine Studies, </journal> <volume> 29 </volume> <pages> 81-95, </pages> <year> 1988. </year>
Reference-contexts: Alternatively, another approach would be to shift the domain of the problem from algebraic space to the probabilistic space, if one can assign prior probabilistic measures to the definable sets <ref> [31, 45] </ref>. In rough set based classification, inconsistent rough classifiers (or decision algorithms) have not received as much attention as consistent rough classifiers. In the rough set literature, the terms 'inconsistent' and 'nondeterministic' decision algorithms (or rules) are used interchangeably [40, 28, 46], though they are different concepts.
Reference: [32] <author> Zdzislaw Pawlak. </author> <title> Rough sets present state and further prospects. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 72-76, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Even though it has been more than a decade since the introduction of the rough set theory, there is still a need for further development of rough functions and for extending rough set model to new applications <ref> [32] </ref>. We believe that the investigation of the rough set methodology for database mining in relational DBMSs is a challenging research area with promise of high payoffs in many business and scientific domains.
Reference: [33] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 81-106, </pages> <year> 1986. </year>
Reference-contexts: Hence, erroneous data can be a significant problem in real-world databases. This implies that a knowledge discovery method should be less sensitive to noise in the data set. This problem has been extensively investigated for variations of inductive decision trees, depending on where and how much the noise occurs <ref> [33, 5] </ref>. 3. Null Values: In DBMSs, a null value (also known as missing value) may appear as the value of any attribute that is not a part of the primary key and is treated as a symbol distinct from any other symbol, including other occurrences of null values [42]. <p> This subject has extensively been investigated in the literature <ref> [9, 17, 33, 38, 29] </ref> and is the primary task of inductive learning. 3.
Reference: [34] <author> J. R. Quinlan. </author> <title> Unknown attribute values in induction. </title> <editor> In A. M. Segre, editor, </editor> <booktitle> Proceedings of the Sixth International Machine Learning Workshop, </booktitle> <pages> pages 164-168, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann Pub. </publisher>
Reference-contexts: For example, in the list of personal computers, the attribute that contains the model type of the sound cards would be null for some model of computers. We have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [24, 11, 34] </ref>. 4. Incomplete or Redundant Data: The fact that data has been organized and collected around the needs of organizational activities causes incomplete or redundant data from the view point of the knowledge discovery task.
Reference: [35] <author> V. V. Raghavan. </author> <title> Clustering algorithms for information retrieval: An AI perspective. </title> <booktitle> In Proc. of 9th Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 142-151, </pages> <address> Honolulu, HI, </address> <year> 1986. </year>
Reference-contexts: Clustering Query: We call unsupervised partitioning of tuples of a relational table a clustering query (also known as unsupervised learning in the context of inductive learning). 4 There are numerous clustering algorithms ranging from the traditional methods of pattern recognition [9, 16] to clustering techniques in machine learning <ref> [35] </ref>. User-defined parameters such as the maximum number of tuples within a cluster or the number of clusters can influence the result of a clustering query. Clustering queries may be helpful for the following two cases. First, the user may not know the nature or structure of the data.
Reference: [36] <author> V. V. Raghavan, H. Sever, and J. S. Deogun. </author> <title> A system architecture for database mining applications. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> pages 73-77, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: Then we consider the case where a KDD system has a DBMS interface. Second, we present a taxonomy of database mining queries, which is not exhaustive; however, it constitutes an interesting subset of the ones cited in the literature <ref> [1, 4, 19, 25, 36] </ref>. 2 In the literature, the database mining problem is also known as data mining or the knowledge discovery in databases. 2 2.1 Characteristic features of the database mining problem 1.
Reference: [37] <author> V. V. Raghavan, H. Sever, and J. S. Deogun. </author> <title> A system architecture for database mining applications. </title> <editor> In W. P. Ziarko, editor, </editor> <title> Rough Sets, Fuzzy Sets and Knowledge Discovery, </title> <booktitle> Workshops in Computing Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: A plausible approach to tackle ultra large data is to reduce the data set horizontally by merging identical tuples following the substitution of an attribute value by its higher level value in a generalization hierarchy of categorical attributes <ref> [13, 49, 37] </ref> or vertically by applying some feature selection methods [17, 3]. One could also use random sampling methods along with the horizontal/vertical pruning methods, but this method has not received much attention yet. 2.
Reference: [38] <author> S. Salzberg. </author> <title> Learning with Nested Generalized Exemplars. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: This subject has extensively been investigated in the literature <ref> [9, 17, 33, 38, 29] </ref> and is the primary task of inductive learning. 3.
Reference: [39] <author> A. Silberschatz, M. Stonebraker, and J. Ullman. </author> <title> Database systems: achievements and opportuni-ties. </title> <type> Technical Report TR-90-22, </type> <institution> University of Texas at Austin, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: It comes from the idea that large databases can be viewed as data mines which can be discovered by efficient knowledge discovery techniques. Database mining is a promising area of interest shared by database systems and AI research communities <ref> [10, 39] </ref>.
Reference: [40] <author> R. Slowinski and J. Stefanowiski. </author> <title> Rough classification with valued closeness relation. </title> <booktitle> In Submitted to Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year>
Reference-contexts: In rough set based classification, inconsistent rough classifiers (or decision algorithms) have not received as much attention as consistent rough classifiers. In the rough set literature, the terms 'inconsistent' and 'nondeterministic' decision algorithms (or rules) are used interchangeably <ref> [40, 28, 46] </ref>, though they are different concepts. The 'inconsistency' is attributed to the result of a classification method while the 'nondetermism' is attributed to the interpretation of that result. As shown in [7], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterministically. <p> For example, Kira & Rendell suggested a binary scale and the authors used it in their Relief algorithm for feature selection [21]. A similar methodology was adopted for rough classifiers by Slowinski & Stefanowiski <ref> [40] </ref>, in which the authors incorporated a 'closeness' relation defined on all pairs of attribute values into a distance function. 4 Future Directions As mentioned in the previous section, some aspects of the nature of data (i.e., incomplete, redundant, and uncertain data) have already been investigated in the rough set methodology,
Reference: [41] <author> A. J. Szladow. DataLogic/R: </author> <title> for database mining and decision support. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> page 511, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on offline data; KDD-R, an experimental open tool box [48]; LERS, a machine learning system from examples [12]; and DataLogic/R <ref> [41] </ref>, a commercial product for database mining and decision support.
Reference: [42] <editor> J. D. Ulman. </editor> <booktitle> Principles of Database and Knowledge-Base Systems, </booktitle> <volume> volume 1. </volume> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1988. </year>
Reference-contexts: Null Values: In DBMSs, a null value (also known as missing value) may appear as the value of any attribute that is not a part of the primary key and is treated as a symbol distinct from any other symbol, including other occurrences of null values <ref> [42] </ref>. The null value does not only mean an unknown value, but also can mean inapplicable.
Reference: [43] <author> R. Wille. </author> <title> Restructuring lattice theory: An approach based on hierarchies on concepts. </title> <editor> In I. Rival, editor, </editor> <title> Ordered Sets. </title> <publisher> Reidel, </publisher> <address> Dordrecht-Boston, </address> <year> 1982. </year>
Reference-contexts: This subject has been 7 formally studied by Wille <ref> [43] </ref> and used for concept modeling. In previous section, we acknowledged Kent's study [20] on extending the rough set methodology to the area of formal concept analysis.
Reference: [44] <author> Y. .Y Yao and X. Li. </author> <title> Uncertainity reasoning with interval-set algebra. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Knowledge Discovery, </booktitle> <pages> pages 191-201, </pages> <address> Banff, Alberta, Canada, </address> <year> 1993. </year>
Reference-contexts: This subject is, however, is not novel to the rough set methodology because there have been numerous works on developing rough approximation methods based on different definitions positive 3 We assume that the reader is acquainted with the basic notions of the rough set theory [29]. 5 (and boundary) regions <ref> [7, 18, 44, 14] </ref>. For example, in the elementary set approximation of an unknown concept [7], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value.
Reference: [45] <author> Y. Y. Yao and K. M. Wong. </author> <title> A decision theoretic framework for approximating concepts. </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 37 </volume> <pages> 793-809, </pages> <year> 1992. </year>
Reference-contexts: Alternatively, another approach would be to shift the domain of the problem from algebraic space to the probabilistic space, if one can assign prior probabilistic measures to the definable sets <ref> [31, 45] </ref>. In rough set based classification, inconsistent rough classifiers (or decision algorithms) have not received as much attention as consistent rough classifiers. In the rough set literature, the terms 'inconsistent' and 'nondeterministic' decision algorithms (or rules) are used interchangeably [40, 28, 46], though they are different concepts.
Reference: [46] <author> R. Yasdi. </author> <title> Learning classification rules from database in the context of knowledge acquisition and representation. </title> <journal> IEEE Trans. on Knowl. and Data Eng, </journal> <volume> 3(3) </volume> <pages> 293-306, </pages> <year> 1991. </year>
Reference-contexts: In rough set based classification, inconsistent rough classifiers (or decision algorithms) have not received as much attention as consistent rough classifiers. In the rough set literature, the terms 'inconsistent' and 'nondeterministic' decision algorithms (or rules) are used interchangeably <ref> [40, 28, 46] </ref>, though they are different concepts. The 'inconsistency' is attributed to the result of a classification method while the 'nondetermism' is attributed to the interpretation of that result. As shown in [7], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterministically.
Reference: [47] <author> W. Ziarko. </author> <title> The discovery, analysis, and representation of data dependencies in databases. </title> <editor> In G. Piatetsky-Shapiro and W .J. Frawley, editors, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: A plausible remedy for this problem is to design an incremental method and separate the summary and the result of a method from one to another. For example, in the context of rough classification, the strength of a possible decision rule <ref> [47] </ref> is a part of the summary of the decision algorithm.
Reference: [48] <author> W. Ziarko and N. Shan. KDD-R: </author> <title> a comprehensive system for knowledge discovery in databases using rough sets. </title> <booktitle> In Proc. of The Int. Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 164-173, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: For example, in KDD-R system, the data preprocessing unit discretizes the numerical attributes either by using user-supplied discretization formula or by applying an automatic discretization algorithm <ref> [48] </ref>. Alternatively, horizontal reduction of a very large data set table may use a generalization hierarchy of attributes to merge identical tuples, after the substitution of an attribute value, by its higher level concept in the generalization hierarchy. <p> Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on offline data; KDD-R, an experimental open tool box <ref> [48] </ref>; LERS, a machine learning system from examples [12]; and DataLogic/R [41], a commercial product for database mining and decision support.
Reference: [49] <author> J. M. Zytkow and J. Baker. </author> <title> Interactive mining of regularities in databases. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 31-53. </pages> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Unfortunately, the database technology of today offers little functionality to explore such data. At the same time, knowledge discovery 1 techniques for intelligent data analysis are not yet mature for large data sets <ref> [23, 2, 49, 10, 25] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources. <p> A plausible approach to tackle ultra large data is to reduce the data set horizontally by merging identical tuples following the substitution of an attribute value by its higher level value in a generalization hierarchy of categorical attributes <ref> [13, 49, 37] </ref> or vertically by applying some feature selection methods [17, 3]. One could also use random sampling methods along with the horizontal/vertical pruning methods, but this method has not received much attention yet. 2.
References-found: 49

