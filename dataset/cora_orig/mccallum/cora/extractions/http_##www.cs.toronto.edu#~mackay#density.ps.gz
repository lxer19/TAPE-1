URL: http://www.cs.toronto.edu/~mackay/density.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@mrao.cam.ac.uk  
Title: Density Networks and their Application to Protein Modelling  
Author: David J.C. MacKay 
Address: Cambridge, CB3 0HE. U.K.  
Affiliation: Cavendish Laboratory,  
Abstract: I define a latent variable model in the form of a neural network for which only target outputs are specified; the inputs are unspecified. Although the inputs are missing, it is still possible to train this model by placing a simple probability distribution on the unknown inputs and maximizing the probability of the data given the parameters. The model can then discover for itself a description of the data in terms of an underlying latent variable space of lower dimensionality. I present preliminary results of the application of these models to protein data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Eddy, S. R., and Durbin, R., </author> <title> (1994) RNA sequence analysis using covariance models. NAR, </title> <publisher> in press. </publisher>
Reference-contexts: This model is not inherently capable of discovering long-range correlations, as Markov models, by definition, produce no correlations between the observables, given a hidden state sequence. The next-door neighbour of proteins, RNA, has been modelled with a `covariance model' capable of capturing correlations between base-pairs in anti-parallel RNA strands <ref> (Eddy and Durbin 1994) </ref>. The aim of the present work is to develop a model capable of discovering general correlations between multiple arbitrary columns in a protein family. E. Steeg (personal communication) has developed an efficient statistical test for discovering correlated groups of residues.
Reference: <author> Everitt, B. S. </author> <title> (1984) An Introduction to Latent Variable Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Hinton, G. E., and Zemel, R. S. </author> <year> (1994) </year> <month> Autoencoders, </month> <title> minimum description length and Helmholtz free energy. </title> <booktitle> In Advances in Neural Information Processing Systems 6 , ed. by J. </booktitle> <address> D. </address>
Reference-contexts: MacKay for discriminating correct from incorrect alignements of beta strands. The present work is not of sufficient numerical accuracy to achieve this, but possibly by introducing superior sampling methods in tandem with free energy minimization <ref> (Hinton and Zemel 1994) </ref>, these models may make a contribution to the protein folding problem.
Reference: <editor> Cowan, G. Tesauro, and J. Alspector, </editor> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Krogh, A., Brown, M., Mian, I. S., Sjolander, K., and Haussler, D. </author> <title> (1994) Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> Journal of Molecular Biology 235: </journal> <pages> 1501-1531. </pages>
Reference-contexts: The only probabilistic model that has so far been applied to protein families is a hidden Markov model <ref> (Krogh et al. 1994) </ref>. This model is not inherently capable of discovering long-range correlations, as Markov models, by definition, produce no correlations between the observables, given a hidden state sequence.
Reference: <author> Luttrell, S. P., </author> <title> (1994) The partitioned mixture distribution: an adaptive Bayesian network for low-level image processing. </title> <note> to appear. </note>
Reference: <author> MacKay, D. J. C. </author> <title> (1992) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: The data-dependent term here is a product of the normalizing constants of the level 1 inferences: P (Djw; H) = n=1 The evaluation of the evidence P (t (n) jw; H) for a particular n is a problem similar to the evaluation of the evidence for a supervised neural network <ref> (MacKay 1992) </ref>. There, the inputs x are given, and the parameters w are unknown; we obtain the evidence by integrating over w. In the present problem, on the other hand, the hidden vector x (n) is unknown, and the parameters w are conditionally fixed. <p> No explicit Gaussian approximation was made to the posterior distribution of w; rather, the hyperparameters fff c g were adapted during the optimization of the parameters w, using a cheap and cheerful method motivated by Gaussian approximations <ref> (MacKay 1992) </ref>, thus: ff c := f P i Here k c is the number of parameters in class c and f is a `fudge factor' incorporated to imitate the effect of integrating over w (set to a value between 0.1 and 1.0).
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, Section A. </title>
Reference-contexts: Simple linear models of this form in the statistics literature come under the label of `factor analysis'. In a `density network' <ref> (MacKay 1995) </ref> P (tjx; w) is defined by a more general non-linear parameterized mapping, and interesting priors on w may be used. The model The `latent inputs' of the model are a vector x indexed by h = 1 : : : H (`h' mnemonic for `hidden').
Reference: <author> MacKay, D. J. C., and Neal, R. M. </author> <title> (1994) Automatic relevance determination for neural networks. </title> <note> Technical Report in preparation, </note> <institution> Cambridge University. </institution>
Reference-contexts: Here I create a non-spherical prior on the parameters by using multiple undetermined regularization constants fff c g, each one associated with a class of weights (c.f. the automatic relevance determination model <ref> (MacKay and Neal 1994) </ref>).
Reference: <author> Nakai, K., Kidera, A., and Kanehisa, M. </author> <title> (1988) Cluster analysis of amino acid indices for prediction of protein structure and function. </title> <journal> Prot. Eng. </journal> <volume> 2: </volume> <pages> 93-100. </pages>
Reference-contexts: There are twenty different amino acids, and columns can often be characterized by a predominance of particular amino acids. Lists of marginal frequencies over amino acids in different structural contexts are given in <ref> (Nakai et al. 1988) </ref>. The development of models for protein families is useful for two reasons. The first is that a good model might be used to identify new members of an existing family, and discover new families too, in data produced by genome sequencing projects. <p> The hope was that meaningful physical properties such as this would be learned from the data. Analysis The parameters of a typical optimized density network are shown in figure 2. The parameter vectors were compared, column by column, with a large number of published amino acid indices <ref> (Nakai et al. 1988) </ref> to see if they corresponded to established physical properties of amino acids. Each index was normalized by subtracting the mean from each vector and scaling it to unit length.
Reference: <author> Neal, R. M. </author> <title> (1993) Bayesian learning via stochastic dynamics. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by C. L. Giles, S. J. Hanson, and J. D. </editor> <booktitle> Cowan, </booktitle> <pages> pp. 475-482, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More sophisticated methods using stochastic dynamics <ref> (Neal 1993) </ref> are currently under development. Alternative implementations An alternative approach to making such componential models scale well is the free energy minimization approximation of Hinton and Zemel (1994). <p> This algorithm could be converted to a correct `stochastic dynamics' Monte Carlo method <ref> (Neal 1993) </ref> by adding an appropriate amount of noise to gradient descent on w and setting f = 1. Toy data A toy data set was created imitating a protein family with four columns each containing one of five amino acids.
Reference: <author> I thank Radford Neal, Geoff Hinton, Sean Eddy, Richard Durbin, Tim Hubbard and Graeme Mitchison for invaluable discussions. </author> <title> I gratefully acknowledge the support of this work by the Royal Society Smithson Research Fellowship. </title>
References-found: 12

