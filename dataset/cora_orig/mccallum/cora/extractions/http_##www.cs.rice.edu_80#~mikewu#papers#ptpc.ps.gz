URL: http://www.cs.rice.edu:80/~mikewu/papers/ptpc.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~mikewu/papers.html
Root-URL: 
Email: mikewu@rice.edu  willy@rice.edu  
Title: Improving TLB Miss Handling with Page Table Pointer Caches  
Author: Michael Wu Willy Zwaenepoel 
Date: December 12, 1997  
Affiliation: Dept. of Electrical and Computer Engineering Rice University  Department of Computer Science Rice University  
Abstract: Page table pointer caches are a hardware supplement for TLBs that cache pointers to pages of page table entries rather than page table entries themselves. A PTPC traps and handles most TLB misses in hardware with low overhead (usually a single memory access). PTPC misses are filled in software, allowing for an easy hardware implementation, similar in structure to a TLB. Since each PTPC entry refers to an entire page of page table entries, even a small PTPC maps a large amount of address space and achieves a very high hit rate. The primary goal of a PTPC is to lower TLB miss handling penalties. The combination of a TLB with a small PTPC provides good performance even in situations where standard TLBs alone perform badly (large workloads or multimedia applications). The advantage of this design is that we can continue to use small fixed size pages with standard TLBs. Since PTPCs use traditional page table structures and page sizes, they are very simple to implement in hardware and require minimal operating system modifications. Our simulations show that the addition of a PTPC to a system with a TLB can reduce miss handling costs by nearly an order of magnitude. Small PTPCs are extremely effective and the combination of small to medium sized TLBs coupled with small PTPCs are an efficient alternative to large TLBs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Bala, M. Frans Kaashoek, and W. Weihl. </author> <title> Software prefetching and caching for translation looka-side buffers. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 243-253, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Systems with virtual caches are rare since the need to eliminate virtual synonyms in software complicates the operating system and virtual tags make shared memory coherence much more difficult and expensive. Bala et al. <ref> [1] </ref> have developed an interesting software handler that caches and prefetches PTEs and PTPs in software. They achieve significant reductions (almost 50%) in the amount of time spent handling page directory misses for a group of operating system intensive applications.
Reference: [2] <author> J. Bradley Chen. </author> <title> Memory behavior for an X11 windows system. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 189-200, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: However, programs with large working sets or applications that spend significant time communicating with other address spaces can spend as much as 50% of total execution time handling TLB misses <ref> [2, 6, 9] </ref>. In the future, TLB miss handling time can become more of a problem because both hardware and software miss handlers have costs that are increasing relative to the cost of executing an instruction.
Reference: [3] <author> J. Bradley Chen, A. Borg, and N. Jouppi. </author> <title> A simulation based study of TLB performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 114-123, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For many programs, a TLB provides a very high hit rate and address translation overhead represents an insignificant part of the overall execution time <ref> [3] </ref>. However, programs with large working sets or applications that spend significant time communicating with other address spaces can spend as much as 50% of total execution time handling TLB misses [2, 6, 9]. <p> Most recent work in TLB performance enhancement has revolved around the use of superpages to increase the reach of a basic TLB with some very simple extensions of the basic design and significant operating system support <ref> [3, 7, 9, 10] </ref>. There are several drawbacks to this approach. Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained. <p> Research in the area has supported the notion that extending the reach of a TLB is crucial to achieving high hit rates in the future, and that increasing the page size in some way has potential for reaching this goal <ref> [3, 11] </ref>. Since the number of TLB entries is somewhat fixed because of speed constraints, the most common approach to increase the reach of a TLB is to enhance each entry to support larger pages called superpages.
Reference: [4] <author> J. Huck and J. Hays. </author> <title> Architectural support for translation table management in large address space machines. </title> <booktitle> In SIGARCH93, </booktitle> <pages> pages 39-50, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The amount of memory required for virtual address translation is then proportional to the size of physical memory rather than virtual memory. Hashed page tables <ref> [4] </ref> improve on the basic inverse page table mechanism by using the inverse mapping as a convenient method of page table lookup rather than as a fixed page table structure. Like PTPCs, hashed page tables can greatly reduce the number of memory accesses needed to service a TLB miss. <p> Inverse page tables cannot have virtual address aliases. Hashed page tables support aliases, but with decreasing efficiency as the number of aliases increases. Also, some per page information such as dirty bits cannot be accurately maintained with aliased pages, requiring special operating system support. As suggested in <ref> [4] </ref>, a hashed data structure may be useful in speeding up software TLB miss handling with traditional page tables. 8 Conclusion This paper introduces the use of page table caches as an architectural extension of the MMU in order to reduce TLB miss handling times.
Reference: [5] <author> M. Johnson. </author> <title> Superscalar Processor Design. </title> <publisher> Pren-tice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Since the hardware can satisfy most TLB fills on the fly, the instruction stream never gets stalled to handle an interrupt. In modern superscalar processors with dynamic execution and non-blocking loads <ref> [5] </ref>, only the instruction that caused the TLB miss actually gets stalled. The cost of the PTPC lookup and associated PTE fetch from memory can be partially, and in some cases entirely, hidden by overlapping the operation with the execution of other instructions. In addition, there is no software overhead. <p> We expect the relative performance of PTPCs to increase when non-blocking load costs are modeled. We are currently modifying Mike Johnson's superscalar processor simulator ssim <ref> [5] </ref> to model PTPC costs in a dynamic instruction execution environment. In the near future, we will be investigating tracing alternatives that will allow us to simulate multi-process and operating system interaction behavior.
Reference: [6] <author> D. Nagle, R. Uhlig, and T. Stanley. </author> <title> Design tradeoffs for software-managed TLBs. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 27-38, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, programs with large working sets or applications that spend significant time communicating with other address spaces can spend as much as 50% of total execution time handling TLB misses <ref> [2, 6, 9] </ref>. In the future, TLB miss handling time can become more of a problem because both hardware and software miss handlers have costs that are increasing relative to the cost of executing an instruction.
Reference: [7] <author> T. Romer, W. Ohlrich, A. Karlin, and B. Bershad. </author> <title> Reducing tlb and memory overhead using online superpage promotion. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 176-187, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Most recent work in TLB performance enhancement has revolved around the use of superpages to increase the reach of a basic TLB with some very simple extensions of the basic design and significant operating system support <ref> [3, 7, 9, 10] </ref>. There are several drawbacks to this approach. Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained. <p> Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained. Attempts to remedy the situation using a combination of small pages and larger pages when possible have shown that it is possible, but requires extensive operating system modifications <ref> [7, 9] </ref>. Page table pointer caches cache pointers to pages of page table entries rather than page table entries as a TLB does. <p> This relatively simple concept of "page promotion" greatly complicates several resource management decisions and kernel data structures that have tradi tionally been simple because memory cannot continue to be treated as a pool of homogeneous pages. <ref> [7, 9, 11] </ref>. These areas include memory allocation policies, paging policies, and protection guarantees made by the virtual memory system. Even grouping and ungrouping policies, the core of the implementation, are difficult and potentially costly.
Reference: [8] <author> M. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Tpca is a hand-coded version of the TPC-A database benchmark which simulates accesses to the main data structures in a scaled down (16 Mbyte) subset of the database. Full traces are gathered from all programs using the SGI/MIPS program pixie <ref> [8] </ref>. Pixie is a post-compilation tool that annotates each load and store with code that records their reference addresses at run time. Traces are consumed on-the-fly by our simulator. It emulates a fully associative 64 entry TLB with page table pointer caches varying in size from 2 to 16 entries.
Reference: [9] <author> M. Talluri and M. Hill. </author> <title> Surpassing the TLB performance of superpages with less operating system support. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 171-182, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: However, programs with large working sets or applications that spend significant time communicating with other address spaces can spend as much as 50% of total execution time handling TLB misses <ref> [2, 6, 9] </ref>. In the future, TLB miss handling time can become more of a problem because both hardware and software miss handlers have costs that are increasing relative to the cost of executing an instruction. <p> Most recent work in TLB performance enhancement has revolved around the use of superpages to increase the reach of a basic TLB with some very simple extensions of the basic design and significant operating system support <ref> [3, 7, 9, 10] </ref>. There are several drawbacks to this approach. Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained. <p> Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained. Attempts to remedy the situation using a combination of small pages and larger pages when possible have shown that it is possible, but requires extensive operating system modifications <ref> [7, 9] </ref>. Page table pointer caches cache pointers to pages of page table entries rather than page table entries as a TLB does. <p> Operating system support for concurrent variable sized pages attempts to overcome the limitations of fixed size superpages by allowing several small pages to be grouped into a single larger superpage when possible, but adds significant complexity with questionable benefit <ref> [9] </ref>. This relatively simple concept of "page promotion" greatly complicates several resource management decisions and kernel data structures that have tradi tionally been simple because memory cannot continue to be treated as a pool of homogeneous pages. [7, 9, 11]. <p> This relatively simple concept of "page promotion" greatly complicates several resource management decisions and kernel data structures that have tradi tionally been simple because memory cannot continue to be treated as a pool of homogeneous pages. <ref> [7, 9, 11] </ref>. These areas include memory allocation policies, paging policies, and protection guarantees made by the virtual memory system. Even grouping and ungrouping policies, the core of the implementation, are difficult and potentially costly. <p> In their system (and others based on MIPS processors), the lower 8 entries of the MIPS R3000 TLB are used to cache certain types of PTPs. It does not fill the TLB hardware, it caches to reduce cascading TLB faults in the software miss handler. Sub-block TLBs <ref> [9] </ref> have been suggested as an enhancement to TLB design that shares the comparators used for address matching among several TLB entries, similar to what is done with cache blocks.
Reference: [10] <author> M. Talluri, M. Hill, and Y. Khalidi. </author> <title> A new page table for 64-bit address spaces. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: Most recent work in TLB performance enhancement has revolved around the use of superpages to increase the reach of a basic TLB with some very simple extensions of the basic design and significant operating system support <ref> [3, 7, 9, 10] </ref>. There are several drawbacks to this approach. Larger pages reduce efficiency because they increase internal fragmentation and makes the unit of protection and sharing more coarse-grained.
Reference: [11] <author> M. Talluri, S. Kong, M. Hill, and D. Patterson. </author> <title> Tradeoffs in supporting two page sizes. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 415-424, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Research in the area has supported the notion that extending the reach of a TLB is crucial to achieving high hit rates in the future, and that increasing the page size in some way has potential for reaching this goal <ref> [3, 11] </ref>. Since the number of TLB entries is somewhat fixed because of speed constraints, the most common approach to increase the reach of a TLB is to enhance each entry to support larger pages called superpages. <p> This relatively simple concept of "page promotion" greatly complicates several resource management decisions and kernel data structures that have tradi tionally been simple because memory cannot continue to be treated as a pool of homogeneous pages. <ref> [7, 9, 11] </ref>. These areas include memory allocation policies, paging policies, and protection guarantees made by the virtual memory system. Even grouping and ungrouping policies, the core of the implementation, are difficult and potentially costly.
Reference: [12] <author> R. Uhlig, D. Nagle, T. Mudge, and S. Sechrest. Tapeworm ii, </author> <title> a new method for measuring OS effects on memory architecture performance. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 132-144, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: We are currently modifying Mike Johnson's superscalar processor simulator ssim [5] to model PTPC costs in a dynamic instruction execution environment. In the near future, we will be investigating tracing alternatives that will allow us to simulate multi-process and operating system interaction behavior. Among the alternatives are trap-driven simulation <ref> [12] </ref> and other tracing tools such as ATOM running on Digital Alpha workstations.
Reference: [13] <author> D. A. Wood, S. J. Eggers, G. Gibson, M. Hill, J. Pendleton, S.A. Ritchie, R. H. Katz, and D. A. Patterson. </author> <title> An in-cache address translation mechanism. </title> <booktitle> In IEEE Computer Society, </booktitle> <pages> pages 358-365, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: The fixed function nature of the registers also limit their effectiveness. The SPARC processors that incorporate this feature use virtually tagged caches and only need to reference the TLB on a miss, reducing the need to have a fast TLB. Wood et al. <ref> [13] </ref> also promote the use of virtually addressed caches as a way to ease TLB performance requirements. Systems with virtual caches are rare since the need to eliminate virtual synonyms in software complicates the operating system and virtual tags make shared memory coherence much more difficult and expensive.
References-found: 13

