URL: ftp://ftp.imag.fr/pub/CALCUL_FORMEL/RAPPORT/1998/RR992.ps.gz
Refering-URL: http://www-lmc.imag.fr/CF/publi.html
Root-URL: http://www.imag.fr
Title: RAPPORT DE RECHERCHE Processor efficient parallel solution of linear systems of equations  
Author: G. Villard 
Note: N o 992 I-M Fevrier 1998  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> S.J. Berkowitz, </author> <title> On computing the determinant in small parallel time usnig a small number of processors, </title> <journal> Inform. Process. Letters. </journal> <volume> 18 (1984), </volume> <pages> 147-150. </pages>
Reference-contexts: Over a field of any characteristic, the exceeding factor is n <ref> [1, 4] </ref>. These results hold if A is invertible. When A is singular, to test whether the system is consistent and to possibly compute a solution over any field leads to the even greater exceeding factor n 4 [2]- p333.
Reference: 2. <author> D. Bini and V. Pan, </author> <title> Polynomial and matrix computations, </title> <publisher> Birkhauser, </publisher> <year> 1994. </year>
Reference-contexts: Over a field of any characteristic, the exceeding factor is n [1, 4]. These results hold if A is invertible. When A is singular, to test whether the system is consistent and to possibly compute a solution over any field leads to the even greater exceeding factor n 4 <ref> [2] </ref>- p333. Over an abstract field, Kaltofen and Pan have discovered the only known approach to handle the problem of processor efficiency for linear system solution. This approach leads to the following class of Las Vegas randomized algorithms.
Reference: 3. <author> A. Borodin and I. Munro, </author> <title> The computational complexity of algebraic and numeric problems, </title> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1975. </year>
Reference: 4. <author> Chistov, A.L., </author> <title> Fast parallel computation of the rank of matrices over a field of arbitrary characteristic, </title> <booktitle> Proc. </booktitle> <address> FCT'95, </address> <publisher> LNCS 199, Springer Verlag, </publisher> <year> 1985, </year> <pages> pp. 63-69. </pages>
Reference-contexts: Over a field of any characteristic, the exceeding factor is n <ref> [1, 4] </ref>. These results hold if A is invertible. When A is singular, to test whether the system is consistent and to possibly compute a solution over any field leads to the even greater exceeding factor n 4 [2]- p333.
Reference: 5. <author> L. Csanky, </author> <title> Fast parallel matrix inversion algorithms, </title> <journal> SIAM J. Comput. </journal> <volume> 5 (1976), no. 4, </volume> <pages> 618-623. </pages>
Reference-contexts: Each time unit in the algorithms represents an arithmetic operation in K. If K is of characteristic zero or greater than n, the processor count measures of the known deterministic algorithms to solve Ax = b, exceed by a factor almost p n the processor complexity of matrix multiplication <ref> [5, 16, 8] </ref>. Over a field of any characteristic, the exceeding factor is n [1, 4]. These results hold if A is invertible.
Reference: 6. <author> W. Eberly, </author> <title> Processor-efficient parallel matrix inversion over abstract fields: two extensions, </title> <booktitle> Second International Symposium on Parallel Symbolic Computation (PASCO'97), </booktitle> <address> Maui, Hawaii, USA, </address> <month> Jul </month> <year> 1997, </year> <pages> pp. 38-45. </pages>
Reference-contexts: Over any field if A is invertible, the time increases to O (log 3 n) using the algorithm in [10] combined with the ones in [11, 15]; the processor complexity remains in O (M (n)) using the improvement of Eberly <ref> [6] </ref>. For A singular, the time used increases to O (log 4 n) with the same processor complexity [10, 11, 15]. In the case of fields of characteristic zero, a direct application of known results allows us to present a deterministic version of above processor efficient algorithms. 2. <p> It is a classical fact that as called by Wilkinson [18] the algorithm may seriously break down. The randomizations proposed in [17] and used, as seen above, for parallelization <ref> [10, 11, 6] </ref>, precisely avoid this problem especially over finite fields. LMC/IMAG - February 5, 1998. Key words.
Reference: 7. <author> W. Eberly and E. Kaltofen, </author> <title> On randomized Lanczos algorithms, </title> <booktitle> International Symposium on Symbolic and Algebraic Computation, </booktitle> <address> Maui, Hawaii, USA, </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1997, </year> <pages> pp. 176-183. </pages>
Reference-contexts: Since the algorithm relies heavily on this latter property, the question of deterministic processor efficiency for any field is still open. For a randomized version of the symmetric Lanczos method over finite fields, the reader may refer to <ref> [7] </ref>. In Parallel solution of a linear system 7 the same way, the solution of the system is computed using the minimal polynomial of a particular vector, thus even in characteristic zero, we do not solve the problem of matrix inversion.
Reference: 8. <author> Z. Galil and V.Y. Pan, </author> <title> Parallel evaluation of the determinant and of the inverse of a matrix, </title> <journal> Inform. Proc. Lett. </journal> <volume> 30 (1989), </volume> <pages> 41-45. </pages>
Reference-contexts: Each time unit in the algorithms represents an arithmetic operation in K. If K is of characteristic zero or greater than n, the processor count measures of the known deterministic algorithms to solve Ax = b, exceed by a factor almost p n the processor complexity of matrix multiplication <ref> [5, 16, 8] </ref>. Over a field of any characteristic, the exceeding factor is n [1, 4]. These results hold if A is invertible.
Reference: 9. <author> E. Kaltofen and V. Pan, </author> <title> Processor efficient parallel solution of linear systems over an abstract field, </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architecture, </booktitle> <address> ACM-Press, </address> <year> 1991. </year> <title> 10. , Processor efficient parallel solution of linear systems II: the general case, </title> <booktitle> Proc. 33rd IEEE Symp. Foundations Of Computer Science, </booktitle> <address> Pittsburg, USA, </address> <year> 1992. </year> <title> 11. , Parallel solution of Toeplitz and Toeplitz-like linear systems over fields of small positive characteristic, </title> <booktitle> First International Symposium on Parallel Symbolic Computation (PASCO'94), Lecture Notes Series in Computing - Vol. 5, World Scientific, </booktitle> <year> 1994, </year> <pages> pp. 225-233. </pages>
Reference-contexts: This approach leads to the following class of Las Vegas randomized algorithms. One can solve Ax = b, for any n fi n matrix A, in randomized time O (log 2 n) using O (M (n)) processors if the characteristic of K is zero or greater than n <ref> [9, 10] </ref>. Over any field if A is invertible, the time increases to O (log 3 n) using the algorithm in [10] combined with the ones in [11, 15]; the processor complexity remains in O (M (n)) using the improvement of Eberly [6].
Reference: 12. <author> R.M. Karp and V. Ramachandran, </author> <title> Parallel algorithms for shared-memory machines, </title> <booktitle> Handbook of Theoretical Computer Science Vol. </booktitle> <editor> A (J. van Leuwen, ed.), </editor> <publisher> North-Holland, </publisher> <year> 1990, </year> <pages> pp. 869-941. </pages>
Reference-contexts: For an n-dimensional input, an algorithm that has a running time in log O (1) n is called processor efficient if it supports the bound log O (1) n on the inefficiency ratio <ref> [12] </ref>. We suppose that the product of two n fi n matrices over K can be computed in O (log n) parallel time using M (n) processors. Each time unit in the algorithms represents an arithmetic operation in K.
Reference: 13. <author> C. </author> <title> Lanczos, Solutions of systems of linear equations by minimized iterations, </title> <institution> J. Res. Bur. Standards, Sect. </institution> <address> B 49 (1952), </address> <pages> 33-53. </pages>
Reference-contexts: The algorithm For a matrix A or a vector over a field K of characteristic zero, A fl will denote the Hermitian transpose. Above processor efficient algorithms are nontrivial parallelizations of the Wiedemann sequential method [17]. They are thus strongly related to Lanczos bi-orthogonalization <ref> [13] </ref>. The solution of Ax = b is computed in the Krylov subspace spanfb; Ab; A 2 b; : : : g, using an auxiliary vector u and the associated Krylov subspace spanfu fl ; u fl A; u fl A 2 ; : : : g. <p> LMC/IMAG - February 5, 1998. Key words. Fast parallel algorithm, processor-efficient algorithm, linear system solution, Krylov subspace, Toeplitz matrix. 5 6 Gilles Villard Our observation is simply that since as well known the Hermitian Lanczos method <ref> [13] </ref>, i.e. when A = A fl and with u = b, does not seriously break down, then the corresponding parallelization proposed in [10] must be deterministic. The algorithm follows immediately. 2.1. We begin with A invertible.
Reference: 14. <author> V. Pan, </author> <title> Parametrization of Newton's iteration for computations with structured matrices and applications, </title> <booktitle> Computers and Mathematics (with applications) 24 (1992), </booktitle> <volume> no. 3, </volume> <pages> 61-75. </pages> <month> 15. </month> , <title> Parallel computation of polynomial GCD and some related parallel computations over abstract fields, </title> <booktitle> Theoretical Computer Science (1996), </booktitle> <volume> no. 162, </volume> <pages> 173-223. </pages>
Reference-contexts: We know that computing the rank of the Hankel matrix H and solving the corresponding system can be done in time O (log 2 n) with O (n 2 (log log n)= log n) processors <ref> [14, 15] </ref>. From there, x is computed within the same bounds. Hence the whole algorithm takes O (log 2 n) time using O (M (n)) processors. 2.2.
Reference: 16. <author> F.P. Preparata and D.V. Sarwate, </author> <title> An improved parallel processor bound in fast matrix inversion, </title> <journal> Inform. Proc. Lett. </journal> <volume> 7 (1978), </volume> <pages> 148-150. </pages>
Reference-contexts: Each time unit in the algorithms represents an arithmetic operation in K. If K is of characteristic zero or greater than n, the processor count measures of the known deterministic algorithms to solve Ax = b, exceed by a factor almost p n the processor complexity of matrix multiplication <ref> [5, 16, 8] </ref>. Over a field of any characteristic, the exceeding factor is n [1, 4]. These results hold if A is invertible.
Reference: 17. <author> D. </author> <title> Wiedemann, Solving sparse linear equations over finite fields, </title> <journal> IEEE Transf. Inform. Theory 32 (1986), </journal> <pages> 54-62. </pages>
Reference-contexts: The algorithm For a matrix A or a vector over a field K of characteristic zero, A fl will denote the Hermitian transpose. Above processor efficient algorithms are nontrivial parallelizations of the Wiedemann sequential method <ref> [17] </ref>. They are thus strongly related to Lanczos bi-orthogonalization [13]. <p> It is a classical fact that as called by Wilkinson [18] the algorithm may seriously break down. The randomizations proposed in <ref> [17] </ref> and used, as seen above, for parallelization [10, 11, 6], precisely avoid this problem especially over finite fields. LMC/IMAG - February 5, 1998. Key words. <p> The algorithm follows immediately. 2.1. We begin with A invertible. Forming if necessary the system AA fl y = b, we will work with a Hermitian matrix. Following the parallelization of Kaltofen and Pan [10] of the Wiedemann method <ref> [17] </ref>, the parallel algorithm is: Algorithm 1. Input: A invertible in M n;n (K); b 6= 0 a n-dimensional vector. Compute C = AA fl .
Reference: 18. <author> J.H Wilkinson, </author> <title> The algebraic eigenvalue problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <address> 1965. LMC-IMAG, B.P. 53 F38041 Grenoble cedex 9, E-mail : Gilles.Villard@imag.fr. </address>
Reference-contexts: It is a classical fact that as called by Wilkinson <ref> [18] </ref> the algorithm may seriously break down. The randomizations proposed in [17] and used, as seen above, for parallelization [10, 11, 6], precisely avoid this problem especially over finite fields. LMC/IMAG - February 5, 1998. Key words.
References-found: 15

