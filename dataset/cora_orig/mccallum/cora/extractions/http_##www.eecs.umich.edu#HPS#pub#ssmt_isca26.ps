URL: http://www.eecs.umich.edu/HPS/pub/ssmt_isca26.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_micro.html
Root-URL: http://www.eecs.umich.edu
Email: swkim@mipos2.intel.com  pattg@eecs.umich.edu  
Title: Simultaneous Subordinate Microthreading (SSMT)  
Author: Robert S. Chappelly Jared Starky Sangwook P. Kimyz Steven K. Reinhardty Yale N. Patty frobc, starkj, stever, 
Address: Santa Clara, CA 95052 Ann Arbor, Michigan 48109-2122  
Affiliation: yEECS Department (ACAL) zIntel Corporation The University of Michigan  
Abstract: Current work in Simultaneous Multithreading provides little benefit to programs that aren't partitioned into threads. We propose Simultaneous Subordinate Mi-crothreading (SSMT) to correct this by spawning subordinate threads that perform optimizations on behalf of the single primary thread. These threads, written in microcode, are issued and executed concurrently with the primary thread. They directly manipulate the microarchitecture to improve the primary thread's branch prediction accuracy, cache hit rate, and prefetch effectiveness. All contribute to the performance of the primary thread. This paper introduces SSMT and discusses its potential to increase performance. We illustrate its usefulness with an SSMT machine that executes subordinate microthreads to improve the branch prediction of the primary thread. We show simulation results for the SPECint95 benchmarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. August, D. Connors, J. Gyllenhaal, and W. Hwu. </author> <title> Support for compiler-synthesized dynamic branch prediction strategies: Rationale and initial results. </title> <booktitle> In Proceedings of the Third IEEE International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 8493, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: This section concentrates on three areas that have potential for improvement with SSMT: branch prediction, prefetching, and cache management. 3.3.1. Branch Prediction. Studies have examined use of the compiler to synthesize dynamic branch prediction <ref> [11, 1] </ref>. By analyzing program semantics, the compiler can insert instructions in the program to communicate with the processor's branch prediction hardware. In many cases, this can lead to very accurate prediction for certain types of branches.
Reference: [2] <author> D. Bernstein, D. Cohen, A. Freund, and D. Maydan. </author> <title> Compiler techniques for data prefetching on the powerpc. </title> <booktitle> In Proceedings of the 1995 ACM/IEEE Conference on Parallel Architectures and Compilation Techniques, </booktitle> <year> 1995. </year>
Reference-contexts: One could easily envision combining multiple branch prediction enhancements. As mentioned above, any combination of mi-crothread routines can be selected to maximize performance of the primary thread. 3.3.2. Prefetching. Prefetching is an important mechanism for improving memory system performance. Many software-based and hardware-based mechanisms have been proposed <ref> [9, 2, 3, 10] </ref>. An SSMT machine provides opportunity for creating extremely sophisticated prefetches. Most existing ISAs that support prefetching provide an instruction, such as Alpha's FETCH, to load a single cache line into the first-level cache.
Reference: [3] <author> M. Charney and T. Puzak. </author> <title> Prefetching and memory system behavior of the spec95 benchmark suite. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 41(3):265286, </volume> <month> May </month> <year> 1997. </year>
Reference-contexts: One could easily envision combining multiple branch prediction enhancements. As mentioned above, any combination of mi-crothread routines can be selected to maximize performance of the primary thread. 3.3.2. Prefetching. Prefetching is an important mechanism for improving memory system performance. Many software-based and hardware-based mechanisms have been proposed <ref> [9, 2, 3, 10] </ref>. An SSMT machine provides opportunity for creating extremely sophisticated prefetches. Most existing ISAs that support prefetching provide an instruction, such as Alpha's FETCH, to load a single cache line into the first-level cache. <p> As with all microthread optimizations, prefetch-ing routines can be processed concurrently with instructions from the primary thread. An SSMT machine could also implement many existing hardware prefetching algorithms by using event spawns. For example, shadow-directory prefetching <ref> [3] </ref> operates by tracking cache miss patterns and using this information to later prefetch chains of cache lines. Using a microthread spawned by cache miss events, a more sophisticated version of this general algorithm could be implemented without any additional hardware.
Reference: [4] <author> J. C. Dehnert, P. Y. T. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the Cydra 5. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2638, </pages> <year> 1989. </year>
Reference-contexts: The number of physical registers may also need to be increased to maintain the same renaming capability. Available cache memory must be tagged and shared by the active threads. 2.2. Multiple-Path Mechanisms Mechanisms for multiple-path execution have been proposed <ref> [19, 4, 20] </ref>. These implementations can vary widely, but the fundamental approach is the same. Branch mispre-dictions are a major performance limitation. Rather than predicting all of the conditional branches in a program, a multiple-path machine issues instructions from both taken and not-taken paths.
Reference: [5] <author> M. Evers, P.-Y. Chang, and Y. N. Patt. </author> <title> Using hybrid branch predictors to improve branch prediction accuracy in the presence of context switches. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 3 11, </pages> <year> 1996. </year>
Reference-contexts: Most current generation processors devote a significant portion of hardware to implement sophisticated branch prediction algorithms. However, even for the most advanced prediction schemes, such as the multi-hybrid <ref> [5] </ref> and the variable length path-based predictor [16], a significant fraction of performance is lost due to branch mispredictions, as was shown in Figure 1. In this section, we present a branch prediction mechanism that is enhanced through the use of a microthread-based branch predictor.
Reference: [6] <author> M. Evers, S. J. Patel, R. S. Chappell, and Y. N. Patt. </author> <title> An analysis of correlation and predictability: What makes two-level branch predictors work. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 52 61, </pages> <year> 1998. </year>
Reference-contexts: All benchmarks were compiled for the Alpha ISA using the Digital compiler with optimizations O2 and Olimit 3000. All benchmarks were run to completion. 5.2. The Microthread Predictor Our baseline hardware predictor is a 16 KB gshare predictor. It accurately predicts branches that exhibit strong global correlation <ref> [6] </ref>. A weakness of this predictor is its inability to predict branches that only exhibit self (or per-address) correlation. To compensate for this weakness, our SSMT microthread routine implements a PAg [21] predictor.
Reference: [7] <author> H. Hirata, K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and T. Nishizwa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Section 5 provides simulation results for our branch prediction mechanism. Section 6 provides conclusions. 2. Previous Work Several studies have examined the notion of increasing machine performance by supplying additional concurrent work to the execution core. Two approaches are multi-threading and multiple-path execution. 2.1. Multithreading Mechanisms A multithreaded machine <ref> [15, 13, 7, 17] </ref> has the ability to process instructions from several different threads without performing context switches. The processor maintains a list of active threads and dynamically decides which thread's instructions to issue into the machine.
Reference: [8] <author> M. Horowitz, M. Martonosi, T. C. Mowry, and M. D. Smith. </author> <title> Informing memory operations: Providing memory performance feedback in modern processors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: These feedback mi-crothreads could alter the prefetching algorithm to prioritize useful prefetches, filter out useless prefetches, and perhaps reschedule late prefetches. This idea is similar in approach to that of informing memory operations <ref> [8] </ref>, but has all of the advantages associated with using microthreads. 5 3.3.3. Cache Management. Caches use replacements policies to decide if a line should be replaced, and if so, which line should be replaced.
Reference: [9] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364 373, </pages> <year> 1990. </year>
Reference-contexts: One could easily envision combining multiple branch prediction enhancements. As mentioned above, any combination of mi-crothread routines can be selected to maximize performance of the primary thread. 3.3.2. Prefetching. Prefetching is an important mechanism for improving memory system performance. Many software-based and hardware-based mechanisms have been proposed <ref> [9, 2, 3, 10] </ref>. An SSMT machine provides opportunity for creating extremely sophisticated prefetches. Most existing ISAs that support prefetching provide an instruction, such as Alpha's FETCH, to load a single cache line into the first-level cache.
Reference: [10] <author> C.-K. Luk and T. C. Mowry. </author> <title> Cooperative prefetching: Compiler and hardware support for effective instruction prefetch-ing in modern processors. </title> <booktitle> In Proceedings of the 31th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <year> 1998. </year>
Reference-contexts: One could easily envision combining multiple branch prediction enhancements. As mentioned above, any combination of mi-crothread routines can be selected to maximize performance of the primary thread. 3.3.2. Prefetching. Prefetching is an important mechanism for improving memory system performance. Many software-based and hardware-based mechanisms have been proposed <ref> [9, 2, 3, 10] </ref>. An SSMT machine provides opportunity for creating extremely sophisticated prefetches. Most existing ISAs that support prefetching provide an instruction, such as Alpha's FETCH, to load a single cache line into the first-level cache.
Reference: [11] <author> S. Mahlke and B. Natarajan. </author> <title> Compiler synthesized dynamic branch prediciton. </title> <booktitle> In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This section concentrates on three areas that have potential for improvement with SSMT: branch prediction, prefetching, and cache management. 3.3.1. Branch Prediction. Studies have examined use of the compiler to synthesize dynamic branch prediction <ref> [11, 1] </ref>. By analyzing program semantics, the compiler can insert instructions in the program to communicate with the processor's branch prediction hardware. In many cases, this can lead to very accurate prediction for certain types of branches.
Reference: [12] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: However, hardware predictors generate predictions faster than microthread predictors and are very accurate for some branches. Hence, we use a complex microthread-based predictor only for those branches that are likely to be poorly predicted by the hardware predictor. Conceptually, this is the same idea that makes a hybrid predictor <ref> [12] </ref> more accurate than any of its component predictors. In our case, the hybrid we are building includes two components: the hardware predictor of the processor, and the microthread-based predictor supplied via the SSMT mechanism. We use profiling to identify conditional branches suited to microthread prediction. <p> The window size was 512 instructions. The instruction and data caches were 64 KBytes (KB), direct mapped, with a 64 byte line size, and a 10 cycle miss penalty. We modeled two different conditional branch predictors. The first predictor was a 16 KB gshare <ref> [12] </ref> predictor. The second predictor was a hybrid consisting of an 8 KB variable-length path (VLP) predictor [16] and an 8 KB SAg predictor [21]. The minimum branch misprediction penalty was 7 cycles. A processor with a poor fetch mechanism will underuti-lize its execution resources.
Reference: [13] <author> G. M. Papadopoulos and K. R. Traub. </author> <title> Multithreading: A revisionist view of dataflow architectures. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 342351, </pages> <year> 1991. </year>
Reference-contexts: Section 5 provides simulation results for our branch prediction mechanism. Section 6 provides conclusions. 2. Previous Work Several studies have examined the notion of increasing machine performance by supplying additional concurrent work to the execution core. Two approaches are multi-threading and multiple-path execution. 2.1. Multithreading Mechanisms A multithreaded machine <ref> [15, 13, 7, 17] </ref> has the ability to process instructions from several different threads without performing context switches. The processor maintains a list of active threads and dynamically decides which thread's instructions to issue into the machine.
Reference: [14] <author> Y. N. Patt. </author> <title> Keynote Address, </title> <booktitle> Workshop on Simultaneous Multithreading (HPCA-4), </booktitle> <year> 1998. </year>
Reference-contexts: A subsequent improvement restricted fetch to one thread per cycle, but still allowed concurrent execution of instructions from multiple threads [17]. This taxonomy was first noted in <ref> [14] </ref>. 2 MicroRAM MicroRAM Sequencer Instruction Cache / Trace Cache Fetch Buffer Microthread Buffer Renaming Logic Decode Logic / Spawn Instruction Info GPR RAT Micro RATs context FU FU FU FU FU FU FU FU Event Spawn Info Reservation Stations machine. in a microcode RAM (the microRAM).
Reference: [15] <author> B. J. Smith. </author> <title> A pipelined shared resource mimd computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <year> 1978. </year>
Reference-contexts: Section 5 provides simulation results for our branch prediction mechanism. Section 6 provides conclusions. 2. Previous Work Several studies have examined the notion of increasing machine performance by supplying additional concurrent work to the execution core. Two approaches are multi-threading and multiple-path execution. 2.1. Multithreading Mechanisms A multithreaded machine <ref> [15, 13, 7, 17] </ref> has the ability to process instructions from several different threads without performing context switches. The processor maintains a list of active threads and dynamically decides which thread's instructions to issue into the machine. <p> These additional threads are supplied in the form of microcode, and we refer to them as microthreads. routines are code sequences written in an internal instruction format of the processor. They can be stored on-chip 1 Multithreading originally fetched and executed instructions in-order from multiple threads <ref> [15] </ref>. Simultaneous multithreading first meant fetching instructions from more than one active thread in a single cycle and executing them out-of-order using the same core [18]. A subsequent improvement restricted fetch to one thread per cycle, but still allowed concurrent execution of instructions from multiple threads [17].
Reference: [16] <author> J. Stark, M. Evers, and Y. N. Patt. </author> <title> Variable length path branch predcition. </title> <booktitle> In Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 170 179, </pages> <year> 1998. </year>
Reference-contexts: Most current generation processors devote a significant portion of hardware to implement sophisticated branch prediction algorithms. However, even for the most advanced prediction schemes, such as the multi-hybrid [5] and the variable length path-based predictor <ref> [16] </ref>, a significant fraction of performance is lost due to branch mispredictions, as was shown in Figure 1. In this section, we present a branch prediction mechanism that is enhanced through the use of a microthread-based branch predictor. This example demonstrates the potential usefulness of SSMT. <p> We modeled two different conditional branch predictors. The first predictor was a 16 KB gshare [12] predictor. The second predictor was a hybrid consisting of an 8 KB variable-length path (VLP) predictor <ref> [16] </ref> and an 8 KB SAg predictor [21]. The minimum branch misprediction penalty was 7 cycles. A processor with a poor fetch mechanism will underuti-lize its execution resources. In such an environment, mi-crothread instructions would rarely compete with primary thread instructions for execution resources.
Reference: [17] <author> D. M. Tullsen, S. J. Eggers, J. S. Emer, and H. M. Levy. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191202, </pages> <year> 1996. </year>
Reference-contexts: Section 5 provides simulation results for our branch prediction mechanism. Section 6 provides conclusions. 2. Previous Work Several studies have examined the notion of increasing machine performance by supplying additional concurrent work to the execution core. Two approaches are multi-threading and multiple-path execution. 2.1. Multithreading Mechanisms A multithreaded machine <ref> [15, 13, 7, 17] </ref> has the ability to process instructions from several different threads without performing context switches. The processor maintains a list of active threads and dynamically decides which thread's instructions to issue into the machine. <p> Simultaneous multithreading first meant fetching instructions from more than one active thread in a single cycle and executing them out-of-order using the same core [18]. A subsequent improvement restricted fetch to one thread per cycle, but still allowed concurrent execution of instructions from multiple threads <ref> [17] </ref>.
Reference: [18] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: They can be stored on-chip 1 Multithreading originally fetched and executed instructions in-order from multiple threads [15]. Simultaneous multithreading first meant fetching instructions from more than one active thread in a single cycle and executing them out-of-order using the same core <ref> [18] </ref>. A subsequent improvement restricted fetch to one thread per cycle, but still allowed concurrent execution of instructions from multiple threads [17].
Reference: [19] <author> A. Uht and V. Sindagi. </author> <title> Disjoint eager execution: An optimal form of speculative execution. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 313325, </pages> <year> 1995. </year>
Reference-contexts: The number of physical registers may also need to be increased to maintain the same renaming capability. Available cache memory must be tagged and shared by the active threads. 2.2. Multiple-Path Mechanisms Mechanisms for multiple-path execution have been proposed <ref> [19, 4, 20] </ref>. These implementations can vary widely, but the fundamental approach is the same. Branch mispre-dictions are a major performance limitation. Rather than predicting all of the conditional branches in a program, a multiple-path machine issues instructions from both taken and not-taken paths.
Reference: [20] <author> S. Wallace, B. Calder, and D. Tullsen. </author> <title> Threaded multiple path execution. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: The number of physical registers may also need to be increased to maintain the same renaming capability. Available cache memory must be tagged and shared by the active threads. 2.2. Multiple-Path Mechanisms Mechanisms for multiple-path execution have been proposed <ref> [19, 4, 20] </ref>. These implementations can vary widely, but the fundamental approach is the same. Branch mispre-dictions are a major performance limitation. Rather than predicting all of the conditional branches in a program, a multiple-path machine issues instructions from both taken and not-taken paths.
Reference: [21] <author> T.-Y. Yeh and Y. N. Patt. </author> <title> Alternative implementations of two-level adaptive branch prediction. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124134, </pages> <year> 1992. </year> <month> 10 </month>
Reference-contexts: We modeled two different conditional branch predictors. The first predictor was a 16 KB gshare [12] predictor. The second predictor was a hybrid consisting of an 8 KB variable-length path (VLP) predictor [16] and an 8 KB SAg predictor <ref> [21] </ref>. The minimum branch misprediction penalty was 7 cycles. A processor with a poor fetch mechanism will underuti-lize its execution resources. In such an environment, mi-crothread instructions would rarely compete with primary thread instructions for execution resources. <p> It accurately predicts branches that exhibit strong global correlation [6]. A weakness of this predictor is its inability to predict branches that only exhibit self (or per-address) correlation. To compensate for this weakness, our SSMT microthread routine implements a PAg <ref> [21] </ref> predictor. In addition to providing more accurate predictions for some branches, PAg is a simple predictor well-suited to mi-crothread implementation. A PAg predictor goes through two steps to generate a prediction. The first step is a lookup into the Branch History Table (BHT).
References-found: 21

