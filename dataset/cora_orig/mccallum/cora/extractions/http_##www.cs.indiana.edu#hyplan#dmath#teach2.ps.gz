URL: http://www.cs.indiana.edu/hyplan/dmath/teach2.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/dmath.html
Root-URL: http://www.cs.indiana.edu
Email: dmath@cs.indiana.edu  Editor:  
Title: A Model of Interactive Teaching  
Author: H. DAVID MATHIAS 
Keyword: computational learning theory, teaching, query learning, DNF formulas  
Address: Bloomington, IN 47405  
Affiliation: Dept. of Computer Science, Indiana University,  
Abstract: Previous teaching models in the learning theory community have been batch models. That is, in these models the teacher has generated a single set of helpful examples to present to the learner. In this paper we present an interactive model in which the learner has the ability to ask queries as in the query learning model of Angluin [1]. We show that this model is at least as powerful as previous teaching models. We also show that anything learnable with queries, even by a randomized learner, is teachable in our model. In all previous teaching models, all classes shown to be teachable are known to be efficiently learnable. An important concept class that is not known to be learnable is DNF formulas. We demonstrate the power of our approach by providing a deterministic teacher and learner for the class of DNF formulas. The learner makes only equivalence queries and all hypotheses are also DNF formulas. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: Lange and Wiehagen [20] examine learning pattern languages and show that this can be achieved with good examples. 3. Preliminaries The teaching model that we present in this paper is based on the model of learning with queries developed by Angluin <ref> [1] </ref>. In this model the learner's goal is to infer an unknown target concept f chosen from known concept class C. More precisely, C is a representation class, a set of representations of functions. Throughout this paper, however, we use representation class and concept class interchangeably. <p> Another consequence of the results in this section follows from the work of An-gluin <ref> [1] </ref> in which she gives an algorithm for learning pattern languages of length n. Her algorithm uses restricted superset queries and runs in time polynomial in n. <p> restricted, our learner can ignore the particular counterexample received and simply determine whether the counterexample is positive or negative. (For obvious reasons, it is not possible to simulate restricted superset queries using restricted equivalence queries.) The last result we discuss in this section also follows from a result of Angluin <ref> [1] </ref>. The "double sunflower" is a concept class defined by participants in the learning seminar at the University of California, Santa Cruz in the Fall of 1987. The class is defined as follows. Let N = 2 n for some given positive n.
Reference: 2. <author> Dana Angluin, </author> <year> 1994. </year> <type> Personal communication. </type>
Reference-contexts: We demonstrate a system implemented by the teacher and learner that allows collusion in the presence of this adversary. This construction is due to Dana Angluin <ref> [2] </ref>. Let p be an n=2 bit prime. In polynomial time a randomized teacher can generate p with high probability. If the teacher is computationally unbounded then it can generate p with probability 1.
Reference: 3. <author> Dana Angluin and Martin Krikis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 57-66, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: It is easily seen that this model is robust against some types of noise. Specifically, the model can easily be extended to handle both incomplete membership queries [4] and malicious membership queries <ref> [3] </ref>. We briefly discuss the issue of noise in Section 7. 10 H. D. MATHIAS 4.2. <p> Finally, we consider malicious membership queries as introduced by Angluin and Krikis <ref> [3] </ref>. In this model the membership queries are answered incorrectly at the discretion of an omniscient adversary. As with incomplete membership queries the noise is persistent.
Reference: 4. <author> Dana Angluin and Donna K. </author> <title> Slonim. Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <year> 1994. </year>
Reference-contexts: A probabilistic teacher defines a distribution over the space of possible sequences of answers to the learner's queries. It is easily seen that this model is robust against some types of noise. Specifically, the model can easily be extended to handle both incomplete membership queries <ref> [4] </ref> and malicious membership queries [3]. We briefly discuss the issue of noise in Section 7. 10 H. D. MATHIAS 4.2. <p> It seems unlikely, however, that this change increases the power of the model. The next change we consider to the model concerns allowing noise. Specifically we examine noise in the membership queries. Angluin and Slonim <ref> [4] </ref> introduced the model of incomplete membership queries in which any membership query can be answered "I don't know" independently at random. The only restriction is that the answers are persistent the answer given for a query the first time it is asked is given every time it is asked.
Reference: 5. <author> Martin Anthony, Graham Brightwell, Dave Cohen, and John Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Aside from the work on formal models of teaching there has also been interest in complexity measures of various concept classes in existing learning models. Perhaps most general is the work of Heged-us [15, 16] who defines several general combinatorial measures on the complexity of teaching. Anthony, et.al. <ref> [5] </ref> consider subclasses of linearly separable boolean functions. They compute bounds on the size of the smallest sample with which only the target function is consistent.
Reference: 6. <author> Nader H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In 34th Annual Symposium on Foundations of Computer Science, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We return to this point in Section 5, when we show the power of such interaction using the work of Bshouty <ref> [6] </ref>. When we discuss the desirability of preventing collusion we beg the question of what constitutes collusion. Collusion is difficult to define formally. Unfortunately, in an interactive model of teaching, it is even more difficult to prevent. <p> Relationship to Monotone Theory One of the most interesting recent results in learning theory research is the development of the monotone theory, and its application to the learning of decision trees, by Bshouty <ref> [6] </ref>. Bshouty defines a complexity measure for concept classes called the monotone dimension, denoted M dim (C) for concept class C.
Reference: 7. <author> Nader H. Bshouty, Richard Cleve, Sampath Kannan, and Christino Tamon. </author> <title> Oracles and queries that are sufficient for exact learning. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 130-139, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: While many subclasses of DNF have been shown to be learnable not much progress has been made for the general case. Two recent results illustrate the state-of-the-art in DNF learning. Bshouty et.al. <ref> [7] </ref> gave a randomized algorithm, using restricted subset and superset queries, to learn DNF. In the PAC model, Jackson [17] has given an algorithm using membership queries to learn DNF against the uniform distribution. <p> Superset queries are symmetric. Thus, even if the learner is randomized, a T I L pair can simulate either subset of superset queries using equivalence queries. Some interesting results follow from the above theorems. The first of these uses a result of Bshouty, Cleve, Kannan and Tamon <ref> [7] </ref> in which they show that DNF formulas and polynomial-size circuits are learnable by a randomized learner using only subset and superset queries. They do this by showing that equivalence queries and and NP oracle can be simulated 14 H. D. MATHIAS using subset and superset queries.
References-found: 7

