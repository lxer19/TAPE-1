URL: http://www.cs.purdue.edu/homes/spaf/spyder/TR120P.ps
Refering-URL: http://www.cs.purdue.edu/homes/spaf/spyder.html
Root-URL: http://www.cs.purdue.edu
Email: fspaf, viravang@cs.purdue.edu  
Phone: (317) 494-7825  
Title: Experimental Designs: Testing a Debugging Oracle Assistant SERC-TR-120-P  
Author: Eugene H. Spafford Chonchanok Viravan 
Date: December 18, 1992  
Address: West Lafayette, IN 47907-1398  
Affiliation: Software Engineering Research Center Department of Computer Sciences Purdue University  
Abstract: This paper documents the design of an experiment to test a debugging oracle assistant. A debugging oracle is responsible for judging correctness of program parts or program states. A programmer usually acts as a debugging oracle. The goal of a debugging oracle assistant is to improve the programmer's speed and accuracy. Factors that complicate our design process include: (1) programmer variability, (2) interaction between programmers and programs, (3) interaction between programs and faults, (4) possible confounding experimental factors, (5) any learning effect from the assistance, (6) any learning effect from the program, and (7) the lack of experienced programmers for our experimental studies. This paper explains the rationale behind our design. It explains why the above factors can make other choices, such as a Latin square design, produce misleading results. It questions the validity of the so-called within-subjects factorial design when the experimental factors exclude programmers. It explains the factors related to programs, programmers, and faults that we need to control. It also explains why we prefer to use analysis of covariance to reduce experimental error caused by programmer variability instead of grouping programmers by expertise. The paper also covers types of analysis to (1) test our hypotheses, (2) verify assumptions behind the analysis of variance, (3) verify assumptions behind the analysis of covariance, and (4) estimate adequate sample size. Lastly, we define the inference space to which we can generalize the experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [ADS91] <author> Hiralal Agrawal, Richard A. DeMillo, and Eugene H. Spafford. </author> <title> An Execution Backtracking Approach to Program Debugging. </title> <journal> IEEE Software, </journal> <month> May </month> <year> 1991. </year>
Reference-contexts: A programmer usually assumes the role of a debugging oracle. To check if faults lie in a suspicious program part, the programmers usually fix and rerun the program until they obtain correct output <ref> [Gou75, Ves85, ADS91] </ref>. To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities.
Reference: [ADS93] <author> Hiralal Agrawal, Richard A. DeMillo, and Eugene H. Spafford. </author> <title> Debugging with Dynamic Slicing and Backtracking. </title> <journal> Software Practice and Experience, </journal> <note> 1993. (To appear). </note>
Reference-contexts: The design proposed here should work with a set of hypothesized fault locations, whatever its source. Either a programmer or a fault localization technique such as the ones described in <ref> [ADS93, PS93] </ref> can define this set. 3.1 Independent Variables Four independent variables or factors for our experimental design are assistance, program, fault, and programmers. <p> To keep the programming language from becoming a factor, we pick the programming language C (because our debugger prototype, Spyder <ref> [ADS93, Agr91] </ref> , works with C programs). To tap into an extensive collection of programs for the experiment, we use archie.
Reference: [Agr91] <author> Hiralal Agrawal. </author> <title> Towards Automatic Debugging of Computer Program. </title> <type> PhD thesis, </type> <institution> Purdue University, West Lafayette, IN, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Debugging, a process of locating and fixing program faults, is considered one of the most serious bottlenecks in software development today <ref> [Agr91] </ref>. Program faults, or bugs, are physical evidence of errors; errors are inappropriate actions made during software development that may ultimately cause software to fail [IEE83]. Most debugging tools, or debuggers, assist the debugging process by providing as much program information as possible. <p> A program part can vary from a collection of (not necessarily contiguous) program statements, to an expression within one statement, to an operation or a variable. A program state is composed of a control flow location and values of all variables visible at such a location <ref> [Agr91] </ref>. A programmer usually assumes the role of a debugging oracle. To check if faults lie in a suspicious program part, the programmers usually fix and rerun the program until they obtain correct output [Gou75, Ves85, ADS91]. <p> To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. Unlike a debugging assistant that may identify suspicious program parts (called fault localization techniques) <ref> [Agr91, Wei84, CC87, Sha83, Pan91] </ref>, no automated debugging oracle assistant is currently available. Because an automated oracle is far-fetched (if not impossible) without using details of formal program specification, most fault localization techniques assume that programmers know enough to judge a program properly. <p> To keep the programming language from becoming a factor, we pick the programming language C (because our debugger prototype, Spyder <ref> [ADS93, Agr91] </ref> , works with C programs). To tap into an extensive collection of programs for the experiment, we use archie.
Reference: [AM74] <author> Virgil L. Anderson and Robert A. McLean. </author> <title> Design of Experiments: A Realistic Approach. </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: Why cross program and fault? A design with multiple programs, each of which contains different faults, confounds the effects of program and fault. A confounding problem occurs when we cannot separate the effects of two (or more) factors <ref> [AM74] </ref>. When used wisely, confounding helps create special design arrangements that require fewer programmers or divide the experiment to be carried out in different time periods [Hic73]. Such design generally confounds two-way or higher interactions between factors.
Reference: [AR78] <author> M. E. Atwood and H. R. Ramsey. </author> <title> Cognitive Structures in the Comprehension and Memory of Computer Programs: An Investigation of Computer Program Debugging. </title> <type> Technical report, </type> <institution> Army Research Institute for the Behavioral and Social Sciences, </institution> <address> Alexandria, VA, </address> <year> 1978. </year>
Reference-contexts: According to [Gla81], fault of omission (the failure to do something) is harder to find than a fault of commission. To keep fault location from becoming a factor, we plant the fault in procedures in the same nesting level. The study by Atwood and Ramsey <ref> [AR78] </ref> reports that an error both lower in the propositional hierarchy 4 and lower in the program structure is more difficult to detect and correct than a similar error higher in the program structure. <p> This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., [GO86, Bae88]) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. Studies by Sheppard et al. [SCML79] and Atwood and Ramsey <ref> [AR78] </ref> observe significant interaction between fault and program. If we risk using this design and find that interaction exists, we cannot draw any conclusion from the study.
Reference: [Bae88] <author> Ronald Baecker. </author> <title> Enhancing Program Readability and Comprehensibility with Tools for Program Visualization. </title> <booktitle> In Proceedings 10th International Conference on Software Engineering, </booktitle> <month> April </month> <year> 1988. </year>
Reference-contexts: Besides the need for equal levels for all three effects (two levels in our case), another disadvantage of a Latin square is the assumption of no interaction between any of the three main effects [NW74, Mon91]. This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., <ref> [GO86, Bae88] </ref>) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. Studies by Sheppard et al. [SCML79] and Atwood and Ramsey [AR78] observe significant interaction between fault and program.
Reference: [Bow80] <author> J. B. Bowen. </author> <title> Standard error classification to support software reliability assessment. </title> <booktitle> In AFIPS National Computer Conference Proceedings, </booktitle> <volume> volume 49, </volume> <pages> pages 607 - 705, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: No password is required. 6 3.1.3 Faults The levels of fault factor correspond to fault categories from which the fault types are randomly selected. To expand our inference space, we choose two frequently occurring fault categories: logic faults and data definition/handling faults. Most error studies <ref> [Lip84, Bow80, PAFB82, MB77] </ref> rank logic faults first and data definition/handling faults second in frequency of occurrence. A few studies, like [WO84], rank data definition/handling first.
Reference: [Bro80] <author> Ruven E. Brooks. </author> <title> Studying Programmer Behavior Experimentally: The Problems of Proper Methodology. </title> <journal> Communications of the ACM, </journal> <volume> 23(4) </volume> <pages> 207-213, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: Fault Programmers 1 2 5 6 7 8 9 10 11 12 Program Assistance 1 2 1 2 1 2 1 2 1 2 1 2 2. Why use more than one program? More than one program is needed to define and expand our inference space. Brooks <ref> [Bro80] </ref> complains that the lack of knowledge about the description and specification of differences among subjects and programs has a damaging effect on the generalizability of the experimental finding.
Reference: [Bro83] <author> R. Brooks. </author> <title> Towards a theory of the comprehension of computer programs. </title> <journal> International Journal of Man-Machines Studies, </journal> <volume> 18:543 - 554, </volume> <year> 1983. </year>
Reference-contexts: This task is not trivial when the specification of the variables/functions is absent. A DOA can do the next best thing by providing information that can enhance programmer understanding of the program. Viravan refers to this information collectively as decision support evidence [Vir91]. Brook's beacon <ref> [Bro83] </ref>, the information that suggest the presence of a particular data structure or operations in the program, is also potential decision support evidence. The experimental design presented here can test whether a DOA helps the programmer improve his speed or accuracy when he acts as a debugging oracle.
Reference: [CC57] <author> William G. Cochran and Gertrude M. Cox. </author> <title> Experimental Designs. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: The word error is not synonymous with mistakes, but includes all types of extraneous variations. Such variations tend to mask the effect of the treatments <ref> [CC57] </ref>. A major problem in programmer-related experiments is that the effect of programmer variability is frequently greater than the effects of treatments [Ves85, Cur80, MS81]. <p> Why randomize things in the design? Randomization is a mean of ensuring that a treatment will not be continually favored or handicapped in successive replications by some extraneous source of variation, known or unknown <ref> [CC57] </ref>. Random allocation of programmers helps average out the effect of inhomogeneous experimental units (programmers). Random order of programs assigned to each programmer guards against systematic biases. In repeated measure design, the systematic biases may come from the learning effect and the fatigue effect. <p> Random order of programs assigned to each programmer guards against systematic biases. In repeated measure design, the systematic biases may come from the learning effect and the fatigue effect. The former makes the second measure better than the first; the latter does the opposite <ref> [CC57] </ref>. Fault type selection and fault locations are also randomly selected to avoid introducing bias. 7. Why not group programmers by expertise? Grouping programmers by expertise is another means to control programmer variability [Ves85]. We opt not to for two reasons. <p> No further grouping is required [Hic73]. 8. Why use at least twelve programmers? The answer lies in the degree of freedom of the estimate of error. A degree of freedom (d.f.) associated with any component is the number of independent parameters required to describe that component in the model <ref> [CC57] </ref>. When the number of degrees of freedom for error becomes smaller, the probability of obtaining a significant result decreases [CC57]. <p> A degree of freedom (d.f.) associated with any component is the number of independent parameters required to describe that component in the model <ref> [CC57] </ref>. When the number of degrees of freedom for error becomes smaller, the probability of obtaining a significant result decreases [CC57]. In our design, if we use eight programmers (for a sample size of two for each assistance and fault combination), the between-subjects error degree of freedom is four. 7 The test of assumptions in analysis of covariance can reduce this degree of freedom further.
Reference: [CC87] <author> James S. Collofello and Larry Cousins. </author> <title> Toward automatic software fault localization through decision-to-decision path analysis. </title> <booktitle> In Proceedings of AFIP 1987 National Computer Conference, </booktitle> <pages> pages 539-544, </pages> <year> 1987. </year>
Reference-contexts: To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. Unlike a debugging assistant that may identify suspicious program parts (called fault localization techniques) <ref> [Agr91, Wei84, CC87, Sha83, Pan91] </ref>, no automated debugging oracle assistant is currently available. Because an automated oracle is far-fetched (if not impossible) without using details of formal program specification, most fault localization techniques assume that programmers know enough to judge a program properly.
Reference: [Coc57] <author> William G. Cochran. </author> <title> Analysis of Covariance: Its Nature and Uses. </title> <journal> Biometrics, </journal> <volume> 13(3):261 - 281, </volume> <month> September </month> <year> 1957. </year>
Reference-contexts: This supplementary measurement should, to some extent, predict the performance of the experimental units (e.g., programmers) or remove some biases that arise from uncontrolled variables in the experiment <ref> [Coc57] </ref>. Analysis of covariance can adjust the observed response variable Y for the effect of the covariate X [Mon91]. Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91].
Reference: [CSM79] <author> Bill Curtis, Sylvia B. Sheppard, and Phil Milliman. </author> <title> Third time charm: Stronger prediction of programmer performance by software complexity metrics. </title> <booktitle> In Proceedings 4th International Conference on Software Engineering, </booktitle> <pages> pages 356-360, </pages> <year> 1979. </year>
Reference-contexts: Other possible covariates includes time measurement and software complexity metrics. Time measurement may reduce biases in the accuracy measurement. A software metric may adjust for variability in program complexity. According to the study by Curtis et al. <ref> [CSM79] </ref>, both Halstead's E and MacCabe's v (G) are good predictors of time to find and fix bugs. A program's complexity covariate is not needed when each programmer sees all programs during the experiment. Such is the case in our proposed experimental design model.
Reference: [Cur80] <author> Bill Curtis. </author> <booktitle> Measurement and Experimentation in Software Engineering. In Proceedings of the IEEE, </booktitle> <volume> volume 68, </volume> <pages> pages 1144-1157, </pages> <month> September </month> <year> 1980. </year>
Reference-contexts: Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91]. We may find the differences among programmers greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. We may need hundreds of programmers to see the statistical significance of our treatments. We want to find a covariate X that can reduce the experimental error caused by programmer variability. <p> The word error is not synonymous with mistakes, but includes all types of extraneous variations. Such variations tend to mask the effect of the treatments [CC57]. A major problem in programmer-related experiments is that the effect of programmer variability is frequently greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. The study by Sackman et al. [SEG68] points out a 28:1 performance difference among the professional programmers employed in the same position in the same firm. <p> Why not group programmers by expertise? Grouping programmers by expertise is another means to control programmer variability [Ves85]. We opt not to for two reasons. First, we are not interested in novices because their performance does not always scale up <ref> [Cur80, Jef82] </ref>. Second, we do not have a cost-effective, accurate, and reliable method to measure expertise. Vessey's ex-post classification [Ves85] is promising but costly because 6 This meaning is not consistent with the meaning of within-subject comparison for a repeated measure design.
Reference: [Dic81] <author> Thomas E. Dickey. </author> <title> Programmer Variability. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 69(7):844, </address> <month> July </month> <year> 1981. </year>
Reference-contexts: The study by Sackman et al. [SEG68] points out a 28:1 performance difference among the professional programmers employed in the same position in the same firm. Dickey <ref> [Dic81] </ref> later points out that this figure is misleading because it encompasses all differences between (1) time sharing and batch systems, (2) JTS and machine language programmers, and (3) prior experience with time-sharing systems. After accounting for these differences, only a range of 5:1 can be attributed to programmer variability [Dic81]. <p> <ref> [Dic81] </ref> later points out that this figure is misleading because it encompasses all differences between (1) time sharing and batch systems, (2) JTS and machine language programmers, and (3) prior experience with time-sharing systems. After accounting for these differences, only a range of 5:1 can be attributed to programmer variability [Dic81]. The nested factorial design shown in Figure 3 can have a large error term that reflects programmer variability.
Reference: [Gla81] <author> Robert L. Glass. </author> <title> Persistent Software Errors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(2):162-168, </volume> <month> March </month> <year> 1981. </year>
Reference-contexts: A few studies, like [WO84], rank data definition/handling first. To keep fault presence from becoming a factor, we select a fault type from a list of either fault of commission or fault of omission, not both. According to <ref> [Gla81] </ref>, fault of omission (the failure to do something) is harder to find than a fault of commission. To keep fault location from becoming a factor, we plant the fault in procedures in the same nesting level.
Reference: [GO86] <author> Leo Gugerty and Gary M. Olson. </author> <title> Comprehension Differences in Debugging by Skilled and Novice Programmers, chapter 2, pages 13 - 27. Empirical Studies of Programmers. </title> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1986. </year>
Reference-contexts: Besides the need for equal levels for all three effects (two levels in our case), another disadvantage of a Latin square is the assumption of no interaction between any of the three main effects [NW74, Mon91]. This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., <ref> [GO86, Bae88] </ref>) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. Studies by Sheppard et al. [SCML79] and Atwood and Ramsey [AR78] observe significant interaction between fault and program.
Reference: [Gou75] <author> J. D. Gould. </author> <title> Some psychological evidence on how people debug computer program. </title> <journal> International Journal of Man-Machines Studies, </journal> <volume> 7:151 - 182, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: A programmer usually assumes the role of a debugging oracle. To check if faults lie in a suspicious program part, the programmers usually fix and rerun the program until they obtain correct output <ref> [Gou75, Ves85, ADS91] </ref>. To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities.
Reference: [GS84] <author> D. J. Gilmore and H. T. Smith. </author> <title> An investigation of the utility of flowcharts during computer program debugging. </title> <journal> International Journal of Man-Machines Studies, </journal> <volume> 20 </volume> <pages> 357-372, </pages> <year> 1984. </year>
Reference-contexts: Most debugging tools, or debuggers, assist the debugging process by providing as much program information as possible. Unfortunately, only a small subset of such information has been experimentally evaluated. Even so, existing experimental results already contradict several researchers' expectations. Shneiderman et al. [SMMH] and Gilmore and Smith <ref> [GS84] </ref> were surprised that detailed flow charts do not significantly improve debugging speed and accuracy. Weiser and Lyle [WL91] were surprised that static slices 1 do not help programmers improve debugging performance. Experimental evaluations of debugging assistants can improve the quality of current debuggers.
Reference: [Hal79] <author> Maurice H. Halstead. </author> <booktitle> Elements of Software Science. Operating and Programming Systems Series. </booktitle> <publisher> Elsevier North Holland, Inc., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: To keep the program size from becoming a factor, we pick comparable length programs. The programs should contain statements within the same hundreds of lines. If possible, we pick programs whose vocabulary sizes (Halstead's total number of unique operators and operands <ref> [Hal79] </ref>) are approximately the same. To keep programming style from becoming a factor, we adjust both programs to make their style consistent: * Adjust the indentation level to four spaces.
Reference: [Har82] <author> Donald L. Harnett. </author> <title> Statistical Methods. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1982. </year>
Reference-contexts: To test this hypothesis, the best method is the analysis of variance (ANOVA). ANOVA is a method of estimating how much of the total variation in a set of data can be attributed to certain assignable causes of variation (independent variables) <ref> [Har82] </ref>. It uses the F-test to test the following hypotheses: H 0 : Factor f causes no variation in the programmers' performance. H a : Factor f causes the variation.
Reference: [Hic73] <author> Charles Robert Hicks. </author> <title> Fundamental Concepts in the Design of Experiments. </title> <address> New York, </address> <publisher> Holt, Rinehart and Windston, </publisher> <year> 1973. </year>
Reference-contexts: Other terms in the model represent the interaction, or the combined effects, between the above factors. An interaction between two factors means that a change in response between levels of one factor is different for all levels of the other factor <ref> [Hic73] </ref>. For example, if assistance and fault interact, we would not be able to tell if the presence of the assistant always helps improve verifying time or if certain fault type always required longer time to detect. <p> One possibility is to analyze the reciprocal of the time. The reciprocal value of infinity is zero. Vessey uses this approach to solve a similar problem in [Ves85]. 3.3 Covariance A covariate or a concomitant variable X is a variable that varies with the response variable Y <ref> [Hic73] </ref>. This supplementary measurement should, to some extent, predict the performance of the experimental units (e.g., programmers) or remove some biases that arise from uncontrolled variables in the experiment [Coc57]. Analysis of covariance can adjust the observed response variable Y for the effect of the covariate X [Mon91]. <p> We may need hundreds of programmers to see the statistical significance of our treatments. We want to find a covariate X that can reduce the experimental error caused by programmer variability. To qualify as a covariate X, a measurement variable M must meet the following assumptions <ref> [Hic73] </ref>. * Y correlates linearly with M (e.g., regression model is linear). * M can predict Y to a certain extent (e.g., regression coefficient is not zero). * M is not affected by treatments given to the groups (e.g., the regression coefficients within each group are homogeneous). <p> A confounding problem occurs when we cannot separate the effects of two (or more) factors [AM74]. When used wisely, confounding helps create special design arrangements that require fewer programmers or divide the experiment to be carried out in different time periods <ref> [Hic73] </ref>. Such design generally confounds two-way or higher interactions between factors. A design that confounds main effects, as in Figure 4, is a bad design. To avoid this problem, we cross the program factor and fault factor. <p> We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers [SE84]. The use of covariance analysis will provide the handicap for the programmers. No further grouping is required <ref> [Hic73] </ref>. 8. Why use at least twelve programmers? The answer lies in the degree of freedom of the estimate of error. A degree of freedom (d.f.) associated with any component is the number of independent parameters required to describe that component in the model [CC57]. <p> No main effect is confounded. This is called a 2x2 Latin Square design. 8 A Latin square design is a design in which each level of each factor is combined once and only once with each level of two other factors <ref> [Hic73] </ref>. According to Neter and Wasserman [NW74], it has three advantages. First, its use of two blocking variables (e.g., assistance and fault here) reduces experimental errors. Second, it minimizes the experimental units required. Third, it allows repeated measure design to take the order effect of treatments into account. <p> ANOVA can simultaneously test the significance of terms in the model in Figure 1: (1) the assistant, (2) programs, (3) faults, (4) interactions among assistance, fault, and program, and (5) and interaction of program and programmers within assistance and fault. 4.2 Verify ANOVA assumptions According to Hicks <ref> [Hic73] </ref>, three assumptions for ANOVA should be checked: 1. The process is controlled, that is, it is repeatable. 2. The population distribution being sampled is normal. 3. The error variances are homogeneous. Our design already meets assumption 1. <p> To check for assumption (3), Bartlett's test of homogeneity of variances can be used. More discussion of these tests can be found in [Mon91, NW74]. If either one of the last two assumptions is not true originally, suitable transformations on the response variable Y may make it true <ref> [Hic73] </ref>. When the normality assumption is unjustified, alternatives include the Kruskal-Wallis test and the Chi-square test [Mon91]. 4.3 Verify ANCOVA assumptions Because ANCOVA is an extension of ANOVA, it inherits all ANOVA assumptions. Presuming that they are met, we can test if a variable M is a suitable covariate. <p> Based on (1) the expected difference in magnitude of the effects of experimental treatment, 19 (2) an estimate of variance, and (3) the size of risks we are willing to take 9 <ref> [MS81, Hic73] </ref>, we can find out if fi, the chance of not finding a significant difference caused by experimental treatment when it exists (or the chance of type II error) [MS81], is too high. <p> If so, we need to use either a power table, a power curve, or Operating Characteristics curve <ref> [Hic73, Mon91] </ref> to estimate the required sample size. We need to replicate our experiment accordingly.
Reference: [IEE83] <institution> IEEE Standard Glossary of Software Engineering Terminology, </institution> <year> 1983. </year> <journal> IEEE Std. </journal> <pages> 729-1983. </pages>
Reference-contexts: 1 Introduction Debugging, a process of locating and fixing program faults, is considered one of the most serious bottlenecks in software development today [Agr91]. Program faults, or bugs, are physical evidence of errors; errors are inappropriate actions made during software development that may ultimately cause software to fail <ref> [IEE83] </ref>. Most debugging tools, or debuggers, assist the debugging process by providing as much program information as possible. Unfortunately, only a small subset of such information has been experimentally evaluated. Even so, existing experimental results already contradict several researchers' expectations.
Reference: [Jef82] <author> R. A. Jefferies. </author> <title> Comparison of Debugging Behavior of Novice and Expert programmers. </title> <type> Technical report, </type> <institution> Department of Psychology, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1982. </year>
Reference-contexts: To keep program reading strategies from becoming a factor, we rearrange the procedures to follow (approximately) the execution order of the programs. The study by Jefferies <ref> [Jef82] </ref> shows that experts understand programs better because they read them in execution order whereas novices read programs in linear order. We will vary only the complexity among programs. This will be done by varying the types of data structure and the number of nesting levels. <p> Why not group programmers by expertise? Grouping programmers by expertise is another means to control programmer variability [Ves85]. We opt not to for two reasons. First, we are not interested in novices because their performance does not always scale up <ref> [Cur80, Jef82] </ref>. Second, we do not have a cost-effective, accurate, and reliable method to measure expertise. Vessey's ex-post classification [Ves85] is promising but costly because 6 This meaning is not consistent with the meaning of within-subject comparison for a repeated measure design.
Reference: [KA85] <author> John C. Knight and Paul E. Ammann. </author> <title> An Experimental Evaluation of Simple Methods for Seeding Program Errors. </title> <booktitle> In Proceedings 8th International Conference on Software Engineering, </booktitle> <pages> pages 337-342, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: For example, when the fault type is an incorrect boolean operator, the statements in the list include if-then-else, while-do, case, etc. Simple syntactic manipulation in randomly selected statements for fault seeding, according to Knight and Ammann <ref> [KA85] </ref>, can yield the diversity of mean-time-to-failure (MTF) similar to that of unintended faults. 3.1.4 Programmers Programmers, our experimental subjects, will be graduate students or seniors in the department of Computer Sciences at Purdue University.
Reference: [KBMR91] <author> Jurgen Koenemann-Belliveau, Thomas G. Moher, and Scott P. Robertson, </author> <title> editors. Empirical Studies of Programmer: </title> <booktitle> Fourth Workshop. Human Computer Interaction. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1991. </year> <month> 22 </month>
Reference-contexts: To deal with factor (1), recent empirical studies of programmers found in <ref> [SI86, OSS87, KBMR91] </ref> still focus on grouping programmers by expertise rather than using the covariance alternative. Several studies overlook factors (2) and (3) when they use their within-subject design or a Latin square design.
Reference: [Lip84] <author> Myron Lipow. </author> <title> Prediction of Software Failure. </title> <journal> The Journal of Systems and Software, </journal> <volume> 4(4):71 - 76, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: No password is required. 6 3.1.3 Faults The levels of fault factor correspond to fault categories from which the fault types are randomly selected. To expand our inference space, we choose two frequently occurring fault categories: logic faults and data definition/handling faults. Most error studies <ref> [Lip84, Bow80, PAFB82, MB77] </ref> rank logic faults first and data definition/handling faults second in frequency of occurrence. A few studies, like [WO84], rank data definition/handling first.
Reference: [LPLS86] <author> David C. Littman, Jeannine Pinto, Stanley Letovsky, and Elliot Soloway. </author> <title> Mental Model and Software Maintenance, volume 1 of Empirical Studies of Programmers, </title> <booktitle> chapter 6, </booktitle> <pages> pages 80 - 97. </pages> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1986. </year>
Reference-contexts: The study by Pennington [Pen87] suggests that programmers need both forms of knowledge to achieve high program comprehension. Causal knowledge is also promising. Causal knowledge is the understanding of causal connections in the program as the program executes. According to Littman et al. <ref> [LPLS86] </ref> programmers need it to modify programs correctly. Both program comprehension and modification are tasks related to debugging. Moher and Schnei-der found that a measurement of programmers performing one task correlates with a measurement of another task better than any biographical variables [MS81]. <p> See Section 3.4.1. 13 it requires analysis of verbal protocol during a debugging process. Biographical data, like years of experience, do not always predict programmer performances. Several studies <ref> [Ves85, LPLS86, SCML79] </ref> report that years of experience (beyond three years [SCML79]) do not correlate to the programmers' ability to understand, modify, or debug programs. We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers [SE84].
Reference: [MB77] <author> R. W. Motley and W. D. Brooks. </author> <title> Statistical Prediction of Programming Errors. </title> <type> Technical Report RADC-TR-77-175, </type> <year> 1977. </year>
Reference-contexts: No password is required. 6 3.1.3 Faults The levels of fault factor correspond to fault categories from which the fault types are randomly selected. To expand our inference space, we choose two frequently occurring fault categories: logic faults and data definition/handling faults. Most error studies <ref> [Lip84, Bow80, PAFB82, MB77] </ref> rank logic faults first and data definition/handling faults second in frequency of occurrence. A few studies, like [WO84], rank data definition/handling first.
Reference: [MMNS83] <author> Richard J. Miaara, Joyce A. Musselman, Juan A Navarro, and Ben Shneiderman. </author> <title> Program indentation and comprehensibility. </title> <journal> Communications of the ACM, </journal> <volume> 26(11) </volume> <pages> 861-867, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: To keep programming style from becoming a factor, we adjust both programs to make their style consistent: * Adjust the indentation level to four spaces. According to the study by Miara et al. <ref> [MMNS83] </ref>, 2-4 spaces should optimize the aid of indentation to program comprehension. * Adjust the comment style, perhaps by leaving only header comments in each procedure. * Adjust the programs to have approximately the same percentage of comments over non-blank lines and same percentage of blank lines. * Adjust the program
Reference: [Mon91] <author> Douglas C. Montgomery. </author> <title> Design and Analysis of Experiments. </title> <publisher> John Wiley and sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This supplementary measurement should, to some extent, predict the performance of the experimental units (e.g., programmers) or remove some biases that arise from uncontrolled variables in the experiment [Coc57]. Analysis of covariance can adjust the observed response variable Y for the effect of the covariate X <ref> [Mon91] </ref>. Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91]. We may find the differences among programmers greater than the effects of treatments [Ves85, Cur80, MS81]. <p> Analysis of covariance can adjust the observed response variable Y for the effect of the covariate X <ref> [Mon91] </ref>. Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91]. We may find the differences among programmers greater than the effects of treatments [Ves85, Cur80, MS81]. We may need hundreds of programmers to see the statistical significance of our treatments. We want to find a covariate X that can reduce the experimental error caused by programmer variability. <p> An example of such a learning effect is when the given assistance improves the understanding of one hypothesized location, it may indirectly improve the understanding of other locations as well. 7 The computation of degree of freedom is explained in Montgomery <ref> [Mon91] </ref>. 14 Pi = Program i Fault j . . Programmers 1 2 5 6 7 8 9 10 11 12 1 Assistance P1-B1 3 4 . . Fault Programmers 1 2 4 5 6 7 8 9 10 11 123 1 1 2 1 2 1 2Program Assistance .. <p> Third, it allows repeated measure design to take the order effect of treatments into account. Besides the need for equal levels for all three effects (two levels in our case), another disadvantage of a Latin square is the assumption of no interaction between any of the three main effects <ref> [NW74, Mon91] </ref>. This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., [GO86, Bae88]) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. <p> To check for assumption (2), a normality plot and normality test can be used. To check for assumption (3), Bartlett's test of homogeneity of variances can be used. More discussion of these tests can be found in <ref> [Mon91, NW74] </ref>. If either one of the last two assumptions is not true originally, suitable transformations on the response variable Y may make it true [Hic73]. <p> If either one of the last two assumptions is not true originally, suitable transformations on the response variable Y may make it true [Hic73]. When the normality assumption is unjustified, alternatives include the Kruskal-Wallis test and the Chi-square test <ref> [Mon91] </ref>. 4.3 Verify ANCOVA assumptions Because ANCOVA is an extension of ANOVA, it inherits all ANOVA assumptions. Presuming that they are met, we can test if a variable M is a suitable covariate. <p> If so, we need to use either a power table, a power curve, or Operating Characteristics curve <ref> [Hic73, Mon91] </ref> to estimate the required sample size. We need to replicate our experiment accordingly.
Reference: [MS81] <author> Tom Moher and G. Michael Schneider. </author> <title> Methods for improving controlled experimentation in software engineering. </title> <booktitle> In Proceedings of the Fifth International Conference on Software Engineering, </booktitle> <pages> pages 224-233, </pages> <year> 1981. </year>
Reference-contexts: Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91]. We may find the differences among programmers greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. We may need hundreds of programmers to see the statistical significance of our treatments. We want to find a covariate X that can reduce the experimental error caused by programmer variability. <p> Understanding of a program domain 4. Understanding of causal knowledge in a program 5. Accuracy and time to judge hypothesized fault locations with no assistance Two promising biographical factors are experience (e.g., number of computer science classes) and aptitude (e.g., GPA). Moher and Schneider <ref> [MS81] </ref> found that both factors explain about 40% of the variations in program comprehension scores for student programmers (including novices). 9 Both programming language familiarity and the understanding of the domain are promising covariate candidates. <p> According to Littman et al. [LPLS86] programmers need it to modify programs correctly. Both program comprehension and modification are tasks related to debugging. Moher and Schnei-der found that a measurement of programmers performing one task correlates with a measurement of another task better than any biographical variables <ref> [MS81] </ref>. Accuracy and time to judge hypothesized fault locations with no assistance are promising covariate candidates because we will get the same unit of estimate as those from the experiment. Though the programmers carry out the same tasks, the measurements are not 100% guaranteed to work [PIS80]. <p> The word error is not synonymous with mistakes, but includes all types of extraneous variations. Such variations tend to mask the effect of the treatments [CC57]. A major problem in programmer-related experiments is that the effect of programmer variability is frequently greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. The study by Sackman et al. [SEG68] points out a 28:1 performance difference among the professional programmers employed in the same position in the same firm. <p> With twelve programmers, the error degree of freedom before adjusting for covariates is eight. We are not claiming that twelve programmers are adequate. By adequate, we mean fi, the chance of not finding significant difference caused by experimental treatment when it exists <ref> [MS81] </ref>, is sufficiently low (e.g., like 5%). Twelve is what we need for an initial pilot study. Measurements from an initial pilot study can be used to estimate the actual number of programmers needed. <p> Based on (1) the expected difference in magnitude of the effects of experimental treatment, 19 (2) an estimate of variance, and (3) the size of risks we are willing to take 9 <ref> [MS81, Hic73] </ref>, we can find out if fi, the chance of not finding a significant difference caused by experimental treatment when it exists (or the chance of type II error) [MS81], is too high. <p> experimental treatment, 19 (2) an estimate of variance, and (3) the size of risks we are willing to take 9 [MS81, Hic73], we can find out if fi, the chance of not finding a significant difference caused by experimental treatment when it exists (or the chance of type II error) <ref> [MS81] </ref>, is too high. If so, we need to use either a power table, a power curve, or Operating Characteristics curve [Hic73, Mon91] to estimate the required sample size. We need to replicate our experiment accordingly.
Reference: [MW88] <author> Jeffrey Mitchell and Charles Welty. </author> <title> Experimentation in computer science: an empirical view. </title> <journal> International Journal of Man-Machines Studies, </journal> <volume> 29 </volume> <pages> 613-624, </pages> <year> 1988. </year>
Reference-contexts: done by either taking the learning factor into account or by letting programmers become familiar with the programs prior to the experiments. 9 For instance, ff = the chance of falsely rejecting the null hypothesis = the chance of type I error = 5%. 20 According to Mitchell and Welty <ref> [MW88] </ref>, computer science ranks almost the lowest among scientific disciplines in publishing experimental studies. This is not because this field does not need them. Mitchell and Welty suspect that many computer science researchers do not really know how to do experiments nor are they willing to spend time on it.
Reference: [NW74] <author> John Neter and William Wasserman. </author> <title> Applied Linear Statistical Models. </title> <editor> Richard D. Irwin, </editor> <publisher> Inc., Homewood, </publisher> <address> Illinois 60430, </address> <year> 1974. </year>
Reference-contexts: Section 4.3 covers the test for these assumptions to ensure the validity of co-variance analysis. If the treatment affects M , for example, the covariance analysis will remove some (or much) of the effect that the treatments had on the response variable and badly distort the analysis <ref> [NW74] </ref>. Candidates for X that might reduce programmer variability include: 1. Biographical data 2. Familiarity with the programming language C 3. Understanding of a program domain 4. Understanding of causal knowledge in a program 5. <p> No main effect is confounded. This is called a 2x2 Latin Square design. 8 A Latin square design is a design in which each level of each factor is combined once and only once with each level of two other factors [Hic73]. According to Neter and Wasserman <ref> [NW74] </ref>, it has three advantages. First, its use of two blocking variables (e.g., assistance and fault here) reduces experimental errors. Second, it minimizes the experimental units required. Third, it allows repeated measure design to take the order effect of treatments into account. <p> Third, it allows repeated measure design to take the order effect of treatments into account. Besides the need for equal levels for all three effects (two levels in our case), another disadvantage of a Latin square is the assumption of no interaction between any of the three main effects <ref> [NW74, Mon91] </ref>. This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., [GO86, Bae88]) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. <p> To check for assumption (2), a normality plot and normality test can be used. To check for assumption (3), Bartlett's test of homogeneity of variances can be used. More discussion of these tests can be found in <ref> [Mon91, NW74] </ref>. If either one of the last two assumptions is not true originally, suitable transformations on the response variable Y may make it true [Hic73].
Reference: [OM88] <author> Bernard Ostle and Linda C. Malone. </author> <title> Statistics in Research: Basic Concepts and Techniques for Research Workers. </title> <institution> Iowa State University Press, Ames, Iowa, </institution> <year> 1988. </year>
Reference-contexts: This is called a repeated measure design. A repeated measure design is a design that takes several observations from the same subject under different treatments <ref> [OM88] </ref>. Two kinds of comparisons in a repeated measure design are between-subjects and within-subjects. Between-subjects comparison is made when subjects are nested under the treatment levels. Thus, we compare assistance and fault 10 between-subjects. Within-subject comparison is made when subjects are crossed (repeatedly measured) with treatment levels. <p> Why use a repeated measure design? A repeated measure design reduces the experimental error caused by programmer variability, making it possible to use fewer subjects to gather as much information as with a larger design <ref> [OM88] </ref>. The word error is not synonymous with mistakes, but includes all types of extraneous variations. Such variations tend to mask the effect of the treatments [CC57]. <p> This leaves * (ijkl)m equal to zero. R (ij)k is the between-subject error term. RP (ij)kl is the within-subject error term. 11 no carry-over effect from learning, training, ordering, or fatigue when a program-mer is measured more than once <ref> [OM88] </ref>. We show the impact of these limitations later in this section. . Fault Programmers 1 2 5 6 7 8 9 10 11 12 Program Assistance 1 2 1 2 1 2 1 2 1 2 1 2 2. <p> Besides the need for equal levels for all three effects (two levels in our case), another disadvantage of a Latin square is the assumption of no interaction between any of the three main effects [NW74, Mon91]. This assumption is frequently overlooked by researchers <ref> [OM88] </ref>. Several programmer-related studies (e.g., [GO86, Bae88]) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. Studies by Sheppard et al. [SCML79] and Atwood and Ramsey [AR78] observe significant interaction between fault and program.
Reference: [OSS87] <author> Gary M. Olson, Sylvia Sheppard, and Elliot Soloway, </author> <title> editors. Empirical Studies of Programmer: </title> <booktitle> Second Workshop. Human Computer Interaction. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1987. </year>
Reference-contexts: To deal with factor (1), recent empirical studies of programmers found in <ref> [SI86, OSS87, KBMR91] </ref> still focus on grouping programmers by expertise rather than using the covariance alternative. Several studies overlook factors (2) and (3) when they use their within-subject design or a Latin square design.
Reference: [PAFB82] <author> D. Potier, J. L. Albin, R. Ferreol, and A. Bilodeau. </author> <title> Experiments with Computer Software Complexity and Reliability. </title> <booktitle> In Proceedings 6th International Conference on Software Engineering, </booktitle> <pages> pages 94-103, </pages> <year> 1982. </year>
Reference-contexts: No password is required. 6 3.1.3 Faults The levels of fault factor correspond to fault categories from which the fault types are randomly selected. To expand our inference space, we choose two frequently occurring fault categories: logic faults and data definition/handling faults. Most error studies <ref> [Lip84, Bow80, PAFB82, MB77] </ref> rank logic faults first and data definition/handling faults second in frequency of occurrence. A few studies, like [WO84], rank data definition/handling first.
Reference: [Pan91] <author> Hsin Pan. </author> <title> Debugging with Dynamic Instrumentation and Test-Based Knowledge. </title> <type> Technical Report SERC-TR-105-P, </type> <institution> Software Engineering Research Center, Purdue University, West Lafayette, IN, </institution> <year> 1991. </year>
Reference-contexts: To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. Unlike a debugging assistant that may identify suspicious program parts (called fault localization techniques) <ref> [Agr91, Wei84, CC87, Sha83, Pan91] </ref>, no automated debugging oracle assistant is currently available. Because an automated oracle is far-fetched (if not impossible) without using details of formal program specification, most fault localization techniques assume that programmers know enough to judge a program properly.
Reference: [Pen87] <author> Nancy Pennington. </author> <title> Comprehension Strategies in Programming, pages 100-113. Empirical Studies of Programmer: </title> <booktitle> Second Workshop. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1987. </year>
Reference-contexts: Moher and Schneider [MS81] found that both factors explain about 40% of the variations in program comprehension scores for student programmers (including novices). 9 Both programming language familiarity and the understanding of the domain are promising covariate candidates. The study by Pennington <ref> [Pen87] </ref> suggests that programmers need both forms of knowledge to achieve high program comprehension. Causal knowledge is also promising. Causal knowledge is the understanding of causal connections in the program as the program executes. According to Littman et al. [LPLS86] programmers need it to modify programs correctly.
Reference: [PIS80] <author> Tom Di Persio, Dan Isbister, and Ben Schneiderman. </author> <title> An experiment using memorization/reconstruction as a measure of programmer ability. </title> <journal> International Journal of Man-Machines Studies, </journal> <pages> pages 339-354, </pages> <month> March </month> <year> 1980. </year>
Reference-contexts: Accuracy and time to judge hypothesized fault locations with no assistance are promising covariate candidates because we will get the same unit of estimate as those from the experiment. Though the programmers carry out the same tasks, the measurements are not 100% guaranteed to work <ref> [PIS80] </ref>. The characteristics of the program and the fault we choose can still affect the programmer's performance. Other possible covariates includes time measurement and software complexity metrics. Time measurement may reduce biases in the accuracy measurement. A software metric may adjust for variability in program complexity.
Reference: [PS93] <author> Hsin Pan and E. H. Spafford. </author> <title> Fault Localization Methods for Software Debugging. </title> <journal> Journal of Computer and Software Engineering, </journal> <note> 1993. (to appear). </note>
Reference-contexts: The design proposed here should work with a set of hypothesized fault locations, whatever its source. Either a programmer or a fault localization technique such as the ones described in <ref> [ADS93, PS93] </ref> can define this set. 3.1 Independent Variables Four independent variables or factors for our experimental design are assistance, program, fault, and programmers.
Reference: [SCML79] <author> S. B. Sheppard, B. Curtis, P. Milliman, and T. Love. </author> <title> Modern coding practices and programmer performance. </title> <journal> Computer, </journal> <volume> 12(12) </volume> <pages> 41-49, </pages> <month> December </month> <year> 1979. </year> <month> 23 </month>
Reference-contexts: The experimental results may become misleading. This fact makes us question the validity of a popular design in empirical studies of programmers called within-subjects factorial design. This design allows each subject to see each level of each experimentally manipulated variable once and only once 6 <ref> [SCML79] </ref>. For example, Sheppard et al. generate 81 treatment combinations from their within-subject, 3 4 design. Each programmer sees three different treatment combinations. Twenty-seven programmers exhaust all 81 conditions. Nine other programmers repeat the tasks of nine previous participants [SCML79]. <p> level of each experimentally manipulated variable once and only once 6 <ref> [SCML79] </ref>. For example, Sheppard et al. generate 81 treatment combinations from their within-subject, 3 4 design. Each programmer sees three different treatment combinations. Twenty-seven programmers exhaust all 81 conditions. Nine other programmers repeat the tasks of nine previous participants [SCML79]. Though this design repeatedly measured programmers three times, it did not include programmers as a factor. As a result, Sheppard et al. admit that they cannot separate the variance attributed to individual programs from those attributed to programmers. <p> See Section 3.4.1. 13 it requires analysis of verbal protocol during a debugging process. Biographical data, like years of experience, do not always predict programmer performances. Several studies <ref> [Ves85, LPLS86, SCML79] </ref> report that years of experience (beyond three years [SCML79]) do not correlate to the programmers' ability to understand, modify, or debug programs. We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers [SE84]. <p> See Section 3.4.1. 13 it requires analysis of verbal protocol during a debugging process. Biographical data, like years of experience, do not always predict programmer performances. Several studies [Ves85, LPLS86, SCML79] report that years of experience (beyond three years <ref> [SCML79] </ref>) do not correlate to the programmers' ability to understand, modify, or debug programs. We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers [SE84]. <p> This assumption is frequently overlooked by researchers [OM88]. Several programmer-related studies (e.g., [GO86, Bae88]) use Latin square without mentioning that they verify this assumption. Unfortunately, programs and faults do have a history of significant interaction. Studies by Sheppard et al. <ref> [SCML79] </ref> and Atwood and Ramsey [AR78] observe significant interaction between fault and program. If we risk using this design and find that interaction exists, we cannot draw any conclusion from the study.
Reference: [SE84] <author> Elliot Soloway and Kate Ehrlich. </author> <title> Empirical Studies of Programming Knowledge. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(5) </volume> <pages> 595-609, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers <ref> [SE84] </ref>. The use of covariance analysis will provide the handicap for the programmers. No further grouping is required [Hic73]. 8. Why use at least twelve programmers? The answer lies in the degree of freedom of the estimate of error.
Reference: [SEG68] <author> H. Sackman, W. J. Erikson, and E. Grant. </author> <title> Exploratory experimental studies comparing online and offline programming performance. </title> <journal> Communications of the ACM, </journal> <volume> 11(1) </volume> <pages> 3-11, </pages> <month> January </month> <year> 1968. </year>
Reference-contexts: Such variations tend to mask the effect of the treatments [CC57]. A major problem in programmer-related experiments is that the effect of programmer variability is frequently greater than the effects of treatments [Ves85, Cur80, MS81]. The study by Sackman et al. <ref> [SEG68] </ref> points out a 28:1 performance difference among the professional programmers employed in the same position in the same firm.
Reference: [Sha83] <author> E. Y. Shapiro. </author> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1983. </year>
Reference-contexts: To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. Unlike a debugging assistant that may identify suspicious program parts (called fault localization techniques) <ref> [Agr91, Wei84, CC87, Sha83, Pan91] </ref>, no automated debugging oracle assistant is currently available. Because an automated oracle is far-fetched (if not impossible) without using details of formal program specification, most fault localization techniques assume that programmers know enough to judge a program properly.
Reference: [SI86] <author> Elliot Soloway and Sitharama Iyengar, </author> <title> editors. Empirical Studies of Programmer. Human Computer Interaction. </title> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1986. </year>
Reference-contexts: To deal with factor (1), recent empirical studies of programmers found in <ref> [SI86, OSS87, KBMR91] </ref> still focus on grouping programmers by expertise rather than using the covariance alternative. Several studies overlook factors (2) and (3) when they use their within-subject design or a Latin square design.
Reference: [SMMH] <author> B. Shneiderman, R. E. Mayer, D. McKay, and P. Heller. </author> <title> Experimental investigations of the utility of detailed flowcharts in programming. </title> <journal> Communications of the ACM, </journal> <volume> 20 </volume> <pages> 373-381. </pages>
Reference-contexts: Most debugging tools, or debuggers, assist the debugging process by providing as much program information as possible. Unfortunately, only a small subset of such information has been experimentally evaluated. Even so, existing experimental results already contradict several researchers' expectations. Shneiderman et al. <ref> [SMMH] </ref> and Gilmore and Smith [GS84] were surprised that detailed flow charts do not significantly improve debugging speed and accuracy. Weiser and Lyle [WL91] were surprised that static slices 1 do not help programmers improve debugging performance. Experimental evaluations of debugging assistants can improve the quality of current debuggers.
Reference: [Ves85] <author> I. Vessey. </author> <title> Expertise in Debugging Computer Programs: A Process Analysis. </title> <journal> International Journal of Man-Machines Studies, </journal> <volume> 23:459 - 494, </volume> <year> 1985. </year>
Reference-contexts: A programmer usually assumes the role of a debugging oracle. To check if faults lie in a suspicious program part, the programmers usually fix and rerun the program until they obtain correct output <ref> [Gou75, Ves85, ADS91] </ref>. To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. <p> Data transformation is required to analyze data with infinite value. One possibility is to analyze the reciprocal of the time. The reciprocal value of infinity is zero. Vessey uses this approach to solve a similar problem in <ref> [Ves85] </ref>. 3.3 Covariance A covariate or a concomitant variable X is a variable that varies with the response variable Y [Hic73]. This supplementary measurement should, to some extent, predict the performance of the experimental units (e.g., programmers) or remove some biases that arise from uncontrolled variables in the experiment [Coc57]. <p> Without the adjustment, a covariate could inflate the experimental error term and make true differences in response caused by treatments harder to detect [Mon91]. We may find the differences among programmers greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. We may need hundreds of programmers to see the statistical significance of our treatments. We want to find a covariate X that can reduce the experimental error caused by programmer variability. <p> The word error is not synonymous with mistakes, but includes all types of extraneous variations. Such variations tend to mask the effect of the treatments [CC57]. A major problem in programmer-related experiments is that the effect of programmer variability is frequently greater than the effects of treatments <ref> [Ves85, Cur80, MS81] </ref>. The study by Sackman et al. [SEG68] points out a 28:1 performance difference among the professional programmers employed in the same position in the same firm. <p> Why does each programmer not see all program versions? A learning effect from seeing the same program twice is the problem. Vessey found that programmers cut their debugging time in half (despite the fault) when they see the same program again <ref> [Ves85] </ref>. 12 By preventing advance study of the programs by experimental subjects, we create a more artificial environment. In real debugging scenarios, the users often are familiar with the software; in some cases, users are debugging software they may have maintained for decades. <p> The former makes the second measure better than the first; the latter does the opposite [CC57]. Fault type selection and fault locations are also randomly selected to avoid introducing bias. 7. Why not group programmers by expertise? Grouping programmers by expertise is another means to control programmer variability <ref> [Ves85] </ref>. We opt not to for two reasons. First, we are not interested in novices because their performance does not always scale up [Cur80, Jef82]. Second, we do not have a cost-effective, accurate, and reliable method to measure expertise. Vessey's ex-post classification [Ves85] is promising but costly because 6 This meaning <p> expertise is another means to control programmer variability <ref> [Ves85] </ref>. We opt not to for two reasons. First, we are not interested in novices because their performance does not always scale up [Cur80, Jef82]. Second, we do not have a cost-effective, accurate, and reliable method to measure expertise. Vessey's ex-post classification [Ves85] is promising but costly because 6 This meaning is not consistent with the meaning of within-subject comparison for a repeated measure design. See Section 3.4.1. 13 it requires analysis of verbal protocol during a debugging process. Biographical data, like years of experience, do not always predict programmer performances. <p> See Section 3.4.1. 13 it requires analysis of verbal protocol during a debugging process. Biographical data, like years of experience, do not always predict programmer performances. Several studies <ref> [Ves85, LPLS86, SCML79] </ref> report that years of experience (beyond three years [SCML79]) do not correlate to the programmers' ability to understand, modify, or debug programs. We instead post a restriction that our student programmers have three or more years of experience. Soloway and Ehrlich call them advanced student programmers [SE84].
Reference: [Vir91] <author> Chonchanok Viravan. </author> <title> Fault Investigation and Trial. </title> <type> Technical Report SERC-TR-104-P, </type> <institution> Software Engineering Research Center, Purdue University, West Lafayette, IN, </institution> <year> 1991. </year>
Reference-contexts: The presumption that a programmer is an accurate or reliable oracle lacks supporting evidence. When a programmer judges suspected locations, he can still waste time investigating the correct locations, or ignoring the faulty one <ref> [Vir91] </ref>. <p> A suspect list initially consists of all parts identified in the given hypothesized fault locations. 2 A prime suspect is the suspect that ranks highest in its likelihood to be faulty <ref> [Vir91] </ref>. An incorrect suspect is a suspect that is not faulty. To improve debugging accuracy, a DOA should help a programmer verify a program part in question. Given a statement as a prime suspect, for example, a programmer usually restores and inspects one of its program states. <p> This task is not trivial when the specification of the variables/functions is absent. A DOA can do the next best thing by providing information that can enhance programmer understanding of the program. Viravan refers to this information collectively as decision support evidence <ref> [Vir91] </ref>. Brook's beacon [Bro83], the information that suggest the presence of a particular data structure or operations in the program, is also potential decision support evidence. <p> This design guards against several complicating factors. In the following sections, we will explain how our experiment is structured to mitigate these factors. 2 These definitions are broader than, but related to, the ones originally proposed by Viravan in <ref> [Vir91] </ref>. 4 Y ijklm = + A i + B j + AB ij + R (ij)k + P l AP il + BP jl + ABP ijl + RP (ij)kl + * (ijkl)m Y ijklm = accuracy or time = average of Y A i = Assistance; i = 1;
Reference: [WDS81] <author> S. N. Woodfield, H. E. Dunsmore, and V. Y. Shen. </author> <title> The Effect of Modularization and Comments on Program Comprehension. </title> <booktitle> In Proceedings 5th International Conference on Software Engineering, </booktitle> <pages> pages 215-223, </pages> <year> 1981. </year>
Reference-contexts: The study by Weissman [Wei74] shows a higher comprehension score with structured programs. To keep the procedure interconnections from becoming a factor, we pick programs whose procedures have the similar number of parameters, if possible. The study by Woodfield et al. <ref> [WDS81] </ref> suggests that module interconnection may play a more important role in ease of comprehension than the level of modularization. To keep program reading strategies from becoming a factor, we rearrange the procedures to follow (approximately) the execution order of the programs.
Reference: [Wei74] <author> L. M. Weissman. </author> <title> A Method for Studying the Psychological Complexity of Computer Programs. </title> <type> Technical Report TR-CSRG-37, </type> <institution> University of Toronto, Department of Computer Science, Toronto, Canada, </institution> <year> 1974. </year>
Reference-contexts: To keep the program control structure from becoming a factor, we pick programs that contain no goto's. The study by Weissman <ref> [Wei74] </ref> shows a higher comprehension score with structured programs. To keep the procedure interconnections from becoming a factor, we pick programs whose procedures have the similar number of parameters, if possible.
Reference: [Wei84] <author> Mark Weiser. </author> <title> Program Slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: To check variable values or flow of control in a program state, programmers usually rely on their intuitions and deductive abilities. Unlike a debugging assistant that may identify suspicious program parts (called fault localization techniques) <ref> [Agr91, Wei84, CC87, Sha83, Pan91] </ref>, no automated debugging oracle assistant is currently available. Because an automated oracle is far-fetched (if not impossible) without using details of formal program specification, most fault localization techniques assume that programmers know enough to judge a program properly.
Reference: [WL91] <author> Mark Weiser and Jim Lyle. </author> <title> Experiments on Slicing-Based Debugging Aids, pages 187-197. Empirical Studies of Programmer: </title> <booktitle> Fourth Workshop. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, New Jersey, </address> <year> 1991. </year>
Reference-contexts: Unfortunately, only a small subset of such information has been experimentally evaluated. Even so, existing experimental results already contradict several researchers' expectations. Shneiderman et al. [SMMH] and Gilmore and Smith [GS84] were surprised that detailed flow charts do not significantly improve debugging speed and accuracy. Weiser and Lyle <ref> [WL91] </ref> were surprised that static slices 1 do not help programmers improve debugging performance. Experimental evaluations of debugging assistants can improve the quality of current debuggers. They can help check if a debugger provides information that significantly improves a programmer's performance.
Reference: [WO84] <author> Elaine Weyuker and Thomas Ostrand. </author> <title> Collecting and Categorizing Software Error Data in an Industrial Environment. </title> <journal> The Journal of Systems and Software, </journal> <volume> 4(4):289 - 300, </volume> <month> November </month> <year> 1984. </year> <month> 24 </month>
Reference-contexts: To expand our inference space, we choose two frequently occurring fault categories: logic faults and data definition/handling faults. Most error studies [Lip84, Bow80, PAFB82, MB77] rank logic faults first and data definition/handling faults second in frequency of occurrence. A few studies, like <ref> [WO84] </ref>, rank data definition/handling first. To keep fault presence from becoming a factor, we select a fault type from a list of either fault of commission or fault of omission, not both.
References-found: 54

