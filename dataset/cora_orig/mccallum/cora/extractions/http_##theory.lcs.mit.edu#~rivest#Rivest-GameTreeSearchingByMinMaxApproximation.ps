URL: http://theory.lcs.mit.edu/~rivest/Rivest-GameTreeSearchingByMinMaxApproximation.ps
Refering-URL: http://theory.lcs.mit.edu/~rivest/publications.html
Root-URL: 
Title: Game Tree Searching by Min/Max Approximation  
Author: Ronald L. Rivest 
Keyword: Games, game trees, searching, alpha-beta, heuristics, min/max approximation.  
Note: 0 This research was supported by NSF grants DCR-8006938 and DCR-8607494.  
Date: March 29, 1995  
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: MIT Laboratory for Computer Science  
Abstract: We present an iterative method for searching min/max game trees based on the idea of approximating the "min" and "max" operators by generalized mean-valued operators. This approximation is used to guide the selection of the next leaf node to expand, since the approximations allow one to select efficiently that leaf node upon whose value the (approximate) value at the root most highly depends. Experimental results from almost 1; 000 games of Connect-Four 1 suggest that our scheme is superior to minimax search with alpha-beta pruning, for the same number of calls to the move routine. However, our scheme has higher overhead, so that further work is needed before it becomes competitive when CPU time per turn is the limiting resource. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baudet, Gerard, </author> <title> "The Design and Analysis of Algorithms for Asynchronous Multiprocessors", </title> <note> Carnegie-Mellon Computer Science Technical Report CMU-CS-78-116 (April, </note> <year> 1978). </year>
Reference-contexts: Can our ideas be combined effectively with more traditional approaches? In particular, what is the best way to blend the efficiency of depth-first search with our ideas? 3. How well can these ideas be parallelized? (See <ref> [1] </ref> for a fascinating discussion on how to parallelize the alpha-beta heuristic.) 4. How well does our approach work in games where sacrifices are important? (Connect Four seems not to be such a game.) Will useful sacrifice plays be discovered? 5.
Reference: [2] <author> Berliner, Hans, </author> <title> "The B* Tree Search Algorithm: A Best-First Proof Procedure," </title> <journal> Artificial Intelligence, </journal> <volume> 12(1979), </volume> <pages> 23-40. </pages>
Reference-contexts: The combinatorial explosion of possibilities in a game such as chess tax our most powerful computers, and even special-purpose hardware soon reaches its limits. Clearly, the most careful organization and allocation of computational resources is needed to obtain expert-level play. Techniques such as alpha-beta pruning and its successors <ref> [6, 2] </ref> have been essential in reducing the computational burden of exploring a game tree. Still, new techniques are needed. <p> This estimate is based on "static" features of the current configuration that can be evaluated without further look-ahead (e.g. piece count and an advantage for the player to move). Our proposed technique requires a single static evaluator ^v (). Some other methods - most notably the B* algorithm <ref> [2] </ref> require two static evaluation functions which are upper and lower bounds on v (). <p> The tree grown by an iterative heuristic need not be of uniform depth: some branches may be searched to a much greater depth than other branches. Examples of such iterative techniques are the Berliner's B fl algorithm <ref> [2] </ref>, Nilsson's "arrow" method [11], Palay's probability-based method [12], and McAllester's "conspiracy number" method [7]. <p> For example, if there is only one move to make from the root, then a penalty-based scheme may search the subtree below that move extensively, even though such exploration can't affect the decision to be made at the root. By contrast, the B* algorithm <ref> [2] </ref>, another iterative search heuristic, is oriented towards making the best choice, and will not waste any time when there is only one move to be made. Third, the penalty-based schemes as presented require that a tip be expanded by generating and evaluating all of the tip's successors.
Reference: [3] <editor> Barr, Avron, and E.A. Feigenbaum (eds.) </editor> <booktitle> The Handbook of Artificial Intelligence (Kaufmann, 1981) (vol. I), Section II.C. </booktitle>
Reference: [4] <author> Campbell, Murray S., and T.A. Marsland, </author> <title> "A Comparison of Minimax Tree Search Algorithms," </title> <booktitle> Artificial Intelligence 20(1983), </booktitle> <pages> 347-367. </pages>
Reference: [5] <author> Hardy, G. H., J. E. Littelwood, and G. </author> <title> Polya, </title> <publisher> Inequalities (Cambridge University Press, </publisher> <year> 1934). </year>
Reference-contexts: We begin with the fact that p &lt; q ) M p (a) M q (a) (3) where equality only holds on the right if all the a i 's are equal. (See <ref> [5] </ref> for proofs and other facts about generalized mean values.) For our purposes, we are most interested in the following two facts: lim M p (a) = max (a 1 ; : : : ; a n ) (4) p!1 To illustrate the above facts, consider Table 1 where various values
Reference: [6] <author> Knuth, Donald E., and Ronald M. Moore, </author> <title> "An Analysis of Alpha-Beta Pruning," </title> <journal> Artificial Intelligence, </journal> <volume> 6(1975), </volume> <pages> 293-325. </pages>
Reference-contexts: The combinatorial explosion of possibilities in a game such as chess tax our most powerful computers, and even special-purpose hardware soon reaches its limits. Clearly, the most careful organization and allocation of computational resources is needed to obtain expert-level play. Techniques such as alpha-beta pruning and its successors <ref> [6, 2] </ref> have been essential in reducing the computational burden of exploring a game tree. Still, new techniques are needed. <p> The value of the game is 16; optimal play is indicated with the heavy arrows. 5 3.2 Searching a Game Tree When C is small, the tree can be explored completely, so optimal play is possible. On slightly larger trees minimax search with alpha-beta pruning <ref> [6] </ref> may produce optimal play even though only a small fraction of the game tree is explored the portions of the tree that are "pruned" (not explored) are known not to be relevant. However, for most interesting games the game tree is so large that heuristic approximations are needed. <p> However, it does preclude our using the "negamax" formulation of games <ref> [6] </ref>. 11 Since we want to expand the expandable tip x with the largest D (s; x), we should choose the expandable tip with the least penalty, since the penalties are defined by P s (x) = c 2 A (x) so that the tip x with largest value D (s; <p> With this definition, the tip node to be expanded next is the tip node estimated to be most likely to be reached in play. This idea was originally proposed by Floyd (see <ref> [6] </ref>), although it does not seem to have been seriously tried. 2.
Reference: [7] <author> McAllester, David A., </author> <title> "A New Procedure for Growing Min-Max Trees," </title> <note> (to appear in Artificial Intelligence). </note>
Reference-contexts: Examples of such iterative techniques are the Berliner's B fl algorithm [2], Nilsson's "arrow" method [11], Palay's probability-based method [12], and McAllester's "conspiracy number" method <ref> [7] </ref>. The heuristic proposed in this paper is an iterative technique in the above sense. 3.3 Iterative Search Heuristics Details We now formalize how a tree is explored using an iterative search heuristic, and how estimates are "backed up" to the root. <p> However, this depends on the static evaluator returning meaningful estimates. If the static 19 evaluator were to return only constant values except at terminal positions, our scheme would perform a breadth-first search. (We observe that the scheme of McAllester <ref> [7] </ref> performs like alpha-beta search in this case.) Other penalty-based schemes are of course possible. We note two in particular: 1.
Reference: [8] <author> Nau, Dana S., </author> <title> "An Investigation of the Causes of Pathology in Games," </title> <booktitle> Artificial Intelligence 19(1982), </booktitle> <pages> 257-278. </pages>
Reference-contexts: This technique is known as iterative deepening. For familiar games (e.g. chess) as one increases d the accuracy of the value computed for the root seems to improve. However, there are "pathological" games for which increasing d seems to yield less accuracy <ref> [8, 9] </ref>. We assume our game is non-pathological. A different class of heuristics are the iterative heuristics, which "grow" the search tree one step at a time.
Reference: [9] <author> Nau, Dana S., </author> <title> "Pathology on Game Trees Revisited, and an Alternative to Minimaxing," </title> <booktitle> Artificial Intelligence 21 (1983), </booktitle> <pages> 221-244. </pages>
Reference-contexts: This technique is known as iterative deepening. For familiar games (e.g. chess) as one increases d the accuracy of the value computed for the root seems to improve. However, there are "pathological" games for which increasing d seems to yield less accuracy <ref> [8, 9] </ref>. We assume our game is non-pathological. A different class of heuristics are the iterative heuristics, which "grow" the search tree one step at a time. <p> For software implementations more development of the min-max approach is needed to reduce the computational overhead per call to the move operator. Overall, we find these experimental results very encouraging. We note, as Nau <ref> [9] </ref> points out, that there are many subtle methodological questions that arise when trying to compare heuristics by playing them off against each other, since the "evenness" of play or variation in quality of play during different portions of the game can have a dramatic influence on the results. <p> This idea was originally proposed by Floyd (see [6]), although it does not seem to have been seriously tried. 2. If we estimate for each node c the probability that c is a forced win for Max (see <ref> [9] </ref> for discussions of this idea) then we can select the tip node to expand upon which our estimate at the root depends most heavily. This can be done, in a manner similar to our min/max approximation technique, beginning with the formulas in [9]. <p> is a forced win for Max (see <ref> [9] </ref> for discussions of this idea) then we can select the tip node to expand upon which our estimate at the root depends most heavily. This can be done, in a manner similar to our min/max approximation technique, beginning with the formulas in [9]. This idea was suggested by David McAllester. 7 Open Problems 1. How should one best choose which generalized mean value functions, or penalty func tions, to use? 2.
Reference: [10] <author> Nau, Dana S., Paul Purdom, and Chun-Hung Tzeng, </author> <title> "An evaluation of two alternatives to minimax," </title> <booktitle> Proc. on Uncertainty and Probability in Artificial Intelligence (AAAI, UCLA, </booktitle> <month> August 14-16, </month> <year> 1985), </year> <pages> 232-235. </pages>
Reference-contexts: Clearly, the most careful organization and allocation of computational resources is needed to obtain expert-level play. Techniques such as alpha-beta pruning and its successors [6, 2] have been essential in reducing the computational burden of exploring a game tree. Still, new techniques are needed. Nau et. al. <ref> [10] </ref>, after much expermentation with existing methods, assert that "A method is needed which will always expand the node that is expected to have the largest effect on the value". This paper suggests such a method.
Reference: [11] <author> Nilsson, N. J., </author> <title> "Searching Problem-Solving and Game-Playing Trees for Minimal-Cost Solutions," </title> <booktitle> Proc. IFIP (1968, </booktitle> <volume> vol. 2), </volume> <pages> 1556-1562. 21 </pages>
Reference-contexts: The tree grown by an iterative heuristic need not be of uniform depth: some branches may be searched to a much greater depth than other branches. Examples of such iterative techniques are the Berliner's B fl algorithm [2], Nilsson's "arrow" method <ref> [11] </ref>, Palay's probability-based method [12], and McAllester's "conspiracy number" method [7]. <p> Otherwise a (c) is that d 2 S (c) such that b (c) 2 E d . Think of a (c) as an "arrow" from c to one of its children, such that following successive arrows leads from c to b (c). (See <ref> [11] </ref> for the origin of this "arrow" terminology.) With each node c of E we store ~v E (c), a (c), and (c) = P c (b (c)), the penalty of the best expandable tip b (c) of E c relative to the subtree E c , or else 1 if
Reference: [12] <author> Palay, A. J., </author> <title> Searching with Probabilities, </title> <publisher> (Pitman, </publisher> <address> Boston, </address> <year> 1985). </year>
Reference-contexts: The tree grown by an iterative heuristic need not be of uniform depth: some branches may be searched to a much greater depth than other branches. Examples of such iterative techniques are the Berliner's B fl algorithm [2], Nilsson's "arrow" method [11], Palay's probability-based method <ref> [12] </ref>, and McAllester's "conspiracy number" method [7]. The heuristic proposed in this paper is an iterative technique in the above sense. 3.3 Iterative Search Heuristics Details We now formalize how a tree is explored using an iterative search heuristic, and how estimates are "backed up" to the root.
Reference: [13] <author> Pearl, J., </author> <title> HEURISTICS: Intelligent Search Strategies for Computer Problem Solving, </title> <publisher> (Addison-Wesley, </publisher> <address> Reading Massachusetts, </address> <year> 1984). </year>
Reference: [14] <author> Roizen, I., and J. Pearl, </author> <title> "A Minimax Algorithm Better than Alpha-Beta? Yes and No," </title> <booktitle> Artificial Intelligence 21 (1983), </booktitle> <pages> 199-220. </pages>
Reference: [15] <author> Stockman, G., </author> <title> "A Minimax Algorithm Better than Alpha-Beta?," </title> <booktitle> Artificial Intelligence 12 (1979), </booktitle> <pages> 179-196. 22 </pages>
References-found: 15

