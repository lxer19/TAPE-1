URL: http://www.icsi.berkeley.edu/~morgan/PAPERS/rapchap.ps
Refering-URL: http://www.icsi.berkeley.edu/~morgan/pubs.html
Root-URL: http://www.icsi.berkeley.edu
Title: Neurocomputing on the RAP  
Author: NELSON MORGAN JAMES BECK PHIL KOHN JEFF BILMES 
Address: Berkeley, CA  
Affiliation: International Computer Science Institute,  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Asanovic, B. Kingsbury, J. Beck, N. Morgan, and J. Wawrzynek, SPerT: </author> <title> A Microcoded SIMD Array for Synthetic Perceptron Training, </title> <institution> International Computer Science Institute Technical Report, </institution> <note> In Prep </note>
Reference-contexts: In particular, our target network is extremely sparse, and this will need to be incorporated in the design from the beginning. We are currently in the final design stages for a preliminary microcoded processor chip that is a simplified form of what we will need for this machine <ref> [1] </ref>. 10 SUMMARY Ring architectures have been shown to be a good match to a variety of signal processing and connectionist algorithms.
Reference: [2] <author> K. Asanovic and N. Morgan, </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks , International Computer Science Institute TR-91-036, </title> <year> 1991. </year>
Reference-contexts: The new system will require much higher levels of performance, so custom VLSI designs will be necessary. These designs can capitalize on moderate fixed point precision requirements, which we have verified for our connectionist speech training algorithms <ref> [2] </ref>. As with the RAP's DSP, however, the CNS-1's custom processor will implement a general-purpose instruction set (albeit at a lower rate than the vector-style connection ist operations). 3. Network connectivity the RAP architecture and software was optimized for fully connected layered networks.
Reference: [3] <author> H. Bourlard and N. Morgan, </author> <title> Merging Multilayer Perceptrons and Hidden Markov Models: Some Experiments in Continuous Speech Recognition, </title> <booktitle> International Computer Science Institute TR-89-033, 1989. Neurocomputing on the RAP 21 </booktitle>
Reference: [4] <author> H. Bourlard, N. Morgan, and C. Wellekens, </author> <title> Statistical Inference in Multilayer Perceptrons and Hidden Markov Models with Applications in Continuous Speech Recognition, </title> <booktitle> in Neuro Computing, Algorithms, Architectures and Applications, NATO ASI Series, </booktitle> <year> 1990. </year>
Reference: [5] <author> H. Bourlard and N. Morgan, </author> <title> Merging Multilayer Perceptrons and Hidden Markov Models: Some Experiments in Continuous Speech Recognition, in Neural Networks: Advances and Applications, </title> <editor> E. Gelenbe (Ed.), </editor> <publisher> Elsevier Science Publichers B.V. (North-Holland), </publisher> <year> 1991. </year>
Reference: [6] <author> H. Bourlard and C. Wellekens, </author> <title> Links Between Markov Models and Multilayer Perceptrons, </title> <booktitle> in Advances in Neural Information Processing Systems 1, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> pp.502-510, </address> <year> 1989. </year>
Reference-contexts: In either case, the neural network is trained by back-propagation [22][24] augmented by a generalization-based stopping criterion [17]. It can be shown <ref> [6] </ref> that the net outputs can be trained to estimate emission probabilities for the Viterbi decoding step of an HMM speech recognizer.
Reference: [7] <author> J. Feldman, M. Fanty, N. Goddard, and K. Lynne, </author> <title> Computing with Structured Connectionist Networks in Communications of the ACM, </title> <year> 1988. </year>
Reference-contexts: In the meanwhile, we have begun work on the design of VLSI elements for a successor machine that will have much higher performance, while retaining the flexibility of the current system. 4 Architectural Considerations Artificial neural networks (ANNs) frequently do not have complete connectivity <ref> [7] </ref>, even between layers of a feedforward network [15]. Nonetheless, an extremely useful subclass of these networks uses nonsparse connectivity between layers of "units", which are (for the most common case) nonlinear functions of the weighted sums of their inputs.
Reference: [8] <author> M. Franzini, K. Lee, and A. Waibel, </author> <title> Connectionist Viterbi Training: </title>
References-found: 8

