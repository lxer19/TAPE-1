URL: http://www.caip.rutgers.edu/~prabhura/reports/reportcomplete.ps
Refering-URL: 
Root-URL: 
Phone: (908) 445-4208  
Title: SPEAKER AND ENVIRONMENT ADAPTATION IN CONTINUOUS SPEECH RECOGNITION  
Author: Prabhu Raghavan 
Address: CoRE Building Frelinghuysen Road  New Jersey Piscataway, New Jersey 08855-1390  
Affiliation: COMPUTER AIDS FOR INDUSTRIAL PRODUCTIVITY  Rutgers The State University of  
Date: 8 June 1998  
Abstract: Technical Report CAIP-TR-227 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> N. Laird A. Dempster and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Soc., </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Problem 3 : The parameter estimation is one of the most difficult problems in HMMs. An iterative procedure called the "Baum-Welch re-estimation" is used to obtain a Maximum Likelihood (ML) fit of the training data. This procedure is also 7 known as the EM (Expectation Maximization) method <ref> [15, 1] </ref>. This procedure however requires an initial estimate of the HMM to start the iteration. The initialization procedure described here is the one used in the HTK HMM Toolkit [35].
Reference: 2. <author> S. M. Ahadi-Sarkani. </author> <title> Bayesian and Predictive Techniques for Speaker Adaptation. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: This is a subject of major study in itself and is beyond the scope of this report. A more detailed study of prior parameter estimation can be found in <ref> [2] </ref>. 5. Evaluation of the Adaptation Techniques The adaptation techniques studied in Chapter 3 and 4 are experimentally used here to adapt an HMM model trained on clean speech to: * different speakers, * data from a reverberant environment at different reverberance levels.
Reference: 3. <author> N. Aoshima. </author> <title> Computer-generated pulse signal applied for sound measurement. </title> <journal> J. Acous. Soc. Am, </journal> <volume> 69(5) </volume> <pages> 1484-1488, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Position 0 was at (2.07,2.20), Position 1 was at (4.51,2.20), Position 2 was at (2.07, 4.54) and Position 3 was at (4.51, 4.54). The loudspeaker was placed at a height of 1.4 meters. 28 Recordings with a time stretched pulse (TSP) <ref> [3] </ref> as the stimulus were made in order to determine the acoustic transfer function of the Varechoic Chamber for each location. A separate recording was made for each position. The appropriate room responses were obtained by deconvolving the TSP from these recordings.
Reference: 4. <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: Choosing the new model parameters to maximize the auxiliary function increases the value of the objective function (unless at the maximum). Therefore successive iterations of maximizing the auxiliary function successively maximizes the objective function <ref> [4, 30] </ref>. <p> The clustering may be bottom-up or top down. The symmetric divergence function can be used for clustering [22], D sym = 2 1 2 + 1 1 ( 1 2 ) 0 ( 1 2 )( 1 2 ): (4.16) 4.2 MAP Adaptation The ML estimates, using Baum-Welch <ref> [4] </ref> during training are estimated purely on the basis of the training data. No prior information is used for this, as an assump 20 different phone HMMs [30]. tion is made that such information is not available.
Reference: 5. <author> S. S. Chen and P. DeSouza. </author> <title> Speaker adaptation by correlation (ABC). </title> <booktitle> In Proc. DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: This method is studied in detail in the next chapter. Methods similar to the MLLR are the Maximum Likelyhood Non-linear Adaptation (MLNA) [34] and Adaptation By Correlation (ABC) <ref> [5] </ref>. The first method, MLNA, uses a second-order nonlinear transformation computed in an ML sense, instead of the first-order that MLLR uses. It is observed in [34] that MLNA does not provide any significant improvent over the MLLR. <p> This approach is claimed to be more stable compared to the MLLR for small amounts of adaptation data (~ 5s) <ref> [5] </ref>. 3.2 Model-Based Techniques These methods modify the model directly, without using any form of transformation. An important method of this approach is the Maximum a posteriori estimation [27, 12, 14, 16].
Reference: 6. <author> L. Jin D-S. Yuk, C. Che and Q. Lin. </author> <title> Environment-independent continuous speech recognition using neural networks and hidden Markov models. </title> <booktitle> In Proc. ICASSP, </booktitle> <year> 1996. </year>
Reference-contexts: The MLP was used to map different speaker clusters to one "golden cluster" which contained the maximum number of speakers. This algorithm effectively normalized the speaker to the golden cluster on which the model was trained thus improving recognition accuracies. Nakamura et al [19] and Yuk et al <ref> [6] </ref> also applied the neural net non-linear mapping for speaker and environmental adaptation. These also used stereo data. The neural net was trained, like the affine transform to map/move the input data to the means of their respective Gaussian pdfs.
Reference: 7. <author> M. H. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: No prior information is used for this, as an assump 20 different phone HMMs [30]. tion is made that such information is not available. But if such information is available, Bayesian methods present an optimal way to use them <ref> [7] </ref>. The application of Bayesian inference is achieved by assuming that the parameters to be estimated in adaptation are random and that the priors are themselves distributions. This idea was first used in Brown's work in speaker adaptation for connected-digit applications [27]. <p> The prior parameters for are chosen from a family of conjugate priors <ref> [7] </ref>, that are a family of distributions which have the same distribution as the parameter . <p> t 2 x (A.9) In this case, = t 2 2 1 This is true because, if we assume a marginal density p (x) is greater than zero and that the factorization theorem holds then if t = T (x) then p ( j x) = p ( j t) <ref> [7] </ref> 37 A.1 Computation of prior parameters This is a very critical part for the success of the MAP estimate.
Reference: 8. <author> E. Eide and H. Gish. </author> <title> A parametric approach to vocal tract length normalization. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 233-236, </pages> <address> Atlanta, </address> <month> May </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: Since CDCN compensates the incoming cepstra, model-based techniques like the MLLR can still used to compensate the model to the already compensated cepstra [28]. Another approach, one that falls under speaker normalization, is that of Vocal Tract Length Normalization <ref> [8] </ref>. The differences in vocal tract lengths of different speakers is reduced by scaling the warping function of the mel-frequency filter-bank. If the factor is 1, the original mel-scale is obtained.
Reference: 9. <author> M. J. F. Gales and S. Young. </author> <title> An improved approach to hidden Markov model decomposition of speech and noise. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 233-236, </pages> <year> 1992. </year>
Reference-contexts: MAP uses the concept of prior distributions to mitigate this problem. MAP is also studied in detail in the next chapter. Another important technique in this category is the Parallel Model Combination (PMC) algorithm <ref> [9, 10] </ref>. The algorithm, as the name suggests, adds two models to provide a robust composite model. The two models in question are the clean speech model and the noise model. The algorithm stores statistics, rather than extended data, about a background noise.
Reference: 10. <author> M. J. F. Gales and S. Young. </author> <title> PMC for speech recognition in additive and convolutional noise. </title> <type> Technical Report CUED/F-INFENG/TR. 154, </type> <institution> Cambridge University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: MAP uses the concept of prior distributions to mitigate this problem. MAP is also studied in detail in the next chapter. Another important technique in this category is the Parallel Model Combination (PMC) algorithm <ref> [9, 10] </ref>. The algorithm, as the name suggests, adds two models to provide a robust composite model. The two models in question are the clean speech model and the noise model. The algorithm stores statistics, rather than extended data, about a background noise. <p> The resulting composite model has been found to have improved recognition performance. Modifications have also been done on this algorithm for comensation for channel distortion <ref> [10] </ref>. 3.3 Adaptation Methodologies The above algorithms themselves can be applied in different ways achieving different performance levels. These are broadly seen as Supervised and Unsupervised adaptation. In the supervised adapatation mode, speech data with a correct transcription is provided.
Reference: 11. <author> M. J. F. Gales and S. Young. </author> <title> The generation and use of regression class trees for MLLR adaptation. </title> <type> Technical Report CUED/F-INFENG/TR. 263, </type> <institution> Cambridge University, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: In this case, however, there may be matrices which may not have enough data to robustly estimate a transform. In such cases, the data can be pooled to obtain a global transform for all mixtures. Alternatively, we can use regression class trees <ref> [11] </ref>. This is discussed later in this section. The W matrix is computed to maximize the likelihood of the adapted models generating the adapted data.
Reference: 12. <author> J-L. Gauvain and C-H. Lee. </author> <title> Bayesian learning of gaussian mixture densities of hidden markov models. </title> <booktitle> In DARPA Speech and Natural Lang. Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: This approach is claimed to be more stable compared to the MLLR for small amounts of adaptation data (~ 5s) [5]. 3.2 Model-Based Techniques These methods modify the model directly, without using any form of transformation. An important method of this approach is the Maximum a posteriori estimation <ref> [27, 12, 14, 16] </ref>. MAP is different from ML in the sense that the MAP estimate of the parameter MAP is given by MAP = arg max P r ( j x); (3.2) where P r () is the prior distribution of the parameter . <p> This may result in poorly adapted models. The solution is to use the adaptation data to compute transforms for groups of similar data. The MLLR and the ABC achieve this. 16 4. MLLR and MAP The two most successful algorithms for speaker adaptation are the MAP <ref> [12] </ref> and MLLR [24]. These are discussed in detail in this chapter. 4.1 MLLR Adaptation The MLLR adaptation of the mean computes a transformation to move the model means to fit the adaptation data. <p> This idea was first used in Brown's work in speaker adaptation for connected-digit applications [27]. More recently Gauvain and Lee have applied it to isolated word recognition and continuous speech recognition <ref> [12, 13, 14] </ref>. Bayesian Estimation has been used to update all parameters but in this report we only consider the MAP estimate of the mean. <p> As in <ref> [12] </ref>, we define the prior estimates for an HMM state with multiple mixtures as: = m=1 t 2 = m=1 2 = m=1 m (A.12) where, w m is the weight of the m th mixture and m is the mean of the m th mixture of the state. 38 Appendix
Reference: 13. <author> J-L. Gauvain and C-H. Lee. </author> <title> Bayesian learning for hidden markov models with gaussian mixture state observation densities. </title> <journal> Speech Communication, </journal> (11):205-213, 1992. 
Reference-contexts: This idea was first used in Brown's work in speaker adaptation for connected-digit applications [27]. More recently Gauvain and Lee have applied it to isolated word recognition and continuous speech recognition <ref> [12, 13, 14] </ref>. Bayesian Estimation has been used to update all parameters but in this report we only consider the MAP estimate of the mean.
Reference: 14. <author> J-L. Gauvain and C-H. Lee. </author> <title> Maximum a posteriori estimation form mul--tivariate gaussian mixture observations of markov chains. </title> <journal> IEEE Trans. SAP, </journal> <volume> 2(2) </volume> <pages> 291-298, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This approach is claimed to be more stable compared to the MLLR for small amounts of adaptation data (~ 5s) [5]. 3.2 Model-Based Techniques These methods modify the model directly, without using any form of transformation. An important method of this approach is the Maximum a posteriori estimation <ref> [27, 12, 14, 16] </ref>. MAP is different from ML in the sense that the MAP estimate of the parameter MAP is given by MAP = arg max P r ( j x); (3.2) where P r () is the prior distribution of the parameter . <p> This idea was first used in Brown's work in speaker adaptation for connected-digit applications [27]. More recently Gauvain and Lee have applied it to isolated word recognition and continuous speech recognition <ref> [12, 13, 14] </ref>. Bayesian Estimation has been used to update all parameters but in this report we only consider the MAP estimate of the mean. <p> Another choice for the MAP mean is <ref> [14] </ref>: = t + i=allframes fl i (t) + i=allframes fl i (t)o t t + i=allframes fl i (t) : (4.21) As can be expected, the MAP adaptation technique depends on the choice of the prior parameters.
Reference: 15. <author> K.E. Basford G.J. McLachlan. </author> <title> Mixture Models Inference and applications to clustering. </title> <publisher> Marcel Dekker Inc., </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Problem 3 : The parameter estimation is one of the most difficult problems in HMMs. An iterative procedure called the "Baum-Welch re-estimation" is used to obtain a Maximum Likelihood (ML) fit of the training data. This procedure is also 7 known as the EM (Expectation Maximization) method <ref> [15, 1] </ref>. This procedure however requires an initial estimate of the HMM to start the iteration. The initialization procedure described here is the one used in the HTK HMM Toolkit [35].
Reference: 16. <author> Y. Grenier. </author> <title> Speaker adaptation through cannonical correlation analysis. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 888-891, </pages> <year> 1980. </year>
Reference-contexts: This approach is claimed to be more stable compared to the MLLR for small amounts of adaptation data (~ 5s) [5]. 3.2 Model-Based Techniques These methods modify the model directly, without using any form of transformation. An important method of this approach is the Maximum a posteriori estimation <ref> [27, 12, 14, 16] </ref>. MAP is different from ML in the sense that the MAP estimate of the parameter MAP is given by MAP = arg max P r ( j x); (3.2) where P r () is the prior distribution of the parameter .
Reference: 17. <author> J. H. L. Hansen and L. M. Arslan. </author> <title> Robust feature-estimation and objective quality assessment for noisy speech recognition using the credit card corpus. </title> <journal> IEEE Trans. of Speech and Audio Processing, </journal> <volume> 3(3) </volume> <pages> 169-184, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: However, the current state-of-the-art is still limited in that mismatches between the training and testing speech lead to a rapid fall in word accuracies. It is well known that wide variations exist in the speech signal <ref> [17] </ref>. The variations can be categorized as speaker and environmental variability. These variations are studied in the next section. 1.1 Sources of Variations in the Speech Signal Variations in the speech signal can be due to the speaker or the environment. <p> While humans can comfortably cope with these problems, a robust recognition system which automatically adopts these variations is a major challenge. The presence of background noise may also alter the speech a speaker uses in order to communicate effectively, also known as the Lombard effect <ref> [17] </ref>. The speaker may also experience stress due to anger, fear, or due other emotions which may affect speech. Correlated noise sources like reverberation also cause heavy degradation in recognition accuracies. In the paper on quality assessment for noisy speech [17], Hansen and Arslan introduce the following composite model of degraded <p> order to communicate effectively, also known as the Lombard effect <ref> [17] </ref>. The speaker may also experience stress due to anger, fear, or due other emotions which may affect speech. Correlated noise sources like reverberation also cause heavy degradation in recognition accuracies. In the paper on quality assessment for noisy speech [17], Hansen and Arslan introduce the following composite model of degraded speech: y (n) = s (n) fi fi WORKLOAD STRESS LOMBARD EFFECTfd 1 g o h MIKE (n) + d 2 (n) h CHANNEL (n) where d 1 (n) is the acoustic background noise, LOMBARD EFFECTfd 1 g is the <p> The application of the same algorithm for distant-talking speech recognition is also described. The use of microphone array processing for this environment is also explored. Chapter 6 presents the conclusions and plans for future work. quality of speech in a speech recognition system <ref> [17] </ref> 3 2. Hidden Markov Models in Speech Recognition 2.1 Speech Recognition The task of a speech recognition system is to produce an estimate of the word string, called the hypothesis, associated with a given speech signal.
Reference: 18. <author> A. J. Hewett. </author> <title> Training and Speaker Adaptation in Template-based Speech Recognition. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year> <note> Reference from C. J. Leg-etter Thesis. </note>
Reference-contexts: Jaschul used the simple Affine transformation for transforming data belonging to a class of vowel sounds [21]. Both global and Multiple transforms for vowel classes were estimated and though the latter was computationally expensive, it was found to yield better results. Hewett <ref> [18] </ref> furthered this idea by using a dynamic time warping to align mismatches and then estimating a transform that takes into account the frequency shift and scaling. This transform was also investigated in [31]. These methods needed stereo data for adaptation.
Reference: 19. <author> X. D. Huang. </author> <title> Minimizing speaker variation effects for speaker-independent speech recognition. </title> <booktitle> In DARPA Speech & Natural Lang. Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The adaptation data is Viterbi force-aligned using the correct transcription to determine which Gaussian mixture is most likely to have generated a particular cepstral vector. The input data is then mapped feature-for-feature to means of the HMMs using a transformation. The transform is computed using Least-squares regression. Huang <ref> [19] </ref> used a non-linear mapping function, instead. A Multi-Layer Perceptron 13 (MLP) was used to to reduce speaker variability. The MLP was used to map different speaker clusters to one "golden cluster" which contained the maximum number of speakers. <p> The MLP was used to map different speaker clusters to one "golden cluster" which contained the maximum number of speakers. This algorithm effectively normalized the speaker to the golden cluster on which the model was trained thus improving recognition accuracies. Nakamura et al <ref> [19] </ref> and Yuk et al [6] also applied the neural net non-linear mapping for speaker and environmental adaptation. These also used stereo data. The neural net was trained, like the affine transform to map/move the input data to the means of their respective Gaussian pdfs.
Reference: 20. <author> X. D. Huang and K. F. Lee. </author> <title> On speaker-independent, speaker-dependent and speaker adaptive speech processing. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 877-880, </pages> <address> Toronto, </address> <year> 1991. </year>
Reference-contexts: This leads to speech models that may not accurately model speech units for a particular speaker but are good on average. A comparison of SI systems to speaker dependent (SD) systems shows that the SD system may have 2-3 times lower error rates compared to an SI system <ref> [20] </ref>. In an experiment in [22], SI and SD error rates tested on the same Resource Management data showed that SD had an error rate of only 1.8% compared to an SI error rate of 4.8%.
Reference: 21. <author> J. Jaschul. </author> <title> Speaker adaptation by a linear transformation with optimized parameters. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 1657-1670, </pages> <year> 1982. </year>
Reference-contexts: These transformations can be linear like the affine transform or be non-linear like a neural network. 3.1.1 Feature-space transformations Various types of mapping functions have been investigated in the past few years. Jaschul used the simple Affine transformation for transforming data belonging to a class of vowel sounds <ref> [21] </ref>. Both global and Multiple transforms for vowel classes were estimated and though the latter was computationally expensive, it was found to yield better results.
Reference: 22. <author> C. J. Leggetter. </author> <title> Improved Acoustic Modelling for HMMs using Linear Transformations. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1995. </year>
Reference-contexts: the M MFCCs from filter-bank output is, P p p ; i = 1; 2; : : : ; M; (2.23) where c i is the i th MFCC, p is the number of filter bank channels and X j is the log of the j th filter bank output <ref> [22, 30] </ref>. 10 The mel-scale is an adjustment which is done to match the perception characteris-tics of the human ear [30]. It has been shown that the use of these features results in improved performance in speech recognition systems. <p> A comparison of SI systems to speaker dependent (SD) systems shows that the SD system may have 2-3 times lower error rates compared to an SI system [20]. In an experiment in <ref> [22] </ref>, SI and SD error rates tested on the same Resource Management data showed that SD had an error rate of only 1.8% compared to an SI error rate of 4.8%. <p> Therefore successive iterations of maximizing the auxiliary function successively maximizes the objective function [4, 30]. If the matrix is shared by W s is shared by R states fs 1 ; s 1 ; : : : ; s R g, maximizing Eq. (4.5) would give <ref> [22] </ref>, T X T X fl s r (t)C 1 0 = t=1 r=1 s r W s d s r d s r This is used to compute the matrix W which involves computation of the ff fi forward-backward probabilities. These are used to compute the state occupation probabilities. <p> The clustering may be bottom-up or top down. The symmetric divergence function can be used for clustering <ref> [22] </ref>, D sym = 2 1 2 + 1 1 ( 1 2 ) 0 ( 1 2 )( 1 2 ): (4.16) 4.2 MAP Adaptation The ML estimates, using Baum-Welch [4] during training are estimated purely on the basis of the training data.
Reference: 23. <author> C. J. Leggetter and P. C. Woodland. </author> <title> Speaker adaptation of continuous density HMMs using multivariate linear regression. </title> <booktitle> In Proc. ICSLP, </booktitle> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: The obvious disadvantage of this approach is the generation of multiple models which can weigh heavily on the disk space requirements. 14 A very effective technique in this category, and one of the most popular, is the Maximum Likelihood Linear Regression <ref> [24, 23] </ref>. The method computes an Affine transformation in an ML sense to move model parameters so that the resulting model is better suited to the speaker being adapted. This method is studied in detail in the next chapter.
Reference: 24. <author> C. J. Leggetter and P. C. Woodland. </author> <title> Speaker adaptation of HMMs using linear regression. </title> <type> Technical Report CUED/F-INFENG/TR. 181, </type> <institution> Cambridge University Engineering Dept., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The obvious disadvantage of this approach is the generation of multiple models which can weigh heavily on the disk space requirements. 14 A very effective technique in this category, and one of the most popular, is the Maximum Likelihood Linear Regression <ref> [24, 23] </ref>. The method computes an Affine transformation in an ML sense to move model parameters so that the resulting model is better suited to the speaker being adapted. This method is studied in detail in the next chapter. <p> This may result in poorly adapted models. The solution is to use the adaptation data to compute transforms for groups of similar data. The MLLR and the ABC achieve this. 16 4. MLLR and MAP The two most successful algorithms for speaker adaptation are the MAP [12] and MLLR <ref> [24] </ref>. These are discussed in detail in this chapter. 4.1 MLLR Adaptation The MLLR adaptation of the mean computes a transformation to move the model means to fit the adaptation data. <p> These are used to compute the state occupation probabilities. In case the matrix is used at a mixture level, fl s r (t) is replaced by the mixture occupation probability. Eq. (4.6) is simplified to a more usable form and is quoted from <ref> [24] </ref>, T X T X fl s r (t)C 1 0 = r=1 where V (r) , an n fi n matrix, is the state distribution inverse covariance matrix scaled by the state occupation probability, V (r) = t=1 s r ; (4.8) and D (r) is the outer product of
Reference: 25. <author> S. Cox M. Lincoln and S. Ringland. </author> <title> A fast method of speaker normalization using formant estimation. </title> <booktitle> In Proc. Eurospeech, </booktitle> <year> 1997. </year>
Reference-contexts: If the factor is 1, the original mel-scale is obtained. A scaling factor less than one stretches the mel-frequency bins to cover higher frequency elements while a factor less than one compresses it. In <ref> [25] </ref>, the warping scale was automatically determined while earlier methods used a more computationally intensive search using multiple recognition passses.
Reference: 26. <author> J. Bernstien P. Price, W. Fisher and D. Pallett. </author> <title> A database for continuous speech recognition in a 1000-word domain. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 651-654, </pages> <year> 1988. </year>
Reference-contexts: The models are adapted to in both supervised and unsupervised modes. The experiments were performed on the 993 word DARPA Resource Management task <ref> [26] </ref>. Cross-word triphone models were used which were generated using tree-based clustering for unseen triphones. There were 3 emitting states per triphone and 4 mixtures per state. These were generated using the HTK HMM Toolkit [35].
Reference: 27. <author> J.C. Spohrer P.F. Brown, C.H.Lee. </author> <title> Bayesian adaptation in speech recognition. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> number 2, </volume> <pages> pages 761-764. </pages> <publisher> IEEE, </publisher> <year> 1983. </year> <month> 44 </month>
Reference-contexts: This approach is claimed to be more stable compared to the MLLR for small amounts of adaptation data (~ 5s) [5]. 3.2 Model-Based Techniques These methods modify the model directly, without using any form of transformation. An important method of this approach is the Maximum a posteriori estimation <ref> [27, 12, 14, 16] </ref>. MAP is different from ML in the sense that the MAP estimate of the parameter MAP is given by MAP = arg max P r ( j x); (3.2) where P r () is the prior distribution of the parameter . <p> The application of Bayesian inference is achieved by assuming that the parameters to be estimated in adaptation are random and that the priors are themselves distributions. This idea was first used in Brown's work in speaker adaptation for connected-digit applications <ref> [27] </ref>. More recently Gauvain and Lee have applied it to isolated word recognition and continuous speech recognition [12, 13, 14]. Bayesian Estimation has been used to update all parameters but in this report we only consider the MAP estimate of the mean.
Reference: 28. <author> B. Raj. R. M. Stern, V. N. Parikh. </author> <title> Speaker adaptation and enviornmental compensation for the 1996 broadcast news task. </title> <booktitle> In Proc. DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: This method was used to move the input cepstral data toward the VQ codewords of the training environment. Since CDCN compensates the incoming cepstra, model-based techniques like the MLLR can still used to compensate the model to the already compensated cepstra <ref> [28] </ref>. Another approach, one that falls under speaker normalization, is that of Vocal Tract Length Normalization [8]. The differences in vocal tract lengths of different speakers is reduced by scaling the warping function of the mel-frequency filter-bank. If the factor is 1, the original mel-scale is obtained.
Reference: 29. <author> Y. Oshima T. M. Sullivan R. M. Stern, F-H Liu and A. Acero. </author> <title> Multiple approaches to robust speech recognition. </title> <booktitle> In Darpa Speech and Natural Language Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: These also used stereo data. The neural net was trained, like the affine transform to map/move the input data to the means of their respective Gaussian pdfs. An approach similar to the affine transform for adaptation is the SNR-Dependent Cepstral Normalization (SDCN) <ref> [29] </ref>. This computes only the bias term (effectively setting the matrix term in the affine transform to an identity matrix). <p> But the power of this method is that these bias averages are computed for various SNRs and the choice of the bias term for a particular test cepstral vector depends on the instantenous SNR of that vector. A more robust approach is the Codeword Dependent Cepstral Normalization (CDCN) <ref> [29] </ref>. This approach is very similar to the ones discussed above except that it uses an ML approach to compute the transformations. The computation of the transformation thus does not require "stereo" data.
Reference: 30. <author> L. Rabiner and B. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: Traditional approaches use various methods such as template matching, or vector quantization, but HMMs are the most widely used technique <ref> [30] </ref>. Recently, connectionist approaches such as neural networks have been found to offer comparable performance. The rest of this report, however will deal entirely with the HMM technique for speech recognition. <p> The discussion assumes that the HMM is being used for word recognition and later, it be extended for continuous speech. A thorough discussion of these can be found in <ref> [30] </ref>. Here the salient aspects of HMMs are quoted, 1. <p> In a compact notation the model is denoted as, = (A; B; ): 2.2.2 The Three Basic Problems for HMMs As described in <ref> [30] </ref>, three fundamental problems of interest for the use of HMMs in speech recognition are: 1. Given the observation sequence O = (o 1 ; o 2 ; : : : ; o T ) and = (A; B; ) how do we compute P (O j ) efficiently. 2. <p> the M MFCCs from filter-bank output is, P p p ; i = 1; 2; : : : ; M; (2.23) where c i is the i th MFCC, p is the number of filter bank channels and X j is the log of the j th filter bank output <ref> [22, 30] </ref>. 10 The mel-scale is an adjustment which is done to match the perception characteris-tics of the human ear [30]. It has been shown that the use of these features results in improved performance in speech recognition systems. <p> (2.23) where c i is the i th MFCC, p is the number of filter bank channels and X j is the log of the j th filter bank output [22, 30]. 10 The mel-scale is an adjustment which is done to match the perception characteris-tics of the human ear <ref> [30] </ref>. It has been shown that the use of these features results in improved performance in speech recognition systems. Further dynamic information from the cepstra can be obtained from time-derivative information. The first order time-derivatives are called MF CCs and the second order MF CCs. <p> Choosing the new model parameters to maximize the auxiliary function increases the value of the objective function (unless at the maximum). Therefore successive iterations of maximizing the auxiliary function successively maximizes the objective function <ref> [4, 30] </ref>. <p> No prior information is used for this, as an assump 20 different phone HMMs <ref> [30] </ref>. tion is made that such information is not available. But if such information is available, Bayesian methods present an optimal way to use them [7].
Reference: 31. <author> P. Raghavan. </author> <title> Speaker adaptation of CDHMMs using the affine transform. </title> <month> May </month> <year> 1997. </year>
Reference-contexts: Hewett [18] furthered this idea by using a dynamic time warping to align mismatches and then estimating a transform that takes into account the frequency shift and scaling. This transform was also investigated in <ref> [31] </ref>. These methods needed stereo data for adaptation. The adaptation data is Viterbi force-aligned using the correct transcription to determine which Gaussian mixture is most likely to have generated a particular cepstral vector. The input data is then mapped feature-for-feature to means of the HMMs using a transformation.
Reference: 32. <author> P. Raghavan and C. Che. </author> <title> Speaker adaptation in speech recognition. </title> <booktitle> In 134th Meeting of the Acoustical Society of America, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: Speaker Normalization techniques operate on the feature domain to achieve similar results. In this report, those techniques are also viewed as falling into the category of Speaker Adaptation. Speaker Adaptation can be broadly classified into two types <ref> [32] </ref>: Transformation-based and Model-based techniques. These techniques move the parameters of the model or modify the input features to get an improved WER by removing or adapting to the speaker's variation.
Reference: 33. <author> R. J. Renomeron. </author> <title> Spatially selective sound capture for teleconferencing systems. </title> <type> Master's thesis, </type> <institution> Rutgers University, Dept. Electrical and Computer Engineering, </institution> <address> New Brunswick, NJ, </address> <month> October </month> <year> 1997. </year> <note> Also CAIP Technical Report No. TR-219, </note> <year> 1997. </year>
Reference-contexts: The microphone array has additional processing which significantly improves the SNR of the speech. This is the Matched Filter Array (MFA) processing <ref> [33] </ref> 5.2.1 MFA Processing Sound captured through a microphone, in any environment, can be modeled as application of a transfer function on the sound. <p> The denotes time-domain convolution. The MFA algorithm <ref> [33] </ref> consists of filtering the input signal obtained from each microphone with the time reverse of the focus-to-sensor impulse response, where the focus is the focal point of the microphone array. <p> It has been shown <ref> [33] </ref> that the MFA has a distinct advantage over simple beam-forming in that the MFA can remove reverberation from a captured signal. The SNR improvement has been shown to be proportional to the number of sensors instead of the number of reflections in the case of beam forming. <p> The MFA has also been shown to reject signals which are not on the focal point. The MFA algorithm was shown to have average improvements as much as 6:39dB, in experiments performed in <ref> [33] </ref>, for different reverberation times. 5.2.2 Capture of the Reverberant RM Speech To simulate effectively the degradation on speech due to distance talking and reverberation, like a conference room environment, several acoustical experiments were performed using speech data collected at the Varechoic Chamber at Lucent Technologies in Murray Hill, NJ. <p> Speech files were played through loudspeakers and captured by an 8-element microphone array and processed off-line. The experiments in the Varechoic Chamber will be explained first, followed by the experiment in the "real" conference room. All the data collected was done in <ref> [33] </ref>, and used for speech recognition experiments in this report. The description of the Varechoic chamber is block-quoted verbatim from [33]. 27 5.2.2.1 Varechoic Chamber The Varechoic Chamber [33] is a 6.7 fi 6.1 fi 2.9 meter room with double wall construction for insulation from the outside environment. <p> The experiments in the Varechoic Chamber will be explained first, followed by the experiment in the "real" conference room. All the data collected was done in <ref> [33] </ref>, and used for speech recognition experiments in this report. The description of the Varechoic chamber is block-quoted verbatim from [33]. 27 5.2.2.1 Varechoic Chamber The Varechoic Chamber [33] is a 6.7 fi 6.1 fi 2.9 meter room with double wall construction for insulation from the outside environment. The interior of the room is covered with double sliding stainless steel panels with perforations. <p> All the data collected was done in <ref> [33] </ref>, and used for speech recognition experiments in this report. The description of the Varechoic chamber is block-quoted verbatim from [33]. 27 5.2.2.1 Varechoic Chamber The Varechoic Chamber [33] is a 6.7 fi 6.1 fi 2.9 meter room with double wall construction for insulation from the outside environment. The interior of the room is covered with double sliding stainless steel panels with perforations. The panels are backed by acoustic absorbing material.
Reference: 34. <author> B. Raj V. N. Parikh and R. M. Stern. </author> <title> Speaker adaptaion and enviornmental compensation for the 1996 broadcast news task. </title> <booktitle> In Proc. DARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: The method computes an Affine transformation in an ML sense to move model parameters so that the resulting model is better suited to the speaker being adapted. This method is studied in detail in the next chapter. Methods similar to the MLLR are the Maximum Likelyhood Non-linear Adaptation (MLNA) <ref> [34] </ref> and Adaptation By Correlation (ABC) [5]. The first method, MLNA, uses a second-order nonlinear transformation computed in an ML sense, instead of the first-order that MLLR uses. It is observed in [34] that MLNA does not provide any significant improvent over the MLLR. <p> Methods similar to the MLLR are the Maximum Likelyhood Non-linear Adaptation (MLNA) <ref> [34] </ref> and Adaptation By Correlation (ABC) [5]. The first method, MLNA, uses a second-order nonlinear transformation computed in an ML sense, instead of the first-order that MLLR uses. It is observed in [34] that MLNA does not provide any significant improvent over the MLLR. It could be concluded that a first-order nonlinear transformation, like the Affine transform, sufficiently captures the speaker/ environment variability.
Reference: 35. <author> S. J. Young. </author> <title> The HTK Hidden Markov Model Toolkit V2.0. </title> <address> Cambridge University Engg. Deppt. </address>
Reference-contexts: This procedure is also 7 known as the EM (Expectation Maximization) method [15, 1]. This procedure however requires an initial estimate of the HMM to start the iteration. The initialization procedure described here is the one used in the HTK HMM Toolkit <ref> [35] </ref>. <p> The experiments were performed on the 993 word DARPA Resource Management task [26]. Cross-word triphone models were used which were generated using tree-based clustering for unseen triphones. There were 3 emitting states per triphone and 4 mixtures per state. These were generated using the HTK HMM Toolkit <ref> [35] </ref>. The language model was word-pair grammar. 5.1 Speaker Adaptation on Clean RM Data As a first step, to check the effectiveness of the different adaptation algorithms, supervised adaptation is performed successively on different amounts of adaptation data.
Reference: 36. <author> S. J. Young. </author> <title> The general use of tying in phoneme-based HMM speech recog-nizers. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 569-572, </pages> <year> 1992. </year> <month> 45 </month>
Reference-contexts: Tied parameters thus see more training data and can be robustly estimated. Tying can be done at a phone level (i.e. at the HMM level), at the state level, at the mixture level and even at the parameter level. Tying is usually done based on linguistic knowledge <ref> [36] </ref>. 9 2.3 Parameterization of Speech The first stage of any digital processing system is the conversion of speech to digital form.
References-found: 36

