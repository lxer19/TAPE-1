URL: http://www.cs.duke.edu/~jsv/Papers/KrV97.worst-case-prefetching.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node50.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: jsv@cs.duke.edu.  
Title: Optimal Prediction for Prefetching in the Worst Case  
Author: P. Krishnan Jeffrey Scott Vitter 
Date: January 1994, pages 392-401.  
Note: An extended abstract appears in the Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, Arlington, Virginia,  Support was provided in part by National Science Foundation research grants CCR-9007851 and CCR-9522047, by Air Force Office of Scientific Research grants F49620-92-J-0515 and F49620-94-1-0217, and by a Universities Space Research Association/CESDIS associate membership.  
Address: 101 Crawfords Corner Road Holmdel, NJ 07733-3030  Durham, NC 27708-0129  
Affiliation: Bell Laboratories  Dept. of Computer Science Duke University  
Abstract: Response time delays caused by I/O are a major problem in many systems and database applications. Prefetching and cache-replacement methods are attracting renewed attention because of their success in avoiding costly I/Os. Prefetching can be looked upon as a type of online sequential prediction, where the predictions must be accurate as well as made in a computationally efficient way. Unlike other online problems, prefetching cannot admit a competitive analysis, since the optimal o*ine prefetcher incurs no cost when it knows the future page requests. Previous analytical work on prefetching [36] consisted of modeling the user as a probabilistic Markov source. In this paper, we look at the much stronger form of worst-case analysis and derive a randomized algorithm for pure prefetching. We compare our algorithm for every page request sequence against the important class of finite state prefetchers, making no assumptions as to how the sequence of page requests is generated. We prove analytically that the fault rate of our online prefetching algorithm converges almost surely and for every page request sequence to the fault rate of the optimal finite state prefetcher for the sequence. This analysis model can be looked upon as a generalization of the competitive framework, in that it compares an online algorithm in a worst-case manner over all sequences against a powerful yet non-clairvoyant opponent. We simultaneously achieve the computational goal of implementing our prefetcher in optimal constant expected time per prefetched page, using the optimal dynamic discrete random variate generator of [26]. y Support was provided in part by the Office of Naval Research and the Defense Advanced Research Projects Agency under contract N00014-91-J-4052, ARPA order 8225, and by Air Force Office of Scientific Research grants F49620-92-J-0515 and F49620-94-1-0217. This work was done while the author was associated with Brown University and Duke University. Email: pk@research.bell-labs.com. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Aldous and U. Vazirani, </author> <title> "A Markovian Extension of Valiant's Learning Model," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 392-396. </pages>
Reference-contexts: In [20, 36], the online prefetching or cache replacement algorithm is compared against the optimal online algorithm that has full prior knowledge of the source. A PAC learning framework incorporating Markov sources of examples was developed in <ref> [1] </ref>. Recent empirical work on prefetching include a pattern matching approach to prediction [30], computing various first-order statistics for prediction [32], a growing-order Markov predictor [24], prefetching in a parallel environment [22], and research projects at a lower level of abstraction including compiler-directed prefetching [6,29,31]. <p> The use of the rejection method [21] is used to adjust the probabilities of generation appropriately, since the weights in each bucket may vary by a factor of 2. After two recursive levels, the problem reduces to generating one of O (log log ff) weights, each in the range <ref> [1; log ff] </ref>, which can be done dynamically in constant time by the clever table lookup method of Hagerup, Mehlhorn, and Munro [16].
Reference: [2] <author> Y. Amit and M. Miller, </author> <title> "Large Deviations for Coding Markov Chains and Gibbs Random Fields," </title> <institution> Washington University, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference: [3] <author> L. A. Belady, </author> <title> "A Study of Replacement Algorithms for Virtual Storage Computers," </title> <journal> IBM Systems Journal 5 (1966), </journal> <pages> 78-101. </pages>
Reference: [4] <author> D. Blackwell, </author> <title> "An Analog to the Minimax Theorem for Vector Payoffs," </title> <journal> Pacific Journal of Mathematics 6 (1956), </journal> <pages> 1-8. </pages>
Reference: [5] <author> A. Borodin, S. Irani, P. Raghavan, and B. Schieber, </author> <title> "Competitive Paging with Locality of Reference," </title> <booktitle> Proceedings of the 23rd Annual ACM Symposium on Theory of Computation (May 1991). </booktitle>
Reference: [6] <author> T. F. Chen and J. L. Baer, </author> <title> "Reducing Memory Latency via Non-blocking and Prefetching Caches," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference: [7] <author> T. M. </author> <title> Cover, "Behavior of Predictors of Binary Sequences," </title> <booktitle> Proceedings of the 4th Prague Conference on Information Theory, Statistical Decision Functions, Random Processes (1967), </booktitle> <pages> 263-272. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal for every sequence when compared against FSPs <ref> [7] </ref>. (Also see Appendix A.) In the fields of information theory and statistics [4,8,12,17] interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed independently to [36] and <p> Intuitively P 0 1 should choose for 6 Krishnan and Vitter the cache the page i with the highest or nearly highest frequency count f i ( t 1 ). (Randomness in the picking is required, by the remark in Section 1 <ref> [7] </ref>, so it does not suffice to simply pick the page with the highest frequency count; see Appendix A.1. <p> We have shown analytically that P 's fault rate converges almost surely to that of the best FSP for every (worst-case) sequence of page requests. It has been shown in <ref> [7] </ref> that any optimal algorithm for the binary alphabet case has to be necessarily randomized. By the way our algorithm is designed, we need spend at most constant expected time in making the random choices for each prediction, which is optimal.
Reference: [8] <author> T. M. Cover and A. Shenhar, </author> <title> "Compound Bayes Predictors with Apparent Markov Structure," </title> <journal> IEEE Transactions on System Man and Cybernetics SMC-7 (June 1977), </journal> <pages> 421-424. </pages>
Reference: [9] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In Section 3, we present our core prefetching algorithm P 1 , which makes use of sampling without replacement, and we analyze it in Section 4 by comparing it against the best one-state prefetcher. In Section 5 we draw on ideas from information theory <ref> [9] </ref> applied to predicting [8,12,28] and generalize P 1 to get a universal prefetcher P that is optimal in the limit against a general finite state prefetcher. The resulting optimal prefetcher P is a blend of P 1 and the prefetcher [36] based on the Lempel-Ziv data compressor [18,25,37]. <p> We note that even for the ff = 2, k = 1 case, the convergence rate cannot be faster than O (1= p The importance of the above theorem lies in its generalization to higher order using techniques from information theory <ref> [9] </ref>. The approach of [12] allows us to combine P 1 with a prefetcher [36] based on the Lempel-Ziv data compressor [18,25,37] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers.
Reference: [10] <author> K. Curewitz, P. Krishnan, and J. S. Vitter, </author> <title> "Practical Prefetching via Data Compression," </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data (May 1993), </booktitle> <pages> 257-266. </pages>
Reference-contexts: Pure prefetchers can be converted into efficient and practical non-pure prefetchers by melding them with good cache replacement strategies. In <ref> [10] </ref>, pure prefetchers are used with the popular least recently used (or LRU) cache replacement strategy, and significant reductions in page fault rate (number of page faults divided by the number of page requests) are demonstrated.
Reference: [11] <author> P. J. Denning, </author> <title> "Working Sets Past and Present," </title> <journal> IEEE Transactions on Software Engg., </journal> <month> SE-6 </month> <year> (1980), </year> <pages> 64-84. </pages>
Reference: [12] <author> M. Feder, N. Merhav, and M. Gutman, </author> <title> "Universal Prediction of Individual Sequences," </title> <journal> IEEE Transactions on Information Theory IT-38 (July 1992), </journal> <pages> 1258-1270. </pages>
Reference-contexts: We note that even for the ff = 2, k = 1 case, the convergence rate cannot be faster than O (1= p The importance of the above theorem lies in its generalization to higher order using techniques from information theory [9]. The approach of <ref> [12] </ref> allows us to combine P 1 with a prefetcher [36] based on the Lempel-Ziv data compressor [18,25,37] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers. <p> n 1 ) = O (log n= n ): (5) The proofs of the above two theorems are dealt with in the next two subsections. 4.1 The Approximately Balanced Sequence is Sufficiently Worst Case In this subsection we prove Theorem 4 using an interesting extension of the switch analysis of <ref> [12] </ref> in conjunction with the important notion of boosted frequency counts (2). We denote the jth r-subsequence j by 1 , where = 4 j 4 j1 is the length of j . The sequence 1 can be converted to a balanced form b 1 by an iterative balancing strategy. <p> Our proof of Theorem 4 consists in showing that each switch in the balancing strategy does not lower the page fault rate of the entire sequence. A similar but simpler idea worked in the binary case for a different algorithm <ref> [12] </ref>, in which the sequence did not need to be broken up into subsequences, and the sequence b n 1 could be shown to be strictly worst-case. We break n 1 into subsequences as part of our method for achieving optimal computational efficiency (as discussed in Section 6). <p> An example snapshot of the data structure of P is given in Figure 1b. We now briefly explain why P is optimal against an arbitrary s-state machine (Theorem 2), using the interesting approach of <ref> [12] </ref>. An mth-order Markov prefetcher predicts its k choices for the next page based solely on the previous m page requests of the sequence. <p> If we let m be large, an mth-order Markov prefetcher achieves, for every sequence n 1 and any s, a fault rate close to the fault rate of the best s-state prefetcher. In particular, by simple extensions to <ref> [12, Theorem 2] </ref> as shown in Appendix B, we see that Fault M (ff m ) ( n 1 ) Fault F (s) ( n 1 ) + O @ log s 1 The idea of the proof in [12] is to consider a "cross-product" machine of the mth order Markov <p> In particular, by simple extensions to [12, Theorem 2] as shown in Appendix B, we see that Fault M (ff m ) ( n 1 ) Fault F (s) ( n 1 ) + O @ log s 1 The idea of the proof in <ref> [12] </ref> is to consider a "cross-product" machine of the mth order Markov predictor and the s state prefetcher, and show that the cross-product machine is not much better than either of its constituents. The prefetcher P can be looked upon as a Markov prefetcher of growing order. <p> This idea was used in <ref> [12, Theorem 4] </ref> for a binary alphabet, and the simple changes required to obtain (21) are summarized in Appendix B. Theorem 2 follows from (20) and (21). <p> Theorem 2 follows from (20) and (21). Given that P 1 is optimal against F (1) (Theorem 1), in order to show optimality of P against F (s) by the approach described above, we need to extend some results of <ref> [12] </ref> to hold for the prefetching problem. These extensions are simple as described in Appendix B.
Reference: [13] <author> A. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator, and N. E. Young, </author> <title> "On Competitive Algorithms for Paging Problems," </title> <booktitle> Journal of Algorithms 12 (1991), </booktitle> <pages> 685-699. </pages>
Reference: [14] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: Some restrictions on the power of the o*ine algorithm are therefore needed for a meaningful analysis. Vitter and Krishnan [36] analyzed pure prefetching using a form of the competitive philosophy; they assumed that the sequence of page requests was generated by a probabilistic Markov source <ref> [14] </ref>. They showed that the prediction techniques inherent in data compression methods (such as the Lempel-Ziv algorithm [37]) can be used to get optimal pure prefetchers.
Reference: [15] <author> R. L. Graham, D. E. Knuth, and O. Patashnik, </author> <title> Concrete Mathematics, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year> <note> 20 Krishnan and Vitter </note>
Reference-contexts: i ) equals 1 j i j=(ffi) X ffi1 X k X *(u; i; k 1 ) (19) where *(u; i; k 1 ) = 1 (1 ffi 1 (u; i) ffi 2 (u; i)) k 1 : The following facts can be verified by using the asymptotic techniques from <ref> [15, Chapter 9] </ref>: 1. ffi 1 (u; i) r=(L i + u 1) if L i + u 1 r.
Reference: [16] <author> T. Hagerup, K. Mehlhorn, and I. Munro, </author> <title> "Optimal Algorithms for Generating Discrete Random Variables with Changing Distributions," Max Planck Institute, </title> <type> Technical Report MPI-I-92-145, </type> <month> October 15, </month> <year> 1992. </year>
Reference-contexts: how to implement the prefetcher in constant expected time per prefetched page, independent of alphabet size ff and cache size k, by use of the optimal dynamic algorithm for generating discrete random variates of Matias, Vitter, and Ni [26], which uses a table lookup method of Hagerup, Mehlhorn, and Munro <ref> [16] </ref>. Other issues are discussed in Section 7. 2 Analysis Model and Main Results We denote the cache size by k and the total number of different pages (or alphabet size) by ff. <p> After two recursive levels, the problem reduces to generating one of O (log log ff) weights, each in the range [1; log ff], which can be done dynamically in constant time by the clever table lookup method of Hagerup, Mehlhorn, and Munro <ref> [16] </ref>.
Reference: [17] <author> J. F. Hannan, </author> <title> "Approximation to Bayes Risk in Repeated Plays," Contributions to the Theory of Games, </title> <booktitle> Vol 3, Annals of Mathematical Studies (1957), </booktitle> <pages> 97-139. </pages>
Reference-contexts: However, the ff = 2, k = 1 case is clearly unsuitable for our prefetching scenario. The procedure in <ref> [17] </ref> may possibly be generalizable to the arbitrary alphabet case ff 2 for cache size k = 1, but it cannot possibly make a prediction in constant time independent of ff, and the k &gt; 1 case is open.
Reference: [18] <author> P. G. Howard and J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763, </pages> <note> invited paper in Special Issue on Data Compression for Images and Texts. </note>
Reference: [19] <author> S. Irani, A. R. Karlin, and S. Phillips, </author> <title> "Strongly Competitive Algorithms for Paging with Locality of Reference," </title> <booktitle> Proceedings of the 3rd Annual ACM-SIAM Symposium of Discrete Algorithms (January 1992). </booktitle>
Reference: [20] <author> A. R. Karlin, S. J. Phillips, and P. Raghavan, </author> <title> "Markov Paging," </title> <booktitle> Proceedings of the 33rd Annual IEEE Conference on Foundations of Computer Science (October 1992), </booktitle> <pages> 208-217. </pages>
Reference-contexts: They showed that the prediction techniques inherent in data compression methods (such as the Lempel-Ziv algorithm [37]) can be used to get optimal pure prefetchers. Cache replacement has been studied by Karlin, Phillips, and Raghavan <ref> [20] </ref> under a different stochastic version of the competitive framework; the sequence of page requests was assumed to be generated by a Markov chain (a subset of Markov sources). <p> Cache replacement has been studied by Karlin, Phillips, and Raghavan [20] under a different stochastic version of the competitive framework; the sequence of page requests was assumed to be generated by a Markov chain (a subset of Markov sources). In <ref> [20, 36] </ref>, the online prefetching or cache replacement algorithm is compared against the optimal online algorithm that has full prior knowledge of the source. A PAC learning framework incorporating Markov sources of examples was developed in [1]. <p> unlimited online algorithm with full a priori knowledge of the source, it is the case for prefetching and cache replacement that when the page request sequences are generated by a finite state Markov source (or a Markov chain), the optimal online algorithm is finite state. (See [36, Definition 4] and <ref> [20, Theorem 2] </ref>.) In particular, the fault rate of the prefetcher we develop in this paper is asymptotically the same as the Optimal Prediction for Prefetching 3 fault rate of the optimal prefetcher from [36] when the source is finite state Markov.
Reference: [21] <author> D. Knuth, </author> <booktitle> The Art of Computer Programming, Volume 2: Seminumerical Algorithms, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition 1981. </note>
Reference-contexts: With high probability, the individual weight chosen during the generation will be within the first O (log ff) ranges, so each successive group of O (log ff) ranges should be processed in a recursive data structure according to the weights of the ranges. The use of the rejection method <ref> [21] </ref> is used to adjust the probabilities of generation appropriately, since the weights in each bucket may vary by a factor of 2. <p> It suffices to determine if lg U r lg f i , which can be done in constant time using finite-precision by generating the exponentially distributed random variate lg U directly <ref> [21, page 128] </ref>. The expected number of steps needed before the acceptance or rejection is determined is a small constant, so finite precision suffices.
Reference: [22] <author> D. F. Kotz and C. S. Ellis, </author> <title> "Prefetching in File Systems for MIMD Multiprocessors," </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1 (April 1990), </journal> <pages> 218-230. </pages>
Reference-contexts: A PAC learning framework incorporating Markov sources of examples was developed in [1]. Recent empirical work on prefetching include a pattern matching approach to prediction [30], computing various first-order statistics for prediction [32], a growing-order Markov predictor [24], prefetching in a parallel environment <ref> [22] </ref>, and research projects at a lower level of abstraction including compiler-directed prefetching [6,29,31]. In this paper, we develop a randomized algorithm for pure prefetching and show its optimality in the limit under the following analysis strategy.
Reference: [23] <author> P. Krishnan and J. S. Vitter, </author> <title> "Optimal Prediction for Prefetching in the Worst Case," </title> <institution> DUKE-CS-93-26, Duke University Technical Report. </institution>
Reference-contexts: Pages 0 and 1 are the likely pages to be chosen. Algorithm P 0 1 can be shown to be optimal against the best one-state FSP for general ff 2, but only when k = 1 <ref> [23] </ref>. We can modify algorithm P 0 1 to get algorithm P 1 that is optimal against the best one-state FSP for general ff 2; k 1. Definition 3 We define subsequence 0 = 2 1 , and j = 4 j +1 4 j1 +2 for j 1.
Reference: [24] <author> P. Laird, </author> <title> "Discrete Sequence Prediction and its Applications," </title> <institution> AI Research Branch, NASA Ames Research Center, </institution> <type> manuscript, </type> <year> 1992. </year>
Reference-contexts: A PAC learning framework incorporating Markov sources of examples was developed in [1]. Recent empirical work on prefetching include a pattern matching approach to prediction [30], computing various first-order statistics for prediction [32], a growing-order Markov predictor <ref> [24] </ref>, prefetching in a parallel environment [22], and research projects at a lower level of abstraction including compiler-directed prefetching [6,29,31]. In this paper, we develop a randomized algorithm for pure prefetching and show its optimality in the limit under the following analysis strategy.
Reference: [25] <author> G. G. Langdon, </author> <title> "A Note on the Ziv-Lempel Model for Compressing Individual Sequences," </title> <journal> IEEE Transactions on Information Theory 29 (March 1983), </journal> <pages> 284-287. </pages>
Reference: [26] <author> Y. Matias, J. S. Vitter, and W. C. Ni, </author> <title> "Dynamic Generation of Discrete Random Vari-ates," </title> <booktitle> Proceedings of the 4th Annual SIAM/ACM Symposium on Discrete Algorithms (January 1993). </booktitle>
Reference-contexts: We show in Section 6 how to implement the prefetcher in constant expected time per prefetched page, independent of alphabet size ff and cache size k, by use of the optimal dynamic algorithm for generating discrete random variates of Matias, Vitter, and Ni <ref> [26] </ref>, which uses a table lookup method of Hagerup, Mehlhorn, and Munro [16]. Other issues are discussed in Section 7. 2 Analysis Model and Main Results We denote the cache size by k and the total number of different pages (or alphabet size) by ff. <p> The expected running time for prefetcher P can be made optimal, by use of the optimal dynamic random variate generator of <ref> [26] </ref>: Theorem 3 The prefetcher P runs in constant expected time (independent of ff and k) for each page prefetched; that is, it requires an average of O (k) time to determine which k pages to prefetch. <p> a large power and then choosing a page with probability proportional to its boosted count. (The boosted counts will be very large, but can be represented with O (log n) bits, using the scheme discussed in Section 6.) Efficient random variate generation with dynamically changing weights can be done using <ref> [26] </ref>, as discussed in Section 6. The algorithm P 0 1 is a simple randomized weighting algorithm that makes k predictions at each time step for the next page request. <p> problem of generating a random variate with a value in the range f0, 1, 2, . . . , ff1g and distributed according to ff dynamically changing weights w 0 , w 1 , w 2 , . . . w ff1 is solved optimally by Matias, Vitter, and Ni <ref> [26, Section 5] </ref>. The idea at an intuitive level is to group the weights into ranges according to their values. Range j stores weights in the range [2 j ; 2 j+1 ). Each range is said to have a weight equal to the sum of the weights it contains. <p> After two recursive levels, the problem reduces to generating one of O (log log ff) weights, each in the range [1; log ff], which can be done dynamically in constant time by the clever table lookup method of Hagerup, Mehlhorn, and Munro [16]. There is also extensive concern in <ref> [26] </ref> about the choice of hashing parameters in the universal hashing schemes used to get linear space, since no a priori bound on the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in [26].) The model of computation allows arithmetic computation and truncated <p> There is also extensive concern in <ref> [26] </ref> about the choice of hashing parameters in the universal hashing schemes used to get linear space, since no a priori bound on the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in [26].) The model of computation allows arithmetic computation and truncated logarithms of quantities up to value O (W ), where W is the maximum weight. In our application, the computation assumption of [26] is unreasonable, since it allows constant-time operations on arbitrary numbers of bits. <p> the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in <ref> [26] </ref>.) The model of computation allows arithmetic computation and truncated logarithms of quantities up to value O (W ), where W is the maximum weight. In our application, the computation assumption of [26] is unreasonable, since it allows constant-time operations on arbitrary numbers of bits. <p> Fortunately we can get around the precision problem by approximating w i by 2 dr lg f i e . The first level of the algorithm in <ref> [26] </ref> is applied to these approximated weights, using arithmetic on the exponents, which involves 18 Krishnan and Vitter only O (log n) bits. <p> The range j can be computed therefore in constant time using O (log n)-bit arithmetic. The resulting recursive subproblems have polynomially sized weights, and the rest of the construction continues as in <ref> [26] </ref>. Because of the initial approximation, if the "approximated" page i is selected for generation, a final acceptance-rejection test must be done before actually choosing page i; the acceptance probability is (f i ) r =2 dr lg f i e , which is at least 1=2.
Reference: [27] <author> L. A. McGeoch and D. D. Sleator, </author> <title> "A Strongly Competitive Randomized Paging Algorithm," </title> <institution> Carnegie-Mellon University, CS-89-122, </institution> <month> March </month> <year> 1989. </year>
Reference: [28] <author> N. Merhav and M. Feder, </author> <title> "Universal Sequential Learning and Decision from Individual Data Sequences," </title> <booktitle> Proceedings of the 5th ACM Workshop on Computational Learning Theory (July 1992), </booktitle> . 
Reference-contexts: The procedure in [17] may possibly be generalizable to the arbitrary alphabet case ff 2 for cache size k = 1, but it cannot possibly make a prediction in constant time independent of ff, and the k &gt; 1 case is open. In <ref> [28] </ref>, predictors are developed for various continuous loss functions, but they are not relevant to the harder-to-analyze discontinuous 0-1 loss functions associated with cache replacement and prefetching.
Reference: [29] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and Evaluation of a Compiler Algorithm for Prefetching," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference: [30] <author> M. Palmer and S. Zdonik, </author> <title> "Fido: A Cache that Learns to Fetch," </title> <booktitle> Proceedings of the 1991 International Conference on Very Large Databases (September 1991). </booktitle>
Reference-contexts: A PAC learning framework incorporating Markov sources of examples was developed in [1]. Recent empirical work on prefetching include a pattern matching approach to prediction <ref> [30] </ref>, computing various first-order statistics for prediction [32], a growing-order Markov predictor [24], prefetching in a parallel environment [22], and research projects at a lower level of abstraction including compiler-directed prefetching [6,29,31].
Reference: [31] <author> A. Rogers and K. Li, </author> <title> "Software Support for Speculative Loads," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). Optimal Prediction for Prefetching 21 </booktitle>
Reference: [32] <author> K. Salem, </author> <title> "Adaptive Prefetching for Disk Buffers," </title> <type> CESDIS, </type> <institution> Goddard Space Flight Center, TR-91-46, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: A PAC learning framework incorporating Markov sources of examples was developed in [1]. Recent empirical work on prefetching include a pattern matching approach to prediction [30], computing various first-order statistics for prediction <ref> [32] </ref>, a growing-order Markov predictor [24], prefetching in a parallel environment [22], and research projects at a lower level of abstraction including compiler-directed prefetching [6,29,31]. In this paper, we develop a randomized algorithm for pure prefetching and show its optimality in the limit under the following analysis strategy.
Reference: [33] <author> G. S. Shedler and C. Tung, </author> <title> "Locality in Page Reference Strings," </title> <journal> SIAM Journal on Computing 1 (1972), </journal> <pages> 218-241. </pages>
Reference: [34] <author> D. D. Sleator and R. E. Tarjan, </author> <title> "Amortized Efficiency of List Update and Paging Rules," </title> <booktitle> Communications of the ACM 28 (February 1985), </booktitle> <pages> 202-208. </pages>
Reference-contexts: In this paper, we study online pure prefetching. 1.1 Analysis Technique The notion of competitiveness introduced by Sleator and Tarjan <ref> [34] </ref> determines the goodness of an online algorithm by comparing its performance to that of o*ine algorithms. Competitive algorithms for cache replacement are well examined in the literature [5,13,27,34]. It is unreasonable to expect prefetching algorithms to be competitive in this sense.
Reference: [35] <author> J. S. Vitter, K. Curewitz, and P. Krishnan, </author> <title> "Online Background Predictors and Prefetch-ers," </title> <institution> Duke University, United States Patent No. 5,485,609, </institution> <month> January 16, </month> <year> 1996. </year>
Reference: [36] <author> J. S. Vitter and P. Krishnan, </author> <title> "Optimal Prefetching via Data Compression," </title> <editor> J. </editor> <booktitle> of the ACM 143 (September 1996), An extended abstract appears in Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1991, </year> <pages> 121-130. </pages>
Reference-contexts: In order to be competitive, an online algorithm would have to be an almost perfect predictor for any sequence, which seems intuitively impossible. Some restrictions on the power of the o*ine algorithm are therefore needed for a meaningful analysis. Vitter and Krishnan <ref> [36] </ref> analyzed pure prefetching using a form of the competitive philosophy; they assumed that the sequence of page requests was generated by a probabilistic Markov source [14]. <p> Cache replacement has been studied by Karlin, Phillips, and Raghavan [20] under a different stochastic version of the competitive framework; the sequence of page requests was assumed to be generated by a Markov chain (a subset of Markov sources). In <ref> [20, 36] </ref>, the online prefetching or cache replacement algorithm is compared against the optimal online algorithm that has full prior knowledge of the source. A PAC learning framework incorporating Markov sources of examples was developed in [1]. <p> against the optimal computationally unlimited online algorithm with full a priori knowledge of the source, it is the case for prefetching and cache replacement that when the page request sequences are generated by a finite state Markov source (or a Markov chain), the optimal online algorithm is finite state. (See <ref> [36, Definition 4] </ref> and [20, Theorem 2].) In particular, the fault rate of the prefetcher we develop in this paper is asymptotically the same as the Optimal Prediction for Prefetching 3 fault rate of the optimal prefetcher from [36] when the source is finite state Markov. <p> (or a Markov chain), the optimal online algorithm is finite state. (See [36, Definition 4] and [20, Theorem 2].) In particular, the fault rate of the prefetcher we develop in this paper is asymptotically the same as the Optimal Prediction for Prefetching 3 fault rate of the optimal prefetcher from <ref> [36] </ref> when the source is finite state Markov. <p> FSPs [7]. (Also see Appendix A.) In the fields of information theory and statistics [4,8,12,17] interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed independently to <ref> [36] </ref> and the comparison is made against the best finite state predictor. However, the ff = 2, k = 1 case is clearly unsuitable for our prefetching scenario. <p> The resulting optimal prefetcher P is a blend of P 1 and the prefetcher <ref> [36] </ref> based on the Lempel-Ziv data compressor [18,25,37]. <p> The approach of [12] allows us to combine P 1 with a prefetcher <ref> [36] </ref> based on the Lempel-Ziv data compressor [18,25,37] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers. <p> From the observation in Section 1.1 that under the model from <ref> [36] </ref> (where the sequences of page requests are generated by a finite state Markov source), the optimal prefetcher is also an FSP, we get the following corollary. Corollary 1 Under the model from [36], where the sequences of page requests are generated by a finite state Markov source M , the <p> From the observation in Section 1.1 that under the model from <ref> [36] </ref> (where the sequences of page requests are generated by a finite state Markov source), the optimal prefetcher is also an FSP, we get the following corollary. Corollary 1 Under the model from [36], where the sequences of page requests are generated by a finite state Markov source M , the fault rate of prefetcher P converges almost surely to the minimum fault rate of any online prefetcher with complete a priori knowledge of the source M .
Reference: [37] <author> J. Ziv and A. Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <note> IEEE Transactions on Information Theory 24 (September 1978), 530-536. 22 Krishnan and Vitter </note>
Reference-contexts: Vitter and Krishnan [36] analyzed pure prefetching using a form of the competitive philosophy; they assumed that the sequence of page requests was generated by a probabilistic Markov source [14]. They showed that the prediction techniques inherent in data compression methods (such as the Lempel-Ziv algorithm <ref> [37] </ref>) can be used to get optimal pure prefetchers. Cache replacement has been studied by Karlin, Phillips, and Raghavan [20] under a different stochastic version of the competitive framework; the sequence of page requests was assumed to be generated by a Markov chain (a subset of Markov sources).
References-found: 37

