URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/topic-spotting.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: 
Email: wiener@cs.colorado.edu  pedersen@parc.xerox.com  andreas@cs.colorado.edu,  
Phone: 2  3  
Title: A Neural Network Approach to Topic Spotting  
Author: Erik Wiener Jan O. Pedersen and Andreas S. Weigend 
Web: http://www.cs.colorado.edu/~andreas/Home.html  
Address: Boulder, Boulder, CO 80309,  3333 Coyote Hill Rd., Palo Alto, CA 94304,  Campus Box 430,  Boulder, Boulder, CO 80309,  
Affiliation: 1 Xerox Palo Alto Research Center and Dept. of Computer Science, CB 430, University of Colorado at  Xerox Palo Alto Research Center,  Institute of Cognitive Science and Dept. of Computer Science,  University of Colorado at  
Abstract: This paper presents an application of nonlinear neural networks to topic spotting. Neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features. In the context of this model, we compare two approaches to dimensionality reduction in representation: one based on term selection and another based on Latent Semantic Indexing (LSI). Two different methods are proposed for improving LSI representations for the topic spotting task. We find that term selection and our modified LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Apte, F. Damerau, and S. Weiss. </author> <title> Towards language independent automated learning of text categorization models. </title> <booktitle> In Proceedings of the 17th Annual ACM/SIGIR Conference, </booktitle> <year> 1994. </year> <editor> Erik Wiener, Jan O. Pedersen, Andreas S. </editor> <publisher> Weigend </publisher>
Reference-contexts: our neural network models; Section 6 presents experimental results; and Section 7 is a concluding discussion. 2 Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks [16], decision trees [10], nearest neighbor algorithms [11], Bayesian classifiers [10], and boolean decision rules <ref> [1] </ref>. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. <p> Results showed the decision tree method to give slightly higher performance than the Bayesian classifier, and the authors noted that the decision tree method also resulted in a classifier whose rules were easier to interpret. Apte, Damerau, and Weiss <ref> [1] </ref> used a rule induction technique called Swap-1 to produce disjunctive binary decision rules relating the presence of certain combinations of terms to the presence of topics. As in the previous approach, separate classifiers were trained for each topic, each using their own local feature set. <p> A Neural Network Approach to Topic Spotting 3 The Corpus For our experiments, we used the Reuters-22173 corpus of Reuters newswire stories from 1987, which is now becoming a standard testbed for text categorization research <ref> [1, 9, 10] </ref>. 2 Although there are 21,450 stories in the full collection, we used only those stories which had at least one topic assigned, which left 9,610 for training and 3,662 for testing. <p> While it would have been better to use the full collection (including the stories deemed by the human assigners to be about none of the predefined topics), we believe our partitioning supported meaningful experiments and seems to have been used by at least one other group <ref> [1] </ref>. The stories have a mean length of 90.6 words with standard deviation 91.6. There are 92 topics which appear at least once in our training set, and cover such areas as commodities, interest rates, and foreign exchange. <p> We found that using about 20 terms yielded, on average, the best classification performance, as measured on an independent test set (see Figure 1). This number falls in the range of feature set sizes used in [10] and is a little smaller than the number used in <ref> [1] </ref>. Note that we can always decrease the error on the training set by including more terms, but classification performance on out-of-sample data quickly falls off after about 20 terms due to overfit-ting. <p> The breakeven points are 0.801, 0.820, 0.795, and 0.775, respectively. Since we used the same corpus, we can roughly compare our results to the best algorithm from <ref> [1] </ref> that didn't give special weight to words in the headlines of the stories, which had a reported breakeven point of 0.789. While our results seem quite good, microaver-aged performance is somewhat misleading on this task because more frequent topics are weighted heavier in the average.
Reference: [2] <author> S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: To address these problems, we applied Latent Semantic Indexing to reduce the dimensionality of our feature set. 4.2 Latent Semantic Indexing LSI, originally developed to address the problems created by synonymy and poly-semy in the standard vector space model of information retrieval <ref> [2] </ref>, seeks to transform the original document vectors to a meaningful lower-dimensional space by analyzing the correlational structure of terms in the document collection. The transformation is computed by applying a singular-value decomposition (SVD) to the the original term by document matrix. <p> However, they are orthogonal. Terms are related in LSI to the extent to which they "travel together" in the corpus. Thus, doc A Neural Network Approach to Topic Spotting uments using different terminology to talk about the same concept are positioned near each other in the new space. (See <ref> [2] </ref> for further details of the LSI technique.) One property of LSI vectors is that higher dimensions capture increasingly less of the variance of the original data, and hence can be dropped with minimal loss. <p> Original experiments with LSI for IR tasks found that retaining about the first 100 dimensions gave the optimal performance <ref> [2] </ref>, but more recent results suggest that using more dimensions (around 300) is better for some tasks [7]. In our topic spotting experiments, we found that performance continues to improve up to at least 250 dimensions, although the improvement rapidly slows down after about 100 dimensions.
Reference: [3] <author> S. T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, </title> <journal> Instruments and Computers, </journal> <volume> 23(2) </volume> <pages> 229-236, </pages> <year> 1991. </year>
Reference-contexts: Dumais <ref> [3] </ref> showed that using an inverse document frequency (IDF) weighting on the term by document matrix before applying the SVD led to 40% average improvement on a set of standard IR test sets.
Reference: [4] <author> P. Hayes and S. Weinstein. CONSTRUE/TIS: </author> <title> A system for content-based indexing of a database of news stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: While several successful text categorization systems take an expert systems approach, manually constructing a system of inference rules on top of a large body of linguistic and domain knowledge (e.g., <ref> [4, 6] </ref>), data-driven approaches induce a set of rules from a corpus of labeled training documents.
Reference: [5] <author> D. Hull. </author> <title> Improving text retrieval for the routing problem using latent semantic indexing. </title> <booktitle> In Proceedings of the 17th Annual ACM/SIGIR Conference, </booktitle> <pages> pages 282-291, </pages> <year> 1994. </year>
Reference-contexts: We experiment with two approaches to constructing local LSI representations, embodying two methods of defining local structure. In one approach, we define five broad meta-topics|agriculture, 4 This method is similar in spirit to a method proposed by Hull <ref> [5] </ref> for improving classification performance on the routing problem. Erik Wiener, Jan O. Pedersen, Andreas S. Weigend energy, foreign exchange, government, and metals|and break the corpus into five clusters, each containing all the documents on a particular meta-topic.
Reference: [6] <author> P. Jacobs and L. Rau. SCISOR: </author> <title> Extracting information from on-line news. </title> <journal> Communications of the ACM, </journal> <volume> 33(11) </volume> <pages> 88-97, </pages> <year> 1990. </year>
Reference-contexts: While several successful text categorization systems take an expert systems approach, manually constructing a system of inference rules on top of a large body of linguistic and domain knowledge (e.g., <ref> [4, 6] </ref>), data-driven approaches induce a set of rules from a corpus of labeled training documents.
Reference: [7] <author> T. </author> <title> Landauer. </title> <type> Personal communication. </type> <year> 1994. </year>
Reference-contexts: Original experiments with LSI for IR tasks found that retaining about the first 100 dimensions gave the optimal performance [2], but more recent results suggest that using more dimensions (around 300) is better for some tasks <ref> [7] </ref>. In our topic spotting experiments, we found that performance continues to improve up to at least 250 dimensions, although the improvement rapidly slows down after about 100 dimensions. The process of constructing LSI representations from a term by document matrix is straightforward.
Reference: [8] <author> D. D. Lewis. </author> <title> Evaluating text categorization. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 312-318, </pages> <year> 1991. </year>
Reference-contexts: each topic, we can summarize performance either by microaveraging, which means adding all the contingency tables together across topics at a certain threshold and then computing precision and recall, or we can macroaverage, meaning we compute precision and recall individually for each topic and then take an average across topics <ref> [8] </ref>. For microaveraging, we use proportional assignment for picking decision thresholds, and for macroaveraging we use a fixed set of recall levels.
Reference: [9] <author> D. D. Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> Computer Science Dept., Univ. of Massachussetts at Amherst, </institution> <month> February </month> <year> 1992. </year> <type> Technical Report 91-93. </type>
Reference-contexts: Most systems have used words or word-couples for features, but Lewis <ref> [9] </ref> and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. We describe in more detail two recent text categorization studies based on the Reuters-22173 corpus, which is also the subject of the experiments in this paper. <p> A Neural Network Approach to Topic Spotting 3 The Corpus For our experiments, we used the Reuters-22173 corpus of Reuters newswire stories from 1987, which is now becoming a standard testbed for text categorization research <ref> [1, 9, 10] </ref>. 2 Although there are 21,450 stories in the full collection, we used only those stories which had at least one topic assigned, which left 9,610 for training and 3,662 for testing.
Reference: [10] <author> D. D. Lewis and M. Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1994. </year>
Reference-contexts: describes the corpus; Section 4 discusses our representational approach; Section 5 discusses our neural network models; Section 6 presents experimental results; and Section 7 is a concluding discussion. 2 Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks [16], decision trees <ref> [10] </ref>, nearest neighbor algorithms [11], Bayesian classifiers [10], and boolean decision rules [1]. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. <p> representational approach; Section 5 discusses our neural network models; Section 6 presents experimental results; and Section 7 is a concluding discussion. 2 Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks [16], decision trees <ref> [10] </ref>, nearest neighbor algorithms [11], Bayesian classifiers [10], and boolean decision rules [1]. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. <p> We describe in more detail two recent text categorization studies based on the Reuters-22173 corpus, which is also the subject of the experiments in this paper. Lewis and Ringuette <ref> [10] </ref> compared two categorization techniques, one a Bayesian classifier based on a fairly simple conditional probability model and another using decision trees. In both cases, a separate classifier was constructed for each topic. <p> A Neural Network Approach to Topic Spotting 3 The Corpus For our experiments, we used the Reuters-22173 corpus of Reuters newswire stories from 1987, which is now becoming a standard testbed for text categorization research <ref> [1, 9, 10] </ref>. 2 Although there are 21,450 stories in the full collection, we used only those stories which had at least one topic assigned, which left 9,610 for training and 3,662 for testing. <p> Pedersen, Andreas S. Weigend scores indicate useful terms for discrimination. We found that using about 20 terms yielded, on average, the best classification performance, as measured on an independent test set (see Figure 1). This number falls in the range of feature set sizes used in <ref> [10] </ref> and is a little smaller than the number used in [1]. Note that we can always decrease the error on the training set by including more terms, but classification performance on out-of-sample data quickly falls off after about 20 terms due to overfit-ting. <p> Pedersen, Andreas S. Weigend the topic, and precision is the percentage of documents predicted as having the topic that actually have the topic. We pick decision thresholds in two ways. In proportional assignment <ref> [10] </ref>, we pick a different set of thresholds for each topic by varying an integer parameter, k, over a set of values and at each level setting the decision threshold just below the probability value for the kp'th highest ranked document, where p is the fraction of positive examples of the
Reference: [11] <author> B. Masand, G. Linoff, and D. Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of the 15th Annual ACM/SIGIR Conference, </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: 4 discusses our representational approach; Section 5 discusses our neural network models; Section 6 presents experimental results; and Section 7 is a concluding discussion. 2 Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks [16], decision trees [10], nearest neighbor algorithms <ref> [11] </ref>, Bayesian classifiers [10], and boolean decision rules [1]. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations.
Reference: [12] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> London, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Our basic framework is a regression model relating the input variables (features) to the output variables (binary topic assignments) which can be fit using training data. For linear analysis, we use logistic regression <ref> [12] </ref>, which is appropriate for modeling binary output variables. In this case, the functional form is p = 1 + e ; where = fi T x is a linear combination of the input features. The logistic function guarantees that p 2 (0; 1).
Reference: [13] <author> M. D. Richard and R. P. Lipp-mann. </author> <title> Neural network classifiers estimate bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> (3):461-483, 1991. 
Reference-contexts: As in logistic networks, nonlinear networks with logistic activations in the outputs estimate the Bayesian a posteriori probability of the output given the input features, assuming certain conditions are met in training. <ref> [13] </ref>. 5.2 Neural Networks for Topic Spotting In the context of topic spotting, nonlinear neural networks extract "hidden" features out of nonlinear combinations of terms. Erik Wiener, Jan O. Pedersen, Andreas S.
Reference: [14] <author> D. E. Rumelhart, R. Durbin, R. Golden, and Y. Chauvin. </author> <title> Backpropagation: the basic theory. </title> <editor> In P. Smolensky, M. C. Mozer, and D. E. Rumelhart, editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks, </booktitle> <pages> pages 1-39. </pages> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1995. </year>
Reference-contexts: The search in weight space for a set of weights which minimizes the cost function is the training process. In the case of binary targets, cross-entropy is theoretically the most appropriate cost function because it presumes a binomial error model for the outputs <ref> [14] </ref>. We use the standard backpropagation method of gradient descent as a search technique. The simplest linear classifier network (also called a logistic network) has an output unit with a logistic activation and no hidden layer, resulting in a functional form equivalent to the logistic regression model.
Reference: [15] <author> G. Salton and C. Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: The score, which we call the relevancy score because of its relation to what Salton and Buckley call the relevancy weight <ref> [15] </ref>, measures how "unbalanced" the term is across documents with and without the topic: r k = log w t k =d t + 1=6 where w tk is the number of documents with the topic that contain the term, d t is the total number of documents with the topic, <p> It is based on the assumption that, in general, low frequency terms are better discriminators than high frequency terms. In topic spotting, however, we can tune this general assumption. We use relevancy weighting to emphasize terms in proportion to their estimated topic discrimination power, following a suggestion in <ref> [15] </ref>. The global relevancy weight for each term is computed by summing the absolute value of the relevancy score it has for each topic and cutting the total value off after a certain threshold.
Reference: [16] <author> K. Tzeras and S. Hartmann. </author> <title> Automatic indexing based on bayesian inference networks. </title> <booktitle> In Proceedings of the 16th Annual ACM/SIGIR Conference, </booktitle> <pages> pages 22-34, </pages> <year> 1993. </year>
Reference-contexts: work; Section 3 describes the corpus; Section 4 discusses our representational approach; Section 5 discusses our neural network models; Section 6 presents experimental results; and Section 7 is a concluding discussion. 2 Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks <ref> [16] </ref>, decision trees [10], nearest neighbor algorithms [11], Bayesian classifiers [10], and boolean decision rules [1]. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. <p> Related Work Among the many classification methods that have been used for text categorization are Bayesian belief networks <ref> [16] </ref>, decision trees [10], nearest neighbor algorithms [11], Bayesian classifiers [10], and boolean decision rules [1]. Most systems have used words or word-couples for features, but Lewis [9] and Tzeras and Hartmann [16], for example, incorporated phrasal structure into their document representations. We describe in more detail two recent text categorization studies based on the Reuters-22173 corpus, which is also the subject of the experiments in this paper.
Reference: [17] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: Overfit-ting is typically more of a problem the more free parameters (weights) there are in the model compared to training samples. See, for example, <ref> [17] </ref>. As a sanity check, we computed how well topics could be predicted using only a single key term. We found that by us ing the top scoring term for each topic as that topic's sole predictor, 20 of the 92 topics could be predicted with over 90% average precision. <p> When applying logistic regression in our experiments, we typically use a logistic network because it gives us greater control over the fitting process, such as allowing us to add priors to the cost function and to use early stopping in training to minimize overfitting <ref> [17] </ref>. The use of one or more hidden layers of nonlinear activation functions allows the network to model nonlinear relationships between the input and output variables. Each hidden layer learns to re-represent the input data by discovering higher-level features formed out of combinations of the features from the previous level. <p> To help alleviate this problem, we employ a simple regularization scheme based on weight-elimination in which we add a term penalizing network complexity to the cross-entropy cost function. 6 We also use the technique of early stopping based on cross-validation <ref> [17] </ref>. 5.2.2 Modular Architecture In our second approach, we use a modular architecture to decompose the learning problem into a set of smaller problems. The architecture is shown in Figure 3.
References-found: 17

