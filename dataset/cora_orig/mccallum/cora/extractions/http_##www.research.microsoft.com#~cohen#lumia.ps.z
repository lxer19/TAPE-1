URL: http://www.research.microsoft.com/~cohen/lumia.ps.z
Refering-URL: http://www.research.microsoft.com/~cohen/pubs.htm
Root-URL: http://www.research.microsoft.com
Title: The Lumigraph  
Author: Steven J. Gortler Radek Grzeszczuk Richard Szeliski Michael F. Cohen 
Affiliation: Microsoft Research  
Abstract: This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lu-migraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lu-migraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ADELSON, E. H., AND BERGEN, J. R. </author> <title> The plenoptic function and the elements of early vision. In ComputationalModels of Visual Processing, </title> <editor> Landy and Movs-hon, Eds. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991, </year> <note> ch. 1. </note>
Reference-contexts: This paper extends the work begun with Quicktime VR and Plen-optic Modeling by further developing the idea of capturing the complete flow of light in a region of the environment. Such a flow is described by a plenoptic function <ref> [1] </ref>. The plenoptic function is a five dimensional quantity describing the flow of light at every 3D spatial position (x; y; z) for every 2D direction (; ).
Reference: [2] <author> ASHDOWN, I. </author> <title> Near-field photometry: A new approach. </title> <journal> Journal of the Illumination Engineering Society 22, </journal> <volume> 1 (1993), </volume> <pages> 163-180. </pages>
Reference-contexts: In the context of illumination engineering, this idea has been used to model and represent the illumination due to physical luminaires. Ashdown <ref> [2] </ref> describes a gantry for moving a camera along a sphere surrounding a luminaire of interest. The captured information can then be used to represent the light source in global illumination simulations. Ashdown traces this idea of the surface-restricted plenoptic function back to Levin [15].
Reference: [3] <author> BENTON, S. A. </author> <title> Survey of holographic stereograms. </title> <booktitle> Proceedings of the SPIE 391 (1982), </booktitle> <pages> 15-22. </pages>
Reference-contexts: Thus, the plenoptic function due to the object can be reduced to 4 dimensions 3 . The idea of restricting the plenoptic function to some surrounding surface has been used before. In full-parallax holographic ste-reograms <ref> [3] </ref>, the appearance of an object is captured by moving a camera along some surface (usually a plane) capturing a 2D array of photographs. This array is then transferred to a single holographic image, which can display the appearance of the 3D object.
Reference: [4] <author> BOLLES, R. C., BAKER, H. H., AND MARIMONT, D. H. </author> <title> Epipolar-plane image analysis: An approach to determining structure from motion. </title> <booktitle> International Journal of Computer Vision 1 (1987), </booktitle> <pages> 7-55. </pages>
Reference-contexts: The slope of the flow lines corresponds to the depth of the point on the object tracing out the line. Notice how the function is coherent along these flow lines <ref> [4] </ref>. We expect the Lumigraph to be smooth along the optical flow lines, and thus it would be beneficial to have the basis functions adapt their shape correspondingly. The remapping of u and v values to u 0 and v 0 performs this reshaping.
Reference: [5] <author> BURT, P. J. </author> <title> Moment images, polynomial fit filters, and the problem of surface interpolation. In Proceedings of Computer Vision and Pattern Recognition (June 1988), </title> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 144-152. </pages>
Reference-contexts: First, the sampling density can be quite sparse, with large gaps in many regions. Second, the sampling density is typically very non-uniform. The first of these problems has been addressed in a two dimensional scattered data approximation algorithm described by Burt <ref> [5] </ref>. In his algorithm, a hierarchical set of lower resolution data sets is created using an image pyramid. Each of these lower resolutions represents a blurred version of the input data; at lower resolutions, the gaps in the data become smaller.
Reference: [6] <author> CHEN, S. E. </author> <title> Quicktime VR an image-based approach to virtual environment navigation. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1995, </year> <pages> pp. 29-38. </pages>
Reference-contexts: Unfortunately, these systems are still unable to completely capture small details in geometry and material properties. Existing rendering methods also continue to be limited in their capability to faithfully reproduce real world illumination, even if given accurate geometric models. Quicktime VR <ref> [6] </ref> was one of the first systems to suggest that the traditional modeling/rendering process can be skipped. Instead, a series of captured environment maps allow a user to look around a scene from fixed points in space.
Reference: [7] <author> CHEN, S. E., AND WILLIAMS, L. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1993, </year> <pages> pp. 279-288. </pages>
Reference-contexts: Instead, a series of captured environment maps allow a user to look around a scene from fixed points in space. One can also flip through different views of an object to create the illusion of a 3D model. Chen and Williams <ref> [7] </ref> and Werner et al [30] have investigated smooth interpolation between images by modeling the motion of pixels (i.e., the optical flow) as one moves from one camera position to another. <p> In this paper, we discuss computational methods for capturing and representing a plenoptic function, and for using such a representation to render images of the environment from any arbitrary viewpoint. Unlike Chen and Williams' view interpolation <ref> [7] </ref> and McMil-lan and Bishop's plenoptic modeling [19], our approach does not rely explicitly on any optical flow information. Such information is often difficult to obtain in practice, particularly in environments with complex visibility relationships or specular surfaces.
Reference: [8] <author> CHUI, C. K. </author> <title> An Introduction to Wavelets. </title> <publisher> Academic Press Inc., </publisher> <year> 1992. </year>
Reference-contexts: If we choose the L 2 distance metric, then the projection is defined by integrating L against the duals of the basis functions <ref> [8] </ref>, given by the inner products, x i;j;p;q = &lt; L; ~ B i;j;p;q &gt; (1) In the case of the box basis, B = ~ B. The duals of the quadralinear basis functions are more complex, but these basis functions sufficiently approximate their own duals for our purposes.
Reference: [9] <author> HALLE, M. W. </author> <title> Holographic stereograms as discrete imaging systems. </title> <booktitle> Practical Holography VIII (SPIE) 2176 (1994), </booktitle> <pages> 73-84. </pages>
Reference-contexts: We adopt a parameterization similar to that used in digital holographic stereograms <ref> [9] </ref> and also used by Levoy and Hanrahan [16]. <p> One can interpret this projection as point sampling L after it has been low pass filtered with the kernel ~ B. This interpretation is pursued in the context of holographic stereograms by Halle <ref> [9] </ref>.
Reference: [10] <author> HOPPE, H. </author> <title> Progressive meshes. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1996. </year>
Reference-contexts: The approximation was generated using a mesh simplification program <ref> [10] </ref>. These images show how depth correction reduces the artifacts present in the images. 3 The Lumigraph System This section discusses many of the practical implementation issues related to creating a Lumigraph and generating images from it. Figure 9 shows a block diagram of the system.
Reference: [11] <author> KATAYAMA, A., TANAKA, K., OSHINO, T., AND TAMURA, H. </author> <title> A viewpoint independent stereoscopic display using interpolation of multi-viewpoint images. </title> <booktitle> Steroscopic displays and virtal reality sytems II (SPIE) 2409 (1995), </booktitle> <pages> 11-20. </pages>
Reference-contexts: The captured information can then be used to represent the light source in global illumination simulations. Ashdown traces this idea of the surface-restricted plenoptic function back to Levin [15]. A limited version of the work reported here has been described by Katayama et al. <ref> [11] </ref>. In their system, a camera is moved along a track, capturing a 1D array of images of some object. This information is then used to generate new images of the object from other points in space.
Reference: [12] <author> KLINKER, G. J. </author> <title> A Physical Approach to Color Image Understanding. A K Peters, </title> <type> Wellesley, </type> <institution> Massachusetts, </institution> <year> 1993. </year>
Reference-contexts: We employ the octree construction algorithm described in [26] for this process. Each input image is first segmented into a binary object/background image using a blue-screen technique <ref> [12] </ref> (Figure 12). An octree representation of a cube that completely encloses the object is initialized. Then for each segmented image, each voxel at a coarse level of the octree is projected onto the image plane and tested against the silhouette of the object.
Reference: [13] <author> LAURENTINI, A. </author> <title> The visual hull concept for silhouette-basedimage understanding. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 16, </journal> <volume> 2 (Feb-ruary 1994), </volume> <pages> 150-162. </pages>
Reference-contexts: Each pixel in the input video stream coming from the hand-held camera represents a single sample 6 Technically, the volume is a superset of the visual hull of the object <ref> [13] </ref>. L (s k ; t k ; u k ; v k ), of the Lumigraph function. As a result, the sample points in the domain cannot be pre-specified or controlled. In addition, there is no guarantee that the incoming samples are evenly spaced.
Reference: [14] <author> LAVEAU, S., AND FAUGERAS, O. </author> <title> 3-D scene representation as a collection of images and fundamental matrices. </title> <type> Tech. Rep. 2205, </type> <institution> INRIA-Sophia Antipolis, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Given the disparity (roughly equivalent to depth information), they can then move pixels to create images from new vantage points. Similar work using stereo pairs of planar images is discussed in <ref> [14] </ref>. This paper extends the work begun with Quicktime VR and Plen-optic Modeling by further developing the idea of capturing the complete flow of light in a region of the environment. Such a flow is described by a plenoptic function [1].
Reference: [15] <author> LEVIN, R. E. </author> <booktitle> Photometriccharacteristics of light-controllingapparatus. Illuminating Engineering 66, 4 (1971), </booktitle> <pages> 205-215. </pages>
Reference-contexts: Ashdown [2] describes a gantry for moving a camera along a sphere surrounding a luminaire of interest. The captured information can then be used to represent the light source in global illumination simulations. Ashdown traces this idea of the surface-restricted plenoptic function back to Levin <ref> [15] </ref>. A limited version of the work reported here has been described by Katayama et al. [11]. In their system, a camera is moved along a track, capturing a 1D array of images of some object.
Reference: [16] <author> LEVOY, M., AND HANRAHAN, P. </author> <title> Light-field rendering. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1996. </year>
Reference-contexts: Because they only capture the plenoptic function along a line, they only obtain horizontal parallax, and distortion is introduced as soon as the new virtual camera leaves the line. Finally, in work concurrent to our own, Levoy and Hanrahan <ref> [16] </ref> represent a 4D function that allows for undistorted, full parallax views of the object from anywhere in space. 2.2 Parameterization of the 4D Lumigraph There are many potential ways to parameterize the four dimensions of the Lumigraph. <p> We adopt a parameterization similar to that used in digital holographic stereograms [9] and also used by Levoy and Hanrahan <ref> [16] </ref>. We begin with a cube to organize a Lumigraph and, without loss of generality, only consider for discussion a single square face of the cube (the full Lumigraph is constructed from six such faces). 2 We only consider a snapshot of the function, thus time is eliminated. <p> This analogy is pursued in <ref> [16] </ref>. In Figure 16 we show images generated from Lumigraphs. The geometric scene consisted of a partial cube with the pink face in front, yellow face in back, and the brown face on the floor. <p> One way in which this can be accomplished is to use a special motion control platform to place the real camera at positions and orientations coincident with the (s i ; t j ) gridpoints <ref> [16] </ref>. While this is a reasonable solution, we are interested in acquiring the images with a regular hand-held camera. This results in a simpler and cheaper system, and may extend the range of applicability to larger scenes and objects.
Reference: [17] <author> LEWIS, R. R., AND FOURNIER, A. </author> <title> Light-driven global illumination with a wavelet representation of light transport. </title> <type> UBC CS Technical Reports 95-28, </type> <institution> University of British Columbia, </institution> <year> 1995. </year>
Reference-contexts: The plenoptic function is represented on the surface of a cube surrounding some region; that information is all that is needed to simulate the light transfer from that region of space to all other regions <ref> [17] </ref>. In the context of illumination engineering, this idea has been used to model and represent the illumination due to physical luminaires. Ashdown [2] describes a gantry for moving a camera along a sphere surrounding a luminaire of interest.
Reference: [18] <author> LITWINOWICZ, P., AND WILLIAMS, L. </author> <title> Animating images with drawings. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1994, </year> <pages> pp. 409-412. </pages>
Reference-contexts: Because the number of sample points may be large ( 10 8 ) and because we are working in a 4 dimensional space, it is too expensive to solve systems of equations (as is done when solving thin-plate problems <ref> [28, 18] </ref>) or to build spatial data structures (such as Delauny triangulations). In addition to the number of sample points, the distribution of the data samples have two qualities that make the problem particularly difficult. First, the sampling density can be quite sparse, with large gaps in many regions.
Reference: [19] <author> MCMILLAN, L., AND BISHOP, G. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1995, </year> <pages> pp. 39-46. </pages>
Reference-contexts: Chen and Williams [7] and Werner et al [30] have investigated smooth interpolation between images by modeling the motion of pixels (i.e., the optical flow) as one moves from one camera position to another. In Plenoptic Modeling <ref> [19] </ref>, McMillan and Bishop discuss finding the disparity of each pixel in stereo pairs of cylindrical images. Given the disparity (roughly equivalent to depth information), they can then move pixels to create images from new vantage points. Similar work using stereo pairs of planar images is discussed in [14]. <p> In this paper, we discuss computational methods for capturing and representing a plenoptic function, and for using such a representation to render images of the environment from any arbitrary viewpoint. Unlike Chen and Williams' view interpolation [7] and McMil-lan and Bishop's plenoptic modeling <ref> [19] </ref>, our approach does not rely explicitly on any optical flow information. Such information is often difficult to obtain in practice, particularly in environments with complex visibility relationships or specular surfaces. We do, however, use approximate geometric information to improve the quality of the reconstruction at lower sampling densities.
Reference: [20] <author> MITCHELL, D. P. </author> <title> Generating antialiased images at low samplingdensities. </title> <booktitle> Computer Graphics 21, 4 (1987), </booktitle> <pages> 65-72. </pages>
Reference-contexts: This low resolution data is then used to fill in the gaps at higher resolutions. The second of these problems, the non-uniformity of the sampling density, has been addressed by Mitchell <ref> [20] </ref>. He solves the problem of obtaining the value of a pixel that has been super-sampled with a non-uniform density. In this problem, when averaging the sample values, one does not want the result to be overly influenced by the regions sampled most densely.
Reference: [21] <author> POTMESIL, M. </author> <title> Generating octree models of 3D objects from their silhouettes in a sequence of images. Computer Vision, Graphics, </title> <booktitle> and Image Processing 40 (1987), </booktitle> <pages> 1-29. </pages>
Reference-contexts: To produce complete, closed 3D models, several approaches have been tried. One family of techniques builds 3D volumetric models the current and previous camera positions on a viewing sphere. The goal of the user is to paint the sphere. directly from silhouettes of the object being viewed <ref> [21] </ref>. Another approach is to fit a deformable 3D model to sparse stereo data. Despite over 20 years of research, the reliable extraction of accurate 3D geometric information from imagery (without the use of active illumination and positioning hardware) remains elusive.
Reference: [22] <author> POWELL, M. J. D., AND SWANN, J. </author> <title> Weighted uniform sampling a monte carlo technique for reducing variance. </title> <journal> J. Inst. Maths Applics 2 (1966), </journal> <pages> 228-236. </pages>
Reference-contexts: This variance is similar to that obtained using importance sampling, which is often much smaller than the crude Monte Carlo estimator. For a full analysis of this es timator, see <ref> [22] </ref>. 3.4.2 Pull In the pull phase, lower resolution approximations of the function are derived using a set of wider kernels. These wider kernels are defined by linearly summing together the higher resolution kernels ( ~ B i = k r k ) using some discrete sequence ~ h.
Reference: [23] <author> ROSENFELD, A., AND KAK, A. C. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1976. </year>
Reference-contexts: Pixels with an intensity between T 1 and T 2 are considered black only if they have a black neighbor, otherwise they are considered white. The binary thresholded image is then searched for connected components <ref> [23] </ref>. Sets of connected components with similar centers of gravity are the likely candidates for the markers. Finally, the ratio of radii in each marker is used to uniquely identify the marker.
Reference: [24] <author> SIMONCELLI, E. P., FREEMAN, W. T., ADELSON, E. H., AND HEEGER, D. J. </author> <title> Shiftable multiscale transforms. </title> <journal> IEEE Transactions on Information Theory 38 (1992), </journal> <pages> 587-607. </pages>
Reference-contexts: We have chosen to use the quadralinear basis for its computational simplicity and the C 0 continuity it imposes on ~ L. However, because this basis is not band limited by the Nyquist frequency, and thus the corresponding finite dimensional function space is not shift invariant <ref> [24] </ref>, the grid structure will be slightly noticeable in our results. 2.3.2 Projection into the Chosen Basis Given a continuous Lumigraph, L, and a choice of basis for the finite dimensional Lumigraph, ~ L, we still need to define a projection of L into ~ L (i.e., we need to find
Reference: [25] <author> SNYDER, J. M., AND KAJIYA, J. T. </author> <title> Generative modeling: A symbolic system for geometric modeling. </title> <booktitle> Computer Graphics 26, 2 (1992), </booktitle> <pages> 369-379. </pages>
Reference-contexts: To perform the integration against the kernel ~ B, multiple rays per coefficient can be averaged by jit tering the camera and pixel locations, weighting each image using ~ B. For ray traced renderings, we have used the ray tracing program provided with the Generative Modeling package <ref> [25] </ref>. 3.2 Capture for Real Scenes Computing the Lumigraph for a real object requires the acquisition of object images from a large number of viewpoints.
Reference: [26] <author> SZELISKI, R. </author> <title> Rapid octree construction from image sequences. CVGIP: </title> <booktitle> Image Understanding 58, </booktitle> <month> 1 (July </month> <year> 1993), </year> <pages> 23-32. </pages>
Reference-contexts: Fortunately, a rough estimate of the shape of the object is enough to greatly aid in the capture and reconstruction of images from a Lu-migraph. We employ the octree construction algorithm described in <ref> [26] </ref> for this process. Each input image is first segmented into a binary object/background image using a blue-screen technique [12] (Figure 12). An octree representation of a cube that completely encloses the object is initialized.
Reference: [27] <author> TAUBIN, G. </author> <title> A signal processing approach to fair surface design. </title> <booktitle> In Computer Graphics, Annual Conference Series, </booktitle> <year> 1995, </year> <pages> pp. 351-358. </pages>
Reference-contexts: The resulting 3D model consists of a collection of voxels describing a volume which is known to contain the object 6 (Figure 12). The external polygons are collected and the resulting polyhedron is then smoothed using Taubin's polyhedral smoothing algorithm <ref> [27] </ref>. 3.4 Rebinning As described in Equation 1, the coefficient associated with the basis function B i;j;p;q is defined as the integral of the continuous Lu migraph function multiplied by some kernel function ~ B.
Reference: [28] <author> TERZOPOULOS, D. </author> <title> Regularization of inverse visual problems involvingdiscon-tinuities. </title> <journal> IEEE PAMI 8, </journal> <month> 4 (July </month> <year> 1986), </year> <pages> 413-424. </pages>
Reference-contexts: Because the number of sample points may be large ( 10 8 ) and because we are working in a 4 dimensional space, it is too expensive to solve systems of equations (as is done when solving thin-plate problems <ref> [28, 18] </ref>) or to build spatial data structures (such as Delauny triangulations). In addition to the number of sample points, the distribution of the data samples have two qualities that make the problem particularly difficult. First, the sampling density can be quite sparse, with large gaps in many regions.
Reference: [29] <author> TSAI, R. Y. </author> <title> A versatile camera calibration technique for high-accuracy 3D machine vision metrologyusing off-the-shelf TV cameras and lenses. </title> <journal> IEEE Journal of Robotics and Automation RA-3, </journal> <month> 4 (August </month> <year> 1987), </year> <pages> 323-344. </pages>
Reference-contexts: This latter mapping not only includes a perspective (pinhole) projection from the 3D coordinates to undistorted image coordinates, but also a radial distortion transformation and a final translation and scaling into screen coordinates <ref> [29, 31] </ref>. We use a camera with a fixed lens, thus the intrinsic parameters remain constant throughout the process and need to be estimated only once, before the data acquisition begins. Extrinsic parameters, however, change constantly and need to be recomputed for each new video frame. <p> Extrinsic parameters, however, change constantly and need to be recomputed for each new video frame. Fortunately, given the intrinsic parameters, this can be done efficiently and accurately with many fewer calibration points. To compute the intrinsic and extrinsic parameters, we employ an algorithm originally developed by Tsai <ref> [29] </ref> and extended by Willson [31]. A specially designed stage provides the source of calibration data (see Figure 10). The stage has two walls fixed together at a right angle and a base that can be detached from the walls and rotated in 90 degree increments.
Reference: [30] <author> WERNER, T., HERSCH, R. D., AND HLAVAC, V. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Fifth International Conference on Computer Vision (ICCV'95) (Cambridge, </booktitle> <address> Massachusetts, </address> <month> June </month> <year> 1995), </year> <pages> pp. 957-962. </pages>
Reference-contexts: Instead, a series of captured environment maps allow a user to look around a scene from fixed points in space. One can also flip through different views of an object to create the illusion of a 3D model. Chen and Williams [7] and Werner et al <ref> [30] </ref> have investigated smooth interpolation between images by modeling the motion of pixels (i.e., the optical flow) as one moves from one camera position to another. In Plenoptic Modeling [19], McMillan and Bishop discuss finding the disparity of each pixel in stereo pairs of cylindrical images.
Reference: [31] <author> WILLSON, R. G. </author> <title> Modeling and Calibration of Automated Zoom Lenses. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: This latter mapping not only includes a perspective (pinhole) projection from the 3D coordinates to undistorted image coordinates, but also a radial distortion transformation and a final translation and scaling into screen coordinates <ref> [29, 31] </ref>. We use a camera with a fixed lens, thus the intrinsic parameters remain constant throughout the process and need to be estimated only once, before the data acquisition begins. Extrinsic parameters, however, change constantly and need to be recomputed for each new video frame. <p> Fortunately, given the intrinsic parameters, this can be done efficiently and accurately with many fewer calibration points. To compute the intrinsic and extrinsic parameters, we employ an algorithm originally developed by Tsai [29] and extended by Willson <ref> [31] </ref>. A specially designed stage provides the source of calibration data (see Figure 10). The stage has two walls fixed together at a right angle and a base that can be detached from the walls and rotated in 90 degree increments.
References-found: 31

