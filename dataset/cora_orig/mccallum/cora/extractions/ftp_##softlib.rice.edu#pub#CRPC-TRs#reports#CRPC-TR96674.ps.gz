URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96674.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: RANK ORDERING AND POSITIVE BASES IN PATTERN SEARCH ALGORITHMS  
Author: ROBERT MICHAEL LEWIS AND VIRGINIA TORCZON 
Keyword: Key Words. direct search methods, pattern search, positive linear dependence  
Abstract: We present two new classes of pattern search algorithms for unconstrained minimization: the rank ordered and the positive basis pattern search methods. These algorithms can nearly halve the worst case cost of an iteration compared to the classical pattern search algorithms. The rank ordered pattern search methods are based on a heuristic for approximating the direction of steepest descent, while the positive basis pattern search methods are motivated by a generalization of the geometry characteristic of the patterns of the classical methods. We describe the new classes of algorithms and present the attendant global convergence analysis. 1. Introduction. In this paper we introduce two new classes of pattern search 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. E. P. </author> <title> Box, Evolutionary operation: a method for increasing industrial productivity, </title> <journal> Applied Statistics, </journal> <volume> 6 (1957), </volume> <pages> pp. 81-101. </pages>
Reference-contexts: Moreover, the simple heuristics that motivate rank ordered and positive basis pattern search methods are intuitively appealing and make the methods straightforward to describe. Pattern search methods form a class of "steep descent" procedures (a term we will shortly explain) for nonlinear minimization. Examples include <ref> [1] </ref>, [6], and [11]. While these algorithms have no explicit recourse to a Taylor series model of the objective, or any information about directional derivatives, one can develop a global convergence analysis for pattern search methods with results similar to those for quasi-Newton methods.
Reference: [2] <author> M. J. Box, D. Davies, and W. H. Swann, </author> <title> Non-Linear Optimization Techniques, </title> <journal> ICI Monograph No. </journal> <volume> 5, </volume> <publisher> Oliver & Boyd, Edinburgh, </publisher> <year> 1969. </year>
Reference-contexts: Note that this uses an exploratory moves strategy that is identical to that used for the Evolutionary Operation algorithm (see either <ref> [2] </ref>, [10], or [12]). but instead of using a two-level factorial design, which requires 2 n objective values per iteration, the positive basis allows us to implement a design with as few as n + 1 objective values per iteration.
Reference: [3] <author> C. Davis, </author> <title> Theory of positive linear dependence, </title> <journal> American Journal of Mathematics, </journal> <year> (1954), </year> <pages> pp. 733-746. </pages>
Reference-contexts: It was in the course of re-examining the work on direct search methods by Yu Wen-ci [13, 14] that we realized the utility of the theory of positive linear dependence developed by C. Davis in <ref> [3] </ref> for generalizing pattern search methods in a useful way. One can view the positive basis pattern search methods as the natural generalization of the algorithms considered in [12]. We say "natural" for the following reason. <p> In the class of algorithms studied in [12], the 2n core directions played the technical role of ensuring that we would search in a direction that made a positive inner product with the direction of steepest descent. The notion of a positive basis <ref> [3] </ref> is the correct way to generalize this latter property, and allows us to reduce, a priori, the cardinality of the core set of steps from 2n to as few as n + 1. <p> Positive Basis Pattern Search Methods. The positive basis pattern search methods will be described in terms of the notion of positive linear dependence developed in <ref> [3] </ref>. Positive linear dependence captures the essential technical role played by the core pattern in [12]. The positive basis pattern search methods are also motivated by the requirements of parallel computing. <p> Positive linear dependence. We present here the ideas we will need from the theory of positive linear dependence <ref> [3] </ref>. <p> A positive basis is a positively independent set whose positive span is R n . The following theorem from <ref> [3] </ref> indicates that a positive spanning set contains at least n + 1 vectors. R.M. Lewis, V. Torczon 8 - ? v 0 r k v 1 D D ~ ~ ~ ~ ~ ~ ~ ~ ~ r r r D D D D D Fig. 3. <p> Theorem 2.1. Suppose fa 1 ; ; a r g positively spans R n . Then fa 2 ; ; a r g lin early spans R n . The following characterizations of positive spanning sets can be found in <ref> [3] </ref> as well. Theorem 2.2. Suppose fa 1 ; ; a r g, a i 6= 0, linearly spans R n . Then the follow ing are equivalent: 1. fa 1 ; ; a r g positively spans R n . 2. <p> For every i = 1; ; r, a i is in the convex cone positively spanned by the remaining a i . A positive basis that contains n + 1 elements is called minimal. One can also show <ref> [3] </ref> that a positive basis can have no more than 2n elements; such a basis is called maximal. A maximal positive basis has a very special structure: it must consist of a linear basis for R n and the negatives of those basis vectors.
Reference: [4] <author> J. E. Dennis and V. Torczon, </author> <title> Managing approximation models in optimization, in Multidisciplinary Design Optimization: State-of-the-Art, </title> <editor> N. Alexandrov and M. Y. Hussaini, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <note> to appear. </note>
Reference-contexts: As a consequence, one can construct pattern search algorithms that require, in the best case, only one new objective value per iteration. This feature is used to advantage in a proposed strategy for scientific and engineering optimization <ref> [4] </ref> for problems in which the computational cost of a single objective evaluation is sufficiently great as to merit particular care in choosing steps at which to evaluate the objective.
Reference: [5] <author> J. E. Dennis, Jr. and V. Torczon, </author> <title> Direct search methods on parallel machines, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 448-474. </pages>
Reference-contexts: A straightforward rank ordered pattern search method. 2.1.1. An example of a rank ordered pattern search method. In Fig. 1 we give an example of a rank ordered pattern search method. This algorithm is a sequential variant of the parallel multidirectional search algorithm of Dennis and Torczon <ref> [5] </ref>. One of three possible steps are accepted at the conclusion of each iteration: a reflection step, an expansion step, or a shrink step. Examples for each of these three steps in R 2 can be seen in Figs. 2, 3, and 4, respectively.
Reference: [6] <author> R. Hooke and T. A. Jeeves, </author> <title> Direct search solution of numerical and statistical problems, </title> <journal> Journal of the Association for Computing Machinery (ACM), </journal> <volume> 8 (1961), </volume> <pages> pp. 212-229. </pages>
Reference-contexts: Moreover, the simple heuristics that motivate rank ordered and positive basis pattern search methods are intuitively appealing and make the methods straightforward to describe. Pattern search methods form a class of "steep descent" procedures (a term we will shortly explain) for nonlinear minimization. Examples include [1], <ref> [6] </ref>, and [11]. While these algorithms have no explicit recourse to a Taylor series model of the objective, or any information about directional derivatives, one can develop a global convergence analysis for pattern search methods with results similar to those for quasi-Newton methods.
Reference: [7] <author> R. M. Lewis and V. J. Torczon, </author> <title> Pattern search algorithms for bound constrained minimization, </title> <type> Tech. Rep. 96-20, </type> <institution> ICASE, Mail Stop 132C, NASA Langley Research Center, </institution> <address> Hampton, Virginia 23681-0001, </address> <year> 1996. </year> <note> Submitted to SIAM J. on Optimization. </note>
Reference-contexts: This is a corollary of Theorem 3.18. The outline of the proof for Theorem 3.1 follows that of [12] as developed in <ref> [7] </ref>: 1. First we show that given &gt; 0, there exists ffi &gt; 0 (independent of k) such that if k g (x k ) k &gt; and k &lt; ffi, then a pattern search algorithm will find an acceptable step without further decrease of k . 2. <p> The new results can be found in x3.1, where we develop the case first for the positive basis pattern search methods and then for the rank ordered pattern search methods. The remaining sections complete the analysis, relying largely on results developed in <ref> [7] </ref> and [12]. 3.1. Existence of a direction of descent. The following proposition is the justification for calling pattern search algorithms "methods of steep descent." Proposition 3.3. <p> Finding an acceptable step. From this point on the convergence analyses for positive basis pattern search methods and rank ordered pattern search methods are identical and follow more or less directly from results developed in <ref> [7] </ref> and [12]. The following two results come from [12], to which we refer the reader for the proofs. The first result indicates one sense in which k regulates step length. R.M. Lewis, V. Torczon 22 Lemma 3.13 (Lemma 3.1 from [12]). <p> The positive basis pattern search methods for unconstrained minimization require a positive basis for R n ; the correct analog for bound constrained minimization should require a positive basis for the tangent cone of the feasible region at each iterate. The work in <ref> [7] </ref> uses such a basis that appears to be maximal in size. The theory of positive linear dependence used in the unconstrained case suggests a line of development to sharpen the results in [7], which may in turn reduce the computational cost per iteration in the bound constrained case. <p> The work in <ref> [7] </ref> uses such a basis that appears to be maximal in size. The theory of positive linear dependence used in the unconstrained case suggests a line of development to sharpen the results in [7], which may in turn reduce the computational cost per iteration in the bound constrained case. Similarly, the heuristic of approximating the direction of steepest descent using the best and worst of n + 1 vertices (determined by their objective values) should also be applicable to the bound constrained case.
Reference: [8] <author> K. I. M. McKinnon, </author> <title> Convergence of the Nelder-Mead simplex method to a non-stationary point, </title> <type> Tech. Rep. MS 96-006, </type> <institution> Department of Mathematics and Statistics, University of Edinburgh, </institution> <month> May </month> <year> 1996. </year> <note> Submitted to SIAM J. on Optimization. </note>
Reference-contexts: We note that this heuristic also motivates the direct search method of Nelder and Mead, which is not a pattern search method and is known not to be robust <ref> [8] </ref>, and the aforementioned direct search method of Spendley, Hext, and Himsworth [9], the very interesting analysis of which we will discuss elsewhere.
Reference: [9] <author> W. Spendley, G. R. Hext, and F. R. Himsworth, </author> <title> Sequential application of simplex designs in optimisation and evolutionary operation, </title> <journal> Technometrics, </journal> <volume> 4 (1962), </volume> <pages> pp. 441-461. </pages>
Reference-contexts: This intuition seems to have been in the minds of the early developers of the broader class of direct search methods; for instance, Spendley, Hext, and Himsworth <ref> [9] </ref> refer to their algorithm as a method of "steep ascent" (they were considering maximization) to indicate its kinship to the method of steepest ascent. <p> We note that this heuristic also motivates the direct search method of Nelder and Mead, which is not a pattern search method and is known not to be robust [8], and the aforementioned direct search method of Spendley, Hext, and Himsworth <ref> [9] </ref>, the very interesting analysis of which we will discuss elsewhere. The heuristic of using the best and worst objective values to suggest a direction of steep descent reduces the cardinality of the core set of steps from 2n to n + 1, R.M. Lewis, V.
Reference: [10] <author> W. H. Swann, </author> <title> Direct search methods, in Numerical Methods for Unconstrained Optimization, </title> <editor> W. Murray, ed., </editor> <publisher> Academic Press, </publisher> <address> London and New York, </address> <year> 1972, </year> <pages> pp. 13-28. </pages>
Reference-contexts: Note that this uses an exploratory moves strategy that is identical to that used for the Evolutionary Operation algorithm (see either [2], <ref> [10] </ref>, or [12]). but instead of using a two-level factorial design, which requires 2 n objective values per iteration, the positive basis allows us to implement a design with as few as n + 1 objective values per iteration.
Reference: [11] <author> V. Torczon, </author> <title> Multi-Directional Search: A Direct Search Algorithm for Parallel Machines, </title> <type> PhD thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1989; available as TR90-07, </note> <institution> Department of Computational and Applied Mathematics, Rice University, Houston, TX. </institution> <note> [12] , On the convergence of pattern search methods, SIAM Journal on Optimization, (1997). To appear. </note>
Reference-contexts: Moreover, the simple heuristics that motivate rank ordered and positive basis pattern search methods are intuitively appealing and make the methods straightforward to describe. Pattern search methods form a class of "steep descent" procedures (a term we will shortly explain) for nonlinear minimization. Examples include [1], [6], and <ref> [11] </ref>. While these algorithms have no explicit recourse to a Taylor series model of the objective, or any information about directional derivatives, one can develop a global convergence analysis for pattern search methods with results similar to those for quasi-Newton methods. <p> The notation S k is chosen to suggest simplex, and R k is meant to suggest reflection, as in the multidirectional search algorithm <ref> [11] </ref>.
Reference: [13] <author> Y. Wen-ci, </author> <title> The convergence property of the simplex evolutionary techniques, </title> <journal> Scientia Sinica, Special Issue of Mathematics, </journal> <volume> 1 (1979), </volume> <pages> pp. </pages> <month> 68-77. </month> <title> [14] , Positive basis and a class of direct search techniques, </title> <journal> Scientia Sinica, Special Issue of Mathematics, </journal> <volume> 1 (1979), </volume> <pages> pp. 53-67. </pages>
Reference-contexts: The positive basis pattern search methods have the attractive feature that they avoid this sequentiality and thus are well-suited for parallel implementation. It was in the course of re-examining the work on direct search methods by Yu Wen-ci <ref> [13, 14] </ref> that we realized the utility of the theory of positive linear dependence developed by C. Davis in [3] for generalizing pattern search methods in a useful way. One can view the positive basis pattern search methods as the natural generalization of the algorithms considered in [12].
References-found: 12

