URL: http://www.umiacs.umd.edu/users/sarit/Articles/argaij.ps
Refering-URL: http://www.umiacs.umd.edu/users/sarit/articles.html
Root-URL: 
Email: Email: fargamon,sarit,sinag@cs.biu.ac.il  
Title: Utility-based On-Line Exploration for Repeated Navigation in an Embedded Graph 1  
Author: Shlomo Argamon-Engelson a; Sarit Kraus a;b Sigalit Sina a 
Address: Ramat Gan 52900, Israel  College Park, MD 20742  
Affiliation: a Dept. Mathematics and Computer Science, Bar-Ilan University,  b Institute for Advanced Computer Studies, University of Maryland,  
Abstract: In this paper, we address the tradeoff between exploration and exploitation for agents which need to learn more about the structure of their environment in order to perform more effectively. For example, a robot may need to learn the most efficient routes between important sites in its environment. We compare on-line and off-line exploration for a repeated task, where the agent is given some particular task to perform some number of times. Tasks are modeled as navigation on a graph embedded in the plane. This paper describes a utility-based on-line exploration algorithm for repeated tasks, which takes into account both the costs and potential benefits (over future task repetitions) of different exploratory actions. Exploration is performed in a greedy fashion, with the locally optimal exploratory action performed on each task repetition. We experimentally evaluated our utility-based on-line algorithm against a heuristic search algorithm for off-line exploration as well as a randomized online exploration algorithm. We found that for a single repeated task, utility-based on-line exploration consistently outperforms the alternatives, unless the number of task repetitions is very high. In addition, we extended the algorithms for the case of multiple repeated tasks, where the agent has a different randomly-chosen task to perform each time. Here too, we found that utility-based on-line exploration is often preferred. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. B. Barber and H. Huhdanpaa. </author> <title> Qhull software package, </title> <note> 1995. http://www.geom.umn.edu/software/qhull. </note>
Reference-contexts: Random environments are typically very non-planar, and a small amount of exploration can help a lot, if a good shortcut is found. The other type of environment we considered are triangle environments, in which edges are randomly generated based on the Delaunay triangulation <ref> [1] </ref> of the nodes' positions. These environments model, for example, real-world robot navigation tasks. <p> environments with an overall edge probability of p were generated as subsets of Delaunay triangulations via the following procedure: (1) Generate n points randomly in a unit square (with a uniform distribu tion). (2) Compute the Delaunay triangulation G = (V; E) of the points (using the qhull software package <ref> [1] </ref>). (3) Assign each edge a random weight. (4) Find a minimal spanning tree T = (V; E T ) for this weighted graph. (5) Let E 0 = E T . R = 100 EWP/Def.
Reference: [2] <author> K. Basye, T. Dean, and J. S. Vitter. </author> <title> Coping with uncertainty in map learning. </title> <type> Technical Report CS-89-27, </type> <institution> Brown University Department of Computer Science, </institution> <month> June </month> <year> 1989. </year>
Reference: [3] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference: [4] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1985. </year>
Reference-contexts: The agent's task is to maximize its total reward, learning something about the reward and transition probability distributions in the process (based on known priors over those distributions). Optimal solutions are known for this problem [3,5], as well as its simpler variant the k-armed bandit problem <ref> [4] </ref>, but these solutions are of exponential complexity [20]. Various faster approximate strategies have also been proposed for this problem [19,23,28] and have been shown to be useful.
Reference: [5] <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference: [6] <author> M. Betke, R. L. Rivest, and M. Singh. </author> <title> Piecemeal learning of an unknown environment. </title> <journal> Machine Learning, </journal> 18(2/3):231-254, 1995. 
Reference-contexts: Similar graph-based world models have long been used in theoretical work on off-line learning, such as [12,13,24,26]. These models have also been extended to include sensor and effector noise [2,10]. A task situated between off-line and on-line exploration, is piecemeal exploration <ref> [6] </ref>, in which the robot must return to its `home' every so often. On-line state-space learning has been addressed by so-called `real-time' search algorithms [8,17,22]. These algorithms typically achieve a single task (possibly moving) while learning the structure of the environment.
Reference: [7] <author> A. Blum and P. Chalasani. </author> <title> An on-line algorithm for improving performance in navigation. </title> <booktitle> In Proc. Symp. Foundations of Computer Science, </booktitle> <pages> pages 2-11, </pages> <year> 1993. </year>
Reference-contexts: With on-line exploration, performance of the task becomes more efficient over time, as the agent learns better ways to perform it <ref> [7] </ref>. As the number of repetitions gets very large, it might be preferable to learn the entire structure of the state-space first, so that the agent can be sure to use the most efficient plan for its task.
Reference: [8] <author> F. Chimura and M. Tokoro. </author> <title> The trailblazer search: A new method for searching and capturing moving targets. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 1347-1352, </pages> <year> 1994. </year>
Reference: [9] <author> P. Cucka, N. S. Netanyahu, and A. Rosenfeld. </author> <title> Learning in navigation: Goal finding in graphs. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 10(5), </volume> <year> 1996. </year>
Reference-contexts: Improved efficiency in real-time search has been obtained by restricting the use of heuristic exploitation [27] and by not requiring the algorithm to find an optimal policy [18]. In more specific problem settings, more informed heuristics may be applied, for example Cucka et al. <ref> [9] </ref> use information about the geometric direction of the goal to heuristically improve the exploration process. In this paper, we compare on-line and off-line exploration for a repeated task, 2 where the agent is given some particular task (s) to perform some number of times. <p> Following Cucka et al. <ref> [9] </ref>, we use the minimal-angle heuristic to inform the search. 5 Note that there may be an `exploration edge' between nodes v 1 and v 2 , even if there is no real edge in the environment between them. <p> If all edges incident on the current node have previously been traversed, the agent backtracks to the last node it explored. In the worst case, this strategy will explore the entire environment, but in practice this heuristic is often quite efficient <ref> [9] </ref>. Given the minimal-angle heuristic as a default method for traversing an exploration edge, we now consider how to evaluate the expected cost of such a traversal.
Reference: [10] <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Kaelbling, E. Kokkevis, and O. Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <year> 1995. </year>
Reference: [11] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):35-74, </volume> <year> 1995. </year>
Reference-contexts: Our approach of incremental utility-based exploration may also be compared to the anytime algorithm of Dean et al. <ref> [11] </ref> for decision-theoretic planning in (completely known) stochastic environments. Their method creates an optimal policy for a small part of the environment (the envelope), and incrementally extends the envelope in order to increase the usefulness of the generated policy.
Reference: [12] <author> X. Deng, T. Kameda, and C. Papadimitriou. </author> <title> How to learn in an unknown environment. </title> <booktitle> In Proc. of the 32nd Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 298-303. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference: [13] <author> X. Deng and C. H. Papadimitriou. </author> <title> Exploring an unkhown graph. </title> <booktitle> In IEEE, editor, Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 355-361. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct. </month> <year> 1990. </year>
Reference: [14] <author> J. L. Devore. </author> <title> Probability and Statistics for Engineering and Sciences. </title> <publisher> Brooks/Cole Publishing Company, </publisher> <address> Pacific Grove, California, </address> <year> 1991. </year>
Reference-contexts: These two influences are balanced in environments of intermediate density. In Tables 3 and 4 we compare the performance of the three exploration algorithms (EWP, EBP, and REWP) against each other for random environments. We tested our results statistically using analysis of paired data <ref> [14, Section 9.3] </ref> to compare the algorithms' mean efficiencies. To a 0.05 confidence level, EWP is more efficient than EBP and REWP, while REWP is more efficient than EBP.
Reference: [15] <author> O. Etzioni. </author> <title> Embedding decision-analytic control in a learning architecture. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 129-159, </pages> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The utility of exploration then increases with the number of future task repetitions. The algorithm chooses exploratory actions in a greedy fashion, in order to avoid exponential increase in the number of future courses of action that need to be considered (similar to Etzioni's <ref> [15] </ref> use of a greedy marginal utility heuristic). Methods for solving Markov decision processes (MDPs) also involve an on-line tradeoff between exploration and exploitation [25]. In these problems, the environment is modeled as a probabilistic finite-state machine where the agent receives rewards for being in particular states.
Reference: [16] <author> P. Haddawy, A. Doan, and R. Goodwin. </author> <title> Efficient decision-theoretic planning: Techniques and empirical analysis. </title> <booktitle> In Proc. Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 229-236, </pages> <year> 1995. </year>
Reference: [17] <author> T. Ishida and R. Korf. </author> <title> A moving target search: A real-time search for changing goals. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(6) </volume> <pages> 609-619, </pages> <year> 1995. </year>
Reference: [18] <author> T. Ishida and M. Shimbo. </author> <title> Improving the learning efficiencies of realtime search. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 305-310, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: In the worst case, these algorithms will explore the entire state-space, depending on the quality of the heuristic function. Improved efficiency in real-time search has been obtained by restricting the use of heuristic exploitation [27] and by not requiring the algorithm to find an optimal policy <ref> [18] </ref>. In more specific problem settings, more informed heuristics may be applied, for example Cucka et al. [9] use information about the geometric direction of the goal to heuristically improve the exploration process.
Reference: [19] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [20] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Optimal solutions are known for this problem [3,5], as well as its simpler variant the k-armed bandit problem [4], but these solutions are of exponential complexity <ref> [20] </ref>. Various faster approximate strategies have also been proposed for this problem [19,23,28] and have been shown to be useful.
Reference: [21] <author> G. I. Karakoulas. </author> <title> Probabilistic exploration in planning while learning. </title> <editor> In P. Besnard and S. Hanks, editors, </editor> <booktitle> Eleventh Annual Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 352-361, </pages> <year> 1995. </year>
Reference: [22] <author> R. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42(2-3):189-211, </volume> <year> 1990. </year>
Reference-contexts: The agent first explores any unknown edges incident on its current location (in heuristic order for reaching a target node), and backtracks when no such unexplored edges exist. We did not use a more efficient search strategy such as Real-Time A* <ref> [22] </ref>, since such strategies require that the agent be able in one step to know all the children of its current node, which our model disallows. In early experiments, we found that if EBP is allowed to explore until the entire environment is known, EWP is always considerably more efficient.
Reference: [23] <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference: [24] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> Shortest paths without a map. </title> <journal> Theoretical Computer Science, </journal> <volume> 84(1) </volume> <pages> 127-150, </pages> <year> 1991. </year>
Reference: [25] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Methods for solving Markov decision processes (MDPs) also involve an on-line tradeoff between exploration and exploitation <ref> [25] </ref>. In these problems, the environment is modeled as a probabilistic finite-state machine where the agent receives rewards for being in particular states. The transition from one state to another occurs with some probability depending on what action the agent takes from the first state.
Reference: [26] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences (extended abstract). </title> <booktitle> In Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 411-420, </pages> <address> Seattle, Washington, </address> <year> 1989. </year>
Reference: [27] <author> Y. Smirnov, S. Koenig, M. M. Veloso, and R. G. Simmons. </author> <title> Efficient goal-directed exploration. </title> <booktitle> In Proc. National Conference on Artificial Intelligence, </booktitle> <pages> pages 292-297, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: In the worst case, these algorithms will explore the entire state-space, depending on the quality of the heuristic function. Improved efficiency in real-time search has been obtained by restricting the use of heuristic exploitation <ref> [27] </ref> and by not requiring the algorithm to find an optimal policy [18]. In more specific problem settings, more informed heuristics may be applied, for example Cucka et al. [9] use information about the geometric direction of the goal to heuristically improve the exploration process.
Reference: [28] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. Seventh Int'l Conf. on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [29] <author> S. Thrun. </author> <title> The role of exploration in learning control. In Handbook for Intelligent Control: Neural, Fuzzy and Adaptive Approaches, </title> <address> Florence, Kentucky 41022, 1992. </address> <publisher> Van Nostrand Reinhold. </publisher> <pages> 21 </pages>
Reference-contexts: This randomized exploration while performing (REWP) algorithm is loosely modeled on probabilistic exploration methods used in reinforcement learning (see, eg, <ref> [29] </ref>). The algorithm generally follows the shortest known path towards its current goal (or uses default search if no path is known), but if the current node has any unknown neighboring edges, with exploration probability p e REWP attempts to traverse one of those unknown edges (chosen randomly).
References-found: 29

