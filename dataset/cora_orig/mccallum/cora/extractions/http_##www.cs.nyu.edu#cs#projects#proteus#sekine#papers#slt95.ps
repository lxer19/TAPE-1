URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/slt95.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Title: NYU/BBN 1994 CSR Evaluation  
Author: Satoshi Sekine, John Sterling, and Ralph Grishman 
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Abstract: NYU's research objective is to determine whether non-local, linguistically-based word preferences can be used to enhance speech recognition. We are working jointly with BBN, and our system takes as input the N-best hypotheses generated by BBN (with acoustic and n-gram language model scores for each hypothesis). Our goal is to generate scores based on both intersen-tential dependencies (related to topic coherence) and intrasentential dependencies (connected by syntactic relations) to complement the usual contiguous-word (n-gram) dependencies. We describe our sublanguage model, which is intended to capture the effects on vocabulary of topic coherence within an article. We report several measures of this model, including its effect on word error rate when combined with the BBN acoustic and language model scores. We also briefly describe our initial efforts at applying a syntactic language model, and a word model using syntactic relations (a semantic model). 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R Kuhn: </author> <title> Speech Recognition and the Frequency of Recently Used Words: A Modified Markov Model for Natural Language 12th International Conference on Computational Linguistic (1988) </title>
Reference-contexts: There have been several attempts in the last few years to make use of these properties. One of them is the cache language model <ref> [1] </ref> BBN Speech Recognizer -N-best NYU Language Models C C CW fl fl Acoustic & n-gram scores Language score Combine scores ? Best hypothesis [2] [3]. This is a dynamic language model which utilizes the partially dictated document (cache) in order to predict the next word.
Reference: 2. <author> F Jelinek, B Merialdo, S Roukos, and M Strauss: </author> <title> A Dynamic Language Model for Speech Recognition Proceedings DARPA Speech and Natural Language Workshop (1991) </title>
Reference-contexts: There have been several attempts in the last few years to make use of these properties. One of them is the cache language model [1] BBN Speech Recognizer -N-best NYU Language Models C C CW fl fl Acoustic & n-gram scores Language score Combine scores ? Best hypothesis <ref> [2] </ref> [3]. This is a dynamic language model which utilizes the partially dictated document (cache) in order to predict the next word. In essence, it is based on the observation that a word which has already appeared in a document has an increased probability of reappearing.
Reference: 3. <author> J Kupiec: </author> <title> Probabilistic Models of Short and Long Distance Word Dependencies in Running Text Proceedings DARPA Speech and Natural Language Workshop (1989) </title>
Reference-contexts: There have been several attempts in the last few years to make use of these properties. One of them is the cache language model [1] BBN Speech Recognizer -N-best NYU Language Models C C CW fl fl Acoustic & n-gram scores Language score Combine scores ? Best hypothesis [2] <ref> [3] </ref>. This is a dynamic language model which utilizes the partially dictated document (cache) in order to predict the next word. In essence, it is based on the observation that a word which has already appeared in a document has an increased probability of reappearing.
Reference: 4. <author> Ronald Rosenfeld and Xuedong Huang: </author> <booktitle> Improvements in Stochastic Language Modeling Proceedings DARPA Speech and Natural Language Workshop (1992) </booktitle>
Reference-contexts: For short documents, however, such as newspaper articles, the number of words which can be accumulated from the prior text will be small and accordingly the benefit of the method will generally be small. Rosenfeld proposed the trigger model to try to overcome this limitation <ref> [4] </ref>. He used a large corpus to build a set of trigger pairs, each of which consists of a pair of words appearing in a document of the corpus. These pairs are used as a component in the probabilistic model. <p> allow us to make sharper predictions in the case of well-defined topics or sub-languages, and reduce the problems due to homographs by searching for a conjunction of words. (Rosenfeld has indicated that it may be possible to achieve similar results by an enhancement to trigger pairs which uses multiple triggers <ref> [4] </ref>.) In addition, our approach needs less machine power; this was one of the major problems of Rosen-feld's approach.
Reference: 5. <author> Rukumini Iyer, Mari Ostendirf, J.Robin Rohlicek: </author> <booktitle> Language Modeling with Sentence-Level Mixtures Proceedings ARPA Human Language Technology Workshop (1994) </booktitle>
Reference-contexts: A different approach to capturing the effects of sublanguage is represented by the work at Boston University <ref> [5] </ref>. They sought to capture sentence-level dependencies through the use of a mixture of n-gram language models; each n-gram model was derived from a set of paragraphs which have been clustered together, and loosely corresponds to the notion of topic. Our approach can be briefly summarized as follows.
Reference: 6. <author> Satoshi Sekine: </author> <title> Automatic Sublanguage Identification for a New Text Second Annual Workshop on Very Large Corpora (1994) </title>
Reference-contexts: In the mixture approach, the corpus was statically clustered into a small number of very broad topics. We have previously reported on the effectiveness of sublanguage identification measured in terms of the frequency of overlapping words between the article and the mini-corpus <ref> [6] </ref> [7]. This is the first report on the application of the technique to speech recognition. For speech recognition, the scores calculated by the sublanguagecompo-nent are linearly combined with BBN's scores, with the result used to select the best hypothesis from the N-best sentences.
Reference: 7. <author> Satoshi Sekine: </author> <title> A New Direction for Sublanguage NLP International Conference on New Methods in Language Processing (1994) </title>
Reference-contexts: In the mixture approach, the corpus was statically clustered into a small number of very broad topics. We have previously reported on the effectiveness of sublanguage identification measured in terms of the frequency of overlapping words between the article and the mini-corpus [6] <ref> [7] </ref>. This is the first report on the application of the technique to speech recognition. For speech recognition, the scores calculated by the sublanguagecompo-nent are linearly combined with BBN's scores, with the result used to select the best hypothesis from the N-best sentences.
Reference: 8. <author> K.Sparck-Jones: </author> <title> Index Term Weighting Information Storage and Retrieval, </title> <address> Vol.9, </address> <month> p619-633 </month> <year> (1973) </year>
Reference-contexts: Collect Similar Articles We used a cosine similarity metric with inverse document frequency weighting. This is a standard information retrieval metric based on the assumption that the higher frequency words provide less information about topics <ref> [8] </ref>. Similarity scores are computed between the prior sentences of the current article and all of the articles in our corpus, with each score normalized by the length of the article.
Reference: 9. <editor> Ralph Grishman and John Sterling: </editor> <booktitle> Smoothing of Automatically Generated Selectional Constraints Proceedings ARPA Human Language Technology Workshop (1993) </booktitle>
Reference-contexts: A probability is associated with each dominance relation: the probability that a verb takes a particular noun as the head of its object, the probability that a noun takes a particular adjective as its modifier. This model is described in more detail in <ref> [9] </ref> [10]. Such a model can be trained from a text corpus, in a manner analogous to training a probabilistic context-free grammar, by generating sentence parses and extracting the dominance relations.
Reference: 10. <author> Ralph Grishman and John Sterling: </author> <title> Generalizing Automatically Generated Selectional Patterns ProceedingsCOLING 94 (1994) </title>
Reference-contexts: A probability is associated with each dominance relation: the probability that a verb takes a particular noun as the head of its object, the probability that a noun takes a particular adjective as its modifier. This model is described in more detail in [9] <ref> [10] </ref>. Such a model can be trained from a text corpus, in a manner analogous to training a probabilistic context-free grammar, by generating sentence parses and extracting the dominance relations.
Reference: 11. <author> F Jelinek: </author> <title> Self-organized Language Modeling for Speech Recognition Readings in Speech Recognition (1990) </title>
Reference-contexts: * can be identified rapidly (so that repeated experiments over 1 or 2 GB are feasible) * can be identified reliably (thus introducing little error into the probability estimation) * can be used to significantly reduce language model perplexity While suggestions for limited use of syntactic structures are not new <ref> [11] </ref>, we believe that no systematic exploration along these lines has been made. 6. Acknowledgments The work reported here was supported by the Advanced Research Projects Agency under contract DABT63-93-C-0058 from the Department of the Army. We would like to thank the collaboration partners at BBN, in particular R.
References-found: 11

