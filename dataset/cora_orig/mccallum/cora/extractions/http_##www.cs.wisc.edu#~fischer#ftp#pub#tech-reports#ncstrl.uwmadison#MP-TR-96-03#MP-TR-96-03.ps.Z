URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-03/MP-TR-96-03.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-96-03/
Root-URL: http://www.cs.wisc.edu
Email: email: paulb@cs.wisc.edu, olvi@cs.wisc.edu email:nstreet@cs.okstate.edu  
Phone: 205  
Title: Clustering via Concave Minimization  
Author: P. S. Bradley and O. L. Mangasarian W. N. Street 
Address: 1210 West Dayton Street  Madison, WI 53706 Stillwater, OK 74078  
Affiliation: Computer Sciences Department Computer Science Department University of Wisconsin Oklahoma State University  Mathematical Sciences  
Abstract: The problem of assigning m points in the n-dimensional real space R n to k clusters is formulated as that of determining k centers in R n such that the sum of distances of each point to the nearest center is minimized. If a polyhedral distance is used, the problem can be formulated as that of minimizing a piecewise-linear concave function on a polyhedral set which is shown to be equivalent to a bilinear program: minimizing a bilinear function on a polyhedral set. A fast finite k-Median Algorithm consisting of solving few linear programs in closed form leads to a stationary point of the bilinear program. Computational testing on a number of real-world databases was carried out. On the Wisconsin Diagnostic Breast Cancer (WDBC) database, k-Median training set correctness was comparable to that of the k-Mean Algorithm, however its testing set correctness was better. Additionally, on the Wisconsin Prognostic Breast Cancer (WPBC) database, distinct and clinically important survival curves were extracted by the k-Median Algorithm, whereas the k-Mean Algorithm failed to obtain such distinct survival curves for the same database.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Al-Sultan. </author> <title> A Tabu search approach to the clustering problem. </title> <journal> Pattern Recognition, </journal> <volume> 28(9) </volume> <pages> 1443-1451, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction The unsupervised assignment of elements of a given set to groups or clusters of like points, is the objective of cluster analysis. There are many approaches to this problem, including statistical [9], machine learning [7], integer and mathematical programming <ref> [18, 1] </ref>. In this paper we concentrate on a simple concave minimization formulation of the problem that leads to a finite and fast algorithm.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrim-ination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: The k-Median training set correctness is compared to that of the k-Mean Algorithm as well as the training correctness of a supervised learning method, a perceptron trained by robust linear programming <ref> [2] </ref>. Table 1 shows results averaged over ten random starts for the publicly available Wisconsin Diagnostic Breast Cancer (WDBC) database as well as three others [15, 16]. We note that for two of the databases k-Median outperformed k-Mean, and for the other two k-Mean was better. <p> Testing correctness is determined by the number of points in testing subset correctly classified by this assignment. This is compared to the correctness of a supervised learning method, a perceptron trained via robust linear programming <ref> [2] </ref>, using the leave-one-out strategy applied to the testing subset only. This comparison is then carried out for various sizes of the testing subset. Figure 1 shows the results averaged over 50 runs for each of 7 testing subset sizes.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: This piecewise-linear concave function minimization on a polyhedral set turns out to be equivalent to a bilinear program <ref> [3] </ref>. We use an effective linearization of the bilinear program proposed in [3, Algorithm 2.1] to solve our problem by solving a few linear programs. Because of the simple structure, these linear programs can be explicitly solved in closed form, thus leading to the finite k-Median Algorithm 2.3 below. <p> This piecewise-linear concave function minimization on a polyhedral set turns out to be equivalent to a bilinear program [3]. We use an effective linearization of the bilinear program proposed in <ref> [3, Algorithm 2.1] </ref> to solve our problem by solving a few linear programs. Because of the simple structure, these linear programs can be explicitly solved in closed form, thus leading to the finite k-Median Algorithm 2.3 below. In Section 3 we give computational results on five real-world databases. <p> Hence the Uncoupled Bilinear Program Algorithm UBPA <ref> [3, Algorithm 2.1] </ref> is applicable. Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). <p> Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). The al gorithm terminates in a finite number of iterations at a stationary point satisfying the minimum principle necessary optimality condition for problem (4) <ref> [3, Theorem 2.1] </ref>. We note however, because of the simple structure the bilinear program (4), the two linear programs can be solved explicitly in closed form. This leads to the following algorithmic implementation.
Reference: [4] <author> S.-J. Chung. </author> <title> NP-completeness of the linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 60 </volume> <pages> 393-399, </pages> <year> 1989. </year>
Reference-contexts: If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex. Nevertheless, minimizing a piecewise-linear concave function on a polyhedral set is NP-hard, because the general linear complementarity problem, which is NP-complete <ref> [4] </ref>, can be reduced to such a problem [11, Lemma 1]. Given this fact we try to look for effective methods for processing this problem. We propose reformulation of problem (1) as a bilinear program.
Reference: [5] <author> F. Cordellier and J. Ch. Fiorot. </author> <title> On the Fermat-Weber problem with convex cost functionals. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 295-311, </pages> <year> 1978. </year>
Reference-contexts: Without this squared distance term, the subproblem of the k-Mean Algorithm becomes the considerably harder Weber problem <ref> [17, 5] </ref> which locates a center in R n closest in sum of Euclidean distances (not their squares!) to a finite set of given points. The Weber problem has no closed form solution.
Reference: [6] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <journal> Communications of the ACM, </journal> <volume> 39 </volume> <pages> 27-34, </pages> <year> 1996. </year>
Reference-contexts: Remark 3.3 Separability of Survival Curves In mining medical databases, survival curves [10] are important prognostic tools. We applied the k-Median and k-Mean (k = 3) Algorithms, as knowledge discovery in database (KDD) tools <ref> [6] </ref>, to the Wisconsin Prognostic Breast Cancer Database (WPBC) [15] using only two features: tumor size and lymph node status. Survival curves were constructed for (a) k-Median (b) k-Mean Algorithms each cluster, representing expected percent of surviving patients as a function of time, for patients in that cluster.
Reference: [7] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The unsupervised assignment of elements of a given set to groups or clusters of like points, is the objective of cluster analysis. There are many approaches to this problem, including statistical [9], machine learning <ref> [7] </ref>, integer and mathematical programming [18, 1]. In this paper we concentrate on a simple concave minimization formulation of the problem that leads to a finite and fast algorithm.
Reference: [8] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Although there is no guarantee that such a point is a global solution to our original problem, numerical tests on five real-world databases indicate that the k-Median Algorithm is comparable to or better than the k-Mean Algorithm <ref> [18, 9, 8] </ref>. This may be due to the fact that outliers have less influence on the k-Median Algorithm which utilizes the 1-norm distance. In contrast the k-Mean Algorithm uses squares of 2-norm distances to generate cluster centers which may be inaccurate if outliers are present. <p> In contrast the k-Mean Algorithm uses squares of 2-norm distances to generate cluster centers which may be inaccurate if outliers are present. We also note that clustering algorithms based on statistical assumptions that minimize some function of scatter matrices do not appear to have convergence proofs <ref> [8, pp. 508-515] </ref>, however convergence to a partial optimal solution is given in [18] for k-Mean type algorithms. We outline now the contents of the paper. <p> (b) Cluster Center Update: For ` = 1; : : : ; k choose C j+1 ` as a median of all A T i assigned to C j Stop when C j+1 j Although the k-Median Algorithm is similar to the k-Mean Algorithm wherein the 2-norm distance is used <ref> [18, 8, 9] </ref>, it differs from it computationally, and theoretically.
Reference: [9] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction The unsupervised assignment of elements of a given set to groups or clusters of like points, is the objective of cluster analysis. There are many approaches to this problem, including statistical <ref> [9] </ref>, machine learning [7], integer and mathematical programming [18, 1]. In this paper we concentrate on a simple concave minimization formulation of the problem that leads to a finite and fast algorithm. <p> Although there is no guarantee that such a point is a global solution to our original problem, numerical tests on five real-world databases indicate that the k-Median Algorithm is comparable to or better than the k-Mean Algorithm <ref> [18, 9, 8] </ref>. This may be due to the fact that outliers have less influence on the k-Median Algorithm which utilizes the 1-norm distance. In contrast the k-Mean Algorithm uses squares of 2-norm distances to generate cluster centers which may be inaccurate if outliers are present. <p> (b) Cluster Center Update: For ` = 1; : : : ; k choose C j+1 ` as a median of all A T i assigned to C j Stop when C j+1 j Although the k-Median Algorithm is similar to the k-Mean Algorithm wherein the 2-norm distance is used <ref> [18, 8, 9] </ref>, it differs from it computationally, and theoretically.
Reference: [10] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: The k-Median Algorithm test set correctness remained fairly constant in the range of 92.3% to 93.5%, while the k-Mean Algorithm test set correctness was lower and more varied in the range 88.0% to 91.3%. Remark 3.3 Separability of Survival Curves In mining medical databases, survival curves <ref> [10] </ref> are important prognostic tools. We applied the k-Median and k-Mean (k = 3) Algorithms, as knowledge discovery in database (KDD) tools [6], to the Wisconsin Prognostic Breast Cancer Database (WPBC) [15] using only two features: tumor size and lymph node status.
Reference: [11] <author> O. L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex. Nevertheless, minimizing a piecewise-linear concave function on a polyhedral set is NP-hard, because the general linear complementarity problem, which is NP-complete [4], can be reduced to such a problem <ref> [11, Lemma 1] </ref>. Given this fact we try to look for effective methods for processing this problem. We propose reformulation of problem (1) as a bilinear program.
Reference: [12] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: Given this fact we try to look for effective methods for processing this problem. We propose reformulation of problem (1) as a bilinear program. Such reformulations have been very effective in computationally solving NP-complete linear complementarity problems [14] as well as other difficult machine learning <ref> [12] </ref> and optimization problems with equilibrium constraints [12]. In order to carry out this reformulation we need the following simple lemma. Lemma 2.1 Let a 2 R k . <p> We propose reformulation of problem (1) as a bilinear program. Such reformulations have been very effective in computationally solving NP-complete linear complementarity problems [14] as well as other difficult machine learning <ref> [12] </ref> and optimization problems with equilibrium constraints [12]. In order to carry out this reformulation we need the following simple lemma. Lemma 2.1 Let a 2 R k .
Reference: [13] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: Hence e T D i` bounds the 1-norm distance between A i and C ` . We note immediately that since the objective function of (1) is the sum of minima of k linear (and hence concave) functions, it is a piecewise-linear concave function <ref> [13, Corollary 4.1.14] </ref>. If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex.
Reference: [14] <author> O. L. Mangasarian. </author> <title> The linear complementarity problem as a separable bilinear program. </title> <journal> Journal of Global Optimization, </journal> <volume> 6 </volume> <pages> 153-161, </pages> <year> 1995. </year>
Reference-contexts: Given this fact we try to look for effective methods for processing this problem. We propose reformulation of problem (1) as a bilinear program. Such reformulations have been very effective in computationally solving NP-complete linear complementarity problems <ref> [14] </ref> as well as other difficult machine learning [12] and optimization problems with equilibrium constraints [12]. In order to carry out this reformulation we need the following simple lemma. Lemma 2.1 Let a 2 R k .
Reference: [15] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, www.ics.uci.edu/AI/ML/MLDBRepository.html, 1992. </address>
Reference-contexts: Table 1 shows results averaged over ten random starts for the publicly available Wisconsin Diagnostic Breast Cancer (WDBC) database as well as three others <ref> [15, 16] </ref>. We note that for two of the databases k-Median outperformed k-Mean, and for the other two k-Mean was better. <p> The WDBC database <ref> [15] </ref> is split into training and testing subsets of different proportions. The k-Median and k-Mean Algorithms (k = 2) are applied to the training subset. The centers are given class labels determined by the majority class of training subset points assigned to the cluster. <p> Remark 3.3 Separability of Survival Curves In mining medical databases, survival curves [10] are important prognostic tools. We applied the k-Median and k-Mean (k = 3) Algorithms, as knowledge discovery in database (KDD) tools [6], to the Wisconsin Prognostic Breast Cancer Database (WPBC) <ref> [15] </ref> using only two features: tumor size and lymph node status. Survival curves were constructed for (a) k-Median (b) k-Mean Algorithms each cluster, representing expected percent of surviving patients as a function of time, for patients in that cluster.
Reference: [16] <author> S. Odewahn, E. Stockwell, R. Pennington, R. Hummphreys, and W. Zumach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: Table 1 shows results averaged over ten random starts for the publicly available Wisconsin Diagnostic Breast Cancer (WDBC) database as well as three others <ref> [15, 16] </ref>. We note that for two of the databases k-Median outperformed k-Mean, and for the other two k-Mean was better.
Reference: [17] <author> M. L. Overton. </author> <title> A quadratically convergent method for minimizing a sum of euclidean norms. </title> <journal> Mathematical Programming, </journal> <volume> 27 </volume> <pages> 34-63, </pages> <year> 1983. </year>
Reference-contexts: Without this squared distance term, the subproblem of the k-Mean Algorithm becomes the considerably harder Weber problem <ref> [17, 5] </ref> which locates a center in R n closest in sum of Euclidean distances (not their squares!) to a finite set of given points. The Weber problem has no closed form solution.
Reference: [18] <author> S. Z. Selim and M. A. Ismail. </author> <title> K-Means-Type algorithms: a generalized convergence theorem and characterization of local optimality. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:81-87, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction The unsupervised assignment of elements of a given set to groups or clusters of like points, is the objective of cluster analysis. There are many approaches to this problem, including statistical [9], machine learning [7], integer and mathematical programming <ref> [18, 1] </ref>. In this paper we concentrate on a simple concave minimization formulation of the problem that leads to a finite and fast algorithm. <p> Although there is no guarantee that such a point is a global solution to our original problem, numerical tests on five real-world databases indicate that the k-Median Algorithm is comparable to or better than the k-Mean Algorithm <ref> [18, 9, 8] </ref>. This may be due to the fact that outliers have less influence on the k-Median Algorithm which utilizes the 1-norm distance. In contrast the k-Mean Algorithm uses squares of 2-norm distances to generate cluster centers which may be inaccurate if outliers are present. <p> We also note that clustering algorithms based on statistical assumptions that minimize some function of scatter matrices do not appear to have convergence proofs [8, pp. 508-515], however convergence to a partial optimal solution is given in <ref> [18] </ref> for k-Mean type algorithms. We outline now the contents of the paper. In Section 2, we formulate the clustering problem for a fixed number of clusters, as that of minimizing the sum of the 1-norm distances of each point to the nearest cluster center. <p> (b) Cluster Center Update: For ` = 1; : : : ; k choose C j+1 ` as a median of all A T i assigned to C j Stop when C j+1 j Although the k-Median Algorithm is similar to the k-Mean Algorithm wherein the 2-norm distance is used <ref> [18, 8, 9] </ref>, it differs from it computationally, and theoretically. <p> The concave minimization problem of <ref> [18] </ref> is not in the original space of the problem variables, that is, the cluster center variables, (C; D), but merely in the space of variables T that assign points to clusters.
References-found: 18

