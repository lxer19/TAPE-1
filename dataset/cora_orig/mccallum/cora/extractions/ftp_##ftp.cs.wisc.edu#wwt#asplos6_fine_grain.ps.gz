URL: ftp://ftp.cs.wisc.edu/wwt/asplos6_fine_grain.ps.gz
Refering-URL: http://www.cs.wisc.edu/~stever/pubs.html
Root-URL: 
Email: wwt@cs.wisc.edu  
Title: Fine-grain Access Control for Distributed Shared Memory  
Author: Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.  
Abstract: This paper discusses implementations of fine-grain memory access control, which selectively restricts reads and writes to cache-block-sized memory regions. Fine-grain access control forms the basis of efficient cache-coherent shared memory. This paper focuses on low-cost implementations that require little or no additional hardware. These techniques permit efficient implementation of shared memory on a wide range of parallel systems, thereby providing shared-memory codes with a portability previously limited to message passing. This paper categorizes techniques based on where access control is enforced and where access conflicts are handled. We incorporated three techniques that require no additional hardware into Blizzard, a system that supports distributed shared memory on the CM-5. The first adds a software lookup before each shared-memory reference by modifying the program's executable. The second uses the memory's error correcting code (ECC) as cache-block valid bits. The third is a hybrid. The software technique ranged from slightly faster to two times slower than the ECC approach. Blizzard's performance is roughly comparable to a hardware shared-memory machine. These results argue that clusters of workstations or personal computers with networks comparable to the CM-5's will be able to support the same shared-memory interfaces as supercomputers. fl This work is supported in part by NSF PYI/NYI Awards CCR-9157366 and CCR-9357779, NSF Grants CCR-9101035 and MIP-9225097, an AT&T Ph.D. Fellowship, and donations from Digital Equipment Corporation, Thinking Machines Corporation, and Xerox Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Tom Anderson, David Culler, and David Patterson. </author> <title> A Case for Networks of Workstations: NOW. </title> <type> Technical report, </type> <institution> Computer Science Division (EECS), University of California at Berkeley, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Parallel computing is becoming widely available with the emergence of networks of workstations as the parallel "minicomputers" of the future <ref> [1] </ref>. Unfortunately, current systems directly support only message-passing communication. Shared memory is limited to page-based systems, such as TreadMarks [17], which are not sequentially consistent and which can perform poorly in the presence of fine-grain data sharing [11].
Reference: [2] <author> Henri E. Bal, Andrew S. Tanenbaum, and M. Frans Kaashoek. Orca: </author> <title> A Language for Distributed Programming. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 25(5) </volume> <pages> 17-24, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Either a compiler or a program executable editing tool [21] can insert software tests. We use the latter approach in Blizzard so every compiler need not reim-plement test analysis and code generation. Compiler-inserted lookups, however, can exploit application-level information. Orca <ref> [2] </ref>, for example, provides access control on program objects instead of blocks. TLB. Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems|for example, IVY [26], Munin [4], and TreadMarks [17].
Reference: [3] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Second ACM SIGPLAN 9 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 168-176, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Hardware. The DASH, KSR-1, and S3.mp systems implement actions in dedicated hardware, which provides high performance for a single protocol. While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications [16] or even for all data structures within an application <ref> [3, 12] </ref>. High design costs and resource constraints also make custom hardware unattractive. Hybrid hardware/software 3 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.
Reference: [4] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Compiler-inserted lookups, however, can exploit application-level information. Orca [2], for example, provides access control on program objects instead of blocks. TLB. Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems|for example, IVY [26], Munin <ref> [4] </ref>, and TreadMarks [17]. Though unimplemented by current commodity processors, additional, per-block access bits in a TLB entry could provide fine-grain access control. <p> Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility.
Reference: [5] <author> David Chaiken and John Kubiatowicz. </author> <type> Personal Communication, </type> <month> March </month> <year> 1994. </year>
Reference-contexts: Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility.
Reference: [6] <author> David Chaiken, John Kubiatowicz, and Anant Agar-wal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The "lock bits" in some IBM RISC machines, including the 801 [7] and RS/6000 [29], provide access control on 128-byte blocks, but they do not support the three-state model described above. Cache controller. The MIT Alewife <ref> [6] </ref> and Kendall Square Research KSR-1 [18] shared-memory systems use custom cache controllers to implement access control. In addition to detecting misses in hardware cache (s), these controllers determine when to invoke a protocol action. <p> Table 2: Overheads of fine-grain access control for various systems (in processor cycles). protocols|e.g., Alewife's LimitLESS <ref> [6] </ref> and Dir 1 SW [13]|implement the expected common cases in hardware and trap to system software to handle complex, infrequent events. Primary processor. Performing actions on the main CPU provides protocol flexibility and avoids the additional cost of custom hardware or an additional CPU.
Reference: [7] <author> Albert Chang and Mark F. Mergen. </author> <title> 801 Storage: Architecture and Programming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 28-50, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Nevertheless, it forms the basis of several distributed-shared-memory systems|for example, IVY [26], Munin [4], and TreadMarks [17]. Though unimplemented by current commodity processors, additional, per-block access bits in a TLB entry could provide fine-grain access control. The "lock bits" in some IBM RISC machines, including the 801 <ref> [7] </ref> and RS/6000 [29], provide access control on 128-byte blocks, but they do not support the three-state model described above. Cache controller. The MIT Alewife [6] and Kendall Square Research KSR-1 [18] shared-memory systems use custom cache controllers to implement access control.
Reference: [8] <author> David R. Cheriton, Hendrik A. Goosen, and Philip Ma-chanick. </author> <title> Restructuring a Parallel Simulation to Improve Cache Behavior in a Shared-Memory Multiprocessor: A First Experience. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 109-118, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Most of Tomcatv's computation is on large, private arrays, and it is possible that the KSR-1 suffers expensive, unnecessary remote misses on these arrays due to cache conflicts. Mp3d incurs a large number of misses due to poor locality <ref> [8] </ref>. The high miss ratio explains both Blizzard-E's poor performance relative to the KSR-1and Blizzard-S's ability to outperform Blizzard-E.
Reference: [9] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These benchmarks|four from the SPLASH suite [34]|were written for hardware shared-memory systems. Page-granularity DSM systems generally perform poorly on these codes because of their fine-grain communication and write sharing <ref> [9] </ref>. We ran these benchmarks on five 32-node systems: Blizzard-E, Blizzard-S, Blizzard-ES, Blizzard-P, and a Kendall Square KSR-1. The first three Blizzard systems use a full-map invalidation protocol implemented in user-level software (Stache) [32] with a 128-byte block size. Blizzard-P is a sequentially-consistent, page-granularity version of Blizzard.
Reference: [10] <author> William J. Dally and D. Scott Wills. </author> <title> Universal Mechanism for Concurrency. </title> <booktitle> In PARLE '89: Parallel Architectures and Languages Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Because the deferred-interrupt flag is a bit in a global register, the polling overhead is extremely low. Our current implementation polls on control-flow back-edges. 3.3 Blizzard-E: ECC Although several systems have memory tags and fine-grain access control, e.g., J-machine <ref> [10] </ref>, most contemporary commercial machines|including the CM-5|lack this facility. In Blizzard-E, we synthesized the Invalid state on the CM-5 by forcing uncorrectable errors in the memory's error correcting code (ECC) via a diagnostic mode [30, 31].
Reference: [11] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 257-270, </pages> <year> 1989. </year>
Reference-contexts: Unfortunately, current systems directly support only message-passing communication. Shared memory is limited to page-based systems, such as TreadMarks [17], which are not sequentially consistent and which can perform poorly in the presence of fine-grain data sharing <ref> [11] </ref>. These systems lack fine-grain access control , a key feature of hardware shared-memory machines. Access control is the ability to selectively restrict reads and writes to memory regions.
Reference: [12] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Hardware. The DASH, KSR-1, and S3.mp systems implement actions in dedicated hardware, which provides high performance for a single protocol. While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications [16] or even for all data structures within an application <ref> [3, 12] </ref>. High design costs and resource constraints also make custom hardware unattractive. Hybrid hardware/software 3 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.
Reference: [13] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <note> Earlier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference: [14] <author> W. Daniel Hillis and Lewis W. Tucker. </author> <title> The CM-5 Connection Machine: A Scalable Supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 31-40, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The next section describes our modifications to the CM-5 operating system. We then describe the three implementations of fine-grain access control: Blizzard-S, Blizzard-E, and Blizzard-ES. 3.1 Kernel Support for Blizzard The Thinking Machines CM-5 <ref> [14] </ref> is a distributed-memory message-passing multiprocessor. Each processing node consists of a 33 MHz SPARC microprocessor with a 64 KB direct-mapped unified cache and a memory management unit, up to 128 MB of memory, a custom network interface chip, and optional custom vector units.
Reference: [15] <author> Peter Yan-Tek Hsu. </author> <title> Designing the TFP Microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2) </volume> <pages> 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Misses to remote physical addresses always invoke an action. Due to the KSR-1's COMA architecture, any reference that misses in both levels of cache requires protocol action. A trend toward on-chip second-level cache controllers <ref> [15] </ref> may make modified cache controllers incompatible with future commodity processors. Memory controller. If the system can guarantee that the processor's hardware caches never contain Invalid blocks and that ReadOnly blocks are cached in a read-only state, the memory controller can perform the lookup on hardware cache misses.
Reference: [16] <author> Anna R. Karlin, Mark S. Manasse, Larry Rudolph, and Daniel D. Sleator. </author> <title> Competitive Snoopy Caching. </title> <journal> Algo-rithmica, </journal> (3):79-119, 1988. 
Reference-contexts: Hardware. The DASH, KSR-1, and S3.mp systems implement actions in dedicated hardware, which provides high performance for a single protocol. While custom hardware performs an action quickly, research has shown that no single protocol is optimal for all applications <ref> [16] </ref> or even for all data structures within an application [3, 12]. High design costs and resource constraints also make custom hardware unattractive. Hybrid hardware/software 3 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.
Reference: [17] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <type> Technical Report 93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Parallel computing is becoming widely available with the emergence of networks of workstations as the parallel "minicomputers" of the future [1]. Unfortunately, current systems directly support only message-passing communication. Shared memory is limited to page-based systems, such as TreadMarks <ref> [17] </ref>, which are not sequentially consistent and which can perform poorly in the presence of fine-grain data sharing [11]. These systems lack fine-grain access control , a key feature of hardware shared-memory machines. Access control is the ability to selectively restrict reads and writes to memory regions. <p> Compiler-inserted lookups, however, can exploit application-level information. Orca [2], for example, provides access control on program objects instead of blocks. TLB. Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems|for example, IVY [26], Munin [4], and TreadMarks <ref> [17] </ref>. Though unimplemented by current commodity processors, additional, per-block access bits in a TLB entry could provide fine-grain access control. <p> Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility. <p> A noticeable exception to this rule is Alewife. Its custom support for fast context switching can invoke actions in 13 cycles. By contrast, TreadMarks requires 2600 cycles on a DEC-Station 5000/240 running Ultrix 4.3 <ref> [17] </ref>. Of course, the overhead is the fault of Ultrix 4.3, not Tread-Marks. With careful kernel coding (on a different processor), Blizzard-E's invocation overhead is 250 cycles, including 50 cycles that are added to every 4 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. <p> The systems in the first group of Table 2 provide low-latency interconnects that are closely coupled to the dedicated hardware or auxiliary processors. At the other extreme, TreadMarks communicates through Unix sockets using heavy-weight protocols. Its send time for a minimum size message is 3200 cycles (80 s) <ref> [17] </ref>. Blizzard benefits from the CM-5's low-latency network and user-level network interface. Blizzard's performance would be better if the network supported larger packets (as, for example, the CM-5E). To efficiently communicate, packets must hold at least a virtual address, program counter, and a memory block (40 bytes total on Blizzard).
Reference: [18] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: The "lock bits" in some IBM RISC machines, including the 801 [7] and RS/6000 [29], provide access control on 128-byte blocks, but they do not support the three-state model described above. Cache controller. The MIT Alewife [6] and Kendall Square Research KSR-1 <ref> [18] </ref> shared-memory systems use custom cache controllers to implement access control. In addition to detecting misses in hardware cache (s), these controllers determine when to invoke a protocol action. On Alewife, a local directory is consulted on misses to local physical addresses to determine if a protocol action is required.
Reference: [19] <author> R. E. Kessler and J. L. Schwarzmeier. </author> <title> CRAY T3D: A New Dimension for Cray Research. </title> <booktitle> In Proceedings of COMP-CON 93, </booktitle> <pages> pages 176-182, </pages> <address> San Francisco, California, </address> <month> Spring </month> <year> 1993. </year>
Reference-contexts: For example, combining an RS/6000-like TLB with Alewife's context switching support would permit fast access control and actions at low hardware cost. Unfortunately, modifying a processor chip is prohibitively expensive for most, if not all, parallel system designers. Even the relatively cost-insensitive supercomputer manufacturers are resorting to commodity microprocessors <ref> [19] </ref> because of the massive investment to produce competitive processors. Commodity processor manufacturers are unlikely to consider this hardware until fine-grain distributed shared memory is widespread.
Reference: [20] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Memory controller. If the system can guarantee that the processor's hardware caches never contain Invalid blocks and that ReadOnly blocks are cached in a read-only state, the memory controller can perform the lookup on hardware cache misses. This approach is used by Blizzard-E, Sun's S3.mp [28], and Stanford's FLASH <ref> [20] </ref>. As described in Section 3.3, Blizzard-E uses the CM-5's memory error-correcting code (ECC) to implement a cache-block valid bit. While effective, this approach has several shortcomings. ReadOnly access is enforced with page-level protection, so stores may incur an unnecessary protection trap. <p> Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility.
Reference: [21] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting Executable Files to Measure Program Behavior. </title> <journal> Software Practice & Experience, </journal> <volume> 24(2) </volume> <pages> 197-218, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Reprinted by permission of ACM. interface [32] to support distributed shared memory on a Thinking Machines CM-5. The first variant, Blizzard-S, adds a fast lookup before each shared-memory reference [22] by modifying the program's executable <ref> [21] </ref>. The second, Blizzard-E, employs the memory's error-correcting code (ECC) bits as block valid bits [30]. The third, Blizzard-ES, combines the two techniques. Blizzard's performance|running six programs written for hardware cache-coherent shared-memory machines|is consistent with our simulation results. <p> Reprinted by permission of ACM. potentially eliminate redundant tests. However, the asynchrony in parallel programs makes it difficult to predict whether a cache block will remain accessible between two instructions. Either a compiler or a program executable editing tool <ref> [21] </ref> can insert software tests. We use the latter approach in Blizzard so every compiler need not reim-plement test analysis and code generation. Compiler-inserted lookups, however, can exploit application-level information. Orca [2], for example, provides access control on program objects instead of blocks. TLB. <p> Stodolsky, et al.'s implementation uses a static variable to store the flags. To minimize overhead, our scheme uses a global register. 3.2 Blizzard-S: Software Blizzard-S implements fine-grain access control entirely in software, using a variant of the Fast-Cache simulation system [22]. Fast-Cache rewrites an existing executable file <ref> [21] </ref> to insert a state table lookup before every shared-memory reference. The lookup table is indexed by the virtual address and contains two bits for each 32-byte block (the size is a compile-time constant). The state and reference type (i.e., load or store) determine the handler.
Reference: [22] <author> Alvin R. Lebeck and David A. Wood. Fast-Cache: </author> <title> A New Abstraction for Memory System Simulation. </title> <type> Technical Report 1211, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Reprinted by permission of ACM. interface [32] to support distributed shared memory on a Thinking Machines CM-5. The first variant, Blizzard-S, adds a fast lookup before each shared-memory reference <ref> [22] </ref> by modifying the program's executable [21]. The second, Blizzard-E, employs the memory's error-correcting code (ECC) bits as block valid bits [30]. The third, Blizzard-ES, combines the two techniques. Blizzard's performance|running six programs written for hardware cache-coherent shared-memory machines|is consistent with our simulation results. <p> Stodolsky, et al.'s implementation uses a static variable to store the flags. To minimize overhead, our scheme uses a global register. 3.2 Blizzard-S: Software Blizzard-S implements fine-grain access control entirely in software, using a variant of the Fast-Cache simulation system <ref> [22] </ref>. Fast-Cache rewrites an existing executable file [21] to insert a state table lookup before every shared-memory reference. The lookup table is indexed by the virtual address and contains two bits for each 32-byte block (the size is a compile-time constant).
Reference: [23] <author> Charles E. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: These results show that clusters of workstations or personal computers can efficiently support shared memory when equipped with networks and network interfaces comparable to the CM-5's <ref> [23] </ref>. Blizzard also demonstrates the portability provided by the Tempest interface. Tempest allows clusters to support the same shared-memory abstraction as supercomputers, just as MPI and PVM support a common interface for coarse-grain message passing. The paper is organized as follows. Section 2 examines alternative implementations of fine-grain access control.
Reference: [24] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: However, future processors may integrate on-chip memory controllers (as do the TI Mi-croSPARC and HP PA7100LC). Bus snooping. When a processor supports a bus-based coherence scheme, a separate bus-snooping agent can perform a lookup similar to that performed by a memory controller. Stanford DASH <ref> [24] </ref> and Wisconsin Typhoon [32] employ this approach. On DASH, as on Alewife, local misses may require protocol action based on local directory state and remote misses always invoke an action.
Reference: [25] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hen-nessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility.
Reference: [26] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Compiler-inserted lookups, however, can exploit application-level information. Orca [2], for example, provides access control on program objects instead of blocks. TLB. Standard address translation hardware provides access control, though at memory page granularity. Nevertheless, it forms the basis of several distributed-shared-memory systems|for example, IVY <ref> [26] </ref>, Munin [4], and TreadMarks [17]. Though unimplemented by current commodity processors, additional, per-block access bits in a TLB entry could provide fine-grain access control.
Reference: [27] <author> NIMBUS Technology. </author> <title> NIM 6133 Memory Controller Specification. </title> <type> Technical report, </type> <institution> NIMBUS Technology, </institution> <year> 1993. </year>
Reference-contexts: ReadOnly access is enforced with page-level protection, so stores may incur an unnecessary protection trap. Also, modifying ECC values is an awkward and privileged operation. The Nimbus NIM6133, an MBUS memory controller co-designed by Nimbus Technology, Thinking Machines, and some of the authors <ref> [27] </ref>, addressed these problems. The NIM6133 supports Blizzard-like systems by storing a 4-bit access control tag with each 32-byte cache block. The controller encodes state tags in unassigned ECC values, which requires no additional DRAM. On a block write, the controller converts the 4-bit tag to a unary 1-of-16 encoding.
Reference: [28] <author> A. Nowatzyk, M. Monger, M. Parkin, E. Kelly, M. Bor-wne, G. Aybay, and D. Lee. S3.mp: </author> <title> A Multiprocessor in a Matchbox. </title> <booktitle> In Proc. </booktitle> <address> PASA, </address> <year> 1993. </year>
Reference-contexts: Memory controller. If the system can guarantee that the processor's hardware caches never contain Invalid blocks and that ReadOnly blocks are cached in a read-only state, the memory controller can perform the lookup on hardware cache misses. This approach is used by Blizzard-E, Sun's S3.mp <ref> [28] </ref>, and Stanford's FLASH [20]. As described in Section 3.3, Blizzard-E uses the CM-5's memory error-correcting code (ECC) to implement a cache-block valid bit. While effective, this approach has several shortcomings. ReadOnly access is enforced with page-level protection, so stores may incur an unnecessary protection trap.
Reference: [29] <author> R. R. Oehler and R. D. Groves. </author> <title> IBM RISC System/6000 processor architecture. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1) </volume> <pages> 32-36, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Though unimplemented by current commodity processors, additional, per-block access bits in a TLB entry could provide fine-grain access control. The "lock bits" in some IBM RISC machines, including the 801 [7] and RS/6000 <ref> [29] </ref>, provide access control on 128-byte blocks, but they do not support the three-state model described above. Cache controller. The MIT Alewife [6] and Kendall Square Research KSR-1 [18] shared-memory systems use custom cache controllers to implement access control.
Reference: [30] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proceedings of the Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The first variant, Blizzard-S, adds a fast lookup before each shared-memory reference [22] by modifying the program's executable [21]. The second, Blizzard-E, employs the memory's error-correcting code (ECC) bits as block valid bits <ref> [30] </ref>. The third, Blizzard-ES, combines the two techniques. Blizzard's performance|running six programs written for hardware cache-coherent shared-memory machines|is consistent with our simulation results. Blizzard-S's (software) performance ranged from slightly faster than Blizzard-E to twice as slow, depending on a program's shared-memory communication behavior. <p> Blizzard uses a variant of the "executive interface" extensions developed for the Wisconsin Wind Tunnel <ref> [30] </ref>. These extensions provide protected user-level memory-management support, including the ability to create, manipulate, and execute within subordinate contexts. The executive interface also provides support for fine-grain access control using a memory tag abstraction. Although the executive interface provides the required functionality, there are several important differences discussed below. <p> In Blizzard-E, we synthesized the Invalid state on the CM-5 by forcing uncorrectable errors in the memory's error correcting code (ECC) via a diagnostic mode <ref> [30, 31] </ref>. Running the SPARC cache in write-back mode causes all cache misses to appear as cache block fills. A fill causes an uncorrectable ECC error and generates a precise exception, which the kernel vectors to the user-level handler.
Reference: [31] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmet-rics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Our simulator is a modified version of the Wisconsin Wind Tunnel <ref> [31] </ref> modeling a machine similar to the CM-5. The target nodes contain a single-issue SPARC processor that runs the application, executes protocol action handlers on access faults, and handles incoming messages via interrupts. As on the CM-5, the processor has a 64 Kbyte direct-mapped cache with a 32-byte line size. <p> In Blizzard-E, we synthesized the Invalid state on the CM-5 by forcing uncorrectable errors in the memory's error correcting code (ECC) via a diagnostic mode <ref> [30, 31] </ref>. Running the SPARC cache in write-back mode causes all cache misses to appear as cache block fills. A fill causes an uncorrectable ECC error and generates a precise exception, which the kernel vectors to the user-level handler. <p> Running the SPARC cache in write-back mode causes all cache misses to appear as cache block fills. A fill causes an uncorrectable ECC error and generates a precise exception, which the kernel vectors to the user-level handler. The Wisconsin Wind Tunnel <ref> [31] </ref> and Tapeworm II [36] both use this ECC technique to simulate memory systems. This technique causes no loss of reliability. First, uncorrectable ECC faults are treated in the normal way (e.g., panic) unless a program specified a handler for a page.
Reference: [32] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The paper then focuses on three techniques suitable for existing hardware. We used these techniques to implement three variants of Blizzard, a system that uses the Tempest 1 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. interface <ref> [32] </ref> to support distributed shared memory on a Thinking Machines CM-5. The first variant, Blizzard-S, adds a fast lookup before each shared-memory reference [22] by modifying the program's executable [21]. The second, Blizzard-E, employs the memory's error-correcting code (ECC) bits as block valid bits [30]. <p> However, future processors may integrate on-chip memory controllers (as do the TI Mi-croSPARC and HP PA7100LC). Bus snooping. When a processor supports a bus-based coherence scheme, a separate bus-snooping agent can perform a lookup similar to that performed by a memory controller. Stanford DASH [24] and Wisconsin Typhoon <ref> [32] </ref> employ this approach. On DASH, as on Alewife, local misses may require protocol action based on local directory state and remote misses always invoke an action. <p> Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility. <p> Message data is sent and received using single-cycle 32-bit stores and loads to a memory-mapped network interface. Message interrupts incur a 100-cycle overhead before the interrupt handler starts. Fine-grain access control is maintained at 32-byte granularity. The applications run under the full-map, write-invalidate Stache coherence protocol with 32-byte blocks <ref> [32] </ref>. In the simulations of two programs shown in Figure 1, we varied the overhead of lookups and the overhead of invoking an action handler. The "ideal" case is an upper bound on performance. <p> Tempest is a communication and memory management interface <ref> [32] </ref> that can be supported on on a wide range of systems, ranging from multi-million-dollar supercomputers to low-cost clusters of workstations. Tempest provides four basic mechanisms necessary for both fine-grain shared memory and message passing [32]: active messages, bulk data transfer, virtual memory management, and fine-grain access control. <p> Tempest is a communication and memory management interface <ref> [32] </ref> that can be supported on on a wide range of systems, ranging from multi-million-dollar supercomputers to low-cost clusters of workstations. Tempest provides four basic mechanisms necessary for both fine-grain shared memory and message passing [32]: active messages, bulk data transfer, virtual memory management, and fine-grain access control. This section presents an overview of Blizzard, with a focus on alternative implementations of fine-grain access control. Although we implemented these techniques for Tempest, they could also be used in other distributed-shared-memory systems. <p> Blizzard consists of a modified version of the CM-5 operating system and a user-level library. A shared-memory program is executed by compiling it with a standard compiler (e.g., gcc), linking it with the Blizzard library and a Tempest-compliant user-level protocol (e.g., Stache <ref> [32] </ref>), and running it on a CM-5 with the modified OS. The next section describes our modifications to the CM-5 operating system. <p> We ran these benchmarks on five 32-node systems: Blizzard-E, Blizzard-S, Blizzard-ES, Blizzard-P, and a Kendall Square KSR-1. The first three Blizzard systems use a full-map invalidation protocol implemented in user-level software (Stache) <ref> [32] </ref> with a 128-byte block size. Blizzard-P is a sequentially-consistent, page-granularity version of Blizzard. The KSR-1 is a parallel processor with extensive hardware support for shared memory. Table 4 summarizes the performance of these systems.
Reference: [33] <author> Rafael H. Saavedra, R. Stockton Gaines, and Michael J. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 202-213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Of course, the design effort increases as the processor is more extensively customized. 2.3 Performance Table 2 summarizes the access control overheads and remote miss times for existing and proposed distributed-shared-memory systems <ref> [4, 5, 17, 20, 25, 32, 33] </ref>. Values marked with ' ~ ' are estimated. The left side of Table 2 lists the overhead of testing a shared memory reference for accessibility.
Reference: [34] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference: [35] <author> Daniel Stodolsky, J. Brad Chen, and Brian Bershad. </author> <title> Fast Interrupt Priority Management in Operating Systems. </title> <booktitle> In Second USENIX Symposium on Microkernels and Other Kernel Archtitectures, </booktitle> <pages> pages 105-110, </pages> <address> September 1993. San Diego, CA. </address>
Reference-contexts: Reprinted by permission of ACM. kernel traps to both disable and re-enable interrupts. Instead, we use a software interrupt masking scheme similar to one proposed by Stodolsky, et al. <ref> [35] </ref>. The key observation is that interrupts occur much less frequently than critical sections, so we should optimize for this common case. This approach uses a software flag to mark critical sections. The lowest-level interrupt handler checks this "software-disable" flag.
Reference: [36] <author> Richard Uhlig, David Nagle, Trevor Mudge, and Stuart Sechrest. Tapeworm II: </author> <title> A New Method for Measuring OS Effects on Memory Architecture Performance. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <month> October </month> <year> 1994. </year> <note> To appear. 10 </note>
Reference-contexts: Running the SPARC cache in write-back mode causes all cache misses to appear as cache block fills. A fill causes an uncorrectable ECC error and generates a precise exception, which the kernel vectors to the user-level handler. The Wisconsin Wind Tunnel [31] and Tapeworm II <ref> [36] </ref> both use this ECC technique to simulate memory systems. This technique causes no loss of reliability. First, uncorrectable ECC faults are treated in the normal way (e.g., panic) unless a program specified a handler for a page.
References-found: 36

