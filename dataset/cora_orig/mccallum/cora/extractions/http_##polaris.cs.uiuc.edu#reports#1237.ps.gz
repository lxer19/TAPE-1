URL: http://polaris.cs.uiuc.edu/reports/1237.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Symbolic Program Analysis and Optimization for Parallelizing Compilers  
Author: Mohammad R. Haghighat Constantine D. Polychronopoulos 
Address: Urbana, IL 61801-2932  2630 Walsh Avenue Santa Clara, CA 95051-0905  Urbana, IL 61801-2932  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  Kubota Pacific Computer Inc.  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: A program flow analysis framework is proposed for parallelizing compilers. Within this framework, symbolic analysis is used as an abstract interpretation technique to solve many of the flow analysis problems in a unified way. Some of these problems are constant propagation, global forward substitution, detection of loop invariant computations, and induction variable substitution. The solution space of the above problems is much larger than that handled by existing compiler technology. It covers many of the cases in benchmark codes that other parallelizing compilers can not handle. Employing finite difference methods, the symbolic analyzer derives a functional representation of programs, which is used in dependence analysis. A systematic method for generalized strength reduction based on this representation is also presented. This results in an effective scheme for exploitation of parallelism and optimization of the code. Symbolic analysis also serves as a basis for other code generation optimizations such as elimination of redundant computations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers : Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <month> March </month> <year> 1986. </year> <month> 26 </month>
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. <p> The role and importance of loops in the hierarchical structure of programs has been extensively studied in [28]. Loops may be defined by the notion of dominance relation <ref> [1] </ref>, or by doing a depth first search (DFS) traversal of the control flow graph [31]. <p> By loops, we mean natural loops as defined by the dominance relation. 4 3.3.1 Nesting of Loops It can be shown that when two natural loops have different headers, they are either disjoint or one is entirely contained (nested) within the other <ref> [1, 28] </ref>. However, when two natural loops have the same header, some normalization becomes necessary to transform the loops in such a way that each loop is uniquely identified by its header. This will result in a simpler scheme for node interpretation. Parafrase-2 normalizes the loops of the following categories. <p> Note how Parafrase-2 has recognized the induction variables and loop bounds information. j = 1 j = 1 j = j + n DO 100 i = 1, n0 100 continue n = i + n0 100 CONTINUE 4 Induction Variables Induction variables are of particular interest in optimizing compilers <ref> [2, 1] </ref>. A subclass of them that forms arithmetic progression has a vital role in parallelizing compilers [46, 7, 47]. Induction variables are usually introduced to improve program performance on sequential computers. <p> Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [27, 17, 4, 1] </ref>. Induction variable substitution followed by loop parallelization, accompanied with strength reduction will result in a very sophisticated compiler scheme. However, this optimization can seriously weaken the precision of dependence analysis in parallelizing compilers. <p> Parafrase-2 recognizes the loop-invariant expressions using its induction expression analyzer. Note that the above method, which is based on symbolic analysis, subsumes 22 the traditional flow analysis methods which employ use-def chains derived from the program syntax <ref> [1] </ref>. In those methods, expressions whose operands are modified within the loop can never be recognized as loop-invariants, even if they result in values that are loop-invariant. <p> of the code segment. j = n j = n j = i - j do i = 1, 1000 k = k + j j = n jf = j end do 6 Generalized Strength Reduction Strength reduction is a compiler technique that replaces expensive operations by fast instructions <ref> [27, 17, 4, 1] </ref>. The classical strength reduction algorithm [4] is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables.
Reference: [2] <author> A. V. Aho and J. D. Ullman. </author> <title> The Theory of Parsing, Translation, and Compiling, volume II : Compiling. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. <p> Note how Parafrase-2 has recognized the induction variables and loop bounds information. j = 1 j = 1 j = j + n DO 100 i = 1, n0 100 continue n = i + n0 100 CONTINUE 4 Induction Variables Induction variables are of particular interest in optimizing compilers <ref> [2, 1] </ref>. A subclass of them that forms arithmetic progression has a vital role in parallelizing compilers [46, 7, 47]. Induction variables are usually introduced to improve program performance on sequential computers.
Reference: [3] <author> F. E. Allen and J. Cocke. </author> <title> A program data flow analysis procedure. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 19(3) </volume> <pages> 137-147, </pages> <month> March </month> <year> 1976. </year>
Reference-contexts: in general loops recognized from the control flow graph), a scheme for a very precise and aggressive global forward substitution, symbolic constant propagation, and generalized strength reduction are contributions of this work. 2 Previous Work Early work on data flow analysis was done by Allen, Cocke, and Schwartz among others <ref> [18, 3] </ref>. Kennedy in [35] gives a survey of data flow analysis techniques. Cousots were the first to propose an abstract interpretation framework for flow analysis [19].
Reference: [4] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. <p> Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [27, 17, 4, 1] </ref>. Induction variable substitution followed by loop parallelization, accompanied with strength reduction will result in a very sophisticated compiler scheme. However, this optimization can seriously weaken the precision of dependence analysis in parallelizing compilers. <p> of the code segment. j = n j = n j = i - j do i = 1, 1000 k = k + j j = n jf = j end do 6 Generalized Strength Reduction Strength reduction is a compiler technique that replaces expensive operations by fast instructions <ref> [27, 17, 4, 1] </ref>. The classical strength reduction algorithm [4] is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables. <p> The classical strength reduction algorithm <ref> [4] </ref> is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables. Any arbitrary polynomial ' in induction variables can be reduced by multiple applications of the algorithm.
Reference: [5] <author> R. Allen. </author> <title> Dependence analysis for subscripted variables and its application to program transformations. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: 1 Introduction The power of a parallelizing compiler is derived from its ability to analyze loops and the precision of its dependence analysis. The dependence problem reduces to verification of a set of constraints among program variables <ref> [10, 5] </ref>, and is undecidable in the general case. However, heuristic methods exist to handle a large class of cases that happen in practice. The more precise information about flow of data and control in a program the compiler has, the better transformed code it can generate.
Reference: [6] <author> R. Allen and S. Johnson. </author> <title> Compiling C for vectorization, parallelization, and inline expansion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-249, </pages> <address> Atlanta, Georgia, </address> <month> June 22-24 </month> <year> 1988. </year>
Reference-contexts: Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. In a study on compiling C programs 2 for parallel execution, Allen and Johnson <ref> [6] </ref> discussed induction variable substitution and a heuristic solution for forward substitution, which is implemented in Titan compiler. The notion of generalized induction variables was introduced by Eigenman et al. in their study on automatic parallelization of Perfect benchmark programs [24, 23].
Reference: [7] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used <ref> [46, 7, 47] </ref>. In a study on compiling C programs 2 for parallel execution, Allen and Johnson [6] discussed induction variable substitution and a heuristic solution for forward substitution, which is implemented in Titan compiler. <p> A subclass of them that forms arithmetic progression has a vital role in parallelizing compilers <ref> [46, 7, 47] </ref>. Induction variables are usually introduced to improve program performance on sequential computers. To illustrate this, consider the loop of Figure 10 (a). j is an induction variable which assumes the sequence of values described by the arithmetic progression m * i, i = 1,2,: : :,n. <p> These polynomials, which may even be invisible in the source program, can be the result of a global forward substitution <ref> [38, 7, 47] </ref>.
Reference: [8] <author> B. Alpern, M. N. Wegman, and F. K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Proceedings of the fifteenth Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: The notion of generalized induction variables was introduced by Eigenman et al. in their study on automatic parallelization of Perfect benchmark programs [24, 23]. Recently, there has been an increasing number of research focused on semantics-based static analysis of programs. Alpern, Wegman, and Zadeck <ref> [8] </ref> presented an algorithm for detecting many of the statically detectable classes of equalities by translating the programs into static single assignment (SSA) intermediate form. The SSA form of a program facilitates the recognition of equivalences among program expressions that are not lexically identical [42].
Reference: [9] <author> Z. Ammarguellat and W. L Harrison III. </author> <title> Automatic recognition of induction variables and recurrence relations by abstract interpretation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-295, </pages> <address> White Plains, New York, </address> <month> June 20-22 </month> <year> 1990. </year>
Reference-contexts: The SSA form of a program facilitates the recognition of equivalences among program expressions that are not lexically identical [42]. Jouvelot and Dehbonei [32] presented a unified approach for the parallelization of generalized reductions. Ammarguellat and Harrison <ref> [9] </ref> used abstract interpretation to automatically recognize induction variables and recurrence relations by matching them with patterns from a table of recurrence templates. 3 Symbolic Analysis of Programs Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions [13, 15, 16].
Reference: [10] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction The power of a parallelizing compiler is derived from its ability to analyze loops and the precision of its dependence analysis. The dependence problem reduces to verification of a set of constraints among program variables <ref> [10, 5] </ref>, and is undecidable in the general case. However, heuristic methods exist to handle a large class of cases that happen in practice. The more precise information about flow of data and control in a program the compiler has, the better transformed code it can generate.
Reference: [11] <author> L. Berman and G. Markowsky. </author> <title> Linear and non-linear approximate invariants. </title> <type> Technical Report RC7241, T.J. </type> <institution> Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> February </month> <year> 1976. </year>
Reference-contexts: This problem would turn into verification of a constraint among program variables at a program point. Automatic discovery of linear constraints among variables of a program has been studied in <ref> [34, 11, 20] </ref>, and applied to dependence analysis in [30]. In many of the practical cases, the compiler can come up with a formula that indicates how many iterations of a loop will be executed. For example, consider the loop nest of the Figure 21 (a). <p> Automatic handling of wrap-around expressions, generalized induction expressions involving exponential terms, and useless code elimination are not implemented yet. Although much work has been done on constraint propagation <ref> [34, 11, 20] </ref>, only more efficient heuristics are well-suited to be used in optimizing compilers. Automatic discovery of relationships between program variables is of key importance in symbolic dependence analysis [30]. The symbolic analysis framework proposed in this report can be extended to support constraint propagation.
Reference: [12] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Conference Proceedings The SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <volume> volume 21(7), </volume> <pages> pages 162-175, </pages> <address> Palo Alto, California, </address> <month> June 25-27 </month> <year> 1986. </year>
Reference-contexts: This method should also take memory management issues such as scalar expansion [37] and array privatization [24] into consideration. Current trends in programming methodology increases the need for interprocedural program analysis <ref> [12] </ref>. Our proposed framework can easily be extended to handle function calls. However, precise handling of general recursive function calls is a challenge, although an approach similar to our loop analysis strategy can be employed to solve some of the cases.
Reference: [13] <author> T. E. Cheatham, JR., H. Holloway, G., and J. A. Townley. </author> <title> Symbolic evaluation and the analysis of programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):402-417, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: Ammarguellat and Harrison [9] used abstract interpretation to automatically recognize induction variables and recurrence relations by matching them with patterns from a table of recurrence templates. 3 Symbolic Analysis of Programs Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [13, 15, 16] </ref>. In symbolic execution, functional behavior of a program is derived from algebraic representation of its computations. <p> It is also planned to increase the precision of dependence analysis of Parafrase-2 by using this symbolic framework. The approach to symbolic analysis, employed in Parafrase-2, though related to the previous work on symbolic analysis of programs <ref> [13, 15, 16] </ref>, differs from it in two important aspects. 1. Most other work try to compute the path values, which are the symbolic value of variables on all possible paths that reach a point of a program. <p> In other words, each loop can be replaced by a closed form expression that captures the effect of that loop <ref> [13, 15, 16] </ref>. 3.3 Loop Analysis The power of a parallelizing compiler critically depends on its ability to analyze loops for extraction and exploitation of the available parallelism. The role and importance of loops in the hierarchical structure of programs has been extensively studied in [28].
Reference: [14] <author> J. Choi, R. Cytron, and J. Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Conf. Rec. 18th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 55-66. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1991. </year>
Reference-contexts: The compiler efficiency can be increased by decreasing the amount of information that is propagated. The compiler can achieve this goal by employing more sparse representations such as Global Value Graph [40] or Static Single Assignment form <ref> [21, 14] </ref>. More generally, an intelligent compiler can optimize the compilation process by taking advantage of optimizations similar to those used for code generation.
Reference: [15] <author> L. A. Clarke and Richardson D. J. </author> <title> Symbolic evaluation methods for program analysis. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 264-300. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Ammarguellat and Harrison [9] used abstract interpretation to automatically recognize induction variables and recurrence relations by matching them with patterns from a table of recurrence templates. 3 Symbolic Analysis of Programs Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [13, 15, 16] </ref>. In symbolic execution, functional behavior of a program is derived from algebraic representation of its computations. <p> It is also planned to increase the precision of dependence analysis of Parafrase-2 by using this symbolic framework. The approach to symbolic analysis, employed in Parafrase-2, though related to the previous work on symbolic analysis of programs <ref> [13, 15, 16] </ref>, differs from it in two important aspects. 1. Most other work try to compute the path values, which are the symbolic value of variables on all possible paths that reach a point of a program. <p> Most other work try to compute the path values, which are the symbolic value of variables on all possible paths that reach a point of a program. Associated with each path there is also a path condition, which is the conjunction of all constraints along that path <ref> [15, 16] </ref>. These methods have the potential of leading to exponential time and/or space complexities in terms of number of branches in the program. Efficiency is the main reason why the symbolic interpretation methods have not widely been used in optimizing compilers [35]. <p> However, in practice such nested expressions are rarely used for computation of array subscripts, for which we do the symbolic analysis. 3.2 Global Symbolic Analysis Derivation of functional behavior of a given program is the goal of global symbolic analysis <ref> [15, 16] </ref>. There might be infinite number of execution paths in a given program because of existence of cycles in the program control flow graph. <p> In other words, each loop can be replaced by a closed form expression that captures the effect of that loop <ref> [13, 15, 16] </ref>. 3.3 Loop Analysis The power of a parallelizing compiler critically depends on its ability to analyze loops for extraction and exploitation of the available parallelism. The role and importance of loops in the hierarchical structure of programs has been extensively studied in [28].
Reference: [16] <author> L. A. Clarke and D. J. Richardson. </author> <title> Applications of symbolic evaluation. </title> <journal> Journal of Systems and Software, </journal> <volume> 5(1) </volume> <pages> 15-35, </pages> <year> 1985. </year> <month> 27 </month>
Reference-contexts: Ammarguellat and Harrison [9] used abstract interpretation to automatically recognize induction variables and recurrence relations by matching them with patterns from a table of recurrence templates. 3 Symbolic Analysis of Programs Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [13, 15, 16] </ref>. In symbolic execution, functional behavior of a program is derived from algebraic representation of its computations. <p> The symbolic representation of the computations can serve as a basis for a wide range of program optimizations such as constant propagation, global forward substitution, induction variables substitution, strength reduction, and elimination of redundant computations. A survey of the applications of symbolic evaluation is given in <ref> [16] </ref>. Parafrase-2 uses a symbolic analysis framework to do the data flow and control flow analysis and the associated optimizations. It is also planned to increase the precision of dependence analysis of Parafrase-2 by using this symbolic framework. <p> It is also planned to increase the precision of dependence analysis of Parafrase-2 by using this symbolic framework. The approach to symbolic analysis, employed in Parafrase-2, though related to the previous work on symbolic analysis of programs <ref> [13, 15, 16] </ref>, differs from it in two important aspects. 1. Most other work try to compute the path values, which are the symbolic value of variables on all possible paths that reach a point of a program. <p> Most other work try to compute the path values, which are the symbolic value of variables on all possible paths that reach a point of a program. Associated with each path there is also a path condition, which is the conjunction of all constraints along that path <ref> [15, 16] </ref>. These methods have the potential of leading to exponential time and/or space complexities in terms of number of branches in the program. Efficiency is the main reason why the symbolic interpretation methods have not widely been used in optimizing compilers [35]. <p> Parafrase-2 has a systematic method for solving systems of multivariate scalar recurrence relations. This is specially important when nonlinear inductions and recurrences with cyclic interdependences are involved. Previous methods could not handle cases with cyclic recurrences <ref> [16] </ref>. 3 3.1 The Symbolic Interpreter The main component of the Parafrase-2's symbolic analysis scheme is a symbolic interpreter. Its goal is to derive algebraic representation of expressions of a given program. <p> However, in practice such nested expressions are rarely used for computation of array subscripts, for which we do the symbolic analysis. 3.2 Global Symbolic Analysis Derivation of functional behavior of a given program is the goal of global symbolic analysis <ref> [15, 16] </ref>. There might be infinite number of execution paths in a given program because of existence of cycles in the program control flow graph. <p> In other words, each loop can be replaced by a closed form expression that captures the effect of that loop <ref> [13, 15, 16] </ref>. 3.3 Loop Analysis The power of a parallelizing compiler critically depends on its ability to analyze loops for extraction and exploitation of the available parallelism. The role and importance of loops in the hierarchical structure of programs has been extensively studied in [28].
Reference: [17] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the Association for Computing Machinery, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. <p> Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [27, 17, 4, 1] </ref>. Induction variable substitution followed by loop parallelization, accompanied with strength reduction will result in a very sophisticated compiler scheme. However, this optimization can seriously weaken the precision of dependence analysis in parallelizing compilers. <p> of the code segment. j = n j = n j = i - j do i = 1, 1000 k = k + j j = n jf = j end do 6 Generalized Strength Reduction Strength reduction is a compiler technique that replaces expensive operations by fast instructions <ref> [27, 17, 4, 1] </ref>. The classical strength reduction algorithm [4] is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables.
Reference: [18] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers. </title> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York, NY, </address> <note> second revised edition edition, </note> <year> 1970. </year>
Reference-contexts: in general loops recognized from the control flow graph), a scheme for a very precise and aggressive global forward substitution, symbolic constant propagation, and generalized strength reduction are contributions of this work. 2 Previous Work Early work on data flow analysis was done by Allen, Cocke, and Schwartz among others <ref> [18, 3] </ref>. Kennedy in [35] gives a survey of data flow analysis techniques. Cousots were the first to propose an abstract interpretation framework for flow analysis [19]. <p> Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47].
Reference: [19] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <address> Los Angeles, CA, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: Kennedy in [35] gives a survey of data flow analysis techniques. Cousots were the first to propose an abstract interpretation framework for flow analysis <ref> [19] </ref>. Constant propagation has been extensively studied in the literature in the context of global flow analysis, and is used in all optimizing and parallelizing compilers. While the constant propagation problem is undecidable in general [33], there are conservative algorithms that find subsets of constants in programs.
Reference: [20] <author> P. Cousot and N. Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <address> Tucson, AZ, </address> <month> January </month> <year> 1978. </year>
Reference-contexts: Note that at this point the compiler may need to verify a constraint among some of the program variables. In some cases this can be done by method of constraint propagation <ref> [20, 30] </ref>. A typical example of a loop is a fixed do loop with the start value of 1 and stop value of n where n is a program variable that does not change within the loop. <p> This problem would turn into verification of a constraint among program variables at a program point. Automatic discovery of linear constraints among variables of a program has been studied in <ref> [34, 11, 20] </ref>, and applied to dependence analysis in [30]. In many of the practical cases, the compiler can come up with a formula that indicates how many iterations of a loop will be executed. For example, consider the loop nest of the Figure 21 (a). <p> Automatic handling of wrap-around expressions, generalized induction expressions involving exponential terms, and useless code elimination are not implemented yet. Although much work has been done on constraint propagation <ref> [34, 11, 20] </ref>, only more efficient heuristics are well-suited to be used in optimizing compilers. Automatic discovery of relationships between program variables is of key importance in symbolic dependence analysis [30]. The symbolic analysis framework proposed in this report can be extended to support constraint propagation.
Reference: [21] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The compiler efficiency can be increased by decreasing the amount of information that is propagated. The compiler can achieve this goal by employing more sparse representations such as Global Value Graph [40] or Static Single Assignment form <ref> [21, 14] </ref>. More generally, an intelligent compiler can optimize the compilation process by taking advantage of optimizations similar to those used for code generation.
Reference: [22] <author> Callahan D., K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Conference Proceedings The SIGPLAN '86 Symposium on Compiler Construction, volume SIGPLAN Notices 21(7), </booktitle> <pages> pages 152-161, </pages> <address> Palo Alto, California, </address> <month> June 25-27 </month> <year> 1986. </year>
Reference-contexts: Wegman and Zadeck [45] presented an efficient method for constant propagation when conditional branches are taken into consideration. Their algorithm does a form of constant propagation in combination with dead code elimination to find the same class of constants that can be found by Wegbreit's method. Callahan et al. <ref> [22] </ref> presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature [18, 2, 26, 27, 17, 4, 1].
Reference: [23] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring fortran programs for cedar. </title> <booktitle> In Proceedings of ICPP 91, </booktitle> <volume> volume I, </volume> <pages> pages 57-66, </pages> <address> Santa Clara, CA, </address> <month> August 12-17 </month> <year> 1991. </year>
Reference-contexts: The notion of generalized induction variables was introduced by Eigenman et al. in their study on automatic parallelization of Perfect benchmark programs <ref> [24, 23] </ref>. Recently, there has been an increasing number of research focused on semantics-based static analysis of programs. Alpern, Wegman, and Zadeck [8] presented an algorithm for detecting many of the statically detectable classes of equalities by translating the programs into static single assignment (SSA) intermediate form. <p> They have been named Generalized Induction Variables or GIVs <ref> [24, 23] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> In the program OCEAN, one loop that performs 40% of the operations of the whole program could be parallelized after recognition and substitution of the corresponding generalized induction variable. This results in a speed up of 8:1 on the Cedar multiprocessor <ref> [23] </ref>. j = 1 j = 1 j = j * m j = m ** i end do end do Parafrase-2 is able to handle even more complicated cases where the loop bounds are not necessarily linear expressions of enclosing loop index variables, but rather they are polynomials of arbitrary
Reference: [24] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of four perfect-benchmark programs. </title> <booktitle> In Preliminary Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August 7-9 </month> <year> 1991. </year>
Reference-contexts: The notion of generalized induction variables was introduced by Eigenman et al. in their study on automatic parallelization of Perfect benchmark programs <ref> [24, 23] </ref>. Recently, there has been an increasing number of research focused on semantics-based static analysis of programs. Alpern, Wegman, and Zadeck [8] presented an algorithm for detecting many of the statically detectable classes of equalities by translating the programs into static single assignment (SSA) intermediate form. <p> This complicates the simplification of expressions that use the final value of the induction variables. This problem, called zero-trip loop problem <ref> [24] </ref> can cause problems in the analysis of nested loops. Figure 8 shows the result of loop analysis and induction expression substitution, performed by Parafrase-2, in the case of a loop with multiple exits. <p> They have been named Generalized Induction Variables or GIVs <ref> [24, 23] </ref>. The sequence of values that a GIV receives throughout the execution of a loop does not necessarily form an arithmetic progression. <p> This method should also take memory management issues such as scalar expansion [37] and array privatization <ref> [24] </ref> into consideration. Current trends in programming methodology increases the need for interprocedural program analysis [12]. Our proposed framework can easily be extended to handle function calls.
Reference: [25] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architecture. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: k = 0 do j = 1, n do j = 1, n end do end do s = k s = (n * n + n ** 3) / 2 4.4 Symbolic Division Some of the optimizing compilers that support some sort of symbolic analysis ignore the division operation <ref> [25] </ref>. This is due to the difficulty of handling the division operation when addition and multiplication are also involved. For example, if k is an integer then k=2 + k=2 is not necessarily 20 equal to k; it is equal if and only if k is an even number.
Reference: [26] <author> A. C. Fong, J. B. Kam, and J. D. Ullman. </author> <title> Applications of lattice algebra to loop optimization. </title> <booktitle> Conf. Rec. Second ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 1-9, </pages> <year> 1975. </year>
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47].
Reference: [27] <author> A. C. Fong and J. D. Ullman. </author> <title> Induction variables in very high level languages. </title> <booktitle> Conf. Rec. Third ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 104-112, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature <ref> [18, 2, 26, 27, 17, 4, 1] </ref>. Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used [46, 7, 47]. <p> Replacement of expensive instructions by fast operations is called strength reduction and has been extensively studied for optimizing compilers <ref> [27, 17, 4, 1] </ref>. Induction variable substitution followed by loop parallelization, accompanied with strength reduction will result in a very sophisticated compiler scheme. However, this optimization can seriously weaken the precision of dependence analysis in parallelizing compilers. <p> of the code segment. j = n j = n j = i - j do i = 1, 1000 k = k + j j = n jf = j end do 6 Generalized Strength Reduction Strength reduction is a compiler technique that replaces expensive operations by fast instructions <ref> [27, 17, 4, 1] </ref>. The classical strength reduction algorithm [4] is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables.
Reference: [28] <author> M. B. Girkar. </author> <title> Functional Parallelism : Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The role and importance of loops in the hierarchical structure of programs has been extensively studied in <ref> [28] </ref>. Loops may be defined by the notion of dominance relation [1], or by doing a depth first search (DFS) traversal of the control flow graph [31]. <p> By loops, we mean natural loops as defined by the dominance relation. 4 3.3.1 Nesting of Loops It can be shown that when two natural loops have different headers, they are either disjoint or one is entirely contained (nested) within the other <ref> [1, 28] </ref>. However, when two natural loops have the same header, some normalization becomes necessary to transform the loops in such a way that each loop is uniquely identified by its header. This will result in a simpler scheme for node interpretation. Parafrase-2 normalizes the loops of the following categories.
Reference: [29] <author> M. R. Haghighat. </author> <title> Symbolic Program Analysis for High Performance Parallelizing Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <month> May </month> <year> 1993. </year> <note> In preparation. </note>
Reference-contexts: Lemma 4.1 The problem of whether a given variable, appearing in a statement of a loop, is an induction variable is undecidable <ref> [29] </ref>. 16 Note that an induction variable of a loop may have different characteristic functions at different points of the same loop.
Reference: [30] <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic dependence analysis for high-performance parallelizing compilers. </title> <editor> In A. Nicolau, Gelernter D., T. Gross, and Padua D., editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 310-330. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year> <month> 28 </month>
Reference-contexts: Note that at this point the compiler may need to verify a constraint among some of the program variables. In some cases this can be done by method of constraint propagation <ref> [20, 30] </ref>. A typical example of a loop is a fixed do loop with the start value of 1 and stop value of n where n is a program variable that does not change within the loop. <p> This problem would turn into verification of a constraint among program variables at a program point. Automatic discovery of linear constraints among variables of a program has been studied in [34, 11, 20], and applied to dependence analysis in <ref> [30] </ref>. In many of the practical cases, the compiler can come up with a formula that indicates how many iterations of a loop will be executed. For example, consider the loop nest of the Figure 21 (a). <p> Although much work has been done on constraint propagation [34, 11, 20], only more efficient heuristics are well-suited to be used in optimizing compilers. Automatic discovery of relationships between program variables is of key importance in symbolic dependence analysis <ref> [30] </ref>. The symbolic analysis framework proposed in this report can be extended to support constraint propagation.
Reference: [31] <author> M. S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> Elsevier North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: The role and importance of loops in the hierarchical structure of programs has been extensively studied in [28]. Loops may be defined by the notion of dominance relation [1], or by doing a depth first search (DFS) traversal of the control flow graph <ref> [31] </ref>. By loops, we mean natural loops as defined by the dominance relation. 4 3.3.1 Nesting of Loops It can be shown that when two natural loops have different headers, they are either disjoint or one is entirely contained (nested) within the other [1, 28].
Reference: [32] <author> P. Jouvelot and B. Dehbonei. </author> <title> A unified semantic approach for the vectorization and par-allelization of generalized reductions. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June 5-9 </month> <year> 1989. </year>
Reference-contexts: The SSA form of a program facilitates the recognition of equivalences among program expressions that are not lexically identical [42]. Jouvelot and Dehbonei <ref> [32] </ref> presented a unified approach for the parallelization of generalized reductions.
Reference: [33] <author> J. B. Kam and J. D. Ulllman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 23(1) </volume> <pages> 158-171, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Cousots were the first to propose an abstract interpretation framework for flow analysis [19]. Constant propagation has been extensively studied in the literature in the context of global flow analysis, and is used in all optimizing and parallelizing compilers. While the constant propagation problem is undecidable in general <ref> [33] </ref>, there are conservative algorithms that find subsets of constants in programs. Kildall in his lattice-theoretic approach to the data flow analysis [36] presented a general iterative algorithm for distributive frameworks which converges to the maximum fixed point solution of the problems.
Reference: [34] <author> M. Karr. </author> <title> Affine relationships among variables of a program. </title> <journal> Acta Informatica, </journal> <volume> 6, fasc. 2 </volume> <pages> 133-151, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: This problem would turn into verification of a constraint among program variables at a program point. Automatic discovery of linear constraints among variables of a program has been studied in <ref> [34, 11, 20] </ref>, and applied to dependence analysis in [30]. In many of the practical cases, the compiler can come up with a formula that indicates how many iterations of a loop will be executed. For example, consider the loop nest of the Figure 21 (a). <p> Automatic handling of wrap-around expressions, generalized induction expressions involving exponential terms, and useless code elimination are not implemented yet. Although much work has been done on constraint propagation <ref> [34, 11, 20] </ref>, only more efficient heuristics are well-suited to be used in optimizing compilers. Automatic discovery of relationships between program variables is of key importance in symbolic dependence analysis [30]. The symbolic analysis framework proposed in this report can be extended to support constraint propagation.
Reference: [35] <author> K. Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 5-54. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Kennedy in <ref> [35] </ref> gives a survey of data flow analysis techniques. Cousots were the first to propose an abstract interpretation framework for flow analysis [19]. Constant propagation has been extensively studied in the literature in the context of global flow analysis, and is used in all optimizing and parallelizing compilers. <p> These methods have the potential of leading to exponential time and/or space complexities in terms of number of branches in the program. Efficiency is the main reason why the symbolic interpretation methods have not widely been used in optimizing compilers <ref> [35] </ref>. Instead of computing and keeping the different values and corresponding conditions that reach a point of a program, Parafrase-2 uses a join function that finds the intersection of the information that reach that point.
Reference: [36] <author> G. A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> Conf. Rec. First ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <year> 1973. </year>
Reference-contexts: While the constant propagation problem is undecidable in general [33], there are conservative algorithms that find subsets of constants in programs. Kildall in his lattice-theoretic approach to the data flow analysis <ref> [36] </ref> presented a general iterative algorithm for distributive frameworks which converges to the maximum fixed point solution of the problems.
Reference: [37] <author> B. Leasure. </author> <title> The parafrase project's fortran analyzer major module documentation. </title> <type> Technical Report 504, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <year> 1985. </year>
Reference-contexts: The above transformation , called induction variable substitution, is the inverse of strength reduction, and has been used in parallelizing compilers for a number of years <ref> [37, 38] </ref>. <p> The initial value of such variables are assigned to them before the loop. Many of these cases have been observed in practical codes including Perfect benchmarks (program TRFD). The notion of wrap-around expressions is a generalization of wrap-around variables <ref> [37, 38, 47] </ref>. <p> A sophisticated method of redundant computation elimination, together with code motion, that utilizes the information stored in symbolic computation tree, will be more precise than traditional common subexpression elimination methods based on syntactical redundancies. This method should also take memory management issues such as scalar expansion <ref> [37] </ref> and array privatization [24] into consideration. Current trends in programming methodology increases the need for interprocedural program analysis [12]. Our proposed framework can easily be extended to handle function calls.
Reference: [38] <author> D. A. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The above transformation , called induction variable substitution, is the inverse of strength reduction, and has been used in parallelizing compilers for a number of years <ref> [37, 38] </ref>. <p> These polynomials, which may even be invisible in the source program, can be the result of a global forward substitution <ref> [38, 7, 47] </ref>. <p> The initial value of such variables are assigned to them before the loop. Many of these cases have been observed in practical codes including Perfect benchmarks (program TRFD). The notion of wrap-around expressions is a generalization of wrap-around variables <ref> [37, 38, 47] </ref>.
Reference: [39] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year> <institution> Penn State. </institution>
Reference-contexts: This framework, which is based on symbolic analysis, is implemented as the flow analysis scheme of Parafrase-2 <ref> [39] </ref>. Symbolic analysis is an abstract interpretation method in which values of program expressions are represented by symbolic expressions. Parafrase-2 uses symbolic analysis for a wide range of program optimizations such as constant propagation, global forward substitution, detection of loop-invariant computations, and induction variable substitution.
Reference: [40] <author> J. H. Reif and R. E. Tarjan. </author> <title> Symbolic program analysis in almost linear time. </title> <journal> SIAM J. of Computing, </journal> <volume> 11(1) </volume> <pages> 81-93, </pages> <month> February 81. </month>
Reference-contexts: The compiler efficiency can be increased by decreasing the amount of information that is propagated. The compiler can achieve this goal by employing more sparse representations such as Global Value Graph <ref> [40] </ref> or Static Single Assignment form [21, 14]. More generally, an intelligent compiler can optimize the compilation process by taking advantage of optimizations similar to those used for code generation.
Reference: [41] <author> John H. Reif and Harry R. Lewis. </author> <title> Symbolic evaluation and the global value graph. </title> <booktitle> Conf. Rec. Fourth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 104-118, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Kildall in his lattice-theoretic approach to the data flow analysis [36] presented a general iterative algorithm for distributive frameworks which converges to the maximum fixed point solution of the problems. Reif and Lewis <ref> [41] </ref> used a sparse data structure, called Global Value Graph, to find the same information that can be found by Kildall's algorithm, but in a more efficient way. <p> Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis <ref> [41] </ref>, and Wegman and Zadeck [45]. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature [18, 2, 26, 27, 17, 4, 1].
Reference: [42] <author> B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In Proceedings of the fifteenth Annual ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27, </pages> <address> San Diego, California, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Alpern, Wegman, and Zadeck [8] presented an algorithm for detecting many of the statically detectable classes of equalities by translating the programs into static single assignment (SSA) intermediate form. The SSA form of a program facilitates the recognition of equivalences among program expressions that are not lexically identical <ref> [42] </ref>. Jouvelot and Dehbonei [32] presented a unified approach for the parallelization of generalized reductions.
Reference: [43] <author> H. C. Saxena. </author> <title> Finite Differences and Numerical Analysis. </title> <editor> S. </editor> <publisher> Chand & Company (Pvt) Ltd, </publisher> <address> Ram Nagar, New Delhi-110055, tenth edition, </address> <year> 1988. </year>
Reference-contexts: Since is of degree m, the part arising from (n) will be zero in the (m + 1) th and succeeding differences, and therefore these differences will form a geometric progression with common ratio r <ref> [43] </ref>.
Reference: [44] <author> B. Wegbreit. </author> <title> Property extraction in well-founded property sets. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(3) </volume> <pages> 270-285, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: Reif and Tarjan employed Global Value Graph to obtain covers; mappings from text expressions to symbolic expressions, which is useful in program optimization techniques. Wegbreit showed how symbolic interpretation can be used to determine properties from a well-founded property set <ref> [44] </ref>. Wegman and Zadeck [45] presented an efficient method for constant propagation when conditional branches are taken into consideration. Their algorithm does a form of constant propagation in combination with dead code elimination to find the same class of constants that can be found by Wegbreit's method.
Reference: [45] <author> Mark N. Wegman and Frank Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Proceedings of Twelfth POPL, </booktitle> <pages> pages 291-299, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Reif and Tarjan employed Global Value Graph to obtain covers; mappings from text expressions to symbolic expressions, which is useful in program optimization techniques. Wegbreit showed how symbolic interpretation can be used to determine properties from a well-founded property set [44]. Wegman and Zadeck <ref> [45] </ref> presented an efficient method for constant propagation when conditional branches are taken into consideration. Their algorithm does a form of constant propagation in combination with dead code elimination to find the same class of constants that can be found by Wegbreit's method. <p> Callahan et al. [22] presented a method for interprocedural constant propagation. Their method is based on an algorithm adapted from Reif and Lewis [41], and Wegman and Zadeck <ref> [45] </ref>. Induction variable detection, as a basis for strength reduction, has been extensively studied in the literature [18, 2, 26, 27, 17, 4, 1].
Reference: [46] <author> M. J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report 78-929, </type> <institution> Department of Computer Science, </institution> <month> July </month> <year> 1978. </year> <month> 29 </month>
Reference-contexts: Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used <ref> [46, 7, 47] </ref>. In a study on compiling C programs 2 for parallel execution, Allen and Johnson [6] discussed induction variable substitution and a heuristic solution for forward substitution, which is implemented in Titan compiler. <p> A subclass of them that forms arithmetic progression has a vital role in parallelizing compilers <ref> [46, 7, 47] </ref>. Induction variables are usually introduced to improve program performance on sequential computers. To illustrate this, consider the loop of Figure 10 (a). j is an induction variable which assumes the sequence of values described by the arithmetic progression m * i, i = 1,2,: : :,n.
Reference: [47] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year> <month> 30 </month>
Reference-contexts: Induction variables substitution, the inverse of strength reduction, is used to eliminate the parallelism-inhibiting dependences that are the result of the way induction variables are defined and used <ref> [46, 7, 47] </ref>. In a study on compiling C programs 2 for parallel execution, Allen and Johnson [6] discussed induction variable substitution and a heuristic solution for forward substitution, which is implemented in Titan compiler. <p> A subclass of them that forms arithmetic progression has a vital role in parallelizing compilers <ref> [46, 7, 47] </ref>. Induction variables are usually introduced to improve program performance on sequential computers. To illustrate this, consider the loop of Figure 10 (a). j is an induction variable which assumes the sequence of values described by the arithmetic progression m * i, i = 1,2,: : :,n. <p> These polynomials, which may even be invisible in the source program, can be the result of a global forward substitution <ref> [38, 7, 47] </ref>. <p> The initial value of such variables are assigned to them before the loop. Many of these cases have been observed in practical codes including Perfect benchmarks (program TRFD). The notion of wrap-around expressions is a generalization of wrap-around variables <ref> [37, 38, 47] </ref>.
References-found: 47

