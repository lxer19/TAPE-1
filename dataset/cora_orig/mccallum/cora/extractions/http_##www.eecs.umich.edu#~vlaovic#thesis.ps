URL: http://www.eecs.umich.edu/~vlaovic/thesis.ps
Refering-URL: http://www.eecs.umich.edu/~vlaovic/
Root-URL: http://www.cs.umich.edu
Email: svlaovic@scuacc.scu.edu  
Title: CC-NUMA Page Table Management and Redundant Linked List Based Cache Coherence Protocol  
Author: Stevan Vlaovic 
Address: Santa Clara, CA 95053  
Affiliation: Department of Computer Engineering Santa Clara University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <editor> IEEE Std 1596-1992. </editor> <title> Scalable Coherent Interface. </title> <address> Piscataway, NJ. </address>
Reference-contexts: Believing that distributed systems are a viable approach, this research attempts to achieve high performance parallel computing based on network of workstations connected by SCI (Scalable Coherent Interface, IEEE Std 1596-1992 Scalable Coherent Interface) <ref> [1] </ref>. SCI is a point-to-point protocol providing bus-like connections for multiprocessors. Different from the traditional LAN-connected distributed systems, SCI-connected networks provide physically shared memory and cache coherence among workstations across a network with high bandwidth (1 Gbyte/sec) and low latency (microseconds). <p> DP protocol, the memory includes one pointer to the linked list, and the cache line then has other associated pointers, dependent on the specific protocol. 6.3 SCI's Cache Coherence protocol The cache coherence protocol specified by the Scalable Coherent Interface is a DP protocol defined by a doubly linked list <ref> [1] </ref>. For a cache to join the sharing list, it must become the new head of the list. Insertions must occur at the head of the list, while deletions can happen anywhere along the list. This supports the one writer, multiple reader format that increases parallelism in multiprocessor systems. <p> These extensions are trying to reduce the linear latencies associated with the invalidation command to a logarithmic latency <ref> [1] </ref>. This is achieved by adding an additional pointer to the original specification. A writer is capable of invalidating the sharing list by following the binary tree structure: hence the logarithmic latency. However, this assumes a nearly balanced tree, otherwise this structure will also default to the sequential linked list.
Reference: [2] <author> IEEE Std. 896.1-1992. </author> <title> IEEE Standard for Futurebus+ Logical Protocol Specification. </title>
Reference-contexts: Figure 1 shows the memory layout of a SCI system. CSR stands for Control and Status Registers which is described in IEEE Standard 1212, and also as part of the Futurebus standard <ref> [2] </ref>. For SCI systems, depending on the implementation, the bandwidth can be 1 Gbyte/second (GaAs chips and special cable up to 10 meters) or 1 Gbit/second (CMOS chips and fiber optical cables). The communication latency between two nodes is in the range of sub-microsecond to tens of microseconds [28].
Reference: [3] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for unix development. </title> <booktitle> In Summer Conference Proceedings, USENIX Association, </booktitle> <year> 1986. </year>
Reference-contexts: This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems. We are particularly interested in the work related to microkernel based systems such as <ref> [10, 3, 26] </ref> and synchronization mechanisms such as [23].
Reference: [4] <author> J.A.C. Bogaerts, R. Divia, H. Muller, and J.F. Renardy. </author> <title> Sci based data acquisition architectures. </title> <journal> IEEE Transactions on Nuclear Science, </journal> <pages> pages 85-94, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: SCI protocol is based on point-to-point connections, which avoids the unnecessary contention that is commonly associated with a bus. As a result, SCI connections can reach up to 1 Gbyte/s speeds <ref> [4] </ref>. The focus of this research is mainly in SCI's applications on distributed systems. Although SCI is designed to connect multiprocessors, its point-to-point connection provides a natural solution for implementing shared memory on a network of workstations, transforming it into an effective parallel processing platform.
Reference: [5] <author> H. Burkhardt. </author> <title> Technical summary of ksr-1. </title> <type> Technical report, </type> <institution> Kendall Square Research Corporation, </institution> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: The Architectural Model LNUMA Non-Uniform Memory Access (NUMA) distributed shared memory machines [16, 27, 32] are becoming increasingly significant since they support a shared memory paradigm on a large scale. Examples of NUMA machines are KSR1 <ref> [5] </ref> and BBN TC2000 [17]. In these types of systems, the placement and movement of data are critical to system performance. The "NUMA Problem", as it is often called, is the necessity of dealing with data placement issues. The additional programming load for the applications programmer is usually quite profound.
Reference: [6] <author> L.M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This directory can either be centralized at memory, or distributed among the local nodes in a DSM type machine. The centralized directory was first developed by Tang [30], later modified by Censier and Feautrier <ref> [6] </ref>, and finally implemented in the Stanford DASH [8]. Generally, the centralized directory maintains a bit map of the individual caches, where each bit set represents a shared copy of a particular cache line for the appropriate caches involved.
Reference: [7] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: Once the line is written the rest of the list purged, and if the members wish to obtain the new copy, they must join the list again. This invalidation scheme follows the current trend of invalidation of cache lines rather than updating them <ref> [7] </ref>. Updates are useful when cached copies are frequently reread after and update, and when temporally close updates to the same cache line are gathered before the update is enacted.
Reference: [8] <author> D. Lenoski et al. </author> <title> The directory-based cache coherence protocol for the dash multiprocessor. </title> <booktitle> In Proc. 17th Int'l Symp. Computer Architecture, </booktitle> <address> Los Alamitos, Calif., </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This directory can either be centralized at memory, or distributed among the local nodes in a DSM type machine. The centralized directory was first developed by Tang [30], later modified by Censier and Feautrier [6], and finally implemented in the Stanford DASH <ref> [8] </ref>. Generally, the centralized directory maintains a bit map of the individual caches, where each bit set represents a shared copy of a particular cache line for the appropriate caches involved.
Reference: [9] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings from the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 211-223, </pages> <address> New York, </address> <year> 1989. </year>
Reference-contexts: These applications are good candidates for porting to an SCI based platform. Since shared memory model makes programming and debugging easier, and software more portable, efforts have been made to provide shared memory on such a system via software supported virtual memory management <ref> [25, 29, 21, 9] </ref>. To achieve reasonable performance, shared data must be replicated in various forms such as replicated data in the size of a page or some small units. Data coherency must be dealt with which requires a significant amount of housekeeping on the operating system part.
Reference: [10] <author> A. Forin, J. Barrera, and R. Sanzi. </author> <title> Design, implementation, and performance evaluationof a distributed shared memory server for mach. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems. We are particularly interested in the work related to microkernel based systems such as <ref> [10, 3, 26] </ref> and synchronization mechanisms such as [23].
Reference: [11] <author> G. Geist and V.S. Sunderam. </author> <title> Evolution of the PVM concurrent computing system. </title> <booktitle> In 1993 IEEE Compcon, </booktitle> <pages> pages 549-557, </pages> <month> Feb. </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: Closely coupled MPP systems are built by Convex. As to SCI connected distributed systems, there is a number of companies implementing hardware connections. The low cost is an important factor in the advance of the parallel processing. The emerging software platforms such as PVM <ref> [11] </ref> and Linda [12, 13] in the last few years are strong evidence. With the current network latency and bandwidth, solving a problem in parallel on a number of workstations is not very effective.
Reference: [12] <author> D. Gelernter. </author> <title> Generative communication in linda. </title> <journal> ACM Trans. Prog. Lang. and Systems, </journal> <pages> pages 80-112, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: Closely coupled MPP systems are built by Convex. As to SCI connected distributed systems, there is a number of companies implementing hardware connections. The low cost is an important factor in the advance of the parallel processing. The emerging software platforms such as PVM [11] and Linda <ref> [12, 13] </ref> in the last few years are strong evidence. With the current network latency and bandwidth, solving a problem in parallel on a number of workstations is not very effective.
Reference: [13] <author> D. Gelernter, N. Carriero, S. Chandran, and S. Chang. </author> <title> Parallel programming in linda. </title> <booktitle> In Proc. Int. Conf. Parallel Processing, </booktitle> <pages> pages 255-263, </pages> <year> 1985. </year>
Reference-contexts: Closely coupled MPP systems are built by Convex. As to SCI connected distributed systems, there is a number of companies implementing hardware connections. The low cost is an important factor in the advance of the parallel processing. The emerging software platforms such as PVM [11] and Linda <ref> [12, 13] </ref> in the last few years are strong evidence. With the current network latency and bandwidth, solving a problem in parallel on a number of workstations is not very effective.
Reference: [14] <author> J. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE SCI Meeting, </institution> <year> 1990. </year>
Reference-contexts: Research has been carried out to verify its correctness <ref> [14] </ref> and study its hardware performances. A number of companies are developing SCI node chips, such as LSI Logic, Dolphin SCI Technology, and Vitesse. Closely coupled MPP systems are built by Convex. As to SCI connected distributed systems, there is a number of companies implementing hardware connections.
Reference: [15] <author> David B. Gustavson. </author> <title> The scalable coherent interface and related standards projects. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 10-21, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: SCI can connect a large number (up to 64K) of nodes (processors and machines) so that all memory modules associated with the nodes become parts of a global physical memory space <ref> [15] </ref>. Figure 1 shows the memory layout of a SCI system. CSR stands for Control and Status Registers which is described in IEEE Standard 1212, and also as part of the Futurebus standard [2].
Reference: [16] <author> K. Hwang. </author> <title> Advanced Computer Architecture. </title> <publisher> McGrall-Hill, Inc., </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference-contexts: Additionally, since cache coherence is handled by hardware in SCI, no software bookkeeping is necessary. The Architectural Model LNUMA Non-Uniform Memory Access (NUMA) distributed shared memory machines <ref> [16, 27, 32] </ref> are becoming increasingly significant since they support a shared memory paradigm on a large scale. Examples of NUMA machines are KSR1 [5] and BBN TC2000 [17]. In these types of systems, the placement and movement of data are critical to system performance.
Reference: [17] <institution> BBN Advanced Computers Inc. </institution> <type> Tc2000 technical product summary. Technical report, </type> <institution> BBN Advanced Computer Inc., </institution> <address> Cambridge, MA, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The Architectural Model LNUMA Non-Uniform Memory Access (NUMA) distributed shared memory machines [16, 27, 32] are becoming increasingly significant since they support a shared memory paradigm on a large scale. Examples of NUMA machines are KSR1 [5] and BBN TC2000 <ref> [17] </ref>. In these types of systems, the placement and movement of data are critical to system performance. The "NUMA Problem", as it is often called, is the necessity of dealing with data placement issues. The additional programming load for the applications programmer is usually quite profound.
Reference: [18] <author> D.V. James, A.T. Laundrie, S. Gjessing, and G.S. Sohi. </author> <title> Scalable coherent interface. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Additionally, the reliability of such a scheme is in question, as a fault in the bit map would result in an incorrect sharing list. By distributing the directory, the bottleneck is assuaged, and the reliability weakness is not as great <ref> [18, 31, 24] </ref>. This type of design is called a distributed pointer protocol. In this type of system, a linked list is created dynamically, to reflect the sharing members at that particular time. The caches can insert and delete themselves from the linked list as necessary.
Reference: [19] <author> R.E. Johnson. </author> <title> Extending the Scalable Coherent Interface for Large-Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: However, ATM may be used as the underlying data transferring mechanism for SCI. Efforts are being made to improve SCI cache coherence protocol for large scale sharing (sharing a cache line among hundreds or even thousands of nodes) <ref> [19, 22] </ref> presented later in this paper and in Kiloprocessor Extensions to SCI (P1596.2). This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems. <p> Since new additions to the list must be made at the root (head), maintaining a balanced tree is not trivial. The working group in charge of these extensions is the Kiloprocessor Extensions to SCI (P1596.2). Part of the Wisconsin STEM <ref> [19] </ref> effort on behalf cache coherency is being considered as an extension to SCI. The Wisconsin STEM (permuted acronym for Tree Merging Extensions to SCI) also organizes the sharing set as a binary tree. <p> The overhead included for each 64 bit cache line includes three pointers and one five bit height. The five bit height is sufficient for 64K nodes, as discussed by Johnson <ref> [19] </ref>. To increase the performance of a highly parallel machine, this design is capable of combining requests for the same memory location. The overall memory access time for N simultaneous requests drops from O (N) to O (log (N)).
Reference: [20] <author> C.E. Leiserson. </author> <title> The network architecture of the connection machine cm-5. </title> <booktitle> In Proc. ACM Symp. Parallel Algorithms and Architecture, </booktitle> <address> San Diego, CA, </address> <year> 1992. </year>
Reference-contexts: The supercomputers and MPPs have become more and more powerful, but not less expensive. Examples of theses types of machines are the Cray C-90, the CM-5 <ref> [20] </ref>, and nCube-3. As a result, there are only a few hundred facilities worldwide where one can use a supercomputer or MPP system. This severely limits parallel application development and our understanding of parallel processing in general.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions in Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: These applications are good candidates for porting to an SCI based platform. Since shared memory model makes programming and debugging easier, and software more portable, efforts have been made to provide shared memory on such a system via software supported virtual memory management <ref> [25, 29, 21, 9] </ref>. To achieve reasonable performance, shared data must be replicated in various forms such as replicated data in the size of a page or some small units. Data coherency must be dealt with which requires a significant amount of housekeeping on the operating system part.
Reference: [22] <author> Q. Li and S. Vlaovic. </author> <title> Redundant linked list based cache coherence protocol. </title> <booktitle> In World Computer Congress, IFIP Congress, </booktitle> <year> 1994. </year>
Reference-contexts: However, ATM may be used as the underlying data transferring mechanism for SCI. Efforts are being made to improve SCI cache coherence protocol for large scale sharing (sharing a cache line among hundreds or even thousands of nodes) <ref> [19, 22] </ref> presented later in this paper and in Kiloprocessor Extensions to SCI (P1596.2). This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems.
Reference: [23] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1), </volume> <month> Feb </month> <year> 1991. </year>
Reference-contexts: This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems. We are particularly interested in the work related to microkernel based systems such as [10, 3, 26] and synchronization mechanisms such as <ref> [23] </ref>. There has been development in the area of object oriented operating systems design which would be extremely useful in developing a distributed operating system. 9 2 Analysis 2.1 Modularity and Self-Sufficiency With LNUMA, the goal of an operating system evolves into a slightly disparate form.
Reference: [24] <author> H. Nilsson and P. Stenstrom. </author> <title> The scalable tree protocol a cache coherence approach to large-scale multiprocessors. </title> <booktitle> In Proceedings of the 4th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Additionally, the reliability of such a scheme is in question, as a fault in the bit map would result in an incorrect sharing list. By distributing the directory, the bottleneck is assuaged, and the reliability weakness is not as great <ref> [18, 31, 24] </ref>. This type of design is called a distributed pointer protocol. In this type of system, a linked list is created dynamically, to reflect the sharing members at that particular time. The caches can insert and delete themselves from the linked list as necessary.
Reference: [25] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: These applications are good candidates for porting to an SCI based platform. Since shared memory model makes programming and debugging easier, and software more portable, efforts have been made to provide shared memory on such a system via software supported virtual memory management <ref> [25, 29, 21, 9] </ref>. To achieve reasonable performance, shared data must be replicated in various forms such as replicated data in the size of a page or some small units. Data coherency must be dealt with which requires a significant amount of housekeeping on the operating system part.
Reference: [26] <author> OSF. </author> <type> Osf/1 technical seminar. Technical report, </type> <institution> Open Systems Foundation, Inc., </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: This line of work will eventually lead to more complex but effective directory structures. There is a plethora of current research related to operating systems. We are particularly interested in the work related to microkernel based systems such as <ref> [10, 3, 26] </ref> and synchronization mechanisms such as [23].
Reference: [27] <author> Jr. R.P. LaRowe and C.S. Ellis. </author> <title> Experimental comparison of memory management policies for numa multiprocessors. </title> <journal> ACM Transactions on Computing Systems, </journal> <pages> pages 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Additionally, since cache coherence is handled by hardware in SCI, no software bookkeeping is necessary. The Architectural Model LNUMA Non-Uniform Memory Access (NUMA) distributed shared memory machines <ref> [16, 27, 32] </ref> are becoming increasingly significant since they support a shared memory paradigm on a large scale. Examples of NUMA machines are KSR1 [5] and BBN TC2000 [17]. In these types of systems, the placement and movement of data are critical to system performance.
Reference: [28] <author> Steve Scott, James Goodman, and Mary Vernon. </author> <title> Performance of the sci ring. </title> <booktitle> In Proceedings of 19th INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: For SCI systems, depending on the implementation, the bandwidth can be 1 Gbyte/second (GaAs chips and special cable up to 10 meters) or 1 Gbit/second (CMOS chips and fiber optical cables). The communication latency between two nodes is in the range of sub-microsecond to tens of microseconds <ref> [28] </ref>. Cache Coherence for Networks For distributed systems, the issue of cache coherence was not addressed previously since it was seemingly not practical to maintain coherent caches among remote machines.
Reference: [29] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These applications are good candidates for porting to an SCI based platform. Since shared memory model makes programming and debugging easier, and software more portable, efforts have been made to provide shared memory on such a system via software supported virtual memory management <ref> [25, 29, 21, 9] </ref>. To achieve reasonable performance, shared data must be replicated in various forms such as replicated data in the size of a page or some small units. Data coherency must be dealt with which requires a significant amount of housekeeping on the operating system part.
Reference: [30] <author> C.K. Tang. </author> <title> Cache system design in the tightly coupled multiprocessor system. </title> <booktitle> In AFIPS Proceedings of the National Computer Conference, </booktitle> <year> 1976. </year> <month> 35 </month>
Reference-contexts: This information is useful for the purges that are necessitated by a write to a shared cache line. This directory can either be centralized at memory, or distributed among the local nodes in a DSM type machine. The centralized directory was first developed by Tang <ref> [30] </ref>, later modified by Censier and Feautrier [6], and finally implemented in the Stanford DASH [8]. Generally, the centralized directory maintains a bit map of the individual caches, where each bit set represents a shared copy of a particular cache line for the appropriate caches involved.
Reference: [31] <author> M. Thapar and B. Delagi. </author> <title> Stanford distributed directory protocol. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 78-80, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Additionally, the reliability of such a scheme is in question, as a fault in the bit map would result in an incorrect sharing list. By distributing the directory, the bottleneck is assuaged, and the reliability weakness is not as great <ref> [18, 31, 24] </ref>. This type of design is called a distributed pointer protocol. In this type of system, a linked list is created dynamically, to reflect the sharing members at that particular time. The caches can insert and delete themselves from the linked list as necessary.
Reference: [32] <author> X. Zhang and X. Qin. </author> <title> Performance prediction and evaluation of parallel processing on a numa multiprocessor. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(10) </volume> <pages> 1059-1068, </pages> <month> Oct </month> <year> 1991. </year> <month> 36 </month>
Reference-contexts: Additionally, since cache coherence is handled by hardware in SCI, no software bookkeeping is necessary. The Architectural Model LNUMA Non-Uniform Memory Access (NUMA) distributed shared memory machines <ref> [16, 27, 32] </ref> are becoming increasingly significant since they support a shared memory paradigm on a large scale. Examples of NUMA machines are KSR1 [5] and BBN TC2000 [17]. In these types of systems, the placement and movement of data are critical to system performance.
References-found: 32

