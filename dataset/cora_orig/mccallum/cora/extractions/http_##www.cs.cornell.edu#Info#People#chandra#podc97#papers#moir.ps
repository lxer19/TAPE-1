URL: http://www.cs.cornell.edu/Info/People/chandra/podc97/papers/moir.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/chandra/podc97/newProgram.html
Root-URL: 
Title: Practical Implementations of Non-Blocking Synchronization Primitives  
Author: Mark Moir 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science The University of  
Abstract: This paper is concerned with system support for non-blocking synchronization in shared-memory multiprocessors. Many non-blocking algorithms published recently depend on the Load-Linked (LL), Validate (VL), and Store-Conditional (SC) instructions. However, most systems support either Compare-and-Swap (CAS) or a weak form of LL and SC that imposes several restrictions on the use of these instructions and does not provide the exact semantics expected and assumed by algorithm designers. These limitations currently render several recent non-blocking algorithms inapplicable in most systems. The results presented here eliminate this problem by providing practical means for implementing any algorithm that is based on these instructions on any multiprocessor that provides either CAS or a form of LL and SC that is sufficiently weak that it is provided by all current hardware implementations of these instructions. This is achieved in two steps. First, we propose a slight modification to the interface for LL, VL, and SC, which will not greatly impact programmers. We then exploit this modification to provide time-optimal, space-efficient implementations of the desired primitives using commonly available ones. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alpha Architecture Handbook, Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, because there is only one LLBit per processor, it is impossible to have concurrent LL-SC sequences on the R4000. Other processors that provide LL and SC, such as the DEC Alpha <ref> [1] </ref> and the PowerPC [13], implement these instructions similarly and have similar restrictions. It is easy to see why hardware designers prefer to provide weaker versions of LL and SC than those commonly assumed by algorithm designers.
Reference: [2] <author> J. Anderson and M. Moir, </author> <title> "Universal Constructions for Multi-Object Operations", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing , 1995, </booktitle> <pages> pp. 184-194. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware. <p> First, Israeli and Rappoport [10] present an implementation of LL/SC from CAS that depends upon unrealistic assumptions about the size of machine words and has high time complexity. Anderson and Moir <ref> [2] </ref> improve upon this by providing a constant-time implementation with realistic assumptions. However, as discussed later, this implementation has impractical space requirements if used to implement many variables. Finally, Valois [15] outlines implementations that improve on these space requirements. <p> A naive generalization of this implementation for T W -word variables requires fi (N W T ) space overhead; the implementation given here requires only fi (N W ) space overhead, regardless of the number of words implemented. Also, the implementation in <ref> [2] </ref> is based on stronger LL and SC instructions than those used here. type segmenttype = record tag: tagtype; val: valtype end; headertype = record tag: tagtype; pid: 0::N 1 end; vartype = record hdr: headertype; data: array [0::W 1] of segmenttype end; shared variable A: array [0::N 1][0::W 1] of <p> The first implementation uses reasonably-small, bounded tags, leaving more room for data within each word. Our bounded-tag implementation of LL, VL, and SC is shown in Figure 7. This implementation uses a feedback mechanism similar to one used by Anderson and Moir in <ref> [2] </ref> 3 . This mechanism allows processes to choose new tags from a bounded range so that there is no possibility of "prematurely" reusing a tag. This ensures that a CAS does not succeed when it should fail, thereby performing the role of the unbounded tags of the previous implementations. <p> This slot is recorded for later use in the keep word provided. Then, LL reads the word being accessed (line 2), and announces the tag it has read (line 3). Next, LL rereads the word being accessed, and records in keep whether or 3 The implementation in <ref> [2] </ref> uses a stronger version of LL and SC which allow memory to be accessed between an LL and the subsequent SC.
Reference: [3] <author> J. Anderson and M. Moir, </author> <title> "Universal Constructions for Large Objects", </title> <booktitle> Proceedings of the Ninth International Workshop on Distributed Algorithms, </booktitle> <year> 1995, </year> <pages> pp. 168-182. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware. <p> In this case, WLL is not required to return a value of the implemented variable. WLL was first introduced by Anderson and Moir in <ref> [3] </ref> 2 , and is sufficient for most applications that use LL/SC instructions because the 2 The implementation of WLL, VL, and SC presented in [3] implements only one W -word variable. <p> In this case, WLL is not required to return a value of the implemented variable. WLL was first introduced by Anderson and Moir in <ref> [3] </ref> 2 , and is sufficient for most applications that use LL/SC instructions because the 2 The implementation of WLL, VL, and SC presented in [3] implements only one W -word variable. A naive generalization of this implementation for T W -word variables requires fi (N W T ) space overhead; the implementation given here requires only fi (N W ) space overhead, regardless of the number of words implemented. <p> This allows the tag chosen to be the same as the last tag p successfully SC'd, which enables us to use a substantially simpler tag selection mechanism than the one used in <ref> [3] </ref>, and to use fewer tags. Finally, SC attempts to change the implemented variable from the old value to the new using CAS.
Reference: [4] <author> G. Barnes, </author> <title> "A Method for Implementing Lock-Free Shared Data Structures", </title> <booktitle> Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993, </year> <pages> pp. 261-270. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware.
Reference: [5] <author> M. Greenwald and D. Cheriton, </author> <title> "The Synergy Between Non-Blocking Synchronization and Operating System Structure", </title> <booktitle> Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <year> 1996, </year> <pages> pp. 123-136. </pages>
Reference-contexts: Email: moir@cs.pitt.edu. <ref> [5] </ref>). It is well recognized that "strong" synchronization primitives such as compare-and-swap (CAS) and load-linked/store-conditional (LL/SC) are necessary for general non-blocking synchronization [6]. As a result, most modern shared-memory multiprocessors support some form of strong synchronization primitive, and many non-blocking algorithms have been published that depend on such primitives. <p> While our other two implementations are not disjoint access parallel, we believe that it is unlikely that they will introduce excessive contention because accesses to common variables are not concentrated in any one area. These results have several implications for the design of future processors. First, Greenwald and Cheriton <ref> [5] </ref> dismiss software transactional memory (STM) [14] as an alternative for implementing multi-word synchronization primitives because it is inapplicable in existing systems, and conclude that double-word CAS should be provided in hardware.
Reference: [6] <author> M. Herlihy, </author> <title> "Wait-Free Synchronization", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1), </volume> <year> 1991, </year> <pages> pp. 124-149. </pages>
Reference-contexts: Email: moir@cs.pitt.edu. [5]). It is well recognized that "strong" synchronization primitives such as compare-and-swap (CAS) and load-linked/store-conditional (LL/SC) are necessary for general non-blocking synchronization <ref> [6] </ref>. As a result, most modern shared-memory multiprocessors support some form of strong synchronization primitive, and many non-blocking algorithms have been published that depend on such primitives. Unfortunately, a significant gap remains between the primitives relied upon by designers of non-blocking algorithms, and the primitives provided in hardware.
Reference: [7] <author> M. Herlihy, </author> <title> "A Methodology for Implementing Highly Concurrent Data Objects", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(5), </volume> <year> 1993, </year> <pages> pp. 745-770. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware.
Reference: [8] <author> M. Herlihy and J. Moss, </author> <title> "Transactional Memory: Architectural Support for Lock-Free Data Structures", </title> <booktitle> Proceedings of the 20th International Symposium in Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 289-300. </pages>
Reference: [9] <author> M. Herlihy and J. Wing, </author> <title> "Linearizability: A Correctness Condition for Concurrent Objects", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3), </volume> <year> 1990, </year> <pages> pp. 463-492. </pages>
Reference-contexts: RLL and RSC have semantics similar to those of LL and SC, but with the restrictions listed in Section 1. In the full paper, we prove that each of our results yields a linearizable <ref> [9] </ref> implementation of the stated primitives. It is assumed that variables accessed by our implementations are not modified by other means.
Reference: [10] <author> A. Israeli and L. Rappoport, </author> <title> "Disjoint-Access-Parallel Implementations of Strong Shared Memory Primitives", </title> <booktitle> Proceedings of the 13th Annual ACM Symposium on Principles of Distributed Computing , 1994, </booktitle> <pages> pp. 151-160. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware. <p> First, Israeli and Rappoport <ref> [10] </ref> present an implementation of LL/SC from CAS that depends upon unrealistic assumptions about the size of machine words and has high time complexity. Anderson and Moir [2] improve upon this by providing a constant-time implementation with realistic assumptions. <p> Our RLL/RSC-based implementations all terminate provided only finitely many spurious failures occur per operation. These implementations have a very small window between each RLL and the subsequent RSC, which makes spurious failures unlikely and, accordingly, repeated spurious failures extremely unlikely. Also, our first three implementations are disjoint access parallel <ref> [10] </ref>. Roughly, this means that memory contention is not introduced by these implementations. While our other two implementations are not disjoint access parallel, we believe that it is unlikely that they will introduce excessive contention because accesses to common variables are not concentrated in any one area.
Reference: [11] <author> M. Michael and M. Scott, </author> <title> "Implementation of Atomic Primitives on Distributed Shared Memory Multiprocessors", </title> <booktitle> Proceedings of the 1st Annual Symposium on High Performance Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 221-231. </pages>
Reference-contexts: Finally, Michael and Scott <ref> [11] </ref> have recently suggested that CAS, rather than LL/SC, should be implemented on distributed shared memory multiprocessors. Our results indicate to architects that the choice between CAS and LL/SC (in its various forms) will not greatly impact programmers or program complexity. <p> Our results indicate to architects that the choice between CAS and LL/SC (in its various forms) will not greatly impact programmers or program complexity. This frees them to focus on issues of cost and performance (such as those discussed in <ref> [11] </ref>) in deciding which instructions to support. Acknowledgements: We thank John Valois for his comments on an earlier draft of this paper.
Reference: [12] <institution> MIPS R4000 Microprocessor User's Manual, MIPS Computer Systems, Inc., </institution> <year> 1991. </year>
Reference-contexts: The RLL and RSC primitives used in this paper are sufficiently weak that, to our knowledge, they are provided by all hardware implementations of LL/SC-like primitives. For example, on the MIPS R4000 processor <ref> [12] </ref>, LL and SC are implemented using a single bit LLBit per processor. This bit is set by LL, and a subsequent SC succeeds only if the bit is still set. LLBit is cleared by any cache invalidation.
Reference: [13] <institution> PowerPC 601 RISC Microprocessor User's Manual, Motorola, Inc., </institution> <year> 1993. </year>
Reference-contexts: Furthermore, because there is only one LLBit per processor, it is impossible to have concurrent LL-SC sequences on the R4000. Other processors that provide LL and SC, such as the DEC Alpha [1] and the PowerPC <ref> [13] </ref>, implement these instructions similarly and have similar restrictions. It is easy to see why hardware designers prefer to provide weaker versions of LL and SC than those commonly assumed by algorithm designers.
Reference: [14] <author> N. Shavit and D. Touitou, </author> <title> "Software Transactional Memory", </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Distributed Computing , 1995, </booktitle> <pages> pp. 204-213. </pages>
Reference-contexts: In particular, many machines provide either CAS or LL/SC, but not both. Furthermore, most hardware implementations of the LL/SC instructions do not fully implement the semantics expected and assumed by algorithm designers. As a result, several non-blocking algorithms developed recently (e.g. <ref> [2, 3, 4, 7, 10, 14] </ref>) are not directly applicable on current multiprocessors 1 . The results presented here eliminate this gap by providing time- and space-efficient, wait-free implementations of the primitives required by such algorithms, using primitives that are commonly implemented in hardware. <p> These results have several implications for the design of future processors. First, Greenwald and Cheriton [5] dismiss software transactional memory (STM) <ref> [14] </ref> as an alternative for implementing multi-word synchronization primitives because it is inapplicable in existing systems, and conclude that double-word CAS should be provided in hardware.
Reference: [15] <author> J. Valois, </author> <title> "Space Bounds for Transactional Synchronization", </title> <type> unpublished manuscript, </type> <month> January </month> <year> 1997. </year>
Reference-contexts: This would clearly be expensive to implement. The complexity of supporting concurrent LL-SC sequences in hardware is complicated even further by the possibility that multiple processes on one processor might use these instructions concurrently. Recent lower bound results by Valois <ref> [15] </ref> also suggest that it is prohibitively expensive to provide the full semantics of LL, VL, and SC in hardware. It is therefore natural to investigate the possibility of employing existing hardware versions to efficiently provide the desired semantics in software. <p> Anderson and Moir [2] improve upon this by providing a constant-time implementation with realistic assumptions. However, as discussed later, this implementation has impractical space requirements if used to implement many variables. Finally, Valois <ref> [15] </ref> outlines implementations that improve on these space requirements. However, these results were intended primarily to establish upper bounds on space requirements, and are not practical.
References-found: 15

