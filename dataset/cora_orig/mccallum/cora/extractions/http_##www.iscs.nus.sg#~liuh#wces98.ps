URL: http://www.iscs.nus.sg/~liuh/wces98.ps
Refering-URL: 
Root-URL: 
Title: SCALABLE FEATURE SELECTION FOR LARGE SIZED DATABASES  
Author: Huan Liu Rudy Setiono 
Address: Ridge, Singapore 119260  City  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  Mexico  
Pubnum: WCES'98,  
Email: fliuh,rudysg@iscs.nus.sg  
Phone: Tel: (+65) 772-6563; Fax: (+65) 779-4580  
Date: March 16-20, 1998  
Abstract: Feature selection determines relevant features in the data. It is often applied in pattern classification. A special constraint for feature selection nowadays is that the size of a database is normally very large. An effective method is needed to accommodate the practical demands. A scalable probabilistic algorithm is presented here as an alternative to the exhaustive and heuristics approaches. The scalable probabilistic algorithm is designed and implemented to meet the needs arising from real-world data mining applications. Through experiments, we show that (1) the probabilistic algorithm is effective in obtaining optimal/suboptimal subsets of features; (2) its scalable version expedites feature selection further and can scale up without sacrificing the quality of selected features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Most feature selection methods (refer to [6, 7, 5]) can be grouped into two categories: exhaustive or heuristic search of an optimal set of M attributes. For example, Almuallim and Dietterich's FOCUS algorithm <ref> [1] </ref> starts with an empty feature set and carries out exhaustive search until it finds a minimal combination of features that are sufficient to construct a hypothesis consistent with a given set of examples. It works on binary, noise-free data. <p> Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [13], FRINGE [11] and C4.5 [14]. The results in <ref> [1] </ref> suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features.
Reference: [2] <author> M. Boddy and T.L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67(2) </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: By predefining fl according to prior knowledge, LVS and LVF can handle noisy data, as shown in the case of Monk3. Both can deal with multiple class values. Another feature of LVF is related to so-called anytime algorithms <ref> [2] </ref> that are algorithms whose quality of results improves gradually as computational time increases. LVF prints out a possible solution whenever it is found; afterwards it reports either a better one or equally good ones.
Reference: [3] <author> G. Brassard and P. Bratley. </author> <title> Fundamentals of Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1996. </year>
Reference-contexts: Hence, a scalable probabilistic method is designed and implemented. In the following, we describe the probabilistic method first, then the scalable one, followed by an empirical study and a conclusion. PROBABILISTIC FEATURE SELECTION The proposed probabilistic approach is a Las Vegas Algorithm <ref> [3] </ref>. Las Vegas algorithms make probabilistic choices to help guide them more quickly to a correct solution. <p> Another similar type of algorithms is Monte Carlo algorithms in which it is often possible to reduce the error probability arbitrarily at the cost of a slight increase in computing time (refer to page 341 in <ref> [3] </ref>). One kind of Las Vegas algorithms uses randomness to guide their search in such a way that a correct solution is guaranteed even if unfortunate choices are made. As we mentioned earlier, heuristic search methods are vulnerable to the datasets of high order correlations.
Reference: [4] <author> P.A. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall International, </publisher> <year> 1982. </year>
Reference-contexts: This problem has long been an active research topic within statistics and pattern recognition <ref> [17, 4] </ref>, but most work in this area has dealt with linear regression [7] and is under assumptions that do not apply to most learning algorithms [5].
Reference: [5] <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant feature and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann Publisher, </publisher> <year> 1994. </year>
Reference-contexts: This problem has long been an active research topic within statistics and pattern recognition [17, 4], but most work in this area has dealt with linear regression [7] and is under assumptions that do not apply to most learning algorithms <ref> [5] </ref>. Researchers pointed out that the most common assumption is monotonicity, that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> Recently feature selection has received considerable attention from machine learning and knowledge discovery re 1 The monotonicity assumption is not valid for many induction algorithms used in machine learning. See for dataset 1 (CorrAL) in Section 4 which is reproduced from <ref> [5] </ref>. searchers interested in improving the performance of their algorithms and in cleaning data. In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long to run before the data is reduced. <p> In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long to run before the data is reduced. Most feature selection methods (refer to <ref> [6, 7, 5] </ref>) can be grouped into two categories: exhaustive or heuristic search of an optimal set of M attributes. <p> Training and testing datasets are split randomly if not specified. algorithms or artificially designed so that relevant attributes are known. This set consists of both artificial and real-world data. The artificial datasets are (1) CorrAL The data set was designed in <ref> [5] </ref>. There are six binary features, A 0 ; A 1 ; B 0 ; B 1 ; I; and C. Feature I is irrelevant, feature C is correlated to the class label 75% of the time. <p> The real-world datasets are (4) Credit (or CRX), (5) Labor, (6) Vote, and (7) Mushroom. The choices of training and testing datasets (4 - 6) are the same as in <ref> [5] </ref>. The details are in Table 1. The Mushroom data is randomly separated into training and testing sets. Since most datasets above are relatively small, we only choose Vote and Mushroom plus ParityMix to show the effectiveness of LVS in relation to the size of a dataset below. <p> To choose which solution can be based on the application of a learning algorithm. Hence, one straightforward extension of this work is a combined filter and wrapper <ref> [5] </ref> model of this scalable probabilistic algorithm. Another line of research is to search for other criteria in addition to the inconsistency criterion used in this work. Scalable Feature Selection forLarge Sized Databases Acknowledgments: Thanks H.Y. Lee for the suggestions on an earlier version of this paper; H.L.
Reference: [6] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long to run before the data is reduced. Most feature selection methods (refer to <ref> [6, 7, 5] </ref>) can be grouped into two categories: exhaustive or heuristic search of an optimal set of M attributes. <p> It works on binary, noise-free data. Its time complexity is O (min (N M ; 2 N )). They proposed three heuristic algorithms to speed up the searching. There are many heuristic feature selection algorithms. The Relief algorithm <ref> [6] </ref> assigns a "relevance"weight to each feature, which is meant to denote the relevance of the feature to the target concept.
Reference: [7] <author> P. Langley. </author> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: This problem has long been an active research topic within statistics and pattern recognition [17, 4], but most work in this area has dealt with linear regression <ref> [7] </ref> and is under assumptions that do not apply to most learning algorithms [5]. Researchers pointed out that the most common assumption is monotonicity, that increasing the number of features can only improve the performance of a learning algorithm 1 . <p> In handling large databases, feature selection is even more important since many learning algorithms may falter or take too long to run before the data is reduced. Most feature selection methods (refer to <ref> [6, 7, 5] </ref>) can be grouped into two categories: exhaustive or heuristic search of an optimal set of M attributes.
Reference: [8] <author> H. Liu and W.X. Wen. </author> <title> Concept learning through feature selection. </title> <booktitle> In Proceedings of First Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <year> 1993. </year>
Reference-contexts: The PRESET algorithm [10] is another heuristic feature selector that uses the theory of Rough Sets to heuristically rank the features, assuming a noise-free binary domain. In order to consider higher order information among the attribute, Liu and Wen suggest <ref> [8] </ref> to use high order information gains to select features.
Reference: [9] <author> P. Mehra, L.A. Rendell, and B.W. Wah. </author> <title> Principled constructive induction. </title> <booktitle> In Proceedings of IJCAI, </booktitle> <pages> pages 651-656, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The inconsistency criterion is in line with information-theoretic considerations [16] which suggest that using a good feature of discrimination provides compact descriptions of each of the two classes, and that these descriptions are maximally distinct. Geometrically, this constraint can be interpreted <ref> [9] </ref> to mean that (i) such a feature takes on nearly identical values for all examples of the same class, and (ii) it takes on some different values for all examples of the other class. The inconsistency criterion aims to keep the discriminating 5 77 is chosen by experiment.
Reference: [10] <author> M Modrzejewski. </author> <title> Feature selection using rough sets theory. </title> <editor> In P.B. Brazdil, editor, </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 213-226, </pages> <year> 1993. </year>
Reference-contexts: According to Kira and Rendell, Relief assumes two-class classification problems and does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description. The PRESET algorithm <ref> [10] </ref> is another heuristic feature selector that uses the theory of Rough Sets to heuristically rank the features, assuming a noise-free binary domain. In order to consider higher order information among the attribute, Liu and Wen suggest [8] to use high order information gains to select features.
Reference: [11] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [13], FRINGE <ref> [11] </ref> and C4.5 [14]. The results in [1] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features.
Reference: [12] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: With a good pseudo random number generator <ref> [12] </ref>, the selection of an optimal subset of M features can be considered non-replacement experiments.
Reference: [13] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 <ref> [13] </ref>, FRINGE [11] and C4.5 [14]. The results in [1] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. <p> Therefore, whether the selected features are relevant or not can only be determined indirectly. One way is to see the effect of feature selection through a learning algo rithm. Among many choices, we chose C4.5 [14] and ID3 <ref> [13] </ref> in our experiments because (1) C4.5 works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures. LVF is run 100 times on each training dataset.
Reference: [14] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Another common understanding is that some learning algorithms have built-in feature selection, for example, ID3 [13], FRINGE [11] and C4.5 <ref> [14] </ref>. The results in [1] suggest that one should not rely on ID3 or FRINGE to filter out irrelevant features. <p> However, for the real-world datasets, it is not clear what the relevant features are. Therefore, whether the selected features are relevant or not can only be determined indirectly. One way is to see the effect of feature selection through a learning algo rithm. Among many choices, we chose C4.5 <ref> [14] </ref> and ID3 [13] in our experiments because (1) C4.5 works well on most data sets as reported by many researchers; and (2) it employs a heuristic to find simplest tree structures. LVF is run 100 times on each training dataset.
Reference: [15] <editor> S.B. Thrun and et al. </editor> <title> The monk's problems: A performance comarison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Both ID3 and C4.5 chose feature C as the root. This is an example of datasets in which if a feature like C is removed, a more accurate tree will result; (2) Monk1, Monk2, Monk3 <ref> [15] </ref>, these datasets of 6 attributes can be used to show that a feature selector only selects the relevant features, while keeping all the relevant ones; and (3) Parity5+5, the target concept is the parity of five bits.
Reference: [16] <author> S. Watanabe. </author> <title> Pattern Recognition: Human and Mechanical. </title> <publisher> Wiley Interscience, </publisher> <year> 1985. </year>
Reference-contexts: If c 3 is the largest among the three, the inconsistency count is (n c 3 ); (3) the inconsistency rate is the sum of all the inconsistency counts divided by the total number of patterns (N ). The inconsistency criterion is in line with information-theoretic considerations <ref> [16] </ref> which suggest that using a good feature of discrimination provides compact descriptions of each of the two classes, and that these descriptions are maximally distinct.
Reference: [17] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year>
Reference-contexts: This problem has long been an active research topic within statistics and pattern recognition <ref> [17, 4] </ref>, but most work in this area has dealt with linear regression [7] and is under assumptions that do not apply to most learning algorithms [5].
References-found: 17

