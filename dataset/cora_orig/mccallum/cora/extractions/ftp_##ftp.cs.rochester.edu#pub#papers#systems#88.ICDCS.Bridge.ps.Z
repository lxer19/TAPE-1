URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/88.ICDCS.Bridge.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Title: Bridge: A High-Performance File System for Parallel Processors  
Author: Peter C. Dibble Michael L. Scott Carla Schlatter Ellis 
Address: Rochester, NY 14627  Durham, NC 27706  
Affiliation: Department of Computer Science University of Rochester  Department of Computer Science Duke University  
Abstract: We have designed and prototyped a parallel file system that distributes each file across multiple storage devices and processors. Naive programs are able to access files just as they would with a conventional file system, while more sophisticated programs may export pieces of their code to the processors managing the data, for optimum performance. Theoretical and early empirical data indicate nearly linear speedup on critical operations for up to several dozen devices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> "Butterfly TM parallel processor overview," </institution> <type> Tech. Rep. 6149, Version 2, </type> <institution> BBN Laboratories, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: Following a bit of background information, we introduce the notion of interleaving in section 3 and (in section 4) describe its realization in an experimental file system known as Bridge. Our prototype runs on a BBN Butterfly parallel processor <ref> [1] </ref>, but the ideas on which it is based are equally applicable to a large number of other parallel architectures and to locally-distributed collections of conventional machines. <p> The top layer consists of the Bridge Server and a group of special purpose programs we call tools. The middle layer consists of local file systems on individual nodes. The lowest layer manages physical storage devices. Our implementation runs under the Chrysalis operating system <ref> [1] </ref> on the BBN Butterfly Parallel Processor [14]. The components of the file system are implemented as user processes; no changes to the operating system were required. All components communicate via message passing.
Reference: [2] <author> C. J. Bashe, W. Buchholz, B. V. Hawkins, J. J. Ingram, and N. </author> <title> Rochester, "The architecture of IBM's early computers," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 25, </volume> <pages> pp. 363-375, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Section 6 discusses general issues in the design of algorithms for Bridge. The final section describes the status of our work and discusses future plans. 2 Background I/O bottlenecks have been a source of concern for computer system designers since at least the early 1950's <ref> [2] </ref>. Since that time, parallelism has been built into most of the individual components of the I/O data path. Busses, for example, have transferred bits in parallel for many years. Similar "trivial parallelism" in hardware can increase the performance of controllers and connecting cables almost indefinitely.
Reference: [3] <author> M. Gamerl, </author> <title> "Maturing parallel transfer disk technology finds more applications," </title> <journal> Hardcopy, </journal> <volume> vol. 7, </volume> <pages> pp. 41-48, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Parallelism in storage devices can be achieved in several ways. Traditional memory interleaving can be employed in solid-state "RAM disks" to produce very high transfer rates. Mechanical disks that read several heads at once are available from such vendors as CDC and Fujitsu <ref> [3, 4] </ref>. Other manufacturers have introduced, or are at least investigating, so-called storage arrays that assemble multiple drives into a single logical device with enormous throughput [5, 6].
Reference: [4] <author> H. Boral and D. J. DeWitt, </author> <title> "Database machines: An idea whose time has passed: A critique of the future of database machines," </title> <type> Tech. Rep. 288, </type> <institution> Technion, </institution> <month> August </month> <year> 1983. </year>
Reference-contexts: Parallelism in storage devices can be achieved in several ways. Traditional memory interleaving can be employed in solid-state "RAM disks" to produce very high transfer rates. Mechanical disks that read several heads at once are available from such vendors as CDC and Fujitsu <ref> [3, 4] </ref>. Other manufacturers have introduced, or are at least investigating, so-called storage arrays that assemble multiple drives into a single logical device with enormous throughput [5, 6].
Reference: [5] <author> J. Voelcker, </author> <title> "Winchester disks reach for a gigabyte," </title> <journal> IEEE Spectrum, </journal> <volume> vol. 24, </volume> <pages> pp. 64-67, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Mechanical disks that read several heads at once are available from such vendors as CDC and Fujitsu [3, 4]. Other manufacturers have introduced, or are at least investigating, so-called storage arrays that assemble multiple drives into a single logical device with enormous throughput <ref> [5, 6] </ref>. Unlike multiple-head drives, storage arrays can be scaled to arbitrary levels of parallelism, though they have the unfortunate tendency to maximize rotational latency: each operation must wait for the most poorly positioned disk.
Reference: [6] <author> T. Manuel and C. Barney, </author> <title> "The big drag on computer throughput," </title> <journal> Electronics, </journal> <volume> vol. 59, </volume> <pages> pp. 51-53, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Mechanical disks that read several heads at once are available from such vendors as CDC and Fujitsu [3, 4]. Other manufacturers have introduced, or are at least investigating, so-called storage arrays that assemble multiple drives into a single logical device with enormous throughput <ref> [5, 6] </ref>. Unlike multiple-head drives, storage arrays can be scaled to arbitrary levels of parallelism, though they have the unfortunate tendency to maximize rotational latency: each operation must wait for the most poorly positioned disk.
Reference: [7] <author> K. Salem and H. Garcia-Molina, </author> <title> "Disk striping," </title> <type> Tech. Rep. 332, </type> <institution> EECS Department, Princeton University, </institution> <month> December </month> <year> 1984. </year>
Reference-contexts: Unlike multiple-head drives, storage arrays can be scaled to arbitrary levels of parallelism, though they have the unfortunate tendency to maximize rotational latency: each operation must wait for the most poorly positioned disk. As an alternative to storage arrays, Salem and Garcia-Molina have studied the notion of disk striping <ref> [7] </ref>, in which conventional devices are joined logically at the level of the file system soft-ware. Consecutive blocks are located on different disk drives, so the file system can initiate I/O operations on several blocks in parallel.
Reference: [8] <author> D. J. DeWitt, R. H. Gerber, G. Graefe, M. L. Heytens, K. B. Kumar, and M. Muralikrishna, </author> <title> "Gamma: A high performance dataflow database machine," </title> <type> Tech. Rep. 635, </type> <institution> Department of Computer Sciences, University of Wisconsin - Madison, </institution> <month> March </month> <year> 1986. </year> <title> [9] "Connection machine model CM-2 technical summary," </title> <type> Tech. Rep. </type> <institution> HA87-4, Thinking Machines Inc., </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: In its emphasis on distribution of data, parallel file system code, and the local execution of application-specific operations, Bridge shares a number of ideas with the Gamma project at the University of Wisconsin <ref> [8] </ref>. The fundamental difference is that Bridge is a general-purpose file system while Gamma is a relational database system. Our interest in traditional file operations requires that we present a more general-purpose interface. <p> If the round-robin distribution can start on any node, then the nth block will be found on processor ((n + k) mod p), where block zero belongs to LFS k. Of course, round-robin distribution is not the only possible strategy for allocating blocks to nodes. Gamma <ref> [8] </ref>, for example, allows a file to be divided into exactly p equal-size chunks of contiguous blocks. Each chunk is allocated to a processor in its entirety. Gamma also allows the blocks of a file to be scattered randomly among nodes according to a hash function.
Reference: [10] <author> R. Floyd, </author> <title> "Short-term file reference patterns in a UNIX environment," </title> <type> Tech. Rep. 177, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: The most appropriate distribution strategy for parallel files will ultimately depend on the role that files assume in parallel applications. Unfortunately, the information that is currently available about file usage patterns <ref> [10, 11, 12] </ref> in uniprocessor systems does not necessarily apply to the multiprocessor environment. Preliminary experience allows us to make some educated guesses about what to expect.
Reference: [11] <author> J. Porcar, </author> <title> "File migration in distributed computer systems," </title> <type> Tech. Rep. </type> <institution> LBL-14763, Lawrence Berkeley Laboratory, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: The most appropriate distribution strategy for parallel files will ultimately depend on the role that files assume in parallel applications. Unfortunately, the information that is currently available about file usage patterns <ref> [10, 11, 12] </ref> in uniprocessor systems does not necessarily apply to the multiprocessor environment. Preliminary experience allows us to make some educated guesses about what to expect.
Reference: [12] <author> J. Ousterhout, H. DaCosta, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson, </author> <title> "A trace driven analysis of the UNIX 4.2 BSD file system," </title> <booktitle> Proceedings of 10th Symposium on Operating Systems Principles, Operating Systems Review, </booktitle> <volume> vol. 19, </volume> <pages> pp. 15-24, </pages> <month> December </month> <year> 1985. </year> <title> [13] "The Butterfly RAMFile system," </title> <type> Tech. Rep. 6351, </type> <institution> BBN Advanced Computers Incorporated, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: The most appropriate distribution strategy for parallel files will ultimately depend on the role that files assume in parallel applications. Unfortunately, the information that is currently available about file usage patterns <ref> [10, 11, 12] </ref> in uniprocessor systems does not necessarily apply to the multiprocessor environment. Preliminary experience allows us to make some educated guesses about what to expect.
Reference: [14] <institution> BBN Advanced Computers Inc., Chrysalis Programmers Manual, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: The middle layer consists of local file systems on individual nodes. The lowest layer manages physical storage devices. Our implementation runs under the Chrysalis operating system [1] on the BBN Butterfly Parallel Processor <ref> [14] </ref>. The components of the file system are implemented as user processes; no changes to the operating system were required. All components communicate via message passing.
Reference: [15] <author> R. F. Gurwitz, M. A. Dean, and R. E. Schantz, </author> <booktitle> "Programming support in the Cronus distributed operating system," in Sixth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 486-493, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The implementation and performance of a sorting tool are discussed in section 5.2. 4.3 Local File System The local file servers for Bridge are an adaptation of the Elementary File System (EFS) constructed for the Cronus distributed system project at BBN <ref> [15, 16] </ref>. EFS is a simple, stateless file system with a flat name space and no access control. File names are numbers that are used to hash into a directory. Files are represented as doubly linked circular lists of blocks.
Reference: [16] <author> R. Schantz, </author> <title> "Elementary file system," </title> <type> Tech. Rep. </type> <institution> DOS-79, BBN, </institution> <month> April </month> <year> 1984. </year>
Reference-contexts: The implementation and performance of a sorting tool are discussed in section 5.2. 4.3 Local File System The local file servers for Bridge are an adaptation of the Elementary File System (EFS) constructed for the Cronus distributed system project at BBN <ref> [15, 16] </ref>. EFS is a simple, stateless file system with a flat name space and no access control. File names are numbers that are used to hash into a directory. Files are represented as doubly linked circular lists of blocks.
Reference: [17] <author> P. C. Dibble and M. L. Scott, </author> <title> "Analysis of a parallel disk-based merge sort," </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Rochester. </institution> <note> In preparation. </note>
Reference-contexts: We expect over the course of the upcoming year to develop several additional tools. We have developed an unconventional mathematical analysis of the merge sort algorithm that expresses the maximum available degree of parallelism in terms of the relative performance of processors, communication channels, and physical devices <ref> [17] </ref>. The results we obtain for the constants on the Butterfly agree quite nicely with empirical data. We hope to produce a similar analysis for future tools as well.
References-found: 15

