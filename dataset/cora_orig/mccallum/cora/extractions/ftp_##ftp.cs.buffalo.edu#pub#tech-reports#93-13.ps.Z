URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/93-13.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: e-mail: lammens@cs.buffalo.edu  
Phone: tel: (716) 645-3180, fax: (716) 645-3464  
Title: Of Elephants and Men  
Author: Johan M. Lammens, Henry H. Hexmoor, Stuart C. Shapiro 
Note: To appear in proceedings of the NATO-ASI on the Biology and  (preliminary version)  
Address: Buffalo, NY 14260  Trento, Italy,  
Affiliation: Autonomous Agents Lab Computer Science Department State University of New York at Buffalo  Technology of Intelligent Autonomous Agents,  
Date: April 28, 1993  March 1-12 1993.  
Abstract: In the elephant paper [Bro90], Brooks criticized the ungroundedness of traditional symbol systems, and proposed physically grounded systems as an alternative, in particular the subsumption architecture. Although we are still struggling with many of the issues involved, we believe we have some contributions to make towards solving some of the open problems with physically grounded systems, particularly with respect to whether or how to integrate the old with the new. In this paper we describe an agent architecture that specifies an integration of explicit representation and reasoning mechanisms, embodied semantics through grounding symbols in perception and action, and implicit representations of special-purpose mechanisms of sensory processing, perception, and motor control. We then present components that we place in our general architecture to build agents that exhibit situated activity and learning, and finally a physical agent implementation and two simulation studies. The gist of our paper is that the Brooksian behavior generation approach goes a long way towards modeling elephant behavior, which we find very interesting, but that in order to model more deliberative behavior we may also need something else. fl This work was supported in part by Equipment Grant No. EDUD-US-932022 from SUN Microsystems Computer Corporation. y Apologies to John Steinbeck, who carries no blame for this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [ABN81] <author> James Albus, Anthony Barbera, and Roger Nagel. </author> <title> Theory and practice of hierarchical control. </title> <booktitle> In 23rd International IEEE Computer Society Conference, </booktitle> <pages> pages 18-38, </pages> <year> 1981. </year>
Reference-contexts: Representation, reasoning (including planning), perception, and generation of behavior are distributed through all three levels. Our architecture is best described using a resolution pyramid metaphor as used in computer vision work [BB82], rather than a central vs. peripheral metaphor. Architectures for building physical systems, e.g., robotic architectures <ref> [ABN81] </ref>, tend to address the relationship between a physical entity, (e.g., a robot), sensors, effectors, and tasks to be accomplished. Since these physical systems are performance centered, they often lack general knowledge representation and reasoning techniques. <p> For example, the symbolic layer in SSS seems to be a decision table versus a general KRR as intended in GLAIR. Unlike GLAIR, SSS assigns particular tasks for each layer and uses a hard-wired interconnection channel among layers. Albus et al's hierarchical control architecture <ref> [ABN81] </ref> is an example of a robotic architecture; we would say it is body centered. This architecture proposes abstraction levels for behavior generation, sensory processing, and world modeling. By descending down the hierarchy, tasks are decomposed into robot-motion primitives. This differs from our architecture, which is not strictly top-down controlled.
Reference: [AC87] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <address> Seattle, Wa., </address> <pages> pages 268-272, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: In this type of setting, the agent often does not care to uniquely identify objects. It is sufficient to know the current relationship of the relevant objects to the agent, and what roles the objects play in the agent's activities. Agre and Chapman in <ref> [AC87] </ref> proposed indexical-functional representations (which [Agr88] refers to as deictic representations) to be the more natural way agents refer to objects in common everyday environments. They called entities and relationships of interest entities and aspects, respectively. <p> It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88]. Situated action is used in reactive planning <ref> [AC87, Fir87, Sch87] </ref>. We have developed an implementation mechanism for the PM-level which we call PM-automata (PMA), [HN92]. A PMA is a finite state machine in which each state is associated with an act and arcs are associated with perceptions. <p> A complete setup using MRL consists of a GLAIR-agent, a simulator with an incorporated description of a physical environment, and a graphical interface (figure 6). 3.3.1 Emergent Behaviors A major objective for this project is learning emergent behaviors. Like Agre with his improvised actions <ref> [AC87] </ref> and Brooks with his subsumption architecture [Bro85] we believe complex behaviors emerge from interaction of the agent with its environment without planning. However, previous work in this area hard-coded a lot of primitive actions. Furthermore, it did not attempt to learn the improvised behavior.
Reference: [Agr88] <author> Philip Agre. </author> <title> The dynamic structure of everyday life. </title> <type> Technical Report 1085, </type> <institution> MIT Artificial Intelligence Laboratory, MIT, </institution> <year> 1988. </year>
Reference-contexts: In this type of setting, the agent often does not care to uniquely identify objects. It is sufficient to know the current relationship of the relevant objects to the agent, and what roles the objects play in the agent's activities. Agre and Chapman in [AC87] proposed indexical-functional representations (which <ref> [Agr88] </ref> refers to as deictic representations) to be the more natural way agents refer to objects in common everyday environments. They called entities and relationships of interest entities and aspects, respectively. With respect to its current activities, the agent needs only to focus on representing those entities and relationships. <p> Maes has explored learning and has applied her architecture to robotic systems. In the subsumption architecture, sensations and actions are abstracted by giving them names like "straightening behavior" in order to make things easier to understand for human observers. Much in the spirit of <ref> [Agr88] </ref>, we believe that behavior modules should more naturally emerge from the interaction of the agent with its environment. In contrast to hand coding behaviors and in order to facilitate embodiment, in GLAIR we are experimenting with (unnamed) emergent behavior modules that are learned by a robot from scratch.
Reference: [AHC91] <author> Scott Anderson, David Hart, and Paul Cohen. </author> <title> Two ways to act. </title> <journal> In ACM SIGART Bulletin, </journal> <pages> pages 20-24. </pages> <publisher> ACM publications, </publisher> <year> 1991. </year>
Reference-contexts: We use the term tracking to refer to an action that requires continual adjustments, like steering while driving. Examples of this type of reactive behavior are given in <ref> [Pay86, AHC91] </ref>. Situated behavior requires assessment of the state the system finds itself in (in some state space) and acting on the basis of that. It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88].
Reference: [Alb91] <author> James Albus. </author> <title> Outline for a theory of intelligence. </title> <journal> In IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 21, No. 3, </volume> <pages> pages 473-509, </pages> <year> 1991. </year>
Reference-contexts: planning, learning, exper imentation, and perception. * A Kinematic/Perceptual model: At the Perceptuo-Motor level, we maintain a model of the simple agent-level physics of the objects of interest to the agent. 22 The kinematic/perceptual model models 20 Albus defines a world model as the agent's best estimate of objective reality <ref> [Alb91] </ref>. 21 Situated cognition and reactive planning are proponents of avoiding world modeling, e.g., [Bro90, Suc88]. 22 Even the prominent advocators of doing away with world models actually use a variety of models, some of which qualify as 15 motor capacities and motor memory that might be implemented in different ways
Reference: [And83] <author> J. R. Anderson. </author> <title> The Architecture of Cognition. </title> <publisher> Cambridge: Harvard University Press, </publisher> <year> 1983. </year>
Reference-contexts: (1) having some internal states or representations that are causally connected to the environment through perception, (2) being able to reason explicitly about the environment, and (3) being able to communicate with an external agent about the environment. 3 Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [And83, Pol89, LMA91] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [BB82] <author> Dana H. Ballard and Christopher M. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, 1982. 25 He-man or She-man. </address> <month> 23 </month>
Reference-contexts: Representation, reasoning (including planning), perception, and generation of behavior are distributed through all three levels. Our architecture is best described using a resolution pyramid metaphor as used in computer vision work <ref> [BB82] </ref>, rather than a central vs. peripheral metaphor. Architectures for building physical systems, e.g., robotic architectures [ABN81], tend to address the relationship between a physical entity, (e.g., a robot), sensors, effectors, and tasks to be accomplished.
Reference: [BK69] <author> Brent Berlin and Paul Kay. </author> <title> Basic Color Terms: Their Universality and Evolution. </title> <institution> University of California Press, Berkeley CA, </institution> <note> first paperback edition, 1991 (orig. </note> <year> 1969). </year>
Reference-contexts: This model allows the agent to (1) name colors shown to it, and express a typicality judgement, (2) point out examples of named colors in its environment, and (3) learn new names for colors. This model provides the perceptual grounding for a set of basic color terms <ref> [BK69] </ref> represented at the Knowledge level. The color domain was chosen as a case study for embodied perception because of the relative abundance of psychological, psychophysical and neurophysiological data in the literature [Lam].
Reference: [Bro85] <author> Rodney Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <type> Technical Report 864, </type> <institution> MIT AI Labs, MIT, </institution> <year> 1985. </year>
Reference-contexts: The organization of an architecture may also be influenced by whether or not one adopts the modularity assumption of Fodor [Fod83], or a connectionist point of view, e.g. [MRH86], or an anti-modularity assumption as in Brooks's subsumption architecture <ref> [Bro85] </ref>. The modularity assumption supports (among other things) a division of the mind into a central system, i.e., cognitive processes such as learning, planning, and reasoning, and a peripheral system, i.e., sensory and motor processing [Cha90]. <p> We reserve the term sub-conscious for implicit cognitive processes such as category subsumption in KRR systems. See [Sha90] for a discussion of sub-conscious reasoning. 6 This autonomy is similar to Brooks's subsumption architecture <ref> [Bro85] </ref>, but at a more macroscopic level. Brooks does not distinguish between the three levels we describe, as his work is solely concerned with behaviors whose controlling mechanism we would situate at the Perceptuo-Motor level. 4 like SOAR adhere to this model. <p> We believe that systems endowed with general KRR abilities can and should generate deictic representations to create and maintain a focus on entities in the world, but we have not yet designed an implementation strategy. 2.1.2 Implemented Architectures Brooks's subsumption architecture, <ref> [Bro85, Bro87, Bro90] </ref>, clusters behaviors into layers. Low-level behaviors, like deciding the direction of motion and speed, can be interrupted by behaviors determined at higher levels, such as avoiding obstacles. Subsumption behaviors are written as finite state machines augmented with timing elements. <p> Like Agre with his improvised actions [AC87] and Brooks with his subsumption architecture <ref> [Bro85] </ref> we believe complex behaviors emerge from interaction of the agent with its environment without planning. However, previous work in this area hard-coded a lot of primitive actions. Furthermore, it did not attempt to learn the improvised behavior.
Reference: [Bro87] <author> Rodney Brooks. </author> <title> Planning is just a way of avoiding figuring out what to do next. </title> <type> Technical Report 303, </type> <institution> MIT AI Labs, </institution> <year> 1987. </year>
Reference-contexts: We believe that systems endowed with general KRR abilities can and should generate deictic representations to create and maintain a focus on entities in the world, but we have not yet designed an implementation strategy. 2.1.2 Implemented Architectures Brooks's subsumption architecture, <ref> [Bro85, Bro87, Bro90] </ref>, clusters behaviors into layers. Low-level behaviors, like deciding the direction of motion and speed, can be interrupted by behaviors determined at higher levels, such as avoiding obstacles. Subsumption behaviors are written as finite state machines augmented with timing elements.
Reference: [Bro90] <author> Rodney A. Brooks. </author> <title> Elephants don't play chess. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6 </volume> <pages> 3-15, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction and Overview In the elephant paper <ref> [Bro90] </ref> appearing in the proceedings of the predecessor of the current workshop, Brooks criticizes the ungroundedness of traditional symbolic AI systems, and proposes physically grounded systems as an alternative, particularly the subsumption architecture. <p> Although we are still struggling with many of the issues involved, we believe we can contribute to a solution for some of the problems for both classical systems and physically grounded systems mentioned in <ref> [Bro90] </ref>, in particular: * The ungroundedness of symbolic systems (referred to as "the symbol grounding problem" by [Har90]): our architecture attempts to ground high level symbols in perception and action, through a process of embodiment. * The potential mismatch between symbolic representations and the agent's sensors and actuators: the embodied semantics <p> We believe that systems endowed with general KRR abilities can and should generate deictic representations to create and maintain a focus on entities in the world, but we have not yet designed an implementation strategy. 2.1.2 Implemented Architectures Brooks's subsumption architecture, <ref> [Bro85, Bro87, Bro90] </ref>, clusters behaviors into layers. Low-level behaviors, like deciding the direction of motion and speed, can be interrupted by behaviors determined at higher levels, such as avoiding obstacles. Subsumption behaviors are written as finite state machines augmented with timing elements. <p> we maintain a model of the simple agent-level physics of the objects of interest to the agent. 22 The kinematic/perceptual model models 20 Albus defines a world model as the agent's best estimate of objective reality [Alb91]. 21 Situated cognition and reactive planning are proponents of avoiding world modeling, e.g., <ref> [Bro90, Suc88] </ref>. 22 Even the prominent advocators of doing away with world models actually use a variety of models, some of which qualify as 15 motor capacities and motor memory that might be implemented in different ways (e.g., purely proce--durally or in a network of nodes and weighted links), but we
Reference: [CH93] <author> Guido Caicedo and Henry Hexmoor. </author> <title> Alignment in GLAIR. </title> <type> Technical Report Forthcoming, </type> <institution> Computer Science Department, State University of New York at Buffalo, Buffalo, </institution> <address> NY, </address> <year> 1993. </year>
Reference-contexts: For a detailed discussion of alignment see <ref> [CH93] </ref>. 2.6 Consciousness As we pointed out above, we identify the Knowledge level with consciously accessible data and processing; the Perceptuo-Motor level with "hard-wired", not consciously accessible processing and data involved with motor control and perceptual processing; and the Sensori-Actuator level with the lowest-level muscular and sensor control, also not consciously
Reference: [Cha90] <author> David Chapman. </author> <title> Vision, instruction, and action. </title> <type> Technical Report Technical Report 1204, </type> <institution> MIT Artificial Intelligence Laboratory, MIT, </institution> <year> 1990. </year>
Reference-contexts: The modularity assumption supports (among other things) a division of the mind into a central system, i.e., cognitive processes such as learning, planning, and reasoning, and a peripheral system, i.e., sensory and motor processing <ref> [Cha90] </ref>. Our architecture is characterized by a three-level organization into a Knowledge level, a Perceptuo-Motor level, and a Sensory-Actuator level. This organization is neither modular, anti-modular, hierarchical, anti-hierarchical, nor connectionist in the conventional sense. It integrates a traditional symbol system with a physically grounded system, i.e., a behavior-based architecture. <p> We started ABS with an empty PMA and as the game was played, transitions of the PMA were learned. Also as the transitions were learned, when similar situations occurred and there was an appropriate PMA kinematic/perceptual models. For instance, see Chapman's work on Sonja <ref> [Cha90] </ref> where Sonja has to build a convex hull of obstacles and compute angles in order to decide the best way to avoid them. 17 response, the PMA executed that action.
Reference: [CMN83] <author> S.K. Card, T.P. Moran, and A. Newell. </author> <title> The Psychology of Human-Computer Interaction. </title> <address> Erl-baum, Hillsdale, N.J., </address> <year> 1983. </year>
Reference-contexts: As in behavior-based AI, GLAIR gains validity from its being grounded in its interaction with the environment, while it benefits from a knowledge level that, independent of reacting to a changing environment, performs reasoning and planning. The Model Human Processor (MHP) is a cognitive model <ref> [CMN83] </ref> that suggests the three components of perception, cognition, and motor. Cognition consists of working memory, long-term memory, and the cognitive processor. Perception is a hierarchy of sensory processing. Motor executes the actions in the working memory. This is a traditional symbol-system decomposition of human information processing.
Reference: [Con92] <author> Jonathan Connell. </author> <title> Sss: A hybrid architecture applied to robot navigation. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 2719-2724, </pages> <year> 1992. </year>
Reference-contexts: To bootstrap the learning process, we need a set of primary or first-order ("innate", for the philosophically inclined) sensations and actions. We will return to this point briefly in section 3.3.1. The Servo, Subsumption, Symbolic (SSS) architecture <ref> [Con92] </ref> is a hybrid architecture for mobile robots 7 This kind of designation is merely a mnemonic representation intended to suggest the entity and aspect under consideration, for the purpose of our exposition.
Reference: [Cul63] <author> James Culbertson. </author> <title> The Minds of Robots. </title> <editor> U. </editor> <publisher> of Illinois Press, </publisher> <year> 1963. </year>
Reference-contexts: Figure 1 schematically presents our architecture. paths suggests the amount of information passing through (bandwidth). Sensors include both world-sensors and proprio-sensors. 3 A machine like a vending machine or an industrial robot has responses, but it is unconscious. See <ref> [Cul63] </ref> for a discussion of independence of consciousness from having a response.
Reference: [Fir87] <author> R. James Firby. </author> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <pages> pages 202-206, </pages> <year> 1987. </year>
Reference-contexts: Gat in [Gat91] describes ATLANTIS, an architecture for the control of mobile robots. This architecture has three components: control, sequencing, and deliberation. The control layer is designed as a set of circuit-like functions using Gat's language for circuits, ALPHA. The sequencing is a variation of Jim Firby's RAP system <ref> [Fir87] </ref>. The deliberation layer is the least described layer. As for situated automata, we are not convinced that this is the right kind of formalism to use, for the same reasons. An architecture for low-level and high-level reactivity is suggested in [Hex89]. High-level reactivity is reactivity at the conceptual level. <p> It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88]. Situated action is used in reactive planning <ref> [AC87, Fir87, Sch87] </ref>. We have developed an implementation mechanism for the PM-level which we call PM-automata (PMA), [HN92]. A PMA is a finite state machine in which each state is associated with an act and arcs are associated with perceptions.
Reference: [Fod83] <author> Jerry Fodor. </author> <title> The Modularity of Mind. </title> <publisher> MIT Press, </publisher> <year> 1983. </year>
Reference-contexts: The organization of an architecture may also be influenced by whether or not one adopts the modularity assumption of Fodor <ref> [Fod83] </ref>, or a connectionist point of view, e.g. [MRH86], or an anti-modularity assumption as in Brooks's subsumption architecture [Bro85].
Reference: [Gat91] <author> Erann Gat. </author> <title> Reliable goal-directed reactive control of autonomous mobile robot. </title> <type> Technical Report Tec, </type> <institution> Dept. of Computer Science, Virginia Polytechnic Institute and State University, </institution> <year> 1991. </year>
Reference-contexts: Perhaps the operation of our Perceptuo-motor level could be modeled by a situated automaton, but we are not convinced that this is the right formalism to use, due to its inflexibility. Gat in <ref> [Gat91] </ref> describes ATLANTIS, an architecture for the control of mobile robots. This architecture has three components: control, sequencing, and deliberation. The control layer is designed as a set of circuit-like functions using Gat's language for circuits, ALPHA. The sequencing is a variation of Jim Firby's RAP system [Fir87].
Reference: [Har90] <author> Stevan Harnad. </author> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42(1-3):335-346, </volume> <year> 1990. </year>
Reference-contexts: we are still struggling with many of the issues involved, we believe we can contribute to a solution for some of the problems for both classical systems and physically grounded systems mentioned in [Bro90], in particular: * The ungroundedness of symbolic systems (referred to as "the symbol grounding problem" by <ref> [Har90] </ref>): our architecture attempts to ground high level symbols in perception and action, through a process of embodiment. * The potential mismatch between symbolic representations and the agent's sensors and actuators: the embodied semantics of our symbols makes sure that this match exists. * Our symbolic representations do not have to <p> The substrate of grounding and embodiment <ref> [Har90, Lak87, Suc88] </ref> of actions, concepts, and reasoning is mainly the Perceptuo-Motor level and to some extent the Sensori-Actuator level. <p> Also at this level are elementary categorial representations; the kinds of representations that function as the grounding for elementary grounded symbols at the Knowledge level, i.e., sensory-invariant representations constructed from sensory data by the perceptual processor <ref> [Har90] </ref>. The representations at this level are embodied (cf. [Lak87]), meaning that they depend on the body of the agent, its particular dimensions and characteristics. <p> From the perspective of cognitive science, the problem has been labeled the symbol grounding problem <ref> [Har90] </ref>. The question is how to make the semantics of a robot's systematically interpretable Knowledge level symbols cohere equally systematically with the robot's interactions with the world, such that the symbols refer to the world on their own, rather than merely because of an external interpretation we place on them. <p> like McDermott, we do not take the Tarskian stance which requires the referents of symbols to be in the world; rather, they are system-internal, similar to what Hausser proposes [Hau89], or what Harnad calls iconic representations: "proximal sensory projections of distal objects, events, and states of affairs in the world" <ref> [Har90] </ref>. The Knowledge level is the only level that is accessible for conscious reasoning, and also the only level that is accessible for inter-agent communication.
Reference: [Har92] <author> Stevan Harnad. </author> <title> Electronic symposium on computation, cognition and the symbol grounding problem. </title> <note> E-mail symposium (ftp archive at princeton.edu:/pub/harnad/sg.comp.arch*), </note> <year> 1992. </year>
Reference-contexts: This requires that the robot be able to discriminate, identify, and manipulate the objects, events, and states of affairs that its symbols refer to <ref> [Har92] </ref>. Grounding is accomplished in our architecture in part through the alignment of the Knowledge and Perceptuo-Motor levels. Elementary symbols at the Knowledge level are grounded in the sense that they only attach to "the right kind" of representations at the Perceptuo-Motor level.
Reference: [Hau89] <author> Roland Hausser. </author> <title> Computation of Language: An Essay on Syntax, Semantics and Pragmatics in Natural Man-Machine Communication. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: Note that, like McDermott, we do not take the Tarskian stance which requires the referents of symbols to be in the world; rather, they are system-internal, similar to what Hausser proposes <ref> [Hau89] </ref>, or what Harnad calls iconic representations: "proximal sensory projections of distal objects, events, and states of affairs in the world" [Har90]. The Knowledge level is the only level that is accessible for conscious reasoning, and also the only level that is accessible for inter-agent communication.
Reference: [HCBS93] <author> Henry Hexmoor, Guido Caicedo, Frank Bidwell, and Stuart Shapiro. </author> <title> Air battle simulation: An agent with conscious and unconscious layers. In UBGCCS-93. </title> <institution> Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The real test for our architecture is its usefulness in applications to physical (robotic) autonomous agents (section 3). Knowledge in GLAIR can migrate from conscious to unconscious levels. In <ref> [HCBS93] </ref> we show how a video-game playing agent learns how to dynamically "compile" a game playing strategy that is initially formulated as explicit reasoning rules at the Knowledge level into an implicit form of knowledge at the Perceptuo-Motor level, a Perceptuo-Motor Automaton (PMA). <p> A PMA is a implementation mechanism for routine activities at an "unconscious" level, <ref> [HN92, HLS92, HCBS93] </ref>. Each task involving a robot motion is subsequently submitted to the path constructor. Some motions may have to be decomposed to visit intermediate points in order to avoid fixtures in the environment. The path constructor generates path segments for each robot motion.
Reference: [Hex89] <author> Henry Hexmoor. </author> <title> An architecture for reactive sensor-based robot. </title> <booktitle> In NASA Goddard conference on AI, </booktitle> <address> Greenbelt, MD, </address> <year> 1989. </year>
Reference-contexts: The deliberation layer is the least described layer. As for situated automata, we are not convinced that this is the right kind of formalism to use, for the same reasons. An architecture for low-level and high-level reactivity is suggested in <ref> [Hex89] </ref>. High-level reactivity is reactivity at the conceptual level. This architecture suggests that an autonomous agent maintains several different types of goals. High-level reactivity is charged with noticing impacts of events and actions in the environment on the agent's goals. Subsequently, high-level reactivity needs to guide the agent's low-level reactivity.
Reference: [Hex92] <author> Henry Hexmoor. </author> <title> Representing and learning successful routine activities. </title> <type> Technical Report Unpublished PhD Proposal, </type> <institution> Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York, </address> <year> 1992. </year>
Reference-contexts: When our confidence in a Routine reaches a certain level, a concept is created at the Knowledge level of GLAIR for the routine and from then on, this routine can be treated as a single action at that level, <ref> [Hex92] </ref>. We plan to explore other learning techniques such as experimentation as a form of learning [She89]. We are also interested in developing experiments that will help in psychological validation of GLAIR and the learning strategies used in ABS.
Reference: [HLS92] <author> Henry Hexmoor, Joe Lammens, and Stuart Shapiro. </author> <title> An autonomous agent architecture for integrating perception and acting with grounded, embodied symbolic reasoning. </title> <type> Technical Report CS-92-21, </type> <institution> Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York, </address> <year> 1992. </year>
Reference-contexts: A PMA is a implementation mechanism for routine activities at an "unconscious" level, <ref> [HN92, HLS92, HCBS93] </ref>. Each task involving a robot motion is subsequently submitted to the path constructor. Some motions may have to be decomposed to visit intermediate points in order to avoid fixtures in the environment. The path constructor generates path segments for each robot motion.
Reference: [HN92] <author> Henry Hexmoor and Donald Nute. </author> <title> Methods for deciding what to do next and learning. </title> <type> Technical Report AI-1992-01, </type> <institution> AI Programs, The University of Georgia, Athens, Georgia, </institution> <year> 1992. </year> <note> Also available from SUNY at Buffalo, CS Department TR-92-23. </note>
Reference-contexts: It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88]. Situated action is used in reactive planning [AC87, Fir87, Sch87]. We have developed an implementation mechanism for the PM-level which we call PM-automata (PMA), <ref> [HN92] </ref>. A PMA is a finite state machine in which each state is associated with an act and arcs are associated with perceptions. In each PMA, a distinguished state is used to correspond to the no-op act. Each state also contains an auxiliary part we call Internal State (IS). <p> This means that in our model the agent is never idle, and it is always executing an act. The primary mode of acquiring PMAs in GLAIR is by converting plans in the Knowledge level into PMAs through a process described in <ref> [HN92] </ref>. A PMA may become active as the result of an intention to execute an action at the Knowledge level. Once a PMA becomes active, sensory perception will be used by the PMA to move along the arcs. <p> A PMA is a implementation mechanism for routine activities at an "unconscious" level, <ref> [HN92, HLS92, HCBS93] </ref>. Each task involving a robot motion is subsequently submitted to the path constructor. Some motions may have to be decomposed to visit intermediate points in order to avoid fixtures in the environment. The path constructor generates path segments for each robot motion.
Reference: [Kae88] <author> Leslie Kaelbling. </author> <title> Goals as parallel program specifications. </title> <booktitle> In Proceedings of AAAI-88. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year> <month> 24 </month>
Reference-contexts: Each reflexive behavior has an associated priority, and a central blackboard style manager arbitrates among the reflex behaviors. Some of the problems with the earlier implementation due to using the blackboard model were solved in [RP89]. Rosenschein and Kaelbling's work <ref> [Kae88, KR90] </ref> describes tools (REX, GAPP, RULER) that, given task descriptions of the world, construct reactive control mechanisms termed situated automata. Their architecture consists of perception and action components. The robot's sensory input and its feedback are inputs to the perception component.
Reference: [KR90] <author> Leslie Kaelbling and Stanley Rosenschein. </author> <title> Action and planning in embedded agents. </title> <editor> In Pattie Maes, editor, </editor> <booktitle> Designing Autonomous Agents, </booktitle> <pages> pages 35-48. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Each reflexive behavior has an associated priority, and a central blackboard style manager arbitrates among the reflex behaviors. Some of the problems with the earlier implementation due to using the blackboard model were solved in [RP89]. Rosenschein and Kaelbling's work <ref> [Kae88, KR90] </ref> describes tools (REX, GAPP, RULER) that, given task descriptions of the world, construct reactive control mechanisms termed situated automata. Their architecture consists of perception and action components. The robot's sensory input and its feedback are inputs to the perception component.
Reference: [Lak87] <author> George Lakoff. Women, </author> <title> Fire, and Dangerous Things: What Categories Reveal about the Mind. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, IL, </address> <year> 1987. </year>
Reference-contexts: The substrate of grounding and embodiment <ref> [Har90, Lak87, Suc88] </ref> of actions, concepts, and reasoning is mainly the Perceptuo-Motor level and to some extent the Sensori-Actuator level. <p> Also at this level are elementary categorial representations; the kinds of representations that function as the grounding for elementary grounded symbols at the Knowledge level, i.e., sensory-invariant representations constructed from sensory data by the perceptual processor [Har90]. The representations at this level are embodied (cf. <ref> [Lak87] </ref>), meaning that they depend on the body of the agent, its particular dimensions and characteristics. Robots will therefore have different representations 10 See [SR87] for our use of "intensional representation". 8 at this level than people would, and different robots will have different representations as well.
Reference: [Lam] <author> Johan M. Lammens. </author> <title> A computational model of color perception and color naming: a case study of symbol grounding for natural language semantics. Dissertation proposal, </title> <institution> SUNY/Buffalo CS department, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: This model provides the perceptual grounding for a set of basic color terms [BK69] represented at the Knowledge level. The color domain was chosen as a case study for embodied perception because of the relative abundance of psychological, psychophysical and neurophysiological data in the literature <ref> [Lam] </ref>. It is a complex enough domain to allow the usefulness of embodiment for computational models of perception to be demonstrated, yet feasible enough to be implemented in actual autonomous agents.
Reference: [Lev88] <author> Hector J. Levesque. </author> <title> Logic and the complexity of reasoning. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 17 </volume> <pages> 355-389, </pages> <year> 1988. </year>
Reference-contexts: (section 3.1). 17 The term "knowledge" should be taken in a very broad sense here. 18 Many reasoning problems are NP-complete, meaning there are no polynomial-time deterministic algorithms known for solving them, or in plain English: they are very hard to solve in a reasonable amount of time (see e.g. <ref> [Lev88] </ref>). Elephants don't stand a chance. 12 3 Applications Our architecture as described in section 2 can be populated with components that make up the machinery for mapping sensory inputs to response actions, as does Russell in [Rus91]. We now discuss some applications of GLAIR that we are currently developing.
Reference: [LHS93] <author> Johan Lammens, Henry Hexmoor, and Stuart Shapiro. </author> <title> Embodiment in glair: a grounded layered architecture with integrated reasoning for autonomous agents. In Proceedings of the Florida AI Research Symposium 93, </title> <note> page to appear, </note> <year> 1993. </year>
Reference-contexts: This results in a set of motor concepts that is a subset of the human one. Embodiment also calls for body-centered and body-measured representations, relative to the agent's own physiology. We provide more details on embodiment in GLAIR in <ref> [LHS93] </ref>. 15 This is of course the "duck test", made famous by a former US president. 16 Retrieving "canned" parameterized routines is still a low-level programming style that we want to avoid. 11 2.5 Alignment When a GLAIR-agent notices something in its environment, it registers that it has come to know
Reference: [LHYT91] <author> J. Laird, M. Huka, E. Yager, and C. Tucker. Robo-soar: </author> <title> An integration of external interaction, planning, and learning, using soar. </title> <booktitle> In Robotics and Autonomous Systems, </booktitle> <year> 1991. </year>
Reference-contexts: SOAR [LNR87] was designed to be a general problem solving architecture. SOAR integrates a type of learning known as chunking in its production system. Recently, SOAR has been applied to robotic tasks 6 <ref> [LHYT91] </ref>. In this framework, planning and acting is uniformly represented and controlled in SOAR. This approach lacks the ability of our architecture for generating behavior at non-conscious levels as well as the conscious level (or at different levels in general), and for having different-level behaviors interact in an asynchronous fashion.
Reference: [LMA91] <author> Pat Langley, Kathleen McKusick, and John Allen. </author> <title> A design for the icarus architecture. </title> <journal> In ACM SIGART Bulletin, </journal> <pages> pages 104-109. </pages> <publisher> ACM publications, </publisher> <year> 1991. </year>
Reference-contexts: (1) having some internal states or representations that are causally connected to the environment through perception, (2) being able to reason explicitly about the environment, and (3) being able to communicate with an external agent about the environment. 3 Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [And83, Pol89, LMA91] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [LNR87] <author> J. E. Laird, A. Newell, and P. S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: We also relax the top-down nature of interaction between levels. Reactivity may be initiated at any level of our architecture either due to interaction with other levels or in direct response to external stimuli. SOAR <ref> [LNR87] </ref> was designed to be a general problem solving architecture. SOAR integrates a type of learning known as chunking in its production system. Recently, SOAR has been applied to robotic tasks 6 [LHYT91]. In this framework, planning and acting is uniformly represented and controlled in SOAR.
Reference: [Mae91] <editor> Pattie Maes. </editor> <title> Action selection. </title> <booktitle> In Proceedings of Cognitive Science Society Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Pattie Maes has experimented with a version of a behavior-based architecture, which she calls ANA <ref> [Mae91] </ref>. This architecture consists of competence modules for action and a belief set in a network relating modules through links denoting successors, predecessors, and conflicts. Competence modules have activation levels. Activations are propagated and the competence module with the highest activation level is given control.
Reference: [McD91] <author> Drew McDermott. </author> <title> Robot planning. </title> <type> Technical Report CS-861, </type> <institution> Yale University, </institution> <year> 1991. </year>
Reference-contexts: Reflex-like processes may also be used to shift the focus of attention of the Knowledge level. 2.3 Symbol Grounding: A Non-Tarskian Semantics Tarskian Semantics has nothing to say about how descriptions of objects in plans relate to the objects in the world <ref> [McD91, p. 13] </ref>. Let's digress for a moment to some esoteric matters of semantics and reference. One problem an agent has to solve is how to find and maintain a correspondence between a referent in the world and a symbol in an agent's world model.
Reference: [MRH86] <author> J. L. McClelland, D. E. Rumelhart, and G. E. Hinton. </author> <title> The appeal of parallel distributed processing. </title> <editor> In David Rumelhart, James McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing, chapter 1, </booktitle> <pages> pages 3-44. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference-contexts: The organization of an architecture may also be influenced by whether or not one adopts the modularity assumption of Fodor [Fod83], or a connectionist point of view, e.g. <ref> [MRH86] </ref>, or an anti-modularity assumption as in Brooks's subsumption architecture [Bro85]. The modularity assumption supports (among other things) a division of the mind into a central system, i.e., cognitive processes such as learning, planning, and reasoning, and a peripheral system, i.e., sensory and motor processing [Cha90].
Reference: [Pay86] <author> David Payton. </author> <title> An architecture for reflexive autonomous vehicle control. </title> <booktitle> In Proceedings of Robotics Automation, </booktitle> <pages> pages 1838-1845. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: Extracted environmental information is compared with the expected internal states to find differences. The differences are used for planning at higher levels. Payton in <ref> [Pay86] </ref> introduced an architecture for controlling an autonomous land vehicle. This architecture has four levels: mission planning, map-based planning, local planning, and reflexive planning. All levels operate in parallel. Higher levels are charged with tasks requiring high assimilation and low immediacy. <p> We use the term tracking to refer to an action that requires continual adjustments, like steering while driving. Examples of this type of reactive behavior are given in <ref> [Pay86, AHC91] </ref>. Situated behavior requires assessment of the state the system finds itself in (in some state space) and acting on the basis of that. It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88].
Reference: [Pol89] <author> John Pollock. </author> <title> How to Build a Person. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: We say these systems are 1 Grounded Layered Architecture with Integrated Reasoning 2 Our discussion of architecture in this paper extends beyond any particular physical or software implementation. 2 not concerned with consciousness. These architectures address what John Pollock calls Quick and Inflexible (Q&I) processes <ref> [Pol89] </ref>. <p> (1) having some internal states or representations that are causally connected to the environment through perception, (2) being able to reason explicitly about the environment, and (3) being able to communicate with an external agent about the environment. 3 Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [And83, Pol89, LMA91] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [Pol92] <author> John Pollock. </author> <title> New foundations for practical reasoning. In Minds and Machines, </title> <year> 1992. </year>
Reference-contexts: Today is Thursday and it is near the end of the business day. Also, the agency won't accept telephone reservations. This example is suggested in <ref> [Pol92] </ref>. 7 In anthropomorphic terms, we identify the Knowledge level with consciously accessible data and process-ing; the Perceptuo-Motor level with "hardwired", not consciously accessible processing and data involved with motor control and perceptual processing; and the Sensori-Actuator level with the lowest-level muscular and sensor control, also not consciously accessible.
Reference: [Rap88] <author> William J. Rapaport. </author> <title> Syntactic semantics: Foundations of computational natural-language understanding. </title> <editor> In James H. Fetzer, editor, </editor> <booktitle> Aspects of Artificial Intelligence, </booktitle> <pages> pages 81-131. </pages> <publisher> Kluwer Academic, </publisher> <address> New York NY, </address> <year> 1988. </year>
Reference-contexts: as the differences are not too large. 14 Indeed, we believe that this is quite realistic in human terms as well; no two persons are likely to have exactly the same semantics for their concepts, which nevertheless does not prevent them from understanding each other, grosso modo at least (cf. <ref> [Rap88] </ref>). The problems of translation and communication in general consist at least in part of establishing a correspondence between concepts (and symbols) used by the participants.
Reference: [RB78] <author> D. Regan and K.I. Beverly. </author> <title> Looming detectors in the human visual pathways. </title> <booktitle> In Vision Research 18, </booktitle> <pages> pages 209-212. </pages> <year> 1978. </year>
Reference-contexts: This causes the PMA to stop executing the act in the state and to start executing the act at the next state 19 E.g., visual reflexes in <ref> [RB78] </ref>: Here responses are generated to certain visual stimuli that do not require detailed spatial analysis. 13 at the end of the arc connecting the two states. This means that in our model the agent is never idle, and it is always executing an act.
Reference: [RP89] <author> J. Kenneth Rosenblatt and David Payton. </author> <title> A fine-grained alternative to the subsumption architecture for mobile robot control. </title> <booktitle> In Proceedings of Internations Joint Conference on Neural Networks, </booktitle> <year> 1989. </year>
Reference-contexts: Each reflexive behavior has an associated priority, and a central blackboard style manager arbitrates among the reflex behaviors. Some of the problems with the earlier implementation due to using the blackboard model were solved in <ref> [RP89] </ref>. Rosenschein and Kaelbling's work [Kae88, KR90] describes tools (REX, GAPP, RULER) that, given task descriptions of the world, construct reactive control mechanisms termed situated automata. Their architecture consists of perception and action components. The robot's sensory input and its feedback are inputs to the perception component.
Reference: [Rus91] <author> Stuart Russell. </author> <title> An architecture for bounded rationality. </title> <journal> In ACM SIGART Bulletin, </journal> <pages> pages 146-150. </pages> <publisher> ACM publications, </publisher> <year> 1991. </year>
Reference-contexts: Elephants don't stand a chance. 12 3 Applications Our architecture as described in section 2 can be populated with components that make up the machinery for mapping sensory inputs to response actions, as does Russell in <ref> [Rus91] </ref>. We now discuss some applications of GLAIR that we are currently developing. Some important general features of GLAIR-agent are the following: * Varieties of behaviors are integrated: We distinguish between deliberative, reactive, and reflexive behaviors.
Reference: [Sch87] <author> Marcel J. Schoppers. </author> <title> Universal plans for unpredictable environments. </title> <booktitle> In Proceedings 10th IJCAI, </booktitle> <pages> pages 1039-1046, </pages> <year> 1987. </year>
Reference-contexts: It might be modeled by the workings of a finite state automaton, for example, the Micronesian behavior described in [Suc88]. Situated action is used in reactive planning <ref> [AC87, Fir87, Sch87] </ref>. We have developed an implementation mechanism for the PM-level which we call PM-automata (PMA), [HN92]. A PMA is a finite state machine in which each state is associated with an act and arcs are associated with perceptions.
Reference: [Sha90] <author> Stuart C. Shapiro. </author> <title> Cables, paths, and `subconscious' reasoning in propositional semantic networks. </title> <booktitle> In Principles of Semantic Networks. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year> <month> 25 </month>
Reference-contexts: Indeed, we believe that the unconscious levels of our architecture (the Perceptuo-Motor level and the Sensori-Actuator level) are at least as important to the architecture as the conscious one (the Knowledge level). We reserve the term sub-conscious for implicit cognitive processes such as category subsumption in KRR systems. See <ref> [Sha90] </ref> for a discussion of sub-conscious reasoning. 6 This autonomy is similar to Brooks's subsumption architecture [Bro85], but at a more macroscopic level.
References-found: 48

