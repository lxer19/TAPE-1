URL: http://www.cs.purdue.edu/homes/tto/ps/Computing95.ps
Refering-URL: http://www.cs.purdue.edu/homes/tto/mypaps.html
Root-URL: http://www.cs.purdue.edu
Title: A LOCALLY OPTIMIZED REORDERING ALGORITHM AND ITS APPLICATION TO A PARALLEL SPARSE LINEAR SYSTEM SOLVER  
Author: K. Gallivan P. C. Hansen Tz. Ostromsky Z. Zlatev 
Keyword: sparse matrix, general sparsity, Gaussian elimination, drop-tolerance, reordering, binary tree, block algorithm, coarse-grain parallelism, speed-up.  
Address: Urbana, Illinois, USA  Denmark, Bldg. 305, DK-2800 Lyngby, Denmark  Frederiksborgvej 399, DK-4000 Roskilde, Denmark  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  UNI*C, The Danish Computer Centre for Research and Education, The Technical University of  National Environmental Research Institute  
Note: AMS Subject Classifications:  
Email: e-mail: gallivan@csrd.uiuc.edu  e-mail: Per.Christian.Hansen@uni-c.dk e-mail: Tzvetan.Ostromsky@uni-c.dk  e-mail: luzz@sun2.dmu.min.dk  
Web: 65F05, 65Y05  
Abstract: A coarse-grain parallel solver for systems of linear algebraic equations with general sparse matrices by Gaussian elimination is discussed. Before the factorization two other steps are performed. A reordering algorithm is used during the first step in order to obtain a permuted matrix with as many zero elements under the main diagonal as possible. During the second step the reordered matrix is partitioned into blocks for asynchronous parallel processing (normally the number of blocks is equal to the number of processors). It is possible to obtain blocks with nearly the same number of rows, because there is no requirement to produce square diagonal blocks. The first step is much more important than the second one and has a big influence on the performance of the solver. A straightforward implementation of the reordering algorithm will result in O(n 2 ) operations. By using binary trees this cost can be reduced to O(N Z log n), where N Z is the number of non-zero elements in the matrix and n is its order (normally N Z is much smaller than n 2 ). Some experiments on parallel computers with shared memory have been performed. The results show that a solver based on the proposed reordering performs better than another solver based on a cheaper (but at the same time rather crude) old reordering whose cost is only O(N Z) operations. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. V. Aho, J. E. Hopcroft and J. D. Ullman: </author> <title> "The design and analysis of computer algorithms". </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Ma., USA, </address> <year> 1976. </year>
Reference: 2. <author> A. V. Aho, J. E. Hopcroft and J. D. Ullman: </author> <title> "Data structures and algorithms". </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Ma., USA, </address> <year> 1983. </year>
Reference: 3. <author> F. L. Alvarado, A. Pothen and R. Schreiber: </author> <title> "Highly parallel sparse triangular solution". </title> <type> Report No. </type> <institution> CS-92-09, Department of Computer Science, The Pennsylvania State University, University Park, </institution> <address> PA 16802, USA, </address> <year> 1992. </year>
Reference-contexts: The performance during the solution parts of both the old and the new algorithms may perhaps be improved by applying the algorithm proposed in Alvarado et al., <ref> [3] </ref>. The total time for the new algorithm is much better than the total time for the old one, while the speed-up for the old one is slightly better.
Reference: 4. <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov and D. Sorensen: </author> <title> "LAPACK: Users' Guide" SIAM (Society for Industrial and Applied Mathematics, </title> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Another package of subroutines for dense matrices, developed especially for ALLIANT FX/80, is also available; it gives results that are similar to those obtained by package PARALIN. Finally, an option in which the new LAPACK subroutines (see Anderson et al., <ref> [4] </ref>) are called, can be useful on all sites where package LAPACK is available. It should be mentioned here that LAPACK is already implemented on many sites and the process of implementing this package on more and more sites is continuing. 4.7.
Reference: 5. <author> E. Anderson and Y. Saad: </author> <title> "Preconditioned conjugate gradient methods for general sparse matrices on shared memory machines". In: "PARALLEL PROCESSING 26 FOR SCIENTIFIC COMPUTING" (G. Rodrigue, </title> <publisher> ed.), </publisher> <pages> pp. 88-92. </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: Therefore the factors L d , U s are reordered by the algorithm proposed by Anderson and Saad <ref> [5] </ref>; see more details about the parallel back substitution in Zlatev [27]. 4.8. <p> Now the sparse solution process is much better parallelizable than the dense solution process. This is due to the efficient Anderson-Saad algorithm used in the back substitution of the sparse part; see Anderson and Saad <ref> [5] </ref> or Zlatev [27]. The performance during the solution parts of both the old and the new algorithms may perhaps be improved by applying the algorithm proposed in Alvarado et al., [3].
Reference: 6. <author> M. Arioli, I. S. Duff, N. I. M. Gould and J. K. Reid: </author> <title> "Use of the P 4 and P 5 algorithms for in-core factorization of sparse matrices". </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 11(1990), </volume> <pages> 913-927. </pages>
Reference-contexts: An efficient reordering has been proposed by Hellerman and Rarick [21,22]. It has been used, with some modifications, by many other authors; see, for example Arioli et al., <ref> [6] </ref> or Erisman et al., [11]. Other preliminary reorderings have also been proposed in the literature; see, for example, Gallivan et al., [13]. A common feature of all these reorderings is that one 2 always imposes a requirement to obtain square blocks on the main diagonal.
Reference: 7. <author> T. A. Davis and P.-C. Yew: </author> <title> "A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization". </title> <journal> SIAM J. Matrix. Anal. Appl., </journal> <volume> 3(1990), </volume> <pages> 383-402. </pages>
Reference-contexts: Therefore it is not a big surprise that there are several efficient codes for such computers (in the small set of available codes); see, for example, Davis and Yew <ref> [7] </ref>, Gallivan et al., [17], Gilbert [20] and Zlatev [27]. The situation becomes much more difficult when parallel machines with distributed memory are to be used. The only well-described and efficient code for this class of parallel computers is the code prepared by Van der Stappen et al., [24].
Reference: 8. <author> I. S. Duff, A. M. Erisman and J. K. Reid: </author> <title> "Direct methods for sparse matrices". </title> <publisher> Oxford University Press, </publisher> <address> Oxford-London, </address> <year> 1986. </year>
Reference-contexts: Moreover, it is also required that the reordered matrix B is either an upper block-triangular matrix or a bordered matrix; in both cases with square blocks on the main diagonal (see the references given above or Duff et al., <ref> [8] </ref>). For some matrices these two requirements are too restrictive. Therefore these requirements should not always be imposed. The main purpose of this paper is to show how to avoid them (when this is appropriate).
Reference: 9. <author> I. S. Duff, R. G. Grimes and J. C. Lewis: </author> <title> "Sparse matrix test problems". </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15(1989), </volume> <pages> 1-14. </pages>
Reference-contexts: The iterative method that is actually used is a modified preconditioned orthomin algorithm (see Section 4.8). 18 6 NUMERICAL RESULTS General sparse matrices from the well-known Harwell-Boeing set of sparse test matrices have mostly been used in the experiments (see Duff et al., <ref> [9] </ref>). We selected a set containing all unsymmetric square matrices whose order is greater than 900. There are 25 such matrices. Two smaller matrices were added to the set: the first of them contains many non-zero elements, the second one arises from an air pollution model.
Reference: 10. <author> S. C. Eisenstat, H. C. Elman and M. H. Schultz: </author> <title> "Variational methods for nonsymmetric systems of linear equations". </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20(1983), </volume> <pages> 345-357. </pages>
Reference-contexts: Different iterative methods can successfully be applied in Step 8. A modified preconditioned orthomin algorithm has been chosen. The original orthomin is proposed by Vinsome [25], a good description of the original orthomin method could be found, for example, in Eisenstat et al., <ref> [10] </ref>). A prescribed number of Krylov vectors (say, k; very often k = 1 is used in the codes) is to be given before the beginning of the calculations when the original orthomin is to be applied.
Reference: 11. <author> A. M. Erisman, R. G. Grimes, J. G. Lewis and G. W. Poole, Jr.: </author> <title> "A structurally stable modification of Hellerman-Rarick's P 4 algorithm for reordering unsymmetric sparse matrices". </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 22(1985), </volume> <pages> 369-385. </pages>
Reference-contexts: An efficient reordering has been proposed by Hellerman and Rarick [21,22]. It has been used, with some modifications, by many other authors; see, for example Arioli et al., [6] or Erisman et al., <ref> [11] </ref>. Other preliminary reorderings have also been proposed in the literature; see, for example, Gallivan et al., [13]. A common feature of all these reorderings is that one 2 always imposes a requirement to obtain square blocks on the main diagonal.
Reference: 12. <author> K. A. Gallivan, W. Jalby and U. Meier: </author> <title> "The use of BLAS3 in linear algebra on a parallel processor with hierarchical memory". </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8(1987), </volume> <pages> 1079-1084. </pages>
Reference: 13. <author> K. A. Gallivan, B. Marsolf and H. Wijshoff: </author> <title> "A large-grain parallel sparse system solver". </title> <booktitle> In: "PROCEEDINGS OF THE SIAM CONFERENCE ON PARALLEL PROCESSING FOR SCIENTIFIC COMPUTING", </booktitle> <pages> pp. 23-28. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: It has been used, with some modifications, by many other authors; see, for example Arioli et al., [6] or Erisman et al., [11]. Other preliminary reorderings have also been proposed in the literature; see, for example, Gallivan et al., <ref> [13] </ref>. A common feature of all these reorderings is that one 2 always imposes a requirement to obtain square blocks on the main diagonal.
Reference: 14. <author> K. A. Gallivan, R. J. Plemmons and A. H. Sameh: </author> <title> "Parallel algorithms for dense linear algebra computations". </title> <journal> SIAM Rev., </journal> <volume> 32(1990), </volume> <pages> 54-135. </pages>
Reference-contexts: In the last section, Section 6, conclusions are drawn and plans for future work are presented. 2 THE INITIAL REORDERING As stated in the previous section our main purpose is to improve the performance of the solver for systems of linear algebraic equations proposed in Gallivan et al. <ref> [14] </ref> and Zlatev [27]. It is convenient first to sketch the initial reordering scheme used in the old solver. It consists of two completely separated steps: column reordering and row reordering. The following definition is needed in order to describe the column reordering. 3 Definition 1.
Reference: 15. <author> K. A. Gallivan, A. H. Sameh and Z. Zlatev: </author> <title> "Solving general sparse linear systems using conjugate gradient-type methods". </title> <booktitle> In: "PROCEEDINGS OF THE 1990 INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, </booktitle> <address> June 11-15 1990, Amsterdam, The Netherlands". </address> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: 16. <author> K. A. Gallivan, A. H. Sameh and Z. Zlatev: </author> <title> "A parallel hybrid sparse linear system solver". </title> <journal> Computing Systems in Engineering, </journal> <volume> 1(1990), </volume> <pages> 183-195. </pages>
Reference: 17. <author> K. A. Gallivan, A. H. Sameh and Z. Zlatev: </author> <title> "Parallel direct methods for general sparse matrices". Preprint No.9. "NATO ASI ON COMP. ALG. FOR SOLVING LINEAR EQUATIONS: THE STATE OF THE ART, </title> <institution> University of Bergamo, Italy, </institution> <year> 1990. </year>
Reference-contexts: Therefore it is not a big surprise that there are several efficient codes for such computers (in the small set of available codes); see, for example, Davis and Yew [7], Gallivan et al., <ref> [17] </ref>, Gilbert [20] and Zlatev [27]. The situation becomes much more difficult when parallel machines with distributed memory are to be used. The only well-described and efficient code for this class of parallel computers is the code prepared by Van der Stappen et al., [24]. <p> Therefore these requirements should not always be imposed. The main purpose of this paper is to show how to avoid them (when this is appropriate). A direct solver, where one attempts to exploit coarse-grain parallelism without imposing the above two requirements is described and tested in Gallivan et al., <ref> [17] </ref> and Zlatev [27]. This solver is based on partitioning the matrix into an upper block-triangular form with rectangular diagonal blocks. A reordering algorithm, by which as many as possible zero elements are obtained in the lower left corner of the matrix, is to be applied before the partitioning. <p> This is why we concentrate our attention on the initial reordering. An improvement of the reordering algorithm proposed in Gallivan et al., <ref> [17] </ref> and Zlatev [27], and its application in the solution of systems of linear algebraic equations by Gaussian elimination is discussed in this paper. This algorithm is efficiently implemented by using special graphs: binary trees with leveled nodes (see Section 3). <p> The solution process (which is based on Gaussian elimination) will be sketched in this section. The algorithm used, Y12M3, consists of eight steps. A simpler version of this algorithm is discussed in Gallivan et al. <ref> [17] </ref> and Zlatev [27], where Y12M3 was used as a direct solver. <p> Step 2 | Partition the matrix The partitioning algorithm is based on the following idea. The matrix is divided into p parts, each of them containing a certain number of rows. In all experiments that have been performed in Gallivan et al., <ref> [17] </ref> and in Zlatev [27] as well as in the experiments that will be discussed in the next section, p is set to be equal to eight (the maximal number of processors of the ALLIANT FX/80 computer). <p> Each processor produces zeros under the main diagonal of the diagonal block in its part of the matrix. This is a straightforward operation and may be carried out by calling a slightly modified version of one of the subroutines described in Gallivan et al., <ref> [17] </ref> and Zlatev [27]. Assume that the partitioning is performed with p = 4. The blocks obtained by such a partitioning can be depicted as in Fig. 5. A 11 A 12 A 13 A 14 0 0 A 33 A 34 Partitioning with p = 4. <p> As mentioned above, this is done by dense matrix subroutines. The portable 16 subroutines that are used in this step are the same as the subroutines that are used in two other packages, Y12M1 and Y12M2; see Gallivan et al., <ref> [17] </ref> and Zlatev [27]. There are several other options that could be activated if the packages used in them are available at the site where Y12M3 is to be run. On the ALLIANT FX/80 computer it is worthwhile to use the subroutines from package PARALIN (see again Gallivan et al., [17] <p> <ref> [17] </ref> and Zlatev [27]. There are several other options that could be activated if the packages used in them are available at the site where Y12M3 is to be run. On the ALLIANT FX/80 computer it is worthwhile to use the subroutines from package PARALIN (see again Gallivan et al., [17] and Zlatev [27]. Another package of subroutines for dense matrices, developed especially for ALLIANT FX/80, is also available; it gives results that are similar to those obtained by package PARALIN.
Reference: 18. <author> J. A. George and J. W. Liu: </author> <title> "Computer solution of large sparse positive definite systems". </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1981. </year>
Reference-contexts: These graphs gives us a possibility to represent readily and to modify cheaply certain partial orderings of the columns of a general sparse matrix. Different kinds of graphs have often been used in sparse matrix studies; especially in connection with symmetric and positive definite matrices (see, George and Liu <ref> [18] </ref>). The application of concepts from graph theory in connection with general matrices is not so popular, but also in this field several applications exist. These are based on bipartite graphs; see George et al., [19]). The application proposed in this paper is different from the other applications.
Reference: 19. <author> J. A. George,J. W. Liu and E. Ng: </author> <title> "Row ordering schemes for sparse Givens rotations". </title> <journal> Lin. Alg. Appl., </journal> <volume> 61(1984), </volume> <pages> 55-81. 27 </pages>
Reference-contexts: The application of concepts from graph theory in connection with general matrices is not so popular, but also in this field several applications exist. These are based on bipartite graphs; see George et al., <ref> [19] </ref>). The application proposed in this paper is different from the other applications. The new initial reordering algorithm is introduced in Section 2.
Reference: 20. <author> J. R. Gilbert: </author> <title> "An efficient parallel sparse partial pivoting algorithm". </title> <type> Report No. 88/45052-1. Chr. </type> <institution> Michelsen Institute, Department of Science and Technology, Centre for Computer Science, Fantoftvegen 38, N-5036 Fantoft, Bergen, Norway, </institution> <year> 1988. </year>
Reference-contexts: Therefore it is not a big surprise that there are several efficient codes for such computers (in the small set of available codes); see, for example, Davis and Yew [7], Gallivan et al., [17], Gilbert <ref> [20] </ref> and Zlatev [27]. The situation becomes much more difficult when parallel machines with distributed memory are to be used. The only well-described and efficient code for this class of parallel computers is the code prepared by Van der Stappen et al., [24].
Reference: 21. <author> E. Hellerman and D. C. Rarick: </author> <title> "Reinversion with the preassigned pivot procedure". </title> <journal> Programming, </journal> <volume> 1(1971), </volume> <pages> 195-216. </pages>
Reference: 22. <author> E. Hellerman and D. C. Rarick: </author> <title> "The partitioned preassigned pivot procedure (P 4 )". In: "SPARSE MATRICES AND THEIR APPLICATIONS" (D. </title> <editor> J. Rose and R. A. Willoughby, </editor> <booktitle> eds.), </booktitle> <pages> pp. 67-76. </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: 23. <author> D. Knuth: </author> <booktitle> "The art of computer programming". </booktitle> <volume> Vol. 3, </volume> <pages> pp. 151-152. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1973. </year>
Reference-contexts: The binary tree with leveled nodes could be considered as a modification of the leftist tree discussed in Knuth <ref> [23] </ref>. Some modifications of the tree are often to be carried out (many times, consecutively) during the solution of a given problem. This will be needed, for example, if LORA is implemented by using binary trees. <p> LEV (j) | the minimal distance between the j-th node and a descendant of it with less than two children; LEV (j) = 0 if the j-th node has less than two children. The arrays LEF T; RIGHT and KEY are borrowed from Knuth's definition of leftist tree <ref> [23] </ref>. LEV in our case corresponds to DIST 1 (Knuth [23]), while P REV is an additional array necessary for our purposes. Now we can introduce the class of binary trees with leveled nodes, which will be applied in the implementation of LORA. Definition 5. <p> The arrays LEF T; RIGHT and KEY are borrowed from Knuth's definition of leftist tree <ref> [23] </ref>. LEV in our case corresponds to DIST 1 (Knuth [23]), while P REV is an additional array necessary for our purposes. Now we can introduce the class of binary trees with leveled nodes, which will be applied in the implementation of LORA. Definition 5. <p> The relations stated by Def. 5 can easily be checked by using the information about KEY and LEV of each node, which is also given in Fig. 4. The binary trees with leveled nodes preserve the main properties of the leftist trees, which are described in Knuth <ref> [23] </ref>. Property 1. The root is always a node with minimal KEY , i.e. (3) KEY (root) KEY (j) ; 8j 2 V : Property 2.
Reference: 24. <author> A. F. van der Stappen, R. H. Bisseling and J. G. G. van der Vorst: </author> <title> "Parallel sparse LU decomposition on a mesh network of transputers". </title> <note> SIAM J. Matrix Anal. Appl., 14(1993), to apear. </note>
Reference-contexts: The situation becomes much more difficult when parallel machines with distributed memory are to be used. The only well-described and efficient code for this class of parallel computers is the code prepared by Van der Stappen et al., <ref> [24] </ref>. It should be reiterated here that the matrices are assumed to be general. This means that there is neither an assumption that the matrix under consideration has some special property (such as symmetry and/or positive definiteness) nor some special structure (such as bandedness).
Reference: 25. <author> P. K. W. Vinsome: "Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations". </title> <booktitle> In: "PROCEEDINGS OF THE FOURTH SYMPOSIUM ON RESERVOIR SIMULATION", </booktitle> <pages> pp. 140-159. </pages> <booktitle> Society of Petroleum Engineers of AIME, </booktitle> <year> 1976. </year>
Reference-contexts: Different iterative methods can successfully be applied in Step 8. A modified preconditioned orthomin algorithm has been chosen. The original orthomin is proposed by Vinsome <ref> [25] </ref>, a good description of the original orthomin method could be found, for example, in Eisenstat et al., [10]).
Reference: 26. <author> Z. Zlatev: </author> <title> "Use of iterative refinement in the solution of sparse linear systems". </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19(1982), </volume> <pages> 381-399. </pages>
Reference: 27. <author> Z. Zlatev: </author> <title> "Computational methods for general sparse matrices". </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht-Toronto-London, </address> <year> 1991. </year>
Reference-contexts: The amount of work needed to modify a target row will in general vary from one row to another, because the rows contain different numbers of non-zero elements. The short discussion given above (more details can be found in Zlatev <ref> [27] </ref>) explains why there is only a limited number of parallel codes for general sparse matrices. The solution of the task (development of efficient methods for parallel computers) is considerably easier for parallel machines with shared memory than for distributed-memory machines. <p> Therefore it is not a big surprise that there are several efficient codes for such computers (in the small set of available codes); see, for example, Davis and Yew [7], Gallivan et al., [17], Gilbert [20] and Zlatev <ref> [27] </ref>. The situation becomes much more difficult when parallel machines with distributed memory are to be used. The only well-described and efficient code for this class of parallel computers is the code prepared by Van der Stappen et al., [24]. <p> The main purpose of this paper is to show how to avoid them (when this is appropriate). A direct solver, where one attempts to exploit coarse-grain parallelism without imposing the above two requirements is described and tested in Gallivan et al., [17] and Zlatev <ref> [27] </ref>. This solver is based on partitioning the matrix into an upper block-triangular form with rectangular diagonal blocks. A reordering algorithm, by which as many as possible zero elements are obtained in the lower left corner of the matrix, is to be applied before the partitioning. <p> This is why we concentrate our attention on the initial reordering. An improvement of the reordering algorithm proposed in Gallivan et al., [17] and Zlatev <ref> [27] </ref>, and its application in the solution of systems of linear algebraic equations by Gaussian elimination is discussed in this paper. This algorithm is efficiently implemented by using special graphs: binary trees with leveled nodes (see Section 3). <p> the last section, Section 6, conclusions are drawn and plans for future work are presented. 2 THE INITIAL REORDERING As stated in the previous section our main purpose is to improve the performance of the solver for systems of linear algebraic equations proposed in Gallivan et al. [14] and Zlatev <ref> [27] </ref>. It is convenient first to sketch the initial reordering scheme used in the old solver. It consists of two completely separated steps: column reordering and row reordering. The following definition is needed in order to describe the column reordering. 3 Definition 1. <p> The solution process (which is based on Gaussian elimination) will be sketched in this section. The algorithm used, Y12M3, consists of eight steps. A simpler version of this algorithm is discussed in Gallivan et al. [17] and Zlatev <ref> [27] </ref>, where Y12M3 was used as a direct solver. <p> Step 2 | Partition the matrix The partitioning algorithm is based on the following idea. The matrix is divided into p parts, each of them containing a certain number of rows. In all experiments that have been performed in Gallivan et al., [17] and in Zlatev <ref> [27] </ref> as well as in the experiments that will be discussed in the next section, p is set to be equal to eight (the maximal number of processors of the ALLIANT FX/80 computer). <p> Each processor produces zeros under the main diagonal of the diagonal block in its part of the matrix. This is a straightforward operation and may be carried out by calling a slightly modified version of one of the subroutines described in Gallivan et al., [17] and Zlatev <ref> [27] </ref>. Assume that the partitioning is performed with p = 4. The blocks obtained by such a partitioning can be depicted as in Fig. 5. A 11 A 12 A 13 A 14 0 0 A 33 A 34 Partitioning with p = 4. <p> As mentioned above, this is done by dense matrix subroutines. The portable 16 subroutines that are used in this step are the same as the subroutines that are used in two other packages, Y12M1 and Y12M2; see Gallivan et al., [17] and Zlatev <ref> [27] </ref>. There are several other options that could be activated if the packages used in them are available at the site where Y12M3 is to be run. On the ALLIANT FX/80 computer it is worthwhile to use the subroutines from package PARALIN (see again Gallivan et al., [17] and Zlatev [27]. <p> <ref> [27] </ref>. There are several other options that could be activated if the packages used in them are available at the site where Y12M3 is to be run. On the ALLIANT FX/80 computer it is worthwhile to use the subroutines from package PARALIN (see again Gallivan et al., [17] and Zlatev [27]. Another package of subroutines for dense matrices, developed especially for ALLIANT FX/80, is also available; it gives results that are similar to those obtained by package PARALIN. <p> Therefore the factors L d , U s are reordered by the algorithm proposed by Anderson and Saad [5]; see more details about the parallel back substitution in Zlatev <ref> [27] </ref>. 4.8. Step 8 | Improve the first solution by a modified preconditioned orthomin algorithm In principle, this is an optional step, but it is strongly recommended (because of stability problems; see below) to carry out this step, and this is the reason for including it in the algorithm. <p> The code tries adaptively, during the iterations, to find a good (for the particular matrix treated) number of Krylov vectors, when the modified orthomin algorithm is used. The modified preconditioned orthomin algorithm is described in details in Zlatev <ref> [27] </ref> Chapter 11. 17 5 STABILITY CONSIDERATIONS Stability problems may arise when Y12M3 is used. The stability problems will be discussed below. It is assumed that the new reordering algorithm LORA is used, however the same ideas can be applied to the old algorithm. <p> Two smaller matrices were added to the set: the first of them contains many non-zero elements, the second one arises from an air pollution model. It should be noted that the same set of test matrices has been used in Gallivan et al., [14-17] and Zlatev <ref> [27] </ref>. Some information about the matrices used is given in Table 1. No. <p> The code attempts to estimate the second two-norm of the error vector (Zlatev <ref> [27] </ref>) and stops the computation when this estimation becomes less than ACCU R = 10 5 . If this condition cannot be satisfied then, as mentioned above, the drop-tolerance is reduced and a new trial is started. <p> The factors so calculated L and U are normally inaccurate and, therefore, they are used as preconditioners in some iterative method. The particular method used was a modified preconditioned ORTHMIN algorithm (see Zlatev <ref> [27] </ref>. If the preconditioned iterative method does not converge, then the drop-tolerance is reduced and new preconditioners are calculated. This procedure could be repeated several times. <p> Now the sparse solution process is much better parallelizable than the dense solution process. This is due to the efficient Anderson-Saad algorithm used in the back substitution of the sparse part; see Anderson and Saad [5] or Zlatev <ref> [27] </ref>. The performance during the solution parts of both the old and the new algorithms may perhaps be improved by applying the algorithm proposed in Alvarado et al., [3]. <p> 47 A 48 A 67 A 68 Partitioning of the matrix saylr4 into eight parts after reordering by the old algorithm (above) and by the new algorithm, LORA (below). 24 Some runs with very large matrices (created by one of the generators for general sparse matrices, CLASSF2, described in Zlatev <ref> [27] </ref>) have also been carried out. Numerical results are given in Table 4. It is seen that the speed-up increases with increasing the order of the matrix. It is also seen that the accuracy requirements (which are the same as for the Harwell-Boeing matrices) are satisfied. <p> 8 pr. speed-up error estimation exact error 10000 50110 49.26 15.15 3.25 4.E-07 3.E-08 50000 250110 868.13 189.65 4.53 3.E-07 2.E-08 200000 1000110 12876.13 2522.35 5.10 2.E-07 8.E-09 Table 4 Results obtained by running LORA on systems created by using one of the generators for general sparse matrices from Zlatev <ref> [27] </ref>. 7 CONCLUDING REMARKS AND PLANS FOR FUTURE WORK The main conclusion from all experiments that have been carried out (not only the experiments discussed in the previous section) is that the new algorithm performs better than the old one (although the time needed to perform the preliminary ordering by the <p> There are several other parts of the code that could be improved. As an illustration only, it should be mentioned that the strategy for switching to dense matrix technique can be improved. Some ideas used in the codes described in Zlatev <ref> [27] </ref> can be applied in the efforts to find out when it is best to switch to dense matrix technique. Some work in this direction has already been started.
Reference: 28. <author> Z. Zlatev, Ph. Vu, J. Wasniewski and K. Schaumburg: </author> <title> "Condition number estimators in a sparse matrix software". </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7(1986), </volume> <pages> 1175-1186. </pages>
Reference-contexts: 3750 5.5E+ 3 23 jpwh 991 991 6027 2.0E+ 2 24 west0989 989 3518 4.2E+ 7 25 pde 9511 961 4681 1.4E+ 7 27 steam2 600 5660 3.2E+ 0 Table 1 Matrices used in the experiments (CON D is a condition number estimation calculated by Y12M; see Zlatev et al. <ref> [28] </ref>) 19 The right-hand side vectors b of the systems Ax = b were created so that all com-ponents of the solution vectors are equal to 1.
References-found: 28

