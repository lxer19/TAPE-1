URL: http://www.cs.wustl.edu/cs/techreports/1995/wucs-95-03.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: ARAS: Asynchronous RISC Architecture Simulator  
Author: Chia-Hsing Chien, Mark A. Franklin, Tienyo Pan and Prithvi Prabhu 
Address: St. Louis, Missouri, 63130-4899 U.S.A.  
Affiliation: Computer and Communications Research Center Washington University  
Abstract: In this paper, an asynchronous pipeline instruction simulator, ARAS is presented. With this simulator, one can design selected instruction pipelines and check their performance. Performance measurements of the pipeline configuration are obtained by simulating the execution of benchmark programs on the machine architectures developed. Depending on the simulation results obtained by using ARAS, the pipeline configuration can be altered to improve its performance. Thus, one can explore the design space of asynchronous pipeline architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C-H Chien and P. Prabhu. ARAS: </author> <title> Asynchronous RISC Architecture Simulator. </title> <type> Technical Report WUCCRC-94-18, </type> <institution> Washington Univ., </institution> <address> St. Louis, MO, </address> <year> 1994. </year>
Reference-contexts: For example, pipeline performance is generally improved if each stage takes roughly equal time. Thus, the designer can examine the effect of moving various micro-operations between blocks (done by editing a configuration data file), rerunning ARAS, and comparing the various results. Technical report <ref> [1] </ref> provides procedures to be followed for altering the pipeline configuration data files. 3 Basic ARAS Operation 3.1 Discrete-event simulation The core of ARAS is a standard trace-driven discrete-event simulator. After ARAS has accessed the configuration file for a particular pipeline, a block table is created. <p> Data and stack segments are also created for ARAS. Standard benchmark traces are being developed for users interested in experimenting with various pipeline designs. For users interested in developing their own traces, details on how to use the interpreter programs are described in technical report <ref> [1] </ref>. 4 Pipeline Design Constraints There are some basic constraints on the use of blocks in the design of instruction pipelines. This section discusses these constraints, an associated set of rules, and the way ARAS handles them. The ARAS instruction set is derived from the SPARC instruction set.
Reference: [2] <editor> W.A. Clark. </editor> <booktitle> Macromodular Computer Syhstems. In Proc. AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 489-504, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: The performance advantages associated with asynchronous systems are discussed in more detail in [3]. While the use of asynchronous modules in the design of processors goes back to the late 1960s with work at Washing-ton University (St. Louis) <ref> [2] </ref>, it wasn't until 1988 that an entire asynchronous microprocessor was designed at the California Institute of Technology [10]. Later an asynchronous version of the ARM processor, AMULET1, was completed at University of Manch-ester (UK) [5].
Reference: [3] <author> M.A. Franklin and T. Pan. </author> <title> Clocked and Asynchronous Instruction Pipelines. </title> <booktitle> In Proc. 26th ACM/IEEE Symp. on Microarchitecture, </booktitle> <pages> pages 177-184, </pages> <address> Austin, TX, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Among these are having performance governed by mean versus worst case function delays, eliminating limitations associated with clock skew (although introducing other limitations), and having potentially lower power levels. The performance advantages associated with asynchronous systems are discussed in more detail in <ref> [3] </ref>. While the use of asynchronous modules in the design of processors goes back to the late 1960s with work at Washing-ton University (St. Louis) [2], it wasn't until 1988 that an entire asynchronous microprocessor was designed at the California Institute of Technology [10]. <p> In addition, with longer pipelines, the role of hazards increases since both the probabilities of a hazard and the pipeline stall penalties associated with a hazard increase. The combination of these factors results in a lower throughput <ref> [3] </ref>. Another approach to increasing performance, in addition to dividing the blocks, is to alter the pipeline configuration to take advantage of instruction level parallelism. An example of this is shown in the 4SP/PF case of Figure 6 which illustrates a simple superscalar configuration.
Reference: [4] <author> M.A. Franklin and T. Pan. </author> <title> Performance Comparison of Asynchronous Adders. </title> <booktitle> In Proc. Symp. on Advanced Research in Asynchronous Circuits and Systems, </booktitle> <address> Salt Lake City, Utah, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Another module which is simulated in greater detail is the 32-bit 2's complement integer adder. Three different adder designs are available: asynchronous Ripple-Carry Adder (RCA), asynchronous carry-SELect adder (SEL), and Conditional-Sum Adder (CSA). The structures of these adders are described in <ref> [4] </ref>. Users can observe the change in performance resulting from the use of different adders in a particular configuration. The simulation results for previous configurations using different adders are shown in Table 6.
Reference: [5] <editor> S.B. Furber, P. Day, J.D. Garside, N.C. Paver, and J.V. Woods. A Micropipelined ARM. </editor> <booktitle> In Int'l Conf. on Very Large Scale Integration (VLSI'93), </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Louis) [2], it wasn't until 1988 that an entire asynchronous microprocessor was designed at the California Institute of Technology [10]. Later an asynchronous version of the ARM processor, AMULET1, was completed at University of Manch-ester (UK) <ref> [5] </ref>. Currently, SUN Microsystems is developing an asynchronous microprocessor called Counterflow Pipeline Processor (CFPP) [11] and it appears that several other institutions and companies are also investigating the design of asynchronous machines [8]. with several buffers between each stage.
Reference: [6] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: Early RISC machines often employed a single, 4 or 5-stage pipeline <ref> [9, 6] </ref>. In more advanced machines, performance has been improved by increasing the pipeline depth (i.e., superpipelined) and by employing multiple pipelines (i.e., superscalar). <p> This section discusses these constraints, an associated set of rules, and the way ARAS handles them. The ARAS instruction set is derived from the SPARC instruction set. Each instruction can be divided into a number of micro-operations (initially based on DLX micro-operations <ref> [6] </ref>), some of which may be common for all or a group of instructions. Prior to defining pipeline blocks, the instruction state diagram which indicates sequencing of micro-operations must be constructed. A simple example is shown in Figure 4 where certain ALU and Jump instructions are illustrated [6]. <p> on DLX micro-operations <ref> [6] </ref>), some of which may be common for all or a group of instructions. Prior to defining pipeline blocks, the instruction state diagram which indicates sequencing of micro-operations must be constructed. A simple example is shown in Figure 4 where certain ALU and Jump instructions are illustrated [6]. In general, a pipeline block may consist of a single micro-operation, or a group of sequential micro-operations as defined in the state diagram. <p> The third rule is enforced by the handshaking protocol inherent in the asynchronous design. That is, an instruction requesting resources (other blocks) that are busy will be automatically stalled in the current block until an acknowledgement is received. ARAS handles data hazards by using a standard scoreboarding technique <ref> [6] </ref>. This includes checking the register reservation table at the start of the operand fetch micro-operation to ensure that it has not been reserved by a prior instruction, and stalling at this block if the register has been reserved.
Reference: [7] <author> K. Hwang. </author> <title> Computer Arithmetic: </title> <booktitle> Principles, Architecture, and Design. </booktitle> <publisher> Wiley, </publisher> <year> 1979. </year>
Reference-contexts: The simulation results for previous configurations using different adders are shown in Table 6. Since the CSA has a tree structure and takes a constant amount of time (O (log 2 n)) to complete addition, it is considered to be one of the fastest adders (for clocked design) <ref> [7, 12] </ref> and achieves the highest throughput for all configurations. However, the complexity of the CSA circuit is higher than that of most other adder circuits. Of the remaining adders, the use of the SEL (8) results in the best throughput.
Reference: [8] <editor> IEEE Computer Society. </editor> <booktitle> Proceedings of Int'l Symposium on Advanced Research in Asynchronous Circuits and Systems, </booktitle> <address> Salt Lake City, UT, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Later an asynchronous version of the ARM processor, AMULET1, was completed at University of Manch-ester (UK) [5]. Currently, SUN Microsystems is developing an asynchronous microprocessor called Counterflow Pipeline Processor (CFPP) [11] and it appears that several other institutions and companies are also investigating the design of asynchronous machines <ref> [8] </ref>. with several buffers between each stage.
Reference: [9] <author> N.P. Jouppi and D.W. Wall. </author> <title> Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines. </title> <booktitle> In ASPLOS-III Proceedings, </booktitle> <pages> pages 272-282, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Early RISC machines often employed a single, 4 or 5-stage pipeline <ref> [9, 6] </ref>. In more advanced machines, performance has been improved by increasing the pipeline depth (i.e., superpipelined) and by employing multiple pipelines (i.e., superscalar).
Reference: [10] <author> A.J. Martin, S.M. Burns, T.K. Lee, D. Borkovic, and P.J. Hazewindus. </author> <title> The Design of an Asynchronous Microprocessor. </title> <booktitle> In Proc. Decennial Cal-tech Conf. on VLSI, </booktitle> <pages> pages 20-22. </pages> <publisher> The MIT Press, </publisher> <month> March </month> <year> 1989. </year>
Reference-contexts: While the use of asynchronous modules in the design of processors goes back to the late 1960s with work at Washing-ton University (St. Louis) [2], it wasn't until 1988 that an entire asynchronous microprocessor was designed at the California Institute of Technology <ref> [10] </ref>. Later an asynchronous version of the ARM processor, AMULET1, was completed at University of Manch-ester (UK) [5].
Reference: [11] <author> R.F. Sproull, I.E. Sutherland, and C.E. Mol-nar. </author> <title> Counterflow Pipeline Processor Architecture. </title> <booktitle> IEEE Design and Test of Computers, </booktitle> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Later an asynchronous version of the ARM processor, AMULET1, was completed at University of Manch-ester (UK) [5]. Currently, SUN Microsystems is developing an asynchronous microprocessor called Counterflow Pipeline Processor (CFPP) <ref> [11] </ref> and it appears that several other institutions and companies are also investigating the design of asynchronous machines [8]. with several buffers between each stage.
Reference: [12] <author> S. Waser and M. Flynn. </author> <title> Introduction to Arithmetic for Digital System Designers. </title> <publisher> CBS College Pub., </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The simulation results for previous configurations using different adders are shown in Table 6. Since the CSA has a tree structure and takes a constant amount of time (O (log 2 n)) to complete addition, it is considered to be one of the fastest adders (for clocked design) <ref> [7, 12] </ref> and achieves the highest throughput for all configurations. However, the complexity of the CSA circuit is higher than that of most other adder circuits. Of the remaining adders, the use of the SEL (8) results in the best throughput.
References-found: 12

