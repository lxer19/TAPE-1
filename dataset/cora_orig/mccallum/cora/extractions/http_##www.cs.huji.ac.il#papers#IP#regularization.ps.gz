URL: http://www.cs.huji.ac.il/papers/IP/regularization.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/index.html
Root-URL: 
Email: dkeren@cs.huji.ac.il, werman@cs.huji.ac.il  
Title: A Bayesian Framework for Regularization July 6, 1994  
Author: Daniel Keren and Michael Werman 
Note: This research has been sponsored by the U.S. Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01.  
Address: Jerusalem 91904, Israel  
Affiliation: Institute for Computer Science The Hebrew University of Jerusalem  
Abstract: Regularization is a popular method for interpolating sparse data, as well as smoothing data obtained from noisy measurements. Simply put, regularization looks for an interpolating or approximating function which is both close to the data and also "smooth" in some sense. Formally, this function is obtained by minimizing an error functional which is the sum of two terms, one measuring the distance from the data, the other measuring the smoothness of the function. The classical approach to regularization is: decide the relative weights that should be given to these two terms, and minimize the resulting error functional. This approach, however, suffers from two serious flaws: there is no rigorous way to compute these weights and it does not find the function which is the MAP estimate for the interpolation problem. This may result in grave instability in the reconstruction process; two data sets which are arbitrarily close to each other can be assigned two radically different interpolants. Our solution to this problem is through the Bayesian approach: instead of using only the optimal estimates for the weights, construct a probability distribution over all possible weights. Then, the probability of each function is equal to the integral of its probability over all possible weights, scaled by the probability of the weights. The MAP estimate is the function which maximizes this probability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. A. Adams. </author> <title> Sobolev Spaces. </title> <publisher> Academic Press, </publisher> <year> 1975. </year>
Reference-contexts: The f chosen is the one minimizing M (). The one-dimensional case, which will be addressed in this work, is to minimize M (f ) = i=1 2 + 0 00 Without loss of generality, we will assume from now on that the functions are defined on the interval <ref> [0; 1] </ref>, and will omit those limits in the integrals. However, this approach fails to find the MAP estimate for the interpolant f , as it uses only the optimal weights to construct f . <p> to use functions f for which the smoothness term Z 1 [f 00 (v)] 2 dv is defined, the natural space is the Sobolev space L 2 2 , which consists of the functions having a second derivative which is square integrable (for an extensive treatment of Sobolev spaces, see <ref> [1] </ref>). For technical reasons, we restrict ourselves to the subspace of L 2 2 which is defined by ff 2 L 2 2 j f (0) = f (1) = 0g. The reason is that otherwise the denominator in Equation 1 is not defined.
Reference: [2] <author> H. Akima. </author> <title> Bivariate interpolation and smooth surface fitting based on local procedures. </title> <journal> Comm. ACM, </journal> <volume> 17 </volume> <pages> 26-31, </pages> <year> 1974. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data [26, 27, 10, 3]. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics <ref> [2] </ref> and medical imaging.
Reference: [3] <author> M. Bertero, T.A Poggio, and V. Torre. </author> <title> Ill-posed problems in early vision. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 8 </volume> <pages> 869-889, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data <ref> [26, 27, 10, 3] </ref>. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics [2] and medical imaging.
Reference: [4] <author> R.J Chorley. </author> <title> Spatial Analysis in Geomorphology. </title> <publisher> Methuen and Co., </publisher> <year> 1972. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data [26, 27, 10, 3]. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology <ref> [4] </ref>, electronics [2] and medical imaging.
Reference: [5] <author> P. Craven and G. Whaba. </author> <title> Optimal smoothing of noisy data with spline functions. </title> <journal> Numerische Mathematik, </journal> <volume> 31 </volume> <pages> 377-403, </pages> <year> 1979. </year>
Reference-contexts: In Section 6, a realistic example (hand-written data) is analized. Section 7 summarizes the work and suggests directions for further research. 2 Previous Work The most popular method for determining the smoothing parameter is that of the generalized cross validation (GCV) <ref> [5] </ref>, which is described in more detail in Section 4. <p> The second observation is easier to explain; intuitively, it means that there is no reason to assume that the measurement noise is larger than the data. 4 The GCV Algorithm and How it Compares With the Bayesian Approach A common method for determining is cross-validation <ref> [5] </ref>. The idea is to choose a such that the data points will predict one another. <p> V 0 () is then defined as l X [f k (x k ) y k ] 2 , and the chosen is the one minimizing V 0 (). An extension of this method is the Generalized cross-validation (GCV) <ref> [5] </ref> which proceeds as follows. <p> The chosen is the one minimizing V (). In this work, we have used a version of the cross validation algorithm written by M.F. Hutchinson, and described in the ACM. Trans. Math. Software Vol 12, No. 2, June 1986, p. 150. The algorithm uses the GCV algorithm described in <ref> [5] </ref>, and it gives identical results to the GCV version we implemented. Experiments were run on a Sparc work station. One can see that when the data strongly oscillates, the GCV algorithm can run into grave problems. Before giving some actual results, let us explain why this happens.
Reference: [6] <author> S. Geman and D.Geman. </author> <title> Stochastic relaxation, gibbs distribution, and the bayesian restorat ion of images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [7] <author> L. Gross. </author> <title> Integration and non-linear transformations in hilbert space. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 94 </volume> <pages> 404-440, </pages> <year> 1960. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [8] <author> E. Hille. </author> <title> Introduction to the general theory of reproducing kernels. </title> <journal> Rocky Mountain Journal of Mathematics, </journal> <volume> 2 </volume> <pages> 321-368, </pages> <year> 1972. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [9] <author> B. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The data can be sparse | e.g. the height of a small number of points on a surface, or dense but incomplete | e.g. the case of optical flow and shape from shading <ref> [9] </ref> where data is available at many points but consists of the function's or its derivative's value in a certain direction only.
Reference: [10] <author> B.K.P Horn and B.G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data <ref> [26, 27, 10, 3] </ref>. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics [2] and medical imaging.
Reference: [11] <author> D. Keren and M. Werman. </author> <title> Variations on regularization. </title> <booktitle> In 10'th International Conference on Pattern Recognition, </booktitle> <address> Atlantic City, </address> <year> 1990. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [12] <author> D. Keren and M. Werman. </author> <title> Probabilistic analysis of regularization. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 15 </volume> <pages> 982-995, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given . <p> It was applied to the types of spaces used in regularization in <ref> [12, 13] </ref>. The space M ; is a "Hilbert space" [30]. <p> We can explicitly calculate this function, following the same method as in <ref> [12] </ref>: H x (~) = 0 ~ x : 6 x (1~)(x 2 +~ 2 2~) note that this expression depends only on the location of the sample points x i , and not the value of the samples y i . <p> Now, if f 2 W , then for every 1 i 1, (f; h x i ) 1 = f (x i ) = 0, hence h x i 2 W ? . Since the h x i 's are linearly independent <ref> [12] </ref>, we have from dimension arguments the following important result W ? = span fh x 1 ; h x 2 ::::h x n g next, let us write the expression in the exponent of the integrand in the numerator of Equation 2 using the decomposition into W and W ?
Reference: [13] <author> Daniel Keren. </author> <title> Probabilistic Analyses of Interpolation in Computer Vision. </title> <type> PhD thesis, </type> <institution> Hebrew University of Jerusalem, </institution> <year> 1990. </year>
Reference-contexts: It was applied to the types of spaces used in regularization in <ref> [12, 13] </ref>. The space M ; is a "Hilbert space" [30].
Reference: [14] <author> J. Kuelbs, F.M. Larkin, and J.A. Williamson. </author> <title> Weak probability disributions on reproducing kernel hilbert spaces. </title> <journal> Rocky Mountain Journal of Mathematics, </journal> <volume> 2 </volume> <pages> 369-378, </pages> <year> 1972. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [15] <author> H.H Kuo. </author> <title> Guassian Measures in Banach Spaces. </title> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [16] <author> F.M. Larkin. </author> <title> Gaussian measure in hilbert space and applications in numerical analysis. </title> <journal> Rocky Mountain Journal of Mathematics, </journal> <volume> 2 </volume> <pages> 379-421, </pages> <year> 1972. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [17] <author> Y.G. Leclerc. </author> <title> Image and boundary segmentation via minimal-length encoding on the co nnection machine. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 1056-1069, </pages> <year> 1989. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [18] <author> David J.C. MacKay. </author> <title> Bayesian Methods for Adaptive Models. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: There, the following question is posed: given the data D, what is the most probable value of ? More recent work in this direction was done by MacKay <ref> [18] </ref>.
Reference: [19] <author> J.L Marroquin. </author> <title> Deterministic bayesian estimation of markovian random fields with app lications to computational vision. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 597-601, </pages> <address> London, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [20] <author> L. Matthies, R. Szeliski, and T. Kanade. </author> <title> Incremental estimation of dense depth maps from image sequences. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 366-374, </pages> <address> Ann Arbor, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [21] <author> D. Mumford and J. Shah. </author> <title> Boundary detection by minimizing fuctionals. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 22-26, </pages> <address> San Francisco, </address> <month> June </month> <year> 1985. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [22] <author> J.E. Robinson, H.A.K. Charlesworth, and M.J. Ellis. </author> <title> Structural analysis using spatial filtering in interior plans of south-central alberta. </title> <journal> Amer. Assoc. Petrol. Geol. Bull., </journal> <volume> 53 </volume> <pages> 2341-2367, </pages> <year> 1969. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data [26, 27, 10, 3]. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration <ref> [22] </ref>, geology [4], electronics [2] and medical imaging.
Reference: [23] <author> R. Szeliski. </author> <title> Regularization uses fractal priors. </title> <booktitle> In National Conference on Artificial Intelligence, </booktitle> <pages> pages 749-754, </pages> <year> 1987. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [24] <author> R. Szeliski. </author> <title> Bayesian Modeling of Uncertainty in Low-Level Vision. </title> <publisher> Kluwer, </publisher> <year> 1989. </year>
Reference-contexts: A different approach, which is closer to ours, is that of Bayesian model selection which, to the best of our knowledge, was first suggested in the pioneering work of Szeliski <ref> [24] </ref>. There, the following question is posed: given the data D, what is the most probable value of ? More recent work in this direction was done by MacKay [18]. <p> (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [25] <author> S. Szeliski and D. Terzopoulos. </author> <title> From splines to fractals. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 51-60, </pages> <year> 1989. </year>
Reference-contexts: (f =M ) Regularization, for instance, can be formalized in this way because P r (D=f ) = (2) 2 n n X [f (x i ) y i ] 2 (assuming uncorrelated Gaussian noise of constant variance) and the prior distribution is Z 00 which resembles the Boltzmann distribution <ref> [24, 6, 11, 12, 23, 21, 19, 17, 25, 20] </ref>. Multiplying, we get that the f chosen from M should maximize exp (M (f )), or minimize M (f ). This simple analysis shows how regularization is consistent with Bayes rule for choosing the MAP estimate, given .
Reference: [26] <author> D. Terzopoulos. </author> <title> Multi-level surface reconstruction. </title> <editor> In A. Rosenfeld, editor, </editor> <title> Multiresolution Image Processing and Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data <ref> [26, 27, 10, 3] </ref>. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics [2] and medical imaging.
Reference: [27] <author> D. Terzopoulos. </author> <title> Regularization of visual problems involving discontinuities. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 413-424, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: 1 Introduction In computer vision, regularization [28] is used to reconstruct objects from partial data <ref> [26, 27, 10, 3] </ref>. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics [2] and medical imaging.
Reference: [28] <author> A.N Tikhonov and V.Y Arsenin. </author> <title> Solution of Ill-Posed Problems. </title> <publisher> Winston and Sons, </publisher> <year> 1977. </year>
Reference-contexts: 1 Introduction In computer vision, regularization <ref> [28] </ref> is used to reconstruct objects from partial data [26, 27, 10, 3]. Reconstruction of surfaces from partial data has been studied in many other fields, for example petroleum exploration [22], geology [4], electronics [2] and medical imaging.
Reference: [29] <author> G.W. Wasilkowski. </author> <title> Optimal algorithms for linear problems with gaussian measures. </title> <journal> Rocky Mountain Journal of Mathematics, </journal> <volume> 16 </volume> <pages> 727-749, </pages> <year> 1986. </year>
Reference-contexts: The question of how to compute such integrals as those appearing in Equation 1 which are defined over domains that are infinite dimensional has been solved for some types of integrals in the realm of pure mathematics <ref> [8, 14, 16, 29, 15, 7] </ref>. It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" [30].
Reference: [30] <author> N. Young. </author> <title> An Introduction to Hilbert Space. </title> <publisher> Cambridge Mathematical Textbooks, </publisher> <year> 1988. </year> <month> 29 </month>
Reference-contexts: It was applied to the types of spaces used in regularization in [12, 13]. The space M ; is a "Hilbert space" <ref> [30] </ref>.
References-found: 30

