URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/maass-schnitger-eds.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: e-mail: maass@igi.tu-graz.ac.at  e-mail: georgs@uni-paderborn.de  e-mail: sontag@hilbert.rutgers.edu  
Title: A Comparison of the Computational Power of Sigmoid and Boolean Threshold Circuits  
Author: Wolfgang Maass Georg Schnitger Eduardo D. Sontag 
Note: Partially supported by Siemens Corporate Research and AFOSR-88-0235 and AFOSR-91-0343  
Address: Klosterwiesgasse 32/2 A-8010 Graz, Austria  D-4790 Paderborn, Germany  
Affiliation: Institute for Theoretical Computer Science Technische Universitaet Graz  Fachbereich Mathematik/Informatik Universitat Paderborn  SYCON Rutgers Center for Systems and Control Department of Mathematics Rutgers University  
Abstract: We examine the power of constant depth circuits with sigmoid (i.e. smooth) threshold gates for computing boolean functions. It is shown that, for depth 2, constant size circuits of this type are strictly more powerful than constant size boolean threshold circuits (i.e. circuits with linear threshold gates). On the other hand it turns out that, for any constant depth d, polynomial size sigmoid threshold circuits with polynomially bounded weights compute exactly the same boolean functions as the corresponding circuits with linear threshold gates. fl Partially supported by NSF-CCR-8805978 and AFOSR-87-0400
Abstract-found: 1
Intro-found: 1
Reference: [B] <author> W.J. Bultman, </author> <title> "Topics in the Theory of Machine Learning and Neural Computing", </title> <type> Ph.D Dissertation, </type> <institution> University of Illinois at Chicago, </institution> <year> 1991. </year>
Reference-contexts: This happens, because a "large discrepancy" in x-sum and y-sum is more likely if we assume P n 2 and i=1 y i n 2 than if we assume (say) P n 2 and i=1 y i n 2 . This phenomenon has been independently observed by Bultman <ref> [B] </ref>.
Reference: [DS] <author> B. DasGupta and G. Schnitger, </author> <title> "The power of Approximating: a Comparison of Activation Functions", </title> <booktitle> in Advances in Neural Information Processing Systems 5, S.E. </booktitle> <editor> Hanson, J.D. Cowan and C.L. Giles eds, </editor> <booktitle> pp. </booktitle> <pages> 615-622, </pages> <year> 1993. </year>
Reference-contexts: This extra power of the (generalized) "-Discriminator Lemma is crucial: in Remark 3.10 we show that its conventional version is insufficient for the proof of Theorem 3.1. Subsequent to the preliminary version [MSS] of this paper, DasGupta and Schnitger <ref> [DS] </ref> have shown that the boolean function SQ n with SQ n (x 1 ; : : : x n ; y 1 ; : : : ; y n 2 ) = (( i=1 n 2 i=1 leads to a depth-independent separation of boolean threshold circuits and fl-circuits. <p> However, the gate function fl has to satisfy more stringent differentiabilty requirements than are necessary for the constant-size computation of F n . For the case of analog input, <ref> [DS] </ref> also provides a comparison of the approximation power of various fl-circuits.
Reference: [HKP] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> "Introduction to the Theory of Neural Computation", </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, </address> <year> 1991. </year>
Reference-contexts: a smooth threshold circuit is the sigmoid threshold circuit, which is a -circuit for : R ! R defined by (x) = 1 + exp (x) Smooth threshold circuits (fl-circuits for "smooth" functions fl) have become the standard model for the investigation of learning on multi-layer artificial neural nets ([K], <ref> [HKP] </ref>, [RM], [SS1], [SS2], [WK]). In fact, the most common learning algorithm for multi-layer neural nets, the Backwards-Propagation algorithm, can only be implemented on fl-circuits for differentiable functions fl.
Reference: [HMPST] <author> A.Hajnal, W. Maass, P. Pudlak, M. Szegedy and G. Turan, </author> <title> "Threshold Circuits of Bounded Depth", </title> <booktitle> Proc. of the 28th Annual Symp. on Foundations of Computer Science, </booktitle> <pages> pp. 99-110, </pages> <year> 1987. </year> <note> To appear in J. Comp. </note> <institution> Syst. Sci.. </institution>
Reference-contexts: It is essential for our proof that the "-Discriminator Lemma holds not just for the uniform distribution over the input space (as it is stated in <ref> [HMPST] </ref>), but for any distribution. 2 Hence we have the freedom to construct such a distribution in a malicious manner, where we exploit specific "weak points" of the considered threshold circuit. <p> This is the case, provided k c log k (m) for a suitably small constant c. This in turn is satisfied for k d log n log log n for a suitably small constant d.) The "-Discriminator Lemma of <ref> [HMPST] </ref> can be generalized to hold for any distribution over the input space. We apply it here to the distribution Q r fi Q r over the input space f0; 1g 2m of the circuit D n (which computes the function F m ).
Reference: [K] <author> C.C. Klimasauskas, </author> <title> "The 1989 Neuro-Computing Bibliography", </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: Another motivation for the investigation of smooth threshold circuits is the desire to explore simple models for the (very complicated) information processing mechanisms in neural systems of living organisms. In a first approximation one may view the current firing rate of a neuron as its current output ([S], [RM], <ref> [K] </ref>). The firing rates of neurons are known to change between a few and several hundred firings per second. Hence a smooth threshold gate provides a somewhat better computational model for a neuron than a digital element that has just two different output signals.
Reference: [M] <author> W. Maass, </author> <title> "Bounds for the computational power and learning complexity of analog neural nets", </title> <booktitle> Proc. of the 25th ACM Symposium on the Theory of Computing, </booktitle> <year> 1993, </year> <pages> 335-344. </pages>
Reference-contexts: Case 1.1: 8i (jw i j m 1=8 ju i j) and jw m j 60jw 1 j: This implies that jaj m 1=8 jbj. Hence the line L is very "steep". We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x <p> Case 1.1: 8i (jw i j m 1=8 ju i j) and jw m j 60jw 1 j: This implies that jaj m 1=8 jbj. Hence the line L is very "steep". We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x 0 ; y 0 ) <p> Hence the line L is very "steep". We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x 0 ; y 0 ) 2 Lg is contained in an interval of <p> Hence the line L is very "steep". We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x 0 ; y 0 ) 2 Lg is contained in an interval of length m 7=8 + 2m <p> Hence the line L is very "steep". We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x 0 ; y 0 ) 2 Lg is contained in an interval of length m 7=8 + 2m 3=4 + 1 3 m 7=8 . <p> We have in this case, maxfx 2 <ref> [0; m] </ref> : 9y 2 [0; m] ((x; y) 2 L)g minfx 2 [0; m] : 9y 2 [0; m] ((x; y) 2 L)g m 7=8 : Thus, the set fx 2 [0; m] : 9x 0 ; y 0 2 [0; m] (jx x 0 j m 3=4 ^ (x 0 ; y 0 ) 2 Lg is contained in an interval of length m 7=8 + 2m 3=4 + 1 3 m 7=8 . <p> This implies that jf (x; y) 2 f 2 m + rg 2 : P (x; y)gj 3m 7=8 m = 3m 15=8 ; (8) where P (x; y) is equivalent to (ax + by &lt; t) ^ 9 (x 0 ; y 0 ) 2 <ref> [0; m] </ref> 2 ( (ax 0 + by 0 t) ^ (jx x 0 j m 3=4 ) ): As a first step towards estimating ADV r (G) we consider the set S = f (~x; ~y) 2 U : G (~x; ~y) = 1 ^ F m (~x; ~y) = <p> Remark 4.3 More recently it has been shown (see the paper by Maass in this volume, or the extended abstract <ref> [M] </ref>) that for neural nets with arbitrary piecewise polynomial activation functions fl (with polynomially many polynomial pieces of bounded degree) and arbitrary real weights, the class of boolean functions that can be computed in constant depth and polynomial size (with arbitrarily small separation) is contained in T C 0 . 20
Reference: [MSS] <author> W. Maass, G. Schnitger, E. D. Sontag, </author> <title> "On the computational power of sigmoid versus boolean threshold circuits", </title> <booktitle> Proc. of the 32nd Annual IEEE Symp. on Foundations of Computer Science, 1991, </booktitle> <volume> 767 - 776 </volume>
Reference-contexts: This extra power of the (generalized) "-Discriminator Lemma is crucial: in Remark 3.10 we show that its conventional version is insufficient for the proof of Theorem 3.1. Subsequent to the preliminary version <ref> [MSS] </ref> of this paper, DasGupta and Schnitger [DS] have shown that the boolean function SQ n with SQ n (x 1 ; : : : x n ; y 1 ; : : : ; y n 2 ) = (( i=1 n 2 i=1 leads to a depth-independent separation of <p> An extended abstract of this paper has previously appeared in <ref> [MSS] </ref>.
Reference: [MT] <author> W. Maass and G. Turan, </author> <title> "How Fast Can a Threshold Gate Learn?", in: Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, </title> <editor> G. Drastal, S.J. Hanson and R. Rivest eds., </editor> <publisher> MIT Press, to appear. </publisher>
Reference: [Mu] <author> S. Muroga, </author> <title> "Threshold Logic and its Applications", </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference: [S] <author> E.L. Schwartz, </author> <title> "Computational Neuroscience", </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference: [So] <author> E.D. Sontag, </author> <title> "Feedforward Nets for Interpolation and Classification", </title> <journal> J. Comp. Syst. Sci., </journal> <volume> 45(1992): </volume> <pages> 20-48. </pages>
Reference-contexts: For computations with real (rather than Boolean) inputs, there has been some work dealing with the differences in capabilities between sigmoidal and threshold devices; in particular <ref> [So] </ref> studies questions of interpolation and classification related to learnability (VC dimension). 3 Boolean threshold gates are less powerful Theorem 3.1 No family (C n j n 2 N) of constant size boolean threshold circuits of depth 2 (with unrestricted weights and thresholds) can compute the function F n .
Reference: [SS1] <author> E.D. Sontag and H.J. Sussmann, </author> <title> "Backpropagation can give rise to spurious local minima even for networks without hidden layers", </title> <booktitle> Complex Systems 3, </booktitle> <pages> pp. 91-106, </pages> <year> 1989. </year>
Reference-contexts: threshold circuit is the sigmoid threshold circuit, which is a -circuit for : R ! R defined by (x) = 1 + exp (x) Smooth threshold circuits (fl-circuits for "smooth" functions fl) have become the standard model for the investigation of learning on multi-layer artificial neural nets ([K], [HKP], [RM], <ref> [SS1] </ref>, [SS2], [WK]). In fact, the most common learning algorithm for multi-layer neural nets, the Backwards-Propagation algorithm, can only be implemented on fl-circuits for differentiable functions fl.
Reference: [SS2] <author> E.D. Sontag and H.J. Sussmann, </author> <title> "Backpropagation separates where Perceptrons do", </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> pp. 243-249, </pages> <year> 1991. </year>
Reference-contexts: circuit is the sigmoid threshold circuit, which is a -circuit for : R ! R defined by (x) = 1 + exp (x) Smooth threshold circuits (fl-circuits for "smooth" functions fl) have become the standard model for the investigation of learning on multi-layer artificial neural nets ([K], [HKP], [RM], [SS1], <ref> [SS2] </ref>, [WK]). In fact, the most common learning algorithm for multi-layer neural nets, the Backwards-Propagation algorithm, can only be implemented on fl-circuits for differentiable functions fl.
Reference: [WK] <author> S.M. Weiss, </author> <title> C.A. Kulikowsky, "Computer Systems that Learn", </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991. </year> <month> 21 </month>
Reference-contexts: is the sigmoid threshold circuit, which is a -circuit for : R ! R defined by (x) = 1 + exp (x) Smooth threshold circuits (fl-circuits for "smooth" functions fl) have become the standard model for the investigation of learning on multi-layer artificial neural nets ([K], [HKP], [RM], [SS1], [SS2], <ref> [WK] </ref>). In fact, the most common learning algorithm for multi-layer neural nets, the Backwards-Propagation algorithm, can only be implemented on fl-circuits for differentiable functions fl.
References-found: 14

