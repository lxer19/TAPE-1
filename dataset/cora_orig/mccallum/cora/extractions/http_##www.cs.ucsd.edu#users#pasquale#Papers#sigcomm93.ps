URL: http://www.cs.ucsd.edu/users/pasquale/Papers/sigcomm93.ps
Refering-URL: http://www.cs.ucsd.edu/users/pasquale/Pub.html
Root-URL: http://www.cs.ucsd.edu
Email: -jkay, pasquale-@cs.ucsd.edu  
Title: The Importance of Non-Data Touching Processing Overheads in TCP/IP gle procedure, the Internet checksum routine
Author: Jonathan Kay and Joseph Pasquale 
Keyword: 2.0 Overhead Categories Checksum  
Note: 1.0 Introduction  Computing checksums is accomplished by a sin  This work was supported in part by grants from the National Science Foundation, Digital Equipment Corporation and TRW in connection with the Sequoia 2000 Project, and NCR Corporation.  
Address: San Diego, CA 92093-0114  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Abstract: We present detailed measurements of various processing overheads of the TCP/IP and UDP/IP protocol stacks on a DECstation 5000/200 running the Ultrix 4.2a operating system. These overheads include data-touching operations, such as the checksum computation and data movement, which are well known to be major time consumers. In this study, we also considered overheads due to non-data touching operations, such as network buffer manipulation, protocol-specific processing, operating system functions, data structure manipulations (other than network buf fers), and error checking. We show that when one considers realistic message size distributions, where the majority of messages are small, the cumulative time consumed by the non-data touching overheads represents the majority of processing time. We assert that it will be diffi cult to significantly reduce the cumulative processing time due to non-data touching overheads. The goal of this study is to determine the relative importance of various processing overheads in network software, in particular , the TCP/IP and UDP/IP protocol stacks. In the past, signifi cant focus has been placed on maximizing throughput, noting that data touching operations such as computing checksums and data copying are responsible for the primary bottlenecks in throughput per - formance [3, 5-8]. Maximal throughput is typically achieved by sending lar ge messages. However , most TCP/IP packets on local area networks are smaller than 200 bytes [9]. Furthermore, almost all IP packets sent across wide-area networks are no more than 576 bytes long [4] because of the suggested default TCP maximum segment size [19]. The relative inuences of processing overheads for packets whose sizes are at most 576 bytes greatly dif fer from those of packets whose lengths are the maximum transmission units (MTUs) for typical LANS such as Ethernet (1500 bytes) and especially FDDI (4352 bytes). We present detailed measurements of processing times for various interesting categories of overhead in both TCP/IP and UDP/IP, for a wide variety of message sizes. More importantly , we present detailed aggregated times based on a realistic distribution of packet sizes generated by a measured workload from a real environment. As expected from studies of LAN packet traces, the overwhelming majority of messages are small and and do not require signifi cant time in computing checksums or copying of data. We show that when a realistic distribution of messages sizes is considered, although the checksum computation and data move ment are by themselves lar ge expenses, they contribute 40% (a minority) of the overall processing time. Consequently , we show that the cumulative effect of other overheads, such as protocolspe-cific processing and network buf fer allocation, is important. This paper details which processing overheads are most expensive, and how the predominance of small messages af fects the resulting breakdown of overheads in unexpected ways. The paper is organized as follows. Section 2 describes our categorization of major network software processing overheads. Section 3 describes the experimental setup used to obtain measurements. Section 4 contains an analysis of network software processing overheads for a full range of message sizes. Section 5 presents a message size distribution obtained from measurements we carried out on a real workload. Section 6 analyzes aggregated processing overheads based on the measured message size distribution. Section 7 contains a fi ner analysis for each category of over - heads, with emphasis on which individual overheads are most time consuming. Finally, we present conclusions in Section 8. We categorized the major processing overheads in network software as follows (the name abbreviations used in subsequent graphs are in parentheses): checksum computation (Checksum), data movement (DataMove), data structure manipulations (Data Struct), error checking (ErrorChk), network buf fer management (Mbuf), operating system functions (OpSys), and protocolspe-cific processing (ProtSpec). Other studies have shown some of these overheads to be expensive [3, 5-8, 11, 22]. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <editor> R. T. Braden, D. A. Borman, and C. Partridge, </editor> <title> Computing the Internet Checksum, Internet RFC 1071, </title> <month> September </month> <year> 1988. </year>
Reference-contexts: Other studies have shown some of these overheads to be expensive [3, 5-8, 11, 22]. Checksum : Computing checksums is accomplished by a sin gle procedure, the Internet checksum routine <ref> [1] </ref>, which is per - formed on data in TCP and UDP, and on the header in IP. DataMove : Data movement includes work involved in the moving of data from one place to another.
Reference: [2] <author> R. T. Braden, ed., </author> <title> Requirements for Internet Hosts - Commu nication Layers, Internet RFC 1122, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: An increasing number of TCP/IP implementations succeed in reducing or avoiding the checksumming operation by various strategies. Some vendors have disabled UDP checksumming by default at a loss in packet integrity and in contravention of Internet host requirements <ref> [2] </ref>. Clark and Tennenhouse have suggested combining checksumming and copying using integrated layer processing [8]. Silicon Graphics, Inc. workstations move the burden of per - forming the checksum computation from the CPU to their FDDI interfaces [20].
Reference: [3] <author> L.-F. Cabrera, E. Hunter , M. J. Karels, D. A. </author> <title> Mosher, User Process Communication Performance in Networks of Com puters, </title> <journal> IEEE Transactions on Softwar e Engineering 38-53, </journal> <month> January </month> <year> 1988. </year>
Reference-contexts: In the past, signifi cant focus has been placed on maximizing throughput, noting that data touching operations such as computing checksums and data copying are responsible for the primary bottlenecks in throughput per - formance <ref> [3, 5-8] </ref>. Maximal throughput is typically achieved by sending lar ge messages. However , most TCP/IP packets on local area networks are smaller than 200 bytes [9]. <p> Other studies have shown some of these overheads to be expensive <ref> [3, 5-8, 11, 22] </ref>. Checksum : Computing checksums is accomplished by a sin gle procedure, the Internet checksum routine [1], which is per - formed on data in TCP and UDP, and on the header in IP. <p> Others have remarked on potential problems due to this bimodality [10], and the resulting bimodality is apparent in <ref> [3] </ref>.
Reference: [4] <author> R. Caceres, P. B. Danzig, S. Jamin, D. J. Mitzel. </author> <title> Characteris tics of Wide-Area TCP/IP Conversations, </title> <booktitle> Proceedings of the SIGCOMM 91 Symposium on Communications Architectures and Protocols , pp. </booktitle> <pages> 101-112, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Maximal throughput is typically achieved by sending lar ge messages. However , most TCP/IP packets on local area networks are smaller than 200 bytes [9]. Furthermore, almost all IP packets sent across wide-area networks are no more than 576 bytes long <ref> [4] </ref> because of the suggested default TCP maximum segment size [19]. <p> In Figures 4a-b, we magnify the leftmost regions of the graphs in Figures 2a-b to focus on small messages. The message sizes range from 0 to 614, which includes the most common sizes of packets <ref> [4] </ref> sent over an Internet. <p> One would expect a detectable fraction of messages of size 536 (576 - size of headers), the TCP Default Maximum Segment Size, for lar ge wide-area network data packets <ref> [4, 19] </ref>. However , there is no indication of this. Our data actually contained packets of this size, but the fraction is too small to show up in the graphs.
Reference: [5] <author> D. D. Clark, </author> <title> Modularity and Ef ficiency in Protocol Imple mentation, Internet RFC, </title> <type> 817, </type> <year> 1982. </year>

Reference: [7] <author> D. D. Clark, V. Jacobson, J. Romkey, H. Salwen, </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications Maga zine , pp. </journal> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: R r o c e s s i n g T e ( u s e c ) 22 Aggregated UDP Network Buffer Management Times At fi rst glance, it may be surprising that Demux is so small, under 20 microseconds, given reports that this operation is a bottle neck <ref> [7, 14] </ref>. This is because our single-user testing environment only has a single connection, and hence a single entry in the list of protocol control blocks. 7.3 Mbufs Mbuf is the second largest of the non-data touching categories of overhead.
Reference: [8] <author> D. D. Clark, D. L. Tennenhouse, </author> <title> Architectural Consider - ations for a New Generation of Protocols, </title> <booktitle> Proceedings of the SIGCOMM 90 Symposium on Communications Architectures and Protocols , pp. </booktitle> <pages> 200-208, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Some vendors have disabled UDP checksumming by default at a loss in packet integrity and in contravention of Internet host requirements [2]. Clark and Tennenhouse have suggested combining checksumming and copying using integrated layer processing <ref> [8] </ref>. Silicon Graphics, Inc. workstations move the burden of per - forming the checksum computation from the CPU to their FDDI interfaces [20].
Reference: [9] <author> R. Gusella, </author> <title> A Measurement Study of Diskless Workstation Traffic on an Ethernet, </title> <journal> IEEE Transactions on Communica tions , 38(9), </journal> <pages> pp. 1557-1568, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Maximal throughput is typically achieved by sending lar ge messages. However , most TCP/IP packets on local area networks are smaller than 200 bytes <ref> [9] </ref>. Furthermore, almost all IP packets sent across wide-area networks are no more than 576 bytes long [4] because of the suggested default TCP maximum segment size [19]. <p> Message sizes were computed from the information contained in the packets. Figures 5a-b show the message size distributions for TCP and UDP messages. The observed message size distributions matched our expectations based on previous work on Ethernet-based packet traces <ref> [9, 15] </ref>. Figure 5a shows that almost all the TCP messages are small; over 99% of all the TCP messages are less than 200 bytes long. Figure 5b shows a bimodal distribution for UDP message sizes.
Reference: [10] <author> N. C. Hutchinson, S. Mishra, L. L. Peterson, V. T. Thomas, </author> <title> Tools for Implementing Network Protocols, </title> <journal> Software - Practice and Experience , pp. </journal> <pages> 895-916, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Others have remarked on potential problems due to this bimodality <ref> [10] </ref>, and the resulting bimodality is apparent in [3].
Reference: [11] <author> V. Jacobson, </author> <title> BSD TCP Ethernet Throughput, </title> <address> comp.proto cols.tcp-ip, Usenet, </address> <year> 1988. </year>
Reference-contexts: Other studies have shown some of these overheads to be expensive <ref> [3, 5-8, 11, 22] </ref>. Checksum : Computing checksums is accomplished by a sin gle procedure, the Internet checksum routine [1], which is per - formed on data in TCP and UDP, and on the header in IP.
Reference: [12] <author> J. Kay , J. Pasquale, </author> <title> Measurement, Analysis, and Improve ment of UDP/IP Throughput for the DECstation 5000, </title> <booktitle> ceedings of the Winter 1993 USENIX Conference January 1993. </booktitle>
Reference-contexts: Clark and Tennenhouse have suggested combining checksumming and copying using integrated layer processing [8]. Silicon Graphics, Inc. workstations move the burden of per - forming the checksum computation from the CPU to their FDDI interfaces [20]. In <ref> [12] </ref>, we have proposed a scheme intended to preserve almost all of the performance gain from disabling checksums with nearly none of the loss of integrity nor any additional cost in hardware.
Reference: [13] <author> S. J. Lef er, M. K. Mckusick, M. J. Karels, J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3 BSD UNIX Operat ing System , Addison-Wesley, </title> <month> November </month> <year> 1989. </year>
Reference-contexts: Mbuf : All network software subsystems require a (sometimes complex) buffer descriptor which allows the prepending of headers and defragmentation of packets to be done cheaply. Berkeley Unix based network subsystems buf fer network data in a data structure called an mbuf <ref> [13] </ref>. All mbuf operations are part of this category . Allocation and freeing of mbufs are the most time-consuming mbuf operations. OpSys : Operating system overhead includes support for sock ets, synchronization overhead (sleep/wakeup), and other general operating system support functions.
Reference: [14] <author> P. E. McKenney , K. F . Dove, </author> <title> Ef ficient Demultiplexing of Incoming TCP Packets, </title> <booktitle> Pr oceedings of the SIGCOMM 92 Symposium on Communications Architectures and Pr otocols pp. </booktitle> <pages> 269-279, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: R r o c e s s i n g T e ( u s e c ) 22 Aggregated UDP Network Buffer Management Times At fi rst glance, it may be surprising that Demux is so small, under 20 microseconds, given reports that this operation is a bottle neck <ref> [7, 14] </ref>. This is because our single-user testing environment only has a single connection, and hence a single entry in the list of protocol control blocks. 7.3 Mbufs Mbuf is the second largest of the non-data touching categories of overhead.
Reference: [15] <author> Mogul, J., </author> <title> Network Locality at the Scale of Processes, </title> <booktitle> ceedings of the SIGCOMM 91 Symposium on Communica tions Architectures and Protocols , pp. </booktitle> <pages> 273-284, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Message sizes were computed from the information contained in the packets. Figures 5a-b show the message size distributions for TCP and UDP messages. The observed message size distributions matched our expectations based on previous work on Ethernet-based packet traces <ref> [9, 15] </ref>. Figure 5a shows that almost all the TCP messages are small; over 99% of all the TCP messages are less than 200 bytes long. Figure 5b shows a bimodal distribution for UDP message sizes.
Reference: [16] <author> J. Ousterhout, </author> <title> Why Arent Operating Systems Getting Faster As Fast As Hardware, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference , pp. </booktitle> <pages> 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference: [17] <author> D. C. </author> <title> Plummer , Ethernet Address Resolution Protocol: Or converting network protocol addresses to 48.bit Ethernet address for transmission on Ethernet hardware, Internet RFC 826, </title> <month> November </month> <year> 1982. </year>
Reference-contexts: Interface Protl is in the device driver layer , Link Protl is part of the IEEE 802 encapsulation layer, IP Protl is the IP layer, and TCP Protl is the TCP layer. Arp is the entire Address Resolution Protocol <ref> [17] </ref>, Demux is the operation of fi nding a protocol control block, given a TCP or UDP header (in_pcblookup), and PCB (Dis)connect is the operations of checking that a route already exists for a connection, setting up the protocol control block to re ect current connection state properly (in_pcbconnect and in_pcbdisconnect).
Reference: [18] <author> J. Postel, </author> <title> Transmission Control Protocol, Internet RFC 793, </title> <month> September </month> <year> 1981. </year>
Reference: [19] <author> J. Postel, </author> <title> The TCP Maximum Segment Size and Related Top ics, Internet RFC 879, </title> <month> November </month> <year> 1983. </year>
Reference-contexts: However , most TCP/IP packets on local area networks are smaller than 200 bytes [9]. Furthermore, almost all IP packets sent across wide-area networks are no more than 576 bytes long [4] because of the suggested default TCP maximum segment size <ref> [19] </ref>. The relative inuences of processing overheads for packets whose sizes are at most 576 bytes greatly dif fer from those of packets whose lengths are the maximum transmission units (MTUs) for typical LANS such as Ethernet (1500 bytes) and especially FDDI (4352 bytes). <p> One would expect a detectable fraction of messages of size 536 (576 - size of headers), the TCP Default Maximum Segment Size, for lar ge wide-area network data packets <ref> [4, 19] </ref>. However , there is no indication of this. Our data actually contained packets of this size, but the fraction is too small to show up in the graphs.
Reference: [20] <author> V. J. Schryver, </author> <title> private communication, </title> <month> November </month> <year> 1992. </year>
Reference-contexts: Clark and Tennenhouse have suggested combining checksumming and copying using integrated layer processing [8]. Silicon Graphics, Inc. workstations move the burden of per - forming the checksum computation from the CPU to their FDDI interfaces <ref> [20] </ref>. In [12], we have proposed a scheme intended to preserve almost all of the performance gain from disabling checksums with nearly none of the loss of integrity nor any additional cost in hardware.
Reference: [21] <author> Sun Microsystems, Inc., NFS: </author> <title> Network File System Protocol Specification, Internet RFC 1094, </title> <month> March </month> <year> 1989. </year>
Reference: [22] <author> R. W. Watson, S. A. Mamrak, </author> <title> Gaining Efficiency in Transport Services by Appropriate Design and Implementation Choices, </title> <journal> ACM Transactions on Computer Systems 97-120, </journal> <month> May </month> <year> 1987. </year>
Reference-contexts: Other studies have shown some of these overheads to be expensive <ref> [3, 5-8, 11, 22] </ref>. Checksum : Computing checksums is accomplished by a sin gle procedure, the Internet checksum routine [1], which is per - formed on data in TCP and UDP, and on the header in IP.
References-found: 21

