URL: ftp://ftp.rpi.edu/pub/math/tech/KPB-gto.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email bennek@rpi.edu  
Title: Global Tree Optimization: A Non-greedy Decision Tree Algorithm  
Author: Kristin P. Bennett 
Address: Troy, NY 12180  
Affiliation: Department of Mathematical Sciences Rensselaer Polytechnic Institute  
Abstract: A non-greedy approach for constructing globally optimal multivariate decision trees with fixed structure is proposed. Previous greedy tree construction algorithms are locally optimal in that they optimize some splitting criterion at each decision node, typically one node at a time. In contrast, global tree optimization explicitly considers all decisions in the tree concurrently. An iterative linear programming algorithm is used to minimize the classification error of the entire tree. Global tree optimization can be used both to construct decision trees initially and to update existing decision trees. Encouraging computational experience is reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illi-nois, </address> <year> 1992. </year>
Reference-contexts: The choice of which branch the point follows at equality is arbitrary. This type of decision has been used in greedy algorithms <ref> [6, 1] </ref>. The univariate decisions found by CART [5] for continuous variables can be considered special cases of this type of decision with only one nonzero component of w. A point is classified by following the path of the point through the tree until it reaches a leaf node. <p> misclassification error within each class. min 1 m X y j + k i=1 s:t: y j A j w fl + 1 y j 0 j = 1 : : : m (3) LP (3) has been used recursively in a greedy decision tree algorithm called Multisurface Method-Tree (MSMT) <ref> [1] </ref>. While it compares favorably with other greedy decision tree algorithms, it also suffers the problem of all greedy approaches. Locally good but globally poor decisions near the root of the tree can result in overly large trees with poor generalization.
Reference: [2] <author> K. P. Bennett. </author> <title> Optimal decision trees through mul-tilinear programming. R.P.I. Math Report No. </title> <type> 214, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <year> 1994. </year>
Reference-contexts: If a point is correctly classified at one leaf, the error along the path will be zero, and the product of the leaf errors will be zero. Space does not permit discussion of the general formulation in this paper, thus we refer the reader to <ref> [2] </ref> for more details. 4 Multilinear Programming The multilinear program (3) and its more general formulation can be optimized using the iterative linear programming Frank-Wolfe type method proposed in [4]. We outline the method here, and refer the reader to [2] for the mathematical properties of the algorithm. <p> formulation in this paper, thus we refer the reader to <ref> [2] </ref> for more details. 4 Multilinear Programming The multilinear program (3) and its more general formulation can be optimized using the iterative linear programming Frank-Wolfe type method proposed in [4]. We outline the method here, and refer the reader to [2] for the mathematical properties of the algorithm. <p> This experiment was repeated for trees ranging from 3 to 7 nodes in 2 to 25 dimensions. The results were averaged over 10 trials. We summarize the test results and refer the reader to <ref> [2] </ref> for more details. Figure 3 presents the average results for randomly generated trees with three decision nodes. These results are typical of those observed in the other experiments. MSMT achieved 100% correctness on the training set but used an excessive number of decisions.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: We briefly review one approach which formulates the problem as a set of linear inequalities and then uses linear programming to minimize the errors in the inequalities <ref> [3] </ref>. The reader is referred to [3] for full details of the practical and theoretical benefits of this approach. Let xw = fl be the plane formed by the decision. <p> We briefly review one approach which formulates the problem as a set of linear inequalities and then uses linear programming to minimize the errors in the inequalities <ref> [3] </ref>. The reader is referred to [3] for full details of the practical and theoretical benefits of this approach. Let xw = fl be the plane formed by the decision. <p> : : k f or d = 1 : : : j (9) The coefficients 1 m and 1 k were chosen so that (9) is identical to the LP (3) for the single decision case, thus guaranteeing that w = 0 is never the unique solution for that case <ref> [3] </ref>. These coefficients also help to make the method more numerically stable for large training set sizes. This general approach is applicable to any multivariate binary decision tree used to classify two or more sets. There is an error term for each point in the training set.
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: Consider this seemingly simple but NP-complete problem [9]: Can a tree with just two decision nodes correctly classify two disjoint point sets? In <ref> [4] </ref>, 1 4 w 2 x fl 2 &gt; 0 w 3 x fl 3 0 w 3 x fl 3 &gt; 0w 2 x fl 2 0 1 2 5 Class A Class B this problem was formulated as a bilinear program. <p> Space does not permit discussion of the general formulation in this paper, thus we refer the reader to [2] for more details. 4 Multilinear Programming The multilinear program (3) and its more general formulation can be optimized using the iterative linear programming Frank-Wolfe type method proposed in <ref> [4] </ref>. We outline the method here, and refer the reader to [2] for the mathematical properties of the algorithm. <p> The Frank-Wolfe algorithm for problem is the following: Algorithm 4.1 (Frank-Wolfe algorithm <ref> [7, 4] </ref>) Start with any x 0 2 X . <p> The algorithm terminates at some x j that satisfies the minimum principle necessary optimality condition: 5f (x j )(x x j ) 0, for all x 2 X , or each accumulation point x of the sequence fx i g satisfies the minimum principle <ref> [4] </ref>. The gradient calculation for the GTO function is straightforward.
Reference: [5] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> Califormna, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Global Tree Optimization (GTO) is a new approach for constructing decision trees that classify two or more sets of n-dimensional points. The essential difference between this work and prior decision tree algorithms (e.g. CART <ref> [5] </ref> and ID3 [10]) is that GTO is non-greedy. For greedy algorithms, the "best" decision at each node is found by optimizing some splitting criterion. This process is started at the root and repeated recursively until all or almost all of the points are correctly classified. <p> The choice of which branch the point follows at equality is arbitrary. This type of decision has been used in greedy algorithms [6, 1]. The univariate decisions found by CART <ref> [5] </ref> for continuous variables can be considered special cases of this type of decision with only one nonzero component of w. A point is classified by following the path of the point through the tree until it reaches a leaf node.
Reference: [6] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <type> COINS Technical Report 92-83, </type> <institution> University of Massachussets, Amherst, Massachusetts, </institution> <year> 1992. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: The choice of which branch the point follows at equality is arbitrary. This type of decision has been used in greedy algorithms <ref> [6, 1] </ref>. The univariate decisions found by CART [5] for continuous variables can be considered special cases of this type of decision with only one nonzero component of w. A point is classified by following the path of the point through the tree until it reaches a leaf node.
Reference: [7] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: The Frank-Wolfe algorithm for problem is the following: Algorithm 4.1 (Frank-Wolfe algorithm <ref> [7, 4] </ref>) Start with any x 0 2 X .
Reference: [8] <author> J. H. Friedman. </author> <title> Multivariate adaptive regression splines (with discussion). </title> <journal> Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: GTO overcomes these limitations by treating the decision tree as a function and optimizing the classification error of the entire tree. The function is similar to the one proposed for MARS <ref> [8] </ref>, however MARS is still a greedy algorithm. Greedy algorithms optimize one node at a time and then fix the resulting decisions. GTO starts from an existing tree.
Reference: [9] <author> N. Megiddo. </author> <title> On the complexity of polyhedral separability. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 3 </volume> <pages> 325-337, </pages> <year> 1988. </year>
Reference-contexts: Minimizing the global error of a decision tree with fixed structure is a non-convex optimization problem. The problem of constructing a decision tree with a fixed number of decisions to correctly classify two or more sets is a special case of the NP-complete polyhedral separability problem <ref> [9] </ref>. Consider this seemingly simple but NP-complete problem [9]: Can a tree with just two decision nodes correctly classify two disjoint point sets? In [4], 1 4 w 2 x fl 2 &gt; 0 w 3 x fl 3 0 w 3 x fl 3 &gt; 0w 2 x fl 2 <p> The problem of constructing a decision tree with a fixed number of decisions to correctly classify two or more sets is a special case of the NP-complete polyhedral separability problem <ref> [9] </ref>. Consider this seemingly simple but NP-complete problem [9]: Can a tree with just two decision nodes correctly classify two disjoint point sets? In [4], 1 4 w 2 x fl 2 &gt; 0 w 3 x fl 3 0 w 3 x fl 3 &gt; 0w 2 x fl 2 0 1 2 5 Class A Class B
Reference: [10] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Global Tree Optimization (GTO) is a new approach for constructing decision trees that classify two or more sets of n-dimensional points. The essential difference between this work and prior decision tree algorithms (e.g. CART [5] and ID3 <ref> [10] </ref>) is that GTO is non-greedy. For greedy algorithms, the "best" decision at each node is found by optimizing some splitting criterion. This process is started at the root and repeated recursively until all or almost all of the points are correctly classified.
References-found: 10

