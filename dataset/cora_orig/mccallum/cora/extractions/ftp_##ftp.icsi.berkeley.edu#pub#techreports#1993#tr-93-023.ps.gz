URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-023.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: TRAINING AGENTS TO PERFORM SEQUENTIAL BEHAVIOR  
Author: Marco Colombetti Marco Dorigo 
Address: Piazza Leonardo da Vinci, 32, 20133 Milano, Italy  Berkeley, CA 94704,  Milano, Piazza Leonardo da Vinci, 32, 20133 Milano, Italy  
Affiliation: Sistemi Informatici e Calcolo  Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di Milano,  International Computer Science Institute,  and Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di  
Note: This work has been partly supported by the Italian National Research Council, under the "Progetto Finalizzato  Parallelo", subproject 2 "Processori dedicati", and under the "Progetto Finalizzato Robotica", subproject 2 "Tema: ALPI".  
Pubnum: TR-93-023  
Email: (e-mail: colombet@ipmel2.elet.polimi.it).  (e-mail: dorigo@icsi.berkeley.edu).  
Date: September 1993  
Abstract: This paper is concerned with training an agent to perform sequential behavior. In previous work we have been applying reinforcement learning techniques to control a reactive robot. Obviously, a pure reactive system is limited in the kind of interactions it can learn. In particular, it can only learn what we call pseudo-sequences, that is sequences of actions in which the transition signal is generated by the appearance of a sensorial stimulus. We discuss the difference between pseudo-sequences and proper sequences, and the implication that these differences have on training procedures. A result of our research is that, in case of proper sequences, for learning to be successful the agent must have some kind of memory; moreover it is often necessary to let the trainer and the learner communicate. We study therefore the influence of communication on the learning process. First we consider trainer-to-learner communication introducing the concept of reinforcement sensor, which let the learning robot explicitly know whether the last reinforcement was a reward or a punishment; we also show how the use of this sensor induces the creation of a set of error recovery rules. Then we introduce learner-to-trainer communication, which is used to disambiguate indeterminate training situations, that is situations in which observation alone of the learner behavior does not provide the trainer with enough information to decide if the learner is performing a right or a wrong move. All the design choices we make are discussed and compared by means of experiments in a simulated world. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Beer, R. D., </author> <year> 1993. </year> <title> A dynamical systems perspective on autonomous agents. </title> <note> Submitted to Artificial Intelligence </note> . 
Reference: <author> Bertoni, A., & M. Dorigo, </author> <year> 1993. </year> <title> Implicit Parallelism in Genetic Algorithms. </title> <journal> Artificial Intelligence , 61, </journal> <volume> 2, </volume> <pages> 307314. </pages>
Reference: <author> Booker, L., D. E. Goldberg & J. H. Holland, </author> <year> 1989. </year> <title> Classifier Systems and Genetic Algorithms. </title> <booktitle> Artificial Intelligence , 40, </booktitle> <pages> 1-3, 235282. </pages>
Reference-contexts: In our research, which has a strong experimental orientation, we use ALECSYS, a software tool designed by Dorigo (1992). ALECSYS allows one to implement an agent as a network of interconnected modules, each of which is a learning classifier system <ref> (Booker, Goldberg & Holland, 1989) </ref>. The system, which runs in parallel on a network of transputers, has been connected to both simulated agents and physical robots.
Reference: <author> Colombetti, M., & M. Dorigo, </author> <year> 1992. </year> <title> Learning to control an autonomous robot by distributed genetic algorithms. </title> <booktitle> Proceedings of "From Animals To Animats, 2nd International Conference on Simulation of Adaptive Behavior (SAB92), </booktitle> <address> Honolulu, Hawaii, </address> <publisher> MIT Press, </publisher> <pages> 305 312. </pages>
Reference-contexts: The problem of learning sequential behavior has been tackled by Singh (1992) in the context of Q-learning. In this paper, we present a different approach to the problem of learning sequential behavior patterns, viewed as the result of coordinating separately learned basic behaviors <ref> (Colombetti & Dorigo, 1992) </ref>. The work presented here is part of a wider research effort aimed at developing agents capable of complex behavior through both explicit design and machine learning. In our research, which has a strong experimental orientation, we use ALECSYS, a software tool designed by Dorigo (1992). <p> There are different ways in which behavioral modules can be put together to build a learning system. In a recent study we have investigated a number of them <ref> (Dorigo & Colombetti, 1992) </ref>, which we briefly summarize below. Monolithic architecture. In this architecture (see Figure 1) there is only one learning module which is in charge of learning all the behaviors necessary to accomplish the task. <p> Training policies We view supervised reinforcement learning as a mechanism to translate a specification of the agent's target behavior into a control program that realizes it <ref> (Dorigo & Colombetti, 1992) </ref>. As the translation process takes place in the context of agent-environment interactions, the resulting control program is highly sensitive to features of the environment that would be very difficult to model explicitly in a handwritten program (Dorigo & Colombetti, 1993). <p> The problem has been solved by training each CS to turn the Animat when it does not see the relevant object. This training technique and its results have been described elsewhere <ref> (see for example Dorigo & Colombetti, 1992) </ref>. After the basic behaviors have been learned, the next step is to train the Animat's coordinator to generate the target sequence. Before doing so, we have to decide how the transition signal is to be generated.
Reference: <author> Dorigo, M., </author> <year> 1992. </year> <title> Alecsys and the AutonoMouse: Learning to Control a Real Robot by Distributed Classifier Systems. </title> <institution> Technical Report No.92-011 , Politecnico di Milano, Italy. </institution> <note> (To appear in Machine Learning </note> ). 
Reference-contexts: The problem of learning sequential behavior has been tackled by Singh (1992) in the context of Q-learning. In this paper, we present a different approach to the problem of learning sequential behavior patterns, viewed as the result of coordinating separately learned basic behaviors <ref> (Colombetti & Dorigo, 1992) </ref>. The work presented here is part of a wider research effort aimed at developing agents capable of complex behavior through both explicit design and machine learning. In our research, which has a strong experimental orientation, we use ALECSYS, a software tool designed by Dorigo (1992). <p> There are different ways in which behavioral modules can be put together to build a learning system. In a recent study we have investigated a number of them <ref> (Dorigo & Colombetti, 1992) </ref>, which we briefly summarize below. Monolithic architecture. In this architecture (see Figure 1) there is only one learning module which is in charge of learning all the behaviors necessary to accomplish the task. <p> Training policies We view supervised reinforcement learning as a mechanism to translate a specification of the agent's target behavior into a control program that realizes it <ref> (Dorigo & Colombetti, 1992) </ref>. As the translation process takes place in the context of agent-environment interactions, the resulting control program is highly sensitive to features of the environment that would be very difficult to model explicitly in a handwritten program (Dorigo & Colombetti, 1993). <p> The problem has been solved by training each CS to turn the Animat when it does not see the relevant object. This training technique and its results have been described elsewhere <ref> (see for example Dorigo & Colombetti, 1992) </ref>. After the basic behaviors have been learned, the next step is to train the Animat's coordinator to generate the target sequence. Before doing so, we have to decide how the transition signal is to be generated.
Reference: <author> Dorigo, M., </author> <year> 1993. </year> <title> Genetic and nongenetic operators in A LECSYS. </title> <journal> Evolutionary Computation, </journal> <volume> 1, 2, 149162, </volume> <publisher> MIT Press. </publisher>
Reference-contexts: C1 could have learnt to mediate the two proposals in a +20 movement (other solutions are possible in which the two proposed actions are given different weights). environment LCS + environment LCS LCS environment LCS LCS B3B2 B1 environment 9 In ALECSYS , every single module is an enhanced version <ref> (see Dorigo, 1993) </ref> of a learning classifier system (CS) as proposed for example by Booker, Goldberg & Holland (1989). CSs are a rather complex paradigm for reinforcement learning. Functionally, they can be split in three components. <p> Strength is changed by means of redistribution of reinforcements received by the CS as feedback for actions performed. The algorithm we used is an extended version of the bucket brigade (Holland, 1986) which has been presented in details elsewhere <ref> (Dorigo, 1993) </ref>. Since reinforcements are received at each step from an external trainer, ALECSYS is a supervised reinforcement learning system. The third components is the rule discovery subsystem, which in A LECSYS is implemented by means of a genetic algorithm (GA). <p> It works applying in sequence the crossover and the mutation operators (Goldberg, 1989) and returning the modified population (Figure 4). More details about the actual implementation can be found in <ref> (Dorigo, 1993) </ref>. function GA (Pop); Pop &lt;- Crossover (Pop); Pop &lt;- Mutate (Pop); return 4. Experimental settings When planning an experiment, the environment, the agent, and the target behavior must be designed together. <p> As the translation process takes place in the context of agent-environment interactions, the resulting control program is highly sensitive to features of the environment that would be very difficult to model explicitly in a handwritten program <ref> (Dorigo & Colombetti, 1993) </ref>. As usual in the field, reinforcements are provided to our learning agent by a computer program, that we call the reinforcement program (RP): it is the RP that embodies a specification of the target behavior.
Reference: <author> Dorigo M. & M. Colombetti, </author> <year> 1992. </year> <title> Robot Shaping: Developing Situated Agents through Learning. </title> <type> Technical Report No. </type> <institution> 92-040 , International Computer Science Institute, Berkeley, CA. </institution> <note> (Submitted to Artificial Intelligence , August 1992). </note>
Reference-contexts: The problem of learning sequential behavior has been tackled by Singh (1992) in the context of Q-learning. In this paper, we present a different approach to the problem of learning sequential behavior patterns, viewed as the result of coordinating separately learned basic behaviors <ref> (Colombetti & Dorigo, 1992) </ref>. The work presented here is part of a wider research effort aimed at developing agents capable of complex behavior through both explicit design and machine learning. In our research, which has a strong experimental orientation, we use ALECSYS, a software tool designed by Dorigo (1992). <p> There are different ways in which behavioral modules can be put together to build a learning system. In a recent study we have investigated a number of them <ref> (Dorigo & Colombetti, 1992) </ref>, which we briefly summarize below. Monolithic architecture. In this architecture (see Figure 1) there is only one learning module which is in charge of learning all the behaviors necessary to accomplish the task. <p> Training policies We view supervised reinforcement learning as a mechanism to translate a specification of the agent's target behavior into a control program that realizes it <ref> (Dorigo & Colombetti, 1992) </ref>. As the translation process takes place in the context of agent-environment interactions, the resulting control program is highly sensitive to features of the environment that would be very difficult to model explicitly in a handwritten program (Dorigo & Colombetti, 1993). <p> The problem has been solved by training each CS to turn the Animat when it does not see the relevant object. This training technique and its results have been described elsewhere <ref> (see for example Dorigo & Colombetti, 1992) </ref>. After the basic behaviors have been learned, the next step is to train the Animat's coordinator to generate the target sequence. Before doing so, we have to decide how the transition signal is to be generated.
Reference: <author> Dorigo M. & M. Colombetti, </author> <year> 1993. </year> <title> Design and development of autonomous robots by reinforcement learning (in preparation). </title>
Reference-contexts: C1 could have learnt to mediate the two proposals in a +20 movement (other solutions are possible in which the two proposed actions are given different weights). environment LCS + environment LCS LCS environment LCS LCS B3B2 B1 environment 9 In ALECSYS , every single module is an enhanced version <ref> (see Dorigo, 1993) </ref> of a learning classifier system (CS) as proposed for example by Booker, Goldberg & Holland (1989). CSs are a rather complex paradigm for reinforcement learning. Functionally, they can be split in three components. <p> Strength is changed by means of redistribution of reinforcements received by the CS as feedback for actions performed. The algorithm we used is an extended version of the bucket brigade (Holland, 1986) which has been presented in details elsewhere <ref> (Dorigo, 1993) </ref>. Since reinforcements are received at each step from an external trainer, ALECSYS is a supervised reinforcement learning system. The third components is the rule discovery subsystem, which in A LECSYS is implemented by means of a genetic algorithm (GA). <p> It works applying in sequence the crossover and the mutation operators (Goldberg, 1989) and returning the modified population (Figure 4). More details about the actual implementation can be found in <ref> (Dorigo, 1993) </ref>. function GA (Pop); Pop &lt;- Crossover (Pop); Pop &lt;- Mutate (Pop); return 4. Experimental settings When planning an experiment, the environment, the agent, and the target behavior must be designed together. <p> As the translation process takes place in the context of agent-environment interactions, the resulting control program is highly sensitive to features of the environment that would be very difficult to model explicitly in a handwritten program <ref> (Dorigo & Colombetti, 1993) </ref>. As usual in the field, reinforcements are provided to our learning agent by a computer program, that we call the reinforcement program (RP): it is the RP that embodies a specification of the target behavior.
Reference: <author> Goldberg, D.E., </author> <year> 1989. </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: In our research, which has a strong experimental orientation, we use ALECSYS, a software tool designed by Dorigo (1992). ALECSYS allows one to implement an agent as a network of interconnected modules, each of which is a learning classifier system <ref> (Booker, Goldberg & Holland, 1989) </ref>. The system, which runs in parallel on a network of transputers, has been connected to both simulated agents and physical robots. <p> It works applying in sequence the crossover and the mutation operators <ref> (Goldberg, 1989) </ref> and returning the modified population (Figure 4). More details about the actual implementation can be found in (Dorigo, 1993). function GA (Pop); Pop &lt;- Crossover (Pop); Pop &lt;- Mutate (Pop); return 4.
Reference: <author> Holland, J. H., </author> <year> 1975. </year> <title> Adaptation in natural and artificial systems, </title> <institution> The University of Michigan Press, Ann Arbor, Michigan. </institution>
Reference: <author> Holland, J. H., </author> <year> 1986. </year> <title> Escaping Brittleness: The Possibilities of General Purpose Learning Algorithms Applied to Parallel Rule-based Systems, </title> <editor> in R.S.Michalski, J.G.Carbonell, & T.M.Mitchell (Eds.), </editor> <title> Machine Learning II , Morgan Kaufmann. </title> <note> 28 Lin, </note> <author> L.-J., & T. M. Mitchell, </author> <year> 1992. </year> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> School of Computer Science, Carnagie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Strength is changed by means of redistribution of reinforcements received by the CS as feedback for actions performed. The algorithm we used is an extended version of the bucket brigade <ref> (Holland, 1986) </ref> which has been presented in details elsewhere (Dorigo, 1993). Since reinforcements are received at each step from an external trainer, ALECSYS is a supervised reinforcement learning system. The third components is the rule discovery subsystem, which in A LECSYS is implemented by means of a genetic algorithm (GA).
Reference: <author> Littman, </author> <title> M.L., 1992. An optimization-based categorization of reinforcement learning environments. </title> <booktitle> Proceedings of "From Animals To Animats", 2nd International Conference on Simulation of Adaptive Behavior (SAB92), </booktitle> <address> Honolulu, Hawaii, </address> <publisher> MIT Press, </publisher> <pages> 262270. </pages>
Reference: <author> Mahadevan, S., & J. Connell, </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning, </title> <journal> Artificial Intelligence , 55, </journal> <volume> 2, </volume> <pages> 311365. </pages>
Reference: <author> McCluskey, E.J., </author> <year> 1986. </year> <title> Logic Design Principles. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: We can associate state 0 to 1 In automata theory, this definition corresponds to a class of automata known as Mealy machines <ref> (McCluskey, 1986) </ref>. 5 the box is empty, and state 1 to the gift is in the box. Clearly, the state must switch from 0 to 1 when the agent puts the gift into the box.
Reference: <editor> Riolo R.L., </editor> <year> 1989. </year> <title> The emergence of coupled sequences of classifiers. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <editor> J.D. Schaffer (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> 256264. </pages>
Reference-contexts: Conclusions In this paper we have presented an approach to training an agent which learns proper behavioral sequences. Many other researchers have tackled the problem of learning sequences of actions in the realm of classifier systems <ref> (e.g., Riolo, 1989) </ref>. Our work differentiate itself in that the building blocks of our sequences are elementary behaviors instead of simple actions.
Reference: <author> Rosenschein, S. J., & L. P. Kaelbling, </author> <year> 1986. </year> <title> The synthesis of digital machines with provable epistemic properties. </title> <editor> In J. Halpern, ed., </editor> <booktitle> Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning about Knowledge , Morgan Kaufmann, </booktitle> <address> Los Altos, CA, </address> <month> 8398. </month>
Reference: <author> Singh, S. P., </author> <year> 1992. </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <booktitle> Machine Learning , 8, </booktitle> <pages> 3-4, 323339. </pages>
Reference: <author> Spiessens, P., & B. Manderick, </author> <year> 1991. </year> <title> A massively parallel genetic algorithm: Implementation and first analysis. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <publisher> Morkan Kaufmann, </publisher> <pages> 279286. </pages>
Reference-contexts: New rules, which overwrite low strength rules in the population, are tested and retained in case they demonstrate their utility to the learning system performance. The main strengths of GAs are that: They can be easily implemented on a parallel computer <ref> (e.g., see Spiessens & Manderick, 1991) </ref>. They are very efficient in recombining rule components, favoring the reproduction, and therefore the survival, of those components which are more often contained in rules with a higher than average strength.
Reference: <author> Watkins, C.J.C.H., </author> <year> 1989. </year> <title> Learning with delayed rewards . Ph. </title> <address> D. </address> <institution> dissertation, Psychology Department, University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins, C.J.C.H., & P. Dayan, </author> <year> 1992. </year> <title> Technical Note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 3-4, 279292. </pages>
Reference: <author> Whitehead, S. D., & L. J. Lin, </author> <year> 1993. </year> <title> Reinforcement learning in non-Markov environments. </title> <note> Submitted to Artificial Intelligence </note> . 
Reference: <author> Wilson, S., </author> <year> 1987. </year> <title> Classifier systems and the Animat problem. </title> <journal> Machine Learning, </journal> <volume> 2, 3, </volume> <year> 199228. </year>
Reference-contexts: a circular area of predefined radium around the object (shown by the dashed circles in the figure). 11 B 4.2 The agents body The agent is a simulation of a simple mobile robot, which is intended to play the role of an artificial organism, and is thus called the Animat <ref> (see Wilson, 1987) </ref>. The Animat's sensors are two on/off eyes with limited visual field of 180 and an on/off microphone. The eyes are able to detect the presence of an object in their visual fields, and can discriminate between the two objects A and B.
Reference: <author> Wilson, S., </author> <year> 1990. </year> <title> The Animat path to AI. </title> <booktitle> Proceedings of "From Animals To Animats, 1st International Conference on the Simulation of Adaptive Behavior (SAB90), </booktitle> <address> Cambridge, MA, </address> <publisher> MIT Press, </publisher> <pages> 1521. </pages>
References-found: 23

