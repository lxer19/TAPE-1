URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1495.ps.Z
Refering-URL: http://www.ai.mit.edu/people/maja/maja.html
Root-URL: 
Title: Interaction and Intelligent Behavior  
Author: by Maja J Mataric Rodney A. Brooks 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  Professor of Computer Science and Engineering Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Committee on Graduate Students  
Date: May 1994  May 12, 1994  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1994.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: <author> Abraham, R. H. & Shaw, C. D. </author> <year> (1992), </year> <title> Dynamics: The Geometry of Behavior, </title> <publisher> Addison-Wesley, </publisher> <address> California. </address>
Reference-contexts: This may not appear to be a problem, as most researchers would be satisfied with knowing the system's global, qualitative behavior. Global behavior, however, is generally defined in quantitative terms from which qualitative descriptions are derived, whether it be on the microscopic scale of particle interactions <ref> (Abraham & Shaw 1992) </ref> or on the macroscopic scale of building maps (Chatila & Laumond 1985) of the environment. The path to a qualitative description of a system is indirect, requiring abstracting away the details or through clustering analytical, quantitative information.
Reference: <author> Abravanel, E. & Gingold, H. </author> <year> (1985), </year> <title> `Learning Via Observation During the Second Year of Life', </title> <booktitle> Developmental Psychology 21(4), </booktitle> <pages> 614-623. </pages>
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1987), </year> <title> Pengi: An Implementation of a Theory of Activity, </title> <booktitle> in `Proceedings, AAAI-87', </booktitle> <address> Seattle, WA, </address> <pages> pp. 268-272. </pages>
Reference: <author> Agre, P. E. & Chapman, D. </author> <year> (1990), </year> <title> What Are Plans for?, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back', </title> <publisher> The MIT Press, </publisher> <pages> pp. 17-34. </pages>
Reference-contexts: Most models to date have bypassed continuous state by presuming higher-level sensory operators such as "I see a chair in front of me." But such operators have been shown to be unrealistic and largely unimplementable in systems using physical sensors <ref> (Agre & Chapman 1990, Brooks & Mataric 1993) </ref>. In general, the problem of partitioning continuous state into discrete states is hard (Kosecka 1992), and even if a reasonable partitioning of the world is found, there may be no mapping from the space of sensor readings to this partitioning.
Reference: <author> Altenburg, K. </author> <year> (1994), </year> <title> Adaptive Resource Allocation for a Multiple Mobile Robot System using Communication, </title> <type> Technical Report NDSU-CSOR-TR-9404, </type> <institution> North Dakota State Univeristy. </institution>
Reference: <author> Altenburg, K. & Pavicic, M. </author> <year> (1993), </year> <title> Initial Results in the Use of Inter-Robot Communication for a Multiple, Mobile Robotic System, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 96-100. </pages>
Reference: <author> Arkin, R. C. </author> <year> (1989), </year> <title> Towards the Unification of Navigational Planning and Reactive Control, </title> <booktitle> in `AAAI Spring Symposium on Robot Navigation', </booktitle> <pages> pp. 1-5. </pages>
Reference-contexts: It includes reactive planning or reactive execution used in Reac- tive Action Packages (RAPs), higher-level primitives for planning which hide and take care of the details of execution (Firby 1987), and PRS (Procedural Reason- ing System), an architecture for flexible control rule invocation (Georgeff & Lansky 1987), Schemas <ref> (Arkin 1989) </ref>, and several others (Payton 1990, Connell 1991). These systems tend to separate the control system into two or more communicating but otherwise independent parts.
Reference: <author> Arkin, R. C. </author> <year> (1992), </year> <title> `Cooperation without Communication: Multiagent Schema Based Robot Navigation', </title> <journal> Journal of Robotic Systems. </journal>
Reference: <author> Arkin, R. C., Balch, T. & Nitz, E. </author> <year> (1993), </year> <title> Communication of Behavioral State in Multi-agent Retrieval Tasks, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <pages> pp. 588-594. </pages>
Reference: <author> Asendorpf, J. B. & Baudonniere, P.-M. </author> <year> (1993), </year> <editor> `Self-Awareness and Other-Awareness: Mirror Self-Recognition Synchronic Imitation Among Unfamiliar Peers', </editor> <booktitle> Developmental Psychology 29(1), </booktitle> <pages> 89-95. </pages> <note> 159 Assad, </note> <author> A. & Packard, N. </author> <year> (1992), </year> <title> Emergent Colonization in an Artificial Ecology, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 143-152. </pages>
Reference-contexts: Prior to this stage, occurring around the age of two, children are incapable of separating the internal and external perception of the world (Piaget 1962, Bandura & Walters 1963, Bandura 1971). Even after achieving self-awareness, as determined with the typical dot-and-mirror test <ref> (Asendorpf & Baudonniere 1993) </ref>, around the age of two, children require a number of years before reaching the adult ability to form theories of mind (Bandura 1977, Abravanel & Gingold 1985). Much research has been aimed at testing whether primates have theories of mind.
Reference: <author> Atkeson, C. G. </author> <year> (1989), </year> <title> `Learning Arm Kinematics and Dynamics', </title> <booktitle> Annual Review of Neuroscience 12, </booktitle> <pages> 157-183. </pages>
Reference-contexts: In the case of motor control, the behaviors are designed for specific optimizations, such as minimizing effort by minimizing jerk, executing straight line trajectories, and using bell-shaped velocity profiles <ref> (Atkeson 1989) </ref>. Taking the idea from motor control, we define behaviors as control laws that encapsulate sets of constraints so as to achieve particular goals.
Reference: <author> Atkeson, C. G. </author> <year> (1990), </year> <title> Memory-Based Approaches to Approximating Continuous Functions, </title> <booktitle> in `Proceedings, Sixth Yale Workshop on Adaptive and Learning Systems'. </booktitle>
Reference: <author> Atkeson, C. G., Aboaf, E. W., McIntyre, J. & Reinkensmeyer, D. J. </author> <year> (1988), </year> <title> ModelBased Robot Learning, </title> <type> Technical Report AIM-1024, </type> <institution> MIT. </institution>
Reference: <author> Axelrod, R. </author> <year> (1984), </year> <title> The Evolution of Cooperation, </title> <publisher> Basic Books, </publisher> <address> New York. </address>
Reference-contexts: However, since the connection between individual and collective benefit is rarely direct, societies can harbor deserters who disobey social rules in favor of individual benefit. Game theory offers elaborate studies of the effects of deserters on individual optimality <ref> (Axelrod 1984) </ref>, but domains 6 The problem of maintaining internal models or so called theories of mind is discussed in detail in section 2.3.5. 7 In cultural contexts global efficiency is sometimes elevated to "the common good." 32 treated in game theory are much more cleanly constrained than those treated here.
Reference: <author> Bandura, A. </author> <year> (1971), </year> <title> Analysis of Modeling Processes, </title> <editor> in A. Bandura, ed., </editor> <booktitle> `Psychological Modeling: Conflicting Theories', Aldine-Atherton, </booktitle> <pages> pp. 1-62. </pages>
Reference: <author> Bandura, A. </author> <year> (1977), </year> <title> Social Learning Theory, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: Even after achieving self-awareness, as determined with the typical dot-and-mirror test (Asendorpf & Baudonniere 1993), around the age of two, children require a number of years before reaching the adult ability to form theories of mind <ref> (Bandura 1977, Abravanel & Gingold 1985) </ref>. Much research has been aimed at testing whether primates have theories of mind.
Reference: <author> Bandura, A. & Walters, R. H. </author> <year> (1963), </year> <title> Social Learning and Personality Development, </title> <publisher> Holt, Rinehart and Winston, Inc, </publisher> <address> New York. </address>
Reference: <author> Barman, R. A., Kingdon, J. J., Mackworth, A. K., Pai, D. K., Sahota, M. K., Wilkinson, H. & Zhang, Y. </author> <year> (1993), </year> <title> Dynamite: A Testbed for Multiple Mobile Robots, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 38-44. </pages>
Reference: <author> Barto, A. G. </author> <year> (1990), </year> <title> Some Learning Tasks from a Control Perspective, </title> <type> Technical Report COINS TR 90-122, </type> <institution> University of Massachusetts. </institution>
Reference-contexts: The goal of the learning system is to maximize positive reinforcement (reward) and/or minimize negative reinforcement (punishment) over time. Traditionally, the learner is given no explicit built-in knowledge about the task. If the learner receives no direct instruction or answers from the environment the learning is considered unsupervised <ref> (Barto 1990) </ref>. The learner produces a mapping of states to actions called a policy. 110 Reinforcement learning originated in Ivan Pavlov's classical conditioning experi-ments (Gleitman 1981). Embraced by behaviorism, stimulus-response learning became the predominant methodology for studying animal behavior in psychology and biology.
Reference: <author> Barto, A. G., Bradtke, S. J. & Singh, S. P. </author> <year> (1993), </year> <title> `Learning to Act using Real-Time Dynamic Programming', </title> <journal> AI Journal. </journal>
Reference: <author> Barto, A. G., Sutton, R. S. & Anderson, C. W. </author> <year> (1983), </year> <title> `Neuronlike Elements That Can Solve Difficult Learning Control Problems', </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 13, </journal> <pages> 835-846. </pages>
Reference-contexts: Furthermore, the concept of behavior contains informal notions about generality and adaptivity that are difficult to state precisely without domain-specific grounding. Consequently, most learning control problems appear to be instances of behavior learning, such as learning to balance a pole <ref> (Barto, Sutton & Anderson 1983) </ref>, to play billiards (Moore 1992), and to juggle (Schaal & Atkeson 1994). Furthermore, work on action selection, deciding what action to make in each state, can be viewed as learning a higher-level behavior as an abstraction on the state-action space.
Reference: <author> Beckers, R., Holland, O. E. & Deneubourg, J. L. </author> <year> (1994), </year> <title> From Local Actions to Global Tasks: Stigmergy and Collective Robotics, </title> <editor> in R. Brooks & P. Maes, eds, </editor> <booktitle> 160 `Artificial Life IV, Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems', </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Belic, M. R., Skarka, V., Deneubourg, J. L. & Lax, M. </author> <year> (1986), </year> <title> `Mathematical Model of Honeycomb Construction', </title> <booktitle> Mathematical Biology 24, </booktitle> <pages> 437-449. </pages>
Reference: <author> Bellman, R. E. </author> <year> (1957), </year> <title> Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: However it may be structured, the learner's space in traditional RL is exponential in the size of the input, and thus marred by the curse of dimensionality <ref> (Bellman 1957) </ref>. Some form of input generalization, or collapsing of states into functional equivalence classes, is necessary for almost all problems. Human programmers perform generalization implicitly whenever they use clever orderings of rules, careful arbitration, and default conditions, in crafting control strategies.
Reference: <author> Benhamou, S. & Bovet, P. </author> <year> (1990), </year> <title> Modeling and Simulation of Animal's Movements, </title> <editor> in J. A. Meyer & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Beni, G. & Hackwood, S. </author> <year> (1992), </year> <title> The Maximum Entropy Principle and Sensing in Swarm Intelligence, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 153-160. </pages>
Reference: <author> Bizzi, E. & Mussa-Ivaldi, F. A. </author> <year> (1990), </year> <title> Muscle Properties and the Control of Arm Movement, </title> <editor> in D. N. Osherson, S. Kosslyn & J. M. Hollerbach, eds, </editor> <booktitle> `Visual Cognition and Action, </booktitle> <volume> Vol. 2', </volume> <publisher> The MIT Press, </publisher> <pages> pp. 213-242. </pages>
Reference: <author> Bizzi, E., Mussa-Ivaldi, F. A. & Giszter, S. </author> <year> (1991), </year> <title> `Computations Underlying the Execution of Movement: A Biological Perspective', </title> <booktitle> Science 253, </booktitle> <pages> 287-291. </pages>
Reference: <author> Bonabeau, E. W. </author> <year> (1993), </year> <title> On the appease and dangers of synthetic reductionism, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 86-10. </pages>
Reference: <author> Brock, D. L., Montana, D. J. & Ceranowicz, A. Z. </author> <year> (1992), </year> <title> Coordination and Control of Multiple Autonomous Vehicles, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <pages> pp. 2725-2730. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1986), </year> <title> `A Robust Layered Control System for a Mobile Robot', </title> <journal> IEEE Journal of Robotics and Automation RA-2, </journal> <pages> 14-23. </pages>
Reference-contexts: Per-- haps the most prominent are purely reactive bottom-up approaches which implement the agent's control strategy as a collection of preprogrammed condition-action pairs with minimal state <ref> (Brooks & Connell 1986, Agre & Chapman 1987, Connell 1990) </ref>. These systems maintain no internal models and perform no search, but simply lookup and command the appropriate action for each set of sensor readings. They rely on a direct coupling between sensing and action, and fast feedback from the environment. <p> In most cases, the low-level reactive process takes care of the immediate safety of the robot, while the higher level uses the planner to select action sequences. Behavior-based approaches are an extension of reactive systems that also fall between the purely reactive and the planner-based extremes <ref> (Brooks 1986, Maes 1989) </ref>. Although often confused in the literature, behavior-based strategies are strictly more powerful than purely reactive approaches since they have no fundamental limitations on internal state.
Reference: <author> Brooks, R. A. </author> <year> (1990a), </year> <title> The Behavior Language; User's Guide, </title> <type> Technical Report AIM1127, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: The control systems are programmed in the Behavior Language, a parallel programming language based on the Subsumption Architecture <ref> (Brooks 1990a) </ref>. 1 The IRs are all the same frequency and mechanically positioned for obstacle detection rather than communication. 53 4.2.4 Hardware Limitations Properties of physical hardware impose restrictions not only on the control strategies that can be applied, but alson on the types of tasks and experiments that can be <p> Inter-robot communication consists of broadcasting 6-byte messages at the rate of 1 Hz. In the experiments described here, the radios are used to determine the presence of other nearby robots. As in the first set of robot experiments, the robots are programmed in the Behavior Language <ref> (Brooks 1990a) </ref>. 8.2 The Learning Task The learning task consists of finding a mapping of all conditions and behaviors into the most effective policy for group foraging.
Reference: <author> Brooks, R. A. </author> <year> (1990b), </year> <title> Elephants Don't Play Chess, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents', </title> <publisher> The MIT Press, </publisher> <pages> pp. 3-15. </pages> <note> 161 Brooks, </note> <author> R. A. </author> <year> (1991a), </year> <title> Artificial Life and Real Robots, in `Toward A Practice of Au-tonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: However, uncertainly in sensing and action and changes in the environment can require frequent replanning the cost of which may be prohibitive for complex systems. Planner-based approaches have been criticized for scaling poorly with the complexity of the problem and consequently not allowing for reaction in real-time <ref> (Brooks 1990b, Brooks 1991c) </ref>. 2 The world may or may not be physical. 24 Various attempts at achieving real-time performance have been proposed.
Reference: <author> Brooks, R. A. </author> <year> (1991b), </year> <title> Intelligence Without Reason, </title> <booktitle> in `Proceedings, IJCAI-91'. </booktitle>
Reference: <author> Brooks, R. A. </author> <year> (1991c), </year> <title> `Intelligence Without Representation', </title> <booktitle> Artificial Intelligence 47, </booktitle> <pages> 139-160. </pages>
Reference-contexts: Physical environments pose a great challenge as they usually do not contain the structure, determinism, and thus predictability usually required for formal analysis <ref> (Brooks 1991c, Brooks 1991b) </ref>. Predicting the behavior of a multi- agent system is more complex than the single-agent case.
Reference: <author> Brooks, R. A. & Connell, J. H. </author> <year> (1986), </year> <title> Asynchronous Distributed Control System for a Mobile Robot, </title> <booktitle> in `SPIE', </booktitle> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Per-- haps the most prominent are purely reactive bottom-up approaches which implement the agent's control strategy as a collection of preprogrammed condition-action pairs with minimal state <ref> (Brooks & Connell 1986, Agre & Chapman 1987, Connell 1990) </ref>. These systems maintain no internal models and perform no search, but simply lookup and command the appropriate action for each set of sensor readings. They rely on a direct coupling between sensing and action, and fast feedback from the environment. <p> In most cases, the low-level reactive process takes care of the immediate safety of the robot, while the higher level uses the planner to select action sequences. Behavior-based approaches are an extension of reactive systems that also fall between the purely reactive and the planner-based extremes <ref> (Brooks 1986, Maes 1989) </ref>. Although often confused in the literature, behavior-based strategies are strictly more powerful than purely reactive approaches since they have no fundamental limitations on internal state.
Reference: <author> Brooks, R. A. & Mataric, M. J. </author> <year> (1993), </year> <title> Real Robots, Real Learning Problems, in `Robot Learning', </title> <publisher> Kluwer Academic Press, </publisher> <pages> pp. 193-213. </pages>
Reference-contexts: However, none of the above learning approaches can be said to learn new behaviors according to the precise definition of the problem. The posed "behavior learning problem" <ref> (Brooks & Mataric 1993) </ref> requires that the agent acquire a new behavior using its own perceptual and effector systems, as well as to assign some semantic label to the behavior, in order to later recognize and use it as a coherent and independent unit.
Reference: <author> Brooks, R. A., Maes, P., Mataric, M. J. & Moore, G. </author> <year> (1990), </year> <title> Lunar Base Construction Robots, </title> <booktitle> in `IEEE International Workshop on Intelligent Robots and Systems (IROS-90)', Tokyo, </booktitle> <pages> pp. 389-392. </pages>
Reference-contexts: Most models to date have bypassed continuous state by presuming higher-level sensory operators such as "I see a chair in front of me." But such operators have been shown to be unrealistic and largely unimplementable in systems using physical sensors <ref> (Agre & Chapman 1990, Brooks & Mataric 1993) </ref>. In general, the problem of partitioning continuous state into discrete states is hard (Kosecka 1992), and even if a reasonable partitioning of the world is found, there may be no mapping from the space of sensor readings to this partitioning. <p> Such concurrent multi-modal learning is biologically and pragmatically inspired, and has been an ongoing challenge in the learning community <ref> (Franklin & Selfridge 1990, Brooks & Mataric 1993) </ref>. In our foraging task, basic behaviors were designed by hand and behavior selection was learned. However, basic behaviors themselves could be learned or optimized in parallel with learning behavior selection.
Reference: <author> Brown, T. A. & McBurnett, M. </author> <year> (1993), </year> <title> Political Life on a Lattice, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 113-126. </pages>
Reference: <author> Calenbuhr, V. & Deneubourg, J. L. </author> <year> (1992), </year> <title> `A Model for Osmotropotactic Orientation (I)', </title> <journal> Journal of Theoretical Biology. </journal>
Reference-contexts: This approach to following models tropotaxic behavior in biology, in which two sensory organs are stimulated and the difference between the stimuli determines the motion of the insect (McFarland 1987). Ant osmotropotaxis is based on the differential in pheromone intensity perceived by the left and right antennae <ref> (Calenbuhr & Deneubourg 1992) </ref>, while the agents described here use the binary state of the two directional IR sensors. Under conditions of sufficient density, safe-wandering and following can produce more complex global behaviors.
Reference: <author> Caloud, P., Choi, W., Latombe, J., LePape, C. & Yim, M. </author> <year> (1990), </year> <title> Indoor Automation with Many Mobile Robots, </title> <booktitle> in `IROS-90', Tsuchiura, Japan, </booktitle> <pages> pp. 67-72. </pages>
Reference: <author> Camazine, S. </author> <year> (1993), </year> <title> Collective Intelligence in Insect Colonies by Means of SelfOrganization, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 158-173. </pages>
Reference: <author> Canny, J. F. </author> <year> (1988), </year> <title> The Complexity of Robot Motion Planning, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Cassandra, A. R., Kaelbling, L. P. & Littman, M. L. </author> <year> (1994), </year> <title> Acting Optimally in Partially Observable Stochastic Domains, </title> <booktitle> in `Proceedings, AAAI-94', </booktitle> <address> Seattle, Washington. </address>
Reference-contexts: Partial observ-- ability is added into a Markov model by introducing a discrete probability distribution over a set of possible observations for a given state. POMDPs have been studied and successfully applied to theoretical learners <ref> (Cassandra, Kaelbling & Littman 1994) </ref>, but have not yet been used empirically largely due to the fact that observability models of situated systems are not generally available.
Reference: <author> Chapman, D. </author> <year> (1987), </year> <title> `Planning for Conjunctive Goals', </title> <journal> Aritifical Intelligence 32, </journal> <volume> 333377. </volume>
Reference: <author> Chapman, D. & Kaelbling, L. P. </author> <year> (1991), </year> <title> Input Generalization in Delayed Reinforce-ment Learning: An Algorithm and Performance Comparisons, </title> <booktitle> in `Proceedings, IJCAI-91', </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Chase, I. D. </author> <year> (1982), </year> <title> `Dynamics of Hierarchy Formation: The Sequential Development of Dominance Relationships', </title> <booktitle> Behaviour 80, </booktitle> <pages> 218-240. </pages>
Reference: <author> Chase, I. D. </author> <year> (1993), </year> <title> Generating Societies: Collective Social Patterns in Humans and Animals, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 174-191. </pages>
Reference: <author> Chase, I. D. & Rohwer, S. </author> <year> (1987), </year> <title> `Two Methods for Quantifying the Development of Dominance Hierarchies in Large Groups with Application to Harris' Sparrows', Animal Behavior. </title>
Reference: <author> Chase, I. D., Bartolomeo, C. & Dugatkin, L. A. </author> <year> (1994), </year> <title> `Aggressive interactions and inter-contest interval: how long do winners keep winning?', Animal Behavior, </title> <publisher> in press. </publisher>
Reference: <author> Chatila, R. & Laumond, J.-C. </author> <year> (1985), </year> <title> Position Referencing and Consistent World Modeling for Mobile Robots, </title> <booktitle> in `IEEE International Conference on Robotics and Automation'. </booktitle>
Reference-contexts: Global behavior, however, is generally defined in quantitative terms from which qualitative descriptions are derived, whether it be on the microscopic scale of particle interactions (Abraham & Shaw 1992) or on the macroscopic scale of building maps <ref> (Chatila & Laumond 1985) </ref> of the environment. The path to a qualitative description of a system is indirect, requiring abstracting away the details or through clustering analytical, quantitative information. A qualitative description is a collection of non-analytic symbols (i.e., words instead of numbers) with complicated associated semantics.
Reference: <author> Cheney, D. L. & Seyfarth, R. M. </author> <year> (1990), </year> <title> How Monkeys See the World, </title> <publisher> The University of Chicago Press, Chicago. </publisher>
Reference-contexts: Much research has been aimed at testing whether primates have theories of mind. It has recently been demonstrated that certain species of monkeys, while involved in complex social and cooperative interactions, apparently do not form theories of mind 21 at all <ref> (Cheney & Seyfarth 1990, Cheney & Seyfarth 1991) </ref>. In contrast, chimps appear to have more complex abilities and are indeed able to infer goals of their conspecifics (Cheney & Seyfarth 1990, McFarland 1987). <p> In contrast, chimps appear to have more complex abilities and are indeed able to infer goals of their conspecifics <ref> (Cheney & Seyfarth 1990, McFarland 1987) </ref>. How the internal models are represented and whether they are based on explicit or internal representations, remains open for further study (Gomez 1991). An Alternative to the Theory of Mind Exploring the existence and limits of theory of mind in biology is difficult. <p> The results demonstrate that, particularly in homogeneous groups, significant amount of information about an individual's goals is reflected in the observable external state and behavior, and can be obtained with no direct communication <ref> (Cheney & Seyfarth 1990) </ref>. Consequently, a theory of mind is not necessary for a broad spectrum of behaviors, nor is direct communication.
Reference: <author> Cheney, D. L. & Seyfarth, R. M. </author> <year> (1991), </year> <title> Reading Minds or Reading Behaviour?, </title> <editor> in A. Whiten, ed., </editor> <title> `Natural Theories of Mind', </title> <publisher> Basil Blackwell. </publisher>
Reference: <author> Clearwater, S. H., Huberman, B. A. & Hogg, T. </author> <year> (1991), </year> <title> `Cooperative Solution of Constraint Satisfaction Problems', </title> <booktitle> Science 254, </booktitle> <pages> 1181-1183. </pages>
Reference: <author> Cliff, D., Husbands, P. & Harvey, I. </author> <year> (1993), </year> <title> Evolving Visually Guided Robots, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 374-383. </pages>
Reference: <author> Colorni, A., Dorigo, M. & Maniezzo, V. </author> <year> (1992), </year> <title> Distributed Optimization by Ant Colonies, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 134-142. </pages>
Reference: <author> Connell, J. H. </author> <year> (1990), </year> <title> Minimalist Mobile Robotics: A Colony Architecture for an Artificial Creature, </title> <publisher> Academic Press. 163 Connell, </publisher> <editor> J. H. </editor> <year> (1991), </year> <title> SSS: A Hybrid Architecture Applied to Robot Navigation, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', Nice, France, </booktitle> <pages> pp. 2719-2724. </pages>
Reference-contexts: planning or reactive execution used in Reac- tive Action Packages (RAPs), higher-level primitives for planning which hide and take care of the details of execution (Firby 1987), and PRS (Procedural Reason- ing System), an architecture for flexible control rule invocation (Georgeff & Lansky 1987), Schemas (Arkin 1989), and several others <ref> (Payton 1990, Connell 1991) </ref>. These systems tend to separate the control system into two or more communicating but otherwise independent parts. In most cases, the low-level reactive process takes care of the immediate safety of the robot, while the higher level uses the planner to select action sequences.
Reference: <author> Corbara, B., Drogoul, A., Fresneau, D. & Lalande, S. </author> <year> (1993), </year> <title> Simulating the Sociogenesis Process in Ant Colonies with MANTA, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 224-235. </pages>
Reference: <author> Dallas, J. </author> <year> (1990), </year> <title> Co-operative Search Behavior in a Group of Lego Robots, </title> <type> Master's thesis, </type> <institution> University of Edinburgh. </institution>
Reference: <author> Dario, P. & Rucci, M. </author> <year> (1993), </year> <title> An Approach to Disassembly Problem in Robotics, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 460-468. </pages>
Reference: <author> Dario, P., Ribechini, F., Genovese, V. & Sandini, G. </author> <year> (1991), </year> <title> Instinctive Behaviors and Personalities in Societies of Cellular Robots, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <pages> pp. 1927-1932. </pages>
Reference: <author> Darley, V. </author> <year> (1994), </year> <title> Emergent Phenomena and Complexity, </title> <editor> in R. Brooks & P. Maes, eds, </editor> <booktitle> `Artificial Life IV, Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems', </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: 29 approach level of description complex dynamics microscopic & continuous &lt;?&gt; macroscopic & quasi-continuous state spaces macroscopic & discrete Table 2.2: A desirable level of system description for control and analysis lies between the commonly employed ends of the spectrum. current tools for predictive analysis, and require simulation for prediction <ref> (Darley 1994) </ref>. In order to structure and simplify this process of experimental behavior design, this work will provide a set of basic group behaviors and methods for synthesizing them from local rules.
Reference: <author> DeAngelis, D. L., Post, W. M. & Travis, C. C. </author> <year> (1986), </year> <title> `Positive Feedback in Natural Systems', </title> <publisher> Biomathematics. </publisher>
Reference: <author> Decker, K. & Lesser, V. </author> <year> (1993a), </year> <title> A One-shot Dynamics Coordination Algorithm for Distributed Sensor Networks, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 210-216. </pages>
Reference: <author> Decker, K. & Lesser, V. </author> <year> (1993b), </year> <title> Quantitative Modeling of Complex Computational Task Environments, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 217-224. </pages>
Reference: <author> Deneubourg, J.-L. & Goss, S. </author> <year> (1989), </year> <title> Collective Patterns and Decision-Making, </title> <booktitle> in `Ethology, Ecology and Evolution 1', </booktitle> <pages> pp. 295-311. </pages>
Reference: <author> Deneubourg, J. L., Aron, S., Goss, S., Pasteels, J. M. & Duernick, G. </author> <year> (1986), </year> <title> `Random Behaviour, Amplification Processes and Number of Participants: How they Contribute to the Foraging Properties of Ants', </title> <journal> Physica 22D pp. </journal> <pages> 176-186. </pages>
Reference: <author> Deneubourg, J. L., Goss, S., Franks, N., Sendova-Franks, A., Detrain, C. & Chretien, L. </author> <year> (1990), </year> <title> The Dynamics of Collective Sorting, </title> <booktitle> in `From Animals to Animats: 164 International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 356-363. </pages>
Reference-contexts: In contrast, nature abounds with complex systems whose global behavior results from precisely the type of interactions that current research methodologies try to avoid. These effects can be found at all scales, from the subatomic (Gutzwiller 1992), to the semantic (Minsky 1986), to the social <ref> (Deneubourg, Goss, Franks, Sendova-Franks, Detrain & Chretien 1990) </ref>. Situated behavior is based on the interaction with, and thus feedback from, the 27 environment and other agents. Both negative and positive feedback are relevant.
Reference: <author> Deneubourg, J. L., Goss, S., Pasteels, J. M., Fresneau, D. & Lachaud, J. P. </author> <year> (1987), </year> <title> `Self-Organization Mechanisms in Ant Societies, II: Learning in Foraging and Division of Labor', From Individual to Collective Behavior in Social Insects 54, </title> <type> 177196. </type>
Reference: <author> Deneubourg, J. L., Theraulax, G. & Beckers, R. </author> <year> (1992), </year> <title> Swarm-Made Architectures, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 123-133. </pages>
Reference-contexts: This approach to following models tropotaxic behavior in biology, in which two sensory organs are stimulated and the difference between the stimuli determines the motion of the insect (McFarland 1987). Ant osmotropotaxis is based on the differential in pheromone intensity perceived by the left and right antennae <ref> (Calenbuhr & Deneubourg 1992) </ref>, while the agents described here use the binary state of the two directional IR sensors. Under conditions of sufficient density, safe-wandering and following can produce more complex global behaviors.
Reference: <author> Dennett, D. C. </author> <year> (1987), </year> <title> The Intentional Stance, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Namely, in order to engage in social discourse, agents need to have mental models of each other, attribute mental states to each other, understand each other's intentions, and maintain beliefs about each other <ref> (Dennett 1987, Cheney & Seyfarth 1990) </ref>. Indeed, an entire field of theory of the mind rests on the necessity of inferring the internal workings of the mind of the agent (s) with whom one is interacting (Read & Miller 1993).
Reference: <author> DeScnutter, G. & Nuyts, E. </author> <year> (1993), </year> <title> Birds use self-organized social behaviours to regulate their dispersal over wide areas: evidences from gull roosts., in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 285-309. </pages>
Reference: <author> Donald, B. R., Jennings, J. & Rus, D. </author> <year> (1993), </year> <title> Experimental Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in `Proceedings, International Symposium on Robotics Research', </booktitle> <address> Hidden valley, PA. </address>
Reference: <author> Doyle, J. & Sacks, E. P. </author> <year> (1989), </year> <title> Stochastic Analysis of Qualitative Dynamics, </title> <type> Technical Report LCS-TM-418, </type> <institution> MIT Laboratory for Computer Science. </institution>
Reference: <author> Drogous, A., Ferber, J., Corbara, B. & Fresneau, D. </author> <year> (1992), </year> <title> A Behavioral Simulation Model for the Study of Emergent Social Structures, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 161-170. </pages>
Reference: <author> Dudek, G., Jenkin, M., Milios, E. & Wilkes, D. </author> <year> (1993), </year> <title> A Taxonomy for Swarm Robotics, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 441-447. </pages>
Reference: <author> Durfee, E. H., Lee, J. & Gmytrasiewicz, P. J. </author> <year> (1993), </year> <title> Overeager Reciprocal Rationality and Mixed Strategy Equilibria, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 225-230. </pages>
Reference: <author> Ephrati, E. </author> <year> (1992), </year> <title> Constrained Intelligent Action: Planning Under the Influence of a Master Agent, </title> <booktitle> in `Proceedings, AAAI-92', </booktitle> <address> San Jose, California, </address> <pages> pp. 263-268. </pages> <note> 165 Erdmann, </note> <author> M. </author> <year> (1989), </year> <title> On Probabilistic Strategies for Robot Tasks, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Erdmann, M. & Lozano-Perez, T. </author> <year> (1987), </year> <title> `On Multiple Moving Objects', </title> <journal> Algorithmica 2, </journal> <pages> 477-521. </pages>
Reference: <author> Ferrell, C. </author> <year> (1993), </year> <title> Robust Agent Control of an Autonomous Robot with Many Sensors and Actuators, </title> <type> Technical Report AI-TR-1443, </type> <institution> MIT Artificial Intelligence Laboratory. </institution>
Reference: <author> Fikes, R. E. & Nilsson, N. J. </author> <year> (1971), </year> <title> `STRIPS: A new approach to the application of theorem proving to problem solving', </title> <booktitle> Artificial Intelligence 2, </booktitle> <pages> 189-208. </pages>
Reference-contexts: Search- based methods for plan or action generation are particularly amenable to this type of analysis <ref> (Fikes & Nilsson 1971) </ref>. However, besides the scaling problem, this approach to behavior analysis fails in more realistic worlds in which both the agent and the 30 environment are not deterministic.
Reference: <author> Firby, R. J. </author> <year> (1987), </year> <title> An investigation into reactive planning in complex domains, </title> <booktitle> in `Proceedings, Sixth National Conference on Artificial Intelligence', Seattle, </booktitle> <pages> pp. 202-206. </pages>
Reference-contexts: Hybrid systems span a large and diverse body of research. It includes reactive planning or reactive execution used in Reac- tive Action Packages (RAPs), higher-level primitives for planning which hide and take care of the details of execution <ref> (Firby 1987) </ref>, and PRS (Procedural Reason- ing System), an architecture for flexible control rule invocation (Georgeff & Lansky 1987), Schemas (Arkin 1989), and several others (Payton 1990, Connell 1991). These systems tend to separate the control system into two or more communicating but otherwise independent parts.
Reference: <author> Floreano, D. </author> <year> (1993), </year> <title> Patterns of Interactions in Shared Environments, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 347-366. </pages>
Reference: <author> Forrest, S. </author> <year> (1989), </year> <title> Emergent Computation: Self-Organizing, Collective, </title> <booktitle> and Cooperative Phenomena in Natural and Artificial Computing Networks, </booktitle> <publisher> North Holland, Amsterdam. </publisher>
Reference: <author> Foster, T. C., Castro, C. A. & McNaughton, B. L. </author> <year> (1989), </year> <title> `Spatial Selectivity of Rat Hippocampal Neurons: Dependence on Preparedness for Movement', </title> <journal> Science pp. </journal> <pages> 1589-1582. </pages>
Reference: <author> Franklin, J. A. & Selfridge, O. G. </author> <year> (1990), </year> <title> Some New Directions for Adaptive Control Theory in Robotics, </title> <editor> in W. T. Miller, R. S. Sutton & P. J. Werbos, eds, </editor> <title> `Neural Networks for Control', </title> <publisher> The MIT Press, </publisher> <pages> pp. 349-360. </pages>
Reference-contexts: Such concurrent multi-modal learning is biologically and pragmatically inspired, and has been an ongoing challenge in the learning community <ref> (Franklin & Selfridge 1990, Brooks & Mataric 1993) </ref>. In our foraging task, basic behaviors were designed by hand and behavior selection was learned. However, basic behaviors themselves could be learned or optimized in parallel with learning behavior selection.
Reference: <author> Franks, N. R. </author> <year> (1989), </year> <title> `Army Ants: A Collective Intelligence', </title> <journal> American Scientist 77, </journal> <pages> 139-145. </pages>
Reference-contexts: Similarly, ants cannot be presumed to "know" that pheromones they sense are produced by their conspecifics. However, the appropriate responses to those pheromones result in the formation of trails and other complex structures <ref> (Franks 1989) </ref>. <p> The combination of dispersion and aggregation is an effective tool for regulating density. Density regulation is a ubiquitous and generically useful behavior. For instance, army ants regulate the temperature of their bivouac by aggregating and dispersing according to the local temperature gradient <ref> (Franks 1989) </ref>. Temperature regulation is just one of the many side-effects of density regulation.
Reference: <author> Fukuda, T., Nadagawa, S., Kawauchi, Y. & Buss, M. </author> <year> (1989), </year> <title> Structure Decision for Self Organizing Robots Based on Cell Structures - CEBOT, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <address> Scottsdale, Arizona, </address> <pages> pp. 695700. </pages>
Reference: <author> Fukuda, T., Sekiyama, K., Ueyama, T. & Arai, F. </author> <year> (1993), </year> <title> Efficient Communication Method in the Cellular Robotics System, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 1091-1096. </pages> <note> 166 Gallagher, </note> <author> J. C. & Beer, R. D. </author> <year> (1993), </year> <title> A Qualitative Dynamics Analysis of Evolved Locomotion Controller, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 71-80. </pages>
Reference: <editor> Gasser, L. & Huhns, M. N. </editor> <booktitle> (1989), Distributed Artificial Intelligence, </booktitle> <publisher> Pitman, London. </publisher>
Reference-contexts: Maintaining a theory of mind is a complex task and requires a high computational and cognitive overhead <ref> (Gasser & Huhns 1989, Rosenschein & Genesereth 1985, Axel- rod 1984) </ref>. <p> Goal competition is studied primarily by the Distributed AI community <ref> (Gasser & Huhns 1989) </ref>. It usually involves predicting other agents' goals and intentions, thus requiring agents to maintain models of each other (e.g., Huber & Durfee (1993) and Miceli & Cesta (1993)). Such prediction abilities require computational resources that do not scale well with increased group sizes 6 .
Reference: <author> Georgeff, M. P. & Lansky, A. L. </author> <year> (1987), </year> <title> Reactive Reasoning and Planning, </title> <booktitle> in `Proceedings, Sixth National Conference on Artificial Intelligence', Seattle, </booktitle> <pages> pp. 677682. </pages>
Reference-contexts: It includes reactive planning or reactive execution used in Reac- tive Action Packages (RAPs), higher-level primitives for planning which hide and take care of the details of execution (Firby 1987), and PRS (Procedural Reason- ing System), an architecture for flexible control rule invocation <ref> (Georgeff & Lansky 1987) </ref>, Schemas (Arkin 1989), and several others (Payton 1990, Connell 1991). These systems tend to separate the control system into two or more communicating but otherwise independent parts.
Reference: <author> Giralt, G., Chatila, R. & Vaisset, M. </author> <year> (1983), </year> <title> An Integrated Navigation and Motion Control System for Autonomous Multisensory Mobile Robots, </title> <editor> in M. Brady & R. Paul, eds, </editor> <booktitle> `First International Symposium on Robotics Research', </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Gleitman, H. </author> <year> (1981), </year> <title> Psychology, </title> <editor> W. W. </editor> <publisher> Norton & Co., </publisher> <address> New York. </address>
Reference-contexts: If the learner receives no direct instruction or answers from the environment the learning is considered unsupervised (Barto 1990). The learner produces a mapping of states to actions called a policy. 110 Reinforcement learning originated in Ivan Pavlov's classical conditioning experi-ments <ref> (Gleitman 1981) </ref>. Embraced by behaviorism, stimulus-response learning became the predominant methodology for studying animal behavior in psychology and biology. Ethology, the study of animals in their natural habitats, developed in response to the tightly controlled laboratory experimental conditions commonly used by behaviorists.
Reference: <author> Goldberg, D. E. </author> <year> (1989), </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Genetic learning has also addressed learning behaviors in simulated worlds (Koza 1990). Since learning behaviors requires finding appropriate parameter settings for control, it can be cast as an optimization problem, for which genetic algorithms are particularly well suited <ref> (Goldberg 1989) </ref>. However, since genetic algorithms operate on an abstract encoding of the learning problem, the encoding requires a good model of the agent and the environment in order to generate useful behaviors.
Reference: <author> Gomez, J. C. </author> <year> (1991), </year> <title> Visual Behavior as a Window for Reading the Mind of Others in Primates, </title> <editor> in A. Whiten, ed., </editor> <title> `Natural Theories of Mind', </title> <publisher> Basil Blackwell. </publisher>
Reference-contexts: In contrast, chimps appear to have more complex abilities and are indeed able to infer goals of their conspecifics (Cheney & Seyfarth 1990, McFarland 1987). How the internal models are represented and whether they are based on explicit or internal representations, remains open for further study <ref> (Gomez 1991) </ref>. An Alternative to the Theory of Mind Exploring the existence and limits of theory of mind in biology is difficult. The type and amount of knowledge and representation that an animal brings to bear in its social interactions is impossible to circumscribe.
Reference: <author> Goss, S., Deneubourg, J. L., Beckers, R. & Henrotte, J. </author> <year> (1993), </year> <title> Recipes for Collective Movement, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 400-410. </pages>
Reference: <author> Gould, J. L. </author> <year> (1982), </year> <title> Ethology; The Mechanisms and Evolution of Behavior, </title> <editor> W. W. </editor> <publisher> Norton & Co., </publisher> <address> New York. </address>
Reference-contexts: Most intelligent animals live, obey the rules, and reap the benefits of a society of kin. Societies vary in size and complexity, but have a key common property: they provide and maintain a shared culture <ref> (Gould 1982) </ref>. Culture is both a result and a cause of intelligent behavior. Intelligent creatures create and refine social rules in order to perpetuate the society. <p> Culture is both a result and a cause of intelligent behavior. Intelligent creatures create and refine social rules in order to perpetuate the society. These rules constitute a culture which is communicated and shared by the society, and has important effects on its individual members <ref> (Gould 1982, McFarland 1987) </ref>. Culture allows for genetic parsimony. Social interaction is used to transfer information across generations, though social learning (McFarland 1985). Thus, less genetic investment is necessary, as fewer abilities need to be innate. <p> Interestingly, as culture adapts, the growing complexity of social rules makes increased demands on individual intelligence, specifically on the ability to absorb and adapt to the culture. Humans are an extreme example of cultural complexity, requiring the longest learning and training developmental period of all animals <ref> (Gould 1982) </ref>. Culture allows for faster adaptation. As an alternative to evolution, culture al <p>- 15 lows for testing and adapting social behaviors at a much shorter time scale. Social interactions can be created and destroyed within a single generation. <p> Social interactions can be created and destroyed within a single generation. For example, elephants have been shown to learn to avoid humans even if no harm was inflicted for generations, based on a distant cultural memory of past abuse <ref> (Gould 1982) </ref>. Culture allows for Lamarckian evolution. It enables the direct transfer of learned information to future generations. A single individual's discovery can be adopted by an entire population and passed on. For example, an individual Japanese macaque monkey discovered washing of sweet potatoes. <p> A single individual's discovery can be adopted by an entire population and passed on. For example, an individual Japanese macaque monkey discovered washing of sweet potatoes. The practice was transmitted culturally through the society and on to later generations <ref> (Gould 1982) </ref>. Culture makes up for genetic deficiencies. Social interactions can compensate for individual limitations, both in terms of physical and cognitive capabilities. For example, group organizations, such as herds and packs, allow animals to attack larger prey, share information, and increase the chance of mating and survival (McFarland 1985).
Reference: <author> Gould, J. L. </author> <year> (1987), </year> <editor> Flower-shape, Landmark, and Locale Memory in Honeybees, in R. Menzel & A. Mercer, eds, </editor> <title> `Neurobiology and Behavior of Honeybees', </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This type of social organization appears quite stable and ubiquitous in animal and human societies. It often employs rather elaborate dominance structures requiring the maintenance of identities, distinguishing characteristics, and histories of previous encounters <ref> (McFarland 1987, Gould 1982) </ref>, thus demanding higher cognitive overhead than the agents we have experimented with.
Reference: <author> Gutzwiller, M. </author> <year> (1992), </year> <title> `Quantum Chaos', </title> <publisher> Scientific American. </publisher>
Reference-contexts: In contrast, nature abounds with complex systems whose global behavior results from precisely the type of interactions that current research methodologies try to avoid. These effects can be found at all scales, from the subatomic <ref> (Gutzwiller 1992) </ref>, to the semantic (Minsky 1986), to the social (Deneubourg, Goss, Franks, Sendova-Franks, Detrain & Chretien 1990). Situated behavior is based on the interaction with, and thus feedback from, the 27 environment and other agents. Both negative and positive feedback are relevant.
Reference: <author> Harnad, S. </author> <year> (1990), </year> <title> `The Symbol Grounding Problem', </title> <journal> Physica D 42, </journal> <pages> 335-346. </pages>
Reference-contexts: An the other end of the spectrum, symbolic high- level learning has not traditionally concerned itself with grounding in the physical world. However, for situated systems, which must make a connection between direct sensory experiences and high-level cognitive activities, symbol grounding is an important problem that must be addressed <ref> (Harnad 1990) </ref>. Most work on situated agents to date has not dealt with what are considered to be highly cognitive tasks. However, even learning of "lower-level" capacities, such as complex motor behaviors, requires intermediate and increasingly abstract representations.
Reference: <author> Hillis, W. D. </author> <year> (1990), </year> <title> `Co-evolving Parasites Improve Simulated Evolution as an Optimization Procedure', </title> <journal> Physica D 42, </journal> <pages> 228-234. </pages>
Reference: <author> Hinton, G. E. </author> <year> (1990), </year> <title> Connectionist Learning Procedures, </title> <editor> in Kodratoff & Michalski, eds, </editor> <booktitle> `Machine Learning, An Artificial Intelligence Approach, </booktitle> <volume> Vol. 3', </volume> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 555-610. </pages> <note> 167 Hodgins, </note> <author> J. K. & Brogan, D. C. </author> <year> (1994), </year> <title> Robot Herds: Group Behaviors from Systems with Significant Dynamics, </title> <editor> in R. Brooks & P. Maes, eds, </editor> <booktitle> `Artificial Life IV, Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems', </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: This process is analogous to statistical RL methods (Mataric 1991). The input generalization problem is also addressed by the connectionist RL literature. Multi-layer networks have been trained on a variety of learning problems in which the hidden layers constructed a generalized intermediate representation of the inputs <ref> (Hinton 1990) </ref>. While all of the RL generalization techniques are non- semantic, the table-based methods and Classifier System approaches are somewhat more readable as their results are a direct consequence of explicit hand-coded criteria. Connectionist approaches, in contrast, utilize potentially complex network dynamics and produce effective but largely inscrutable generalizations.
Reference: <author> Hogeweg, P. & Hesper, B. </author> <year> (1985), </year> <title> `Socioinformatic Processes: MIRROR Modelling Methodology', </title> <journal> Journal of Theoretical Biology 113, </journal> <pages> 311-330. </pages>
Reference: <author> Hogg, T. & Williams, C. P. </author> <year> (1993), </year> <title> Solving the Really Hard Problems with Cooperative Search, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 231-236. </pages>
Reference: <author> Holland, J. H. </author> <year> (1985), </year> <title> Properties of the bucket brigade algorithm, </title> <booktitle> in `Proceedings, International Conference on genetic Algorithms and Their Applications', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 1-7. </pages>
Reference-contexts: Soon thereafter, the problem of learning a scoring functions for playing checkers was successfully addressed with an RL algorithm (Samuel 1959). Subsequently, RL was applied to a variety of domains and problems, most notably in the Bucket Brigade algorithm used in Classifier Systems <ref> (Holland 1985) </ref>, and in a class of learning methods based on Temporal Differencing (Sutton 1988). Reinforcement learning has been implemented with a variety of algorithms ranging from table-lookup to neural networks, and on a broad spectrum of applications, including tuning parameters and playing backgammon.
Reference: <author> Holland, J. H. </author> <year> (1986), </year> <title> Escaping brittleness: The possibilities of general-purpose learn-ing algorithms applied to parallel rule-based systems, </title> <editor> in R. S. Michalski, J. </editor> <publisher> G. </publisher>
Reference-contexts: It is also confronted in Classifier Systems that use binary strings as state descriptors <ref> (Holland 1986) </ref>. The state can contain wild cards (#'s) that allow for clustering states, with the flexible grouping potential of full generality (all #'s) to full specificity (no-#'s). Generalization results in so-called "default hierarchies" based on the relevance of individual bits changed from #'s to specific values.
Reference: <editor> Carbonell & T. M. Mitchell, eds, </editor> <booktitle> `Machine Learning, An Artificial Intelligence Approach, </booktitle> <volume> Vol. 2', </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Huang, Q. & Beni, G. </author> <year> (1993), </year> <title> Stationary Waves in 2-Dimensional Cyclic Swarms, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 433-440. </pages>
Reference: <author> Huber, M. J. & Durfee, E. H. </author> <year> (1993), </year> <title> Observational Uncertainty in Plan Recognition Among Interacting Robots, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 68-75. </pages>
Reference: <author> Huberman, B. A. </author> <year> (1990), </year> <title> `The Performance of Cooperative Processes', </title> <journal> Physica D 42, </journal> <pages> 38-47. </pages>
Reference-contexts: The research is focused on global effects and the changes in the system as a whole over time. This process of global changes is usually referred to "co-evolution" <ref> (Kephart, Hogg & Huberman 1990) </ref>. Often the systems studied have some similarities to the global effects found in biological ecosystems, but the complex details of biological systems cannot be reasonably addressed. Co-evolution experiments are used to find improved search-based optimization techniques.
Reference: <author> Jaakkola, T. & Jordan, M. I. </author> <year> (1993), </year> <title> `On the Convergence of Stochastic Iterative Dynamic Programming Algorithms', </title> <note> Submitted to Neural Computation. </note>
Reference: <author> Jordan, M. I. & Rumelhart, D. E. </author> <year> (1992), </year> <title> `Forward Models: Supervised Learning with a Distal Teacher', </title> <booktitle> Cognitive Science 16, </booktitle> <pages> 307-354. </pages>
Reference-contexts: Problems in adaptive control deal with learning the forward or inverse model of the system, i.e., the plant. Forward models provide predictions about the output expected after performing an action in a given state. Analogously, inverse models provide an action, given the current state and a desired output <ref> (Jordan & Rumel- hart 1992) </ref>. Learning control has been applied to a variety of domains and has used a number of different learning methodologies. Connectionist algorithms are most popular, (see Miller, Sutton & Werbos (1990) for a representative collection), but 1 Note: not all maps are explicit and declarative. <p> In the ideal case, reinforcement is both immediate and meaningful. Immediate error signals that provide not only the sign but also the magnitude of the error result in fastest learning. As in supervised learning, then provide the agent with the correct answer after each trial. In learning control <ref> (Jordan & Rumelhart 1992, Atkeson 1990, Schaal & Atke- son 1994) </ref>, such error signals are critical as the learning problem is usually finding a complex mapping between a collection of input parameters and the desired output.
Reference: <author> Kaelbling, L. P. </author> <year> (1990), </year> <title> Learning in Embedded Systems, </title> <type> PhD thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: It is difficult to estimate if obtaining a world model for a given domain requires any more or less time than learning a policy for some set of goals. Consequently, insightful work on learning world models for more intelligent exploration <ref> (Sutton 1990, Kaelbling 1990) </ref> is yet to be made applicable to complex situated domains. We have argued that accurate models of situated domains are difficult to obtain or learn. Instead, we will focus in this work on learning policies in systems without explicit world models. <p> Instead, we will focus in this work on learning policies in systems without explicit world models. The next section describes the general form of RL algorithms that have been used for such policy learning. 6.3.4 Algorithms Reinforcement learning algorithms have the following general form <ref> (Kaelbling 1990) </ref>: 1. Initialize the learner's internal state I to I 0 . 2. Do Forever: a. Observe the current world state s. b. Choose an action a = F (I; s) using the evaluation function F . c. Execute action a. d. <p> If initialized to 0's in a problem set up to have a positive optimal policy, the algorithm will tend to converge to the first positive value, without exploring alternatives, so random actions must be added to guarantee that the entire action space is explored <ref> (Kaelbling 1990) </ref>. Alter- natively, if the optimal policy can be roughly estimated, Q values can be initialized to be higher and decreased over time. However, Q is sensitive to the coupling between the initial values and the reinforcement function.
Reference: <author> Kephart, J. O., Hogg, T. & Huberman, B. A. </author> <year> (1990), </year> <title> `Collective Behavior of Predictive Agents', </title> <journal> Physica D 42, </journal> <pages> 48-65. </pages> <note> 168 Keshet, </note> <author> L. E. </author> <year> (1993), </year> <title> Trail Following as an Adaptable Mechanism for Population Behaviour, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 326-346. </pages>
Reference-contexts: The research is focused on global effects and the changes in the system as a whole over time. This process of global changes is usually referred to "co-evolution" <ref> (Kephart, Hogg & Huberman 1990) </ref>. Often the systems studied have some similarities to the global effects found in biological ecosystems, but the complex details of biological systems cannot be reasonably addressed. Co-evolution experiments are used to find improved search-based optimization techniques.
Reference: <author> Kessin, R. H. & Campagne, M. M. V. L. </author> <year> (1992), </year> <title> `The Development of a Social Amoeba', </title> <journal> American Scientist 80, </journal> <pages> 556-565. </pages>
Reference-contexts: For example, slime mold bases its 20 behavior on the concentration of slime produced by its kin. It cannot be said that it actively "recognizes" kin but it does act in species-specific ways which result in complex group behavior such as the construction of multi-cellular organisms <ref> (Kessin & Campagne 1992) </ref>. Similarly, ants cannot be presumed to "know" that pheromones they sense are produced by their conspecifics. However, the appropriate responses to those pheromones result in the formation of trails and other complex structures (Franks 1989). <p> Camazine (1993) demonstrates similar gull behavior on a ledge. People maintain similar arrangements in enclosed spaces (Gleit- man 1981). Similarly, Floreano (1993) demonstrates that simulated evolved ants use dispersion consistently. Aggregation, as a protective and resource-pooling and sharing behavior, is found in species ranging from the slime mold <ref> (Kessin & Campagne 1992) </ref> to social animals (McFarland 1987). The combination of dispersion and aggregation is an effective tool for regulating density. Density regulation is a ubiquitous and generically useful behavior.
Reference: <author> Kolen, J. F. & Pollack, J. B. </author> <year> (1993), </year> <title> Apparent Computational Complexity in Physical Systems, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 617-622. </pages>
Reference-contexts: In general, this work is concerned with predicting the global behavior of the system rather than the precise behavior of any of its components. At the high level of precision requiring a detailed level of description, most interactions are chaotic and unpredictable <ref> (Kolen & Pollack 1993) </ref>. The goal of analysis is to gain predictive power by modeling the system at the right level. In the case of artificial complex systems, however, it is not possible to determine that level without generating and testing the system itself.
Reference: <author> Kosoresow, A. P. </author> <year> (1993), </year> <title> A Fast First-Cut Protocol for Agent Coordination, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 237-242. </pages>
Reference: <author> Kosecka, J. </author> <year> (1992), </year> <title> Control of Discrete Event Systems, </title> <type> Technical Report MS-CIS92-35 GRASP LAB 313, </type> <institution> University of Pennsylvania. </institution>
Reference-contexts: In general, the problem of partitioning continuous state into discrete states is hard <ref> (Kosecka 1992) </ref>, and even if a reasonable partitioning of the world is found, there may be no mapping from the space of sensor readings to this partitioning. Observability Although continuous and often complex, sensors have limited abilities.
Reference: <author> Koza, J. R. </author> <year> (1990), </year> <title> Evolution and Co-evolution of Computer Programs to Control Independently-acting Agents, </title> <booktitle> in `Proceedings, Simulation of Adaptive Behavior SAB-90', </booktitle> <publisher> The MIT Press, </publisher> <address> Paris, France, </address> <pages> pp. 366-375. </pages>
Reference-contexts: For example, a maze-learning system can be said to learn a specific maze-solving behavior. Genetic learning has also addressed learning behaviors in simulated worlds <ref> (Koza 1990) </ref>. Since learning behaviors requires finding appropriate parameter settings for control, it can be cast as an optimization problem, for which genetic algorithms are particularly well suited (Goldberg 1989).
Reference: <author> Kraus, S. </author> <year> (1993), </year> <title> Agents Contracting Tasks in Non-Collaborative Environments, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 243-248. </pages>
Reference: <author> Kube, C. R. </author> <year> (1992), </year> <title> Collective Robotic Intelligence: A Control Theory for Robot Populations, </title> <type> Master's thesis, </type> <institution> University of Alberta. </institution>
Reference: <author> Kube, C. R. & Zhang, H. </author> <year> (1992), </year> <title> Collective Robotic Intelligence, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <pages> pp. 460-468. </pages>
Reference: <author> Kube, C. R., Zhang, H. & Wang, X. </author> <year> (1993), </year> <title> Controlling Collective Tasks With an ALN, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 289-293. </pages>
Reference: <author> Kurosu, K., Furuya, T. & Soeda, M. </author> <year> (1993), </year> <title> Fuzzy Control of Group With a Leader and Their Behaviors, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 1105-1109. </pages>
Reference: <author> Laird, J. E. & Rosenbloom, P. S. </author> <year> (1990), </year> <title> Integrating, Execution, Planning, and Learning in Soar for External Environments, </title> <booktitle> in `Proceedings, AAAI-90', </booktitle> <pages> pp. 10221029. </pages>
Reference: <author> Langton, C. G. </author> <year> (1989), </year> <title> Artificial Life, </title> <publisher> Addison-Wesley. 169 Langton, </publisher> <editor> C. G. </editor> <year> (1990), </year> <title> `Computation at the Edge of Chaos: Phase Transitions and Emergent Computation', </title> <journal> Physica D 42, </journal> <pages> 12-37. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1991), </year> <title> Self-improvement Based on Reinforcement Learning, Planning and Teaching, </title> <booktitle> in `Proceedings, Eighth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 323-327. </pages>
Reference-contexts: In general, events can take different amounts of time to execute, can have delayed effects, and can have different consequences under identical conditions. In short, situated domains are difficult to model properly. Deterministic models do not capture the dynamics of most situated domains, so nondeterministic alternatives have been considered <ref> (Lin 1991) </ref>. Unfortunately, most are based on unrealistic models of sensor and effector uncertainty with overly simplified error properties. They are typically based on adding Gaussian noise to each sensed state and each commanded action.
Reference: <author> Lozano-Perez, T., Mason, M. T. & Taylor, R. H. </author> <year> (1984), </year> <title> `Automatic Synthesis of Fine Motion Strategies for Robots', </title> <journal> International Journal of Robotics Research 3(1), </journal> <pages> 3-24. </pages>
Reference: <author> Lynch, N. A. </author> <year> (1993), </year> <title> Simulation Techniques for Proving Properties of Real-Time Systems, </title> <type> Technical Report MIT-LCS-TM-494, </type> <institution> MIT. </institution>
Reference: <author> Lynch, N. A. & Tuttle, M. R. </author> <year> (1987), </year> <title> Hierarchical Correctness Proofs for Distributed Algorithms, </title> <type> Technical Report MIT-LCS-TR-387, </type> <institution> MIT. </institution>
Reference: <author> MacLennan, B. J. </author> <year> (1990), </year> <title> Evolution of Communication in a Population of Simple Machines, </title> <institution> Technical Report Computer Science Department Technical Report CS-90-104, University of Tennessee. </institution>
Reference: <author> Maes, P. </author> <year> (1989), </year> <title> The Dynamics of Action Selection, </title> <booktitle> in `IJCAI-89', </booktitle> <address> Detroit, MI, </address> <pages> pp. 991-997. </pages>
Reference: <author> Maes, P. </author> <year> (1991), </year> <title> Learning Behavior Networks from Experience, </title> <editor> in F. Varela & P. Bourgine, eds, </editor> <booktitle> `Toward A Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 48-57. </pages>
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to Coordinate Behaviors, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Boston, MA, </address> <pages> pp. 796-802. </pages>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991a), </year> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference-contexts: The described formulation is a direct extension of behavior-based control (Mataric 1992a, Brooks 1991b, Brooks 1986). The presented heterogeneous reward functions are related to subgoals <ref> (Mahadevan & Connell 1991a) </ref> as well as subtasks (White- head et al. 1993). However, unlike previous work, which has focused on learning action sequences, this work used a higher level of description. The proposed subgoals are directly tied to behaviors used as the basis of control and learning.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991b), </year> <title> Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture, </title> <booktitle> in `Eight International Workshop on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 328-337. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1990a), </year> <title> A Distributed Model for Mobile Robot Environment-Learning and Navigation, </title> <type> Technical Report AI-TR-1228, </type> <institution> MIT Artificial Intelligence Laboratory. </institution> <note> 170 Mataric, </note> <author> M. J. </author> <year> (1990b), </year> <title> Navigating With a Rat Brain: A Neurobiologically-Inspired Model for Robot Spatial Representation, </title> <editor> in J. A. Meyer & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 169-175. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1991), </year> <title> A Comparative Analysis of Reinforcement Learning Methods, </title> <type> Technical Report AIM-1322, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: Generalization results in so-called "default hierarchies" based on the relevance of individual bits changed from #'s to specific values. This process is analogous to statistical RL methods <ref> (Mataric 1991) </ref>. The input generalization problem is also addressed by the connectionist RL literature. Multi-layer networks have been trained on a variety of learning problems in which the hidden layers constructed a generalized intermediate representation of the inputs (Hinton 1990). <p> Sutton's original formalization of temporal differencing (TD ()) deals with such predictions in Markovian environments, and covers a large class of learning approaches. For example, Bucket Brigade, the delayed reinforcement learning method used in Classifier Systems, is an instance of TD <ref> (Mataric 1991) </ref>.
Reference: <author> Mataric, M. J. </author> <year> (1992a), </year> <title> Behavior-Based Systems: Key Properties and Implications, </title> <booktitle> in `IEEE International Conference on Robotics and Automation, Workshop on Architectures for Intelligent Control Systems', Nice, France, </booktitle> <pages> pp. 46-54. </pages>
Reference-contexts: They rely on a direct coupling between sensing and action, and fast feedback from the environment. Purely reactive strategies have proven effective for a variety of problems that can be well defined at design-time, but are inflexible at run-time due to their inability to store information dynamically <ref> (Mataric 1992a) </ref>. The division between reactive and deliberative strategies can be drawn based on the type and amount of computation performed at run-time. Reactive, constant-time run-time strategies can be derived from a planner, by computing all possible plans off-line in advance. <p> Other than centralized reasoning engine and representation, these systems may use different forms of distributed internal representations and perform distributed computations on them in order to decide what effector action to take <ref> (Mataric 1992a) </ref>. A comparative classification of above methodologies based on domains of applicability has not yet been undertaken. 2.4.2 Multi-Agent Control Having overviewed single-agent control, this section discusses how the described approaches scale to multi-agent problems. <p> Consequently, similar strategies apply to the multi-agent case: the behaviors of the agents can be combined in some form, or one of the agents will take precedence over the rest. As previously argued <ref> (Mataric 1992a) </ref>, an unambiguous precedence hierarchy between the competing behaviors or agents is the simplest way to guarantee a globally consistent result. Thus, ensuring minimal higher-order effects and interference in a (locally) heterogeneous society can be accomplished by a strict hierarchy of control. <p> The described formulation is a direct extension of behavior-based control <ref> (Mataric 1992a, Brooks 1991b, Brooks 1986) </ref>. The presented heterogeneous reward functions are related to subgoals (Mahadevan & Connell 1991a) as well as subtasks (White- head et al. 1993). However, unlike previous work, which has focused on learning action sequences, this work used a higher level of description.
Reference: <author> Mataric, M. J. </author> <year> (1992b), </year> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference: <author> Mataric, M. J. </author> <year> (1992c), </year> <title> `Integration of Representation Into Goal-Driven BehaviorBased Robots', </title> <journal> IEEE Transactions on Robotics and Automation 8(3), </journal> <pages> 304-312. </pages>
Reference-contexts: Depending on the complexity of the system, arbitration can and usually must be performed at multiple points. One level of arbitration can be achieved by designing mutually exclusive behavior conditions <ref> (Mataric 1992c) </ref>. Creating a unique one-to- one mapping between conditions and behaviors guarantees a mutually exclusive set of condition-action couplings.
Reference: <author> Mataric, M. J. </author> <year> (1993), </year> <title> Kin Recognition, Similarity, and Group Behavior, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 705-710. </pages>
Reference-contexts: However, none of the above learning approaches can be said to learn new behaviors according to the precise definition of the problem. The posed "behavior learning problem" <ref> (Brooks & Mataric 1993) </ref> requires that the agent acquire a new behavior using its own perceptual and effector systems, as well as to assign some semantic label to the behavior, in order to later recognize and use it as a coherent and independent unit.
Reference: <author> Mataric, M. J. </author> <year> (1994), </year> <title> Learning to Behave Socially, </title> <booktitle> in `The Third International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Such behavior is difficult to learn with individualist reinforcement learning strategies. We 147 are currently working on an algorithm that utilizes the observation of neighboring agents' behavior and received reinforcement in order to acquire and practice social behaviors <ref> (Mataric 1994) </ref>. 8.6.2 Transition Models The learning problem presented here, involving a collection of concurrently learning agents in a noisy and uncertain environment, was purposefully chosen for its complexity. The fact that a state transition model was not available to aid the learner presented one of the major challenges.
Reference: <author> McFarland, D. </author> <year> (1985), </year> <title> Animal Behavior, </title> <publisher> Benjamin Cummings. </publisher>
Reference-contexts: These rules constitute a culture which is communicated and shared by the society, and has important effects on its individual members (Gould 1982, McFarland 1987). Culture allows for genetic parsimony. Social interaction is used to transfer information across generations, though social learning <ref> (McFarland 1985) </ref>. Thus, less genetic investment is necessary, as fewer abilities need to be innate. Interestingly, as culture adapts, the growing complexity of social rules makes increased demands on individual intelligence, specifically on the ability to absorb and adapt to the culture. <p> Culture makes up for genetic deficiencies. Social interactions can compensate for individual limitations, both in terms of physical and cognitive capabilities. For example, group organizations, such as herds and packs, allow animals to attack larger prey, share information, and increase the chance of mating and survival <ref> (McFarland 1985) </ref>. In order to be understood, individual intelligence must be observed and analyzed within its social and therefore cultural context. <p> This ability is innate and ubiquitous in nature, and enables almost all creatures to distinguish others of their own kind, and more specifically to recognize kin from others <ref> (McFarland 1985, McFarland 1987) </ref>. It is important to note that species and kin recognition need not be explicit, i.e., the agent need not "know" or "be aware" that the other agent being recognized is kin, as long as its response to it is kin-specific. <p> Table 4.2 shows a list of behaviors that constitutes a basic set for a flexible repertoire of spatial group interactions. Biology offers numerous justifications for these behaviors. Avoidance and wandering are survival instincts so ubiquitous it obviates discussion. Following, often innate, is seen in numerous species <ref> (McFarland 1985) </ref>. Dispersion is commonplace as well. DeScnutter & Nuyts (1993) show elegant evidence of gulls aggregating by dynamically rearranging their positions in a field to maintain a fixed distance from each other. Camazine (1993) demonstrates similar gull behavior on a ledge.
Reference: <author> McFarland, D. </author> <year> (1987), </year> <title> The Oxford Companion to Animal Behavior, </title> <publisher> in `Oxford, University Press'. </publisher>
Reference-contexts: People maintain similar arrangements in enclosed spaces (Gleit- man 1981). Similarly, Floreano (1993) demonstrates that simulated evolved ants use dispersion consistently. Aggregation, as a protective and resource-pooling and sharing behavior, is found in species ranging from the slime mold (Kessin & Campagne 1992) to social animals <ref> (McFarland 1987) </ref>. The combination of dispersion and aggregation is an effective tool for regulating density. Density regulation is a ubiquitous and generically useful behavior. For instance, army ants regulate the temperature of their bivouac by aggregating and dispersing according to the local temperature gradient (Franks 1989). <p> Besides the described behavior set, numerous other useful group behaviors exist. For example, biology also suggest surrounding and herding as frequent patterns of group movement, related to a higher level achievement goal, such as capture or migration <ref> (McFarland 1987) </ref>. <p> This approach to following models tropotaxic behavior in biology, in which two sensory organs are stimulated and the difference between the stimuli determines the motion of the insect <ref> (McFarland 1987) </ref>. Ant osmotropotaxis is based on the differential in pheromone intensity perceived by the left and right antennae (Calenbuhr & Deneubourg 1992), while the agents described here use the binary state of the two directional IR sensors. <p> This type of social organization appears quite stable and ubiquitous in animal and human societies. It often employs rather elaborate dominance structures requiring the maintenance of identities, distinguishing characteristics, and histories of previous encounters <ref> (McFarland 1987, Gould 1982) </ref>, thus demanding higher cognitive overhead than the agents we have experimented with. <p> Adaptability does not necessitate learning. Many species are genetically equipped with elaborate "knowledge" and abilities, from the very specific, such as the ability to record and utilize celestial maps (Waterman 1989), to the very general, such as plasticity in learning motor control <ref> (McFarland 1987) </ref> and language (Pinker 1994). But genetic code is finite. In fact, primate and human cortical neural topology is too complicated to fully specify in the available genome, and is instead established by 106 Problem Learning in complex situated domains. Assertion Traditional reinforcement learning must be reformulated.
Reference: <author> Meier-Koll, A. & Bohl, E. </author> <year> (1993), </year> <title> Time-structure analysis in a village community of Columbian Indians; A mathematically simulated system of ultradian oscilla-tors, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 718-749. </pages>
Reference: <author> Miceli, M. & Cesta, A. </author> <year> (1993), </year> <title> Strategic Social Planning: Looking for Willingness in Multi-Agent Domains, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 741-746. </pages>
Reference: <author> Miller, W. T., Sutton, R. S. & Werbos, P. J. </author> <year> (1990), </year> <title> Neural Networks for Control, </title> <publisher> The MIT Press. 171 Minsky, </publisher> <editor> M. L. </editor> <year> (1954), </year> <title> Theory of Neural-Analog Reinforcement Systems and Its Ap-plication to the Brain-Model Problem, </title> <type> PhD thesis, </type> <institution> Princeton. </institution>
Reference: <author> Minsky, M. L. </author> <year> (1986), </year> <title> The Society of Mind, </title> <publisher> Simon and Schuster, </publisher> <address> New York. </address>
Reference-contexts: In contrast, nature abounds with complex systems whose global behavior results from precisely the type of interactions that current research methodologies try to avoid. These effects can be found at all scales, from the subatomic (Gutzwiller 1992), to the semantic <ref> (Minsky 1986) </ref>, to the social (Deneubourg, Goss, Franks, Sendova-Franks, Detrain & Chretien 1990). Situated behavior is based on the interaction with, and thus feedback from, the 27 environment and other agents. Both negative and positive feedback are relevant.
Reference: <author> Miramontes, O., Sole, R. V. & Goodwin, B. C. </author> <year> (1993), </year> <title> Antichaos in ants: the ex-citability metaphor at two hierarchical levels, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 790-807. </pages>
Reference: <author> Moore, A. </author> <year> (1991), </year> <title> Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-valued State-Spaces, </title> <booktitle> in `Eight International Workshop on Machine Learning', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. </author> <year> (1992), </year> <title> `Fast, Robust Adaptive Control by Learning only Forward Models', </title> <booktitle> Advances in Neural Information Processing 4 pp. </booktitle> <pages> 571-579. </pages>
Reference-contexts: Furthermore, the concept of behavior contains informal notions about generality and adaptivity that are difficult to state precisely without domain-specific grounding. Consequently, most learning control problems appear to be instances of behavior learning, such as learning to balance a pole (Barto, Sutton & Anderson 1983), to play billiards <ref> (Moore 1992) </ref>, and to juggle (Schaal & Atkeson 1994). Furthermore, work on action selection, deciding what action to make in each state, can be viewed as learning a higher-level behavior as an abstraction on the state-action space.
Reference: <author> Moore, A. W. </author> <year> (1993), </year> <title> `The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces', </title> <booktitle> Advances in Neural Information Processing 6 pp. </booktitle> <pages> 711-718. </pages>
Reference: <author> Moravec, H. P. & Cho, D. W. </author> <year> (1989), </year> <title> A Bayesian Method for Certainty Grids, </title> <booktitle> in `AAAI Spring Symposium on Robot Navigation', </booktitle> <pages> pp. 57-60. </pages>
Reference: <author> Muller, M. & Wehner, R. </author> <year> (1988), </year> <title> Path Integration in Desert Ants: </title> <booktitle> Cataglyphis Fortis, in `Proceedings of the Natural Academy of Sciences'. </booktitle>
Reference: <author> Mussa-Ivaldi, F. A. & Giszter, S. </author> <year> (1992), </year> <title> `Vector field approximation: a computational paradigm for motor control and learning', </title> <booktitle> Biological Cybernetics 67, </booktitle> <pages> 491-500. </pages>
Reference: <author> Noreils, F. R. </author> <year> (1992), </year> <title> An Architecture for Cooperative and Autonomous Mobile Robots, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <pages> pp. 2703-2710. </pages>
Reference: <author> Noreils, F. R. </author> <year> (1993), </year> <title> `Toward a Robot Architecture Integrating Cooperation between Mobile Robots: Application to Indoor Environment', </title> <journal> The International Journal of Robotics Research 12(1), </journal> <pages> 79-98. </pages>
Reference: <author> Parker, L. E. </author> <year> (1993a), </year> <title> An Experiment in Mobile Robotic Cooperation, </title> <booktitle> in `Proceedings, Robotics for Challenging Environment', </booktitle> <address> Albuquerque, New Mexico. </address>
Reference: <author> Parker, L. E. </author> <year> (1993b), </year> <title> Learning in Cooperative Robot Teams, </title> <booktitle> in `Proceedings, IJCAI93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 12-23. </pages> <note> 172 Parker, </note> <author> L. E. </author> <year> (1994), </year> <title> Heterogeneous Multi-Robot Cooperation, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Payton, D. </author> <year> (1990), </year> <title> Internalized Plans: a representation for action resources, </title> <editor> in P. Maes, ed., </editor> <title> `Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back', </title> <publisher> The MIT Press. </publisher>
Reference-contexts: planning or reactive execution used in Reac- tive Action Packages (RAPs), higher-level primitives for planning which hide and take care of the details of execution (Firby 1987), and PRS (Procedural Reason- ing System), an architecture for flexible control rule invocation (Georgeff & Lansky 1987), Schemas (Arkin 1989), and several others <ref> (Payton 1990, Connell 1991) </ref>. These systems tend to separate the control system into two or more communicating but otherwise independent parts. In most cases, the low-level reactive process takes care of the immediate safety of the robot, while the higher level uses the planner to select action sequences.
Reference: <author> Payton, D., Keirsey, D., Kimble, D., Krozel, J. & Rosenblatt, K. </author> <year> (1992), </year> <title> `Do Whatever Works: A robust approach to fault-tolerant autonomous control', </title> <journal> Journal of Applied Intelligence 3, </journal> <pages> 226-249. </pages>
Reference: <author> Piaget, J. </author> <year> (1962), </year> <title> Play, Dreams and Imitation in Children, </title> <editor> W. W. </editor> <publisher> Norton & Co., </publisher> <address> New York. </address>
Reference-contexts: Prior to this stage, occurring around the age of two, children are incapable of separating the internal and external perception of the world <ref> (Piaget 1962, Bandura & Walters 1963, Bandura 1971) </ref>. Even after achieving self-awareness, as determined with the typical dot-and-mirror test (Asendorpf & Baudonniere 1993), around the age of two, children require a number of years before reaching the adult ability to form theories of mind (Bandura 1977, Abravanel & Gingold 1985).
Reference: <author> Pinker, S. </author> <year> (1994), </year> <title> The Language Instinct, </title> <publisher> William Morrow and Company, Inc., </publisher> <address> New York. </address>
Reference-contexts: Adaptability does not necessitate learning. Many species are genetically equipped with elaborate "knowledge" and abilities, from the very specific, such as the ability to record and utilize celestial maps (Waterman 1989), to the very general, such as plasticity in learning motor control (McFarland 1987) and language <ref> (Pinker 1994) </ref>. But genetic code is finite. In fact, primate and human cortical neural topology is too complicated to fully specify in the available genome, and is instead established by 106 Problem Learning in complex situated domains. Assertion Traditional reinforcement learning must be reformulated.
Reference: <author> Pomerleau, D. A. </author> <year> (1992), </year> <title> Neural Network Perception for Mobile Robotic Guidance, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference: <author> Premack, D. & Woodruff, G. </author> <year> (1978), </year> <title> `Does the chimpanzee have a theory of mind?', </title> <booktitle> Behavior and Brain Science 1, </booktitle> <pages> 515-526. </pages>
Reference-contexts: the ability to recognize kin is pragmatic as it allows even the simplest of rules to produce purposive collective behavior. 2.3.5 Mental Models and Theory of Mind A dominant school of thought in cognitive psychology and AI is based on the premise that social interaction requires a theory of mind <ref> (Premack & Woodruff 1978) </ref>. Namely, in order to engage in social discourse, agents need to have mental models of each other, attribute mental states to each other, understand each other's intentions, and maintain beliefs about each other (Dennett 1987, Cheney & Seyfarth 1990).
Reference: <author> Read, S. J. & Miller, J. C. </author> <year> (1993), </year> <title> Explanatory coherence in the construction of mental models of others, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 836-841. </pages>
Reference-contexts: Indeed, an entire field of theory of the mind rests on the necessity of inferring the internal workings of the mind of the agent (s) with whom one is interacting <ref> (Read & Miller 1993) </ref>. Maintaining a theory of mind is a complex task and requires a high computational and cognitive overhead (Gasser & Huhns 1989, Rosenschein & Genesereth 1985, Axel- rod 1984).
Reference: <author> Resnick, M. </author> <year> (1992), </year> <title> Beyond the Centralized Mindset; Exploration in MassivelyParallel Microworlds, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference-contexts: How- ever, in spite of the potentially numerous applications, the distributed, multi-agent approach is an exception rather than the rule in most domains. Parallel, decentralized, non-hierarchical computation requires a paradigm shift <ref> (Resnick 1992) </ref>. Regardless of the domain of application, this approach raises a number of difficult issues.
Reference: <author> Reynolds, C. W. </author> <year> (1987), </year> <title> `Flocks, Herds, and Schools: A Distributed Behavioral Model', </title> <booktitle> Computer Graphics 21(4), </booktitle> <pages> 25-34. </pages>
Reference: <author> Rosenschein, G. & Genesereth, M. R. </author> <year> (1985), </year> <title> Deals Among Rational Agents, </title> <booktitle> in `IJCAI-85', </booktitle> <pages> pp. 91-99. </pages>
Reference: <author> Rosenschein, J. S. </author> <year> (1993), </year> <title> Consenting Agents: Negotiation Mechanisms for MultiAgent Systems, </title> <booktitle> in `IJCAI-93', </booktitle> <pages> pp. 792-799. </pages>
Reference-contexts: However, the types of environments it deals with are relatively simple and low complexity in that they feature no noise or uncertainty and can be accurately characterized. DAI can be divided into two subfields: Distributed Problem Solving (DPS) and Multi-Agent Systems (MAS) <ref> (Rosenschein 1993) </ref>. DPS deals with centrally designed systems solving global problems and using built-in cooperation strategies. In contrast, MAS work deals with heterogeneous, not necessarily centrally designed agents faced with the goal of utility-maximizing coexistence. Decker & Lesser (1993a) is a good example of DPS work.
Reference: <author> Rosenschein, S. J. & Kaelbling, L. P. </author> <year> (1986), </year> <title> The Synthesis of Machines with Provable Epistemic Properties, </title> <editor> in J. Halpern, ed., </editor> <booktitle> `Theoretical Aspects of Reasoning About Knowledge', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <pages> pp. 83-98. </pages>
Reference-contexts: Reactive, constant-time run-time strategies can be derived from a planner, by computing all possible plans off-line in advance. For example, situated automata achieve real-time performance by compiling all of the system's goals and the ways of their achievement into a language that compiles into circuits with constant-time computation properties <ref> (Rosenschein & Kaelbling 1986) </ref>. In general, the entire control system of an agent can be precompiled as a decision graph into a collection of reactive rules ("universal plans") (Schoppers 1987). While theoretically appealing, these strategies often scale poorly with the complexity of the environment and the agent's control system.
Reference: <author> Rosenthal, T. L. & Zimmerman, B. J. </author> <year> (1978), </year> <title> Social Learning and Cognition, </title> <publisher> Academic Press, </publisher> <address> New York. </address> <note> 173 Samuel, </note> <author> A. L. </author> <year> (1959), </year> <title> `Some studies in machine learning using the game of checkers', </title> <journal> IBM Journal of Research and Development 3, </journal> <pages> 211-229. </pages>
Reference: <author> Sandini, G., Lucarini, G. & Varoli, M. </author> <year> (1993), </year> <title> Gradient Driven Self-Organizing Systems, </title> <booktitle> in `IEEE/TSJ International Conference on Intelligent Robots and Systems', </booktitle> <address> Yokohama, Japan, </address> <pages> pp. 429-432. </pages>
Reference: <author> Schaal, S. & Atkeson, C. G. </author> <year> (1994), </year> <title> `Robot Juggling: An Implementation of MemoryBased Learning', </title> <journal> Control Systems Magazine. </journal>
Reference-contexts: Consequently, most learning control problems appear to be instances of behavior learning, such as learning to balance a pole (Barto, Sutton & Anderson 1983), to play billiards (Moore 1992), and to juggle <ref> (Schaal & Atkeson 1994) </ref>. Furthermore, work on action selection, deciding what action to make in each state, can be viewed as learning a higher-level behavior as an abstraction on the state-action space. For example, a maze-learning system can be said to learn a specific maze-solving behavior.
Reference: <author> Schmieder, R. W. </author> <year> (1993), </year> <title> A Knowledge-Tracking Algorithm for Generating Collective Behavior in Individual-Based Populations, in `Toward A Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life', </booktitle> <pages> pp. 980-989. </pages>
Reference: <author> Schoppers, M. J. </author> <year> (1987), </year> <title> Universal Plans for Reactive Robots in Unpredictable Domains, </title> <booktitle> in `IJCAI-87', </booktitle> <address> Menlo Park, </address> <pages> pp. 1039-1046. </pages>
Reference-contexts: In general, the entire control system of an agent can be precompiled as a decision graph into a collection of reactive rules ("universal plans") <ref> (Schoppers 1987) </ref>. While theoretically appealing, these strategies often scale poorly with the complexity of the environment and the agent's control system. Hybrid architectures attempt a compromise between purely reactive and deliberative approaches, usually by employing a reactive system for low-level control, and a planner for higher-level decision making.
Reference: <author> Seeley, T. D. </author> <year> (1989), </year> <title> `The Honey Bee Colony as a Superorganism', </title> <journal> American Scientist 77, </journal> <pages> 546-553. </pages>
Reference-contexts: For example, bees use signals, such as the waggle dance, with the sole purpose of transmitting information and recruiting. In contrast, they also use cues, such as the direction of their flight, which transmit hive information as a by-product of their other behaviors <ref> (Seeley 1989) </ref>. Cooperation is a form of interaction, usually based on communication. Certain types of cooperative behavior depend on directed communication. Specifically, any cooperative behaviors that require negotiation between agents depend on directed communication in order to assign particular tasks to the participants.
Reference: <author> Shoham, Y. & Tennenholtz, M. </author> <year> (1992), </year> <title> On the synthesis of useful social laws for artifi-cial agent societies, </title> <booktitle> in `Proceedings, AAAI-92', </booktitle> <address> San Jose, California, </address> <pages> pp. 276-281. </pages>
Reference: <author> Simon, H. </author> <year> (1969), </year> <booktitle> The Sciences of the Artificial, </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1991), </year> <title> Transfer of Learning Across Compositions of Sequential Tasks, </title> <booktitle> in `Proceedings, Eighth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 348-352. </pages>
Reference-contexts: Therefore they must be sequential and consistent. To enforce a specific goal sequence, the state space must explicitly encode what goals have been reached at any point in time, thus requiring added bits in the input state vector <ref> (Singh 1991) </ref>. Although a natural extension of the RL framework, this method requires the state space to grow with each added goal, and cannot address concurrent goals.
Reference: <author> Sismondo, E. </author> <year> (1990), </year> <title> `Synchronous, Alternating, and Phase-Locked Stridulation by a Tropical Katydid', </title> <booktitle> Science 249, </booktitle> <pages> 55-58. </pages>
Reference: <author> Smithers, T. </author> <year> (1994), </year> <title> On Why Better Robots Make it Harder, </title> <booktitle> in `The Third International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Previous work has shown that synthesis and analysis of correct plans in the presence of uncertainty can be intractable even in highly constrained domains (Lozano-Perez, Mason & Taylor 1984, Canny 1988, Erdmann 1989) and even on the simplest of systems <ref> (Smithers 1994) </ref>. Physical environments pose a great challenge as they usually do not contain the structure, determinism, and thus predictability usually required for formal analysis (Brooks 1991c, Brooks 1991b). Predicting the behavior of a multi- agent system is more complex than the single-agent case.
Reference: <author> Steels, L. </author> <year> (1989), </year> <title> Cooperation Between Distributed Agents Through SelfOrganization, </title> <booktitle> in `Workshop on Multi-Agent Cooperation', </booktitle> <publisher> North Holland, </publisher> <address> Cambridge, UK. </address>
Reference: <author> Steels, L. </author> <year> (1994a), </year> <journal> `The Artificial Life Roots of Artificial Intelligence', </journal> <note> to appear in The Artificial Life Journal. 174 Steels, </note> <author> L. </author> <year> (1994b), </year> <title> Emergent Functionality of Robot Behavior Through On-line Evo-lution, </title> <editor> in R. Brooks & P. Maes, eds, </editor> <booktitle> `Artificial Life IV, Proceedings of the Fourth International Workshop on the Synthesis and Simulation of Living Systems', </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Sussman, G. J. & McDermott, D. V. </author> <year> (1972), </year> <title> From PLANNER to CONNIVER-A genetic approach, </title> <booktitle> in `Proceedings, Fall Joint Computer Conference', </booktitle> <pages> pp. 11711179. </pages>
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> `Learning to Predict by Method of Temporal Differences', </title> <journal> The Journal of Machine Learning 3(1), </journal> <pages> 9-44. </pages>
Reference-contexts: Subsequently, RL was applied to a variety of domains and problems, most notably in the Bucket Brigade algorithm used in Classifier Systems (Holland 1985), and in a class of learning methods based on Temporal Differencing <ref> (Sutton 1988) </ref>. Reinforcement learning has been implemented with a variety of algorithms ranging from table-lookup to neural networks, and on a broad spectrum of applications, including tuning parameters and playing backgammon. Our work is concerned with reinforcement learning on situated, embodied agents. <p> Different RL algorithms vary in their definitions of U and F . 115 The predominant methodology used in RL is based on a class of temporal differ-encing (TD) techniques <ref> (Sutton 1988) </ref>. All TD methods deal with assigning credit or blame to past actions by attempting to predict long-term consequences of each action in each state. Sutton's original formalization of temporal differencing (TD ()) deals with such predictions in Markovian environments, and covers a large class of learning approaches. <p> Temporal credit is assigned by propagating the reward back to the appropriate previous state-action pairs. Temporal differencing methods are based on predicting the expected value of future rewards for a given state-action pair, and assigning credit locally based on the difference between successive predictions <ref> (Sutton 1988) </ref>. Reward functions determine how credit is assigned.
Reference: <author> Sutton, R. S. </author> <year> (1990), </year> <title> Integrated Architectures for Learning, Planning and Reacting Based on Approximating Dynamic Programming, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference-contexts: It is difficult to estimate if obtaining a world model for a given domain requires any more or less time than learning a policy for some set of goals. Consequently, insightful work on learning world models for more intelligent exploration <ref> (Sutton 1990, Kaelbling 1990) </ref> is yet to be made applicable to complex situated domains. We have argued that accurate models of situated domains are difficult to obtain or learn. Instead, we will focus in this work on learning policies in systems without explicit world models. <p> In general, reinforcement learning can be accelerated in two ways: 1) by building- in more information, and 2) by providing more reinforcement. The reward function implicitly encodes domain knowledge and thus biases what the agent can learn. Sim- plifying and minimizing reinforcement, as practiced by some early RL algorithms <ref> (Sutton 1990) </ref>, does diminish this bias, but it also greatly handicaps, and in situated domains, completely debilitates the learner. Domain knowledge can be embedded through a reward-rich and complex reinforcement function.
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Thrun, S. B. & Mitchell, T. M. </author> <year> (1993), </year> <title> Integrating Indictive Neural Network Learning and Explanation-Based Learning, </title> <booktitle> in `Proceedings, IJCAI-93', </booktitle> <address> Chambery, France. </address>
Reference: <author> Tomasello, M., Kruger, A. C. & Rather, H. H. </author> <year> (1992), </year> <note> `Cultural Learning', to appear in The Journal of Brain and Behavior Sciences. </note>
Reference: <author> Travers, M. </author> <year> (1988), </year> <title> Animal Construction Kits, </title> <editor> in C. Langton, ed., </editor> <booktitle> `Artificial Life', </booktitle> <publisher> Addison-Wesley. </publisher>
Reference: <author> Vander, A. J., Sherman, J. H. & Luciano, D. S. </author> <year> (1980), </year> <title> Human Physiology, </title> <publisher> McGrawHill Book Company, </publisher> <address> New York. </address>
Reference-contexts: Another form of common feedback-based behavior involves the synchronization of rhythmic patters of activity. For example, Meier-Koll & Bohl (1993) describe the synchronization of circadian rhythms of in human and animal subjects and models them as a collection of coupled oscillators. Analogous effects are commonly observed in hormonal cycles <ref> (Vander, Sherman & Luciano 1980) </ref>. In such systems, the synchronized state is a stable behavior, as is the evenly dispersed equal-power state, while all other states are transient. <p> Validation Implement learning on a group of mobile robots learning to forage. Table 6.1: A summary of the situated learning problem addressed here, and the structure of the proposed solution. spontaneous synaptic firing in utero and in the first decade of life <ref> (Vander et al. 1980) </ref>. In addition to compensating for genetic parsimony, learning is useful for optimizing the agent's existing abilities, and necessary for coping with complex and changeable worlds.
Reference: <author> Waterman, T. H. </author> <year> (1989), </year> <title> Animal Navigation, </title> <publisher> Scientific American Library, </publisher> <address> New York. </address>
Reference-contexts: The purpose of learning is to make the set of such conditions smaller. Adaptability does not necessitate learning. Many species are genetically equipped with elaborate "knowledge" and abilities, from the very specific, such as the ability to record and utilize celestial maps <ref> (Waterman 1989) </ref>, to the very general, such as plasticity in learning motor control (McFarland 1987) and language (Pinker 1994). But genetic code is finite.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: Sutton's original formalization of temporal differencing (TD ()) deals with such predictions in Markovian environments, and covers a large class of learning approaches. For example, Bucket Brigade, the delayed reinforcement learning method used in Classifier Systems, is an instance of TD (Mataric 1991). Q-learning <ref> (Watkins 1989) </ref>, the most commonly known and used TD algorithm, is defined and explained in Appendix A, as background for subsequent comparison. 6.3.5 Learning Trials Performance properties of various forms of TD applied to Markovian environments have been extensively studied (Watkins & Dayan 1992, Barto, Bradtke & Singh 1993, Jaakkola & <p> Provable convergence of TD and related learning strategies based on dynamic programming is asymptotic and requires infinite trials <ref> (Watkins 1989) </ref>. Generating a complete policy, however incorrect, requires time exponential in the size of the state space, and the optimality of that policy converges in the limit as the number of trials approaches infinity. In practice, this translates into hundreds of thousands of trials for up to ten-bit states.
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992), </year> <title> `Q-Learning', </title> <booktitle> Machine Learning 8, </booktitle> <pages> 279-292. </pages>
Reference: <author> Wehner, R. </author> <year> (1987), </year> <title> `Matched Filters Neural Models of the External World', </title> <journal> Journal of Computational Physiology A(161), </journal> <pages> 511-531. </pages> <note> 175 Weisbuch, </note> <author> G. </author> <year> (1991), </year> <title> Complex System Dynamics, </title> <booktitle> in `Lecture Notes Vol. II, Santa Fe Institute Studies in the Sciences of Complexity', </booktitle> <publisher> Addison-Wesley, </publisher> <address> New York. </address>
Reference-contexts: Algorithm 4.2: sizes. Finding a guaranteed general-purpose collision avoidance strategy for an agent situated in a dynamic world is difficult. In a multi-agent world the problem can become intractable. Inspired by biological evidence which indicates that insects and animals do not have precise avoidance routines <ref> (Wehner 1987) </ref>, we used the following general avoidance behavior: command ( v 0 @ sin ( + u) C where is R's orientation and u is the incremental turning angle away from the obstacle. A simple Avoid-Other-Agents rule was devised, as shown in Algorithm 4.1.
Reference: <author> Werner, G. M. & Dyer, M. G. </author> <year> (1990), </year> <title> Evolution of Communication in Artificial Organisms, </title> <type> Technical Report UCLA-AI-90-06, </type> <institution> University of California, </institution> <address> Los Angeles. </address>
Reference: <author> Whitehead, S. D. </author> <year> (1992), </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action, </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference-contexts: The proposed subgoals are directly tied to behaviors used as the basis of control and learning. Similarly, progress estimators are mapped to one or more behaviors, and expedite learning of the associated goals, unlike a single complete external critic used with a monolithic reinforcement function <ref> (Whitehead 1992) </ref>. Elevating the description, control, and learning level of the system to one based on perceptual conditions and behaviors instead of perceptual states and atomic actions greatly diminishes the agent's learning space and makes learning tractable.
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active Perception and Reinforcement Learning, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference-contexts: The collapse of multiple states into one results in partial observability, i.e. in perceptual aliasing, a many-to-one mapping between world and internal states. The inability to distinguish different states makes it difficult and often impossible for the learning algorithm to assign appropriate utility to actions associated with such states <ref> (Whitehead & Ballard 1990) </ref>. Partially Observable Markov Decision Processes (POMDPs) have been developed 112 by the operation research community for dealing with this problem. Partial observ-- ability is added into a Markov model by introducing a discrete probability distribution over a set of possible observations for a given state.
Reference: <author> Whitehead, S. D., Karlsson, J. & Tenenberg, J. </author> <year> (1993), </year> <title> Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging, </title> <editor> in J. H. Connell & S. Mahadevan, eds, </editor> <title> `Robot Learning', </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 45-78. </pages>
Reference: <author> Wiggins, S. </author> <year> (1990), </year> <title> Introduction to Applied Nonlinear Dynamical Systems and Chaos, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Yanco, H. & Stein, L. A. </author> <year> (1993), </year> <title> An Adaptive Communication Protocol for Cooperating Mobile Robots, </title> <booktitle> in `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 478-485. 176 </pages>
References-found: 203

