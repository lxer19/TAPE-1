URL: http://www.cs.dartmouth.edu/~brd/f/s/algo96-a.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~brd/www/
Root-URL: http://www.cs.dartmouth.edu
Email: briggs@middlebury.edu brd@cs.cornell.edu  
Title: Visibility-Based Planning of Sensor Control Strategies  
Author: Amy J. Briggs Bruce R. Donald 
Keyword: Visibility-based planning, error detection and recovery, sensor con figuration, camera control, surveillance  
Address: Upson Hall  Middlebury, VT 05753, USA Ithaca, NY 14853, USA  
Affiliation: Department of Mathematics Department of Computer Science and Computer Science  Middlebury College Cornell University  
Note: Submitted to the special Algorithmica issue of Algorithmic Foundations of Robotics  
Abstract: We consider the problem of planning sensor control strategies that enable a sensor to be automatically configured for robot tasks. In this paper we present robust and efficient algorithms for computing the regions from which a sensor has unobstructed or partially obstructed views of a target in a goal. We apply these algorithms to the Error Detection and Recovery problem of recognizing whether a goal or failure region has been achieved. Based on these methods and strategies for visually-cued camera control, we have built a robot surveillance system in which one mobile robot navigates to a viewing position from which it has an unobstructed view of a goal region, and then uses visual recognition to detect when a specific target has entered the region. fl Support for this work was provided in part by the National Science Foundation under grants No. IRI-8802390, IRI-9000532, IRI-9201699, and by a Presidential Young Investigator award to Bruce Donald, and in part by the Air Force Office of Sponsored Research, the Mathematical Sciences Institute, Intel Corporation, and AT&T Bell laboratories. The first author was additionally supported by an AT&T Bell Laboratories Graduate Fellowship sponsored by the AT&T Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Asano, T. Asano, L. Guibas, J. Hershberger, and H. Imai. </author> <title> Visibility of disjoint polygons. </title> <journal> Algorithmica, </journal> <volume> 1 </volume> <pages> 49-63, </pages> <year> 1986. </year>
Reference-contexts: was mentioned in the paper by Suri and O'Rourke [30], the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph <ref> [33, 1] </ref>. Thus, the overall time for explicitly computing the boundary of the partial visibility region for target A at any fixed configuration q is O (n 2 (n + m) 2 ).
Reference: [2] <author> B. Bhattacharya, D. G. Kirkpatrick, and G. T. Toussaint. </author> <title> Determining sector visibility of a polygon. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Computational Geometry, Saarbrucken, Germany, </booktitle> <pages> pages 247-253, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Their lower bound of (n 4 ) for explicitly constructing the boundary of the weak visibility region holds as well for our computation of recognizability regions under a weak visibility assumption. Bhattacharya, Kirkpatrick and Toussaint <ref> [2] </ref> introduce the concept of sector visibility of a polygon, and give fi (n) and (n log n) bounds, depending on the size of the visibility wedge, for determining if a polygon is weakly externally visible.
Reference: [3] <author> A. J. Briggs. </author> <title> Efficient Geometric Algorithms for Robot Sensing and Control. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: As the target translates, free shadow vertices trace out point conics if their generating edges are anchored on the target <ref> [3] </ref>. 3.2.3 Swept shadows in the partial visibility model We have shown how to compute shadows for any fixed target position, and have discussed how these shadows change as the target translates.
Reference: [4] <author> R. G. Brown. </author> <title> Algorithms for Mobile Robot Localization and Building Flexible, Robust, Easy to Use Mobile Robots. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: When in a particular configuration, the sensor returns a distance and normal reading to the nearest object, which is accurate to within some known bound. Such a ranging device has been developed in our robotics laboratory at Cornell, and has been used for both map-making and robot localization <ref> [4, 5] </ref>. We denote the space of sensor placements C s p = IR 2 and the space of sensor aims 2 This is similar to the notion introduced by Buckley [6] of a confusable set in the context of motion planning.
Reference: [5] <author> R. G. Brown, L. P. Chew, and B. R. Donald. </author> <title> Localization and map-making algorithms for mobile robots. </title> <booktitle> In Proceedings of the International Association of Science and Technology for Development (IASTED) International Conference on Robotics and Manufacturing, </booktitle> <address> Oxford, England, </address> <pages> pages 185-190, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: When in a particular configuration, the sensor returns a distance and normal reading to the nearest object, which is accurate to within some known bound. Such a ranging device has been developed in our robotics laboratory at Cornell, and has been used for both map-making and robot localization <ref> [4, 5] </ref>. We denote the space of sensor placements C s p = IR 2 and the space of sensor aims 2 This is similar to the notion introduced by Buckley [6] of a confusable set in the context of motion planning.
Reference: [6] <author> S. J. Buckley. </author> <title> Planning and Teaching Compliant Motion Strategies. </title> <type> PhD thesis, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1987. </year>
Reference-contexts: We denote the space of sensor placements C s p = IR 2 and the space of sensor aims 2 This is similar to the notion introduced by Buckley <ref> [6] </ref> of a confusable set in the context of motion planning. There, two contact points x and y are said to be confusable if they are capable of generating the same position and force measurements. 6 SV () (p; ).
Reference: [7] <author> A. J. Cameron and H. Durrant-Whyte. </author> <title> A Bayesian approach to optimal sensor placement. </title> <journal> International Journal of Robotics Research, </journal> <volume> 9(5) </volume> <pages> 70-88, </pages> <year> 1990. </year>
Reference-contexts: The sensor placement problem has previously been addressed by Nelson and Khosla [25] and Kutulakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte <ref> [7] </ref> and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement.
Reference: [8] <author> J. F. Canny. </author> <title> A computational approach to edge detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6) </volume> <pages> 34-43, </pages> <year> 1986. </year> <month> 29 </month>
Reference-contexts: We first built a calibrated visual model of Lily. We used the Panasonic CCD camera mounted on Tommy to take a picture of Lily from a known fixed distance (4 m). We then computed the intensity edges for that image using an implementation of Canny's edge detection algorithm <ref> [8] </ref>. The actual model of Lily that we created and used is shown in Figure 17.
Reference: [9] <author> A. Casta~no and S. Hutchinson. </author> <title> Hybrid vision/position servo control of a robotic manipulator. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, Nice, France, </booktitle> <pages> pages 1264-1269, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The idea is to combine position, force, and visual sensing in order to produce error-tolerant motion strategies. His work builds on that of preimage planners by adding visual feedback to compensate for uncertainty. Details on the implementation of vision-based control are described by Hutchinson and Casta~no <ref> [9] </ref>. Sharma and Hutchinson [29] define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features.
Reference: [10] <author> C. K. Cowan and P. D. Kovesi. </author> <title> Automatic sensor placement from vision task requirements. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 407-416, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Lacroix, Grand-jean, and Ghallab [19] describe a method for selecting view points and sensing tasks to confirm an identification hypothesis. Cowan and Kovesi <ref> [10] </ref> study the problem of automatic camera placement for vision tasks. They consider the constraints on camera location imposed by resolution and focus requirements, visibility and view angle, and forbidden regions depending on the task.
Reference: [11] <author> M. de Berg, L. Guibas, D. Halperin, M. Overmars, O. Schwarzkopf, M. Sharir, and M. Teillaud. </author> <title> Reaching a goal with directional uncertainty. </title> <booktitle> In Proceedings of the Fourth Annual International Symposium on Algorithms and Computation, </booktitle> <address> Hong Kong, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The problem of planar motion planning for a robot with bounded directional uncertainty is considered by de Berg et al. <ref> [11] </ref>. They give algorithms for constructing the regions from which goals may be reached, and show how the complexity of the regions depends on the magnitude of the uncertainty angle. Teller [32] solves the weak polygon visibility problem for a special case in 3D.
Reference: [12] <author> B. R. Donald. </author> <title> Error detection and recovery in Robotics, </title> <booktitle> volume 336 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Less generally, A and B could be part of the same robot, for example, a physically distributed but globally controlled system. Multi-media applications that involve controlling cameras to observe people could also employ our camera control strategies. The Error Detection and Recovery Framework <ref> [12] </ref> provides a natural problem domain in which to apply our strategies. An Error Detection and Recovery (EDR) strategy is one that is guaranteed to achieve a specified goal when the goal is recognizably achievable, and signals failure otherwise. <p> For more details, see Donald <ref> [12] </ref>. 3 A represents the target; its reference point is indicated by the black dot. G is the goal region and H is the failure region. The darkly shaded polygons are obstacles. <p> Zhang [34] considers the problem of optimally placing multiple sensors. A different approach from the one taken in this paper to the incorporation of sensor planning in the EDR framework was first presented by Donald <ref> [12] </ref>. In that approach, an equivalence is established between sensing and motion in configuration 4 space. Active sensing for a mobile robot is reduced to motion, by exploiting the similarity between visibility and generalized damper motions. In contrast, we present here a framework that is closer to actual sensors. <p> The algorithms account for uncertainty in sensor placement and aim. The Error Detection and Recovery (EDR) system of Donald <ref> [12] </ref> provides a framework for constructing manipulation strategies when guaranteed plans cannot be found or do not exist. An EDR strategy attains the goal when the goal is recognizably reachable, and signals failure otherwise.
Reference: [13] <author> S. K. Ghosh. </author> <title> Computing the visibility polygon from a convex set and related problems. </title> <journal> Journal of Algorithms, </journal> <volume> 12 </volume> <pages> 75-95, </pages> <year> 1991. </year>
Reference-contexts: Figure 1 gives an example of the problem we would like to solve. 1.2 Related Work Ghosh has considered the problem of computing complete and weak visibility regions from a convex set in the absence of obstacles <ref> [13] </ref>. LaValle et al. consider the problem of maintaining line-of-sight visibility with a moving target [21]. In related work, LaValle et al. present solutions to a visibility-based motion planning problem in which multiple agents coordinate to find an unpredictable target moving among obstacles [22].
Reference: [14] <author> R. L. Graham and F. F. Yao. </author> <title> Finding the convex hull of a simple polygon. </title> <journal> Journal of Algorithms, </journal> <volume> 4(4) </volume> <pages> 324-331, </pages> <year> 1983. </year>
Reference-contexts: If A has m vertices and G has k vertices, CH (A ) and CH (G) can be computed in O (m log m) 12 and O (k log k) time, respectively <ref> [14] </ref>. So CH (A G) = CH (A ) CH (G) has complexity O (m + k) and can be computed in time O (m log m + k log k).
Reference: [15] <author> G. Hager and M. Mintz. </author> <title> Computational methods for task-directed sensor data fusion and sensor planning. </title> <journal> International Journal of Robotics Research, </journal> <volume> 10(4) </volume> <pages> 285-313, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The sensor placement problem has previously been addressed by Nelson and Khosla [25] and Kutulakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz <ref> [15] </ref> present a Bayesian approach to optimal sensor placement.
Reference: [16] <author> S. Hutchinson. </author> <title> Exploiting visual constraints in robot motion planning. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, Sacramento, </booktitle> <pages> pages 1722-1727, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement. Hutchinson <ref> [16] </ref> introduces the concept of a visual constraint surface to control 1 The generalized damper is a dynamical model specified by the relationship F = B (v v 0 ) between forces and velocities, where F is the vector of forces and torques acting on a moving object, v 0 is
Reference: [17] <author> D. P. Huttenlocher, M. E. Leventon, and W. J. Rucklidge. </author> <title> Visually-guided navigation by comparing two-dimensional edge images. </title> <booktitle> In Proceedings of the Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Seattle, WA, </address> <pages> pages 842-847, </pages> <year> 1994. </year>
Reference-contexts: moved into the computed configura tion. 26 The matcher used in the experiment is based on the Hausdorff distance between sets of points and was written by William Rucklidge [28] and has been used extensively in the Cornell Robotics and Vision Laboratory for image comparison, motion tracking, and visually-guided navigation <ref> [17] </ref>. The particular matcher used here is a translation-only matcher that uses a fractional measure of the Hausdorff distance. Matches are found by searching the 2D space of translations of the model, and computing the Hausdorff distance between the image and the translated model.
Reference: [18] <author> K. N. Kutulakos, C. R. Dyer, and V. J. Lumelsky. </author> <title> Provable strategies for vision-guided exploration in three dimensions. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <pages> pages 1365-1372, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In these papers, visibility regions are the regions visible from the observer, and therefore are combinatorially simpler since they do not depend on the geometry of the target. The sensor placement problem has previously been addressed by Nelson and Khosla [25] and Kutulakos, Dyer, and Lumelsky <ref> [18] </ref> for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement.
Reference: [19] <author> S. Lacroix, P. Grandjean, and M. Ghallab. </author> <title> Perception planning for a multi-sensory interpretation machine. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, Nice, France, </booktitle> <pages> pages 1818-1824, </pages> <month> May </month> <year> 1992. </year> <month> 30 </month>
Reference-contexts: Sharma and Hutchinson [29] define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features. Lacroix, Grand-jean, and Ghallab <ref> [19] </ref> describe a method for selecting view points and sensing tasks to confirm an identification hypothesis. Cowan and Kovesi [10] study the problem of automatic camera placement for vision tasks.
Reference: [20] <author> C. Laugier, A. Ijel, and J. Troccaz. </author> <title> Combining vision based information and partial geometric models in automatic grasping. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Cincinatti, </address> <publisher> Ohio, </publisher> <pages> pages 676-682, </pages> <year> 1990. </year>
Reference-contexts: They consider the constraints on camera location imposed by resolution and focus requirements, visibility and view angle, and forbidden regions depending on the task. Given values bounding these constraints, they compute the set of camera locations affording complete visibility of a surface in 3D. Laugier, Ijel and Troccaz <ref> [20] </ref> employ partial and complete visibility computations in selecting sensor locations to acquire information about the environment and to guide grasping movements. Zhang [34] considers the problem of optimally placing multiple sensors.
Reference: [21] <author> S. M. LaValle, H. H. Gonzalez-Ba~nos, C. Becker, and J.-C. Latombe. </author> <title> Motion strategies for maintaining visibility of a moving target. </title> <type> Manuscript. </type>
Reference-contexts: LaValle et al. consider the problem of maintaining line-of-sight visibility with a moving target <ref> [21] </ref>. In related work, LaValle et al. present solutions to a visibility-based motion planning problem in which multiple agents coordinate to find an unpredictable target moving among obstacles [22].
Reference: [22] <author> S. M. LaValle, D. Lin, L. J. Guibas, J.-C. Latombe, and R. Motwani. </author> <title> Finding an unpredictable target in a workspace with obstacles. </title> <type> Manuscript. </type>
Reference-contexts: LaValle et al. consider the problem of maintaining line-of-sight visibility with a moving target [21]. In related work, LaValle et al. present solutions to a visibility-based motion planning problem in which multiple agents coordinate to find an unpredictable target moving among obstacles <ref> [22] </ref>. In these papers, visibility regions are the regions visible from the observer, and therefore are combinatorially simpler since they do not depend on the geometry of the target.
Reference: [23] <author> T. Lozano-Perez. </author> <title> Spatial planning: A configuration space approach. </title> <journal> IEEE Trans. on Computers (C-32), </journal> <pages> pages 108-120, </pages> <year> 1983. </year>
Reference-contexts: Note that this is not an approximation; only the outermost tangents with the distinguished polygon (in this case A G) generate shadows in the complete visibility model. We can compute the convex hull of A G efficiently by exploiting the fact that for polygons A and B <ref> [23] </ref> CH (A) CH (B) = CH (A B): Note that A and B do not need to be convex. So instead of computing CH (A G) explicitly, we simply convolve CH (A ) and CH (G).
Reference: [24] <author> J. Matousek, N. Miller, J. Pach, M. Sharir, S. Sifrony, and E. Welzl. </author> <title> Fat triangles determine linearly many holes. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 49-58, </pages> <year> 1991. </year>
Reference-contexts: Otherwise, we classify the maximal visibility triangle as an *-fat triangle. After this processing, we now have O (n (n + m)) fat visibility triangles. We can now use a result of Matousek et al. <ref> [24] </ref> on the union of fat triangles. Their result bounds the number of holes in a union of fat triangles. In our case, the "holes" are shadows in a union of visibility triangles.
Reference: [25] <author> B. Nelson and P. K. Khosla. </author> <title> Integrating sensor placement and visual tracking strategies. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <pages> pages 1351-1356, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In these papers, visibility regions are the regions visible from the observer, and therefore are combinatorially simpler since they do not depend on the geometry of the target. The sensor placement problem has previously been addressed by Nelson and Khosla <ref> [25] </ref> and Kutulakos, Dyer, and Lumelsky [18] for visual tracking and vision-guided exploration. Several researchers have explored the problem of optimal sensor placement. Cameron and Durrant-Whyte [7] and Hager and Mintz [15] present a Bayesian approach to optimal sensor placement.
Reference: [26] <author> J. O'Rourke. </author> <title> Art Gallery Theorems and Algorithms. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Research in the area of art gallery theory has introduced and addressed many problems pertaining to polygon visibility. The art gallery problem is to determine the minimum number of guards sufficient to guard the interior of a simple polygon (see <ref> [26] </ref> for more details). Sensor configuration planning addresses the related question of where sensors should be placed in order to monitor a region of interest. In this case we are interested in external visibility of a polygon rather than internal visibility.
Reference: [27] <author> R. Pollack, M. Sharir, and S. Sifrony. </author> <title> Separating two simple polygons by a sequence of translations. </title> <type> Technical Report 215, </type> <institution> Department of Computer Science, New York University, Courant Institute of Mathematical Sciences, </institution> <month> April </month> <year> 1986. </year>
Reference-contexts: Computing a single cell in an arrangement is equivalent to computing the lower envelope of a set of line segments in the plane, which for a set of size n takes time O (nff (n)), where ff (n) is the inverse Ackerman function <ref> [27] </ref>.
Reference: [28] <author> W. J. Rucklidge. </author> <title> Efficient Computation of the Minimum Hausdorff Distance for Visual Recognition. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Run the matcher to find an instance of the scaled model in the cropped image. with Tommy's videocamera once Tommy had moved into the computed configura tion. 26 The matcher used in the experiment is based on the Hausdorff distance between sets of points and was written by William Rucklidge <ref> [28] </ref> and has been used extensively in the Cornell Robotics and Vision Laboratory for image comparison, motion tracking, and visually-guided navigation [17]. The particular matcher used here is a translation-only matcher that uses a fractional measure of the Hausdorff distance.
Reference: [29] <author> R. Sharma and S. Hutchinson. </author> <title> On the observability of robot motion under active camera control. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <pages> pages 162-167, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: His work builds on that of preimage planners by adding visual feedback to compensate for uncertainty. Details on the implementation of vision-based control are described by Hutchinson and Casta~no [9]. Sharma and Hutchinson <ref> [29] </ref> define a measure of robot motion observability based on the relationship between differential changes in the position of the robot to the corresponding differential changes in the observed visual features.
Reference: [30] <author> S. Suri and J. O'Rourke. </author> <title> Worst-case optimal algorithms for constructing visibility polygons with holes. </title> <booktitle> In Proceedings of the Second Annual ACM Symposium on Computational Geometry, </booktitle> <pages> pages 14-23, </pages> <year> 1986. </year>
Reference-contexts: The questions of detecting polygon visibility and constructing visibility regions under a variety of assumptions is a rich area of past and ongoing research in computational geometry. We mention here a few of the papers most closely related to our problem. Suri and O'Rourke <ref> [30] </ref> give an fi (n 4 ) algorithm for the problem of computing the locus of points weakly visible from a distinguished edge in an environment of line segments. <p> For target A at configuration q 2 C r , we construct the partial visibility region using an approach similar to that given by Suri and O'Rourke for computing the region weakly visible from an edge <ref> [30] </ref>. Our algorithm is as follows: 13 A v shaded regions are visibility triangles. Partial visibility algorithm for a stationary target 1. Construct the visibility graph for the entire environment, consisting of distin guished polygon A and obstacles B. 2. <p> Therefore each vertex contributes O (n + m) visibility triangles, so we have O (n (n + m)) visibility triangles overall. In general, the union of these triangles has complexity O (n 2 (n + m) 2 ). As was mentioned in the paper by Suri and O'Rourke <ref> [30] </ref>, the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph [33, 1].
Reference: [31] <author> K. Tarabanis and R. Y. Tsai. </author> <title> Computing occlusion-free viewpoints. </title> <booktitle> In Proceedings of the Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 802-807, </pages> <year> 1992. </year>
Reference-contexts: For an environment of total edge complexity n, he gives an O (n 2 ) time algorithm for computing the piecewise-quadratic boundary of the antipenumbra, which will be non-convex and disconnected in general. Tarabanis and Tsai <ref> [31] </ref> examine the question of complete visibility for general polyhedral environments in 3D.
Reference: [32] <author> S. J. Teller. </author> <title> Computing the antipenumbra of an area light source. </title> <booktitle> Computer Graphics (Proceedings SIGGRAPH '92), </booktitle> <volume> 26(2) </volume> <pages> 139-148, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: They give algorithms for constructing the regions from which goals may be reached, and show how the complexity of the regions depends on the magnitude of the uncertainty angle. Teller <ref> [32] </ref> solves the weak polygon visibility problem for a special case in 3D. Namely, he computes the antipenumbra (the volume from which some, but not all, of a light source can be seen) of a convex area light source shining through a sequence of convex areal holes in three dimensions.
Reference: [33] <author> E. Welzl. </author> <title> Constructing the visibility graph for n line segments in O(n 2 ) time. </title> <journal> Information Processing Letters, </journal> <volume> 20 </volume> <pages> 167-171, </pages> <year> 1985. </year>
Reference-contexts: was mentioned in the paper by Suri and O'Rourke [30], the triangles can be output in constant time per triangle: Asano et al. have shown that the visibility edges at a vertex v can be obtained sorted by slope in linear time with Welzl's algorithm for computing the visibility graph <ref> [33, 1] </ref>. Thus, the overall time for explicitly computing the boundary of the partial visibility region for target A at any fixed configuration q is O (n 2 (n + m) 2 ).
Reference: [34] <author> H. Zhang. </author> <title> Optimal sensor placement. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, Nice, France, </booktitle> <pages> pages 1825-1830, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Given values bounding these constraints, they compute the set of camera locations affording complete visibility of a surface in 3D. Laugier, Ijel and Troccaz [20] employ partial and complete visibility computations in selecting sensor locations to acquire information about the environment and to guide grasping movements. Zhang <ref> [34] </ref> considers the problem of optimally placing multiple sensors. A different approach from the one taken in this paper to the incorporation of sensor planning in the EDR framework was first presented by Donald [12]. In that approach, an equivalence is established between sensing and motion in configuration 4 space.
References-found: 34

