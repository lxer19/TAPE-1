URL: http://www.cs.msu.edu/~stockman/Papers/3D.face.pose.ps.Z
Refering-URL: http://www.cs.msu.edu/~stockman/pubs.html
Root-URL: http://www.cs.msu.edu
Title: Controlling a Computer via Facial Aspect  Pattern Recognition Image Processing  
Author: Philippe Ballard, George C. Stockman 
Address: East Lansing, MI 48824  
Affiliation: Department of Computer Science Michigan State University  
Pubnum: Laboratory  
Abstract: Control of a computer workstation via face position and facial gesturing would be an important advance for people with hand or body disabilities as well as for all users. Steps toward realization of such a system are reported here. A computer system has been developed to track the eyes and the nose of a subject and to compute the direction of the face. Face direction and movement is then used to control the cursor. Test results show that the resulting system is usable, although several improvements are needed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Ballard and Christopher Brown. </author> <title> Computer Vision. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: The shape is defined by three features: the area, or twinkle size which should be less than 20 pixels, the elongation which should be close to one and the compactness which is equal to 4 for a disk. Ballard and Brown <ref> [1] </ref> and Levine [9] give mathematical definitions for these features. The area A is just the number of pixels of a connected component. The elongation is the ratio of the smallest eigenvalue and the largest eigenvalue of the component point set obtained from the co-variance matrix.
Reference: [2] <author> Philippe Ballard and George Stockman. </author> <title> Face Direction Detection using Feature Extraction. M.S.U. PRIP Lab. </title> <type> Technical Report, </type> <month> September </month> <year> 1990. </year>
Reference-contexts: Given the time and accuracy constraints, the feature should be easy to find and be fixed on the face. For instance, as shown in <ref> [2] </ref>, the mouth is not a good feature since it can be partly occluded when the subject looks to the sides. Furthermore, points on the 6 mouth can "move" on the face, independent of the rest of the face. <p> Error analysis and experiments show that these results obtain even with variation of focal length and face position needed to accomodate a moving human user: details can be found in <ref> [2] </ref>. We have not achieved our goal of being able to select one of 64 light buttons within two seconds; our experimental system required about 15 seconds. However, since beginning our project, speed of workstations has increased eightfold and camera attachments are common.
Reference: [3] <author> Robert J. Baron. </author> <title> Mechanisms of Human Facial Recognition. </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 15 </volume> <pages> 137-178, </pages> <year> 1981. </year>
Reference: [4] <author> H.G. Barrow, J.M. Tenenbaum, R.C. Bolles, and H.C. Wolf. </author> <title> Parametric correspondence and Chamfer matching : two new techniques for image matching. </title> <booktitle> In IJCAI-77, </booktitle> <volume> volume 2, </volume> <year> 1977. </year>
Reference-contexts: The distance function not only cuts down the time necessary for evaluation of a match but it also improves the properties of the matching. H. G. Barrow, J. M. Tenenbaum, R.C. Bolles, and H.C. Wolf <ref> [4] </ref> showed that the cost involved in conventional matching techniques is quadratic in time while the chamfering technique is only linear. where the chamfering transformation was applied: the darker the pixel, the closer it is to an edge.
Reference: [5] <author> John Canny. </author> <title> A Computational Approach to Edge Detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: Furthermore, extracting the nose with the wrong threshold led to large errors in the computation of face direction. To solve this problem, we needed a calibration phase in the system. Tests were done to determine the best value of the threshold. Canny <ref> [5] </ref> presents a thresholding method done with hysteresis. If any part of a contour is above a high threshold, those points are immediately output, as is the entire connected segment of contour which contains the points and which lies above a low threshold.
Reference: [6] <author> I. Craw, H. Ellis, and J.R. Lishman. </author> <title> Automatic extraction of face-features. </title> <journal> Pattern Recognition letters, </journal> <volume> 5 </volume> <pages> 183-187, </pages> <year> 1987. </year>
Reference: [7] <author> M.A. Fischler and R.C. Bolles. </author> <title> Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. </title> <journal> Communications of ACM, </journal> <volume> 24 </volume> <pages> 381-395, </pages> <month> June </month> <year> 1981. </year>
Reference: [8] <author> Venu Govindaraju, Sargur. Srihari, and David Sher. </author> <title> A Computational Model For Face Location. </title> <booktitle> In ICCV, </booktitle> <address> Osaka, Japan, </address> <month> November </month> <year> 1990. </year>
Reference: [9] <author> Martin Levine. </author> <booktitle> Vision in man and machine. </booktitle> <address> Mc Graw Hill, </address> <year> 1985. </year>
Reference-contexts: The shape is defined by three features: the area, or twinkle size which should be less than 20 pixels, the elongation which should be close to one and the compactness which is equal to 4 for a disk. Ballard and Brown [1] and Levine <ref> [9] </ref> give mathematical definitions for these features. The area A is just the number of pixels of a connected component. The elongation is the ratio of the smallest eigenvalue and the largest eigenvalue of the component point set obtained from the co-variance matrix.
Reference: [10] <author> K. Ohmura, A. Tomono, and Y. Kobayashi. </author> <title> Method of Detecting Face Direction using Image Processing for Human Interface. </title> <booktitle> SPIE Visual Communication and Image Processing, </booktitle> <volume> 1001 </volume> <pages> 625-632, </pages> <year> 1988. </year>
Reference: [11] <author> Alex Pentland and Kenji Mase. </author> <title> LIP READING: Automatic Visual Recognition of Spoken Words. </title> <institution> M.I.T. Media Lab Vision Science Technical Report 117, </institution> <month> January </month> <year> 1989. </year>
Reference: [12] <author> William Press, Brian Flannery, Saul Teukol-sky, and William Vetterling. </author> <title> Numerical Re-cepes in C. </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference: [13] <author> Chris Schmandt, Mark Acckerman, and Debby Hindus. </author> <title> Augmenting a Window System with Speech Input. </title> <booktitle> Computer, </booktitle> <pages> pages 50-56, </pages> <month> August </month> <year> 1990. </year>
Reference: [14] <author> Robert Sekuler and Randolph Blake. Perception. Mc Graw Hill, </author> <year> 1990. </year>
Reference-contexts: Locate the position of the nose guided by positions of the eyes. 4. Compute face direction from the three feature points. 5. Move the mouse cursor accordingly. 5 1) Extracting a pair of eye twinkles: The hu-man eye is very nearly spherical with a diameter of approximately 24 millimeters <ref> [14] </ref>. A twinkle from the eye is a bright reflection from the cornea which may be assumed to be fixed if a) the light source is fixed, b) the head is fixed and c) we assume that the eye is a sphere whose motion is purely rotational.
Reference: [15] <author> Steven Tanimoto. </author> <booktitle> The Elements of Artificial Intelligence. </booktitle> <publisher> Computer Science Press, </publisher> <year> 1990. </year>
Reference-contexts: This method reduces the sensitivity on the value of the threshold and makes it easier to find automatically. As seen in Figure 6, thresholding the image creates a set of near points. To connect these points we dilate the image <ref> [15] </ref>. Once the preprocessing of the image is done, locating the "nose-twinkle" is not too much of a problem. Deciding where to start the search is a crucial point in the improvement of the speed of the algorithm.
Reference: [16] <author> Matthew Turk and Alex Pentland. </author> <title> Eigen-faces for Recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <month> March </month> <year> 1991. </year>
Reference: [17] <author> Ross Wagner and H.L. Galiana. </author> <title> Image Registration Applied to Measuring eye Movements via Template Matching. </title> <journal> Submitted to IEEE Transactions on Biomedical Engineering, </journal> <year> 1991. </year>
Reference: [18] <author> A. Yarbus. </author> <title> Eye Movements and Vision. </title> <publisher> Plenum Press, </publisher> <year> 1967. </year>

References-found: 18

