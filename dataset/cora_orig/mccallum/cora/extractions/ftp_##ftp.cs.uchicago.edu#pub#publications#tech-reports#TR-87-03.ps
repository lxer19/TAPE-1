URL: ftp://ftp.cs.uchicago.edu/pub/publications/tech-reports/TR-87-03.ps
Refering-URL: http://cs-www.uchicago.edu/publications/tech-reports/
Root-URL: 
Title: Term-Rewriting Implementation of Equational Logic Programming  
Author: Michael J. O'Donnell 
Date: 1987  
Address: Chicago  
Affiliation: The University of  
Abstract: In 1975 I started a small project to explore the consequences of implementing equational programs with no semantic compromises. Latest results include a compiler that executes exactly the logical consequences of an equational program, with run-time speed comparable to compiled Franz LISP. This paper describes the accomplishments of the project very briefly, concentrating on shortcomings and directions for future work. 
Abstract-found: 1
Intro-found: 1
Reference: [BF79] <author> O. P. Buneman and R. E. Frankel. </author> <title> FQL | a functional query language. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 52-57. </pages> <note> SIGMOD, May-June 1979. </note>
Reference-contexts: There is also potential benefit in complex left-hand sides and sophisticated pattern matching if an equational program is used to represent the information in a database. Buneman and Frankel <ref> [BF79] </ref> have proposed equational databases as an alternative to relational ones. Equational Logic Programming seems to have the same natural connection to equational databases that Prolog has to relational databases.
Reference: [BG80] <author> R. M. Burstall and J. A. Goguen. </author> <title> The semantics of Clear, a specification language. </title> <booktitle> In Proceedings of the 1979 Copenhagen Winter School on Abstract Software Specification, volume 86 of Lecture Notes in Computer Science, </booktitle> <pages> pages 292-332, </pages> <year> 1980. </year>
Reference-contexts: Goguen et al. have developed some useful modular constructs for the equational language OBJ, but they do not follow any discipline, such as regularity, for guaranteeing completeness <ref> [BG80] </ref> [FGJM85]. While I still hope that a thoroughly satisfactory modular treatment of Church-Rosser equational programs can be found, there seems to be more immediate promise in abandoning the Church-Rosser property.
Reference: [BS76] <author> N. D. Belnap and T. B. Steel. </author> <title> The Logic of Questions and Answers. </title> <publisher> Yale University Press, </publisher> <year> 1976. </year>
Reference-contexts: Such a view leads to the following schema for Logic Programming. A program is a set of assertions in some logical language. An input is a question, specifying syntactically a set of possible (not necessarily correct) answers <ref> [BS76] </ref> A correct output for a given input is a logical consequence of the program that answers the input question. A sound implementation of a program is code that reads questions, and produces only correct answers. A complete implemen tation is one that produces a correct answer whenever one exists.
Reference: [Che80] <author> L. P. Chew. </author> <title> An improved algorithm for computing with equations. </title> <booktitle> In 21st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 108-117. </pages> <publisher> IEEE, </publisher> <year> 1980. </year> <month> 16 </month>
Reference-contexts: There appears to be some convergence between these ideas based on term rewriting, and recent work on resolution-based equational theorem proving [DH86]. The final problem, of avoiding search through many different reduction sequences, is probably solved by a careful application of Paul Chew's directed congruence closure algorithm <ref> [Che80] </ref>. Congruence closure was originally intended to provide decision procedures for theories of finite sets of equational postulates, with no variables. <p> Instead of actually performing a reduction, Chew's directed congruence closure procedure adds the next instance of a reduction rule that would be applied to an initially empty finite set of ground instances, and performs congruence closure on the result <ref> [Che80] </ref>. The directed congruence closure technique has never been tested in practice, although it appears to be susceptible to efficient implementation.
Reference: [Che81] <author> L. P. Chew. </author> <title> Unique normal forms in term rewriting systems with repeated variables. </title> <booktitle> In 13th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 7-18, </pages> <year> 1981. </year>
Reference-contexts: There are also many natural equations, such as equal (x; x) = true, that violate the left-linearity constraint. Such violations seem to be less common, and less crucial in practice, and the theoretical results needed to deal with them appear to be quite difficult. Paul Chew <ref> [Che81] </ref> proved that nonoverlapping, but not necessarily left-linear, equations produce unique normal forms, but they do not always have the Church-Rosser property.
Reference: [DH86] <author> V. J. Digricoli and M. C. Harrison. </author> <title> Equality-based binary resolution. </title> <journal> Journal of the ACM, </journal> <volume> 33(2) </volume> <pages> 253-289, </pages> <year> 1986. </year>
Reference-contexts: There appears to be some convergence between these ideas based on term rewriting, and recent work on resolution-based equational theorem proving <ref> [DH86] </ref>. The final problem, of avoiding search through many different reduction sequences, is probably solved by a careful application of Paul Chew's directed congruence closure algorithm [Che80]. Congruence closure was originally intended to provide decision procedures for theories of finite sets of equational postulates, with no variables.
Reference: [DST80] <author> P. J. Downey, R. Sethi, and R. E. Tarjan. </author> <title> Variations on the common subexpression problem. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 758-771, </pages> <year> 1980. </year>
Reference-contexts: Kozen noticed that there is a polynomial time algorithm for congruence closure [Koz77], and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms <ref> [DST80] </ref>, but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers [NO80].
Reference: [FGJM85] <author> K. Futatsugi, J. Goguen, J.-P. Jouannaud, and J. Meseguer. </author> <booktitle> Principles of OBJ2. In 12th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 52-66. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1985. </year>
Reference-contexts: Goguen et al. have developed some useful modular constructs for the equational language OBJ, but they do not follow any discipline, such as regularity, for guaranteeing completeness [BG80] <ref> [FGJM85] </ref>. While I still hope that a thoroughly satisfactory modular treatment of Church-Rosser equational programs can be found, there seems to be more immediate promise in abandoning the Church-Rosser property.
Reference: [FW76] <author> D. Friedman and D. Wise. </author> <title> Cons should not evaluate its arguments. </title> <booktitle> In 3rd International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 257-284. </pages> <publisher> Edinburgh Unversity Press, </publisher> <year> 1976. </year>
Reference-contexts: Compiled programs are guaranteed to behave precisely as required by the description of Equational Logic Programming above. The requirement of completeness leads to the uniform use of outermost reduction, which is often called lazy evaluation [HM76] <ref> [FW76] </ref>. Some pragmatically essential notational extensions are available, including the conventional arithmetic primitives, but all such extensions are precisely equivalent semantically to large sets of equations.
Reference: [HL79] <author> G. Huet and J.-J. Levy. </author> <title> Computations in non-ambiguous linear term rewriting systems. </title> <type> Technical Report 359, IRIA, </type> <year> 1979. </year>
Reference-contexts: We now have a compiler, produced by Robert Strandh, that clearly establishes that acceptable performance is achievable. The compiler accepts regular systems of equations, with a small additional restriction called strong left-sequentiality [HO79] based on Huet and Levy's strong sequentiality <ref> [HL79] </ref>, that allows left-right sequential processing similar to, but actually more general than, leftmost-outermost reduction. Compiled programs are guaranteed to behave precisely as required by the description of Equational Logic Programming above. <p> Intuitively, a parallel (or at least interleaved) reduction of ff and fi seems to be required. The known techniques for analyzing sequentiality <ref> [HL79] </ref> [O'D85] [HOS85] [HO79] extend naturally to techniques for detecting a small number of cases in which parallel reduction is required.
Reference: [HM76] <author> P. Henderson and J. H. Morris. </author> <title> A lazy evaluator. </title> <booktitle> In 3rd Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 95-103. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1976. </year>
Reference-contexts: Compiled programs are guaranteed to behave precisely as required by the description of Equational Logic Programming above. The requirement of completeness leads to the uniform use of outermost reduction, which is often called lazy evaluation <ref> [HM76] </ref> [FW76]. Some pragmatically essential notational extensions are available, including the conventional arithmetic primitives, but all such extensions are precisely equivalent semantically to large sets of equations.
Reference: [HO79] <author> C. M. Hoffmann and M. J. O'Donnell. </author> <title> Interpreter generation using tree pattern matching. </title> <booktitle> In 6th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 169-179. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1979. </year>
Reference-contexts: We now have a compiler, produced by Robert Strandh, that clearly establishes that acceptable performance is achievable. The compiler accepts regular systems of equations, with a small additional restriction called strong left-sequentiality <ref> [HO79] </ref> based on Huet and Levy's strong sequentiality [HL79], that allows left-right sequential processing similar to, but actually more general than, leftmost-outermost reduction. Compiled programs are guaranteed to behave precisely as required by the description of Equational Logic Programming above. <p> Intuitively, a parallel (or at least interleaved) reduction of ff and fi seems to be required. The known techniques for analyzing sequentiality [HL79] [O'D85] [HOS85] <ref> [HO79] </ref> extend naturally to techniques for detecting a small number of cases in which parallel reduction is required. That is, index sets may be identified, where in order to reach normal form at least one redex in each set must be reduced, but it is not apparent which.
Reference: [HO80] <author> G. Huet and D. Oppen. </author> <title> Equations and rewrite rules: a survey. </title> <editor> In R. Book, editor, </editor> <title> Formal Languages: Perspectives and Open Problems. </title> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Term rewriting is already widely used for testing equality, using systems of rules that are guaranteed always to produce unique normal forms <ref> [HO80] </ref>. A significant extension of these techniques to systems without termination could be very valuable. Even when applied to terminating systems, techniques designed for nonterminating systems are often more efficient, since they must avoid unnecessary and potentially infinite subcomputations. Three substantial technical problems arise in such a generalization.
Reference: [HOS85] <author> C. M. Hoffmann, M. J. O'Donnell, and R. I. Strandh. </author> <title> Implementation of an interpreter for abstract equations. </title> <journal> Software | Practice and Experience, </journal> <volume> 15(12) </volume> <pages> 1185-1203, </pages> <year> 1985. </year>
Reference-contexts: More thorough descriptions of the current system are available in <ref> [HOS85] </ref> [O'D87] [O'D85]. In this paper I concentrate on shortcomings of the language, and directions for further research. 2 Regular Systems are Too Limited It is easy to write equational programs that look very similar to LISP programs. <p> Intuitively, a parallel (or at least interleaved) reduction of ff and fi seems to be required. The known techniques for analyzing sequentiality [HL79] [O'D85] <ref> [HOS85] </ref> [HO79] extend naturally to techniques for detecting a small number of cases in which parallel reduction is required. That is, index sets may be identified, where in order to reach normal form at least one redex in each set must be reduced, but it is not apparent which.
Reference: [Jay85] <author> B. Jayaraman. </author> <title> Equational programming: A unifying approach to functional and logic programming. </title> <type> Technical Report 85-030, </type> <institution> The University of North Carolina, </institution> <year> 1985. </year> <month> 17 </month>
Reference-contexts: Significant generalization of the Knuth-Bendix procedure in this direction is completely open. The second problem, sequencing, is not solved, but a promising direction is apparent. Jayaraman noticed that, when trying to prove ff = fi, as long as the outermost symbols disagree, only outermost reductions need be tried <ref> [Jay85] </ref>. Once the outermost symbols agree, it is necessary to explore in parallel further outermost reductions and a decomposed problem of proving equality of corresponding arguments. Within each argument the same reasoning applies.
Reference: [KB70] <author> D. E. Knuth and P. Bendix. </author> <title> Simple word problems in universal algebras. </title> <editor> In J. Leech, editor, </editor> <booktitle> Computational Problems in Abstract Algebra, </booktitle> <pages> pages 127-146. </pages> <publisher> Pergammon Press, Oxford, </publisher> <year> 1970. </year>
Reference-contexts: It appears, however, that a really desirable equational programming language should allow benign overlaps that do not destroy the Church-Rosser property. Of course, the Church-Rosser property is undecidable, so decidable sufficient conditions are required. The Knuth-Bendix procedure <ref> [KB70] </ref> 5 solves this problem for systems in which every reduction sequence leads to normal form.
Reference: [Klo80] <author> J. W. Klop. </author> <title> Combinatory Reduction Systems. </title> <type> PhD thesis, </type> <institution> Math-ematisch Centrum, </institution> <address> Amsterdam, </address> <year> 1980. </year>
Reference-contexts: Christoph Hoffmann, Paul Chew, Robert Strandh, Paul Golick, and Giovanni Sacco all collaborated in various ways. [O'D87] describes the project in more detail. In essence, we produced a programming language whose programs are sets of equations which, when treated as rewrite rules, are regular systems of rules <ref> [Klo80] </ref>, i.e., they are nonoverlapping and left-linear. [O'D77] proves that regular systems have the confluence, or Church-Rosser property, which is sufficient to guarantee the completeness of term-rewriting as an implementation of Equational Logic Programming.
Reference: [Koz77] <author> D. Kozen. </author> <title> Complexity of finitely presented algebras. </title> <booktitle> In 9th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 164-177, </pages> <year> 1977. </year>
Reference-contexts: It is easy to see that all equalities between terms corresponding to nodes in the graph that are logical consequences of the postulates will be generated by this procedure. Kozen noticed that there is a polynomial time algorithm for congruence closure <ref> [Koz77] </ref>, and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms [DST80], but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers [NO80].
Reference: [NO80] <author> G. Nelson and D. C. Oppen. </author> <title> Fast decision algorithms based on congruence closure. </title> <journal> Journal of the ACM, </journal> <volume> 27(2) </volume> <pages> 356-364, </pages> <year> 1980. </year>
Reference-contexts: congruence closure [Koz77], and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms [DST80], but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers <ref> [NO80] </ref>. For Equational Logic Programming, the congruence closure technique looks very attractive, because it never evaluates the same term twice. Unfortunately, the technique does not apply directly, because of the use of variables in equational programs.
Reference: [O'D77] <author> M. J. O'Donnell. </author> <title> Computing in Systems Described by Equations, </title> <booktitle> volume 58 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: In essence, we produced a programming language whose programs are sets of equations which, when treated as rewrite rules, are regular systems of rules [Klo80], i.e., they are nonoverlapping and left-linear. <ref> [O'D77] </ref> proves that regular systems have the confluence, or Church-Rosser property, which is sufficient to guarantee the completeness of term-rewriting as an implementation of Equational Logic Programming.
Reference: [O'D85] <author> M. J. O'Donnell. </author> <title> Equational Logic as a Programming Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: More thorough descriptions of the current system are available in [HOS85] [O'D87] <ref> [O'D85] </ref>. In this paper I concentrate on shortcomings of the language, and directions for further research. 2 Regular Systems are Too Limited It is easy to write equational programs that look very similar to LISP programs. <p> These equations code nicely into an executable equational program <ref> [O'D85] </ref>, but they fail to remove trailing zeroes. The conceptually natural way to eliminate the zeroes is to add the two equations 4) X fi 0 = 0 but this introduces overlaps. For example, equations 2 and 4 overlap in (7+X fi0)+23. <p> The conceptually natural way to eliminate the zeroes is to add the two equations 4) X fi 0 = 0 but this introduces overlaps. For example, equations 2 and 4 overlap in (7+X fi0)+23. There is a regular solution, but it is rather ugly <ref> [O'D85] </ref>, and involves sneakily encoding control information into semantically irrelevant aspects of the form of a polynomial. <p> There is one example | weak reduction in the Combinator Calculus, with the binary function AP P LY appearing both leftmost and nonleftmost in left-hand sides | where the perfect system of equations is regular but not LISP-like <ref> [O'D85] </ref>. There is also potential benefit in complex left-hand sides and sophisticated pattern matching if an equational program is used to represent the information in a database. Buneman and Frankel [BF79] have proposed equational databases as an alternative to relational ones. <p> Intuitively, a parallel (or at least interleaved) reduction of ff and fi seems to be required. The known techniques for analyzing sequentiality [HL79] <ref> [O'D85] </ref> [HOS85] [HO79] extend naturally to techniques for detecting a small number of cases in which parallel reduction is required. That is, index sets may be identified, where in order to reach normal form at least one redex in each set must be reduced, but it is not apparent which. <p> This problem is very simple to solve, in principle. Incremental reading of the input as it is needed is also very easy to program, but that has not 8 yet been done <ref> [O'D85] </ref>. A more serious problem conceptually is the arbitrariness of the left-to-right prefix order in which symbols of a term appear in conventional notation. The current implementation's treatment of incremental output, and the easy version of incremental input, require terms to be processed in that fixed order. <p> The most natural representative of the meaning of a program P seems to be a triple consisting of the term language defined by P, the set of models satisfying the equations in P, and the set of normal forms. In <ref> [O'D85] </ref> I tried to develop semantic constructs for combining such triples. Aside from some glaring errors in the definitions, the basic idea seems to be a failure, because combinations of regular systems are not necessarily regular, nor are they Church-Rosser.
Reference: [O'D87] <author> M. J. O'Donnell. </author> <title> Survey of the equational logic programming project. </title> <booktitle> In Colloquium on Resolution of Equations in Algebraic Structures, </booktitle> <year> 1987. </year>
Reference-contexts: In 1975, I started a small project to explore the consequences of implementing Equational Logic Programming with no semantic compromises. Christoph Hoffmann, Paul Chew, Robert Strandh, Paul Golick, and Giovanni Sacco all collaborated in various ways. <ref> [O'D87] </ref> describes the project in more detail. <p> More thorough descriptions of the current system are available in [HOS85] <ref> [O'D87] </ref> [O'D85]. In this paper I concentrate on shortcomings of the language, and directions for further research. 2 Regular Systems are Too Limited It is easy to write equational programs that look very similar to LISP programs.
Reference: [Str84] <author> R. I. Strandh. </author> <title> Incremental suffix trees with multiple subject strings. </title> <type> Technical Report JHU/EECS-84/18, </type> <institution> The Johns-Hopkins University, </institution> <year> 1984. </year> <month> 18 </month>
Reference-contexts: An extension of such techniques incorporating derived equations between nonground terms would be very interesting and useful. Once the theoretical problems regarding which derived equations to add are solved, Strandh's incremental algorithm for generating pattern-matching tables <ref> [Str84] </ref> will probably provide a good technique for adding such rules on the fly. Beyond proving equalities, it would be very useful to be able to solve equalities between terms with variables.
References-found: 23

