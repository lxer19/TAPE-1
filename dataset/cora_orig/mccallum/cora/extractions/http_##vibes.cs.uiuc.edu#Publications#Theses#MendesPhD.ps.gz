URL: http://vibes.cs.uiuc.edu/Publications/Theses/MendesPhD.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/Theses/theses.htm
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Celso Luiz Mendes 
Date: 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Vikram Adve, Alan Carle, Elana Granston, Seema Hiranandani, Ken Kennedy, Charles Koelbel, John Mellor-Crummey, and Scott Warren. </author> <title> Requirements for data parallel programming environments. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 48-58, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Such estimates, however, are presented in terms of predicted values for those selected parameters, instead of the execution time of the program or its component sections. Adve et al <ref> [1] </ref> provided an overview of the various challenges involved in creating an environment for efficient programming in data parallel languages. <p> However, that effort has not yet reached an ideal stage. Some methods derive a prediction for a specific combination of number of processors (P ) and problem size (N ), like in <ref> [1] </ref>, [22] and [50]. Others provide a symbolic model that can be evaluated at desired combinations of N and P , but either have a very limited application domain, as in [64], or require several executions of the program for model calibration, as in [15].
Reference: [2] <author> Vikram S. Adve, John Mellor-Crummey, Mark Anderson, Ken Kennedy, Jhy-Chun Wang, and Daniel A. Reed. </author> <title> An integrated compilation and performance analysis environment for data parallel programs. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Given a construct in one of the codes, the mapping indicates the associated construct (s) in the other. A detailed description of the SDDF record contents is beyond the scope of this study; see <ref> [2] </ref> for details. <p> For each SPMD program primitive, we compute its cost by summing the costs for all its component primitives. By induction, the aggregate program execution time is derived as the sum of its constituent primitive times. 2 The mapping strategy is discussed in depth in <ref> [2] </ref>. 67 compiler to instrument the compiled code, to record information on compiler analysis and transformations, as well as to export static performance information, uses the Pablo instrumentation software's extension interfaces for capturing dynamic performance data, and incorporates a software toolkit to combine the static and dynamic performance data and relate <p> lower and upper bound estimates of the execution time on that system. 7.1.1 Execution of a Real Program As a first example of the use of our prediction methodology, we analyze the performance of the Erlebacher program, an 800 line, ten procedure benchmark 1 written by Thomas Eidson, at ICASE <ref> [2, 21] </ref>. Erlebacher solves 3-D partial differential equations via tridiagonal solves using Alternating-Direction-Implicit (ADI) integration. The program operates in succession on each of the three dimensions (X, Y and Z), according to the pseudocode in Figure 7.1. In each dimension, derivatives are computed, followed by forward and backward substitution steps.
Reference: [3] <author> Vikram S. Adve and Mary K. Vernon. </author> <title> The influence of random delays on parallel execution times. </title> <booktitle> In Proceedings of the ACM Conference on Measurement & Modeling of Computer Systems - SIGMETRICS'93, </booktitle> <pages> pages 61-73, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Mak and Lundstrom [39] modeled a parallel computation as a series-parallel directed acyclic graph and machine resources as service centers in a queueing network model. On several test cases, they obtained very accurate predictions. However, Adve and Vernon <ref> [3] </ref> suggested recently that stochastic models may create unnecessary modeling complexity. They showed that the overall variance of execution time due to non-deterministic factors is small for most shared-memory programs, and presented the potential advantages of using a deterministic model for parallel program performance prediction.
Reference: [4] <author> Ramune Arlauskas. </author> <title> iPSC/2 system: A second generation hypercube. </title> <booktitle> In Proceedings of the 3 rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 38-42, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Using benchmark data, one can build a model of the time to send or receive messages of a given length. 4.1.1 Multicomputer Examples As an example of architectural parameters for real systems, consider the Intel iPSC/2 <ref> [4] </ref> and Intel iPSC/860 [60] multicomputers. They consist of up to 128 processing nodes interconnected in a hypercube structure. The interconnection network is the same on both machines, and there is hardware support for routing messages between the nodes using a circuit-switching scheme.
Reference: [5] <author> Ruth A. Aydt. </author> <title> The Pablo Self-Defining Data Format. </title> <institution> University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: This file, comprising static information about the program, is written in the Pablo SDDF format <ref> [5] </ref>, and contains one record for each occurrence of those primitives (loop, procedure call and communication call) in the SPMD code.
Reference: [6] <author> H. B. Bakoglu, Gregory F. Grohoski, and Robert K. Montoye. </author> <title> The IBM RISC system/6000 processor: Hardware overview. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1) </volume> <pages> 12-22, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: To determine the CP I values for the IBM SP2 and SGI Power-Challenge, we must use only the system specifications (remember that we are assuming that these systems are not available for tests). We consider an IBM SP2 system based on a Power processor <ref> [6] </ref> operating at 62.5 MHz, and an SGI Power-Challenge with a MIPS R10000 processor [45] at 195 MHz. The IBM Power processor has a 3-way superscalar architecture, and can issue up to one integer and two floating-point instructions per cycle.
Reference: [7] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator in the Fortran D programming system. In Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: This tool would enable user interaction, and would evaluate candidate distributions 12 using an integer-programming framework [11]; the expected performance from each candidate distribution would be derived using the previously observed computation and communication behavior of training sets <ref> [7] </ref>, consisting of small meta-benchmarks with the various constructs that are common in data parallel programs. 2.5 Summary Most performance prediction studies assume that the underlying program is stable, and do not attempt to confirm that assumption.
Reference: [8] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1993. </year>
Reference-contexts: than one. * Number of arithmetic operations: Some of the loops contained a large number of operations, involving many different elements, while others contained only one operation. * Type of data dependences: There were all types of data dependences between iterations in the various loops: flow, anti and output dependences <ref> [8] </ref>; some of the loops presented more than one type of dependence. For most loops, the distance vector of the dependence was constant, but for a few of them it was variable. <p> The existing data dependence is a flow dependence in the third dimension of the array, and that dependence is carried by the innermost loop. Loop interchange, a transformation that is available in many current compilers, is a possible transformation for this loop nest <ref> [8] </ref>. Under such interchange, the execution order between dependent iterations is preserved, hence this transformation is always valid. In the present example, we arbitrarily decided to interchange the two innermost loops in that loop nest.
Reference: [9] <author> H. G. Barrow and R. M. Burstall. </author> <title> Subgraph isomorphism, matching relational structures and maximal cliques. </title> <journal> Information Processing Letters, </journal> <volume> 4(4) </volume> <pages> 83-84, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: It can be used to compare graphs G 1 and G 2 and provides a quantitative measure of similarity. Algorithms for direct computation of d are known <ref> [9, 34] </ref>. However, the time complexity of these algorithms is O (n!) for an n node graph.
Reference: [10] <author> Dileep Bhandarkar. </author> <title> RISC versus CISC: A tale of two chips. </title> <journal> Computer Architecture News, </journal> <volume> 25(1) </volume> <pages> 1-12, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Many processors today provide performance counters, in hardware, that can reveal this information for a given execution <ref> [10, 65, 67] </ref>. If such counters are not available, one can obtain the dynamic instruction counts with low-level instrumentation of the program. * CPI values: These values depend on a wide variety of factors, including details in the system organization, processor structure and compiler effectiveness.
Reference: [11] <author> Robert Bixby, Ken Kennedy, and Ulrich Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93349-S, </type> <institution> CRPC/Rice University, </institution> <year> 1993. </year>
Reference-contexts: This tool would enable user interaction, and would evaluate candidate distributions 12 using an integer-programming framework <ref> [11] </ref>; the expected performance from each candidate distribution would be derived using the previously observed computation and communication behavior of training sets [7], consisting of small meta-benchmarks with the various constructs that are common in data parallel programs. 2.5 Summary Most performance prediction studies assume that the underlying program is stable,
Reference: [12] <author> Robert J. Block, Pankaj Mehra, and Sekhar Sarukkai. </author> <title> Automated performance prediction of message-passing parallel programs. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach, however, does not allow a direct comparison between the costs of individual program sections. Also, the source program is assumed to be already in an SPMD form, with explicit message passing. As an improvement of this technique, Block et al <ref> [12] </ref> added to the toolkit a mechanism to automatically recognize communication patterns in the program, and derive scalability models that accurately represented the execution time of these patterns.
Reference: [13] <author> Brian M. Carlson, Thomas D. Wagner, Lawrence W. Dowdy, and Patrick H. Worley. </author> <title> Speedup properties of phases in the execution profile of distributed parallel programs. </title> <type> Technical Report ORNL/TM-11900, </type> <institution> Oak Ridge National Laboratory, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: That work has similarities to our trace transformation technique, but it can be applied only to target machines that already exist or can be simulated, so that the benchmarks can be executed. 2.3 Performance Scalability Analysis Carlson et al <ref> [13] </ref> studied the performance of parallel systems using execution profiles, which specify the number of busy processors as a function of time. On such profiles, the authors identified phases of homogeneous utilization, and characterized the program scalability based on the scalability of individual phases.
Reference: [14] <author> Mark J. Clement and Michael J. Quinn. </author> <title> Analytical performance prediction on multicom-puters. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 886-894, </pages> <address> Portland, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: This technique applied only to programs where the source code had explicit parallel constructs according to the proposed formalism, and did not address the issue of compilation from higher-level languages. Clement and Quinn <ref> [14] </ref> presented an analytical modeling technique to predict the speedup of applications written in Dataparallel C [26], a SIMD model of parallel programming with explicit parallel extensions to the C language.
Reference: [15] <author> Mark J. Clement and Michael J. Quinn. </author> <title> Symbolic performance prediction of scalable parallel programs. </title> <booktitle> In Proceedings of the 9 th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Their technique was also limited to the case where parallelism is already explicit in the source program, and they did not originally conduct any study of scalability under variations in problem size. More recently, they extended this work <ref> [15] </ref> to study scalability of both the problem size and the number of processors, and build a symbolic model that represents the predicted execution time as a function of those parameters. <p> Others provide a symbolic model that can be evaluated at desired combinations of N and P , but either have a very limited application domain, as in [64], or require several executions of the program for model calibration, as in <ref> [15] </ref>. There has been no proposed method, so far, that provides a first-order, easily derivable model of the application's execution time (and of the execution times for internal code sections) as a function of the number of processors and problem size. Our symbolic scalability prediction method targets precisely this area.
Reference: [16] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The general problem of finding the longest common subsequence between two sequences of length n can be solved in O (n 2 ) time using a dynamic programming algorithm <ref> [16] </ref>. However, this time complexity can be improved for special cases.
Reference: [17] <author> Mark E. Crovella and Thomas L. LeBlanc. </author> <title> Parallel performance prediction using lost cycles analysis. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <pages> pages 600-609, </pages> <address> Washington, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Also, the derivation of the sequencing trees might become intractable for a program with a nontrivial number of events. Crovella and LeBlanc <ref> [17] </ref> predicted performance of parallel programs based on lost cycles analysis, which involves measurements and modeling of the sources of overhead in a program.
Reference: [18] <author> Suresh K. Damodaran-Kamal and Joan M. Francioni. </author> <title> Nondeterminacy: Testing and debugging in message passing parallel programs. </title> <booktitle> In ACM/ONR Workshop on Parallel and Distributed Debugging Proceedings, </booktitle> <pages> pages 118-128, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year> <month> 144 </month>
Reference-contexts: This approach allows replaying the program, assuming different architecture parameters, and assessing the influence of such parameters on performance, given the same event order. It provides no indication, however, of possible effects from changes in the event order caused by variations in those parameters. Damodaran-Kamal and Francioni <ref> [18] </ref> presented a technique to test for nondeterminacy in message passing parallel programs, based on controlled execution. The message delivery order at a receive operation can be fully permuted, simulating program behavior under possible 6 race scenarios in which more than one message can satisfy the receive. <p> The studies of program stability, in general, either try to induce and detect the occurrence of races, like in <ref> [18] </ref>, or seek to discover the sensitive of a certain metric (e.g. execution time) to specific sections in the code, like in [38]. None of these methods addresses the quantification of the potential instability in program behavior, in terms of variability in execution events for repeated runs of the program.
Reference: [19] <author> Narsingh Deo. </author> <title> Graph Theory with Applications to Engineering and Computer Science. </title> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: Although it is unknown if graph isomorphism is NP-hard, no polynomial time algorithm is known <ref> [51, 19] </ref> and current algorithms are very computationally expensive. In our case, testing for isomorphism is insufficient | two execution graphs might be similar, but not isomorphic. We need, to determine how "similar" they are. In other words, we need a metric to compare graphs.
Reference: [20] <author> K. M. Dixit. </author> <title> Overview of the SPEC benchmarks. </title> <editor> In Jim Gray, editor, </editor> <booktitle> The Benchmark Handbook for Database and Transaction Processing Systems, </booktitle> <pages> pages 489-524. </pages> <publisher> Morgan Kauf-mann Publishers Inc, </publisher> <address> San Mateo, California, </address> <year> 1994. </year>
Reference-contexts: By assumption, this ratio can be either a constant or dependent on some aspect of the code. The simplest approximation assumes a single ratio that could be derived from published performance data for the two processors (e.g., SPEC ratio <ref> [62, 20] </ref>). In the case of two existing systems, we can derive that ratio by executing a sequential version of the program on both systems with a reduced data set, and computing the ratio of the total execution times. A better alternative uses a variable ratio.
Reference: [21] <author> Thomas M. Eidson and Gordon Erlebacher. </author> <title> Implementation of a fully-balanced periodic tridiagonal solver on a parallel distributed memory architecture. </title> <type> Technical Report TR-94-37, </type> <institution> ICASE, </institution> <year> 1994. </year>
Reference-contexts: lower and upper bound estimates of the execution time on that system. 7.1.1 Execution of a Real Program As a first example of the use of our prediction methodology, we analyze the performance of the Erlebacher program, an 800 line, ten procedure benchmark 1 written by Thomas Eidson, at ICASE <ref> [2, 21] </ref>. Erlebacher solves 3-D partial differential equations via tridiagonal solves using Alternating-Direction-Implicit (ADI) integration. The program operates in succession on each of the three dimensions (X, Y and Z), according to the pseudocode in Figure 7.1. In each dimension, derivatives are computed, followed by forward and backward substitution steps.
Reference: [22] <author> Thomas Fahringer. </author> <title> Automatic Performance Prediction of Parallel Programs. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1996. </year>
Reference-contexts: The derivation of this model, however, required statistical methods and several experimental runs of the program with different problem sizes and numbers of processors. Fahringer <ref> [22] </ref> designed a performance prediction tool named PPPT (Parameter-based Performance Prediction Tool), which analyzes a set of parameters that characterize the behavior of a parallel program, including work distribution, amount of communication and data locality. <p> However, that effort has not yet reached an ideal stage. Some methods derive a prediction for a specific combination of number of processors (P ) and problem size (N ), like in [1], <ref> [22] </ref> and [50]. Others provide a symbolic model that can be evaluated at desired combinations of N and P , but either have a very limited application domain, as in [64], or require several executions of the program for model calibration, as in [15].
Reference: [23] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Available from http://www.crpc.rice.edu/HPFF/hpf2/index.html, January 1997. </note>
Reference-contexts: We present a general overview of our 59 methodology, describe the details involved in the prediction technique, and illustrate its use with examples from Fortran D [28], a precursor to HPF <ref> [23] </ref>. <p> High Performance Fortran (HPF), a recently developed language, extends Fortran with support for data parallel programming <ref> [23, 36] </ref>. One of its main goals is to provide high-performance on parallel computers with non-uniform memory access costs, while ensuring portability across different architectures.
Reference: [24] <author> Ananth Y. Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Derivation of the properties for a given phase, however, may require executions with several distinct numbers of processors. Identifying the borders of such phases can also become computationally difficult for long-running programs with a large number of execution trace events. Grama, Gupta and Kumar <ref> [24] </ref> studied the performance of parallel systems using isoeffi-ciency analysis. For a given algorithm/system combination, they defined the corresponding 8 isoefficiency function as the required growth in the input data set size to maintain the same efficiency, as the number of processors grows.
Reference: [25] <author> William Gropp, Edwing Lusk, and Anthony Skjellum. </author> <title> Using MPI. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: The receiving task is kept blocked until the specified message arrives. An intermediate approach specifies some of the parameters for a required message; the receiver may obtain a message from several senders, as long as that message satisfies the specified constraints. The Message Passing Interface (MPI) <ref> [25, 49] </ref>, an emerging standard for explicit message passing in parallel programs, provides communication functions with all those variations. Those functions allow the specification of a message tag, indicating the type of the underlying message. <p> The regular output from the D95 compiler is an SPMD program in Fortran-77, containing function calls to a runtime support library and to MPI functions <ref> [25] </ref> that implement message passing. As a limitation of the 1 As noted in the previous chapter, the problem size may include multiple dimensions.
Reference: [26] <author> P. J. Hatcher and M. J. Quinn. </author> <title> Data Parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: This technique applied only to programs where the source code had explicit parallel constructs according to the proposed formalism, and did not address the issue of compilation from higher-level languages. Clement and Quinn [14] presented an analytical modeling technique to predict the speedup of applications written in Dataparallel C <ref> [26] </ref>, a SIMD model of parallel programming with explicit parallel extensions to the C language. They decomposed the execution time of an application into a sequential component, a parallelizable component, and some overhead due to communication.
Reference: [27] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, California, </address> <note> second edition, </note> <year> 1995. </year>
Reference-contexts: They vary by application, as well as among implementations with the same instruction set <ref> [27] </ref>. On existing 48 systems, the CP I values can be measured with benchmarks similar to the fragment code. For new systems, those values can be approximated using the system's computational specifications and the types of operations in the code fragment.
Reference: [28] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Data parallel languages, like High Performance Fortran (HPF) [36] and its precursor, Fortran D <ref> [28] </ref>, have been proposed as mechanisms to lessen this parallel programming burden. By allowing the programmer to construct a parallel application at a semantic higher level, without recourse to low-level message passing code, HPF is an effective specification language for regular, data parallel algorithms. <p> We present a general overview of our 59 methodology, describe the details involved in the prediction technique, and illustrate its use with examples from Fortran D <ref> [28] </ref>, a precursor to HPF [23]. <p> The model that we use for this part of our work is based on an integrated compilation and performance analysis environment, developed as an extended version of the Fortran D compiler <ref> [28] </ref> including capabilities of the Pablo system [52]. The Fortran D language and its original compiler were developed at Rice University, in the early 90's, as a set of tools for machine-independent parallel programming.
Reference: [29] <author> Intel Corporation. </author> <title> i860 Microprocessor Family Programmer's Reference Manual, </title> <year> 1992. </year>
Reference-contexts: We obtained the Paragon's CP I value for branches from the i860 specifications <ref> [29] </ref>, considering that most branches in this program correspond to the end of loops; the control flow is highly predictable in these cases.
Reference: [30] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year> <month> 145 </month>
Reference-contexts: For each experiment, factors are set at one of their possible levels (e.g., delay or no-delay). In a full factorial design <ref> [30] </ref>, with p locations selected, we must conduct 2 p experiments to determine the influence of each of the p perturbations. However, in practice, a much smaller number is needed, because interactions among distinct perturbations are not always significant.
Reference: [31] <author> James Kohn and Winifred Williams. </author> <title> ATExpert. </title> <journal> Journal of Parallel and Distributed Com--puting, </journal> <volume> 18(2) </volume> <pages> 205-222, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Also, it requires measurements that sample the entire range of configurations of interest. Finally, the selection of the particular analytic model to represent a certain class of overhead can become an extremely difficult task for the user. Kohn and Williams <ref> [31] </ref> developed ATExpert, a tool that employed expert systems to predict speedups obtainable in various regions of a program as the number of processors in a Cray computer was increased.
Reference: [32] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: This is Lamport's happens before relation <ref> [32] </ref>, denoted by &lt;. It has the following properties: 1.
Reference: [33] <author> Thomas J. Leblanc and John M. Mellor-Crummey. </author> <title> Debugging parallel programs with instant replay. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):471-482, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: After that review, we place our work in this context, underlining the peculiarities in our techniques. 2.1 Stability Assessment Program stability has been studied mostly in the context of parallel program debugging. Leblanc and Mellor-Crummey <ref> [33] </ref> developed a technique termed Instant Replay, that allowed reproduction of the execution behavior of a parallel program. By capturing the event order during program execution, it was possible to reproduce the same event order later, even with extra debugging statements in the program.
Reference: [34] <author> G. Levi. </author> <title> A note on the derivation of maximal common subgraphs of two directed or undirected graphs. </title> <journal> Calcolo, </journal> <volume> 9 </volume> <pages> 341-352, </pages> <year> 1972. </year>
Reference-contexts: It can be used to compare graphs G 1 and G 2 and provides a quantitative measure of similarity. Algorithms for direct computation of d are known <ref> [9, 34] </ref>. However, the time complexity of these algorithms is O (n!) for an n node graph.
Reference: [35] <author> David Levine, David Callahan, and Jack Dongarra. </author> <title> Test Suite for Vectorizing Compilers. </title> <year> 1991. </year>
Reference-contexts: and show that our methodology produces symbolic scalability expressions for all of them; also, in almost all cases, such expressions correctly predict performance under varying values of N and P on an existing parallel system. 6.4.1 Collection of Loops We consider the collection of loops prepared by Levine et al <ref> [35] </ref>. That collection consists of a variety of loop nests that represent different constructs intended to test the analysis capabilities of a vectorizing compiler. It comprises distinct types of computations that occur frequently on scientific applications.
Reference: [36] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Data parallel languages, like High Performance Fortran (HPF) <ref> [36] </ref> and its precursor, Fortran D [28], have been proposed as mechanisms to lessen this parallel programming burden. By allowing the programmer to construct a parallel application at a semantic higher level, without recourse to low-level message passing code, HPF is an effective specification language for regular, data parallel algorithms. <p> Other programs may have different execution graphs even for two executions on the same machine and data set. We call these programs unstable. Variations in execution behavior can also arise from compilation of a program written in a high-level language (e.g. HPF <ref> [36] </ref>), where any change in the data distribution, inserted by the 19 user or by the compiler, may affect the generated code. <p> High Performance Fortran (HPF), a recently developed language, extends Fortran with support for data parallel programming <ref> [23, 36] </ref>. One of its main goals is to provide high-performance on parallel computers with non-uniform memory access costs, while ensuring portability across different architectures.
Reference: [37] <author> Gordon Lyon, Raghu Kacker, and Arnaud Linz. </author> <title> A scalability test for parallel code. </title> <journal> Software Practice and Experience, </journal> <volume> 25(12) </volume> <pages> 1299-1314, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: The major restriction for this type of prediction is that the determination of the system-dependent parameters required several executions of the application on the underlying system. Lyon et al <ref> [37] </ref> proposed a method to assess the scalability of individual program sections based on statistically designed experiments (DEX).
Reference: [38] <author> Gordon Lyon, Robert Snelick, and Raghu Kacker. </author> <title> Synthetic-perturbation tuning of MIMD programs. </title> <type> Technical Report NISTIR 5131, </type> <institution> National Institute of Standards and Technology, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: They showed that the overall variance of execution time due to non-deterministic factors is small for most shared-memory programs, and presented the potential advantages of using a deterministic model for parallel program performance prediction. Lyon et al <ref> [38] </ref> made another claim against stochastic models, arguing that performance analysis is possible at a macro level, where particular details in the system or in the application are largely ignored. They inserted synthetic perturbations in a program, and measured their effects on global performance. <p> The studies of program stability, in general, either try to induce and detect the occurrence of races, like in [18], or seek to discover the sensitive of a certain metric (e.g. execution time) to specific sections in the code, like in <ref> [38] </ref>. None of these methods addresses the quantification of the potential instability in program behavior, in terms of variability in execution events for repeated runs of the program. <p> If the execution graph does not change significantly across machines, we can confidently use the event order on the first machine as a basis for prediction. One possible way to assess program stability relies on time perturbation analysis <ref> [38] </ref>. The idea is to perturb the original program and verify the effect of such perturbation. Several instrumented versions of the program are executed, each with a specific set of time delays inserted in the code.
Reference: [39] <author> Victor W. Mak and Stephen F. Lundstrom. </author> <title> Predicting performance of parallel computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 257-270, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This is implemented by keeping a record of every message transaction during the original execution of the application. Afterward, the execution can be replayed with different delivery orders for the various receive calls, testing for possible race conditions. Mak and Lundstrom <ref> [39] </ref> modeled a parallel computation as a series-parallel directed acyclic graph and machine resources as service centers in a queueing network model. On several test cases, they obtained very accurate predictions. However, Adve and Vernon [3] suggested recently that stochastic models may create unnecessary modeling complexity.
Reference: [40] <author> Allen D. Malony. </author> <title> Performance Observability. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The third reason is that with higher-level programming models (like HPF) there is a bigger semantic gap between source code and compiled code on parallel systems than in sequential ones. Malony <ref> [40] </ref> studied trace transformation, and developed time-based and event-based perturbation models for eliminating perturbations induced by software instrumentation. In an extension of that work, Sarukkai and Malony [57] applied perturbation models to transform message passing program traces.
Reference: [41] <author> Pankaj Mehra, Michelle Gower, and Michael A. Bass. </author> <title> Automated modeling of message-passing programs. </title> <booktitle> In Proceedings of the ACM/IEEE Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems - MASCOTS'94, </booktitle> <pages> pages 187-192, </pages> <address> Durham, </address> <month> January </month> <year> 1994. </year> <month> 146 </month>
Reference-contexts: Mehra, Schulback and Yan [42] conducted performance prediction by modeling message passing programs at varying levels of syntactic detail, using statistical regression techniques to infer the parameters in the model. This modeling process was completely manual. As a next step, Mehra et al <ref> [41] </ref> presented a system for automated modeling, using a grammar-driven approach to describe the observed performance data. Their system estimated numerical parameters in the models applying statistical regression.
Reference: [42] <author> Pankaj Mehra, Catherine H. Schulbach, and Jerry C. Yan. </author> <title> A comparison of two model--based performance-prediction techniques for message-passing parallel programs. </title> <booktitle> In Proceedings of the ACM Conference on Measurement & Modeling of Computer Systems - SIGMETRICS'94, </booktitle> <pages> pages 181-190, </pages> <address> Nashville, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: They also extended the toolkit to enable predictions by simulation of an abstract version of the program [58], represented in a behavior description language. Mehra, Schulback and Yan <ref> [42] </ref> conducted performance prediction by modeling message passing programs at varying levels of syntactic detail, using statistical regression techniques to infer the parameters in the model. This modeling process was completely manual.
Reference: [43] <author> John Mellor-Crummey and Vikram Adve. </author> <title> Fortran D95 Compiler Overview. </title> <note> Available from http://www.cs.rice.edu/ mpal/SC95, </note> <year> 1996. </year>
Reference-contexts: For different values of N and P , the new bottlenecks can be identified in a similar form. Chapter 7 contains examples showing how to apply this functionality. 6.2 Compilation Infrastructure To derive scalability models automatically, we used the infrastructure of the new Fortran D95 compilation system <ref> [43] </ref>. This system was designed to support research on data parallel programming in High Performance Fortran (HPF) and to explore extensions that would broaden HPF's applicability or enhance performance.
Reference: [44] <author> Webb Miller and Eugene W. Myers. </author> <title> A file comparison program. </title> <journal> Software|Practice and Experience, </journal> <volume> 15(11) </volume> <pages> 1025-1040, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: The general problem of finding the longest common subsequence between two sequences of length n can be solved in O (n 2 ) time using a dynamic programming algorithm [16]. However, this time complexity can be improved for special cases. Miller and Myers <ref> [44] </ref> presented an algorithm that has time complexity O (nD), where D is the number of deletions and insertions required to transform one sequence into the other (this is also called the edit distance between the two sequences).
Reference: [45] <institution> MIPS Technologies, Incorporated. MIPS R10000 Microprocessor: Product Overview, </institution> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: We consider an IBM SP2 system based on a Power processor [6] operating at 62.5 MHz, and an SGI Power-Challenge with a MIPS R10000 processor <ref> [45] </ref> at 195 MHz. The IBM Power processor has a 3-way superscalar architecture, and can issue up to one integer and two floating-point instructions per cycle. Meanwhile, the R10000 is also superscalar, issuing up to two integer, one load/store and two floating-point instructions per cycle.
Reference: [46] <author> A. G. Mohamed, G. C. Fox, G. von Laszewski, M. Parashar, T. Haupt, K. Mills, Ying-Hua Lu, NengTan Lin, and Nangkang Yeh. </author> <title> Application benchmark set for Fortran-D and High Performance Fortran. </title> <type> Technical Report SCCS-327, </type> <institution> Northeast Parallel Architectures Center, </institution> <year> 1992. </year>
Reference-contexts: The program is a Gaussian elimination code with partial pivoting (GE-Row), and is part of the HPF/Fortran-D benchmark suite <ref> [46] </ref>. The input matrices are distributed across the processors in a row-blocked fashion. Thus, a given matrix column is spread across all processors, and the location of the pivot element is dependent on the input matrix. <p> After describing our test scenario, defining the programs and machines involved in our experiments, we transform the traces and evaluate the accuracy of our predictions. 4.3.1 Test Scenario We selected two programs from the HPF/Fortran-D benchmark suite <ref> [46] </ref>, a collection of applications intended to support the validation of future HPF compilers. Those two programs were a Fast Fourier Transform (FFT) and a column-scattered version of Gaussian elimination (GE-Col), both with explicit message passing.
Reference: [47] <author> Eugene W. Myers. </author> <title> An O(ND) difference algorithm and its variations. </title> <journal> Algorithmica, </journal> <volume> 1 </volume> <pages> 251-266, </pages> <year> 1986. </year>
Reference-contexts: Thus, the algorithm is very efficient when the two sequences are similar, like in the case of stable programs. In a subsequent paper <ref> [47] </ref>, Myers refined this algorithm such that it also had linear space complexity. The Miller and Myers' algorithm was implemented as the diff utility in the Unix system. As an example, consider again the graphs from Figure 3.3.
Reference: [48] <author> Roger J. Noe. </author> <title> Pablo Instrumentation Environment Reference Manual. </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: For programs where other primitives occur inside the SPMD loop, the instrumentation overhead might become substantial. To reduce the cost of dynamic instrumentation and the total volume of captured data, we exploit the Pablo instrumentation library's support for real-time data reduction <ref> [48] </ref>. Rather than recording the generated events for post-mortem analysis, we would compute performance metrics as the events were generated. On a doubly nested loop, for example, this summarization would generate only the total time spent in the inner loop. Scalability model construction proceeds similarly for more complex codes.
Reference: [49] <author> Peter Pacheco. </author> <title> Parallel Programming with MPI. </title> <publisher> Morgan Kaufmann Publishers Inc, </publisher> <address> San Mateo, California, </address> <year> 1996. </year>
Reference-contexts: The receiving task is kept blocked until the specified message arrives. An intermediate approach specifies some of the parameters for a required message; the receiver may obtain a message from several senders, as long as that message satisfies the specified constraints. The Message Passing Interface (MPI) <ref> [25, 49] </ref>, an emerging standard for explicit message passing in parallel programs, provides communication functions with all those variations. Those functions allow the specification of a message tag, indicating the type of the underlying message.
Reference: [50] <author> Manish Parashar, Salim Hariri, Tomasz Haupt, and Geoffrey C. Fox. </author> <title> Interpreting the performance of HPF/Fortran 90D. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <pages> pages 743-752, </pages> <address> Washington, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: It has the drawback of requiring executions of the program on the actual systems, and is more suited for finding bottlenecks in those executions; it does not provide much information about the program behavior with other system sizes. 2.4 Prediction and Compilation Tools Parashar et al <ref> [50] </ref> presented a framework to predict the performance of HPF/Fortran 90D programs, using an interpretive approach. They abstracted source program constructs with the HPF compiler, and inferred the program performance for a given machine on which specific benchmarks have been previously executed. <p> However, that effort has not yet reached an ideal stage. Some methods derive a prediction for a specific combination of number of processors (P ) and problem size (N ), like in [1], [22] and <ref> [50] </ref>. Others provide a symbolic model that can be evaluated at desired combinations of N and P , but either have a very limited application domain, as in [64], or require several executions of the program for model calibration, as in [15].
Reference: [51] <author> Ronald C. Read and Derek G. Corneil. </author> <title> The graph isomorphism disease. </title> <journal> Journal of Graph Theory, </journal> <volume> 1 </volume> <pages> 339-363, </pages> <year> 1977. </year> <month> 147 </month>
Reference-contexts: Although it is unknown if graph isomorphism is NP-hard, no polynomial time algorithm is known <ref> [51, 19] </ref> and current algorithms are very computationally expensive. In our case, testing for isomorphism is insufficient | two execution graphs might be similar, but not isomorphic. We need, to determine how "similar" they are. In other words, we need a metric to compare graphs.
Reference: [52] <author> Daniel A. Reed, Ruth A. Aydt, Tara M Madhyastha, Roger J. Noe, Keith A. Shields, and Bradley W. Schwartz. </author> <title> The Pablo Performance Analysis Environment. </title> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: The model that we use for this part of our work is based on an integrated compilation and performance analysis environment, developed as an extended version of the Fortran D compiler [28] including capabilities of the Pablo system <ref> [52] </ref>. The Fortran D language and its original compiler were developed at Rice University, in the early 90's, as a set of tools for machine-independent parallel programming.
Reference: [53] <author> Alfred Riddle. </author> <title> Mathematical power tools. </title> <journal> IEEE Spectrum, </journal> <volume> 31(11) </volume> <pages> 35-47, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In the integrated system, Pablo SDDF serves as a flexible medium of data interchange between the compiler and Pablo. Because performance data is presented as symbolic expressions of P and N , the system employs a symbolic expression manipulator (e.g., Maple or Math-ematica <ref> [53] </ref>) to derive such expressions. The performance prediction model interacts with data correlation toolkit and symbolic expression manipulator to analyze performance data and provide performance prediction functionalities. 5.3.2 Prediction Details As an example, consider the Fortran D program and its SPMD code equivalent shown in Figure 5.7.
Reference: [54] <author> Luiz A. De Rose, Ying Zhang, and Ruth Aydt. </author> <title> SvPablo Guide. </title> <institution> University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: We executed a 64K point FFT on four processors of the SGI Power-Challenge activating the hardware performance counters in the MIPS R10000. Using the lower-level infrastructure provided by a tool that accesses those counters <ref> [54] </ref>, we obtained the instruction mixes for the three FFT sections, as shown in Table 4.13. We can compare the Power-Challenge mixes in Table 4.13 to the ones in Table 4.7, for the Intel Paragon.
Reference: [55] <author> Rafael H. Saavedra-Barrera, Alan Jay Smith, and Eugene Miya. </author> <title> Performance prediction by benchmark and machine characterization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1659-1679, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The major goal was to find locations in the original program where optimization efforts should concentrate. This approach allows assessing performance sensitivity to specific program sections in a mechanized way. 2.2 Cross-System Prediction Saavedra-Barrera et al <ref> [55] </ref> proposed a detailed model for performance evaluation and prediction on uniprocessors. This model identified standard operations and constructs in Fortran and characterized application programs by the number and type of these operations that were executed. <p> Cross-machine prediction studies usually require extensive information about systems and about application codes, and focus the prediction process on finding the resulting effects of specific application/system combinations, like in <ref> [55] </ref> and in [59]. By relying our prediction on a traced execution, however, we simply observe such effects, and need only evaluate their change for a different system.
Reference: [56] <author> Sekhar R. Sarukkai. </author> <title> Scalability analysis tools for SPMD message-passing parallel programs. </title> <booktitle> In Proceedings of the ACM/IEEE Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems - MASCOTS'94, </booktitle> <pages> pages 180-186, </pages> <address> Durham, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The characterization of the isoefficiency function, however, requires detailed knowledge of the underlying algorithm and system, and represents a significant challenge for automation of the prediction process with real programs. Sarukkai <ref> [56] </ref> analyzed the scalability of parallel programs using both static program information and dynamic execution traces. His study is the closest one to our approach in scalability analysis, as it also automatically builds scalability models for computation and communication sections of an SPMD program.
Reference: [57] <author> Sekhar R. Sarukkai and Allen D. Malony. </author> <title> Perturbation analysis of high level instrumentation for SPMD programs. </title> <booktitle> In 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming Proceedings, </booktitle> <pages> pages 44-53, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Malony [40] studied trace transformation, and developed time-based and event-based perturbation models for eliminating perturbations induced by software instrumentation. In an extension of that work, Sarukkai and Malony <ref> [57] </ref> applied perturbation models to transform message passing program traces. The predicted behavior produced with the model was very similar to the actual behavior of the non-instrumented program, showing that instrumentation intrusion can be removed for some programs.
Reference: [58] <author> Sekhar R. Sarukkai, Pankaj Mehra, and Robert J. </author> <title> Block. Automated scalability analysis of message-passing parallel programs. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3(4) </volume> <pages> 21-32, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: This recognition phase, however, required both analysis of the SPMD source code and of execution traces, to correctly match the various pairs of sends and receives in the program. They also extended the toolkit to enable predictions by simulation of an abstract version of the program <ref> [58] </ref>, represented in a behavior description language. Mehra, Schulback and Yan [42] conducted performance prediction by modeling message passing programs at varying levels of syntactic detail, using statistical regression techniques to infer the parameters in the model. This modeling process was completely manual.
Reference: [59] <author> Matthias Schumann. </author> <title> Automatic performance prediction to support cross development of parallel programs. </title> <booktitle> In First ACM Symposium on Parallel and Distributed Tools, </booktitle> <address> Philadel-phia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In an extension of that work, Sarukkai and Malony [57] applied perturbation models to transform message passing program traces. The predicted behavior produced with the model was very similar to the actual behavior of the non-instrumented program, showing that instrumentation intrusion can be removed for some programs. Schumann <ref> [59] </ref> presented a methodology to automate cross-machine performance prediction of Fortran programs with explicit message passing. He built tools that extracted processor and network parameters by running benchmark programs, and parsed the source program to find its various components in terms of primitive Fortran constructs. <p> Cross-machine prediction studies usually require extensive information about systems and about application codes, and focus the prediction process on finding the resulting effects of specific application/system combinations, like in [55] and in <ref> [59] </ref>. By relying our prediction on a traced execution, however, we simply observe such effects, and need only evaluate their change for a different system. This approach requires much less detail about systems, and no information about applications other than access to their source codes, to insert instrumentation probes.
Reference: [60] <author> David S. Scott and Gary R. Withers. </author> <title> Performance and assembly language programming of the iPSC/860 system. </title> <booktitle> In Proceedings of the 6 th Distributed Memory Computing Conference, </booktitle> <pages> pages 534-541, </pages> <address> Portland, Oregon, </address> <year> 1991. </year>
Reference-contexts: Using benchmark data, one can build a model of the time to send or receive messages of a given length. 4.1.1 Multicomputer Examples As an example of architectural parameters for real systems, consider the Intel iPSC/2 [4] and Intel iPSC/860 <ref> [60] </ref> multicomputers. They consist of up to 128 processing nodes interconnected in a hypercube structure. The interconnection network is the same on both machines, and there is hardware support for routing messages between the nodes using a circuit-switching scheme.
Reference: [61] <author> J. B. Sinclair and W. P. Dawkins. </author> <title> ES: A tool for predicting the performance of parallel systems. </title> <booktitle> In Proceedings of the ACM/IEEE Workshop on Modeling, Analysis and Simulation 148 of Computer and Telecommunications Systems - MASCOTS'94, </booktitle> <pages> pages 164-168, </pages> <address> Durham, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Their system estimated numerical parameters in the models applying statistical regression. The selection of appropriate terms to include in a model, however, still required human intervention, and the regression procedure needed performance data from many executions with different data set sizes and numbers of processors. 9 Sinclair and Dawkins <ref> [61] </ref> developed a tool named ES (Event Sequencer), to predict the performance of parallel systems in an analytical form. They built a model of the program as a task graph, assigning to each graph node a random variable corresponding to the expected event execution time.
Reference: [62] <author> SPEC. </author> <title> SPEC benchmark suite release 1.0. </title> <journal> SPEC Newsletter, </journal> <volume> 2(2) </volume> <pages> 3-4, </pages> <year> 1990. </year>
Reference-contexts: By assumption, this ratio can be either a constant or dependent on some aspect of the code. The simplest approximation assumes a single ratio that could be derived from published performance data for the two processors (e.g., SPEC ratio <ref> [62, 20] </ref>). In the case of two existing systems, we can derive that ratio by executing a sequential version of the program on both systems with a reduced data set, and computing the ratio of the total execution times. A better alternative uses a variable ratio.
Reference: [63] <author> Arjan J. C. van Gemund. </author> <title> Performance prediction of parallel processing systems: The PAMELA methodology. </title> <booktitle> In Proceedings of the 7 th ACM International Conference on Supercomputing, </booktitle> <pages> pages 318-327, </pages> <address> Tokyo, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: This method requires detailed knowledge of the underlying system, and is more oriented towards providing directions for efficient data distributions. Van Gemund [64] proposed a compile-time mechanism to predict the performance of parallel systems. His approach was based on the Pamela formalism <ref> [63] </ref>, consisting of an imperative set of constructs to explicitly express concurrent operations. As a side effect of compilation, he derived symbolic expressions representing the execution time of the set of constructs, accounting 11 for computation, communication, and overhead due to resource contention.
Reference: [64] <author> Arjan J. C. van Gemund. </author> <title> Compile-time performance prediction of parallel systems. </title> <booktitle> In Proceedings of the 8 th International Conference on Modeling Techniques and Tools for Computer Performance Evaluation, </booktitle> <pages> pages 299-313, </pages> <address> Heidelberg, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: The predictions were derived directly from the source code of the application, and no executions were required. This method requires detailed knowledge of the underlying system, and is more oriented towards providing directions for efficient data distributions. Van Gemund <ref> [64] </ref> proposed a compile-time mechanism to predict the performance of parallel systems. His approach was based on the Pamela formalism [63], consisting of an imperative set of constructs to explicitly express concurrent operations. <p> Others provide a symbolic model that can be evaluated at desired combinations of N and P , but either have a very limited application domain, as in <ref> [64] </ref>, or require several executions of the program for model calibration, as in [15].
Reference: [65] <author> Edward H. Welbon, Christopher C. Chan-Nui, David J. Shippy, and Dwain A. Hicks. </author> <title> The POWER2 performance monitor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5) </volume> <pages> 545-554, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Many processors today provide performance counters, in hardware, that can reveal this information for a given execution <ref> [10, 65, 67] </ref>. If such counters are not available, one can obtain the dynamic instruction counts with low-level instrumentation of the program. * CPI values: These values depend on a wide variety of factors, including details in the system organization, processor structure and compiler effectiveness.
Reference: [66] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley Publishing Company Inc, </publisher> <address> Redwood City, California, </address> <year> 1996. </year>
Reference-contexts: Such a scheme allows a change in one line of the program to change the layout of all or many of the arrays, perhaps to port the program to a different parallel system <ref> [66] </ref>. The programmer specifies a data mapping using HPF directives that can aid the compiler in optimizing parallel performance, but have no effect on the semantics of the program. <p> In the computation partitioning phase, the compiler enumerates candidate partitions, where a partition means a division of work among the processors (in the case of a loop, for example, a partition represents which iterations are assigned to each processor). Many existing compilers use the "owner-computes" rule <ref> [66] </ref> to guide this partitioning: an operation is computed by the processor that owns the element in the left-hand side of the assignment, which receives the computed value.
Reference: [67] <author> Marco Zagha, Brond Larson, Steve Turner, Marty Itzkowitz, and Jun Yu. </author> <title> Performance analysis using the MIPS R10000 performance counters. </title> <booktitle> In Proceedings of Supercomputing'96, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Many processors today provide performance counters, in hardware, that can reveal this information for a given execution <ref> [10, 65, 67] </ref>. If such counters are not available, one can obtain the dynamic instruction counts with low-level instrumentation of the program. * CPI values: These values depend on a wide variety of factors, including details in the system organization, processor structure and compiler effectiveness.
Reference: [68] <author> Bohdan Zelinka. </author> <title> On a certain distance between isomorphism classes of graphs. </title> <journal> Casopis pro pestovan matematiky, </journal> <volume> 100 </volume> <pages> 371-373, </pages> <year> 1975. </year>
Reference-contexts: Under these definitions, the following two statements are equivalent for any graphs G 1 and G 2 with n vertices <ref> [68] </ref>: 1. There exist isomorphic graphs H 1 and H 2 , each with at least n d vertices, such that H 1 is an induced subgraph of G 1 and H 2 is an induced subgraph of G 2 . 2. <p> G j , d 0 (G i ; G j ) to represent the relative distance between G i and G j , and the symbol ~ = to represent graph isomorphism, the following properties hold for any graphs G i , G j and G k of degree n <ref> [68] </ref>: * G i ~ = G j , d (G i ; G j ) = 0 * d (G i ; G k ) d (G i ; G j ) + d (G j ; G k ) * 0 d 0 (G i ; G j )
Reference: [69] <author> Xiaodong Zhang and Zhichen Xu. </author> <title> A semi-empirical approach to scalability study. </title> <booktitle> In Proceedings of the 1995 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 307-308, </pages> <address> Ottawa, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: In practice, however, quantifying this latency increment required detailed hardware measurements of cache misses during executions of the program on a shared-memory system. They <ref> [69, 70] </ref> also proposed a methodology to predict the performance of parallel programs with implicit communication, using a graph model.
Reference: [70] <author> Xiaodong Zhang, Zhichen Xu, and Lin Sun. </author> <title> Performance predictions on implicit communication systems. </title> <booktitle> In Proceedings of the Sixth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 560-568, </pages> <address> Dallas, Texas, </address> <month> October </month> <year> 1994. </year> <month> 149 </month>
Reference-contexts: In practice, however, quantifying this latency increment required detailed hardware measurements of cache misses during executions of the program on a shared-memory system. They <ref> [69, 70] </ref> also proposed a methodology to predict the performance of parallel programs with implicit communication, using a graph model.
Reference: [71] <author> Xiaodong Zhang, Yong Yan, and Keqiang He. </author> <title> Latency metric: An experimental method for measuring and evaluating parallel program and architecture scalability. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 392-410, </pages> <year> 1994. </year> <month> 150 </month>
Reference-contexts: This tool, however, did not address the issue of problem size, and was targeted to vector code, not to parallel code. Zhang et al <ref> [71] </ref> presented an experimental metric, using network latency to evaluate and predict the scalability of parallel programs and architectures. This latency metric was mainly concerned with the average latency increment when both the sizes of the problem and of the machine are adjusted to keep the efficiency constant.
References-found: 71

