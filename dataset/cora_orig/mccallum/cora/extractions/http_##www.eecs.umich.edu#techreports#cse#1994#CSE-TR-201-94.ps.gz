URL: http://www.eecs.umich.edu/techreports/cse/1994/CSE-TR-201-94.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse94.html
Root-URL: http://www.eecs.umich.edu
Email: e-mail: fkhbap,kgshing@eecs.umich.edu; 313-763-0391(voice); 313-763-4617(fax)  
Title: Evaluation of Fault-Tolerance Latency from Real-Time Application's Perspectives 1  
Author: Hagbae Kim and Kang G. Shin 
Keyword: Index Terms Fault-tolerance latency, real-time control systems, fault-tolerant controller computers, time/space and static/dynamic redundancy, hard deadline, dynamic failure  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Real-Time Computing Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: The Fault-Tolerance Latency (FTL) defined as the time required by all sequential steps taken to recover from an error is important to the design and evaluation of fault-tolerant computers used in safety-critical real-time control systems. To meet timing constraints or avoid dynamic failure, the latency of any fault-handling policy | that consists of several stages like error detection, fault location and recovery | must not be larger than the Application Required Latency (ARL), which depends upon the controlled process under consideration and its operating environment. We evaluate the FTL while considering various fault-tolerance mechanisms and use the evaluated FTL to check if a fault-handling policy can meet the timing constraint, FTL ARL, for a given real-time application. The FTL is dependent on the underlying fault-handling mechanisms as well as fault behaviors during the application of temporal-redundancy recovery such as instruction retry or program rollback. We investigate all possible fault-handling scenarios and represent FTL with several random and deterministic variables that model the fault behaviors and/or the capability and performance of fault-handling mechanisms. We also present a simple example to demonstrate the application of the evaluated FTL in real-time systems, where an appropriate fault-handling policy is selected to meet the timing requirement with the minimum degree of spatial redundancy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Barton, </author> <title> "Fault latency white paper," </title> <type> Technical report, </type> <institution> Texas Instruments, Microelectronics Department, Plano, TX, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: In [5, 10], the experimental data/statistical methods (i.e., sampling and parameter 2 estimation methods) for characterizing the times of fault detection, system reconfiguration, and computation recovery were discussed based on hardware fault injections in the Fault-Tolerant Multiple Processor (FTMP). In <ref> [1, 13] </ref>, the recovery times were estimated for a pooled-spare and N -modular redundant systems. The effects of various fault-tolerance features on FTL were described there. <p> Thus, all the stages necessary to handle faults/failures upon occurrence of an error should be studied and their effects on the FTL must be analyzed. In a specific application context, the recovery times were estimated in <ref> [1, 13] </ref> by decomposing the fault recovery process into stages and analyzing the effects of various fault-tolerance features on the FTL. We also use a similar approach to the problem of evaluating the FTL, but for more general fault-tolerance strategies. <p> The expected reduction of FTL in the next 5-10 years was also estimated by considering such improvement factors as throughput and memory capacities. In another paper dealing with the pooled-spares system <ref> [1] </ref>, various fault-tolerance techniques covering both software and hardware issues were addressed by focusing on their latencies. This work also analyzed the FTL in the pooled-spares system based on the results of the DRDS Program in [13], and included part of the fault-masking method of N -modular redundancy. <p> We assume that t r lies in a deterministic interval, t r1 t r t r2 , where t r1 and t r2 are determined by the type of reconfiguration and several other factors described above. In fact, these values can be determined experimentally as was done in <ref> [1, 13] </ref>. 3.2.5 Retry This is the simplest recovery method using temporal redundancy, which repeats the execution of a micro-operation or instruction.
Reference: [2] <author> R. W. Butler and A. L. White, </author> <title> "SURE reliability analysis," </title> <type> NASA Technical Paper, </type> <month> March </month> <year> 1990. </year>
Reference-contexts: Our results | that focus on a sequence of error-/failure- handling stages | can also be used in those well-developed reliability or dependability models <ref> [2, 4, 7] </ref>. In Section 2, general fault-tolerance features are described by classifying fault-tolerance mechanisms and considering the tradeoff between temporal and spatial redundancy.
Reference: [3] <author> P. K. Chande, A. K. Ramani, and P. C. Sharma, </author> <title> "Modular TMR multiprocessor system," </title> <journal> IEEE Trans. on Industrial Electronics, </journal> <volume> vol. 36, no. 1, </volume> <pages> pp. 34-41, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Although the time required for this type of recovery is almost zero it induces high spatial costs; when the number of modules available is limited, this method is not as reliable as the dynamic-redundancy method [13] and must be equipped with separate detection and recovery mechanisms for ultra-reliable systems <ref> [3] </ref>. A hazardous environment, like the one resulting from EMI, will affect the entire system and induce coincident, or common-source, faults in the multiple modules of an n-modular redundant system.
Reference: [4] <author> J. Dugan, K. Trivedi, M. Smotherman, and R. Geist, </author> <title> "The hybrid automated reliability prediction," </title> <journal> AIAA Journal of Guidance, Control and Dynamics, </journal> <pages> pp. 319-331, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Our results | that focus on a sequence of error-/failure- handling stages | can also be used in those well-developed reliability or dependability models <ref> [2, 4, 7] </ref>. In Section 2, general fault-tolerance features are described by classifying fault-tolerance mechanisms and considering the tradeoff between temporal and spatial redundancy.
Reference: [5] <author> G. B. Finelli, </author> <title> "Characterization of fault recovery through fault injection on FTMP," </title> <journal> IEEE Trans. on Reliability, </journal> <volume> vol. R-36, no. 2, </volume> <pages> pp. 164-170, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: In <ref> [5, 10] </ref>, the experimental data/statistical methods (i.e., sampling and parameter 2 estimation methods) for characterizing the times of fault detection, system reconfiguration, and computation recovery were discussed based on hardware fault injections in the Fault-Tolerant Multiple Processor (FTMP). <p> Error detection mechanisms are classified into (i) signal-level detection mechanisms, (ii) function-level detection mechanisms, and (iii) periodic diagnostics. Let t el and F el (t) be the error latency and its cumulative probability distribution, respectively. Several well-known pdf's, such as Weibull, Gamma, and lognormal distributions, were examined in <ref> [5] </ref> to model the error latency. If t i is the mean execution time of an instruction, F el (t i ) 1 for a high-coverage signal-level detection mechanism. For a function-level detection mechanism, t el depends on the detection mechanism used and the 7 executing task.
Reference: [6] <author> R. M. Geist, M. Smotherman, and R. Talley, </author> <title> "Modeling recovery time distributions in ultra-reliable fault-tolerant systems," </title> <booktitle> in Digest of Papers, FTCS-20, </booktitle> <pages> pp. 499-504, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In [12], recovery procedures were represented by instantaneous probabilities which measure the effectiveness of fault-/error- handling mechanisms while ignoring the time spent on the recovery procedures due to the stiffness existing between fault occurrence and recovery. The authors of <ref> [6] </ref> derived a distribution of system-recovery times by using a truncated normal distribution and a displaced exponential distribution, which captures general short periods of normal recovery and special long durations of rare abnormal recovery. This work was based on the recovery time data collected from various (experimental) sources.
Reference: [7] <author> R. M. Geist and K. S. Trivedi, </author> <title> "Ultrahigh reliability prediction for fault-tolerant computer systems," </title> <journal> IEEE Trans. on Computer., </journal> <volume> vol. C-32, no. 12, </volume> <pages> pp. 1118-1127, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: Our results | that focus on a sequence of error-/failure- handling stages | can also be used in those well-developed reliability or dependability models <ref> [2, 4, 7] </ref>. In Section 2, general fault-tolerance features are described by classifying fault-tolerance mechanisms and considering the tradeoff between temporal and spatial redundancy.
Reference: [8] <author> A. L. Hopkins Jr., T. B. Smith III, and J. H. Lala, </author> <title> "FTMP-a highly reliable fault-tolerant multiprocessor for aircraft," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 66, no. 10, </volume> <pages> pp. 1221-1239, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: This process is necessary for both dynamic and hybrid redundancy. Specific hardware like the Configuration Control Unit (CCU) in FTMP <ref> [8] </ref>, may be dedicated to handling system reconfiguration. This process (of using cold spares) generally consists of (i) switching power and bus connections, (ii) running built-in-test (BIT) on the selected spare module, (iii) loading programs and data, (iv) initializing the software.
Reference: [9] <author> C. M. Krishna, K. G. Shin, and R. W. Butler, </author> <title> "Synchronization and fault-masking in redundant real-time systems," </title> <booktitle> in Digest of Papers, FTCS-14, </booktitle> <pages> pp. 152-157, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The method induces the time overhead of redundancy management such as synchronization and voting/interactive consistency techniques even in the absence of faults, which increases with the degree of redundancy <ref> [9] </ref>.
Reference: [10] <author> J. H. Lala, </author> <title> "Fault detection, isolation and configuration in FTMP: Methods and experimental results," </title> <booktitle> in Proc. 5th IEEE/AIAA Digital Avionics Systems Conf., </booktitle> <pages> pp. </pages> <address> 21.3.1-21.3.9, </address> <year> 1983. </year>
Reference-contexts: In <ref> [5, 10] </ref>, the experimental data/statistical methods (i.e., sampling and parameter 2 estimation methods) for characterizing the times of fault detection, system reconfiguration, and computation recovery were discussed based on hardware fault injections in the Fault-Tolerant Multiple Processor (FTMP).
Reference: [11] <author> S. R. McConnel, D. P. Siewiorek, and M. M. Tsao, </author> <title> "The measurement and analysis of transient errors in digital computer systems," </title> <booktitle> in Digest of Papers, FTCS-9, </booktitle> <pages> pp. 67-70, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: occurrence of an error consists of several stages, some of which depend on each other, and the FTL is defined as the time spent for 2 Note that more than 90% of faults are known to be non-permanent; as few as 2% of field failures are caused by permanent faults <ref> [11] </ref>. 4 3.Static/Hybrid redundancy ) (rollback ) (n-modular redundancy) Time redundancy Space redundancy 2.Dynamic/Time redundancy (restart (including the case of replacement)) (retry (+overhead) as n increases. (+overhead) 1.Detection (without recovery) scheme mechanisms. the entire recovery process.
Reference: [12] <author> J. McGough, M. Smotherman, and K. S. Trivedi, </author> <title> "The conservativeness of reliability estimates based on instantaneous coverage," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-34, no. 7, </volume> <pages> pp. 602-608, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Reliability or dependability models assumed the recovery time to have a certain probability distribution; if the recovery time follows an exponential (or general) distribution, the transition from error state to normal state is represented by the mean rate in a Markov model (or a semi-Markov model). In <ref> [12] </ref>, recovery procedures were represented by instantaneous probabilities which measure the effectiveness of fault-/error- handling mechanisms while ignoring the time spent on the recovery procedures due to the stiffness existing between fault occurrence and recovery.
Reference: [13] <author> C. Roark, D. Paul, D. Struble, D. Kohalmi, and J. </author> <title> Newport, "Pooled spares and dynamic reconfiguration," </title> <booktitle> in Proceedings of NAECON'93, </booktitle> <pages> pp. 173-179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In [5, 10], the experimental data/statistical methods (i.e., sampling and parameter 2 estimation methods) for characterizing the times of fault detection, system reconfiguration, and computation recovery were discussed based on hardware fault injections in the Fault-Tolerant Multiple Processor (FTMP). In <ref> [1, 13] </ref>, the recovery times were estimated for a pooled-spare and N -modular redundant systems. The effects of various fault-tolerance features on FTL were described there. <p> Thus, all the stages necessary to handle faults/failures upon occurrence of an error should be studied and their effects on the FTL must be analyzed. In a specific application context, the recovery times were estimated in <ref> [1, 13] </ref> by decomposing the fault recovery process into stages and analyzing the effects of various fault-tolerance features on the FTL. We also use a similar approach to the problem of evaluating the FTL, but for more general fault-tolerance strategies. <p> We also use a similar approach to the problem of evaluating the FTL, but for more general fault-tolerance strategies. For completeness, these approaches are summarized below. 3.1 FTLs of a Pooled-Spares System In <ref> [13] </ref>, the FTL was estimated for a pooled-spares system implemented in the Dynamic Reconfiguration Demonstration System (DRDS) Program. 3 Phase 1 of the DRDS Program is reported to have shown potential benefits of the dynamic run-time reconfiguration implied by the pooled-spares approach, such as increased functional availability and flexibility, higher reliability, <p> In another paper dealing with the pooled-spares system [1], various fault-tolerance techniques covering both software and hardware issues were addressed by focusing on their latencies. This work also analyzed the FTL in the pooled-spares system based on the results of the DRDS Program in <ref> [13] </ref>, and included part of the fault-masking method of N -modular redundancy. <p> Several characteristics of checkpointing such as consistency, independence, programmer-transparency, and conversation were introduced with the methods of software re-execution and rollback, where the time required for this recovery procedure was assumed to be a single checkpoint period as in <ref> [13] </ref>. <p> Although the time required for this type of recovery is almost zero it induces high spatial costs; when the number of modules available is limited, this method is not as reliable as the dynamic-redundancy method <ref> [13] </ref> and must be equipped with separate detection and recovery mechanisms for ultra-reliable systems [3]. A hazardous environment, like the one resulting from EMI, will affect the entire system and induce coincident, or common-source, faults in the multiple modules of an n-modular redundant system. <p> We assume that t r lies in a deterministic interval, t r1 t r t r2 , where t r1 and t r2 are determined by the type of reconfiguration and several other factors described above. In fact, these values can be determined experimentally as was done in <ref> [1, 13] </ref>. 3.2.5 Retry This is the simplest recovery method using temporal redundancy, which repeats the execution of a micro-operation or instruction. <p> of the controlled process, the information on fault behaviors involving environmental characteristics (such as electro-magnetic interferences), and the control algorithms programmed in the controller computers [14]. 13 When an error/failure occurs in a controller computer, the error must be recovered within a certain period, called the Application Required Latency (ARL) <ref> [13] </ref>, in order to avoid a dynamic failure. Roark et al. [13] presented several empirical examples of ARL for flight control, missile guidance, air data system, automatic tracking and recognition applications. <p> characteristics (such as electro-magnetic interferences), and the control algorithms programmed in the controller computers [14]. 13 When an error/failure occurs in a controller computer, the error must be recovered within a certain period, called the Application Required Latency (ARL) <ref> [13] </ref>, in order to avoid a dynamic failure. Roark et al. [13] presented several empirical examples of ARL for flight control, missile guidance, air data system, automatic tracking and recognition applications.
Reference: [14] <author> K. G. Shin and H. Kim, </author> <title> "Derivation and application of hard deadlines for real-time control systems," </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> vol. 22, no. 6, </volume> <pages> pp. 1403-1413, </pages> <address> Nov./Dec. </address> <year> 1992. </year>
Reference-contexts: The hard deadline represents system inertia/resilience against a dynamic failure, which can be derived experimentally or analytically using the state dynamic equations of the controlled process, the information on fault behaviors involving environmental characteristics (such as electro-magnetic interferences), and the control algorithms programmed in the controller computers <ref> [14] </ref>. 13 When an error/failure occurs in a controller computer, the error must be recovered within a certain period, called the Application Required Latency (ARL) [13], in order to avoid a dynamic failure. <p> Roark et al. [13] presented several empirical examples of ARL for flight control, missile guidance, air data system, automatic tracking and recognition applications. It is important to note that one can derive the ARL analytically using the hard-deadline information <ref> [14] </ref>, because the sum of ARL and the minimum time to execute the remaining control task to generate a correct control input is equal to the system's hard deadline.
Reference: [15] <author> K. G. Shin, C. M. Krishna, and Y.-H. Lee, </author> <title> "A unified method for evaliating real-time computer controller and its application," </title> <journal> IEEE Trans. on Automat. Contr., </journal> <volume> vol. AC-30, no. 4, </volume> <pages> pp. 357-366, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: However, a serious degradation of system performance or catastrophe called a dynamic failure (or system failure), occurs if the duration of missing the update (or incorrect update) of the control input due to malfunctioning of the controller computer exceeds a certain limit called the hard deadline <ref> [15] </ref>.
Reference: [16] <author> K. G. Shin and Y.-H. Lee, </author> <title> "Measurement and application of fault latency," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-35, no. 4, </volume> <pages> pp. 370-375, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: The distribution of fault latency was estimated in <ref> [16] </ref> by using the Gamma and Weibull distributions. Since the FTL begins with the occurrence of an error, we are mainly interested in the error latency which depends upon the active duration of a fault and the underlying detection mechanism.
Reference: [17] <author> D. P. Siewiorek and R. S. Swarz, </author> <title> The Theory and Practice of Reliable System Design, </title> <institution> Digital Equipment Corporation, Bedford, </institution> <address> MA, </address> <year> 1982. </year> <month> 19 </month>
Reference-contexts: We define Fault-Tolerance Latency (FTL) as the total time spent on such sequential fault-handling stages as error detection, fault location, system reconfiguration, and recovery of the contaminated application program. Most work on fault-tolerance used simple models for FTL, which was also represented in <ref> [17] </ref> as the sum of Mean Time To Detection (MTTD) and Mean Time To Repair (MTTR).
References-found: 17

