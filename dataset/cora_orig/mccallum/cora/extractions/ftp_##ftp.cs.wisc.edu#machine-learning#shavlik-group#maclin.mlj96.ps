URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/maclin.mlj96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/maclin.mlj96.ps.abstract.html
Root-URL: 
Title: Machine Learning,  Creating Advice-Taking Reinforcement Learners  
Author: RICHARD MACLIN AND JUDE W. SHAVLIK Editor: Leslie Pack Kaelbling 
Keyword: Reinforcement learning, advice-giving, neural networks, Q-learning, learning from instruction, theory refinement, knowledge-based neural networks, adaptive agents  
Address: Wisconsin, 1210 W. Dayton St., Madison, WI 53706  
Affiliation: Computer Sciences Dept., University of  
Note: c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 22,  
Email: maclin@cs.wisc.edu and shavlik@cs.wisc.edu  
Date: 251-282 (1996)  
Abstract: Learning from reinforcements is a promising approach for creating intelligent agents. However, reinforcement learning usually requires a large number of training episodes. We present and evaluate a design that addresses this shortcoming by allowing a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer. In our approach, the advice-giver watches the learner and occasionally makes suggestions, expressed as instructions in a simple imperative programming language. Based on techniques from knowledge-based neural networks, we insert these programs directly into the agent's utility function. Subsequent reinforcement learning further integrates and refines the advice. We present empirical evidence that investigates several aspects of our approach and show that, given good advice, a learner can achieve statistically significant gains in expected reward. A second experiment shows that advice improves the expected reward regardless of the stage of training at which it is given, while another study demonstrates that subsequent advice can result in further gains in reward. Finally, we present experimental results that indicate our method is more powerful than a naive technique for making use of advice. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1995). </year> <title> Hints. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 639-671. </pages>
Reference: <author> Agre, P., & Chapman, D. </author> <year> (1987). </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 268-272 Seattle, WA. </address>
Reference: <author> Anderson, C. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. </pages> <address> 103-114 Irvine, CA. </address>
Reference: <author> Barto, A., Sutton, R., & Anderson, C. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 834-846. </pages>
Reference: <author> Barto, A., Sutton, R., & Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., & Moore, J. (Eds.), </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pp. 539-602. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Berenji, H., & Khedkar, P. </author> <year> (1992). </year> <title> Learning and tuning fuzzy logic controllers through reinforcements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 724-740. </pages>
Reference: <author> Chapman, D. </author> <year> (1991). </year> <title> Vision, Instruction, and Action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Clouse, J., & Utgoff, P. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 92-101 Aberdeen, Scotland. </address>
Reference: <author> Crangle, C., & Suppes, P. </author> <year> (1994). </year> <title> Language and Learning for Robots. </title> <publisher> CSLI Publications, Stanford, </publisher> <address> CA. </address>
Reference: <author> Craven, M., & Shavlik, J. </author> <year> (1994). </year> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 37-45 New Brunswick, NJ. </address>
Reference: <author> Diederich, J. </author> <year> (1989). </year> <title> "Learning by instruction" in connectionist systems. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pp. </pages> <address> 66-68 Ithaca, NY. </address> <note> CREATING ADVICE-TAKING REINFORCEMENT LEARNERS 281 Dietterich, </note> <author> T. </author> <year> (1991). </year> <title> Knowledge compilation: Bridging the gap between specification and implementation. </title> <journal> IEEE Expert, </journal> <volume> 6, </volume> <pages> 80-82. </pages>
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference: <author> Frasconi, P., Gori, M., Maggini, M., & Soda, G. </author> <year> (1995). </year> <title> Unified integration of explicit knowledge and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7, </volume> <pages> 340-346. </pages>
Reference: <author> Fu, L. M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 325-340. </pages>
Reference: <author> Ginsberg, A. </author> <year> (1988). </year> <title> Automatic Refinement of Expert System Knowledge Bases. </title> <publisher> Pitman, London. </publisher>
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1994). </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 331-346. </pages>
Reference: <author> Gruau, F. </author> <year> (1994). </year> <title> Neural Network Synthesis using Cellular Encoding and the Genetic Algorithm. </title> <type> Ph.D. thesis, </type> <institution> Ecole Normale Superieure de Lyon, France. </institution>
Reference: <author> Hayes-Roth, F., Klahr, P., & Mostow, D. J. </author> <year> (1981). </year> <title> Advice-taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In Anderson, J. (Ed.), </editor> <booktitle> Cognitive Skills and their Acquisition, </booktitle> <pages> pp. 231-253. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Huffman, S., & Laird, J. </author> <year> (1993). </year> <title> Learning procedures from interactive natural language instructions. </title> <booktitle> In Machine Learning: Proceedings on the Tenth International Conference, </booktitle> <pages> pp. </pages> <address> 143-150 Amherst, MA. </address>
Reference: <author> Kaelbling, L. </author> <year> (1987). </year> <title> REX: A symbolic language for the design and parallel implementation of embedded systems. </title> <booktitle> In Proceedings of the AIAA Conference on Computers in Aerospace Wakefield, </booktitle> <address> MA. </address>
Reference: <author> Kaelbling, L., & Rosenschein, S. </author> <year> (1990). </year> <title> Action and planning in embedded agents. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6, </volume> <pages> 35-48. </pages>
Reference: <author> Laird, J., Hucka, M., Yager, E., & Tuck, C. </author> <year> (1990). </year> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 235-243 Austin, TX. </address>
Reference: <author> Le Cun, Y., Denker, J., & Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 598-605. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Although recent evidence (Weigend, 1993) suggests overly large networks are not a problem given that one uses proper training techniques, we plan to evaluate techniques for periodically "cleaning up" and shrinking the learner's network. We plan to use standard neural-network techniques for removing network links with low saliency <ref> (e.g., Le Cun, Denker, & Solla, 1990) </ref>. In our current implementation, a plan (i.e., a sequence of actions) can be interrupted if another action has higher utility (see Figure 5).
Reference: <author> Levine, J., Mason, T., & Brown, D. </author> <year> (1992). </year> <title> Lex & yacc. </title> <address> O'Reilly, Sebastopol, CA. </address>
Reference-contexts: Due to the complexities of natural language processing, we require that the external observer express its advice using a simple programming language and a list of task-specific terms. We then parse the advice, using traditional methods from the programming-languages literature <ref> (Levine, Mason, & Brown, 1992) </ref>. Table 1 shows some sample advice that the observer could provide to an agent learning to play a video game. <p> Advice must be expressed in the language defined by the grammar in Appendix B. Step 2. Convert the advice into an internal representation. We built ratle's advice parser using the standard Unix compiler tools lex and yacc <ref> (Levine et al., 1992) </ref>. 258 R. MACLIN AND J. W.
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 182-189 Amherst, MA. </address>
Reference-contexts: Suddarth and Holden's work however only deals with hints in the form of useful output signals, and still requires network learning, while ratle incorporates advice immediately. Our work on ratle is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik, 1993) </ref>. Fskbann uses a type of recurrent neural network introduced by Elman (1990) that maintains information from previous activations using the recurrent network links. Fskbann extends kbann to deal with state units, but it does not create new state units.
Reference: <author> Maclin, R. </author> <year> (1995). </year> <title> Learning from Instruction and Experience: Methods for Incorporating Procedural Domain Theories into Knowledge-Based Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference-contexts: We accomplish this re-representation by applying the method of Berenji and Khedkhar (1992), adapted slightly <ref> (Maclin, 1995) </ref> to be consistent with kbann's mapping algorithm. Though fuzzy logic is a powerful method that allows humans to express advice using intuitive terms, it has the disadvantage that someone must explicitly define the fuzzy terms in advance. <p> To operationalize a fuzzy condition, ratle determines a set of weights and a threshold that implement the given sigmoidal membership function, as a function of the current sensor readings. The exact details depend on the structure of a given domain's sensors <ref> (see Maclin, 1995, for additional details) </ref> and have not been a major focus of this research. The result of this process essentially defines a perceptron; hence, operationalized fuzzy conditions can be directly inserted into the agent's neural network during Step 4. Step 4. <p> Enemies may move off the board (they appear again after a random interval), but the agent is constrained to remain on the board. Enemies do not push obstacles. The initial mazes are generated randomly using a maze-creation program <ref> (Maclin, 1995) </ref> that randomly lays out lines of obstacles and then creates connections between "rooms." The percentage of the total board covered by obstacles is controlled by a parameter, as are the number of enemies and food items. <p> In other experiments (not reported here), we demonstrate that an agent can quickly overcome the effects of "bad" advice <ref> (Maclin, 1995) </ref>. We corroborated our Pengo results using a second testbed (Maclin, 1995). A significant feature of our second testbed is that its agent's sensors record the complete state of the environment. <p> In other experiments (not reported here), we demonstrate that an agent can quickly overcome the effects of "bad" advice <ref> (Maclin, 1995) </ref>. We corroborated our Pengo results using a second testbed (Maclin, 1995). A significant feature of our second testbed is that its agent's sensors record the complete state of the environment.
Reference: <author> Maclin, R., & Shavlik, J. </author> <year> (1993). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 195-215. </pages>
Reference-contexts: Suddarth and Holden's work however only deals with hints in the form of useful output signals, and still requires network learning, while ratle incorporates advice immediately. Our work on ratle is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik, 1993) </ref>. Fskbann uses a type of recurrent neural network introduced by Elman (1990) that maintains information from previous activations using the recurrent network links. Fskbann extends kbann to deal with state units, but it does not create new state units.
Reference: <author> Maclin, R., & Shavlik, J. </author> <year> (1994). </year> <title> Incorporating advice into agents that learn from reinforcements. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 694-699 Seattle, WA. </address>
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <pages> 311-365. </pages>
Reference: <author> McCarthy, J. </author> <year> (1958). </year> <title> Programs with common sense. </title> <booktitle> In Proceedings of the Symposium on the Mechanization of Thought Processes, </booktitle> <volume> Vol. I, </volume> <pages> pp. 77-84. </pages> <note> (Reprinted in M. </note> <editor> Minsky, editor, </editor> <booktitle> 1968, Semantic Information Processing. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, 403-409.). </publisher>
Reference: <author> Monahan, G. </author> <year> (1982). </year> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28, </volume> <pages> 1-16. </pages>
Reference: <author> Mostow, D. J. </author> <year> (1982). </year> <title> Transforming declarative advice into effective procedures: A heuristic search example. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 1. </volume> <publisher> Tioga Press, </publisher> <address> Palo Alto. </address>
Reference: <author> Nilsson, N. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 139-158. </pages> <note> 282 R. </note> <author> MACLIN AND J. W. SHAVLIK Noelle, D., & Cottrell, G. </author> <year> (1994). </year> <title> Towards instructable connectionist systems. In Sun, </title> <editor> R., & Bookman, L. (Eds.), </editor> <booktitle> Computational Architectures Integrating Neural and Symbolic Processes. </booktitle> <publisher> Kluwer Academic, </publisher> <address> Boston. </address>
Reference: <author> Omlin, C., & Giles, C. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 361-366 Ab-erdeen, Scotland. </address>
Reference: <author> Ourston, D., & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309. </pages>
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference: <author> Riecken, D. </author> <year> (1994). </year> <journal> Special issue on intelligent agents. Communications of the ACM, </journal> <volume> 37 (7). </volume>
Reference-contexts: In particular, we intend to explore tasks involving multiple agents working in cooperation. Such a domain would be interesting in that an observer could give CREATING ADVICE-TAKING REINFORCEMENT LEARNERS 271 advice on how a "group" of agents could solve a task. Another domain of interest is software agents <ref> (Riecken, 1994) </ref>. For example, a human could advise a software agent that looks for "interesting" papers on the World-Wide Web. We see algorithmic extensions as fitting into the three categories explained below.
Reference: <author> Shavlik, J., & Towell, G. </author> <year> (1989). </year> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 233-255. </pages>
Reference: <author> Siegelmann, H. </author> <year> (1994). </year> <booktitle> Neural programming language. In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 877-882 Seattle, WA. </address>
Reference: <author> Suddarth, S., & Holden, A. </author> <year> (1991). </year> <title> Symbolic-neural systems and the use of hints for developing complex systems. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 35, </volume> <pages> 291-311. </pages>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: In another form of reinforcement learning, the agent predicts the utility of a state rather than the utility of an action in a state <ref> (Sutton, 1988) </ref>; here the learner has a model of how its actions change the world, and determines the action to take by checking the utility of the states that are reachable from the current state.
Reference: <author> Sutton, R. </author> <year> (1991). </year> <title> Reinforcement learning architectures for animats. </title> <editor> In Meyer, J., & Wilson, S. (Eds.), </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 288-296. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The agent operates as a normal Q-learning agent when not presented with advice. play against itself (Tesauro, 1992) or when an agent builds an internal world model <ref> (Sutton, 1991) </ref> it would be possible to quickly estimate whether the advice improves performance.) The advisor judges the value of his or her advice similarly (i.e., by watching the learner's post-advice behavior). This may lead to the advisor giving further advice thereby restarting the advice-taking process. 3.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference-contexts: The agent operates as a normal Q-learning agent when not presented with advice. play against itself <ref> (Tesauro, 1992) </ref> or when an agent builds an internal world model (Sutton, 1991) it would be possible to quickly estimate whether the advice improves performance.) The advisor judges the value of his or her advice similarly (i.e., by watching the learner's post-advice behavior).
Reference: <author> Thrun, S., & Mitchell, T. </author> <year> (1993). </year> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 930-936 Chambery, France. </address>
Reference: <author> Towell, G., & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 71-101. </pages>
Reference: <author> Towell, G., & Shavlik, J. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70, </volume> <pages> 119-165. </pages>
Reference-contexts: The RATLE System 256 R. MACLIN AND J. W. SHAVLIK system as a mechanism for evaluating this framework. In order to explain ratle, we first review connectionist Q-learning (Sutton, 1988; Watkins, 1989), the form of reinforcement learning that we use in our implementation, and then kbann <ref> (Towell & Shavlik, 1994) </ref>, a technique for incorporating knowledge in the form of rules into a neural network. We then discuss our extensions to these techniques by showing how we implement each of the five steps described in the previous section. <p> The resulting composite system we refer to as ratle. Background Knowledge-Based Neural Networks In order for us to make use of the advice provided by the observer, we must incorporate this advice into the agent's neural-network utility function. To do so, we extend the kbann algorithm <ref> (Towell & Shavlik, 1994) </ref>. Kbann is a method for incorporating knowledge, in the form of simple propositional rules, into a neural network. In a kbann network, the units of the network represent Boolean concepts.
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 861-866 Boston, MA. </address>
Reference: <author> Utgoff, P., & Clouse, J. </author> <year> (1991). </year> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 596-600 Anaheim, CA. </address>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: In our augmentation, an observer watches the learner and periodically provides advice, which ratle incorporates into the action-choosing module of the RL agent. In Q-learning <ref> (Watkins, 1989) </ref> the action-choosing module uses a utility function that maps states and actions to a numeric value (the utility).
Reference: <author> Watkins, C., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference-contexts: One key question arises from our Pengo results: will the baseline system eventually achieve the same level of performance that the advice-taking system achieves? After all, Q-learning converges to the optimal Q function when a Q table is used to represent the function <ref> (Watkins & Dayan, 1992) </ref>. However, a backpropagation-trained network may only converge to a local minimum in the weight space defining the Q function.
Reference: <author> Weigend, A. </author> <year> (1993). </year> <title> On overfitting and the effective number of hidden units. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pp. </pages> <address> 335-342 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also plan to explore mechanisms for specifying multi-user advice when we explore domains with multiple agents. Improving the Algorithmic Details At present, our algorithm adds hidden units to the learner's neural network whenever advice is received. Hence, the network's size grows monotonically. Although recent evidence <ref> (Weigend, 1993) </ref> suggests overly large networks are not a problem given that one uses proper training techniques, we plan to evaluate techniques for periodically "cleaning up" and shrinking the learner's network.
Reference: <author> Whitehead, S. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 607-613 Anaheim, CA. </address>
Reference: <author> Zadeh, L. </author> <year> (1965). </year> <title> Fuzzy sets. </title> <journal> Information and Control, </journal> <volume> 8, </volume> <pages> 338-353. </pages>
Reference-contexts: In order to specify the preconditions of the if-then and looping constructs, the advisor lists logical combinations of conditions (basic "sensors" and any derived features). To make the language easier to use, we also allow the observer to state "fuzzy" conditions <ref> (Zadeh, 1965) </ref>, which we believe provide a natural way to articulate imprecise advice. Step 3. Convert the advice into a usable form. As will be seen in Step 4, most of the concepts expressible in our grammar can be directly translated into a neural network.
References-found: 54

