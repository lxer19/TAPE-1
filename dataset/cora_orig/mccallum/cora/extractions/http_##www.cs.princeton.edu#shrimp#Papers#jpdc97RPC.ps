URL: http://www.cs.princeton.edu/shrimp/Papers/jpdc97RPC.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_10.html
Root-URL: http://www.cs.princeton.edu
Email: fbilas,felteng@cs.princeton.edu  
Title: Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface Special Issue on Workstation Clusters
Author: Angelos Bilas and Edward W. Felten 
Note: To appear in the Journal of Parallel and Distributed Computing,  Computing, Feb. '97.  
Address: Princeton NJ 08544 USA  
Affiliation: Department of Computer Science, Princeton University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems 9, </journal> <month> 2 (May </month> <year> 1991), </year> <pages> 175-198. </pages>
Reference-contexts: The stub generator and runtime library were designed with SHRIMP in mind, so we believe they come close to the best possible RPC performance on the SHRIMP hardware. Buffer Management The design of ShrimpRPC is similar to Bershad's URPC <ref> [1] </ref>; the main difference is that URPC runs on shared-memory machines while ShrimpRPC runs on the distributed-memory SHRIMP system. Each RPC binding consists of one receive buffer on each side (client and server) with bidirectional import-export mappings between them. <p> Finally, different memory bus protocols may reduce or eliminate the DMA coherence penalty. We cannot predict which transfer mechanism will be better on future architectures. 8 Related Work Our approach is similar in some ways to URPC <ref> [1] </ref>, since both exploit user-level communication. URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC [2] tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation.
Reference: [2] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems 8, </journal> <month> 1 (Feb. </month> <year> 1990), </year> <pages> 37-55. </pages>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 20] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> We cannot predict which transfer mechanism will be better on future architectures. 8 Related Work Our approach is similar in some ways to URPC [1], since both exploit user-level communication. URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC <ref> [2] </ref> tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation. Thekkath and Levy [8] investigated the impact of recent improvements in network technology on communication software.
Reference: [3] <author> A. Bilas, and E. Felten. </author> <title> Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface. </title> <type> Technical Report TR-512-96, </type> <institution> Dept. of Computer Science, Princeton University. </institution>
Reference-contexts: The first two words of the buffer are reserved for protocol control information: a synchronization flag and a cursor to the end of valid data in the buffer. A description of the protocol can be found in <ref> [3] </ref>. The low level stream protocol is implemented separately in both directions to form a bidirectional channel. The protocol allows the sender and the receiver to send and receive data with no handshaking as long as there is enough space. <p> Finally the third version, Optimized vRPC, is tuned for the VMMC interface. We also have a modified Optimized vRPC that avoids copying at the cost of slightly deviating from the SunRPC standard. Measurements Detailed performance numbers for vRPC and SunRPC can be found in <ref> [3] </ref>. Our experiments compare SunRPC and the three versions of vRPC. We divide the code into sections and present timing measurements for each section of the code, for null calls with six argument/result sizes: zero bytes, 8 bytes, 64 bytes, 256 bytes, 1440 bytes and 3K bytes. <p> The Stub Generator ShrimpRPC uses a specialized stub generator which is designed to make low-overhead stubs 12 that cause marshaled data to be laid out as described above. A description of the stub generator and examples of the data structures it produces can be found in <ref> [3] </ref>. Performance Figure 3 compares the performance of two versions of RPC: the SunRPC-compatible VRPC, and the non-compatible ShrimpRPC. We show the fastest version of each library, which uses automatic update in both cases.
Reference: [4] <author> A. Birrell, and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 1 (Feb. </month> <year> 1984), </year> <pages> 39-59. </pages>
Reference: [5] <author> M.A. Blumrich, C. Dubnicki, E.W. Felten, and Kai Li. </author> <title> Protected, User-level DMA for the SHRIMP Network Interface. </title> <booktitle> Proceedings of 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <month> February, </month> <year> 1996, </year> <pages> pages 154-165. </pages>
Reference-contexts: These accesses specify the source address, destination address, and size of a transfer. The ordinary virtual memory protection mechanisms (MMU and page tables) are used to maintain protection <ref> [5] </ref>. VMMC guarantees the in-order, reliable delivery of all data transfers, provided that the ordinary, blocking version of the deliberate-update send operation is used. The ordering guarantees are a bit more complicated when the non-blocking deliberate-update send operation is used.
Reference: [6] <author> M.A. Blumrich, K. Li, R.D. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> Proceedings of 21st Annual Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994, </year> <pages> pages 142-153. </pages>
Reference-contexts: The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. It is not always clear, however, which interface is most appropriate for which task. The SHRIMP project <ref> [6, 7] </ref> at Princeton University supports user level communication between processes by mapping memory pages between virtual address spaces. This Virtual Memory Mapped network interface seems to have many advantages including flexible user-level communication, and very low overhead to initiate data transfer. <p> The key hardware component is the network interface board, which supports the virtual memory mapped communication (VMMC) model, to provide low-overhead, protected, user-level communication. For more details on the SHRIMP architecture the reader can consult <ref> [6, 7] </ref>. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) [10] was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: [7] <author> M.A. Blumrich, C. Dubnicki, E.W. Felten, K. Li, and M. Mesarina. </author> <title> Virtual memory mapped network interfaces. </title> <booktitle> IEEE Micro 15(1) </booktitle> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. It is not always clear, however, which interface is most appropriate for which task. The SHRIMP project <ref> [6, 7] </ref> at Princeton University supports user level communication between processes by mapping memory pages between virtual address spaces. This Virtual Memory Mapped network interface seems to have many advantages including flexible user-level communication, and very low overhead to initiate data transfer. <p> The key hardware component is the network interface board, which supports the virtual memory mapped communication (VMMC) model, to provide low-overhead, protected, user-level communication. For more details on the SHRIMP architecture the reader can consult <ref> [6, 7] </ref>. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) [10] was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: [8] <author> C. A. Thekkath, and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems 11, </journal> <month> 2 (May </month> <year> 1993), </year> <pages> 179-203. </pages>
Reference-contexts: with vRPC show that even without changing the stub generator or the kernel, RPC can be made several times faster on the new network interface than it is on conventional networks. vRPC outperforms by more than a factor of 4 the best, to our knowledge, reported implementation for fast networks <ref> [8] </ref>, with a round trip time of about 33s for a null RPC. Even greater gains could be achieved by applying well-known techniques that rely on changes to the stub generator. The second library, ShrimpRPC, is a full-functionality RPC system but is not compatible with any standard. <p> URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC [2] tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation. Thekkath and Levy <ref> [8] </ref> investigated the impact of recent improvements in network technology on communication software. They point out that both high throughput and low latency are required by modern distributed systems and that newer networks strive only for high throughput.
Reference: [9] <author> D. Cheriton. </author> <title> The V kernel: A software base for distributed systems. </title> <booktitle> IEEE Software 1(2) </booktitle> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year>
Reference: [10] <author> C. Dubnicki, L. Iftode, E.W. Felten, and K. Li. </author> <title> Software support for virtual memory-mapped communication. </title> <booktitle> Proceedings of 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: For more details on the SHRIMP architecture the reader can consult [6, 7]. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) <ref> [10] </ref> was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth. These performance goals are achieved by allowing applications to transfer data directly between two virtual memory address spaces over the network.
Reference: [11] <author> C. Dubnicki. </author> <title> SHRIMP Basic Library and Its Simulator. </title> <institution> Dept. of Computer Science, Princeton Univ., </institution> <year> 1995. </year>
Reference-contexts: This effort shows that new architectures can open new horizons to distributed programming by providing high performance at low implementation cost. Acknowledgments We are grateful to Cezary Dubnicki for his expert advice on the SHRIMP simulator <ref> [11] </ref>, and for his implementation of the SHRIMP system software. We would also like to thank Liviu Iftode for his implementation of an early mini version of the system software. We thank the members of the SHRIMP team for many useful discussions during the course of this work.
Reference: [12] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. </author> <title> Schauser Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pages 256-266. 16 </pages>
Reference-contexts: Unlike signals, however, notifications are queued when blocked. Currently we have an implementation of notifications on top of UNIX signals, which works correctly but is slow. Soon, we expect to have an implementation similar to active messages <ref> [12] </ref>, and thus to have much better performance than signals in the common case. VMMC provides a call that allows a process to block until a notification arrives for it. <p> They develop techniques to achieve low latency in communication software, using RPC as a case study. Their goal along with demonstrating the new techniques is to provide design guidelines for network controllers that will facilitate writing low latency communication software 14 in traditional architectures. Active messages <ref> [12] </ref> are a restricted form of RPC, in which the server-side procedure may not perform any actions that might block. Active messages achieve performance similar to ShrimpRPC on high-performance hardware, but without allowing general handlers to be invoked.
Reference: [13] <author> E.W. Felten, R.D. Alpert, A. Bilas, M.A. Blumrich, D.W. Clark, S.N. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early Experience with Message-Passing on the SHRIMP Multicomputer. </title> <booktitle> Proceedings of 23rd International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996, </year> <pages> pages 296-307. </pages>
Reference-contexts: Hence, there is no explicit receive operation. CPU involvement in receiving data can be as little as checking a flag, although a hardware notification mechanism is also supported. Numbers for the latency and bandwidth delivered by the SHRIMP VMMC layer can be found in <ref> [13] </ref>. Notifications The notification mechanism is used to transfer control to a receiving process, or to notify the receiving process about external events. It consists of a message transfer followed by an invocation of a user-specified, user-level handler function.
Reference: [14] <author> D. Johnson, and W. Zwaenepoel, </author> <title> The Peregrine high performance RPC system. </title> <type> Tech. Rep. </type> <institution> COMP TR91-152, Dept. of Computer Science, Rice Univ., </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 20] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> The Optimistic Active Messages [21] approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties. Several papers (e.g. <ref> [14, 17] </ref>) describe optimizations that dramatically improve the performance of RPC in traditional systems. This is generally done by avoiding copying, and reducing context switching overhead and network and RPC protocol overhead. 9 Discussion and conclusions Network interfaces can have a great impact on communication performance and ease of programming.
Reference: [15] <author> A. Karlin, K. Li, M. Menasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of 13th Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1991, </year> <pages> pages 41-55. </pages>
Reference-contexts: When the buffer fills there is very little handshaking; one extra message sent from the receiver to the sender. Waiting in the above protocol is implemented by spinning. Timeouts are used to avoid indefinite postponement. More sophisticated techniques for waiting are given in <ref> [15] </ref>. Blocking the server when there is no work to be done and then waking it up when a request arrives is not a problem, since the server will service all pending requests when it wakes up.
Reference: [16] <author> D. Ritchie and K. Thompson. </author> <title> The Unix time-sharing system. </title> <journal> Communications of the ACM, </journal> <volume> 17(7) </volume> <pages> 365-375, </pages> <month> July </month> <year> 1974. </year>
Reference: [17] <author> M. Schroeder, and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems 8(1) </journal> <pages> 1-17, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 20] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> The Optimistic Active Messages [21] approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties. Several papers (e.g. <ref> [14, 17] </ref>) describe optimizations that dramatically improve the performance of RPC in traditional systems. This is generally done by avoiding copying, and reducing context switching overhead and network and RPC protocol overhead. 9 Discussion and conclusions Network interfaces can have a great impact on communication performance and ease of programming.
Reference: [18] <author> Sun Microsystems, Inc. XDR: </author> <title> external data representation standard. Internet Request For Comments RFC 1014, </title> <institution> Internet Network Working Group, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: SunRPC consists of a set of library functions and a stub generator that follows the XDR <ref> [18] </ref> standard for data representation. SunRPC is a single thread implementation. The general structure of SunRPC is shown in Figure 1. <p> The stream layer hides the details of buffer management and network packet size from the higher layers. Its existence is very important to the performance of standard SunRPC implementations, since it reduces accesses to the expensive lower layers. 6 * The XDR <ref> [18] </ref> layer implements the XDR data representation specification, which insulates the higher layers from issues of machine-specific data representation. Data transferred between nodes in a network are translated to XDR format before sending, and translated back from XDR when received.
Reference: [19] <author> Sun Microsystems, Inc. </author> <title> RPC: Remote Procedure Call Protocol Specification Version 2. Internet Request For Comments RFC 1057, </title> <institution> Internet Network Working Group, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: VMMC provides a call that allows a process to block until a notification arrives for it. When a process is waiting for a VMMC message to arrive, this call can be used to switch easily between spinning and blocking as appropriate. 4 SunRPC SunRPC <ref> [19] </ref> is a widely used remote procedure call interface and specification. SunRPC consists of a set of library functions and a stub generator that follows the XDR [18] standard for data representation. SunRPC is a single thread implementation. The general structure of SunRPC is shown in Figure 1.
Reference: [20] <author> R. Van Renesse, H. Van Staveren, and A. Tanenbaum. </author> <title> The performance of the Amoeba distributed operating system. </title> <journal> Software Practice and Experience, </journal> <volume> 19(3) </volume> <month> 223-234 (Mar. </month> <year> 1989). </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 20] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software.
Reference: [21] <author> D.A. Wallach, W.C. Hsieh, K. Johnson, M..F. Kaashoek, and W.E. Weihl. </author> <title> Optimistic Active Messages: </title>
Reference-contexts: Active messages [12] are a restricted form of RPC, in which the server-side procedure may not perform any actions that might block. Active messages achieve performance similar to ShrimpRPC on high-performance hardware, but without allowing general handlers to be invoked. The Optimistic Active Messages <ref> [21] </ref> approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties.
References-found: 21

