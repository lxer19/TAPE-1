URL: http://www.cs.utexas.edu/users/rupert/ps/paper.ps
Refering-URL: http://www.cs.utexas.edu/users/rupert/research.html
Root-URL: 
Title: A Corpus-Based Connectionist Approach to the Building of Word Representations  
Author: Rupert L. Tang 
Note: E-mail:  
Address: Austin  
Affiliation: Department of Computer Sciences University of Texas at  
Pubnum: CS 379H Term Paper  
Email: rupert@cs.utexas.edu  
Date: December 21, 1994  
Abstract: A system for building distributed representations for words using a sliding window is described here which takes a corpora of sentences from either artificial data or natural data. The system is able to develope representations that capture the regularity of words occurrences underlying a given body of text. The paradigm presented here is based on a FGREP module with some variations on the architecture and the training methodology. Experiments are done to show that the system is capable of capturing the surface regularity of words.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R. </author> <year> (1976). </year> <title> Language, memory, </title> <booktitle> and thought. </booktitle> <address> Hillsdale, NJ.: Eribaum. </address>
Reference: <author> Cairns, H. S., and J. </author> <month> Kamerman </month> <year> (1975). </year> <title> Lexical information processing during sentences compre 12 hension. </title> <journal> Journal of Verbal Learning and Verbal Behavior 14, </journal> <pages> 170-179. </pages>
Reference: <author> Collins, A. M., and M. R. </author> <title> Quillian (1969). Retrieval time from semantic memory. </title> <journal> Journal of Verbal Learning and Verbal Behavior 8, </journal> <pages> 240-247. </pages>
Reference: <author> Conard, C. </author> <year> (1974). </year> <title> Context effects in sentence comprehension: A study of the subjective lexicon. </title> <booktitle> Memory and Cognition 2, </booktitle> <pages> 130-138. </pages>
Reference: <author> Cottrell, G. W. </author> <year> (1989). </year> <title> A Connectionist Approach to Word Sense Disambiguation. </title> <publisher> London: Pitman. </publisher>
Reference-contexts: Related Work There are several approaches to the building of semantic representations under connectionism. The localist schemes use a separate node for each word <ref> (e.g. Cottrell 1989) </ref>. Another approach is to use microfeatures for forming the representation patterns such as the one in the case-role assignment task solved by McClelland and Kawamoto (1986). The problem with these two approach is that they can hardly scale up to deal with large body of words.
Reference: <author> Elman, J. L. </author> <year> (1989). </year> <title> Structured representations and connectionist models. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: Statistical method for building word representations also exists (Schutze 1993) in which cooccurrence statistics for words is used to produce corresponding fourgram vectors. The fourgram vectors are then used to compute the word vectors. Yet another approach is to develope internal representations in hidden layers of a backpropagation network <ref> (Elman 1989) </ref>. The problem with this approach is that representations developed in the hidden later are hidden and not readily available outside the network. And they are developed just as a side effect of training. An approach that addresses this problem is to build external distributed representations instead (Miikkulainen 1993).
Reference: <author> Fischler, I., and P. </author> <title> Bloom (1979). Automatic and attentional process in the effects of sentence contexts on word recognition. </title> <journal> Journal of Verbal Learning and Verbal Behavior 18, </journal> <pages> 1-20. </pages>
Reference: <author> Fodor, J.D.., J.A. Fodor, and M.F. </author> <title> Garrett (1975). The psychological unreality of semantic representations. </title> <journal> Linguistic Inquiry, </journal> <pages> 4 515-531. </pages>
Reference: <author> Holmes, V. M., R. Arwas, and M. F. </author> <title> Garrett (1977). Prior context and the perception of lexically ambiguous sentences. </title> <booktitle> Memory and Cognition 5, </booktitle> <pages> 103-110. </pages>
Reference: <author> Kintsch, W. </author> <year> (1974). </year> <title> The representation of meaning in memory. Hillsdale, </title> <editor> N. J.: </editor> <publisher> Erlbaum. </publisher>
Reference: <author> McClelland, J. L., and A. H. Kawamoto. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In J. L. McClelland, D. E. Rumelhart, </editor> <title> and the PDP Research Group (Ends.), Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Column 2: Psychological and Biological Models, </title> <address> 275-325. Cambridge MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: spoon chicken Human Utensil Breaker Animal Fragile Object Food carrot chicken chicken sheep sheep Prey lion wolf lion Predator bat ball ball bat bat hatchet hatchet paperwt hammer hammer rock Hitter vase dog doll Possession 9 The training data is from the original data set in the case-role assignment task <ref> (McClelland and Kawamoto 1986) </ref>. The only difference we have this time is that part of the words have their representations fixed throughout the course of the training. The network took 80 epochs to lower the output error to 0.02.
Reference: <author> Meyer, D. E., and R. W. </author> <month> Schvaneveldt </month> <year> (1971). </year> <title> Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations. </title> <journal> Journal of Experimental Psychology 90, </journal> <pages> 227-234. </pages>
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subassembly Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The problem with this approach is that representations developed in the hidden later are hidden and not readily available outside the network. And they are developed just as a side effect of training. An approach that addresses this problem is to build external distributed representations instead <ref> (Miikkulainen 1993) </ref>. Word representations are developed at the input layers and could be used or reused in other network modules. <p> Input representations at other units remain the unaffected. The changed representation for a particular word is stored in the lexicon. The mapping task is dynamically changing throughout the whole course of the training but the representations should converge <ref> (Miikkulainen 1993) </ref>. <p> So, categorization is achieved at several aspects. Or, the system is able to perceive such an existence of several levels of categorization inherent in the input data. The clusters that are discovered as a side effect of the training is different from that of FGREP, though <ref> (Miikkulainen 1993) </ref>. The clustering is less fine-grained. For example, the group -vase, hatchet, rock, paperwt, hammer, ball, bat- is clustered as one thing instead of splitting into -ball, hatchet, hammer-, -paperwt, rock with vase and bat staying close to the two clusters. <p> Fixing some of the representations hinders the network from converging, and the side effect is the building of finer word representations. Such prohibition to convergence is conductive to refining word representations. Categorization has usually reached its dynamic equilibrium <ref> (Miikkulainen 1993) </ref> probably at the end of one third of all the training epochs if trained with some decent learning parameters.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Ends.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. Vol.1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The work presented here goes 4 along with the last approach with the potential benefit of automatic discovering of relevant contextual information. 3 Developing Global Distributed Representations The Architecture The system employs a basic frame work of a standard error backpropagation network <ref> (Rumelhart et al. 1986) </ref>. However, error signals from the output layer are extended to the input layer to deve-lope representations for words (Figure 1). The word representations developed at the input layer are then stored in a lexicon external to the network.
Reference: <author> Schaeffer, B., and R. </author> <title> Wallace (1970). The comparison of word meanings. </title> <journal> Journal of Experimen 13 tal Psychology, </journal> <volume> 86, </volume> <pages> 144-152. </pages>
Reference: <author> Schuberth, R. E., and P. D. </author> <month> Eimas </month> <year> (1977). </year> <title> Effects of context in the classification of words and non-words. Journal of Experimental Psychology: </title> <booktitle> Human Perception and Performance 3, </booktitle> <pages> 27-36. </pages>
Reference: <author> Schutze, H. </author> <year> (1993). </year> <title> Word Space. </title> <booktitle> Neural Information Processing System V, </booktitle> <pages> 213-218. </pages>
Reference-contexts: Such global representations for words can be used as a means for intercommunication between different modules. Statistical approaches do exist to derive representations for words instead of having them built by 2 neural networks <ref> (Schutze 1993) </ref>. Such approaches suffer from a need for the manual work of collecting statistics such as the lexical cooccurrence counts and all the computations involved in managing such huge amount of statistical data. <p> The problem with these two approach is that they can hardly scale up to deal with large body of words. And, hand coding of word representations can be biased. Statistical method for building word representations also exists <ref> (Schutze 1993) </ref> in which cooccurrence statistics for words is used to produce corresponding fourgram vectors. The fourgram vectors are then used to compute the word vectors. Yet another approach is to develope internal representations in hidden layers of a backpropagation network (Elman 1989). <p> Having globally available semantic representations for a large amount of words is surely desirable. Attempts have been made with statistical method to accomplish such a task <ref> (schutze 1993) </ref>, though with potential problems outlined before. The work presented here provides another approach to the same task, namely the connectionist approach which is probably an improvement over the more labor-intensive statistical approach.
Reference: <author> Smith, E. E., E. J. Shebang, and L. J. </author> <title> Rips (1974). Structure and process in semantic memory: A factorial model for semantic decisions. </title> <journal> Psychological Review 81, </journal> <pages> 214-241. </pages>
Reference: <author> Swinney, D. A., W. Infer, P. Rather, and M. </author> <month> Hirshkowitz </month> <year> (1979). </year> <title> Semantic facilitation across sensory modalities in the processing of individual words and sentences. </title> <booktitle> Memory and Cognition 7, </booktitle> <pages> 159-165. </pages>
Reference: <author> Tanenhaus, M. K., and M. M. </author> <title> Lucas (1987). Context effects in lexical priming. </title> <journal> Cognition 25, </journal> <pages> 213-234. </pages>
Reference: <author> Tweedy, J. R., R. H. Lapinksy, and R. W. </author> <month> Schvaneveldt </month> <year> (1977). </year> <title> Semantic context effects on word recognition: Inuence of varying the proportion of items presented in an appropriate context. </title> <booktitle> Memory and Cognition 5, </booktitle> <pages> 84-98. </pages>
Reference: <author> St. John, M. F., & McClelland, J. L. </author> <year> (1990). </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 217-258. </pages>
References-found: 22

