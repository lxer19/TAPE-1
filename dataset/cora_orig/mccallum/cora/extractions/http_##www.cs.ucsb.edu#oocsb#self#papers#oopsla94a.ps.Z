URL: http://www.cs.ucsb.edu/oocsb/self/papers/oopsla94a.ps.Z
Refering-URL: http://www.cs.ucsb.edu/oocsb/self/papers/third-generation.html
Root-URL: http://www.cs.ucsb.edu
Keyword: Reconciling Responsiveness with Performance  
Abstract: 1 Abstract: Programming systems should be both responsive (to support rapid development) and efficient (to complete computations quickly). Pure object-oriented languages are harder to implement efficiently since they need optimization to achieve good performance. Unfortunately, optimization conicts with interactive responsiveness because it tends to produce long compilation pauses, leading to unresponsive programming environments. Therefore, to achieve good responsiveness, existing exploratory programming environments such as the Smalltalk-80 environment rely on interpretation or non-optimizing dynamic compilation. But such systems pay a price for their interactive-ness, since they may execute programs several times slower than an optimizing system. SELF-93 reconciles high performance with responsiveness by combining a fast, non-optimizing compiler with a slower, optimizing compiler. The resulting system achieves both excellent performance (two or three times faster than existing Smalltalk systems) and good responsiveness. Except for situations requiring large applications to be (re)compiled from scratch, the system allows for pleasant interactive use with few perceptible compilation pauses. To our knowledge, SELF-93 is the first implementation of a pure object-oriented language achieving both good performance and good responsiveness. When measuring interactive pauses, it is imperative to treat multiple short pauses as one longer pause if the pauses occur in short succession, since they are perceived as one pause by the user. We propose a definition of pause clustering and show that clustering can make an order-of-magnitude difference in the pause time distribution. A Third-Generation SELF Implementation: 
Abstract-found: 1
Intro-found: 1
Reference: [APS93] <author> Ole Agesen, Jens Palsberg, and Michael I. Schwartzbach. </author> <title> Type Inference of SELF: Analysis of Objects with Dynamic and Multiple Inheritance. </title> <booktitle> In ECOOP '93 Conference Proceedings, p. </booktitle> <pages> 247-267. </pages> <address> Kaiserslautern, Germany, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: die down and execution time a Excluding blank lines. b Time excludes the time spent in graphics primitives Benchmark Size (lines) a Description CecilComp 11,500 Cecil-to-C compiler compiling the Fibonacci function CecilInt 9,000 interpreter for the Cecil language [Cha93] running a short test program Typeinf 8,600 type inferencer for SELF <ref> [APS93] </ref> UI1 15,200 prototype user interface using anima tion techniques [CU93] b Table 4: Large SELF applications 10 becomes stable for all programs except UI1 which experiences another urry of compilation around run 15.
Reference: [CUL89] <author> Craig Chambers, David Ungar, and Elgin Lee. </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA 89 Conference Proceedings, p. </booktitle> <pages> 49-70, </pages> <address> New Orleans, LA, </address> <month> October </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October </month> <year> 1989. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: In response to this performance problem, previous SELF compilers have concentrated on optimization techniques aimed at reducing the overhead of message passing. The first-generation SELF compiler <ref> [CUL89] </ref> achieved a respectable speedup over standard Small-talk implementations. The second-generation compiler [CU91] improved performance even more, bringing SELFs performance to within a factor of less than two relative to C for a set of small integer benchmarks. <p> SELFs pure semantics result in very frequent message sends; in this respect, it is even harder to implement efficiently than Smalltalk. These implementation difficulties required some unusual compilation techniques <ref> [CUL89, CU91, HCU91, HU94] </ref>. The following sections briey review the important aspects of SELF-93s implementation. 2.1 Adaptive optimization The SELF-93 system uses dynamic compilation [DS84]. When a source method is invoked for the first time, it is compiled quickly by a very simple but completely non-optimizing compiler.
Reference: [CU91] <author> Craig Chambers and David Ungar. </author> <title> Making Pure Object-Oriented Languages Practical. </title> <booktitle> OOPSLA 91 Conference Proceedings, </booktitle> <address> Phoenix, AZ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: As a result, computationally intensive Smalltalk programs can be an order of magnitude slower <ref> [CU91] </ref> than programs written in hybrid object-oriented languages like C++ or conventional languages like C. In response to this performance problem, previous SELF compilers have concentrated on optimization techniques aimed at reducing the overhead of message passing. The first-generation SELF compiler [CUL89] achieved a respectable speedup over standard Small-talk implementations. <p> In response to this performance problem, previous SELF compilers have concentrated on optimization techniques aimed at reducing the overhead of message passing. The first-generation SELF compiler [CUL89] achieved a respectable speedup over standard Small-talk implementations. The second-generation compiler <ref> [CU91] </ref> improved performance even more, bringing SELFs performance to within a factor of less than two relative to C for a set of small integer benchmarks. <p> SELFs pure semantics result in very frequent message sends; in this respect, it is even harder to implement efficiently than Smalltalk. These implementation difficulties required some unusual compilation techniques <ref> [CUL89, CU91, HCU91, HU94] </ref>. The following sections briey review the important aspects of SELF-93s implementation. 2.1 Adaptive optimization The SELF-93 system uses dynamic compilation [DS84]. When a source method is invoked for the first time, it is compiled quickly by a very simple but completely non-optimizing compiler.
Reference: [Cha92] <author> Craig Chambers, </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1992 </year>
Reference-contexts: Table 3 shows the individual interactions of the sequence. For the measurements, the interactions were executed in the sequence shown in the table, with no a same action as no. 12, but slower because of the preceding change (see text) System Description SELF-91 Chambers SELF compiler <ref> [Cha92] </ref>; all methods are always optimized from the beginning. SELF-93 The current SELF system using dynamic recompi lation; methods are compiled by a fast non-opti mizing compiler first, then recompiled with the optimizing compiler if necessary.
Reference: [Cha93] <author> Craig Chambers. </author> <title> The Cecil Language - Specification and Rationale. </title> <institution> University of Washington, </institution> <type> Technical Report UW CS TR 93-03-05, </type> <year> 1993. </year>
Reference-contexts: After a few runs, the compilations die down and execution time a Excluding blank lines. b Time excludes the time spent in graphics primitives Benchmark Size (lines) a Description CecilComp 11,500 Cecil-to-C compiler compiling the Fibonacci function CecilInt 9,000 interpreter for the Cecil language <ref> [Cha93] </ref> running a short test program Typeinf 8,600 type inferencer for SELF [APS93] UI1 15,200 prototype user interface using anima tion techniques [CU93] b Table 4: Large SELF applications 10 becomes stable for all programs except UI1 which experiences another urry of compilation around run 15.
Reference: [CU93] <author> Bay-Wei Chang and David Ungar. </author> <title> Animation: From Cartoons to the User Interface. </title> <booktitle> User Interface Software and Technology Conference Proceedings, </booktitle> <address> Atlanta, GA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The second-generation compiler [CU91] improved performance even more, bringing SELFs performance to within a factor of less than two relative to C for a set of small integer benchmarks. However, as larger SELF programs were being written (for example, a graphical user interface <ref> [CU93] </ref> consisting of 15,000 lines of SELF code), it became increasingly clear that the existing SELF systems had neglected interactive performance. <p> Compile pauses during an interactive session We measured the (clustered) compilation pauses occurring during a 50-minute session of the SELF user interface <ref> [CU93] </ref>. The session involved completing a SELF tutorial, which includes browsing, editing, and making small programming changes. During the tutorial, a bug in the tutorials cut-and-paste code was discovered, so that the session also includes some real-life debugging. <p> Time excludes the time spent in graphics primitives Benchmark Size (lines) a Description CecilComp 11,500 Cecil-to-C compiler compiling the Fibonacci function CecilInt 9,000 interpreter for the Cecil language [Cha93] running a short test program Typeinf 8,600 type inferencer for SELF [APS93] UI1 15,200 prototype user interface using anima tion techniques <ref> [CU93] </ref> b Table 4: Large SELF applications 10 becomes stable for all programs except UI1 which experiences another urry of compilation around run 15.
Reference: [Dig91] <author> Digitalk Inc. Smalltalk/V system, </author> <year> 1991. </year>
Reference-contexts: Traditionally, system designers have used interpreters or non-optimizing compilers in exploratory programming environments to achieve immediate feedback. For example, commercial Smalltalk implementations use either interpretation <ref> [Dig91] </ref> or non-optimizing dynamic compilation [DS84, PP92]. Unfortunately, the overhead of interpretation, combined with the efficiency problems created by the high call frequency and the heavy use of dynamic dispatch in pure object-oriented languages, slows down execution and can limit the usefulness of such systems.
Reference: [DS84] <author> L. Peter Deutsch and Alan Schiffman, </author> <title> Efficient Implementation of the Smalltalk-80 System. </title> <booktitle> Proceedings of the 11th Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <year> 1984. </year>
Reference-contexts: Traditionally, system designers have used interpreters or non-optimizing compilers in exploratory programming environments to achieve immediate feedback. For example, commercial Smalltalk implementations use either interpretation [Dig91] or non-optimizing dynamic compilation <ref> [DS84, PP92] </ref>. Unfortunately, the overhead of interpretation, combined with the efficiency problems created by the high call frequency and the heavy use of dynamic dispatch in pure object-oriented languages, slows down execution and can limit the usefulness of such systems. <p> These implementation difficulties required some unusual compilation techniques [CUL89, CU91, HCU91, HU94]. The following sections briey review the important aspects of SELF-93s implementation. 2.1 Adaptive optimization The SELF-93 system uses dynamic compilation <ref> [DS84] </ref>. When a source method is invoked for the first time, it is compiled quickly by a very simple but completely non-optimizing compiler. Conversely, whenever the user changes a source method, all compiled code depending on the old definition is invalidated. <p> If the method overowing its counter isnt recompiled, its counter is reset to zero. Counter values decay exponentially with time. unoptimized code source methods if executed often if needed for debugging [HCU92] is first invoked optimized code when method dynamic compilation <ref> [DS84] </ref> adaptive optimization [HU94] stack grows downwards A method over-ows its invocation counter and triggers a recompilation The system inspects the stack to determine which method to recompile. Then, it calls the compiler to generate new code. <p> The system tried to allocate compile time wisely in order to minimize total execution time, i.e. the sum of compile and runtime; being a batch system, interactive pauses were not an issue. The Deutsch-Schiffman Smalltalk system <ref> [DS84] </ref> (and its commercial successor, ParcPlace Smalltalk [PP92]) were the first object-oriented systems to use dynamic compilation. Using a very fast non-optimizing compiler, the system achieves excellent interactive performance; compilation pauses are virtually unnoticeable on current hardware.
Reference: [Han74] <author> Gilbert J. Hansen, </author> <title> Adaptive Systems for the Dynamic Run-Time Optimization of Programs. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1974. </year>
Reference-contexts: His design (the system was not actually implemented) mixed interpretation and compilation: code was first interpreted, but subsequent executions used compiled code that was generated as a side-effect of interpretation. Hansen <ref> [Han74] </ref> describes an adaptive compiler for Fortran. His compiler optimized the inner loops of Fortran programs at runtime. The main goal was to minimize the total cost of running a program (which presumably was executed only once), and programs were run batch-style.
Reference: [HCU91] <author> Urs Hlzle, Craig Chambers, and David Ungar. </author> <title> Optimizing Dynamically-Typed Object-Oriented Languages with Polymorphic Inline Caches. </title> <booktitle> In ECOOP91 Conference Proceedings, Geneva, 1991. Published as Springer Verlag Lecture Notes in Computer Science 512, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: SELFs pure semantics result in very frequent message sends; in this respect, it is even harder to implement efficiently than Smalltalk. These implementation difficulties required some unusual compilation techniques <ref> [CUL89, CU91, HCU91, HU94] </ref>. The following sections briey review the important aspects of SELF-93s implementation. 2.1 Adaptive optimization The SELF-93 system uses dynamic compilation [DS84]. When a source method is invoked for the first time, it is compiled quickly by a very simple but completely non-optimizing compiler. <p> left on the stack until they return. 4 2.4 Type Feedback When recompiling a method, the system extracts type information from the previous version of compiled code and feeds it back to the compiler. (This technique is called Type Feedback [HU94].) Specifically, the SELF-93 system uses Polymorphic Inline Caches (PICs) <ref> [HCU91] </ref> to record the programs type profile, i.e., a list of receiver types (and, optionally, their frequencies) for every single call site in the program. PICs were originally conceived to speed up dynamic dispatch, but as observed in [HCU91] they record receiver types as a side-effect. <p> Type Feedback [HU94].) Specifically, the SELF-93 system uses Polymorphic Inline Caches (PICs) <ref> [HCU91] </ref> to record the programs type profile, i.e., a list of receiver types (and, optionally, their frequencies) for every single call site in the program. PICs were originally conceived to speed up dynamic dispatch, but as observed in [HCU91] they record receiver types as a side-effect. Therefore, a programs type profile is readily available, and collecting the type feedback data does not incur an execution time overhead.
Reference: [HCU92] <author> Urs Hlzle, Craig Chambers, and David Ungar. </author> <title> Debugging Optimized Code with Dynamic Deop-timization. </title> <booktitle> In Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 32-43. </pages> <note> Published as SIGPLAN Notices 27(7), </note> <month> July </month> <year> 1992. </year>
Reference-contexts: Conversely, whenever the user changes a source method, all compiled code depending on the old definition is invalidated. To accomplish this, the system keeps dependency links between source and compiled methods <ref> [HCU92, Ch92] </ref>. Since there is no explicit compilation or linking step, the traditional edit-compile-link-run cycle is collapsed into an edit-continue cycle. Programs can be changed while they are running so that the application being debugged need not even be restarted. <p> If the method overowing its counter isnt recompiled, its counter is reset to zero. Counter values decay exponentially with time. unoptimized code source methods if executed often if needed for debugging <ref> [HCU92] </ref> is first invoked optimized code when method dynamic compilation [DS84] adaptive optimization [HU94] stack grows downwards A method over-ows its invocation counter and triggers a recompilation The system inspects the stack to determine which method to recompile. Then, it calls the compiler to generate new code. <p> In this way, a programs execution speed will gradually improve as more and more of its hot spots are optimized. This process is the reverse of dynamic deoptimization as described in <ref> [HCU92] </ref>; that paper also describes how the compiler represents the source-level state of optimized code. <p> Pause clustering achieves this by combining consecutive short pauses into one longer pause, rather than just The SELF system uses a similar mechanism to provide source-level debugging of optimized code and to allow the programmer to change programs while they are running <ref> [HCU92] </ref>. measuring individual pauses. Applying pause clustering to the compilation pauses of the SELF-93 system changes the pause distribution by an order of magnitude, emphasizing the importance of pause clustering. We believe that pause clustering should be used whenever pause length is important, for example, when evaluating incremental garbage collectors.
Reference: [Hl94] <author> Urs Hlzle. </author> <title> Adaptive Optimization in SELF: Reconciling High Performance with Exploratory Programming. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1994. </year> <note> (Available via ftp/http from self.stanford.edu.) </note>
Reference-contexts: SELF is available via Mosaic URLs http://www.sun.com/smli and http://self.stanford.edu, or via ftp from self.stanford.edu. The system described here is largely identical to the current public release (3.0) but contains several performance improvements. This description is based on [HU94]; more details can be found in <ref> [Hl94] </ref>. 3 needed, SELF-93 uses adaptive optimization to dynamically discover and optimize the hot spots of a program. If a method is executed often, it is recompiled with an optimizing compiler. <p> In the SELF-93 system, programs like these usually spend less than 20% of their time in unoptimized code <ref> [Hl94] </ref>; in contrast, the SELF-91 system optimizes all code. The system described here uses slightly different optimization parameters that the system measured in [HU94], reducing performance by about 10%. 5 occur in clusters. <p> Using a very fast non-optimizing compiler, the system achieves excellent interactive performance; compilation pauses are virtually unnoticeable on current hardware. Smalltalk is easier to compile 12 with a non-optimizing compiler than SELF since Smalltalk hardwires important control structures (such as if and while) whereas SELF doesnt <ref> [Hl94] </ref>. However, compared to SELF-93, unoptimized Smalltalk code runs roughly three times slower on the programs measured in [HU94], demonstrating the limits of non-optimizing compilation. Lisp systems have long used a mixture of interpreted and compiled code (or optimized and unoptimized compiled code).
Reference: [HU94] <author> Urs Hlzle and David Ungar. </author> <title> Optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback. </title> <booktitle> In Proceedings of the SIGPLAN 94 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 326-336. </pages> <note> Published as SIGPLAN Notices 29(6), </note> <month> June </month> <year> 1994. </year>
Reference-contexts: It uses a fast, non-optimizing compiler to generate the initial code, and then recompiles only the time-critical parts with a slower, optimizing compiler. Introducing dynamic recompilation dramatically improves interactive performance, making it possible to combine optimizing compilation with an exploratory programming environment. As described elsewhere <ref> [HU94] </ref>, SELF-93 provides excellent execution-time performance. This paper concentrates on the interactive behavior of the system and shows that it can provide good interactive performance on current workstations and should provide excellent interactive performance (i.e., virtually unnoticeable compile pauses) on future workstations. <p> SELFs pure semantics result in very frequent message sends; in this respect, it is even harder to implement efficiently than Smalltalk. These implementation difficulties required some unusual compilation techniques <ref> [CUL89, CU91, HCU91, HU94] </ref>. The following sections briey review the important aspects of SELF-93s implementation. 2.1 Adaptive optimization The SELF-93 system uses dynamic compilation [DS84]. When a source method is invoked for the first time, it is compiled quickly by a very simple but completely non-optimizing compiler. <p> SELF is available via Mosaic URLs http://www.sun.com/smli and http://self.stanford.edu, or via ftp from self.stanford.edu. The system described here is largely identical to the current public release (3.0) but contains several performance improvements. This description is based on <ref> [HU94] </ref>; more details can be found in [Hl94]. 3 needed, SELF-93 uses adaptive optimization to dynamically discover and optimize the hot spots of a program. If a method is executed often, it is recompiled with an optimizing compiler. <p> If the method overowing its counter isnt recompiled, its counter is reset to zero. Counter values decay exponentially with time. unoptimized code source methods if executed often if needed for debugging [HCU92] is first invoked optimized code when method dynamic compilation [DS84] adaptive optimization <ref> [HU94] </ref> stack grows downwards A method over-ows its invocation counter and triggers a recompilation The system inspects the stack to determine which method to recompile. Then, it calls the compiler to generate new code. The system replaces the old (un-optimized) stack frames with the frame of the newly compiled method. <p> This process is the reverse of dynamic deoptimization as described in [HCU92]; that paper also describes how the compiler represents the source-level state of optimized code. SELF-93 cannot always replace unoptimized with optimized frames (see <ref> [HU94] </ref>); in such cases, the unoptimized frames are left on the stack until they return. 4 2.4 Type Feedback When recompiling a method, the system extracts type information from the previous version of compiled code and feeds it back to the compiler. (This technique is called Type Feedback [HU94].) Specifically, the <p> frames (see <ref> [HU94] </ref>); in such cases, the unoptimized frames are left on the stack until they return. 4 2.4 Type Feedback When recompiling a method, the system extracts type information from the previous version of compiled code and feeds it back to the compiler. (This technique is called Type Feedback [HU94].) Specifically, the SELF-93 system uses Polymorphic Inline Caches (PICs) [HCU91] to record the programs type profile, i.e., a list of receiver types (and, optionally, their frequencies) for every single call site in the program. <p> With type feedback, the compiler can inline more message sends and thus to achieve better performance than previous compilers. On average, SELF-93 executes a suite of six large (4,00015,000 lines) and three medium sized (4001,100 lines) programs 1.5 times faster than the SELF-91 compiler <ref> [HU94] </ref>. For the two medium-sized programs that are also available in Smalltalk, SELF-93 is about three times faster than ParcPlace Smalltalk. But raw execution performance is not the focus of this paper. Rather, if focuses on how optimizing compilation inuences the interactive behavior of a system. <p> In the SELF-93 system, programs like these usually spend less than 20% of their time in unoptimized code [Hl94]; in contrast, the SELF-91 system optimizes all code. The system described here uses slightly different optimization parameters that the system measured in <ref> [HU94] </ref>, reducing performance by about 10%. 5 occur in clusters. When two compilations occur back-to-back, they are perceived as one pause by the user and thus should be counted as a single long pause rather than two shorter pauses. <p> Smalltalk is easier to compile 12 with a non-optimizing compiler than SELF since Smalltalk hardwires important control structures (such as if and while) whereas SELF doesnt [Hl94]. However, compared to SELF-93, unoptimized Smalltalk code runs roughly three times slower on the programs measured in <ref> [HU94] </ref>, demonstrating the limits of non-optimizing compilation. Lisp systems have long used a mixture of interpreted and compiled code (or optimized and unoptimized compiled code). However, the user usually has to manually compile programs.
Reference: [ISE93] <author> ISE Inc. ISE Eiffel 3.0, </author> <year> 1993. </year>
Reference-contexts: However, the user usually has to manually compile programs. Since the transition from un-optimized (interpreted) code to optimized (compiled) code is not automatic, such systems cannot directly be compared to systems using dynamic recompilation. Some commercial implementations of Eiffel <ref> [ISE93] </ref> and C++ [SGI93] can handle the reverse transition (from compiled to interpreted) automatically. That is, after a source method is changed, the compiled code is no longer executed and the method is interpreted until the programmer recompiles that part of the program.
Reference: [Mit70] <author> J. G. Mitchell, </author> <title> Design and Construction of Flexible and Efficient Interactive Programming Systems. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1970. </year>
Reference-contexts: On a SPARCstation-2, a 10,000-line program approaches stable performance after about one to two minutes in SELF-93 (see Figure 10), which is adequate. 7. Related work One of the first people concerned with the implementation of efficient but exible programming systems was Mitchell <ref> [Mit70] </ref>. His design (the system was not actually implemented) mixed interpretation and compilation: code was first interpreted, but subsequent executions used compiled code that was generated as a side-effect of interpretation. Hansen [Han74] describes an adaptive compiler for Fortran. His compiler optimized the inner loops of Fortran programs at runtime.
Reference: [PP92] <author> ParcPlace Systems. VisualWorks 1.0, </author> <year> 1992. </year>
Reference-contexts: Traditionally, system designers have used interpreters or non-optimizing compilers in exploratory programming environments to achieve immediate feedback. For example, commercial Smalltalk implementations use either interpretation [Dig91] or non-optimizing dynamic compilation <ref> [DS84, PP92] </ref>. Unfortunately, the overhead of interpretation, combined with the efficiency problems created by the high call frequency and the heavy use of dynamic dispatch in pure object-oriented languages, slows down execution and can limit the usefulness of such systems. <p> The system tried to allocate compile time wisely in order to minimize total execution time, i.e. the sum of compile and runtime; being a batch system, interactive pauses were not an issue. The Deutsch-Schiffman Smalltalk system [DS84] (and its commercial successor, ParcPlace Smalltalk <ref> [PP92] </ref>) were the first object-oriented systems to use dynamic compilation. Using a very fast non-optimizing compiler, the system achieves excellent interactive performance; compilation pauses are virtually unnoticeable on current hardware.
Reference: [SM+93] <author> Michael Sannella, John Maloney, Bjorn Freeman-Benson, and Alan Borning. </author> <title> Multi-way versus One-way Constraints in User Interfaces: Experience with the DeltaBlue Algorithm. </title> <booktitle> Software Practice and Experience 23 (5): </booktitle> <pages> 529-566, </pages> <year> 1993. </year>
Reference: [SGI93] <institution> Silicon Graphics, Inc. SGI C++, </institution> <year> 1993. </year>
Reference-contexts: However, the user usually has to manually compile programs. Since the transition from un-optimized (interpreted) code to optimized (compiled) code is not automatic, such systems cannot directly be compared to systems using dynamic recompilation. Some commercial implementations of Eiffel [ISE93] and C++ <ref> [SGI93] </ref> can handle the reverse transition (from compiled to interpreted) automatically. That is, after a source method is changed, the compiled code is no longer executed and the method is interpreted until the programmer recompiles that part of the program.
Reference: [US87] <author> David Ungar and Randall B. Smith. </author> <title> SELF: The Power of Simplicity. </title> <booktitle> In OOPSLA 87 Conference Proceedings, p. </booktitle> <pages> 227-241, </pages> <address> Orlando, FL, </address> <month> October </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 22(12), </note> <month> December </month> <year> 1987. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Background SELF <ref> [US87] </ref> is a pure object-oriented language: all data are objects, and all computation is performed via dynamically-bound message sends (including accesses to all instance variables, even those in the receiver object).
Reference: [WM89] <author> Paul R. Wilson and Thomas G. Moher. </author> <title> Design of the Opportunistic Garbage Collector. </title> <booktitle> In OOPSLA 89 Conference Proceedings, </booktitle> <pages> pp. 23-35, </pages> <address> New Orleans, LA, </address> <month> October, </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October </month> <year> 1989. </year> <month> 14 </month>
Reference-contexts: Thus, if the system decides that a certain method should be optimized, the actual optimizing compilation could be deferred if desired. For example, the system could enter the optimization requests into a queue and process them during the users think pauses (similar to opportunistic garbage collection <ref> [WM89] </ref>). Alternatively, optimizing compilations could be performed in parallel with program execution on a multiprocessor machine. Adaptive recompilation gives implementors of pure object-oriented languages such as Smalltalk a new option for implementing their systems.
References-found: 20

