URL: http://www.cs.dartmouth.edu/~zhangq/papers/dcc97.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~zhangq/homepage.html
Root-URL: http://www.cs.dartmouth.edu
Email: fzhangq, jmd, neyg@cs.dartmouth.edu  
Title: A codebook generation algorithm for document image compression  
Author: Qin Zhang John M. Danskin Neal E. Young 
Address: Hanover, NH 03755, USA  
Affiliation: 6211 Sudikoff Laboratory Department of Computer Science Dartmouth College  
Abstract: Pattern-matching based document compression systems rely on finding a small set of patterns that can be used to represent all of the ink in the document. Finding an optimal set of patterns is NP-hard; previous compression schemes have resorted to heuristics. We extend the cross-entropy approach, used previously for measuring pattern similarity, to this problem. Using this approach we reduce the problem to the fixed-cost k-median problem, for which we present a new algorithm with a good provable performance guarantee. We test our new algorithm in place of the previous heuristics (First Fit, with and without generalized Lloyd's (k-means) postprocessing steps). The new algorithm generates a better codebook, resulting in an overall improvement in compression performance of almost 17%.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. N. Ascher and Nagy, </author> <title> "A means for achieving a high degree of compaction on scan-digitized printed text," </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-23(11):1174-1179, </volume> <year> 1974. </year>
Reference-contexts: 1 INTRODUCTION For scanned text, pattern matching based compression achieves the best compression ratios currently known: roughly four times that of 2D-FAX coding [11]. Pattern matching based compression has been studied by several research groups <ref> [1, 6, 9, 11] </ref>. This kind of compression involves the following steps: Extract from the image of the scanned document a sequence of glyphs. Each glyph typically represents one connected blob of ink occuring somewhere in the doc ument, represented as a positioned bitmap.
Reference: [2] <author> V. Chvatal, </author> <title> "A greedy heuristic for the set-covering problem"' Mathematics of Operations Research, </title> <journal> Vol. </journal> <volume> 4, Number 3, </volume> <pages> p 233-235, </pages> <year> 1979. </year>
Reference-contexts: Roughly, Hochbaum observes that the problem reduces to a traditional weighted set-cover problem with exponentially many sets. She then argues it suffices to consider only quadratically many of these sets and obtains a cubic-time algorithm by adapting the weighted greedy set cover algorithm of Chvatal <ref> [2] </ref>.
Reference: [3] <author> T. Cormen, C. Leiserson, and R. Rivest, </author> <booktitle> Introduction to Algorithms, </booktitle> <pages> pp. 974-978, </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1991. </year> <month> 9 </month>
Reference: [4] <author> Allen Gersho and Robert Gray, </author> <title> Vector Quantization and Signal Compression, pp. </title> <publisher> 362--369, Kluwer Academic Publishers,Boston/Dordrecht/London, </publisher> <year> 1992 </year>
Reference-contexts: In this paper we use the measure proposed in [14]. The main contribution of this paper is a new method for the partitioning step. Recent works have used relatively ad-hoc heuristics for this step, with the most succesful bearing a resemblance to Lloyd's algorithm for vector quantizer design <ref> [4] </ref>. The conceptual basis of the new method is an extension of a particular cross-entropy model, proposed and approximated in [14], for measuring distances between bitmaps. Here is a summary of this model. <p> We also test a variant of this algorithm which reassigns each glyph to its best matching pattern. This algorithm is a specialization of the Generalized Lloyd (k-means) algorithm for vector quantizer design, because the repartitioned equvalence classes satisfy the nearest-neighbor condition <ref> [4] </ref>. We call this algorithm "modified k-means" because the averaging and thresholding steps do not necessarily produce an optimal pattern for each equivalance class. This can degrade the performance of the algorithm. 3 THE GREEDY K-MEDIAN ALGORITHM The weighted fixed-cost k-median problem is the following.
Reference: [5] <author> Dorit S. Hochbaum, </author> <title> "Heuristics for the fixed cost median problem," </title> <booktitle> Mathematical Programming 22 (1982) pp. </booktitle> <pages> 148-162. </pages>
Reference-contexts: Because of the properties of glyphs and their patterns, the weights in this graph satisfy the usual triangle inequality, as well as c (v) c (u) + d (u; v) for all u; v. 3.1 Previous work. Hochbaum <ref> [5] </ref> gives a polynomial-time algorithm that finds a set for which the cost plus the distortion is at most 1 + ln n times the minimum possible. Roughly, Hochbaum observes that the problem reduces to a traditional weighted set-cover problem with exponentially many sets.
Reference: [6] <author> M. J. Holt and C.S.Xydeas, </author> <title> "Recent developments in image data compression for digital facsimile," </title> <journal> ICL Technical Journal, </journal> <pages> pp. 123-146, </pages> <year> 1986. </year>
Reference-contexts: 1 INTRODUCTION For scanned text, pattern matching based compression achieves the best compression ratios currently known: roughly four times that of 2D-FAX coding [11]. Pattern matching based compression has been studied by several research groups <ref> [1, 6, 9, 11] </ref>. This kind of compression involves the following steps: Extract from the image of the scanned document a sequence of glyphs. Each glyph typically represents one connected blob of ink occuring somewhere in the doc ument, represented as a positioned bitmap.
Reference: [7] <author> S. Inglis and I.H.Witten, </author> <title> "Compression-based template matching," </title> <booktitle> Proc. IEEE Data Compression Conference, </booktitle> <address> pp.106-115, </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: For this work, the codebook is compressed using Moffatt's two-level context-based method (following [11]). The glyph indices are coded using PPMC. The glyph positions are coded using structure-based position coding developed in [15]. Several recent papers have focussed on how to measure the distance between pairs of bitmaps <ref> [7, 14] </ref> via cross-entropy measures. These measures approximate the number of bits needed to encode one bitmap given the other under various models. In this paper we use the measure proposed in [14]. The main contribution of this paper is a new method for the partitioning step.
Reference: [8] <author> J-H Line and J.S. Vitter, </author> <title> "Approximations with miminum packing constraint violation," </title> <booktitle> Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> p 771-782, </pages> <year> 1992. </year>
Reference-contexts: For the variant in which the desired cost k is specified, and the problem is to minimize the resulting distortion, Lin and Vitter <ref> [8] </ref> give a polynomial-time approximation algorithm with the following performance guarantee: given * &gt; 0, the algorithm returns a set of cost at most (1 + 1=*)(1 + ln (jV j))k with distortion at most 1 + * times the minimum distortion for a set of cost k.
Reference: [9] <author> K. M. Mohiuddin, </author> <title> "Lossless binary image compression based on pattern matching," </title> <booktitle> Proc. International Conference on Computers, System and Signal Processing, </booktitle> <address> pp.447-451, </address> <year> 1984. </year>
Reference-contexts: 1 INTRODUCTION For scanned text, pattern matching based compression achieves the best compression ratios currently known: roughly four times that of 2D-FAX coding [11]. Pattern matching based compression has been studied by several research groups <ref> [1, 6, 9, 11] </ref>. This kind of compression involves the following steps: Extract from the image of the scanned document a sequence of glyphs. Each glyph typically represents one connected blob of ink occuring somewhere in the doc ument, represented as a positioned bitmap.
Reference: [10] <author> G. Nemhauser and L. Wolsey, </author> <title> Integer and Combinatorial Optimization, </title> <publisher> pp.709-712, John Wiley & Sons, </publisher> <year> 1988. </year>
Reference-contexts: The algorithm is based on the greedy algorithm for minimizing a linear function subject to a submodular constraint <ref> [10] </ref> (a generalization of the greedy set-cover algorithm). Define the capped distortion of a set S to be ffi (S) = v2V The algorithm is the following. GREEDY-K-MEDIAN (G = (V; E); c; w) 1.
Reference: [11] <author> I. H. Witten, T.C. Bell, H. Emberson, and S. Inglis, </author> <title> "Textual image compression: two-stage lossy/lossless encoding of textual images," </title> <booktitle> Proceedings of the IEEE, v. </booktitle> <volume> 86, No. 6, </volume> <pages> pp. 878-888, </pages> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION For scanned text, pattern matching based compression achieves the best compression ratios currently known: roughly four times that of 2D-FAX coding <ref> [11] </ref>. Pattern matching based compression has been studied by several research groups [1, 6, 9, 11]. This kind of compression involves the following steps: Extract from the image of the scanned document a sequence of glyphs. <p> 1 INTRODUCTION For scanned text, pattern matching based compression achieves the best compression ratios currently known: roughly four times that of 2D-FAX coding [11]. Pattern matching based compression has been studied by several research groups <ref> [1, 6, 9, 11] </ref>. This kind of compression involves the following steps: Extract from the image of the scanned document a sequence of glyphs. Each glyph typically represents one connected blob of ink occuring somewhere in the doc ument, represented as a positioned bitmap. <p> For effective compression, the transmission step requires additional compression techniques to economically represent the codebook, indices, and position information. For this work, the codebook is compressed using Moffatt's two-level context-based method (following <ref> [11] </ref>). The glyph indices are coded using PPMC. The glyph positions are coded using structure-based position coding developed in [15]. Several recent papers have focussed on how to measure the distance between pairs of bitmaps [7, 14] via cross-entropy measures. <p> By simply substituting GKM for First Fit in the partitioning step, we reduce the size of the compressed documents in our test suite by 17%. 2 2 PREVIOUS WORK Previous systems <ref> [11] </ref> used a simple codebook generation algorithm which we call the First Fit algorithm. This algorithm works as follows: FIRST-FIT (S) 1. <p> We compute cost (cjp) using the spatial sampling error based cross entropy measure mentioned above [14]. We also implemented the First Fit algorithm with multi-pass optimization as used in the MGTIC system <ref> [11] </ref>. We have tested these configurations on ten high quality scanned document pages with over 20,000 glyphs. Two of the pages, CCITT1 and CCITT4, are from the CCITT test image set, scanned at 200 dpi. <p> In each case we used the same underlying pattern matching algorithm, EPM [14]. We use the first match version of the modified k-means optimization (used in MGTIC <ref> [11] </ref>) because it produces slightly better results and runs faster than the best match version. Compared with the First Fit algorithm, GKM reduces the codebook size by an average of 26%, a remarkable improvement. Notice that for CCITT4, the number of patterns grows.
Reference: [12] <author> I. H. Witten, Alistair Moffat, and T. C. Bell, </author> <title> Managing Gigabytes: compressing and indexing documents and images, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [13] <author> I. H. Witten, Alistair Moffat, T. Bell, et al. </author> <title> Software kit for mg. </title> <note> Available via anonymous ftp from munnari.oz.au in the directory /pub/mg. </note>
Reference-contexts: So the final partition is ffa; bg; fcg; fdgg. If we change the order of the list to fb; a; c; dg, however, the algorithm finds the partition ffb; a; c; dgg. Several compression systems, including MGTIC <ref> [13] </ref> and CDIS [16], combine a bitmap averaging method with the First-Fit algorithm to form a multi-pass pattern classification method. For each equivalence class, all of the bitmaps in the class are averaged and the result is thresholded to obtain a new pattern.
Reference: [14] <author> Qin Zhang and John M. Danskin, </author> <title> "Entropy-based pattern matching for document image compression," </title> <booktitle> Proceedings of the International Conference on Image Processing 1996, </booktitle> <pages> pp 192-199, </pages> <address> Lausanne, Switzerland, </address> <month> 16-19 September </month> <year> 1996. </year>
Reference-contexts: For this work, the codebook is compressed using Moffatt's two-level context-based method (following [11]). The glyph indices are coded using PPMC. The glyph positions are coded using structure-based position coding developed in [15]. Several recent papers have focussed on how to measure the distance between pairs of bitmaps <ref> [7, 14] </ref> via cross-entropy measures. These measures approximate the number of bits needed to encode one bitmap given the other under various models. In this paper we use the measure proposed in [14]. The main contribution of this paper is a new method for the partitioning step. <p> Several recent papers have focussed on how to measure the distance between pairs of bitmaps [7, 14] via cross-entropy measures. These measures approximate the number of bits needed to encode one bitmap given the other under various models. In this paper we use the measure proposed in <ref> [14] </ref>. The main contribution of this paper is a new method for the partitioning step. Recent works have used relatively ad-hoc heuristics for this step, with the most succesful bearing a resemblance to Lloyd's algorithm for vector quantizer design [4]. <p> Recent works have used relatively ad-hoc heuristics for this step, with the most succesful bearing a resemblance to Lloyd's algorithm for vector quantizer design [4]. The conceptual basis of the new method is an extension of a particular cross-entropy model, proposed and approximated in <ref> [14] </ref>, for measuring distances between bitmaps. Here is a summary of this model. Each bitmap is assumed to have been generated by scanning an ideal character (a blob of ink) at some random offset. Thus, any ideal character induces a probability distribution on the bitmaps. <p> The average improvement is 13.7%. 4 EXPERIMENTAL RESULTS We have implemented the GKM algorithm with and without modified k-means post-processing. cost (c) is estimated as the area of each glyph. We compute cost (cjp) using the spatial sampling error based cross entropy measure mentioned above <ref> [14] </ref>. We also implemented the First Fit algorithm with multi-pass optimization as used in the MGTIC system [11]. We have tested these configurations on ten high quality scanned document pages with over 20,000 glyphs. <p> Table 1 compares the sizes of the codebook produced by the First Fit codebook generation algorithm with and without the multi-pass optimization, and our GKM algorithm. In each case we used the same underlying pattern matching algorithm, EPM <ref> [14] </ref>. We use the first match version of the modified k-means optimization (used in MGTIC [11]) because it produces slightly better results and runs faster than the best match version. Compared with the First Fit algorithm, GKM reduces the codebook size by an average of 26%, a remarkable improvement. <p> GKM improves compression significantly. The average lossy output of GKM is about 83% of First Fit's output. thresholding effects. Table 3 compares overall compression ratios achieved using GKM and First Fit. We used the same lossy glyph position coding scheme [15] and Entropy-based Pattern Matching (EPM) <ref> [14] </ref> to compress the test images. With the exception of CCITT4, GKM dominated First Fit. The use of GKM reduced the size of the compressed documents by an average of 17%. We also compared reconstructed images resulting from plain First Fit and GKM.
Reference: [15] <author> Qin Zhang and John M. Danskin, </author> <title> "A pattern-based lossy compression scheme for document images," </title> <journal> Electronic Publishing. </journal> <volume> Vol. 8 (2&3), </volume> <month> pp 235-246 June & September </month> <year> 1995. </year>
Reference-contexts: For this work, the codebook is compressed using Moffatt's two-level context-based method (following [11]). The glyph indices are coded using PPMC. The glyph positions are coded using structure-based position coding developed in <ref> [15] </ref>. Several recent papers have focussed on how to measure the distance between pairs of bitmaps [7, 14] via cross-entropy measures. These measures approximate the number of bits needed to encode one bitmap given the other under various models. In this paper we use the measure proposed in [14]. <p> GKM improves compression significantly. The average lossy output of GKM is about 83% of First Fit's output. thresholding effects. Table 3 compares overall compression ratios achieved using GKM and First Fit. We used the same lossy glyph position coding scheme <ref> [15] </ref> and Entropy-based Pattern Matching (EPM) [14] to compress the test images. With the exception of CCITT4, GKM dominated First Fit. The use of GKM reduced the size of the compressed documents by an average of 17%. We also compared reconstructed images resulting from plain First Fit and GKM.
Reference: [16] <author> Qin Zhang and John M. Danskin, </author> <title> "Bitmap Reconstruction for Document Image Compression," to appear in SPIE's International Symposium on Voice, Video, </title> <journal> and Data Communications, </journal> <month> 18-22 November </month> <year> 1996, </year> <institution> Boston, </institution> <address> MA. </address> <month> 10 </month>
Reference-contexts: So the final partition is ffa; bg; fcg; fdgg. If we change the order of the list to fb; a; c; dg, however, the algorithm finds the partition ffb; a; c; dgg. Several compression systems, including MGTIC [13] and CDIS <ref> [16] </ref>, combine a bitmap averaging method with the First-Fit algorithm to form a multi-pass pattern classification method. For each equivalence class, all of the bitmaps in the class are averaged and the result is thresholded to obtain a new pattern.
References-found: 16

