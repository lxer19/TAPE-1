URL: http://mnemosyne.itc.it:1024/ricci/papers/TR9701-08.ps.Z
Refering-URL: http://mnemosyne.itc.it:1024/ricci/tech-reports-list.html
Root-URL: 
Email: Email: ricci@irst.itc.it, aha@aic.nrl.navy.mil  
Phone: Phone: 202 404-4940 FAX: 202 767-3172  
Title: Extending Local Learners with Error-Correcting Output Codes  
Author: Francesco Ricci David W. Aha 
Keyword: Case-based learning,  
Note: classification, error-correcting output codes  Submitted to ICML-97; not submitted elsewhere  
Address: 38050 Povo (TN), Italy  Code 5510 Washington, DC 20375 USA  
Affiliation: Istituto per la Ricerca Scientifica e Tecnologica  Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: Error-correcting output codes (ECOCs) represent classes with a set of output bits, where each bit encodes a binary classification task corresponding to a unique partition of the classes. Algorithms that use ECOCs learn the function corresponding to each bit, and combine them to generate class predictions. ECOCs can reduce both variance and bias errors for mul-ticlass classification tasks when the errors made at the output bits are not correlated. They work well with global (e.g., C4.5) but not with local (e.g., nearest neighbor) classifiers because the latter use the same information to predict each bit's value, which yields correlated errors. This is distressing because local learners are excellent classifiers for some types of applications. We show that the output bit errors of local learners can be decorrelated by selecting different features for each bit. This yields bit-specific distance functions, which causes different information to be used for each bit's prediction. We present promising empirical results for this combination of ECOCs, nearest neighbor, and feature selection. We also describe modifications to racing algorithms for feature selection that improve their performance in this context. 
Abstract-found: 1
Intro-found: 1
Reference: [ Aha and Bankert, 1996 ] <author> D. W. Aha and R. L. Bankert. </author> <title> Cloud classification using error-correcting output codes. </title> <type> Technical Report AIC-96-024, </type> <institution> Naval Research Laboratory, Navy Center for Applied Research in Artificial Intelligence, </institution> <address> Washington, DC, </address> <year> 1996. </year>
Reference: [ Aha, 1992 ] <author> D. W. Aha. </author> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36 </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: Two types of distributed output representations are one-per-class and error-correcting. In one-per-class each bit separates one class from the remaining classes. Learning algorithms that use one-per-class encodings induce a separate concept description per class <ref> [ Quinlan, 1993; Aha, 1992 ] </ref> , where positive instances of a class c i are negative instances for all other classes c j (i6=j). They then classify a query according to the class with the highest confidence. <p> We argue that, in tasks where feature selection increases 1-NN's accuracy (i.e., by varying it decision boundary hypotheses per bit), ECOCs can yield further increases. 2 Local Learning with ECOCs ECOCs require that the errors for each of the output bits be uncorrelated. Therefore, we extended IB1 <ref> [ Aha, 1992 ] </ref> , an implementation of 1-NN, to use different features when computing distances for each output bit. Figure 2 summarizes this extension, named IB1-ECOC, and its evaluation.
Reference: [ Bottou and Vapnik, 1992 ] <author> Leon Bottou and Vladimir Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference-contexts: However, they cannot greatly benefit local learning algorithms <ref> [ Kong and Dietterich, 1995; Bottou and Vapnik, 1992 ] </ref> , whose classification boundaries, for a given query q, are based only on the information in samples local to q.
Reference: [ Dietterich and Bakiri, 1995 ] <author> T. G. Dietterich and G. Bakiri. </author> <title> Solving multi-class learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction ECOCs can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms <ref> [ Dietterich and Bakiri, 1995 ] </ref> (e.g., C4.5 [ Quinlan, 1993 ] , backpropagation [ Rumelhart et al., 1986 ] ), which induce classification boundaries that are heuristically determined using all training examples rather than only those nearby the boundaries. <p> Algorithms for generating codewords should maximize both row and column separation. For classification tasks where m7, we used the exhaustive codes technique <ref> [ Dietterich and Bakiri, 1995 ] </ref> . It creates all 2 m1 1 possible codewords that are both column and row separated. The resulting codewords have Hamming distance h = 2 m2 .
Reference: [ Geman et al., 1992 ] <author> Stuart Geman, Alie Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: Thus, our contributions are useful for multiclass classification tasks where (1) feature selection is needed and (2) obtaining high predictive accuracy is a priority. The reason why IB1-ECOC reduces IB1's expected error is linked to the bias/variance dilemma <ref> [ Geman et al., 1992 ] </ref> . 1-NN is known to have small bias and high variance. k-NN decreases variance and introduces bias through its voting process. In general, a classifier's accuracy can be increased by introducing the level of bias that matches the task.
Reference: [ John et al., 1994 ] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <address> Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 15 </pages>
Reference-contexts: Previous research on ECOCs did not stress feature selection, which is crucial for some tasks. Nearest neighbor classifiers are excellent choices for evaluating feature subsets (i.e., their training costs are nil) in FSAs that guide search using classifier feedback <ref> [ John et al., 1994 ] </ref> , which tend to outperform other FSAs. However, these algorithms evaluate many feature subsets, which means that the selected classifier should have low training costs.
Reference: [ Kong and Dietterich, 1995 ] <author> E. B. Kong and T. G Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 313-321, </pages> <address> Tahoe City, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, they cannot greatly benefit local learning algorithms <ref> [ Kong and Dietterich, 1995; Bottou and Vapnik, 1992 ] </ref> , whose classification boundaries, for a given query q, are based only on the information in samples local to q.
Reference: [ Maron and Moore, 1997 ] <author> O. Maron and A. W. Moore. </author> <title> The racing algorithm: model selection for lazy learners. </title> <journal> Artificial Intelligence review, </journal> <note> 1997. </note> [ <author> Merz and Murphy, 1996 ] C. Merz and P. M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> [http://www.ics.uci.edu/~mlearn/MLRepository.html], 1996. </note>
Reference-contexts: This in turn causes a different nearest neighbor to be selected for each bit's prediction, which decorrelates the output bit errors. We also describe a modification of the schemata racing FSA <ref> [ Maron and Moore, 1997 ] </ref> that is more appropriate for this context. <p> does not always increase total separation, but it helps to escape from local maximum by exchanging row with column separation (or vice versa). 2.2 Selecting features Step 4 of IB1-ECOC calls race-schemata () (Figure 3), which returns the subset of features selected by a variant of the schemata racing algorithm <ref> [ Maron and Moore, 1997 ] </ref> . It searches over the space of schemata strings of length l, the number of features, whose characters are 0's (feature is not selected), 1's (selected) and ?'s (selected with probability 50%). <p> If e 1 e 0 + &gt; z ff , then the winner is B 0 . where is a small positive constant used to stop a race between equally good racers <ref> [ Maron and Moore, 1997 ] </ref> , and z ff is the usual statistic of a normal gaussian variable. <p> Mixed is essentially the original version described in <ref> [ Maron and Moore, 1997 ] </ref> , differing only in the use of a gaussian rather than a t statistic, which is not appropriate here because the error has a binomial distribution.
Reference: [ Perrone and Cooper, 1993 ] <author> M. P. Perrone and L. N. Cooper. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In R. J. Mammone, editor, </editor> <title> Neural Networks for Speech and Image Processing. </title> <publisher> Chapman and Hall, </publisher> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: Variance results from random variation and noise in the training set and from any random behaviors of the learning algorithm itself. It can be reduced by averaging the contributions of multiple predictions <ref> [ Perrone and Cooper, 1993 ] </ref> . ECOCs reduce variance through a voting process: because Hamming distance determines the "winning" prediction (i.e., closest codeword), each output bit prediction corresponds to a vote for classes whose codewords match the predicted value. Bias errors instead refer to an algorithm's systematic errors.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction ECOCs can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms [ Dietterich and Bakiri, 1995 ] (e.g., C4.5 <ref> [ Quinlan, 1993 ] </ref> , backpropagation [ Rumelhart et al., 1986 ] ), which induce classification boundaries that are heuristically determined using all training examples rather than only those nearby the boundaries. <p> The standard monolithic strategy uses a unique monolithic encoding for each class label. Learning algorithms that use this approach (e.g., C4.5 <ref> [ Quinlan, 1993 ] </ref> ) induce a single concept description that distinguishes all class boundaries. Monolithic class labels are not related by a continuous distance measure. This limitation distinguishes them from distributed output representations. <p> Two types of distributed output representations are one-per-class and error-correcting. In one-per-class each bit separates one class from the remaining classes. Learning algorithms that use one-per-class encodings induce a separate concept description per class <ref> [ Quinlan, 1993; Aha, 1992 ] </ref> , where positive instances of a class c i are negative instances for all other classes c j (i6=j). They then classify a query according to the class with the highest confidence. <p> This ensures that the errors of the output bit predictions are uncorrelated. They reported that ECOCs often significantly increased the classification accuracies for C4.5 <ref> [ Quinlan, 1993 ] </ref> and networks trained by backpropagation [ Rumelhart et al., 1986 ] , although training ECOCs is slow because they must learn a concept for each bit.
Reference: [ Rumelhart et al., 1986 ] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction ECOCs can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms [ Dietterich and Bakiri, 1995 ] (e.g., C4.5 [ Quinlan, 1993 ] , backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> ), which induce classification boundaries that are heuristically determined using all training examples rather than only those nearby the boundaries. <p> This ensures that the errors of the output bit predictions are uncorrelated. They reported that ECOCs often significantly increased the classification accuracies for C4.5 [ Quinlan, 1993 ] and networks trained by backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> , although training ECOCs is slow because they must learn a concept for each bit.
Reference: [ Stanfill and Waltz, 1986 ] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communication of ACM, </journal> <volume> 29 </volume> <pages> 1213-1229, </pages> <year> 1986. </year>
Reference-contexts: We avoided data sets with symbolic features because they often require distinct weighting metrics (e.g., <ref> [ Stanfill and Waltz, 1986 ] </ref> ), which complicates isolating the effects of feature selection. Data sets with fewer than four classes do not greatly benefit from ECOCs. We also used three additional proprietary data sets concerning cloud classification. We will use abbreviations for the data set names.
Reference: [ Wettschereck and Dietterich, 1992 ] <author> D. Wettschereck and T. G. Dietterich. </author> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference: [ Zhang et al., 1992 ] <author> X. Zhang, J. Mesirov, and D. Waltz. </author> <title> Hybrid system for protein structure prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225 </volume> <pages> 1049-1063, </pages> <year> 1992. </year>
Reference-contexts: Bias errors instead refer to an algorithm's systematic errors. These can also be reduced by voting, but only when the individual predictions are uncorrelated, such as by averaging the contributions of different prediction algorithms <ref> [ Zhang et al., 1992 ] </ref> . Alternatively, the same algorithm can be used multiple times, but it must vote on different subproblems (i.e., using different class decision boundaries) that cause the algorithm to generate different bias errors.
References-found: 14

