URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn15.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Parallel Performance of a Symmetric Eigensolver based on the Invariant Subspace Decomposition Approach  
Author: Christian Bischof Steven Huss-Lederman Xiaobai Sun Anna Tsao Thomas Turnbull 
Address: Argonne, IL 60439  MD 20715  
Affiliation: Math. Comp. Sci. Div., Argonne National Laboratory,  Supercomputing Research Center, Bowie,  
Abstract: In this paper, we discuss work in progress on a complete eigensolver based on the Invariant Subspace Decomposition Algorithm for dense symmetric matrices (SYISDA). We describe a recently developed acceleration technique that substantially reduces the overall work required by this algorithm and review the algorithmic highlights of a distributed-memory implementation of this approach. These include a fast matrix-matrix multiplication algorithm, a new approach to parallel band reduction and tridiagonalization, and a harness for coordinating the divide-and-conquer parallelism in the problem. We present performance results for the dominant kernel, dense matrix multiplication, as well as for the overall SYISDA implementation on the Intel Touchstone Delta and the Intel Paragon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigen-solvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261. </pages>
Reference-contexts: The goal of the PRISM project is the development of algorithms and software for solving large-scale eigenvalue problems based on the invariant subspace decomposition approach (ISDA) suggested by Auslander and Tsao <ref> [1] </ref>. The symmetric invariant subspace decomposition algorithm (SYISDA) for an n fi n symmetric matrix A proceeds as follows. fl This paper is PRISM Working Note #15, available via anonymous ftp to ftp.super.org in the directory pub/prism. <p> Typeset by A M S-T E X Scaling: Compute upper and lower bounds on the spectrum (A) of A and compute ff and fi such that for B = ffA + fiI we have (B) <ref> [0; 1] </ref>, with the mean eigenvalue of A being mapped to 1 2 . Eigenvalue Smoothing: Let p i (x), i = 1; 2; : : : be polynomials such that lim i!1 p i ([0; 1]) = f0; 1g, that is, in the limit all values are mapped to either <p> take the matrix B and "stretch" it so that its eigenvalues now lie over some interval, say [ff; 1 + ff], where 0 &lt; ff ae 1, then the eigenvalues of B near 1=2 are moved away from 1=2 and B 1 will still map the eigenvalues of B into <ref> [0; 1] </ref>. By "stretching", we mean to apply a linear function 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 that maps 0 and 1 to 1 ff and ff, respectively, leaving 1=2 fixed.
Reference: [2] <author> Bai, Z. & J. Demmel, </author> <title> Design of parallel nonsymmetric eigenroutine toolbox, Part I, </title> <type> Research report 92-09, </type> <institution> University of Kentucky (Dec. </institution> <year> 1992), </year> <note> (also PRISM Working Note #5). </note>
Reference-contexts: In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. We note that other functional iterations, such as approximation methods for the matrix sign function <ref> [14, 15, 21, 2] </ref>, can be used in the Eigenvalue Smoothing step as well. The two key primitives of the algorithm are matrix-matrix multiplication, which accounts for the majority of the computation, and computation of the range and null space of a matrix having eigenvalues clustered around 0 and 1.
Reference: [3] <author> Berry, M. & A. Sameh, </author> <title> Parallel algorithms for the singular value and dense symmetric eigenvalue problem, </title> <journal> CSRD (1988), </journal> <volume> no. </volume> <pages> 761. </pages>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky.
Reference: [4] <author> Bischof, C., </author> <title> A parallel QR factorization with controlled local pivoting, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), </volume> <pages> 36-57. </pages>
Reference-contexts: This property has been demonstrated in extensive testing [6, 11], particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations <ref> [4, 5, 12, 30] </ref>, tridiagonalization does not involve any data-dependent pivoting strategies. In particular, in the parallel setting, the predictability of data flow greatly contributes to simplicity of implementation as well as to the ability to overlap communication and computation.
Reference: [5] <author> Bischof, C. H. & P. C. Hansen, </author> <title> A block algorithm for computing rank-revealing QR factorizations, also MCS-P251-0791, </title> <booktitle> Numerical Algorithms 2 (1992), </booktitle> <volume> no. </volume> <pages> 3-4, 371-392. </pages>
Reference-contexts: This property has been demonstrated in extensive testing [6, 11], particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations <ref> [4, 5, 12, 30] </ref>, tridiagonalization does not involve any data-dependent pivoting strategies. In particular, in the parallel setting, the predictability of data flow greatly contributes to simplicity of implementation as well as to the ability to overlap communication and computation.
Reference: [6] <author> Bischof, C. H., S. Huss-Lederman, X. Sun, & A. Tsao, </author> <title> The PRISM Project: Infrastructure and Algorithms for Parallel Eigensolvers, </title> <booktitle> Proceedings, Scalable Parallel Libraries Conference (Starksville, </booktitle> <address> MS, </address> <month> Oct. </month> <pages> 6-8, </pages> <year> 1993), </year> <note> IEEE, 1993, (also PRISM Working Note #12). </note>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky. <p> Orthogonality of the eigenvectors is guaranteed due to the exclusive use of orthogonal transformations. We have considerable freedom in implementing SYISDA, particularly with respect to choosing the polynomials p i and the method for computing the in-variant subspaces. As in <ref> [27, 25, 6] </ref>, we use predominantly the first incomplete Beta function B 1 (x) = 3x 2 2x 3 in our implementation. In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. <p> The sequential complexity of SYISDA, when applied to dense matrices, is considerably greater than that of other algorithms. Nonetheless, the algorithm is promising from both a scalability and a numerical point of view <ref> [27, 6] </ref>. First, since most of the computation is in matrix multiplication, high efficiencies and near optimal speedups are achieved on large problems. Second, since the algorithm performs only orthogonal transformations, orthogonality in the computed eigenvectors is guaranteed. In the next section, we briefly describe the parallel algorithm. <p> This traditional approach also underlies the parallel implementations described in [20] and [17]. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in <ref> [6] </ref> and in another paper at this conference [10]. <p> We are able to skip large numbers of the orthogonal transformations, since the block diagonality of the matrices generated results in many transformations that would act on columns that are already negligible and hence need not be performed. This property has been demonstrated in extensive testing <ref> [6, 11] </ref>, particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations [4, 5, 12, 30], tridiagonalization does not involve any data-dependent pivoting strategies. <p> We have developed a sequential code for performing both SBR and RRTRID that incorporates some of the blocking schemes discussed in our previous papers, with promising results <ref> [6, 11] </ref>. However, the parallel implementation of RRTRID currently only performs unblocked operations. Nonetheless, the skipping alluded to previously still leads to good performance. A second algorithmic change was necessitated by the use of the rank-revealing tridiagonalization scheme of Bischof and Sun. <p> 150 200 250 0 2 4 6 8 number of processors Gflops local dim=100 local dim=200 local dim=300 local dim=450 Delta 0 50 100 150 200 250 0 2 4 6 8 10 number of processors Gflops local dim=100 local dim=200 local dim=450 Paragon rently consists of two separate stages <ref> [25, 6] </ref>. The first stage encompasses the early divides, where large sub-problems are solved sequentially and the scalability of the dense matrix multiplication and rank-revealing tridiagonalization lead to high efficiencies. <p> Parallel performance of eigensolver We are currently studying both the numerical performance and scaling properties of our algorithm. Since our code currently uses only the unblocked versions of SBR, we expect our performance to continue to improve. Preliminary results indicate that SYISDA provides excellent scaling and accuracy <ref> [6] </ref>. Our code can currently accommodate matrices of local dimension up to about 450.
Reference: [7] <author> Bischof, C., M. Marques, & X. Sun, </author> <title> Parallel ban-dreduction and tridiagonalization, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (also PRISM Working Note #8). </note>
Reference-contexts: In comparison, conventional Householder tridiagonalization approaches [19] or block variants thereof [16] elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in [20] and [17]. The SBR variant used in our implementation is discussed in detail in <ref> [7] </ref>; performance results are given in [6] and in another paper at this conference [10].
Reference: [8] <author> Bischof, C. & X. Sun, </author> <title> A divide-and-conquer method for tridiagonalizing symmetric matrices with repeated eigenvalues, </title> <type> Preprint MCS-P286-0192, </type> <institution> Argonne National Laboratory (1992), </institution> <note> (also PRISM Working Note #1). </note>
Reference-contexts: This property has been demonstrated in extensive testing [6, 11], particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in <ref> [8] </ref>. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations [4, 5, 12, 30], tridiagonalization does not involve any data-dependent pivoting strategies.
Reference: [9] <author> Bischof, C. & X. Sun, </author> <title> A framework for symmetric band reduction and tridiagonalization, </title> <type> Preprint MCS-P298-0392, </type> <institution> Argonne National Laboratory (1992), </institution> <note> (also PRISM Working Note #3). </note>
Reference-contexts: In order to reduce the given matrix to tridiagonal form, we employ a variant of the successive band reduction (SBR) framework suggested by Bischof and Sun in <ref> [9] </ref>, which eliminates subdiagonals of C k in a piecemeal fashion as illustrated in Figure 1. In comparison, conventional Householder tridiagonalization approaches [19] or block variants thereof [16] elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in [20] and [17].
Reference: [10] <author> Bischof, C. H. & X. Sun, </author> <title> Parallel tridiagonalization through two-step band reduction, </title> <booktitle> Proceedings: Scalable High Performance Computing Conference '94, </booktitle> <address> Knoxville, Tennessee, May 1994, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994, </year> <note> (also PRISM Working Note #17) (to appear). </note>
Reference-contexts: This traditional approach also underlies the parallel implementations described in [20] and [17]. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in [6] and in another paper at this conference <ref> [10] </ref>. An exciting new development discussed in [10] is the ability to fully block all stages of the succession of band reductions. sequence of band reductions The key observation in the context of SYISDA is that, under some very general conditions, a banded matrix having only two distinct eigenvalues and bandwidth <p> This traditional approach also underlies the parallel implementations described in [20] and [17]. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in [6] and in another paper at this conference <ref> [10] </ref>. An exciting new development discussed in [10] is the ability to fully block all stages of the succession of band reductions. sequence of band reductions The key observation in the context of SYISDA is that, under some very general conditions, a banded matrix having only two distinct eigenvalues and bandwidth n=2 j must be block diagonal, with
Reference: [11] <author> Bischof, C., X. Sun, A. Tsao, & T. Turnbull, </author> <title> A study of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices, </title> <booktitle> Proceedings: Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <address> Snow-bird, UT, </address> <month> June, </month> <year> 1994, </year> <note> SIAM, 1994, (also PRISM Working Note #16) (to appear). </note>
Reference-contexts: We are able to skip large numbers of the orthogonal transformations, since the block diagonality of the matrices generated results in many transformations that would act on columns that are already negligible and hence need not be performed. This property has been demonstrated in extensive testing <ref> [6, 11] </ref>, particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations [4, 5, 12, 30], tridiagonalization does not involve any data-dependent pivoting strategies. <p> We have developed a sequential code for performing both SBR and RRTRID that incorporates some of the blocking schemes discussed in our previous papers, with promising results <ref> [6, 11] </ref>. However, the parallel implementation of RRTRID currently only performs unblocked operations. Nonetheless, the skipping alluded to previously still leads to good performance. A second algorithmic change was necessitated by the use of the rank-revealing tridiagonalization scheme of Bischof and Sun. <p> One potential method of achieving this is to use a Strassen-type algorithm [31, 13] to perform the dense matrix multiplications. Another particularly promising algorithm, banded SYISDA <ref> [11] </ref>, uses SBR to reduce A to a narrow band and then periodically reduces matrices in the Eigenvalue Smoothing step to a narrow band. Multiplication of two matrices of bandwidths b only requires O (b 2 n) work versus O (n 3 ) for two dense matrices.
Reference: [12] <author> Chandrasekaran, S. & I. Ipsen, </author> <title> On rank-revealing QR factorizations, </title> <type> Technical Report YALEU/DCS/RR-880, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: This property has been demonstrated in extensive testing [6, 11], particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations <ref> [4, 5, 12, 30] </ref>, tridiagonalization does not involve any data-dependent pivoting strategies. In particular, in the parallel setting, the predictability of data flow greatly contributes to simplicity of implementation as well as to the ability to overlap communication and computation.
Reference: [13] <author> Coppersmith, D. and S. Winograd, </author> <title> Matrix multiplication via arithmetric progressions, </title> <booktitle> Proceeding of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 1-6. </pages>
Reference-contexts: One potential method of achieving this is to use a Strassen-type algorithm <ref> [31, 13] </ref> to perform the dense matrix multiplications. Another particularly promising algorithm, banded SYISDA [11], uses SBR to reduce A to a narrow band and then periodically reduces matrices in the Eigenvalue Smoothing step to a narrow band.
Reference: [14] <author> Denman, E. D. & A. N. Beavers, Jr., </author> <title> The matrix sign function and computations in systems, </title> <journal> Appl. Math. Comp. </journal> <volume> 2 (1976), </volume> <pages> 63-94. </pages>
Reference-contexts: In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. We note that other functional iterations, such as approximation methods for the matrix sign function <ref> [14, 15, 21, 2] </ref>, can be used in the Eigenvalue Smoothing step as well. The two key primitives of the algorithm are matrix-matrix multiplication, which accounts for the majority of the computation, and computation of the range and null space of a matrix having eigenvalues clustered around 0 and 1.
Reference: [15] <author> Denman, E. D. & J. Leyva-Ramos, </author> <title> Spectral decomposition of a matrix using the generalized sign matrix, </title> <journal> Appl. Math. Comp. </journal> <volume> 8 (1981), </volume> <pages> 237-50. </pages>
Reference-contexts: In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. We note that other functional iterations, such as approximation methods for the matrix sign function <ref> [14, 15, 21, 2] </ref>, can be used in the Eigenvalue Smoothing step as well. The two key primitives of the algorithm are matrix-matrix multiplication, which accounts for the majority of the computation, and computation of the range and null space of a matrix having eigenvalues clustered around 0 and 1.
Reference: [16] <author> Dongarra, Jack J., Sven J. Hammarling and Danny C. Sorensen, </author> <title> Block reduction of matrices to condensed form for eigenvalue computations, </title> <journal> MCS-TM-99, </journal> <note> Ar-gonne National Laboratory (September 1987). </note>
Reference-contexts: In comparison, conventional Householder tridiagonalization approaches [19] or block variants thereof <ref> [16] </ref> elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in [20] and [17]. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in [6] and in another paper at this conference [10].
Reference: [17] <author> Dongarra, J. & R. van de Geijn, </author> <title> Reduction to condensed form for the eigenvalue problem on distributed-memory architectures, </title> <type> Technical Report ORNL/TM-12006, </type> <institution> Oak Ridge National Laboratory, Engineering Physics and Mathematics Division (January 1992). </institution>
Reference-contexts: In comparison, conventional Householder tridiagonalization approaches [19] or block variants thereof [16] elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in [20] and <ref> [17] </ref>. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in [6] and in another paper at this conference [10].
Reference: [18] <author> Fox, G., M. Johnson, G. Lyzenga, S. Otto, J. Salmon, & D. Walker, </author> <title> Solving Problems on Concurrent Processors, Vol. I, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: To this end, we designed BiM-MeR to exploit the availability of highly optimized assembly-coded implementations of matrix multiplication on single nodes and to ensure maximal granularity in local computations. The Broadcast-Multiply-Roll (BMR) algorithm <ref> [18] </ref> has been demonstrated to scale extremely well on loosely coupled square processor meshes and uses two readily portable communication kernels: one-dimensional broadcast and roll. Since many distributed-memory machines do not currently overlap communication and computation effectively, BiMMeR uses a variant of BMR that is highly synchronous [24, 23].
Reference: [19] <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: In order to reduce the given matrix to tridiagonal form, we employ a variant of the successive band reduction (SBR) framework suggested by Bischof and Sun in [9], which eliminates subdiagonals of C k in a piecemeal fashion as illustrated in Figure 1. In comparison, conventional Householder tridiagonalization approaches <ref> [19] </ref> or block variants thereof [16] elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in [20] and [17]. <p> In fact, in the early divides, the number of applications of B 1 required tends to be larger than in later stages. One reason for this is that when no a priori spectral information is available, Scaling is done using bounds obtained from Gershgorin disks <ref> [19] </ref>. Since these bounds are generally quite poor, B from the Scaling step tends to have eigenvalues closer to 1=2 than would be the case if better bounds on the spectrum were available, as is the case in later divides.
Reference: [20] <author> Hendrikson, B. & D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <institution> SAND92-0792, Sandia National Laboratories (1992). </institution>
Reference-contexts: In comparison, conventional Householder tridiagonalization approaches [19] or block variants thereof [16] elimi nate all subdiagonals at one time. This traditional approach also underlies the parallel implementations described in <ref> [20] </ref> and [17]. The SBR variant used in our implementation is discussed in detail in [7]; performance results are given in [6] and in another paper at this conference [10].
Reference: [21] <author> Howland, J. L., </author> <title> The sign matrix and the separation of matrix eigenvalues, </title> <journal> Lin. Alg. Appl. </journal> <volume> 49 (1983), </volume> <pages> 221-32. </pages>
Reference-contexts: In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. We note that other functional iterations, such as approximation methods for the matrix sign function <ref> [14, 15, 21, 2] </ref>, can be used in the Eigenvalue Smoothing step as well. The two key primitives of the algorithm are matrix-matrix multiplication, which accounts for the majority of the computation, and computation of the range and null space of a matrix having eigenvalues clustered around 0 and 1.
Reference: [22] <author> Huo, Y. & R. Schreiber, </author> <title> Efficient, massively parallel eigenvalue computation, </title> <type> RIACS Technical Report 93.02, </type> <institution> Research Institute for Advanced Computer Science (1993). </institution>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky.
Reference: [23] <author> Huss-Lederman, S., E. M. Jacobson, & A. Tsao, </author> <title> Comparison of Scalable Parallel Matrix Multiply Libraries, </title> <booktitle> Proceedings, Scalable Parallel Libraries Conference (Starksville, </booktitle> <address> MS, </address> <month> Oct. </month> <pages> 6-8, </pages> <year> 1993), </year> <note> IEEE, 1993, (also PRISM Working Note #13, also appears as Technical Report SRC-TR-93-108, </note> <institution> Supercomputing Research Center, </institution> <year> 1993). </year>
Reference-contexts: Since many distributed-memory machines do not currently overlap communication and computation effectively, BiMMeR uses a variant of BMR that is highly synchronous <ref> [24, 23] </ref>.
Reference: [24] <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Matrix Multiplication on the Intel Touchstone Delta, </title> <type> Technical Report SRC-TR-93-101, </type> <institution> Supercomputing Research Center (1993), </institution> <note> (also PRISM Working Note #14). </note>
Reference-contexts: Since many distributed-memory machines do not currently overlap communication and computation effectively, BiMMeR uses a variant of BMR that is highly synchronous <ref> [24, 23] </ref>. <p> Our code has also achieved a parallel efficiency of 86%, with overall peak performance in excess of 8 Gflops on 256 nodes of the Delta for an 8800 fi 8800 double precision matrix <ref> [24] </ref>. Note that BiMMeR has achieved a parallel efficiency of about 80%, with peak performance thus far of about 9:5 Gflops on 256 nodes of the Paragon for a 7200 fi7200 double precision matrix. We expect performance on the Paragon to improve as the machine matures. 3.
Reference: [25] <author> Huss-Lederman, S., A. Tsao, & G. Zhang, </author> <title> A Parallel Implementation of the Invariant Subspace Decomposition Algorithm for Dense Symmetric Matrices, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (also PRISM Working Note #9). </note>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky. <p> Orthogonality of the eigenvectors is guaranteed due to the exclusive use of orthogonal transformations. We have considerable freedom in implementing SYISDA, particularly with respect to choosing the polynomials p i and the method for computing the in-variant subspaces. As in <ref> [27, 25, 6] </ref>, we use predominantly the first incomplete Beta function B 1 (x) = 3x 2 2x 3 in our implementation. In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. <p> 150 200 250 0 2 4 6 8 number of processors Gflops local dim=100 local dim=200 local dim=300 local dim=450 Delta 0 50 100 150 200 250 0 2 4 6 8 10 number of processors Gflops local dim=100 local dim=200 local dim=450 Paragon rently consists of two separate stages <ref> [25, 6] </ref>. The first stage encompasses the early divides, where large sub-problems are solved sequentially and the scalability of the dense matrix multiplication and rank-revealing tridiagonalization lead to high efficiencies. <p> However, this approach guarantees near-perfect load balancing in the costly early stages. A discussion of the rationale used to arrive at this scheme is presented in <ref> [25] </ref>. When the subproblems become small (dimension of a few hundred), they are currently solved using DSYEV (QR algorithm) from LAPACK. Since the computational cost for SYISDA is dominated by dense matrix-matrix multiplication, its performance depends heavily on the matrix multiplication code.
Reference: [26] <author> Ipsen, I. & E. Jessup, </author> <title> Solving the symmetric tridiag-onal eigenvalue problem on the hypercube, </title> <type> Tech. Rep. </type> <institution> RR-548, Yale University (1987). </institution>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky.
Reference: [27] <author> Huss-Lederman, S., A. Tsao, & T. Turnbull, </author> <title> A par-allelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center (1991). </institution>
Reference-contexts: Orthogonality of the eigenvectors is guaranteed due to the exclusive use of orthogonal transformations. We have considerable freedom in implementing SYISDA, particularly with respect to choosing the polynomials p i and the method for computing the in-variant subspaces. As in <ref> [27, 25, 6] </ref>, we use predominantly the first incomplete Beta function B 1 (x) = 3x 2 2x 3 in our implementation. In addition, in Section 2, we describe an acceleration technique we have recently developed that substantially reduces the number of iterations required in the Eigenvalue Smoothing step. <p> The sequential complexity of SYISDA, when applied to dense matrices, is considerably greater than that of other algorithms. Nonetheless, the algorithm is promising from both a scalability and a numerical point of view <ref> [27, 6] </ref>. First, since most of the computation is in matrix multiplication, high efficiencies and near optimal speedups are achieved on large problems. Second, since the algorithm performs only orthogonal transformations, orthogonality in the computed eigenvectors is guaranteed. In the next section, we briefly describe the parallel algorithm. <p> Nonetheless, the skipping alluded to previously still leads to good performance. A second algorithmic change was necessitated by the use of the rank-revealing tridiagonalization scheme of Bischof and Sun. The rank-revealing tridiagonal--ization scheme requires that the rank-deficient matrix, C k , have only two distinct eigenvalues. As in <ref> [27] </ref>, we began by using first incomplete Beta function, B 1 (x) = 3x 2 2x 3 , exclusively in the Eigenvalue Smoothing step. However, because B 1 has a fixed point at 1=2, C k occasionally has eigenvalues at 1=2.
Reference: [28] <author> Li, T.-Y., H. Zhang, & X.-H. Sun, </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 3, </volume> <pages> 469-87. </pages>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [26, 3, 28, 22, 25, 6] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of Cal-ifornia at Berkeley, and the University of Kentucky.
Reference: [29] <author> Pan, V. & R. Schreiber, </author> <title> An improved Newton iteration for the generalized inverse of a matrix, with applications, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 5, </volume> <pages> 1109-1129. </pages>
Reference-contexts: B 1 takes on the value 1=2 three times: at 1=2, ae, and 1 ae, where ae = (1 + p 3)=2 1:366. We propose the following scheme, which is a slight modification of a technique suggested by Pan and Schreiber <ref> [29] </ref>.
Reference: [30] <author> G. W. Stewart, </author> <title> Updating a rank-revealing ULV decomposition, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 14 (1993), no. </volume> <pages> 2. </pages>
Reference-contexts: This property has been demonstrated in extensive testing [6, 11], particularly of the sequential algorithm. Algorithmic details of RRTRID, as well as some of the subtle numerical issues are discussed in [8]. It is important to realize that, unlike other approaches for computing so-called rank-revealing factorizations <ref> [4, 5, 12, 30] </ref>, tridiagonalization does not involve any data-dependent pivoting strategies. In particular, in the parallel setting, the predictability of data flow greatly contributes to simplicity of implementation as well as to the ability to overlap communication and computation.
Reference: [31] <author> Strassen, V., </author> <title> Gaussian elimination is not optimal, </title> <journal> Numer. Math. </journal> <volume> 13 (1969), </volume> <pages> 354-6. </pages>
Reference-contexts: One potential method of achieving this is to use a Strassen-type algorithm <ref> [31, 13] </ref> to perform the dense matrix multiplications. Another particularly promising algorithm, banded SYISDA [11], uses SBR to reduce A to a narrow band and then periodically reduces matrices in the Eigenvalue Smoothing step to a narrow band.
Reference: [32] <author> Tsao, A. & T. Turnbull, </author> <title> A comparison of algorithms for banded matrix multiplication, </title> <type> Technical Report SRC-TR-093-092, </type> <institution> Supercomputing Research Center (1993), </institution> <note> (also PRISM Working Note #6). </note>
Reference-contexts: Furthermore, the special properties of the iterates in the Eigenvalue Smoothing step result in surprisingly slow band growth in the iterated matrices. In our sequential implementation, using specialized routines for multiplying symmetric band matrices <ref> [32] </ref>, the run times for banded SYISDA are competitive with the symmetric QR algorithm for large problems. In fact, the time spent in SBR becomes the dominant time. Banded SYISDA uses essentially the same harness as SYISDA and the two computational kernels of SBR and banded matrix multiplication.
References-found: 32

