URL: ftp://ftp.idsia.ch/pub/juergen/colin.saturation.ps.gz
Refering-URL: http://www.idsia.ch/reports.html
Root-URL: 
Title: Learning Beyond Saturation  
Author: C. Campbell 
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: Constructive algorithms typically use a single-layered learning algorithm to construct a multi-layered neural network. In many cases this algorithm is the pocket algorithm or perceptron rule and at each step the task is to store all the patterns or maximise the number of stored patterns and grow further hidden nodes if this is not possible. The perceptron rule may not be able to store all the patterns because the pattern set is not linearly separable or because the network is being trained beyond saturation. When trained beyond saturation the storage capacity of the neural network has been exceeded and only a fraction of the patterns can be stored correctly. In this paper we outline recent analytical work to establish the maximal fraction of patterns which can be stored correctly when training beyond saturation. This gives an indication of the optimal architectures (minimal number of hidden nodes) generated by these constructive algorithms. Particular emphasis is given to constructive algorithms for binary classification developed by the author and collaborators in which the pocket algorithm is used to store all the patterns of one class and as many as possible of the opposite class.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Fahlman and C. Libiere, </author> <title> "The Cascade Correlation Architecture", </title> <editor> in D. Touretzky (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2 (Morgan Kaufman, </booktitle> <address> San Mateo, CA) p. </address> <pages> 524-532. </pages>
Reference-contexts: 1 Introduction A number of constructive algorithms have been proposed recently to generate the weights and architectures of feed-forward neural networks. Examples are the Cascade Correlation algorithm <ref> [1] </ref>, the Upstart algorithm [2], the Tiling algorithm [3], and algorithms for generating a single hidden layer [4] or two hidden layers [5].
Reference: [2] <author> M. Frean, </author> <note> Neural Computation 2 (1990) 1403-1413. </note>
Reference-contexts: 1 Introduction A number of constructive algorithms have been proposed recently to generate the weights and architectures of feed-forward neural networks. Examples are the Cascade Correlation algorithm [1], the Upstart algorithm <ref> [2] </ref>, the Tiling algorithm [3], and algorithms for generating a single hidden layer [4] or two hidden layers [5]. The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer [7, 8, 9].
Reference: [3] <author> M. Mezard and J.-P. Nadal, J. Phys. </author> <month> A22 </month> <year> (1989) </year> <month> 2191-2203. </month>
Reference-contexts: 1 Introduction A number of constructive algorithms have been proposed recently to generate the weights and architectures of feed-forward neural networks. Examples are the Cascade Correlation algorithm [1], the Upstart algorithm [2], the Tiling algorithm <ref> [3] </ref>, and algorithms for generating a single hidden layer [4] or two hidden layers [5]. The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer [7, 8, 9].
Reference: [4] <author> M. Marchand, M. Golea and P. Rujan, </author> <note> Europhysics Letters 11(1990)487-492. </note>
Reference-contexts: 1 Introduction A number of constructive algorithms have been proposed recently to generate the weights and architectures of feed-forward neural networks. Examples are the Cascade Correlation algorithm [1], the Upstart algorithm [2], the Tiling algorithm [3], and algorithms for generating a single hidden layer <ref> [4] </ref> or two hidden layers [5]. The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer [7, 8, 9]. <p> For binary inputs (quantised 1) it is always possible to find a set of weights and thresholds which will correctly store all the patterns belonging to one of these sets and at least one member belonging to the other set <ref> [4] </ref>. For example, suppose pattern = 1 has target +1.
Reference: [5] <author> D. Martinez and S. Esteve, </author> <note> Europhysics Letters 18(1992)95-100 </note>
Reference-contexts: 1 Introduction A number of constructive algorithms have been proposed recently to generate the weights and architectures of feed-forward neural networks. Examples are the Cascade Correlation algorithm [1], the Upstart algorithm [2], the Tiling algorithm [3], and algorithms for generating a single hidden layer [4] or two hidden layers <ref> [5] </ref>. The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer [7, 8, 9].
Reference: [6] <author> C.Campbell and C. Perez Vicente, </author> <note> Neural Computation 7 (1995) 1221-1240. </note>
Reference-contexts: Examples are the Cascade Correlation algorithm [1], the Upstart algorithm [2], the Tiling algorithm [3], and algorithms for generating a single hidden layer [4] or two hidden layers [5]. The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures <ref> [6] </ref> or networks with a single hidden layer [7, 8, 9]. Apart from Cascade Correlation all these algorithms typically use a pocket version of the perceptron rule or similar variants [10]. <p> In addition the theoretical minimal fraction f can give a comparison of the efficiency of different constructive algorithms in terms of the minimal architectures possible. In section 2 we outline the Target Switch algorithms developed by the author and collaborators <ref> [6, 7, 8] </ref>. The Target Switch algorithms construct a multi-layer neural network for binary classification tasks based on using the pocket algorithm. The basic step involves storing all the patterns of one target sign and as many as possible of the opposite target sign. <p> Since learnt patterns (i.e. two +1's or two 1's) are discarded from the training set of succeeding dichotomies we avoid disrupting previously learnt patterns by introducing a cascade structure of linear nodes or a tree of thresholding nodes between the hidden layer and output <ref> [6] </ref>. Thus, for example, if P + i and P i are the pattern sets at hidden node i then we can generate a feed-forward neural network with a single hidden layer as follows [7, 9]: 3 1. We perform a -dichotomy with the current training set. <p> We use a threshold at the output node equal to the difference between the number of hidden nodes inducing -dichotomies and the number inducing -dichotomies. Other ways of generating a single hidden layer using dichotomies are possible [8]. A second general strategy <ref> [6] </ref> is to grow pairs of hidden nodes with alternating and -dichotomies. Patterns learnt by a pair of hidden nodes (two +1's or two 1's) are discarded from the training set of succeeding pairs of hidden nodes. <p> We avoid disrupting previously stored patterns by introducing a further structure of hidden nodes between the initial hidden layer and the output. These additional hidden nodes may be either linear nodes or thresholding nodes and the arrangement used is outlined in <ref> [6] </ref>. Numerical simulations of the Target Switch algorithms have been presented elsewhere [6, 7, 8, 9] and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. <p> These additional hidden nodes may be either linear nodes or thresholding nodes and the arrangement used is outlined in [6]. Numerical simulations of the Target Switch algorithms have been presented elsewhere <ref> [6, 7, 8, 9] </ref> and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. <p> linear nodes or thresholding nodes and the arrangement used is outlined in <ref> [6] </ref>. Numerical simulations of the Target Switch algorithms have been presented elsewhere [6, 7, 8, 9] and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. For Boolean problems (e.g. mirror symmetry, n-parity, shift detection, etc) it has been shown that generalisation is particularly good if the networks are trained using binary weights [6, 7, 8] since the number of free parameters in the network is thereby reduced. <p> The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. For Boolean problems (e.g. mirror symmetry, n-parity, shift detection, etc) it has been shown that generalisation is particularly good if the networks are trained using binary weights <ref> [6, 7, 8] </ref> since the number of free parameters in the network is thereby reduced. The Target Switch algorithms all rely on the ability to find dichotomies of the pattern set. <p> The Target Switch algorithms all rely on the ability to find dichotomies of the pattern set. An efficient method for doing this is presented in <ref> [6] </ref> and involves selective switching of the target values of one target-sign (members of P if we are doing a -dichotomy) until a separation is achieved (this target switching procedure gives the name to these algorithms).
Reference: [7] <author> C. Campbell and C. Perez Vicente, </author> <booktitle> European Symposium on Artificial Neural Networks, </booktitle> <address> ESANN '95 (D. </address> <publisher> Facto Publications, </publisher> <address> Brussels, 1995) p. </address> <pages> 241-246. </pages>
Reference-contexts: The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer <ref> [7, 8, 9] </ref>. Apart from Cascade Correlation all these algorithms typically use a pocket version of the perceptron rule or similar variants [10]. <p> In addition the theoretical minimal fraction f can give a comparison of the efficiency of different constructive algorithms in terms of the minimal architectures possible. In section 2 we outline the Target Switch algorithms developed by the author and collaborators <ref> [6, 7, 8] </ref>. The Target Switch algorithms construct a multi-layer neural network for binary classification tasks based on using the pocket algorithm. The basic step involves storing all the patterns of one target sign and as many as possible of the opposite target sign. <p> Thus, for example, if P + i and P i are the pattern sets at hidden node i then we can generate a feed-forward neural network with a single hidden layer as follows <ref> [7, 9] </ref>: 3 1. We perform a -dichotomy with the current training set. For hidden node i the training set P + i is equal to the original P + whereas P i only consists of members of P previously unstored at earlier hidden nodes inducing a -dichotomy. <p> These additional hidden nodes may be either linear nodes or thresholding nodes and the arrangement used is outlined in [6]. Numerical simulations of the Target Switch algorithms have been presented elsewhere <ref> [6, 7, 8, 9] </ref> and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. <p> The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. For Boolean problems (e.g. mirror symmetry, n-parity, shift detection, etc) it has been shown that generalisation is particularly good if the networks are trained using binary weights <ref> [6, 7, 8] </ref> since the number of free parameters in the network is thereby reduced. The Target Switch algorithms all rely on the ability to find dichotomies of the pattern set.
Reference: [8] <author> C. Campbell, S. Coombes and A. </author> <title> Surkan, </title> <booktitle> in Proceedings of the 3rd SNN Neural Networks Symposium (Nijmegen, </booktitle> <year> 1995) </year> <month> (Springer-Verlag, </month> <note> to appear) </note>
Reference-contexts: The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer <ref> [7, 8, 9] </ref>. Apart from Cascade Correlation all these algorithms typically use a pocket version of the perceptron rule or similar variants [10]. <p> In addition the theoretical minimal fraction f can give a comparison of the efficiency of different constructive algorithms in terms of the minimal architectures possible. In section 2 we outline the Target Switch algorithms developed by the author and collaborators <ref> [6, 7, 8] </ref>. The Target Switch algorithms construct a multi-layer neural network for binary classification tasks based on using the pocket algorithm. The basic step involves storing all the patterns of one target sign and as many as possible of the opposite target sign. <p> We use a threshold at the output node equal to the difference between the number of hidden nodes inducing -dichotomies and the number inducing -dichotomies. Other ways of generating a single hidden layer using dichotomies are possible <ref> [8] </ref>. A second general strategy [6] is to grow pairs of hidden nodes with alternating and -dichotomies. Patterns learnt by a pair of hidden nodes (two +1's or two 1's) are discarded from the training set of succeeding pairs of hidden nodes. <p> These additional hidden nodes may be either linear nodes or thresholding nodes and the arrangement used is outlined in [6]. Numerical simulations of the Target Switch algorithms have been presented elsewhere <ref> [6, 7, 8, 9] </ref> and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. <p> The algorithms can handle both binary and analogue inputs but the outputs must be binary [6]. For Boolean problems (e.g. mirror symmetry, n-parity, shift detection, etc) it has been shown that generalisation is particularly good if the networks are trained using binary weights <ref> [6, 7, 8] </ref> since the number of free parameters in the network is thereby reduced. The Target Switch algorithms all rely on the ability to find dichotomies of the pattern set.
Reference: [9] <author> C. Campbell, C. Perez Vicente and J.G. Keating, </author> <booktitle> in Proceedings of the Irish Neural Networks Conference '94 (1994) p. </booktitle> <pages> 241-246. </pages>
Reference-contexts: The author and collaborators have also proposed several constructive algorithms (the Target Switch algorithms) for generating tree-structured networks, cascade architectures [6] or networks with a single hidden layer <ref> [7, 8, 9] </ref>. Apart from Cascade Correlation all these algorithms typically use a pocket version of the perceptron rule or similar variants [10]. <p> Thus, for example, if P + i and P i are the pattern sets at hidden node i then we can generate a feed-forward neural network with a single hidden layer as follows <ref> [7, 9] </ref>: 3 1. We perform a -dichotomy with the current training set. For hidden node i the training set P + i is equal to the original P + whereas P i only consists of members of P previously unstored at earlier hidden nodes inducing a -dichotomy. <p> These additional hidden nodes may be either linear nodes or thresholding nodes and the arrangement used is outlined in [6]. Numerical simulations of the Target Switch algorithms have been presented elsewhere <ref> [6, 7, 8, 9] </ref> and compare well with other constructive algorithms. The algorithms can handle both binary and analogue inputs but the outputs must be binary [6].
Reference: [10] <author> S.I. </author> <title> Gallant, </title> <journal> IEEE Transactions on Neural Networks 1(1990)179-191. </journal>
Reference-contexts: Apart from Cascade Correlation all these algorithms typically use a pocket version of the perceptron rule or similar variants <ref> [10] </ref>. Generally the basic step is to attempt to store all the pattern mappings, or if this cannot be done, to store the largest possible fraction of these patterns and grow further hidden nodes to store the remaining patterns.
Reference: [11] <author> T. </author> <title> Cover, </title> <journal> IEEE Trans. Electron. Comput. </journal> <pages> 14 (1965)326-334. </pages>
Reference-contexts: For single-layered neural networks, trained using the perceptron algorithm, there is a maximal storage capacity achievable. This limit has been calculated by Cover <ref> [11] </ref> and Gardner [12] for the case of random and uncorrelated pattern sets and corresponds to p = 2N for the storage of p patterns in a network with N weights (leading from the inputs to a single output node).
Reference: [12] <author> E. Gardner, J. Phys. </author> <month> A21 </month> <year> (1988)257-270. </year>
Reference-contexts: For single-layered neural networks, trained using the perceptron algorithm, there is a maximal storage capacity achievable. This limit has been calculated by Cover [11] and Gardner <ref> [12] </ref> for the case of random and uncorrelated pattern sets and corresponds to p = 2N for the storage of p patterns in a network with N weights (leading from the inputs to a single output node). <p> This invariance can be removed using the standard Gardner constraint <ref> [12, 14] </ref>: X W 2 No generality is lost by introducing the Gardner constraint since successful learning in volves finding a hyperplane separating the two pattern sets ( P condition for a hyperplane). Hence this constraint amounts to normalisation of the weight 5 vectors orthogonal to the hyperplanes.
Reference: [13] <author> P. Majer, A. Engel and A. Zippelius, J. Phys. </author> <year> A26(1993)7405-7416. </year>
Reference-contexts: The solution of this problem gives an estimate of the number of hidden nodes generated by the Target Switch algorithms. After outlining the general method for calculating f in section 3.1 (following <ref> [13] </ref>) we present the solution to this problem in section 3.2. <p> Unfortunately we can only reliably average over Z and not ln Z and consequently we follow the standard method [17] of using the replica method and the replica trick: &lt; ln Z &gt;= lim &lt; Z n &gt; 1 Following this line of argument we eventually find <ref> [13] </ref>: f = min f 2x (1 + wq) ln (1 + wq) + wx 1 2 Z z 0 p q 0 p (z 0 p q 0 + p 2x) p Dz 1 e w p q 0 +z 1 q) 2 p p p ! z 0 q <p> P which are incorrectly stored can then be calculated from the partition function [16]: &lt; p f &gt;= dfi Z = j 1 0 X W 2 1 Y (fl T ) exp @ fi 2P 1 Using the replica method and following a argument similar to Majer et al <ref> [13] </ref> we obtain: f = min " 2x (1 + wq) ln (1 + wq) + F (x; q 0 ; w) where: F (x; q 0 ; w) = wx 1 p ! p p ! Z A p A p 2x p q w (A z 1 q) 2
Reference: [14] <author> E. Gardner and B. Derrida, J. Phys. </author> <year> A22(1989)271-284. </year>
Reference-contexts: This invariance can be removed using the standard Gardner constraint <ref> [12, 14] </ref>: X W 2 No generality is lost by introducing the Gardner constraint since successful learning in volves finding a hyperplane separating the two pattern sets ( P condition for a hyperplane). Hence this constraint amounts to normalisation of the weight 5 vectors orthogonal to the hyperplanes. <p> Hence this constraint amounts to normalisation of the weight 5 vectors orthogonal to the hyperplanes. Consequently, integrating over this constrained volume of weight space gives the correct partition function <ref> [14] </ref> for the problem: Z = j 1 0 X W 2 1 X (fl ) To find f we now introduce integral representations for the ffi and functions and perform the average over the pattern set.
Reference: [15] <author> C. Campbell and S. </author> <title> Coombes, </title> <booktitle> in Proceedings of the World Congress on Neural Networks, </booktitle> <publisher> 1995 (Lawrence Erlbaum Associates Inc, </publisher> <address> 1995) 1 (1995) p. </address> <pages> 606-609. </pages>
Reference-contexts: As before calculation of f requires a 3-dimensional minimisation. There also exists an approximate replica symmetric solution <ref> [15] </ref>. In section 2 we also mentioned that it is possible to obtain binary-weight solutions of the Target Switch algorithms for Boolean mappings and that such solutions exhibit good generalisation (binary-weight solutions are also easier to implement in hardware).
Reference: [16] <author> C. Campbell and S. Coombes, </author> <note> preprint in preparation </note>
Reference-contexts: After outlining the general method for calculating f in section 3.1 (following [13]) we present the solution to this problem in section 3.2. Only the essential details are given here and a fuller description of the derivation and simulations will be presented elsewhere <ref> [16] </ref>. 2 2 The Target Switch Algorithms Let us consider a neural network with N input nodes labeled by index j and one output node. Suppose we wish to map inputs ~ j onto a set of targets , where is the pattern index and has components 1. <p> The requirement that all the P + are stored correctly is: Y (fl T ) = 1 The minimal fraction of the P which are incorrectly stored can then be calculated from the partition function <ref> [16] </ref>: &lt; p f &gt;= dfi Z = j 1 0 X W 2 1 Y (fl T ) exp @ fi 2P 1 Using the replica method and following a argument similar to Majer et al [13] we obtain: f = min " 2x (1 + wq) ln (1 + <p> A detailed derivation of these equations, graphs and simulations will be presented elsewhere <ref> [16] </ref>. 7
Reference: [17] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> (Addison-Wesley, </publisher> <address> 1991) p. </address> <pages> 266-271. 8 </pages>
Reference-contexts: Unfortunately we can only reliably average over Z and not ln Z and consequently we follow the standard method <ref> [17] </ref> of using the replica method and the replica trick: &lt; ln Z &gt;= lim &lt; Z n &gt; 1 Following this line of argument we eventually find [13]: f = min f 2x (1 + wq) ln (1 + wq) + wx 1 2 Z z 0 p q 0
References-found: 17

