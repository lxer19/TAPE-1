URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub23.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: BEYOND MASSIVE PARALLELISM: NUMERICAL COMPUTATION USING ASSOCIATIVE TABLES  
Author: CRAIG C. DOUGLAS AND WILLARD L. MIRANKER 
Keyword: Key words. Associative memory, linear systems of equations, ordinary and partial differential equations, Karmarkar's algorithm, table methods  
Web: 65W05, 65F10, 65N10  
Note: AMS(MOS) subject classifications.  
Abstract: Novel computing devices are exploited for numerical computation. The solution of a numerical problem is sought, which has been solved many times before, but this time with a different set of input data. A table is a classical way to collect the old solutions in order to exploit them to find the new one. This process is extended to more general problems than the usual function value approximation. To do this, a new concept of table is introduced. These tables are addressed associatively. Several problems are treated both theoretically and computationally. These problems include solving linear systems of equations, partial differential equations, nonlinear systems of ordinary differential equations, and Karmarkar's algorithm. Hardware requirements are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. A. Anderson and E. Rosenfeld, </author> <title> Neurocomputing, </title> <booktitle> Foundations of Research, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Some examples where these models are used include cognitive problems (e.g., pattern matching or language processing), combinatorial problems (e.g., traveling salesman), organ simulations (e.g., retina), etc. (see [15]). The vitality and fecundity of this new parallel computing framework is impressive from all of the obvious points of view (see <ref> [1] </ref>). Massive parallelism usually refers to the case when some thousands or a few million digital processors are used in a computation. The new models can be applied when billions (or more) of analog components are available. This type of parallelism goes beyond the usual scope of massive parallelism. <p> In either case, after discretization, we get an N 2 fi N 2 matrix B P (the subscript P , as in Poisson, is used here so that these particular matrices may be referenced later without confusion) which is block tridiagonal (see [22, 23]): where Q = <ref> [1; 4; 1] </ref> is an N fi N tridiagonal matrix.
Reference: [2] <author> R. E. Bank and C. C. Douglas, </author> <title> An efficient implementation of the SSOR and ILU precon-ditionings, </title> <journal> Appl. Numer. Math., </journal> <volume> 1 (1985), </volume> <pages> pp. 489-492. </pages>
Reference-contexts: While there is more computational work associated with these iterative methods <ref> [2] </ref>, the total number of inferences is reduced enough to offset this extra work. We used the same x 0 and b as used in Figure 2.2. Figure 2.3 contains the scaled residual norms for the same case as used in Figure 2.2.
Reference: [3] <author> L. Blum, M. Shub, and S. Smale, </author> <title> On a theory of computation and complexity over the real numbers, NP-completeness, recursive functions and universal machines, </title> <journal> Bulletin AMS, </journal> <volume> 21 (1989), </volume> <pages> pp. 1-46. </pages>
Reference-contexts: This type of parallelism goes beyond the usual scope of massive parallelism. In this paper, we show how numerical analysis and the new models can interact. Ours is not likely to be the only approach (see <ref> [3] </ref>), but the results presented here point to a rich and varied set of future developments within the framework of this massively parallel computing model. Our results have a speculative character, since we deal with numerical computation on a connectionist (analog) parallel model and a putative class of hardware implementations.
Reference: [4] <author> C. C. Douglas and W. L. Miranker, </author> <title> Numerical computation using associative tables, </title> <type> Tech. Report 14660, </type> <institution> IBM Research Division, </institution> <address> Yorktown Heights, New York, </address> <year> 1989. </year>
Reference-contexts: In either case, after discretization, we get an N 2 fi N 2 matrix B P (the subscript P , as in Poisson, is used here so that these particular matrices may be referenced later without confusion) which is block tridiagonal (see [22, 23]): where Q = <ref> [1; 4; 1] </ref> is an N fi N tridiagonal matrix. <p> Fig. 4. Inferential Solve Random Data We also experimented (see <ref> [4] </ref>) with modifying B P so that its eigenvalue structure was clustered. Results similar to Fig. 2.2 were observed. Finally, we constructed real, dense, symmetric, positive definite matrices A r starting from dense matrices with random elements.
Reference: [5] <author> H. P. Graf and P. deVegvar, </author> <title> A CMOS implementation of a neural network model, in Advanced Research in VLSI, </title> <editor> P. Lasleben, ed., </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: In 1990, another five to ten fold increase in neurons per chip is expected [9]. The density of these chips follows behind standard memory chips by roughly one generation. A description of an associative memory using high density neural network chips is contained in <ref> [5] </ref>. Loading the memory is equivalent to writing data into conventional VLSI memory. The query time is negligible, and getting data back is equivalent to reading data from conventional memory.
Reference: [6] <author> H. P. Graf, L. D. Jackel, R. E. Howard, B. Straughn, J. S. Denker, W. Hubbard, D. M. Tennant, and D. Schwartz, </author> <title> VLSI iimplementation of a neural network memory with several hundreds of neurons, </title> <booktitle> in Proceedings of Conference on Neural Networks for Computing, Snowbird, Utah, </booktitle> <editor> J. S. Denker, ed., </editor> <booktitle> American Institute of Physics, </booktitle> <year> 1986, </year> <pages> pp. 182-187. </pages>
Reference-contexts: This device can be modified so that the cycle time is lower, but at the expense of the memory size. A description of recent digital-analog devices is described in <ref> [6] </ref> and [10]. It is interesting to note that in 1986, several hundred neurons could be placed on a single CMOS chip. Yet in 1989, 100,000 neurons could be placed on a similarly sized chip. In 1990, another five to ten fold increase in neurons per chip is expected [9].
Reference: [7] <author> T. N. E. Greville, </author> <title> Some applications of the pseudoinverse of a matrix, </title> <journal> SIAM Review, </journal> <volume> 2 (1960), </volume> <pages> pp. 15-22. </pages>
Reference-contexts: We note on a conventional digital computer, the cost of computing (1.2) is normally prohibitive. One way of reducing the cost somewhat is to use Greville's Theorem (see <ref> [7] </ref>). This is an O (N 2 ) work per step (maximum N iterations) iterative algorithm for computing the pseudoinverse. <p> This process is defined by the following recurrence: x k+j+1 = x k+j + M k;j u k+j1 ; j = 1; 2; The output of this memory reference could be refined by a bootstrapping process (e.g., Greville's Theorem <ref> [7] </ref>) or possibly a residual correction process before the new signal u k+1 is computed for memory update. 14 3.2. Numerical Experiments. In this section, we discuss numerical experi-ments for the inferential extrapolation method.
Reference: [8] <author> S. Horiguchi and W. L. Miranker, </author> <title> Noisy sort, a memory-intensive sorting algorithm, Linear Algebra & Its Applications, </title> <booktitle> (1989), </booktitle> <pages> pp. 641-658. </pages>
Reference-contexts: The post processing consists of a very small number of Newton iterations for solving f (q) = 0. In this paper we show how to prepare and exploit tables to solve a variety of problems. An earlier version of this approach dealt with the problem of sorting <ref> [8] </ref>. Here, we show how to use tables to solve linear systems of equations in x2 and systems of differential equations in x3 and x5. We also show how Karmarkar's algorithm may be accelerated by our methods in x4. <p> Table 6.1 contains a summary of the requirements for inferential solve, inferential extrapolation, Karmarkar's algorithm, inferential multistep, and noisy sort (see <ref> [8] </ref>). Except for the latter two methods, the theoretical bound for k is N . For the inferential multistep method, the number of pairs must be larger than half of the problem dimension, but may be less than N .
Reference: [9] <author> L. D. Jackel, </author> <year> 1989. </year> <title> private communication. </title>
Reference-contexts: Some divergent cases are shown for completeness. 6. Current Technology and Parameter Requirements. Associative memories now under construction limit the number of (signal,key) pairs that are stored to k N , where N is the dimension of these pairs <ref> [9] </ref>. Associative memories are typically used only for pattern matching. <p> It is interesting to note that in 1986, several hundred neurons could be placed on a single CMOS chip. Yet in 1989, 100,000 neurons could be placed on a similarly sized chip. In 1990, another five to ten fold increase in neurons per chip is expected <ref> [9] </ref>. The density of these chips follows behind standard memory chips by roughly one generation. A description of an associative memory using high density neural network chips is contained in [5]. Loading the memory is equivalent to writing data into conventional VLSI memory.
Reference: [10] <author> L. D. Jackel, H. P. Graf, and R. E. Howard, </author> <title> Electronic neural network chips, </title> <journal> Applied Optics, </journal> <volume> 26 (1987), </volume> <pages> pp. 5077-5080. </pages>
Reference-contexts: This device can be modified so that the cycle time is lower, but at the expense of the memory size. A description of recent digital-analog devices is described in [6] and <ref> [10] </ref>. It is interesting to note that in 1986, several hundred neurons could be placed on a single CMOS chip. Yet in 1989, 100,000 neurons could be placed on a similarly sized chip. In 1990, another five to ten fold increase in neurons per chip is expected [9].
Reference: [11] <author> R. E. </author> <title> Kalman, Fundamental study of adaptive control systems, </title> <type> ASTIA AD 8 28 73, </type> <year> (1962). </year>
Reference-contexts: In Control Theory, the least squares problem is generalized in a number of ways, such as by adding a small random perturbation to the problem (i.e., by introducing noise). A large class of solution procedures for such generalizations is commonly named Kalman filtering <ref> [11] </ref>. This theory can have relevance to the approach here when input-output errors or rounding errors are added to the signals and keys. Since Kalman filters are extensively modeled, both synthetically and by special dedicated circuitry, this connection can have a significant pragmatic impact on our work. 2.
Reference: [12] <author> N. Karmarkar, </author> <title> A new polynomial time algorithm for linear programming, </title> <journal> Combinatorica, </journal> <volume> 4 (1984), </volume> <pages> pp. 373-395. </pages>
Reference-contexts: We note that for the three examples in this section, small values of k produce acceptable results. Note: These experiments illustrate the table lookup aspect of our approach without any corrective postprocessing, e.g., a residual correction process (see x2.2). 4. Karmarkar's Algorithm. Karmarkar's algorithm <ref> [12] </ref> for solving the linear programming problem may be formulated in terms of an associative table.
Reference: [13] <author> T. Kohonen, </author> <title> Associative Memory: A System-Theoretical Approach, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1977. </year> <title> [14] , Self-Organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: We also show how Karmarkar's algorithm may be accelerated by our methods in x4. In x6, we summarize the hardware requirements of the methods of this paper and comment on current hardware technology. Information about a problem is collected in a type of memory (table) called an associative memory <ref> [13, 14] </ref>. The model of table lookup proceeds by association. An access to this memory by a key produces an output by association. The numerical methods used here are of interest in their own right. <p> Current Technology and Parameter Requirements. Associative memories now under construction limit the number of (signal,key) pairs that are stored to k N , where N is the dimension of these pairs [9]. Associative memories are typically used only for pattern matching. Kohonen (see <ref> [13] </ref>) suggests a holographic implementation and includes a picture of such a device. 25 Table 1 Inferential Multistep Numerical Experiments case p = 1, ff = fi = 1, ` = 5 d = 2 1 2:36 fi 10 3 7:35 fi 10 4 3:52 fi 10 7 10 3:25 fi
Reference: [15] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Among the new parallel computing models are analog devices and related connectionist architectures (neural nets, associative memories, connection machines, , see [19]). Some examples where these models are used include cognitive problems (e.g., pattern matching or language processing), combinatorial problems (e.g., traveling salesman), organ simulations (e.g., retina), etc. (see <ref> [15] </ref>). The vitality and fecundity of this new parallel computing framework is impressive from all of the obvious points of view (see [1]). Massive parallelism usually refers to the case when some thousands or a few million digital processors are used in a computation. <p> An associative memory is an analog device with the well known advantage of great speed, but limited accuracy. Analog devices can be simulated in digital VLSI hardware <ref> [15] </ref> with a loss of some speed, but increased accuracy. We stress that even fully digital floating point computations have limited accuracy, usually graded by the problem's condition. The method of residual correction is the standard technique for addressing limited accuracy for digital devices.
Reference: [16] <author> C. Moler, J. Little, S. Bangert, and S. Kleiman, </author> <title> MATLAB User's Guide, The MathWorks, </title> <publisher> Inc., </publisher> <address> Sherborn, MA, </address> <year> 1987. </year>
Reference-contexts: Numerical Experiments. In this section, we discuss numerical experiments for inferential solve. We apply this technique to two classes of model problems: discretizations of Poisson's equation on a square and random symmetric, positive definite matrices. We simulated the associative memory paradigm using a workstation, programmed in Matlab <ref> [16] </ref>. The inferences, s i , are calculated using (2.14). We chose Matlab since one of its arithmetic operators is the pseudoinverse of a matrix. Let and Throughout this section, we graph a scaled residual norm, kr i k versus i; where k k is the Euclidean norm.
Reference: [17] <author> D. Psaltis, </author> <year> 1990. </year> <title> private communication. </title>
Reference-contexts: FAX 914-945-3434. 1 In order for these methods to be useful in practice, certain hardware must become available in effective scale and performance. The largest associative memory device we are aware of has 10 10 bits with a cycle time of 10 4 seconds <ref> [17] </ref>. We comment on the hardware environment in x6. The new model and hardware as applied customarily to higher order (e.g., cognitive) problems are often described as extensions of the traditional construction and use of tables in numerical computations.
Reference: [18] <author> R. D. Richtmyer and K. W. Morton, </author> <title> Difference Methods for Initial-Value Problems, </title> <publisher> Inter-science Publishers (J. Wiley & Sons), </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: For example, W i;j+1 W i;j = k 2 W i1;j 2W i;j + W i+1;j i = 1; 2; ; h 1 1, and j = 1; 2; , where = 1 is the Backward Euler scheme and = :5 is the Crank-Nicolson scheme (see <ref> [18] </ref>).
Reference: [19] <author> D. E. Rummelhardt and J. L. McClelland, </author> <title> Parallel Distributed Processing, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1. Introduction. Among the new parallel computing models are analog devices and related connectionist architectures (neural nets, associative memories, connection machines, , see <ref> [19] </ref>). Some examples where these models are used include cognitive problems (e.g., pattern matching or language processing), combinatorial problems (e.g., traveling salesman), organ simulations (e.g., retina), etc. (see [15]).
Reference: [20] <author> D. A. Smith, W. F. Ford, and A. Sidi, </author> <title> Extrapolation methods for vector sequences, </title> <journal> SIAM Review, </journal> <volume> 29 (1987), </volume> <pages> pp. 199-233. </pages>
Reference-contexts: The numerical methods used here are of interest in their own right. For example, in the case of linear systems, we borrow from an existing body of work dealing with the acceleration of convergence of vector sequences <ref> [20] </ref>. However, when the memory mechanism is implemented in hardware which provides this storage and associative retrieval both rapidly and efficiently, our methods have the potential of being superior to conventional algorithms. An associative memory is an analog device with the well known advantage of great speed, but limited accuracy. <p> In fact, methods exist which specify s in terms of the first k + 1 iterates fx 0 ; x 1 ; ; x k g where the integer k N is well defined by A and x 0 . Such a method <ref> [20] </ref>, called reduced rank extrapolation (RRE) is a vectorized version of Aitken's 2 -method. Indeed an associative memory provides a method for assembling these iterates (i.e., the information) for implementing the accelerative process and for determining an effective approximation to k. <p> case, the pseudoinverse U + of U exists and U + = (U T U ) 1 U T : Using (2.4), we see that the pseudoinverse of V exists, and V + = U + (A I) 1 :(2.9) Consider the following theorem whose proof may be found in <ref> [20] </ref>: Theorem 2.1. For any k + 1 consecutive terms of the sequence, say x m ; x m+1 ; ; x m+k , we have k X c j x m+j = ( j=0 5 Now let U U N and V V N . <p> In the case that U and V in (2.10) are replaced by U k and V k , respectively, we have the following reduced rank equivalent of (2.10): s = x 0 U V + u 0 :(2.11) For a proof of this reduced rank equivalent, (2.5) - (2.6), see <ref> [20] </ref>. <p> k are linearly independent, U + k exists, and (2.8) gives c = U + Now if zero is not an eigenvalue of A, which we assume, then P () is the minimal polynomial of A with respect to u j ; j = 1; 2; (for a proof, see <ref> [20] </ref>).
Reference: [21] <author> G. W. Stewart, </author> <title> Introduction to Matrix Computation, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Note that (A I) 1 (* 1 * 0 ) = * 0 + (A I) 1 f 1 ; so that T 3 = (U + E)(U + F ) + (* 0 + (A I) 1 f 1 ): To estimate T 1 , we use Theorem 2.2 <ref> [21, page 223] </ref> for characterizing perturbations in least squares problems with the normal equations Cx = d: In Theorem 2.2, R will be a perturbation of C and the subscripts 1 and 2 denote projections onto the ranges R (A) and R (A) ? , respectively. Theorem 2.2.
Reference: [22] <author> G. Strang and G. J. Fix, </author> <title> An Analysis of the Finite Element Method, </title> <publisher> Prentice-Hall, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In either case, after discretization, we get an N 2 fi N 2 matrix B P (the subscript P , as in Poisson, is used here so that these particular matrices may be referenced later without confusion) which is block tridiagonal (see <ref> [22, 23] </ref>): where Q = [1; 4; 1] is an N fi N tridiagonal matrix.
Reference: [23] <author> R. S. Varga, </author> <title> Matrix Iterative Analysis, </title> <publisher> Prentice-Hall, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: Now lim x j = s for any choice of x 0 , if and only if the spectral radius (A) &lt; 1. Henceforth, we assume this to be the case. Richardson's method is often very slowly convergent, and is seldom used without modification <ref> [23, 24] </ref>. We take the view that the successive iterates provide information about the solution sought. We intend to store this information in an associative memory, in such a manner that inquiries addressed to the memory will produce information which accelerates the determination of s. <p> In either case, after discretization, we get an N 2 fi N 2 matrix B P (the subscript P , as in Poisson, is used here so that these particular matrices may be referenced later without confusion) which is block tridiagonal (see <ref> [22, 23] </ref>): where Q = [1; 4; 1] is an N fi N tridiagonal matrix.
Reference: [24] <author> D. M. Young, </author> <title> Iterative Solution of Large Linear Systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year> <month> 28 </month>
Reference-contexts: Now lim x j = s for any choice of x 0 , if and only if the spectral radius (A) &lt; 1. Henceforth, we assume this to be the case. Richardson's method is often very slowly convergent, and is seldom used without modification <ref> [23, 24] </ref>. We take the view that the successive iterates provide information about the solution sought. We intend to store this information in an associative memory, in such a manner that inquiries addressed to the memory will produce information which accelerates the determination of s. <p> As an aside, we modified our Matlab program for inferential solve to accept vectors x i , which were generated by three conjugate direction methods: conjugate gradients, conjugate gradients preconditioned by SSOR, and conjugate residuals preconditioned with SSOR (see <ref> [24] </ref>). While there is more computational work associated with these iterative methods [2], the total number of inferences is reduced enough to offset this extra work. We used the same x 0 and b as used in Figure 2.2.
References-found: 23

