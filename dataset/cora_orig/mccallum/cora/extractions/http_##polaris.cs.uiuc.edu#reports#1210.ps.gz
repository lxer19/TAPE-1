URL: http://polaris.cs.uiuc.edu/reports/1210.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Phone: 2  
Title: Preconditioned Conjugate Gradient-Like Methods for Nonsymmetric Linear Systems 1  
Author: Ulrike Meier Yang 
Affiliation: Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign  
Web: DE-FG02 85ER25001.  
Note: 1 This research was supported by the U.S. Department of Energy under Grant No.  
Date: July 19, 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> O. Axelsson, </author> <title> Conjugate gradient type methods for unsymmetric and inconsistent systems of linear equations, </title> <journal> Linear Algebra and Appl. </journal> <volume> 29: </volume> <month> 1-16 </month> <year> (1980). </year>
Reference-contexts: One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. This approach has been used in methods like Orthomin [23], Orthodir, and other generalized conjugate gradient schemes <ref> [1] </ref> [4] [16] [24]. We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) [17]. <p> The proof for this theorem can be found in [21]. The convergence behavior of the conjugate gradient algorithm has been investigated among others by <ref> [1] </ref> [20]. These considerations show the dependence of the rate of convergence of the spectrum of A. We state one important theorem here, which takes into account the condition number (A). More detailed theoretical convergence results that also consider the shape of the spectrum can be found in [20]. <p> CGS (polynomial version) Initialization: 0 = 1 0 = 1 Repeat for k = 1; 2; : : :: ff k1 = <ref> [1; # k1 ] </ref> fi k = k1 ff k1 # k1 (2.51) fi k = [1; k1 ] k = k + fi k fi k (2.54) This can be transformed into the vectorial variant of CGS by substituting r k := k (2.56) t k := fi k (2.58) <p> CGS (polynomial version) Initialization: 0 = 1 0 = 1 Repeat for k = 1; 2; : : :: ff k1 = [1; # k1 ] fi k = k1 ff k1 # k1 (2.51) fi k = <ref> [1; k1 ] </ref> k = k + fi k fi k (2.54) This can be transformed into the vectorial variant of CGS by substituting r k := k (2.56) t k := fi k (2.58) and replacing the bilinear form by the corresponding vector product. One obtains Algorithm 2.5. <p> BICGSTAB (polynomial version) Initialization: 0 = 1 (2:82) Repeat for k = 1; 2; : : :: ff k1 = <ref> [1; # k1 ] </ref> k = k1 ff k1 # k1 (2.84) [ k ; # k ] (2.85) fi k = [1; k1 ]fl k k = k + fi k ( k1 fl k # k1 ) (2.88) Now replacing s k := k (2.89) p k := k <p> BICGSTAB (polynomial version) Initialization: 0 = 1 (2:82) Repeat for k = 1; 2; : : :: ff k1 = [1; # k1 ] k = k1 ff k1 # k1 (2.84) [ k ; # k ] (2.85) fi k = <ref> [1; k1 ] </ref>fl k k = k + fi k ( k1 fl k # k1 ) (2.88) Now replacing s k := k (2.89) p k := k (2.91) and substituting the corresponding vector products leads to the vector variant of BICGSTAB. Algorithm 2.7. <p> Consider the following three-dimensional elliptic partial differential equation au xx + bu yy + cu zz + du x + eu y + f u z + gu = F: (4:6) on the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. <p> Consider the following three-dimensional elliptic partial differential equation au xx + bu yy + cu zz + du x + eu y + f u z + gu = F: (4:6) on the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. For ellipticity, we need a; b; c &gt; 0 on [0; 1]fi <p> Consider the following three-dimensional elliptic partial differential equation au xx + bu yy + cu zz + du x + eu y + f u z + gu = F: (4:6) on the unit cube <ref> [0; 1] </ref> fi [0; 1] fi [0; 1]. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. For ellipticity, we need a; b; c &gt; 0 on [0; 1]fi [0; 1]fi [0; <p> fi <ref> [0; 1] </ref> fi [0; 1]. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. For ellipticity, we need a; b; c &gt; 0 on [0; 1]fi [0; 1]fi [0; 1]. <p> 1] fi <ref> [0; 1] </ref>. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. For ellipticity, we need a; b; c &gt; 0 on [0; 1]fi [0; 1]fi [0; 1]. We consider here the following nine test problems: Problem 1: u xx + u yy + u zz + 1000u x = F (4:7) with solution u = xyz (1 x)(1 y)(1 z): The large coefficient of u x causes a loss of diagonal dominance. <p> <ref> [0; 1] </ref>. a, b, c, d, e, f are functions of (x; y; z). Using a seven point centered finite difference operator, one obtains linear systems with a seven-diagonal sparse matrix that is in general nonsymmetric. For ellipticity, we need a; b; c &gt; 0 on [0; 1]fi [0; 1]fi [0; 1]. We consider here the following nine test problems: Problem 1: u xx + u yy + u zz + 1000u x = F (4:7) with solution u = xyz (1 x)(1 y)(1 z): The large coefficient of u x causes a loss of diagonal dominance.
Reference: [2] <author> R. Bramley, </author> <title> Row projection methods for linear systems, </title> <type> CSRD Report 881, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign (1989). </institution>
Reference-contexts: methods are just the former algorithms applied to the preconditioned systems, this is the residual of the preconditioned system, and not the original system. 21 4.2 Three-Dimensional Elliptic Problems 4.2.1 Description of the Problems In this section, we use a set of test problems which can also be found in <ref> [2] </ref>. <p> The spectra of Problem 2, 3, 5, and 7 as well as those of the matrices preconditioned by ILU (0), MILU (0), and ILUT (0) are shown in the Appendices A and B. The spectra for the remaining problems can be found in <ref> [2] </ref>. 4.2.2 Numerical Results Before we consider the individual results, we present a statistical summary of all the results achieved in Tables 4.1 and 4.2. Table 4.1 summarizes the number of successes for each method with and without preconditioning. <p> So, CGNE is here converging extremely slow, whereas all the other methods fail completely. The use of another preconditioner can lead to better results. This has been done for row projection methods <ref> [2] </ref> that are equivalent to CGNE using a block Jacobi (SUM) or block SSOR preconditioner (RP-9). Here, faster convergence is achieved in almost all cases (see Table 4.2), which shows that for the problems considered, CGNE possesses a robustness that the other solvers do not possess.
Reference: [3] <author> R. Bramley, A. Sameh, </author> <title> Row projection methods for large nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 13: </volume> <month> 168-193 </month> <year> (1992). </year>
Reference-contexts: It is therefore important to precondition the system, in order to improve the condition number. This has been investigated to some degree in [6] where an incomplete LU factorization as well as its modified form are used. Other preconditioned forms of CGNE are the row projection methods <ref> [3] </ref>, where a block Jacobi or block SSOR preconditioning is used. These methods turn out to be very robust in many cases, however, they often converge slowly.
Reference: [4] <author> P. Concus, G. Golub, </author> <title> A generalized conjugate gradient method for nonsymmetric systems of linear equations, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems: Computing Methods in Applied Sciences and Engineering 134: </booktitle> <month> 56-65 </month> <year> (1976). </year>
Reference-contexts: One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. This approach has been used in methods like Orthomin [23], Orthodir, and other generalized conjugate gradient schemes [1] <ref> [4] </ref> [16] [24]. We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) [17].
Reference: [5] <author> I. Duff, R. Grimes, J. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. on Math. </journal> <volume> Software 15: </volume> <month> 1-14 </month> <year> (1989). </year>
Reference-contexts: 7 44 CGNE 2 6 7 4 7 6 6 38 CGNR 1 5 6 4 6 6 6 33 GMRES (10) 2 6 6 5 7 6 6 38 P 4.3 Harwell/Boeing Collection (RUA) 4.3.1 Description of the Matrices The Harwell/Boeing collection consists of a variety of test matrices <ref> [5] </ref>. We consider here only 10 of them, which were taken from the subset RUA.
Reference: [6] <author> H. Elman, </author> <title> Iterative methods for large, sparse, nonsymmetric systems of linear equations, </title> <type> Research Report 229, </type> <institution> Yale University (1982). </institution>
Reference-contexts: One can maintain both properties by considering the normal equations instead of the original system. The new matrices A T A (CGNR) or AA T (CGNE) are symmetric positive definite, and the CG algorithm can therefore be applied to them. This approach is considered in <ref> [6] </ref>. The disadvantage of using the normal equations is that the condition number which plays a role for the convergence of CG increases significantly. These algorithms would be expected to converge very slowly. It is therefore important to precondition the system, in order to improve the condition number. <p> These algorithms would be expected to converge very slowly. It is therefore important to precondition the system, in order to improve the condition number. This has been investigated to some degree in <ref> [6] </ref> where an incomplete LU factorization as well as its modified form are used. Other preconditioned forms of CGNE are the row projection methods [3], where a block Jacobi or block SSOR preconditioning is used. <p> types of normal equations: A T Ax = A T b (2:107) AA T y = b; x = A T y (2:108) where we will denote CG applied to A T Ax = A T b by CGNR and CG applied to AA T y = b by CGNE <ref> [6] </ref> [18]. The matrices of the above systems A T A and AA T are symmetric and positive definite, and therefore the classical conjugate gradient method can be successfully applied to them. Theoretically, no division by zero will occur, and it will terminate within at most n iterations. <p> (xyz)u x ) x + (exp (xyz)u y ) y + (exp (xyz)u z ) z 1+x+y+z = F with solution u = exp (xyz) sin (x) sin (y) sin (z) This problem has a positive definite symmetric part 1 2 (A + A T ) for any mesh size <ref> [6] </ref>. Problem 5: u xx + u yy + u zz + 100xu x yu y + zu z + 100 xyz with solution u = exp (xyz) sin (x) sin (y) sin (z) This problem [19] leads to an extremely ill-conditioned linear system with an indefinte symmetric part.
Reference: [7] <author> R. Fletcher, </author> <title> Conjugate gradient methods for indefinite systems, </title> <booktitle> Numerical Analysis Dundee 1975, </booktitle> <pages> 73-89, </pages> <editor> ed. G. Watson, </editor> <publisher> Springer-Verlag, </publisher> <address> New York (1976). </address>
Reference-contexts: In general, the convergence rate improves with increases in k. One can maintain the three-term recursion property. This is done in the biconjugate gradient algorithm (BICG), conjugate gradient squared (CGS), and the stabilized version of CGS (BICGSTAB). In BICG <ref> [7] </ref>, the approximations are constructed in such a way that the residuals are orthogonal to some "pseudo"-residuals involving A T . This leads to two three-term recursions for the residuals and the "pseudo"-residuals. <p> Then (i) r T i r j = 0 for i &gt; j 0 (orthogonality) (ii) p T i Ap j = 0 for i &gt; j 0 (A-conjugacy). The proof which is performed by induction can be found in <ref> [7] </ref>. Rewriting the above algorithm in terms of some matrix equations gives further insight into the algorithm. For this approach, which can also be found in [7], we need to define the following matrices: R k = @ q 0 r 0 r 1 r T ; :::; q k r <p> The proof which is performed by induction can be found in <ref> [7] </ref>. Rewriting the above algorithm in terms of some matrix equations gives further insight into the algorithm. For this approach, which can also be found in [7], we need to define the following matrices: R k = @ q 0 r 0 r 1 r T ; :::; q k r k A ; (2:8) 0 p 0 r T ; q 1 r 1 p k r T 1 L k = B B B B <p> Then kx x k k A 2kx x 0 k A ( (A) 1 (A) + 1 2.2 The Bi-Conjugate Gradient Algorithm The bi-conjugate gradient algorithm (BICG) was suggested by Fletcher <ref> [7] </ref>. It originally was used by Lanczos [11] to compute the eigenvalues of an unsymmetric matrix. In the classical conjugate gradient algorithm, applied to a symmetric positive definite matrix, the residuals obtained in different iteration steps are orthogonal to each other as stated in Theorem 2.1.
Reference: [8] <author> I. Gustafsson, </author> <title> A class of first order factorizations, </title> <type> BIT 18: </type> <month> 142-156 </month> <year> (1978). </year>
Reference-contexts: where M = M 1 M 2 and M 1 2 y = M 1 In this chapter, several incomplete LU factorization preconditioners are described, with positional and numerical dropping. 3.1 Incomplete LU Factorization with Regard to Structure (ILU (k)) Incomplete LU factorization has been suggested by many researchers [14] <ref> [8] </ref> [9] [12]. The drawback of a complete LU factorization for a sparse matrix is that in general while the LU factorization is performed, the sparsity of the original matrix is lost, which leads to a large storage requirement and possibly a large number of operations. <p> But in many cases this factorization leads to an effective preconditioner. 3.2 Modified Incomplete LU Factorization (MILU (k)) In order to improve the ILU factorization, Gustafsson <ref> [8] </ref> [9] proposed a modification of the diagonal elements of L so that the rowsums of the error matrix LU A vanish. Let us define Z k as in the preceding section. Then MILU (k) can be formulated as follows. Algorithm 3.2.
Reference: [9] <author> I. Gustafsson, </author> <title> Modified incomplete Cholesky methods, Preconditioning Methods: Theory and Applications, </title> <editor> ed. D. Evans, </editor> <address> 265-293, </address> <publisher> Gordon and Breach, </publisher> <address> New York (1983). </address>
Reference-contexts: M = M 1 M 2 and M 1 2 y = M 1 In this chapter, several incomplete LU factorization preconditioners are described, with positional and numerical dropping. 3.1 Incomplete LU Factorization with Regard to Structure (ILU (k)) Incomplete LU factorization has been suggested by many researchers [14] [8] <ref> [9] </ref> [12]. The drawback of a complete LU factorization for a sparse matrix is that in general while the LU factorization is performed, the sparsity of the original matrix is lost, which leads to a large storage requirement and possibly a large number of operations. <p> But in many cases this factorization leads to an effective preconditioner. 3.2 Modified Incomplete LU Factorization (MILU (k)) In order to improve the ILU factorization, Gustafsson [8] <ref> [9] </ref> proposed a modification of the diagonal elements of L so that the rowsums of the error matrix LU A vanish. Let us define Z k as in the preceding section. Then MILU (k) can be formulated as follows. Algorithm 3.2.
Reference: [10] <author> M. Hestenes, E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <institution> J. Res. Nat. Bur. Standards 49: </institution> <month> 409-436 </month> <year> (1952). </year>
Reference: [11] <author> C. </author> <title> Lanczos, An iteration method for the solution of eigenvalue problems of linear differential and integral operators, </title> <institution> J. Res. Nat. Bur. Standards 45: </institution> <month> 255-282 </month> <year> (1950). </year>
Reference-contexts: Then kx x k k A 2kx x 0 k A ( (A) 1 (A) + 1 2.2 The Bi-Conjugate Gradient Algorithm The bi-conjugate gradient algorithm (BICG) was suggested by Fletcher [7]. It originally was used by Lanczos <ref> [11] </ref> to compute the eigenvalues of an unsymmetric matrix. In the classical conjugate gradient algorithm, applied to a symmetric positive definite matrix, the residuals obtained in different iteration steps are orthogonal to each other as stated in Theorem 2.1. They also fulfill a three-term recursion.
Reference: [12] <author> T. Manteuffel, </author> <title> An incomplete factorization technique for positive definite linear systems, </title> <journal> Math. Comp. </journal> <volume> 34: </volume> <month> 473-497 </month> <year> (1980). </year> <month> 53 </month>
Reference-contexts: = M 1 M 2 and M 1 2 y = M 1 In this chapter, several incomplete LU factorization preconditioners are described, with positional and numerical dropping. 3.1 Incomplete LU Factorization with Regard to Structure (ILU (k)) Incomplete LU factorization has been suggested by many researchers [14] [8] [9] <ref> [12] </ref>. The drawback of a complete LU factorization for a sparse matrix is that in general while the LU factorization is performed, the sparsity of the original matrix is lost, which leads to a large storage requirement and possibly a large number of operations.
Reference: [13] <author> U. Meier, G. Skinner, J. Gunnels, </author> <title> A collection of tools for sparse matrix compu-tations, </title> <type> CSRD Report 1134, </type> <institution> Center for Research and Development, University of Illinois at Urbana-Champaign (1991). </institution>
Reference-contexts: The codes were written in standard ANSI Fortran 77 and taken from the CSRD library SPLIB <ref> [13] </ref> and SPARSKIT [15].
Reference: [14] <author> T. Meijerink, H. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp. </journal> <volume> 31: </volume> <month> 148-162 </month> <year> (1977). </year>
Reference-contexts: preconditioning, where M = M 1 M 2 and M 1 2 y = M 1 In this chapter, several incomplete LU factorization preconditioners are described, with positional and numerical dropping. 3.1 Incomplete LU Factorization with Regard to Structure (ILU (k)) Incomplete LU factorization has been suggested by many researchers <ref> [14] </ref> [8] [9] [12]. The drawback of a complete LU factorization for a sparse matrix is that in general while the LU factorization is performed, the sparsity of the original matrix is lost, which leads to a large storage requirement and possibly a large number of operations.
Reference: [15] <author> Y. Saad, SPARSKIT: </author> <title> A basic tool kit for sparse matrix computation, </title> <type> CSRD Report 1029, </type> <institution> Center for Research and Development, University of Illinois at Urbana-Champaign (1990). </institution>
Reference-contexts: In many cases, the number of iterations decreased significantly. In the case of general sparse matrices, however, it often turns out to be worse than ILU or even fails completely. 3.3 Incomplete LU Factorization with Regard to Element Size (ILUT (k)) This factorization, which was suggested by Saad <ref> [15] </ref>, uses a different criterion for dropping elements. <p> The codes were written in standard ANSI Fortran 77 and taken from the CSRD library SPLIB [13] and SPARSKIT <ref> [15] </ref>.
Reference: [16] <author> Y. Saad, M. Schultz, </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems, </title> <journal> Math. </journal> <volume> Comp 44: </volume> <month> 417-424 </month> <year> (1985). </year>
Reference-contexts: One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. This approach has been used in methods like Orthomin [23], Orthodir, and other generalized conjugate gradient schemes [1] [4] <ref> [16] </ref> [24]. We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) [17].
Reference: [17] <author> Y. Saad, M. Schultz, </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 7: </volume> <month> 856-869 </month> <year> (1986). </year>
Reference-contexts: We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) <ref> [17] </ref>. The disadvantage of these methods is that they require much storage for the previous direction vectors. In order to reduce the necessary storage, in general restarted versions of the above algorithms are used which involve a fixed number k of previous direction vectors. <p> This is supported by the experimental results we obtained and which we comment on in Chapter 4. 2.6 The Generalized Minimal Residual Algorithm (GMRES) GMRES is a fairly well known and often successful method in the context of nonsymmetric sparse linear systems <ref> [17] </ref>. Because we consider it in our experiments for purposes of comparison, we will give a short description here. However, our focus is on the preceding methods. For more detailed information, the reader is referred to [17]. It is based on maintaining the orthogonality property, instead of the 3-term recurrence. <p> well known and often successful method in the context of nonsymmetric sparse linear systems <ref> [17] </ref>. Because we consider it in our experiments for purposes of comparison, we will give a short description here. However, our focus is on the preceding methods. For more detailed information, the reader is referred to [17]. It is based on maintaining the orthogonality property, instead of the 3-term recurrence. Consequently, we need to determine an orthonormal basis fv 1 ; v 2 ; :::; v k g of the Krylov subspace K k which can be generated using Arnoldi's method.
Reference: [18] <institution> Scientific Computing Associates, Inc., PCGPAK User's Guide, </institution> <address> New Haven, Conn. </address> <year> (1987). </year>
Reference-contexts: of normal equations: A T Ax = A T b (2:107) AA T y = b; x = A T y (2:108) where we will denote CG applied to A T Ax = A T b by CGNR and CG applied to AA T y = b by CGNE [6] <ref> [18] </ref>. The matrices of the above systems A T A and AA T are symmetric and positive definite, and therefore the classical conjugate gradient method can be successfully applied to them. Theoretically, no division by zero will occur, and it will terminate within at most n iterations.
Reference: [19] <author> A. Sherman, </author> <title> An empirical investigation of methods for nonsymmetric linear systems, Elliptic Problem Solvers, 429-434, </title> <editor> ed. M. Schultz, </editor> <publisher> Academic Press, </publisher> <address> New York (1981). </address>
Reference-contexts: Problem 5: u xx + u yy + u zz + 100xu x yu y + zu z + 100 xyz with solution u = exp (xyz) sin (x) sin (y) sin (z) This problem <ref> [19] </ref> leads to an extremely ill-conditioned linear system with an indefinte symmetric part.
Reference: [20] <author> A. van der Sluis, H. van der Vorst, </author> <title> The rate of convergence of conjugate gradients, </title> <journal> Numer. Math. </journal> <volume> 48: </volume> <month> 543-560 </month> <year> (1986). </year>
Reference-contexts: The proof for this theorem can be found in [21]. The convergence behavior of the conjugate gradient algorithm has been investigated among others by [1] <ref> [20] </ref>. These considerations show the dependence of the rate of convergence of the spectrum of A. We state one important theorem here, which takes into account the condition number (A). More detailed theoretical convergence results that also consider the shape of the spectrum can be found in [20]. <p> others by [1] <ref> [20] </ref>. These considerations show the dependence of the rate of convergence of the spectrum of A. We state one important theorem here, which takes into account the condition number (A). More detailed theoretical convergence results that also consider the shape of the spectrum can be found in [20]. Theorem 2.3 Let A be symmetric positive definite, x k the k-th iterate of the CG-process (Algorithm 2.1) and x the solution of the linear system Ax = b.
Reference: [21] <author> P. Sonneveld, </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <volume> 10: </volume> <month> 36-52 </month> <year> (1989). </year>
Reference-contexts: In BICG [7], the approximations are constructed in such a way that the residuals are orthogonal to some "pseudo"-residuals involving A T . This leads to two three-term recursions for the residuals and the "pseudo"-residuals. In CGS <ref> [21] </ref>, both residuals and "pseudo"-residuals of BICG are used to generate new residuals, in order to take advantage of the convergence of the "pseudo"-residuals which is ignored in BICG. The corresponding residual polynomials of CGS are the squared residual polynomials of BICG. <p> Then as long as the algorithm does not break down due to division by zero, the polynomials i and i satisfy (i) [ i ; j ] = 0 for i 6= j. The proof for this theorem can be found in <ref> [21] </ref>. The convergence behavior of the conjugate gradient algorithm has been investigated among others by [1] [20]. These considerations show the dependence of the rate of convergence of the spectrum of A. We state one important theorem here, which takes into account the condition number (A). <p> The two equations above are completely equivalent which, shows us that the amount of work in BICG is doubled, and a lot of the operations performed possibly could be avoided or made better use of. This was also realized by Sonneveld <ref> [21] </ref>, who saw that while the residuals r i converge toward zero, the ~r i converge about equally fast toward zero, but the algorithm does not exploit this. <p> He tried to make use of this fact in the conjugate gradient squared algorithm which is described in the next section. 2.3 The Conjugate Gradient Squared (CGS) Algorithm Sonneveld <ref> [21] </ref> derived the CGS algorithm from BICG.
Reference: [22] <author> H. Van der Vorst, </author> <title> BI-CGSTAB: a fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <volume> 13:, </volume> <month> 631-644 </month> <year> (1992). </year>
Reference-contexts: In CGS [21], both residuals and "pseudo"-residuals of BICG are used to generate new residuals, in order to take advantage of the convergence of the "pseudo"-residuals which is ignored in BICG. The corresponding residual polynomials of CGS are the squared residual polynomials of BICG. BICGSTAB <ref> [22] </ref> is an attempt to stabilize CGS by replacing the squared residual polynomial with the product of two polynomials, one of which can be selected suitably. If we use a good preconditioner, these methods can give high convergence, often, however, they fail completely. <p> In order to stabilize this kind of behavior, which is mainly caused by squaring the residual polynomials, Van der Vorst suggested a new algorithm called BICGSTAB (CGS stabilized) <ref> [22] </ref>, in which the original residual polynomial of CG is replaced by the product of an arbitrary polynomial and the residual polynomial instead of the squared residual polynomial.
Reference: [23] <author> P. Vinsome, Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations, </title> <booktitle> Proc. Fourth Symp. on Reservoir Simulation, </booktitle> <pages> 149-159, </pages> <institution> Society of Petr. Eng. of AIME (1976). </institution>
Reference-contexts: These methods turn out to be very robust in many cases, however, they often converge slowly. One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. This approach has been used in methods like Orthomin <ref> [23] </ref>, Orthodir, and other generalized conjugate gradient schemes [1] [4] [16] [24]. We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) [17].
Reference: [24] <author> D. Young, K. Jea, </author> <title> Generalized conjugate-gradient acceleration of nonsymmetrizable iterative methods, </title> <journal> Linear Algebra and Appl. </journal> <volume> 34: </volume> <month> 159-194 </month> <year> (1980). </year> <month> 54 </month>
Reference-contexts: One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. This approach has been used in methods like Orthomin [23], Orthodir, and other generalized conjugate gradient schemes [1] [4] [16] <ref> [24] </ref>. We will not consider these methods here. We will, however, consider another method which is theoretically equivalent to Orthodir and the Generalized Conjugate Residual method, but more robust, the generalized minimal residual algorithm (GMRES) [17].
References-found: 24

