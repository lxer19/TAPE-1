URL: ftp://ftp.cs.yale.edu/pub/hager/tutorial.ps.gz
Refering-URL: http://www.cs.cmu.edu/~rll/talks/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: seth@uiuc.edu  Email: hager@cs.yale.edu  pic@brb.dmt.csiro.au  
Phone: Phone: 203 432-6432  
Title: A Tutorial on Visual Servo Control  
Author: Seth Hutchinson Greg Hager Peter Corke 
Date: May 14, 1996  
Address: 405 N. Mathews Avenue Urbana, IL 61801  New Haven, CT 06520-8285  P.O. Box 883, Kenmore. Australia, 4069.  
Affiliation: Department of Electrical and Computer Engineering The Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign  Department of Computer Science Yale University  CSIRO Division of Manufacturing Technology  
Abstract: This paper provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordinate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed. Since any visual servo system must be capable of tracking image features in a sequence of images, we include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Shirai and H. Inoue, </author> <title> "Guiding a robot by visual feedback in assembling tasks," </title> <journal> Pattern Recognition, </journal> <volume> vol. 5, </volume> <pages> pp. 99-108, </pages> <year> 1973. </year>
Reference-contexts: Unlike the manufacturing application, it will not be cost effective to re-engineer `our world' to suit the robot. Vision is a useful robotic sensor since it mimics the human sense of vision and allows for noncontact measurement of the environment. Since the seminal work of Shirai and Inoue <ref> [1] </ref> (who describe how a visual feedback loop can be used to correct the position of a robot to increase task accuracy), considerable effort has been devoted to the visual control of robot manipulators. Robot controllers with fully integrated vision systems are now available from a number of vendors.
Reference: [2] <author> J. Hill and W. T. Park, </author> <title> "Real time control of a robot with a mobile camera," </title> <booktitle> in Proc. 9th ISIR, </booktitle> <address> (Washington, DC), </address> <pages> pp. 233-246, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Taken to the extreme, machine vision can provide closed-loop position control for a robot end-effector | this is referred to as visual servoing. This term appears to have been first introduced by Hill and Park <ref> [2] </ref> in 1979 to distinguish their approach from earlier `blocks world' experiments where the system alternated between picture taking and moving. Prior to the introduction of this term, the less specific term visual feedback was generally used.
Reference: [3] <author> P. Corke, </author> <title> "Visual control of robot manipulators | a review," </title> <editor> in Visual Servoing (K. Hashimoto, ed.), </editor> <volume> vol. </volume> <booktitle> 7 of Robotics and Automated Systems, </booktitle> <pages> pp. 1-31, </pages> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: A comprehensive review of the literature in this field, as well the history and applications reported to date, is given by Corke <ref> [3] </ref> and includes a large bibliography. Visual servoing is the fusion of results from many elemental areas including high-speed image processing, kinematics, dynamics, control theory, and real-time computing. <p> Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in <ref> [3] </ref>. In less structured situations, vision has typically relied on the extraction of sharp contrast changes, referred to as "corners" or "edges", to indicate the presence of object boundaries or surface markings in an image.
Reference: [4] <author> A. C. Sanderson and L. E. Weiss, </author> <title> "Image-based visual servo control using relational graph error signals," </title> <booktitle> Proc. IEEE, </booktitle> <pages> pp. 1074-1077, </pages> <year> 1980. </year>
Reference-contexts: The remainder of this article is structured as follows. Section 2 establishes a consistent nomenclature and reviews the relevant fundamentals of coordinate transformations, pose representation, and image formation. In Section 3, we present a taxonomy of visual servo control systems (adapted from <ref> [4] </ref>). The two major classes of systems, position-based visual servo systems and image-based visual servo systems, are discussed in Sections 4 and 5 respectively. <p> computer vision community (good solutions to the calibration problem can be found in a number of references, e.g., [24-26]). 1 The word target will be used to refer to the object of interest, that is, the object that will be tracked. 9 3 Servoing Architectures In 1980, Sanderson and Weiss <ref> [4] </ref> introduced a taxonomy of visual servo systems, into which all subsequent visual servo systems can be categorized. Their scheme essentially poses two questions: 1.
Reference: [5] <author> J. C. Latombe, </author> <title> Robot Motion Planning. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Since the task space is merely the configuration space of the robot tool, the task space is a smooth m-manifold (see, e.g., <ref> [5] </ref>). If the tool is a single rigid body moving arbitrarily in a three-dimensional workspace, then T = SE 3 = &lt; 3 fi SO 3 , and m = 6. In some applications, the task space may be restricted to a subspace of SE 3 .
Reference: [6] <author> J. J. Craig, </author> <title> Introduction to Robotics. </title> <address> Menlo Park: </address> <publisher> Addison Wesley, </publisher> <editor> second ed., </editor> <year> 1986. </year>
Reference-contexts: In this case, we often prefer to parameterize a pose using a translation vector and three angles, (e.g., roll, pitch and yaw <ref> [6] </ref>). Although such parameterizations are inherently local, it is often convenient to represent a pose by a vector r 2 &lt; 6 , rather than by x e 2 T . This notation can easily be adapted to the case where T SE 3 . <p> x c ffi c ^ x t : In order to compute a velocity screw, we first note that the rotation matrix e fl R e can be represented as a rotation through an angle e fl e about an axis defined by a unit vector e fl k e <ref> [6] </ref>. Thus, we can define = k 1 e fl ^ k e (31) e fl where t e is the origin of the end-effector frame in base coordinates.
Reference: [7] <author> B. K. P. Horn, </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This is illustrated in Figure 1. Perspective Projection. Assuming that the projective geometry of the camera is modeled by perspective projection (see, e.g., <ref> [7] </ref>), a point, c P = [x; y; z] T , whose coordinates are expressed with respect to the camera coordinate frame, will project onto the image plane with coordinates p = [u; v] T , given by (x; y; z) = u # " y (15) If the coordinates of
Reference: [8] <author> W. Jang, K. Kim, M. Chung, and Z. </author> <title> Bien, "Concepts of augmented image space and transformed feature space for efficient visual servoing of an "eye-in-hand robot"," </title> <journal> Robotica, </journal> <volume> vol. 9, </volume> <pages> pp. 203-212, </pages> <year> 1991. </year>
Reference-contexts: We define an image feature parameter to be any real-valued quantity that can be calculated from one or more image features. Examples include, moments, relationships between regions or vertices, and polygon face areas. Jang <ref> [8] </ref> provides a formal definition of what we term feature paramters as image functionals. Most commonly the coordinates of a feature point or a region centroid are used.
Reference: [9] <author> J. Feddema and O. Mitchell, </author> <title> "Vision-guided servoing with feature-based trajectory generation," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 5, </volume> <pages> pp. 691-700, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket <ref> [9] </ref> or a contrived pattern [10, 11]. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [10, 12-17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [9, 18] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane <p> The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [9, 12, 13, 22] </ref>. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. In visual servoing we are interested in determining the manipulator velocity, _ r, required to achieve some desired value of _ f . <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [9, 19] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> By observing visual cues such as the ball, the arm's pivot point, and another point on the arm, the interception task can be specified, even if the relationship between camera and arm is not known a priori. Feddema <ref> [9] </ref> uses a feature space trajectory generator to interpolate feature parameter 26 values due to the low update rate of the vision system used. 6 Image Feature Extraction and Tracking Irrespective of the control approach used, a vision system is required to extract the information needed to perform the servoing task. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [9, 12, 54, 69] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [10] <author> B. Espiau, F. Chaumette, and P. Rives, </author> <title> "A New Approach to Visual Servoing in Robotics," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 8, </volume> <pages> pp. 313-326, </pages> <year> 1992. </year>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [9] or a contrived pattern <ref> [10, 11] </ref>. <p> Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image <ref> [10, 12-17] </ref>, the distance between two points in the image plane and the orientation of the line connecting those two points [9, 18], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected <p> and the orientation of the line connecting those two points [9, 18], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> 18], perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane <ref> [10] </ref>, and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. In order to perform visual servo control, we must select a set of image feature parameters. <p> The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [10, 33] </ref>, the kinematic error function can be 12 thought of as representing a virtual kinematic constraint between the end-effector and the target. <p> Thus, the number of columns in the image 20 Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix <ref> [10] </ref> and the B matrix [14, 15]. Other applications of the image Jacobian include [9, 12, 13, 22]. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose.
Reference: [11] <author> M. L. Cyros, </author> <title> "Datacube at the space shuttle's launch pad," </title> <journal> Datacube World Review, </journal> <volume> vol. 2, </volume> <pages> pp. 1-3, </pages> <month> Sept. </month> <year> 1988. </year> <institution> Datacube Inc., </institution> <address> 4 Dearborn Road, Peabody, MA. </address>
Reference-contexts: Most commonly the coordinates of a feature point or a region centroid are used. A good feature point is one that can be located unambiguously in different views of the scene, such as a hole in a gasket [9] or a contrived pattern <ref> [10, 11] </ref>.
Reference: [12] <author> A. Castano and S. A. Hutchinson, </author> <title> "Visual compliance: </title> <journal> Task-directed visual servo control," IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 10, </volume> <pages> pp. 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [9, 12, 13, 22] </ref>. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. In visual servoing we are interested in determining the manipulator velocity, _ r, required to achieve some desired value of _ f . <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality <ref> [12] </ref>. 5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [9, 19] were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> For the case of a non-square image Jacobian, the techniques described in Section 5.3 would be used to compute for u. Similar results have been presented in <ref> [12, 13] </ref>. 5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [9, 12, 54, 69] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> These include control issues, such as adaptive visual servo control [14, 85], hybrid control (e.g., hybrid vision/position control <ref> [12] </ref>, or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78].
Reference: [13] <author> K. Hashimoto, T. Kimoto, T. Ebine, and H. Kimura, </author> <title> "Manipulator control with image-based visual servo," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2267-2272, </pages> <year> 1991. </year>
Reference-contexts: The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [9, 12, 13, 22] </ref>. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. In visual servoing we are interested in determining the manipulator velocity, _ r, required to achieve some desired value of _ f . <p> For the case of a non-square image Jacobian, the techniques described in Section 5.3 would be used to compute for u. Similar results have been presented in <ref> [12, 13] </ref>. 5.5 Example Servoing Tasks In this section, we revisit the problems that were described in Section 4.1. Here, we describe image-based solutions for these problems. Point to Point Positioning Consider the task of bringing some point P on the manipulator to a desired stationing point S.
Reference: [14] <author> N. P. Papanikolopoulos and P. K. Khosla, </author> <title> "Adaptive Robot Visual Tracking: Theory and Experiments," </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> vol. 38, no. 3, </volume> <pages> pp. 429-445, </pages> <year> 1993. </year>
Reference-contexts: The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [9, 12, 13, 22]. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. <p> In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating <ref> [14] </ref> this depth value, or develop feedback methods which do not use depth in the feedback formulation [67]. There are often computational advantages to image-based control, particularly in ECL configurations. <p> These include control issues, such as adaptive visual servo control <ref> [14, 85] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78].
Reference: [15] <author> N. P. Papanikolopoulos, P. K. Khosla, and T. Kanade, </author> <title> "Visual Tracking of a Moving Target by a Camera Mounted on a Robot: A Combination of Vision and Control," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 14-35, </pages> <year> 1993. </year> <month> 37 </month>
Reference-contexts: The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix <ref> [14, 15] </ref>. Other applications of the image Jacobian include [9, 12, 13, 22]. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. <p> This burden can be reduced by performing the optimization starting at low resolution and proceeding to higher resolution, and by ordering the candidates in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. <p> in D from most to least likely and terminating the search once a candidate with an acceptably low SSD value is found <ref> [15] </ref>. Once the discrete minimum is found, the location can be refined to subpixel accuracy by interpolation of the SSD values about the minimum. Even with these improvements, [15] reports that a special signal processor is required to attain frame-rate performance. It is also possible to solve (59) using continuous optimization methods [76-79].
Reference: [16] <author> S. Skaar, W. Brockman, and R. Hanson, </author> <title> "Camera-space manipulation," </title> <journal> Int. J. Robot. Res., </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 20-32, </pages> <year> 1987. </year>
Reference-contexts: Jang et al. [68] describe a generalized approach to servoing on image features, with trajectories specified in feature space leading to trajectories (tasks) that are independent of target geometry. Skaar et al. <ref> [16] </ref> describes the example of a 1DOF robot catching a ball. By observing visual cues such as the ball, the arm's pivot point, and another point on the arm, the interception task can be specified, even if the relationship between camera and arm is not known a priori.
Reference: [17] <author> S. B. Skaar, W. H. Brockman, and W. S. Jang, </author> <title> "Three-Dimensional Camera Space Manipulation," </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 22-39, </pages> <year> 1990. </year>
Reference: [18] <author> J. T. Feddema, C. S. G. Lee, and O. R. Mitchell, </author> <title> "Weighted selection of image features for resolved rate visual feedback control," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 7, </volume> <pages> pp. 31-47, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [10, 12-17], the distance between two points in the image plane and the orientation of the line connecting those two points <ref> [9, 18] </ref>, perceived edge length [19], the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane <p> We now discuss each of these. When k = m and J v is nonsingular, J 1 v exists. Therefore, in this case, _ r = J 1 v _ f . Such an approach has been used by Feddema <ref> [18] </ref>, who also describes an automated approach to image feature selection in order to minimize the condition number of J v . When k 6= m, J 1 v does not exist. <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [18, 19] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate.
Reference: [19] <author> A. C. Sanderson, L. E. Weiss, and C. P. Neuman, </author> <title> "Dynamic sensor-based control of robots with visual feedback," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. RA-3, </volume> <pages> pp. 404-417, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Image feature parameters that have been used for visual servo control include the image plane coordinates of points in the image [10, 12-17], the distance between two points in the image plane and the orientation of the line connecting those two points [9, 18], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. <p> the image plane coordinates of points in the image [10, 12-17], the distance between two points in the image plane and the orientation of the line connecting those two points [9, 18], perceived edge length <ref> [19] </ref>, the area of a projected surface and the relative areas of two projected surfaces [19], the centroid and higher order moments of a projected surface [19-22], the parameters of lines in the image plane [10], and the parameters of an ellipse in the image plane [10]. In this tutorial we will restrict our attention to point features whose parameters are their image plane coordinates. <p> Thus, the number of columns in the image 20 Jacobian will vary depending on the task. The image Jacobian was first introduced by Weiss et al. <ref> [19] </ref>, who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include [9, 12, 13, 22]. <p> The null space of the image Jacobian plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control <ref> [9, 19] </ref> were based on resolved-rate motion control [28], which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d . <p> Discussion of the issues related to feature selection for visual servo control applications can be found in <ref> [18, 19] </ref>. The "right" image feature tracking method to use is extremely application dependent. For example, if the goal is to track a single special pattern or surface marking that is approximately planar and moving at slow to moderate speeds, then SSD tracking is appropriate.
Reference: [20] <author> R. L. Andersson, </author> <title> A Robot Ping-Pong Player. Experiment in Real-Time Intelligent Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [21] <author> M. Lei and B. K. Ghosh, </author> <title> "Visually-Guided Robotic Motion Tracking," </title> <booktitle> in Proc. Thirtieth Annual Allerton Conference on Communication, Control, and Computing, </booktitle> <pages> pp. 712-721, </pages> <year> 1992. </year>
Reference: [22] <author> B. Yoshimi and P. K. Allen, </author> <title> "Active, uncalibrated visual servoing," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <address> (San Diego, CA), </address> <pages> pp. 156-161, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The image Jacobian was first introduced by Weiss et al. [19], who referred to it as the feature sensitivity matrix. It is also referred to as the interaction matrix [10] and the B matrix [14, 15]. Other applications of the image Jacobian include <ref> [9, 12, 13, 22] </ref>. The relationship given by (35) describes how image feature parameters change with respect to changing manipulator pose. In visual servoing we are interested in determining the manipulator velocity, _ r, required to achieve some desired value of _ f .
Reference: [23] <author> B. Nelson and P. K. Khosla, </author> <title> "Integrating Sensor Placement and Visual Tracking Strategies," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 1351-1356, </pages> <year> 1994. </year>
Reference-contexts: A variant of this is for the camera to be agile, mounted on another robot or pan/tilt head in order to observe the visually controlled robot from the best vantage <ref> [23] </ref>. For either choice of camera configuration, prior to the execution of visual servo tasks, camera calibration must be performed. For the eye-in-hand case, this amounts to determining e x c . For the fixed camera case, calibration is used to determine 0 x c .
Reference: [24] <author> I. E. Sutherland, </author> <title> "Three-dimensional data input by tablet," </title> <journal> Proc. IEEE, </journal> <volume> vol. 62, </volume> <pages> pp. 453-461, </pages> <month> Apr. </month> <year> 1974. </year>
Reference: [25] <author> R. Tsai and R. Lenz, </author> <title> "A new technique for fully autonomous and efficient 3D robotics hand/eye calibra tion," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 5, </volume> <pages> pp. 345-358, </pages> <month> June </month> <year> 1989. </year>
Reference: [26] <author> R. Tsai, </author> <title> "A versatile camera calibration technique for high accuracy 3-D machine vision m etrology using off-the-shelf TV cameras and lenses," </title> <journal> IEEE Trans. Robot. Autom., </journal> <volume> vol. 3, </volume> <pages> pp. 323-344, </pages> <month> Aug. </month> <year> 1987. </year>
Reference: [27] <author> P. I. Corke, </author> <title> High-Performance Visual Closed-Loop Robot Control. </title> <type> PhD thesis, </type> <institution> University of Melbourne, Dept.Mechanical and Manufacturing Engineering, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: First, the relatively low sampling rates available from vision make direct control of a robot end-effector with complex, nonlinear dynamics an extremely challenging control problem. Using internal feedback with a high sampling rate generally presents the visual controller with idealized axis dynamics <ref> [27] </ref>. Second, many robots already have an interface for accepting Cartesian velocity or incremental position commands. This simplifies the construction of the visual servo system, and also makes the methods more portable. <p> A good example of this is the common Unimate Puma robot whose position loops operate at a sample interval of 14 or 28 ms while vision systems operate at sample intervals of 33 or 40 ms for RS 170 or CCIR video respectively <ref> [27] </ref>. It is well known that a feedback system including delay will become unstable as the loop gain is increased. Many visual closed-loop systems are tuned empirically, increasing the loop gain until overshoot or oscillation becomes intolerable. <p> Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [27, 82] </ref>. 7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator. For mobile robots the pose of the robot is generally poorly known and can be estimated from the relative pose of known fixed objects or landmarks.
Reference: [28] <author> D. E. Whitney, </author> <title> "The mathematics of coordinated control of prosthetic arms and manipulators," </title> <journal> Journal of Dynamic Systems, Measurement and Control, </journal> <volume> vol. 122, </volume> <pages> pp. 303-309, </pages> <month> Dec. </month> <year> 1972. </year>
Reference-contexts: This simplifies the construction of the visual servo system, and also makes the methods more portable. Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate <ref> [28] </ref> controllers have specialized mechanisms for dealing with kinematic singularities [29], the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control. <p> plays a significant role in hybrid methods, in which some degrees of freedom are controlled using visual servo, while the remaining degrees of freedom are controlled using some other modality [12]. 5.4 Resolved-Rate Methods The earliest approaches to image-based visual servo control [9, 19] were based on resolved-rate motion control <ref> [28] </ref>, which we will briefly describe here. Suppose that the goal of a particular task is to reach a desired image feature parameter vector, f d .
Reference: [29] <author> S. Chieaverini, L. Sciavicco, and B. Siciliano, </author> <title> "Control of robotic systems through singulari-ties," in Proc. Int. Workshop on Nonlinear and Adaptive Control: Issues i n Robotics (C. </title> <editor> C. de Wit, ed.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Thirdly, look-and-move separates the kinematic singularities of the mechanism from the visual controller, allowing the robot to be considered as an ideal Cartesian motion device. Since many resolved rate [28] controllers have specialized mechanisms for dealing with kinematic singularities <ref> [29] </ref>, the system design is again greatly simplified. In this article, we will utilize the look-and-move model exclusively. The second major classification of systems distinguishes position-based control from image-based control.
Reference: [30] <author> S. Wijesoma, D. Wolfe, and R. Richards, </author> <title> "Eye-to-hand coordination for vision-guided robot control applications," </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 12, no. 1, </volume> <pages> pp. 65-78, </pages> <year> 1993. </year>
Reference: [31] <author> N. Hollinghurst and R. Cipolla, </author> <title> "Uncalibrated stereo hand eye coordination," </title> <journal> Image and Vision Computing, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 187-192, </pages> <year> 1994. </year> <month> 38 </month>
Reference-contexts: For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla <ref> [31] </ref> describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance.
Reference: [32] <author> G. D. Hager, W.-C. Chang, and A. S. Morse, </author> <title> "Robot hand-eye coordination based on stereo vision," </title> <journal> IEEE Control Systems Magazine, </journal> <month> Feb. </month> <year> 1995. </year>
Reference: [33] <author> C. Samson, M. Le Borgne, and B. Espiau, </author> <title> Robot Control: The Task Function Approach. </title> <publisher> Oxford, </publisher> <address> England: </address> <publisher> Clarendon Press, </publisher> <year> 1992. </year>
Reference-contexts: The value d will be referred to as the degree of the constraint. As noted by Espiau et al. <ref> [10, 33] </ref>, the kinematic error function can be 12 thought of as representing a virtual kinematic constraint between the end-effector and the target.
Reference: [34] <author> G. Franklin, J. Powell, and A. Emami-Naeini, </author> <title> Feedback Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <editor> 2nd ed., </editor> <year> 1991. </year>
Reference-contexts: This regulator produces at every time instant a desired end-effector velocity screw u 2 &lt; 6 which is sent to the robot control subsystem. For the purposes of this section, we use simple proportional control methods for linear and linearized systems to compute u <ref> [34] </ref>. These methods are illustrated below, and are discussed in more detail in Section 5. We now present examples of positioning tasks for end-effector and fixed cameras in both ECL and EOL configurations. In Section 4.1, several examples of positioning tasks based on directly observable features are presented. <p> pp ( ^ x e ; ^ x c ffi c b S; e P) = k x e ffi e P ^ x c ffi c b S : (20) will drive the system to an equilibrium state in which the estimated value of the error function is zero <ref> [34] </ref>. The value k &gt; 0 is a proportional feedback gain. Note that we have written ^ x e in the feedback law to emphasize the fact that this value is also subject to errors.
Reference: [35] <author> G. D. Hager, </author> <title> "Six DOF visual control of relative position," </title> <institution> DCS RR-1038, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Full six degree-of-freedom positioning can be attained by enforcing another point-to-line constraint using an additional point on the end-effector and an additional point in the world. See <ref> [35] </ref> for details. These formulations can be adjusted for end-effector mounted camera and can be implemented as ECL or EOL systems. We leave these modifications as an exercise for the reader. 4.2 Pose-Based Tasks In the previous section, positioning was defined in terms of directly observable features.
Reference: [36] <author> T. S. Huang and A. N. Netravali, </author> <title> "Motion and structure from feature correspondences: A review," </title> <journal> IEEE Proceeding, </journal> <volume> vol. 82, no. 2, </volume> <pages> pp. 252-268, </pages> <year> 1994. </year>
Reference-contexts: This encompasses problems including structure from motion, exterior orientation, stereo reconstruction, and absolute orientation. A comprehensive discussion of these topics can be found in a recent review article <ref> [36] </ref>. We divide the estimation problems that arise into single-camera and multiple-camera situations which will be discussed in the following sections. 16 4.3.1 Single Camera As noted previously, it follows from (15) that a point in a single camera image corresponds to a line in space. <p> Although it is possible to perform geometric reconstruction using a single moving camera, the equations governing this process are often ill-conditioned, leading to stability problems <ref> [36] </ref> Better results can be achieved if target features have some internal structure, or the features come from a known object. Below, we briefly describe methods for performing both point estimation and pose estimation with a single camera assuming such information is available.
Reference: [37] <author> W. Wilson, </author> <title> "Visual servo control of robots using kalman filter estimates of robot pose relative to work-pieces," in Visual Servoing (K. Hashimoto, </title> <publisher> ed.), </publisher> <pages> pp. 71-104, </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson <ref> [37] </ref> for six DOF control of end-effector pose. A similar approach was recently reported in [38]. Briefly, such an approach proceeds as follows. <p> A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to <ref> [37] </ref> for details. 4.3.2 Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration [39] or Kalman filtering <ref> [37] </ref>. Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [37, 53, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [38] <author> C. Fagerer, D. Dickmanns, and E. Dickmanns, </author> <title> "Visual grasping with long delay time of a free floating object in orbit," </title> <booktitle> Autonomous Robots, </booktitle> <volume> vol. 1, no. 1, </volume> <year> 1994. </year>
Reference-contexts: Object Pose Accurate object pose estimation is possible if the vision system observes features of a known object, and uses those features to estimate object pose. This approach has been recently demonstrated by Wilson [37] for six DOF control of end-effector pose. A similar approach was recently reported in <ref> [38] </ref>. Briefly, such an approach proceeds as follows. Let t P 1 ; t P 2 ; : : : t P n be a set of points expressed in an object coordinate system with unknown pose c x t relative to an observing camera.
Reference: [39] <author> C. Lu, E. J. Mjolsness, and G. D. Hager, </author> <title> "Online computation of exterior orientation with application to hand-eye calibration," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Aug. </month> <year> 1994. </year> <note> To appear in Mathematical and Computer Modeling. </note>
Reference-contexts: Numerous methods of solution have been proposed and <ref> [39] </ref> provides a recent review of several techniques. Broadly speaking, solutions divide into analytic solutions and least-squares solutions which employ a variety of simplifications and/or iterative methods. Analytic solutions for three and four points are given by [40-44]. Unique solutions exist for four coplanar, but not collinear, points. <p> Computation time for the relative orientation problem is often cited as a disadvantage of position-based methods. However recent results show that solutions can be computed in only a few milliseconds even using iteration <ref> [39] </ref> or Kalman filtering [37]. Endpoint closed-loop systems are demonstrably less sensitive to calibration. However, particularly in stereo systems, small rotational errors between the cameras can lead to reconstruction errors which do impact the positioning accuracy of the system.
Reference: [40] <author> M. A. Fischler and R. C. Bolles, </author> <title> "Random sample consensus: a paradigm for model fitting with applicatio ns to image analysis and automated cartography," </title> <journal> Communications of the ACM, </journal> <volume> vol. 24, </volume> <pages> pp. 381-395, </pages> <month> June </month> <year> 1981. </year>
Reference: [41] <author> R. M. Haralick, C. Lee, K. Ottenberg, and M. Nolle, </author> <title> "Analysis and solutions of the three point perspective pose estimation problem," </title> <booktitle> in Proc. IEEE Conf. Computer Vision Pat. Rec., </booktitle> <pages> pp. 592-598, </pages> <year> 1991. </year>
Reference: [42] <author> D. DeMenthon and L. S. Davis, </author> <title> "Exact and approximate solutions of the perspective-three-point problem," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 11, </volume> <pages> pp. 1100-1105, </pages> <year> 1992. </year>
Reference: [43] <author> R. Horaud, B. Canio, and O. Leboullenx, </author> <title> "An analytic solution for the perspective 4-point problem," </title> <journal> Computer Vis. Graphics. Image Process, </journal> <volume> no. 1, </volume> <pages> pp. 33-44, </pages> <year> 1989. </year>
Reference: [44] <author> M. Dhome, M. Richetin, J. Lapreste, and G. Rives, </author> <title> "Determination of the attitude of 3-D objects from a single perspective view," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 12, </volume> <pages> pp. 1265-1278, </pages> <year> 1989. </year>
Reference: [45] <author> G. H. </author> <title> Rosenfield, "The problem of exterior orientation in photogrammetry," </title> <booktitle> Photogrammetric Engineering, </booktitle> <pages> pp. 536-553, </pages> <year> 1959. </year>
Reference: [46] <author> D. G. Lowe, </author> <title> "Fitting parametrized three-dimensional models to images," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 5, </volume> <pages> pp. 441-450, </pages> <year> 1991. </year>
Reference: [47] <author> R. Goldberg, </author> <title> "Constrained pose refinement of parametric objects," </title> <journal> Intl. J. Computer Vision, </journal> <volume> no. 2, </volume> <pages> pp. 181-211, </pages> <year> 1994. </year>
Reference: [48] <author> R. Kumar, </author> <title> "Robust methods for estimating pose and a sensitivity analysis," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> no. 3, </volume> <pages> pp. 313-342, </pages> <year> 1994. </year>
Reference: [49] <author> S. Ganapathy, </author> <title> "Decomposition of transformation matrices for robot vision," </title> <journal> Pattern Recognition Letters, </journal> <pages> pp. 401-412, </pages> <year> 1989. </year>
Reference-contexts: Unique solutions exist for four coplanar, but not collinear, points. Least-squares solutions can be found in [45-51]. Six or more points always yield unique solutions. The camera calibration matrix can be computed from features on the target, then decomposed <ref> [49] </ref> to yield the target's pose. The least-squares solution proceeds as follows.
Reference: [50] <author> M. Fischler and R. C. Bolles, </author> <title> "Random sample consensus: A paradigm for model fitting and automatic cartography," </title> <journal> Commun. ACM, </journal> <volume> no. 6, </volume> <pages> pp. 381-395, </pages> <year> 1981. </year>
Reference: [51] <author> Y. Liu, T. S. Huang, and O. D. Faugeras, </author> <title> "Determination of camera location from 2-D to 3-D line and point correspondences," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> no. 1, </volume> <pages> pp. 28-37, </pages> <year> 1990. </year>
Reference: [52] <author> A. Gelb, ed., </author> <title> Applied Optimal Estimation. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1974. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter <ref> [52] </ref>. The reader is referred to [37] for details. 4.3.2 Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision.
Reference: [53] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, </author> <title> "Automated Tracking and Grasping of a Moving Object with a Robotic Hand-Eye System," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 152-165, </pages> <year> 1993. </year>
Reference-contexts: A particularly elegant formulation of this updating procedure results by application of statistical techniques such as the extended Kalman filter [52]. The reader is referred to [37] for details. 4.3.2 Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen <ref> [53] </ref> shows a system which can grasp a toy train using stereo vision. Rizzi [54] demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. <p> To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth [9, 12, 54, 69]. Other authors use extremely task-specific clues: e:g: Allen <ref> [53] </ref> uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. A review of tracking approaches used by researchers in this field is given in [3]. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [37, 53, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [54] <author> A. Rizzi and D. Koditschek, </author> <title> "An active visual estimator for dexterous manipulation," </title> <booktitle> in Proceedings, IEEE International Conference on Robotics and Automaton, </booktitle> <year> 1994. </year>
Reference-contexts: The reader is referred to [37] for details. 4.3.2 Multiple Cameras Many systems utilizing position-based control with stereo vision from free-standing cameras have been demonstrated. For example, Allen [53] shows a system which can grasp a toy train using stereo vision. Rizzi <ref> [54] </ref> demonstrates a system which can bounce a ping-pong ball. All of these systems are EOL. Cipolla [31] describes an ECL system using free-standing stereo cameras. One novel feature of this system is the use of the affine projection model (Section 2.3) for the imaging geometry. <p> Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [9, 12, 54, 69] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip. <p> However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [54, 70, 71] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. We note, however, that initialization of window or region-based systems typically presupposes an automated or human-supplied solution to a potentially complex vision problem. <p> For example, Rizzi <ref> [54] </ref> describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used [37, 53, 72]. <p> If the contrast between the interior of the opening and area around it is high, then binary thresholding followed by a calculation of the first and second central moments can be used to localize the feature <ref> [54] </ref>. 2. If the ambient illumination changes greatly over time, but the brightness of the opening and the brightness of the surrounding region are roughly constant, a circular template could be localized using SSD methods augmented with brightness and contrast parameters. <p> During task execution, other problems arise. The two most common problems are occlusion of features and and visual singularities. Solutions to the former include intelligent observers that note the disappearance of features and continue to predict their locations based on dynamics and/or feedforward information <ref> [54] </ref>, or redundant feature specifications that can perform even with some loss of information. Solution to the latter require some combination of intelligent path planning and/or intelligent acquisition and focus-of-attention to maintain the controllability of the system.
Reference: [55] <author> J. Pretlove and G. Parker, </author> <title> "The development of a real-time stereo-vision system to aid robot guidance in carrying out a typical manufacturing task," </title> <booktitle> in Proc. 22nd ISRR, </booktitle> <address> (Detroit), </address> <pages> pp. </pages> <address> 21.1-21.23, </address> <year> 1991. </year>
Reference-contexts: One novel feature of this system is the use of the affine projection model (Section 2.3) for the imaging geometry. This leads to linear calibration and control at the cost of some system performance. The development of a position-based stereo eye-in-hand servoing system has also been reported <ref> [55] </ref>. Multiple cameras greatly simplify the reconstruction process as illustrated below.
Reference: [56] <author> B. K. P. Horn, H. M. Hilden, and S. Negahdaripour, </author> <title> "Closed-form solution of absolute orientation using orthonomal matrices," </title> <journal> J. Opt. Soc. Amer., </journal> <volume> vol. A-5, </volume> <pages> pp. 1127-1135, 198. </pages>
Reference: [57] <author> K. S. Arun, T. S. Huang, and S. D. Blostein, </author> <title> "Least-squares fitting of two 3-D point sets," </title> <journal> IEEE Trans. Pat. Anal. Machine Intell., </journal> <volume> vol. 9, </volume> <pages> pp. 698-700, </pages> <year> 1987. </year>
Reference: [58] <author> B. K. P. Horn, </author> <title> "Closed-form solution of absolute orientation using unit quaternion," </title> <journal> J. Opt. Soc. Amer., </journal> <volume> vol. A-4, </volume> <pages> pp. 629-642, </pages> <year> 1987. </year>
Reference: [59] <author> G. D. Hager, G. Grunwald, and G. Hirzinger, </author> <title> "Feature-based visual servoing and its application to telerobotics," </title> <institution> DCS RR-1010, Yale University, </institution> <address> New Haven, CT, </address> <month> Jan. </month> <year> 1994. </year> <note> To appear at the 1994 IROS Conference. </note>
Reference-contexts: However, it still may cause problems when both cameras are free to move relative to one another. Feature-based approaches tend to be more appropriate to tasks where there is no prior model of the geometry of the task, for example in teleoperation applications <ref> [59] </ref>. Pose-based approaches inherently depend on an existing object model. The pose estimation problems inherent in many position-based servoing problems requires solution to a potentially difficult correspondence problem.
Reference: [60] <author> G. Agin, </author> <title> "Calibration and use of a light stripe range sensor mounted on the hand of a robot," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 680-685, </pages> <year> 1985. </year>
Reference: [61] <author> S. Venkatesan and C. Archibald, </author> <title> "Realtime tracking in five degrees of freedom using two wrist-mounted laser range finders," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2004-2010, </pages> <year> 1990. </year>
Reference: [62] <author> J. Dietrich, G. Hirzinger, B. Gombert, and J. Schott, </author> <title> "On a unified concept for a new generation of light-weight robots," in Experimental Robotics 1 (V. </title> <editor> Hayward and O. Khatib, eds.), </editor> <volume> vol. </volume> <booktitle> 139 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pp. 287-295, </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year> <month> 40 </month>
Reference: [63] <author> J. Aloimonos and D. P. Tsakiris, </author> <title> "On the mathematics of visual tracking," </title> <journal> Image and Vision Computing, </journal> <volume> vol. 9, </volume> <pages> pp. 235-251, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>.
Reference: [64] <author> R. M. Haralick and L. G. Shapiro, </author> <title> Computer and Robot Vision. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Alternative derivations for this example can be found in a number of references including <ref> [63, 64] </ref>. <p> In this case, the solution is given by (47). For example, as shown in <ref> [64] </ref>, the null space of the image Jacobian given in (45), is spanned by the four vectors 2 6 6 6 4 v 0 0 7 7 7 7 2 6 6 6 4 0 u 7 7 7 7 2 6 6 6 4 (u 2 + 2 )z 2 <p> Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal. In one dimension the DOT is defined as g (x) = signum (x): 28 For a kernal three pixels wide, this is also known as the Prewitt operator <ref> [64] </ref>. Although the latter is not optimal from a signal processing point of view, convolution by the DOT can be implemented using only four additions per pixel. Thus, it is extremely fast to execute on simple hardware. Returning to detecting edge segments, convolutions are employed as follows.
Reference: [65] <author> F. W. Warner, </author> <title> Foundations of Differentiable Manifolds and Lie Groups. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: The least squares solution gives a value for _ r that minimizes the norm k _ f J v _ rk. We first consider the case k &gt; m, that is, there are more feature parameters than task degrees of freedom. By the implicit function theorem <ref> [65] </ref>, if, in some neighborhood of r, m k and rank (J v ) = m (i.e., J v is full rank), we can express the coordinates f m+1 : : : f k as smooth functions of f 1 : : : f m . &gt;From this, we deduce that
Reference: [66] <author> G. D. Hager, </author> <title> "Calibration-free visual control using projective invariance," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Dec. </month> <year> 1994. </year> <note> To appear Proc. ICCV '95. </note>
Reference-contexts: It is interesting to note that these solutions to the point-to-line problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. 5.6 Discussion One of the chief advantages to image-based control over position-based control is that the positioning accuracy of the system is less sensitive camera calibration. This is particularly true for ECL image-based systems. <p> This is particularly true for ECL image-based systems. For example, it is interesting to note that the ECL image-based solutions to the point-to-line positioning problem perform with an accuracy that is independent of calibration, whereas the position-based versions do not <ref> [66] </ref>. It is important to note, however, that most of the image-based control methods appearing in the literature still rely on an estimate of point position or target pose to parameterize the Jacobian. In practice, the unknown parameter for Jacobian calculation is distance from the camera.
Reference: [67] <author> D. Kim, A. Rizzi, G. Hager, and D. Koditschek, </author> <title> "A "robust" convergent visual servoing system." </title> <note> Submitted to Intelligent Robots and Systems 1995, </note> <year> 1994. </year>
Reference-contexts: In practice, the unknown parameter for Jacobian calculation is distance from the camera. Some recent papers present adaptive approaches for estimating [14] this depth value, or develop feedback methods which do not use depth in the feedback formulation <ref> [67] </ref>. There are often computational advantages to image-based control, particularly in ECL configurations. For example, a position-based relative pose solution for an ECL single-camera system must perform two nonlinear least squares optimizations in order to compute the error function.
Reference: [68] <author> W. Jang and Z. </author> <title> Bien, "Feature-based visual servoing of an eye-in-hand robot with improved tracking performance," </title> <booktitle> in Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2254-2260, </pages> <year> 1991. </year>
Reference-contexts: Many tasks can be described in terms of the motion of image features, for instance aligning visual cues in the scene. Jang et al. <ref> [68] </ref> describe a generalized approach to servoing on image features, with trajectories specified in feature space leading to trajectories (tasks) that are independent of target geometry. Skaar et al. [16] describes the example of a 1DOF robot catching a ball.
Reference: [69] <author> R. L. Anderson, </author> <title> "Dynamic sensing in a ping-pong playing robot," </title> <journal> IEEE Transaction on Robotics and Automation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 723-739, </pages> <year> 1989. </year>
Reference-contexts: Hence, visual servoing pre-supposes the solution to a set of potentially difficult static and dynamic vision problems. To this end many reported implementations contrive the vision problem to be simple: e:g: painting objects white, using artificial targets, and so forth <ref> [9, 12, 54, 69] </ref>. Other authors use extremely task-specific clues: e:g: Allen [53] uses motion detection for locating a moving object to be grasped, and welding systems commonly use special filters that isolate the image of the welding tip.
Reference: [70] <author> G. D. Hager, </author> <title> "The "X-Vision" system: A general purpose substrate for real-time vision-based robotics." </title> <booktitle> Submitted to the 1995 Workshop on Vision for Robotics, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [54, 70, 71] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. We note, however, that initialization of window or region-based systems typically presupposes an automated or human-supplied solution to a potentially complex vision problem. <p> In this case, (59) must also include parameters for scaling and aspect ratio <ref> [70] </ref>. 3. The opening could be selected in an initial image, and subsequently located using SSD methods. This differs from the previous method in that this calculation does not compute the center of the opening, only its correlation with the starting image. <p> It is probably safe to say that image processing presents the greatest challenge to general-purpose hand-eye coordination. As an effort to help overcome this obstacle, the methods described above and other related methods have been incorporated into a publically available "toolkit." The interested reader is referred to <ref> [70] </ref> for details. 7 Related Issues In this section, we briefly discuss a number of related issues that were not addressed in the tutorial. 7.1 Image-Based versus Position-Based Control The taxonomy of visual servo introduced in Section 1 has four major architectural classes.
Reference: [71] <author> E. Dickmanns and V. Graefe, </author> <title> "Dynamic monocular machine vision," </title> <journal> Machine Vision and Applications, </journal> <volume> vol. 1, </volume> <pages> pp. 223-240, </pages> <year> 1988. </year>
Reference-contexts: However not all pixels in the image are of interest, and computation time can be greatly reduced if only a small region around each image feature is processed. Thus, a promising technique for making vision cheap and tractable is to use window-based tracking techniques <ref> [54, 70, 71] </ref>. Window-based methods have several advantages, among them; computational simplicity, little requirement for special hardware, and easy reconfiguration for different applications. We note, however, that initialization of window or region-based systems typically presupposes an automated or human-supplied solution to a potentially complex vision problem.
Reference: [72] <author> O. Faugeras, </author> <title> Three-Dimensional Computer Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: In keeping with the minimalist approach of this tutorial, we concentrate on describing the window-based approach to tracking of features in an image. A discussion of methods which use specialized hardware combined with temporal and geometric constraints can be found in <ref> [72] </ref>. The remainder of this section is organized as follows. Section 6.1 describes how window-based methods can be used to implement fast detection of edge segments, a common low-level primitive for vision applications. Section 6.2 describe an approach based on temporally correlating image regions over time. <p> For example, Rizzi [54] describes the use of a Newtonian flight dynamics model to make it possible to track a ping-pong ball during flight. Predictors based on ff fi tracking filters and Kalman filters have also been used <ref> [37, 53, 72] </ref>. Multiresolution techniques can be used provide further performance improvements, particularly when a dynamic model is not available and large search windows must be used. 6.4 Discussion Prior to executing or planning visually controlled motions, a specific set of visual features must be chosen.
Reference: [73] <author> J. Foley, A. van Dam, S. Feiner, and J. Hughes, </author> <title> Computer Graphics. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: The pixel values for all x 2 X are copied into a two-dimensional array that is subsequently treated as a rectangular image. Such acquisitions can be implemented extremely efficiently using line-drawing and region-fill algorithms commonly developed for graphics applications <ref> [73] </ref>. In the second stage, the windows are processed to locate features. Using feature measurements, a new set of window parameters are computed. These parameters may be modified using external geometric constraints or temporal prediction, and the cycle repeats.
Reference: [74] <author> D. Ballard and C. Brown, </author> <title> Computer Vision. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference: [75] <author> J. Canny, </author> <title> "A computational approach to edge detection," </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <pages> pp. 679-98, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Hence, smoothing and differentiation can be combined into a single convolution template. An extremely popular convolution kernel is the derivative of a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating <p> a Gaussian (DOG) <ref> [75] </ref>. In one dimension, the DOG is defined as g (x) = x exp (x 2 = 2 ) where is a design parameter governing the amount of smoothing that takes place. Although the DOG has been demonstrated to be the optimal filter for detecting step edges [75], it requires floating point arithmetic to be computed accurately. Another edge detector which can be implemented without floating point arithmetic is the derivative of a triangle (DOT) kernal.
Reference: [76] <author> B. D. Lucas and T. Kanade, </author> <title> "An iterative image registration technique with an application to stereo vision," </title> <booktitle> in Proc. International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 674-679, </pages> <year> 1981. </year>
Reference-contexts: It is also easy to show that including parameters for contrast and brightness in (60) makes SSD tracking equivalent to finding the maximum correlation between the two image regions <ref> [76] </ref>.
Reference: [77] <author> P. Anandan, </author> <title> "A computational framework and an algorithm for the measurement of structure from motion," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 2, </volume> <pages> pp. 283-310, </pages> <year> 1989. </year>
Reference: [78] <author> J. Shi and C. Tomasi, </author> <title> "Good features to track," </title> <booktitle> in Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 593-600, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference: [79] <author> J. Huang and G. D. Hager, </author> <title> "Tracking tools for vision-based navigation," </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> Dec. </month> <year> 1994. </year> <note> Submitted to IROS '95. </note>
Reference-contexts: Continuous optimization has two principle advantages over discrete optimization. First, a single updating cycle is usually faster to compute. For example, (62) can be computed and solved in less than 5 ms on a Sparc II computer <ref> [79] </ref>. Second, it is easy to incorporate other window parameters such as rotation and scaling into the system without greatly increasing the computation time [78,79].
Reference: [80] <author> M. Kass, A. Witkin, and D. Terzopoulos, "Snakes: </author> <title> active contour models," </title> <journal> International journal of Computer Vision, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 321-331, </pages> <year> 1987. </year> <month> 41 </month>
Reference-contexts: Such situations call for the use of more global task constraints (e:g: the geometry of several edges), more global tracking (e:g: extended contours or snakes <ref> [80] </ref>), or improved or specialized detection methods. To illustrate these tradeoffs, suppose a visual servoing task relies on tracking the image of a circular opening over time. In general, the opening will project to an ellipse in the camera.
Reference: [81] <author> B. Bishop, S. A. Hutchinson, and M. W. Spong, </author> <title> "Camera modelling for visual servo control applications," </title> <booktitle> Mathematical and Computer Modelling Special issue on Modelling Issues in Visual Sensing. </booktitle>
Reference-contexts: Weiss's proposed image-based direct visual-servoing structure does away entirely with axis sensors | dynamics and kinematics are controlled adaptively based on visual feature data. This concept has a certain appeal but in practice is overly complex to implement and appears to lack robustness (see, e.g., <ref> [81] </ref> for an analysis of the effects of various image distortions on such control schemes). The concepts have only ever been demonstrated in simulation for up to 3-DOF and then with simplistic models of axis dynamics which ignore `real world' effects such as Coulomb friction and stiction.
Reference: [82] <author> P. Corke and M. </author> <title> Good, "Dynamic effects in visual closed-loop systems," </title> <journal> Submitted to IEEE Transactions on Robotics and Automation, </journal> <year> 1995. </year>
Reference-contexts: Other issues for consideration include whether or not the vision system should `close the loop' around robot axes which are position, velocity or torque controlled. A detailed discussion of these dynamic issues in visual servo systems is given by Corke <ref> [27, 82] </ref>. 7.3 Mobile robots The discussion above has assumed that the moving camera is mounted on an arm type robot manipulator. For mobile robots the pose of the robot is generally poorly known and can be estimated from the relative pose of known fixed objects or landmarks.
Reference: [83] <author> S. B. Skaar, Y. Yalda-Mooshabad, and W. H. Brockman, </author> <title> "Nonholonomic camera-space manipulation," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 8, </volume> <pages> pp. 464-479, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Most of the techniques described above are directly applicable to the mobile robot case. Visual servoing can be used for navigation with respect to landmarks or obstacle and to control docking (see, e.g., <ref> [83] </ref>). 7.4 A Light-Weight Tracking and Servoing Environment The design of many task-specific visual tracking and vision-based feedback systems used in visual servoing places a strong emphasis on system modularity and reconfigurability. This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications [84]. <p> These include control issues, such as adaptive visual servo control [14, 85], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in 35 mobile robotics, including nonholonmoic systems <ref> [83] </ref>; and, feature selection [18,78]. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. 7.6 The future The future for applications of visual servoing should be bright. Camera's are relatively inexpensive devices and the cost of image processing systems continues to fall.
Reference: [84] <author> G. D. Hager, S. Puri, and K. Toyama, </author> <title> "A framework for real-time vision-based tracking using off-the-shelf hardware," </title> <institution> DCS RR-988, Yale University, </institution> <address> New Haven, CT, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: This has motivated the development of a modular, software-based visual tracking system for experimental vision-based robotic applications <ref> [84] </ref>. The system design emphasizes flexibility and efficiency on standard scientific workstations and PC's. The system is intended to be a portable, inexpensive tool for rapid prototyping and experimentation for teaching and research. The system is written as a set of classes in C++.
Reference: [85] <author> A. C. Sanderson and L. E. Weiss, </author> <title> "Adaptive visual servo control of robots," in Robot Vision (A. Pugh, </title> <publisher> ed.), </publisher> <pages> pp. 107-116, IFS, </pages> <year> 1983. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control <ref> [14, 85] </ref>, hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78].
Reference: [86] <author> N. Mahadevamurty, T.-C. Tsao, and S. Hutchinson, </author> <title> "Multi-rate analysis and design of visual feedback digital servo control systems," ASME Journal of Dynamic Systems, </title> <booktitle> Measurement and Control, </booktitle> <pages> pp. 45-55, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [14, 85], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory <ref> [86] </ref>; issues related to automatic planning of visually controlled robot motions [87, 88]; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78].
Reference: [87] <author> R. Sharma and S. A. Hutchinson, </author> <title> "On the observability of robot motion under active camera control," </title> <booktitle> in Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 162-167, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [14, 85], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78]. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. 7.6 The future The future for applications of visual servoing should be bright.
Reference: [88] <author> A. Fox and S. Hutchinson, </author> <title> "Exploiting visual constraints in the synthesis of uncertainty-tolerant motion plans," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 11, </volume> <pages> pp. 56-71, </pages> <year> 1995. </year>
Reference-contexts: These include control issues, such as adaptive visual servo control [14, 85], hybrid control (e.g., hybrid vision/position control [12], or hybrid force/vision control), and multi-rate system theory [86]; issues related to automatic planning of visually controlled robot motions <ref> [87, 88] </ref>; applications in 35 mobile robotics, including nonholonmoic systems [83]; and, feature selection [18,78]. Many of these are describe in the proceedings of a recent workshop on visual servo control [89]. 7.6 The future The future for applications of visual servoing should be bright.
Reference: [89] <editor> G. Hager and S. Hutchinson, eds., </editor> <booktitle> Proc. IEEE Workshop on Visual Servoing: Achievements, Applications and Open Problems. </booktitle> <institution> Inst. of Electrical and Electronics Eng., Inc., </institution> <year> 1994. </year> <month> 42 </month>
Reference-contexts: Many of these are describe in the proceedings of a recent workshop on visual servo control <ref> [89] </ref>. 7.6 The future The future for applications of visual servoing should be bright. Camera's are relatively inexpensive devices and the cost of image processing systems continues to fall.
References-found: 89

