URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/neurocomp97.ps.gz
Refering-URL: 
Root-URL: 
Title: Connection Pruning with Static and Adaptive Pruning Schedules  
Author: Lutz Prechelt 
Keyword: empirical study, pruning, early stopping, generalization  
Note: Accepted article for "Neurocomputing"  
Address: D-76128 Karlsruhe, Germany  
Affiliation: Fakultat fur Informatik Universitat Karlsruhe  
Email: (prechelt@ira.uka.de)  
Phone: +49/721/608-4068, Fax: +49/721/694092  
Date: August 7, 1996  
Abstract: Neural network pruning methods on the level of individual network parameters (e.g. connection weights) can improve generalization, as is shown in this empirical study. However, an open problem in the pruning methods known today (e.g. OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This work presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. Results of statistical significance tests comparing autoprune, lprune, and static networks with early stopping are given, based on extensive experimentation with 14 different problems. The results indicate that training with pruning is often significantly better and rarely significantly worse than training with early stopping without pruning. Furthermore, lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Jack D. Cowan, Gerald Tesauro, and J. Alspector, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [2] <author> Yann Le Cun, John S. Denker, and Sara A. Solla. </author> <title> Optimal brain damage. </title> <booktitle> In [16], </booktitle> <pages> pages 598-605, </pages> <year> 1990. </year>
Reference-contexts: Several such methods have been suggested. The simplest one | with obvious flaws [5] | is to assume the importance to be proportional to the magnitude of a weight. More sophisticated approaches are the well-known optimal brain damage (OBD) and optimal brain surgeon (OBS) methods. OBD <ref> [2] </ref> uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS [8, 9] avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly.
Reference: [3] <author> Scott E. Fahlman. </author> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: squared error function, and the RPROP parameters + = 1:2, = 0:5, 0 2 [0:05 : : : 0:2] randomly per weight, max = 50, min = 0, initial weights from [-0.1. . . 0.1] randomly. 1 RPROP is a fast backpropagation variant that is about as fast as quickprop <ref> [3] </ref> but more robust in the choice of parameters.
Reference: [4] <author> Scott E. Fahlman and Christian Lebiere. </author> <booktitle> The Cascade-Correlation learning architecture. In [16], </booktitle> <pages> pages 524-532, </pages> <year> 1990. </year>
Reference-contexts: 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping [5, 11], explicit regularization [5, 12, 17], additive learning <ref> [4] </ref>, and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here.
Reference: [5] <author> William Finnoff, Ferdinand Hergert, and Hans Georg Zimmermann. </author> <title> Improving model selection by nonconvergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783, </pages> <year> 1993. </year>
Reference-contexts: 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping <ref> [5, 11] </ref>, explicit regularization [5, 12, 17], additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here. <p> 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping [5, 11], explicit regularization <ref> [5, 12, 17] </ref>, additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here. <p> At the same time it produces a smaller network. Interestingly, many papers on pruning algorithms do show empirically that smaller networks can be obtained without loss of generalization, but do not show that generalization will often be improved compared to reasonable static-network training methods; <ref> [5, 9] </ref> are notable exceptions. 1 2 1. PRUNING AND GENERALIZATION Pruning methods usually either remove complete input or hidden nodes along with all their associated parameters or remove individual connections, each of which carries one free parameter (the weight). <p> This latter approach is very fine-grained and makes pruning particularly powerful. 1.1 Some Known Pruning Methods The key to pruning is a method to calculate the approximate importance of each parameter. Several such methods have been suggested. The simplest one | with obvious flaws <ref> [5] </ref> | is to assume the importance to be proportional to the magnitude of a weight. More sophisticated approaches are the well-known optimal brain damage (OBD) and optimal brain surgeon (OBS) methods. <p> However, even when using accelerations based on Shur's lemma OBS is computationally expensive. Both methods have the disadvantage of requiring training to the error minimum before pruning may occur. For many problems, this introduces massive overfitting which often cannot be repaired by subsequent pruning. The autoprune method <ref> [5] </ref> avoids this problem. <p> A large value of T indicates high importance of the connection with weight w i . Connections with small T can be pruned. Finnoff et al. <ref> [5] </ref> have convincingly shown autoprune to be superior to OBD. Note that many more pruning methods than discussed here have been proposed in the literature. <p> Significantly lower pruning strengths could avoid this, but would exhibit another problem: namely that overfitting cannot be reduced as fast as it builds up. Therefore, pruning with very small pruning strength and static schedule would probably be similar to OBD, which has been shown inferior to autoprune by <ref> [5] </ref>. Adaptive pruning schedules are clearly necessary. 3.3 Quantitative Results As we see in Tables 1 and 2, lprune is better in some cases and autoprune is better in others. For 2 of the 14 problems, there is never a significant difference.
Reference: [6] <author> Stuart Geman, Elie Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: The most important of these are early stopping [5, 11], explicit regularization [5, 12, 17], additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data <ref> [6] </ref>. We consider the pruning technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. If applied properly, this approach often reduces overfitting and improves generalization. At the same time it produces a smaller network.
Reference: [7] <editor> Stephen J. Hanson, Jack D. Cowan, and C. Lee Giles, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [8] <author> Babak Hassibi and David G. Stork. </author> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <booktitle> In [7], </booktitle> <pages> pages 164-171, </pages> <year> 1993. </year>
Reference-contexts: OBD [2] uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS <ref> [8, 9] </ref> avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly. However, even when using accelerations based on Shur's lemma OBS is computationally expensive. Both methods have the disadvantage of requiring training to the error minimum before pruning may occur.
Reference: [9] <author> Babak Hassibi, David G. Stork, Gregory Wolff, and Takahiro Watanabe. </author> <title> Optimal Brain Surgeon: Extensions and performance comparisons. </title> <booktitle> In [1], </booktitle> <year> 1994. </year>
Reference-contexts: At the same time it produces a smaller network. Interestingly, many papers on pruning algorithms do show empirically that smaller networks can be obtained without loss of generalization, but do not show that generalization will often be improved compared to reasonable static-network training methods; <ref> [5, 9] </ref> are notable exceptions. 1 2 1. PRUNING AND GENERALIZATION Pruning methods usually either remove complete input or hidden nodes along with all their associated parameters or remove individual connections, each of which carries one free parameter (the weight). <p> OBD [2] uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS <ref> [8, 9] </ref> avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly. However, even when using accelerations based on Shur's lemma OBS is computationally expensive. Both methods have the disadvantage of requiring training to the error minimum before pruning may occur.
Reference: [10] <editor> Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [11] <author> N. Morgan and H. Bourlard. </author> <title> Generalization and parameter estimation in feedforward nets: Some experiments. </title> <booktitle> In [16], </booktitle> <pages> pages 630-637, </pages> <year> 1990. </year>
Reference-contexts: 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping <ref> [5, 11] </ref>, explicit regularization [5, 12, 17], additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here.
Reference: [12] <author> Steven J. Nowlan and Geoffrey E. Hinton. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493, </pages> <year> 1992. </year>
Reference-contexts: 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping [5, 11], explicit regularization <ref> [5, 12, 17] </ref>, additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here.
Reference: [13] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: It then presents and interprets the results of a quantitative comparison of the algorithms. 3.1 Experiment Setup Extensive benchmark comparisons were made between autoprune, lprune, and static backpropagation with early stopping. 14 different problems were used, all from the Proben1 benchmark set <ref> [13] </ref>, a collection of diagnosis problems. Most of these problems stem from the UCI machine learning databases archive. <p> The topologies have between 2 and 32 hidden nodes, either one or two hidden layers, and contain all possible feedforward connections, not only those from one layer to the next. The derivation of the standard architectures is described in <ref> [13] </ref> 2 . The second is the noshortcut standard architecture, which is derived from the standard architecture by excluding all connections that do not go from one layer to the immediately following layer.
Reference: [14] <author> Lutz Prechelt. </author> <title> A quantitative study of experimental evaluations of neural network learning algorithms: </title> <booktitle> Current research practice. Neural Networks, </booktitle> <volume> 9(3) </volume> <pages> 457-462, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: As the very different results for the various problems and even for the dataset permutations show, benchmarking has to be extensive and careful in order to yield significant and correct results | this is in sharp contrast to the state of the practice <ref> [14] </ref>. Acknowledgements Thanks to Michael Philippsen, Rainer Storn, Ben Gomes, Alexander Linden, and the anonymous reviewers for commenting on drafts of this article.
Reference: [15] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 586-591, </pages> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year> <note> 12 REFERENCES </note>
Reference-contexts: All runs were done using the RPROP weight update rule <ref> [15] </ref>, squared error function, and the RPROP parameters + = 1:2, = 0:5, 0 2 [0:05 : : : 0:2] randomly per weight, max = 50, min = 0, initial weights from [-0.1. . . 0.1] randomly. 1 RPROP is a fast backpropagation variant that is about as fast as quickprop
Reference: [16] <editor> David S. Touretzky, editor. </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [17] <author> Andreas S. Weigend, David E. Rumelhart, and Bernardo A. Huberman. </author> <title> Generalization by weight- elimination with application to forecasting. </title> <booktitle> In [10], </booktitle> <pages> pages 875-882, </pages> <year> 1991. </year>
Reference-contexts: 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are early stopping [5, 11], explicit regularization <ref> [5, 12, 17] </ref>, additive learning [4], and network pruning. All of these techniques try to solve the Bias/Variance dilemma by balancing the representation capability of the network against the information content in the training data [6]. We consider the pruning technique here.
Reference: [18] <author> Peter M. Williams. </author> <title> Bayesian regularization and pruning using a Laplace prior. </title> <type> Technical Report CSRP-312, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <address> Brighton, England, </address> <month> February </month> <year> 1994. </year> <month> ftp://ftp.cogs.susx.ac.uk/pub/reports/csrp/csrp312.ps.Z. </month>
Reference-contexts: Connections with small T can be pruned. Finnoff et al. [5] have convincingly shown autoprune to be superior to OBD. Note that many more pruning methods than discussed here have been proposed in the literature. In particular, Bayesian methods can unify the notions of regularization and pruning <ref> [18] </ref>. 1.2 An Open Problem: How Much To Prune? Given the importance T of each weight at any time during training, two questions remain to be answered: 1. When should we prune? 2.
References-found: 18

