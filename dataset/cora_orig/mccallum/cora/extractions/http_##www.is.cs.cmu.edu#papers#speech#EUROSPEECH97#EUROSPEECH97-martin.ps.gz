URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-martin.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: westphal@ira.uka.de  
Title: THE USE OF CEPSTRAL MEANS IN CONVERSATIONAL SPEECH RECOGNITION  
Author: Martin Westphal 
Address: 76128 Karlsruhe, Germany  
Affiliation: Interactive Systems Laboratories University of Karlsruhe  
Abstract: Environmental robustness and speaker independence are import issues of current speech recognition research. Channel and speaker adaptation methods do the best job when the adaption is done towards a normalized acoustic model. Normalization methods might make use of the model but primarily influence the signal such that important information is kept and unwanted distortions are cancelled out. Most large vocabulary conversational speech recognition systems use Cepstral Mean Subtraction (CMS), a channel normalization approach to compensate for the acoustic channel (and also the speaker). In this paper we discuss the basic algorithm and variations of it in the context of conversational speech and report our experience using different approaches on two widely used conversational speech recognition tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acero. </author> <title> Acoustical and Environmental Robustness in Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1990. </year>
Reference-contexts: Here the SCMS and the 2CDMS got better results than the standard CMS. For a similar test set (column 3), the error rate decreased down to 36.9% [2] by using Codebook Dependent Cepstral Normalization (CDCN) <ref> [1] </ref>. Note that the same test set recorded over a Sennheiser microphone tested with standard CMS had an error rate of 26% and is thus much more difficult than the evalset 96.
Reference: [2] <author> R. Baumgartner. </author> <title> Kanalkompensation in der Spracherkennung. </title> <type> Master's thesis, </type> <institution> University of Karl-sruhe, </institution> <year> 1996. </year>
Reference-contexts: This test set was recorded with the same setup (Sennheiser headset) as the training data. None of the variations helped for this matched conditions although we got improvements with SCMS for an internal development set. In a recent work <ref> [2] </ref> we recorded a set of GSST conversations simultaneously with 5 different microphones. For a 2151 word test set recorded over a room microphone (column 2), placed on the table between the two speakers, the performance dropped down dramatically. <p> Here the SCMS and the 2CDMS got better results than the standard CMS. For a similar test set (column 3), the error rate decreased down to 36.9% <ref> [2] </ref> by using Codebook Dependent Cepstral Normalization (CDCN) [1]. Note that the same test set recorded over a Sennheiser microphone tested with standard CMS had an error rate of 26% and is thus much more difficult than the evalset 96.
Reference: [3] <author> M. Finke, P. Geutner, H. Hild, T. Kemp, K. Ries, and M. Westphal. </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine. </title> <booktitle> In International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 83-86. </pages> <publisher> IEEE, </publisher> <year> 1997. </year>
Reference-contexts: SCMS 38.3 49.9 43.8 Table 1: Word error rates for SWB/CALLHOME 6.2. GSST The GSST system is similar to our 1996-Verbmobil-evaluation system setup <ref> [3] </ref> with 10000 mixtures over 2500 codebooks trained with nearly 14000 utterances. The vocabulary consists of 5800 words. The first column of table 2 shows error rates for the official 1996 Verbmobil evaluation test set with 343 utterances and 6442 words.
Reference: [4] <author> S. Gupta, F. Soong, and R. Haimi-Cohen. </author> <title> High-Accuracy Connected Digit Recognition for Mobile Applications. </title> <booktitle> In International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 57-60. </pages> <publisher> IEEE, </publisher> <year> 1996. </year>
Reference-contexts: The input vector z t is then calculated as z t = y t w t m spe (1 w t ) m pau In <ref> [4] </ref>, 7% to 20% relative improvements of the error rate for digit recognition in a car environment were reported using 2CMS with an energy based speech detector. On the other hand, [5] reported a 3.5% to 6.5% relative increase of the error rate when using 2CMS instead of SCMS.
Reference: [5] <author> M. Wittmann, O. Schmidbauer, and A. Aktas. </author> <title> Online Channel Compensation for Robust Speech Recognition. </title> <booktitle> In EUROSPEECH'93 (3rd European Conference on Speech Communication and Technology), </booktitle> <pages> pages 1251-1254, </pages> <year> 1993. </year>
Reference-contexts: On the other hand, <ref> [5] </ref> reported a 3.5% to 6.5% relative increase of the error rate when using 2CMS instead of SCMS. When we take a look at the approximations for 2CMS speech: z t x t pause: z t n t n t pau we see that h was eliminated.
Reference: [6] <author> T. Zeppenfeld, M. Finke, K. Ries, M. Westphal, and A. Waibel. </author> <title> Recognition of Conversational Telephone Speech using the Janus Speech Engine. </title> <booktitle> In International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 1815-1818. </pages> <publisher> IEEE, </publisher> <year> 1997. </year>
References-found: 6

