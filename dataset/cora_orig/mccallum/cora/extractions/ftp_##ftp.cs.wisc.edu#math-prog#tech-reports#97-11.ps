URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-11.ps
Refering-URL: http://www.cs.wisc.edu/~paulb/papers.html
Root-URL: 
Email: paulb@cs.wisc.edu, olvi@cs.wisc.edu  
Title: PARSIMONIOUS SIDE PROPAGATION  
Author: P. S. Bradley O. L. Mangasarian 
Date: 97-11, October  
Note: f will This work was supported by National Science Foundation Grant CCR-9322479 and Air Force Office of Scientific Research Grant F49620-97-1-0326 as Mathematical Programming Technical Report  1997.  
Address: 1210 West Dayton Street, Madison, WI 53706  
Affiliation: Computer Sciences Department, University of Wisconsin  
Abstract: A fast parsimonious linear-programming-based algorithm for training neural networks is proposed that suppresses redundant features while using a minimal number of hidden units. This is achieved by propagating sideways to newly added hidden units the task of separating successive groups of unclassified points. Computational results show an improvement of 26.53% and 19.76% in tenfold cross-validation test correctness over a parsimonious perceptron on two publicly available datasets. We consider the problem of determining the weights and thresholds of a neural network to discriminate between the elements of two disjoint sets A and B in n-dimensional real space R n . We present a parsimonious, linear-programming-based approach to determine a minimal number of hidden units and weight vectors with a minimal number of nonzero components so that the neural network achieves a prescribed degree of accuracy on the training data. Using tenfold cross-validation we compare neural networks trained by this method with a parsimonious perceptron (i.e. trained with a minimal number of nonzero weights) to discriminate between points in A and B. A word about our notation. All vectors will be column vectors unless transposed to a row vector by a superscript T . The scalar product of two vectors x and y in R n will be denoted by x T y. The notation A 2 R mfin will represent an m fi n real matrix A, A i will denote the ith row of A and A ij will denote the element in row i and column j. A vector of ones of arbitrary dimension will be denoted by e. The base of the natural logarithm will be denoted by ", and for y 2 R m , " y will denote a vector in R m with components"y i ; i = 1; : : : ; m. The notation arg min denote the set of minimizers of f (x) on the set S. For a vector x 2 R n , x fl 2 R n is the step function applied to each component of x with (x fl ) i = 1 when x i &gt; 0 and (x fl ) i = 0 when x i 0, i = 0; : : : ; n. By a separating plane, with respect to two given point sets A and B in R n , we shall mean a plane that attempts to separate R n into two half spaces such that each open halfspace contains points mostly of A or B. Alternatively, such a plane can also be interpreted as a classical perceptron [3, 4]. Tenfold cross-validation refers to the re-sampling method which successively removes 10% of the available data for testing a separator generated with the remaining 90%. For a point set A 2 R n , card(A) will denote the cardinality of A, the number of points of A. x2S
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 5667, </pages> <address> Ams-terdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: m e T z )+(ne " ); 8 &gt; : fi fi fi fi Bw efl + e z; v w v &gt; = : The number of features utilized by the separating plane (i.e. the number of nonzero elements of w) is determined by the feature suppression parameter 2 <ref> [0; 1] </ref>. When = 0, no features are suppressed while attempting to separate A and B. When = 1, w is totally suppressed to a useless zero solution. <p> Similarly, the value of fl 1 0 is determined so that the purity of H 1 0 is above the accuracy tolerance and the number of points of A 1 [ B 1 is maximal. See Figure 1. (Such a construction was first proposed in <ref> [1] </ref> using a different separation criterion that is determined by the worst error, without feature suppression and letting the sets A and B fall only in prescribed halfspaces.) Definition 1.1 Purity. Let H be a halfspace. <p> This is a simple linesearch in one dimension. Call the optimal value of this line-search procedure t 1 . The first candidate output unit weight vector is then v 1 2 R h and threshold is t 1 2 R. The second candidate weight vector v 2 is preemptive <ref> [1] </ref> in the sense that the hidden unit outputs are weighted according to the order in which they were calculated. This will ensure separation of the training set by the successive planes in the order in which they were generated. Here v 2 is determined in the following way. <p> Candidate output units are then computed and performance of the classification functions (4) is determined. Iterations cease when one of the classification functions (4) performs acceptably. We summarize the algorithm now. Algorithm 1.3 Parsimonious Side Propagation (PSP) Algorithm. Choose 2 <ref> [0; 1); ff &gt; 0 and choose 2 [0; 1] </ref> to be an accuracy tolerance. 0. Initialization. Set A 1 = A, A 1 = A, B 1 = B, B 1 = B, j = 1 and the number of hidden units h = 0. <p> Candidate output units are then computed and performance of the classification functions (4) is determined. Iterations cease when one of the classification functions (4) performs acceptably. We summarize the algorithm now. Algorithm 1.3 Parsimonious Side Propagation (PSP) Algorithm. Choose 2 [0; 1); ff &gt; 0 and choose 2 <ref> [0; 1] </ref> to be an accuracy tolerance. 0. Initialization. Set A 1 = A, A 1 = A, B 1 = B, B 1 = B, j = 1 and the number of hidden units h = 0. Compute (w 1 ; fl 1 ) by solving (1). 1.
Reference: [2] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Fea ture selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: The values of w and fl 2 R defining this separating plane are obtained by solving the following optimization problem by a linear-programming-based Successive Linearization Algorithm <ref> [2, Algorithm 3.1] </ref>: (w; fl; y; z; v) 2 arg min (1)( m e T z )+(ne " ); 8 &gt; : fi fi fi fi Bw efl + e z; v w v &gt; = : The number of features utilized by the separating plane (i.e. the number of nonzero <p> Classification error on unseen data, or generalization error, is minimized for solutions of (1) with 1 &gt; &gt; 0 <ref> [2] </ref>.
Reference: [3] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Alternatively, such a plane can also be interpreted as a classical perceptron <ref> [3, 4] </ref>. Tenfold cross-validation refers to the re-sampling method which successively removes 10% of the available data for testing a separator generated with the remaining 90%. For a point set A 2 R n , card (A) will denote the cardinality of A, the number of points of A.
Reference: [4] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural net works. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4):349360, </volume> <year> 1993. </year>
Reference-contexts: Alternatively, such a plane can also be interpreted as a classical perceptron <ref> [3, 4] </ref>. Tenfold cross-validation refers to the re-sampling method which successively removes 10% of the available data for testing a separator generated with the remaining 90%. For a point set A 2 R n , card (A) will denote the cardinality of A, the number of points of A. <p> The problem of training the output unit of the neural network to classify A and B reduces to separating the vertices of the cube to which points of A are mapped from those vertices to which B are mapped <ref> [4] </ref>. We define the m fi h matrix of ones and and zeros H A 2 f0; 1g mfih where (H A ) ij is the output of hidden unit j on data point i of A.
Reference: [5] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, www.ics.uci.edu/AI/ML/MLDBRepository.html, 1992. </address>
Reference-contexts: COMPUTATIONAL RESULTS The Wisconsin Prognostic Breast Cancer (WPBC) and the Ionosphere problems <ref> [5] </ref> were used as test problems. The 32-feature WPBC problem has 28 points in category one of breast cancer patients for which the cancer recurred within 24 months and category two of 119 patients for which the cancer did not recur within 24 months.
References-found: 5

