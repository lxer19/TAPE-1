URL: ftp://ftp.cs.washington.edu/tr/1992/12/UW-CSE-92-12-07.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/zahorjan/homepage/listof.htm
Root-URL: 
Title: Reordering Iterations in Runtime Loop Parallelization  
Author: Shun-Tak Leung and John Zahorjan 
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 92-12-07 December 1992 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adams and J. Ortega. </author> <title> A mutli-color SOR method for parallel computation. </title> <booktitle> In Proceedings of 1982 International Conference on Parallel Processing, </booktitle> <pages> pages 53-56, </pages> <month> August </month> <year> 1982. </year> <month> 14 </month>
Reference-contexts: For instance, the SOR method with 5-point stencils can be parallelized using the classical red-black ordering [9] while 9-point stencils require a similar ordering with four colors <ref> [1] </ref>.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <year> 1990. </year>
Reference-contexts: We have been looking into various ways to improve the performance of runtime parallelization [12, 13]. Although runtime parallelization can be carried out on various multiprocessor architectures, we concentrate on its use in shared-memory multiprocessors, scalable versions of which have recently been proposed <ref> [4, 11, 2] </ref>. This paper presents one aspect of our work. fl Support for this work was provided in part by the National Science Foundation (Grants CCR-8619663, CCR-9123308, and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation (Systems Research Center and External Research Program).
Reference: [3] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1989. </year>
Reference-contexts: However, there are cases in which the algorithm in fact permits a different sequential order of executing the iterations than the one specified by the sequential program. For example, the class of Gauss-Seidel iterative numerical algorithms <ref> [3] </ref> falls into this category. Like other types of iterative numerical algorithms, Gauss-Seidel type algorithms find a solution vector x to some problem by repeatedly computing a new estimate x (t+1) from an old estimate x (t) until convergence. <p> In many situations, the order of going through the array elements is not critical for correctness <ref> [3] </ref>. Different orders all produce valid answers, within an acceptable level of accuracy. Intuitively, this is because the components are indexed arbitrarily anyway. Processing them in a different order is conceptually equivalent to re-indexing the components and then processing them in the new index order. <p> This was motivated by the problem-specific ordering schemes used in many parallel numerical algorithms (see Section 1.3) and by a general technique that does not handle anti-dependencies <ref> [3] </ref>. In our graph, nodes represent iterations and edges represent potential dependencies.
Reference: [4] <author> Henry III Burkhardt, Steven Frank, Bruce Knobe, and James Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: We have been looking into various ways to improve the performance of runtime parallelization [12, 13]. Although runtime parallelization can be carried out on various multiprocessor architectures, we concentrate on its use in shared-memory multiprocessors, scalable versions of which have recently been proposed <ref> [4, 11, 2] </ref>. This paper presents one aspect of our work. fl Support for this work was provided in part by the National Science Foundation (Grants CCR-8619663, CCR-9123308, and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation (Systems Research Center and External Research Program). <p> The results for two models are presented here. Model A has a 174090 fi 174090 coefficient matrix with about 1146000 non-zero elements. Model B has a 45676 fi 45676 matrix with about 252000 non-zero elements 3 . Measurements were taken on a Kendall Square Research KSR1 shared-memory multiprocessor <ref> [4] </ref> running OSF/1. All programs were written in C using KSR1's pthreads. <p> Examples of such machines are the Kendall Square Research KSR1 <ref> [4] </ref> and the Tera Computer Corporation multiprocessor design. We assume that the time it takes to run the executor once consists of two components. The first is the time to execute the iterations themselves; the second is a constant synchronization overhead for each wavefront.
Reference: [5] <institution> CM-5 Technical Summary. Thinking Machines Corporation, </institution> <address> Cambridge, MA., </address> <year> 1991. </year>
Reference-contexts: First, executing the barrier code takes time. The importance of this would be small, though, if there is direct hardware support for barrier operations, as in new machines like the Thinking Machines CM-5 <ref> [5] </ref>. Secondly, 12 since all processors must arrive at the barrier before any one of them can pass through and proceed to the next wavefront, processors arriving earlier have to wait idly for the last arrival.
Reference: [6] <author> R. De Leone and O. L. Mangasarian. </author> <title> Asynchronous parallel successive overrelaxation for the symmetric linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 42(2) </volume> <pages> 347-361, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: However, having fewer wavefronts still improves performance by reducing the total barrier synchronization overhead and also the load imbalance within each wavefront. Finally, we note that there are explicitly parallel algorithms which do not correspond to the parallelization of serial algorithms <ref> [6, 14] </ref>. Since our focus is mainly the parallelization of loops specified in sequential programming languages such as Fortran D [7], these algorithms do not fall into the scope of this paper. 4 1.4 Paper Organization The remainder of this paper is organized as follows.
Reference: [7] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran d language specification. </title> <type> Technical Report TR90-141, </type> <institution> Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Scientific applications written in sequential programming languages (e.g., HPF or Fortran D <ref> [7] </ref>) typically have large loops that can be parallelized to exploit the computing power of multiple processors. However, if a loop contains complicated or data dependent array indexing expressions, the inter-iteration dependencies cannot be fully determined at compile time. <p> Finally, we note that there are explicitly parallel algorithms which do not correspond to the parallelization of serial algorithms [6, 14]. Since our focus is mainly the parallelization of loops specified in sequential programming languages such as Fortran D <ref> [7] </ref>, these algorithms do not fall into the scope of this paper. 4 1.4 Paper Organization The remainder of this paper is organized as follows.
Reference: [8] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Since there is a one-to-one correspondence between colors and wavefronts, minimizing schedule depth is equivalent to using the fewest possible colors. Unfortunately, the problem of finding the minimum number of colors needed is NP-complete <ref> [8] </ref>. It is doubtful that a reasonably efficient algorithm to do this can be found. Because of this, we have used a simple coloring heuristic, as shown in Figure 3. Colors are represented by positive integers.
Reference: [9] <author> Jules J. Lambiotte, Jr. and Robert G. Voigt. </author> <title> The solution of tridiagonal linear systems on the CDC STAR-100 computer. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 1(4) </volume> <pages> 308-329, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: Finally, we note that this heuristic automatically produces some common grid point ordering schemes used in the SOR solution of partial differential equations on rectangular grids. For instance, the SOR method with 5-point stencils can be parallelized using the classical red-black ordering <ref> [9] </ref> while 9-point stencils require a similar ordering with four colors [1].
Reference: [10] <author> Edward D. Lazowska, John Zahorjan, G. Scott Graham, and Kenneth C. Sevcik. </author> <title> Quantitative System Performance. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1984. </year>
Reference-contexts: The linear equations are the global balance equations <ref> [10] </ref> corresponding to a queueing network model with blocking [15]. The model itself consists of a number of service centers in series, each with a finite capacity queue. Such models are commonly used to evaluate the performance of communication networks built from switches with finite buffer space.
Reference: [11] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We have been looking into various ways to improve the performance of runtime parallelization [12, 13]. Although runtime parallelization can be carried out on various multiprocessor architectures, we concentrate on its use in shared-memory multiprocessors, scalable versions of which have recently been proposed <ref> [4, 11, 2] </ref>. This paper presents one aspect of our work. fl Support for this work was provided in part by the National Science Foundation (Grants CCR-8619663, CCR-9123308, and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation (Systems Research Center and External Research Program).
Reference: [12] <author> Shun-Tak Leung and John Zahorjan. </author> <title> Improving executor performance in runtime loop parallelization. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1992. </year> <note> In preparation. </note>
Reference-contexts: Unable to parallelize the loop, the compiler must generate code to execute the loop sequentially. To address this problem, Saltz et al. [17, 18] have proposed runtime parallelization as an alternative. We have been looking into various ways to improve the performance of runtime parallelization <ref> [12, 13] </ref>. Although runtime parallelization can be carried out on various multiprocessor architectures, we concentrate on its use in shared-memory multiprocessors, scalable versions of which have recently been proposed [4, 11, 2].
Reference: [13] <author> Shun-Tak Leung and John Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Proceed ings of Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Unable to parallelize the loop, the compiler must generate code to execute the loop sequentially. To address this problem, Saltz et al. [17, 18] have proposed runtime parallelization as an alternative. We have been looking into various ways to improve the performance of runtime parallelization <ref> [12, 13] </ref>. Although runtime parallelization can be carried out on various multiprocessor architectures, we concentrate on its use in shared-memory multiprocessors, scalable versions of which have recently been proposed [4, 11, 2].
Reference: [14] <author> O. L. Mangasarian and R. De Leone. </author> <title> Parallel successive overrelaxation methods for symmetric linear complementarity problems and linear programs. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 54 </volume> <pages> 437-446, </pages> <year> 1987. </year>
Reference-contexts: However, having fewer wavefronts still improves performance by reducing the total barrier synchronization overhead and also the load imbalance within each wavefront. Finally, we note that there are explicitly parallel algorithms which do not correspond to the parallelization of serial algorithms <ref> [6, 14] </ref>. Since our focus is mainly the parallelization of loops specified in sequential programming languages such as Fortran D [7], these algorithms do not fall into the scope of this paper. 4 1.4 Paper Organization The remainder of this paper is organized as follows.
Reference: [15] <author> Raif O. Onvural. </author> <title> Survey of closed queueing networks with blocking. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(2):83 121, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: The linear equations are the global balance equations [10] corresponding to a queueing network model with blocking <ref> [15] </ref>. The model itself consists of a number of service centers in series, each with a finite capacity queue. Such models are commonly used to evaluate the performance of communication networks built from switches with finite buffer space. The results for two models are presented here.
Reference: [16] <author> J. Ortega and R. Voigt. </author> <title> Solution of partial differential equations on vector and parallel computers. </title> <journal> SIAM Review, </journal> <volume> 27(2) </volume> <pages> 149-240, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: For example, numerous parallel algorithms for solving partial differential equations on rectangular grids have been proposed <ref> [16] </ref>. They generally obtain parallelism by partitioning the set of grid points in certain regular ways specifically chosen for the problem so that points within the same subset are unrelated and thus can be processed in parallel. The subsets themselves are processed in some sequential order.
Reference: [17] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: However, if a loop contains complicated or data dependent array indexing expressions, the inter-iteration dependencies cannot be fully determined at compile time. Unable to parallelize the loop, the compiler must generate code to execute the loop sequentially. To address this problem, Saltz et al. <ref> [17, 18] </ref> have proposed runtime parallelization as an alternative. We have been looking into various ways to improve the performance of runtime parallelization [12, 13]. <p> Under these circumstances, reordering the iterations and then parallelizing the reordered loop may produce more parallelism than simply parallelizing the original loop. In this section, we first review the basic idea of runtime parallelization proposed by Saltz et al. <ref> [17, 18] </ref>. We then discuss the notion of reordering iterations to gain parallelism, and some implications of providing programming support for it. 1.1 Runtime Parallelization of Loops Let us call the loop being parallelized the source loop. <p> The form of source loops we focus on is shown in Figure 1, which is adapted from Saltz et al. <ref> [17] </ref>. The objective is to identify iterations that can be executed concurrently and assign them to different processors in a shared-memory multiprocessor. <p> Sparse matrix operations are good examples of this class of computation. In these operations, g (i), h (i) would be indirection arrays containing indices into the array a, which in turn contains the non-zero elements. The basic idea of runtime parallelization, due to Saltz et al. <ref> [17, 18] </ref>, is for the compiler to generate two code fragments for each source loop: inspector and executor. At run time, the inspector calculates the array index functions, determines the inter-iteration dependencies, and use these to compute a parallel schedule for the iterations. <p> In our graph, nodes represent iterations and edges represent potential dependencies. If iteration k writes to an array element read by iteration l, then nodes k and l are connected by an undirected edge. 2 Saltz et al. <ref> [17, 18] </ref> make special provisions in their executor to deal with anti-dependencies so their schedule in fact respects only flow dependencies. 5 colored graph 1 A 1 A 1 A 2 3 4 9 10 11 12 2 2 4 5 3 4 C B B original iteration order 1, 2,
Reference: [18] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and runtime compilation. </title> <booktitle> In Proceedings of International Workshop on Compilers for Parallel Computers, </booktitle> <address> Paris, </address> <year> 1990. </year> <month> 15 </month>
Reference-contexts: However, if a loop contains complicated or data dependent array indexing expressions, the inter-iteration dependencies cannot be fully determined at compile time. Unable to parallelize the loop, the compiler must generate code to execute the loop sequentially. To address this problem, Saltz et al. <ref> [17, 18] </ref> have proposed runtime parallelization as an alternative. We have been looking into various ways to improve the performance of runtime parallelization [12, 13]. <p> Under these circumstances, reordering the iterations and then parallelizing the reordered loop may produce more parallelism than simply parallelizing the original loop. In this section, we first review the basic idea of runtime parallelization proposed by Saltz et al. <ref> [17, 18] </ref>. We then discuss the notion of reordering iterations to gain parallelism, and some implications of providing programming support for it. 1.1 Runtime Parallelization of Loops Let us call the loop being parallelized the source loop. <p> Sparse matrix operations are good examples of this class of computation. In these operations, g (i), h (i) would be indirection arrays containing indices into the array a, which in turn contains the non-zero elements. The basic idea of runtime parallelization, due to Saltz et al. <ref> [17, 18] </ref>, is for the compiler to generate two code fragments for each source loop: inspector and executor. At run time, the inspector calculates the array index functions, determines the inter-iteration dependencies, and use these to compute a parallel schedule for the iterations. <p> In our graph, nodes represent iterations and edges represent potential dependencies. If iteration k writes to an array element read by iteration l, then nodes k and l are connected by an undirected edge. 2 Saltz et al. <ref> [17, 18] </ref> make special provisions in their executor to deal with anti-dependencies so their schedule in fact respects only flow dependencies. 5 colored graph 1 A 1 A 1 A 2 3 4 9 10 11 12 2 2 4 5 3 4 C B B original iteration order 1, 2,
References-found: 18

