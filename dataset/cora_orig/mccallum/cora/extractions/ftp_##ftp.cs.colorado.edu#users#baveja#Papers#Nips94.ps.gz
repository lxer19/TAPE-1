URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Nips94.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: singh@psyche.mit.edu  tommi@psyche.mit.edu  jordan@psyche.mit.edu  
Title: Reinforcement Learning with Soft State Aggregation  
Author: Satinder P. Singh Tommi Jaakkola Michael I. Jordan 
Address: Cambridge, MA 02139  
Affiliation: Dept. of Brain Cognitive Sciences (E-10) M.I.T.  
Abstract: It is widely accepted that the use of more compact representations than lookup tables is crucial to scaling reinforcement learning (RL) algorithms to real-world problems. Unfortunately almost all of the theory of reinforcement learning assumes lookup table representations. In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approx-imator based on a simple extension to state aggregation (a commonly used form of compact representation), namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm that finds improved compact representations by exploiting the non-discrete nature of soft state aggregation. Preliminary empirical results are also presented. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A. G. Barto, R. S. Sutton, & C. W. Anderson. </author> <title> (1983) Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE SMC, </journal> <volume> 13 </volume> <pages> 835-846. </pages>
Reference-contexts: Case 2: TD (0) and Fixed Soft State Aggregation We present separate results for TD (0) because it forms the basis for policy-iteration-like methods for solving Markov control problems <ref> (e.g., Barto, Sutton & Anderson, 1983) </ref> | a fact that we will use in the next section to derive adaptive state aggregation methods. As before, because of function approximation, the domain of the learned value function is constrained to be the cluster space X .
Reference: <author> D. P. Bertsekas. </author> <title> (1987) Dynamic Programming: Deterministic and Stochastic Models, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: We will first prove a general convergence theorem for Q-learning (Watkins & Dayan, 1992) applied to a sequence of quadruples that may or may not be generated by a Markov process <ref> (Bertsekas, 1987) </ref>. This is required because the RL problem at the level of the clusters may be non-Markovian. <p> In this section, the clustering probabilities P (xjs) are assumed to be fixed. Case 1: Q-learning and Fixed Soft State Aggregation Because of function approximation, the domain of the learned Q-value function is constrained to be X fi A (X is cluster space). This section develops a "Bellman equation" <ref> (e.g., Bertsekas, 1987) </ref> for Q-learning at the level of the cluster space. We assume that the agent follows a stationary stochastic policy that assigns to each state a non-zero probability of executing every action in every state. Furthermore, we assume that the Markov chain under policy is ergodic.
Reference: <author> S. J. Bradtke. </author> <title> (1993) Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 295-302. </pages>
Reference: <author> P. Dayan. </author> <title> (1992) The convergence of TD() for general . Machine Learning, </title> 8(3/4):341-362. 
Reference-contexts: We will first prove a general convergence theorem for Q-learning <ref> (Watkins & Dayan, 1992) </ref> applied to a sequence of quadruples that may or may not be generated by a Markov process (Bertsekas, 1987). This is required because the RL problem at the level of the clusters may be non-Markovian.
Reference: <author> P. Dayan & T.J. Sejnowski. </author> <title> (1994) TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 13(3). </volume>
Reference: <author> T. Jaakkola, M. I. Jordan, & S. P. Singh. </author> <title> (1994) On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201. </pages>
Reference: <author> A. W. Moore. </author> <title> (1991) Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> In Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 333-337. </pages>
Reference-contexts: In this paper we address the pressing issue of combining function approximation and RL, and present 1) a function approximator based on a simple extension to state aggregation <ref> (a commonly used form of compact representation, e.g., Moore, 1991) </ref>, namely soft state aggregation, 2) a theory of convergence for RL with arbitrary, but fixed, soft state aggregation, 3) a novel intuitive understanding of the effect of state aggregation on online RL, and 4) a new heuristic adaptive state aggregation algorithm
Reference: <author> S. P. Singh, T. Jaakkola, & M. I. Jordan. </author> <title> (1994) Learning without state-estimation in partially observable markovian decision processes. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 284-292. </pages>
Reference: <author> R. S. Sutton. </author> <title> (1988) Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> J. Tsitsiklis. </author> <title> (1994) Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202. </pages>
Reference: <author> B. Vanroy & J. Tsitsiklis. (personal communication) C. J. C. H. Watkins & P. Dayan. </author> <year> (1992) </year> <month> Q-learning. </month> <journal> Machine Learning, </journal> 8(3/4):279-292. 
Reference: <author> R. C. Yee. </author> <title> (1992) Abstraction in control learning. </title> <type> Technical Report COINS Technical Report 92-16, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address> <note> A dissertation proposal. </note>
References-found: 12

