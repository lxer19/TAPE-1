URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/link/www/papers/ps/dennis-proposal.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/dennis/www/research.html
Root-URL: http://www.cs.cmu.edu
Title: Statistical Language Modeling Using Grammatical Information  
Author: Dennis Grinberg 
Date: September 11, 1995  
Abstract: We propose to investigate the use of grammatical information to build improved statistical language models. Until recently, language models were primarily influenced by local lexical constraints. Today, language models often utilize longer range lexical information to aid in their predictions. All of these language models ignore grammatical considerations other than those induced by the statistics of lexical constraints. We believe that properly incorporating additional grammatical structure will achieve improved language models. We will use link grammar as our grammatical base. Being highly lexical in nature, the link grammar formalism will allow us to integrate more traditional modeling schemes with grammatical ones. An efficient robust link grammar parser will assist in this undertaking. We will initially build finite state-based language models that will utilize relatively simple grammatical information, such as part-of-speech data, along with information sources used by other language models. Our models feature a new framework for probabilistic automata that makes use of hidden data to construct context-sensitive probabilities. The maximum entropy principle employed by these Gibbs-Markov models facilitates the easy integration of multiple information sources. We will also build language models that take greater advantage of link grammar by including more sophisticated grammatical considerations. These models will include both probabilistic automata as well as models more closely related to the link grammar formalism. Expected contributions of this work will be to demonstrate that grammatical information can be used to construct language models with low perplexity, and that such models can be used to reduce the error rates of speech recognition systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. R. Bahl, F. Jelinek, and R. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March </month> <year> 1983. </year>
Reference-contexts: Finally, trigger models, which have recently been shown to have great potential, are described. 2.3.1. n-gram models One of the most popular types of language models are n-gram models. In these models, the histories are partitioned into classes that depend on the most recent n 1 words <ref> [1] </ref>. The most common n-gram models are trigram models in which n = 3. <p> We describe them here because n-gram systems, both class and word based, can be placed within the HMM framework. A hidden Markov model consists of a set of states S = fs 1 ; s 2 ; : : : ; s g, a probability function : S ! <ref> [0; 1] </ref>, an alphabet W = fw 1 ; w 2 ; : : : ; w ! g, a set of state transition probabilities fa s;t g s;t2S , and a set of output probability functions fb s : W ! [0; 1]g s2S with the requirements that X (s) <p> s g, a probability function : S ! <ref> [0; 1] </ref>, an alphabet W = fw 1 ; w 2 ; : : : ; w ! g, a set of state transition probabilities fa s;t g s;t2S , and a set of output probability functions fb s : W ! [0; 1]g s2S with the requirements that X (s) = 1; t2S X b s (w) = 1; 8s 2 S. <p> P : R ! <ref> [0; 1] </ref> is a probability function that satisfies 1: P (A ! ff) 0; 8 (A ! ff) 2 R P where R i j is the j-th rule in R with V i as its left-hand-side and n i is the number of such rules.
Reference: [2] <author> J. K. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <address> Boston, MA, </address> <year> 1979. </year>
Reference-contexts: This allows us to rank hypothesized sentences as being more or less likely. 6. Algorithms exist to find the k most likely parse trees that correspond to a sentence [21]. 7. Algorithms exist to modify rule probabilities in order to maximize the probability of gener ating a corpus <ref> [2] </ref> [21]. This allows us to use a corpus to train the model's parameters. 8. Algorithms exist to compute prefix probabilities, the probabilities of initial word strings [20].
Reference: [3] <author> A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <note> Computational Linguistics, 1995. To appear. </note>
Reference-contexts: While deleted interpolation can be used to combine our language model components, we will be using a different method, which uses a maximum entropy approach [42] <ref> [3] </ref> [40]. 5.2. Maximum entropy principle The maximum entropy method [18] chooses a model that agrees with certain explicit constraints yet avoids assuming anything that has not been specified. For example, we might know that in our training data, the noun "toad" is modified by adjective "slimy" 10 times. <p> Among all of the elements of the exponential family E, the maximum entropy distribution is also the maximum likelihood solution. The maximum entropy principle has been successfully used as a method to combine information sources in natural language applications [42] <ref> [3] </ref> [40] and has a number of appealing properties: 1. The principle is simple, intuitive and easily allows one to combine multiple information sources. The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2. <p> The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2. A number of numerical algorithms (including generalized iterative scaling [13] and improved versions of it <ref> [3] </ref> [40]) exist for computing the parameters i of the maximum entropy solution The maximum entropy solution is unique, and these algorithms are guaranteed to converge to it. 3. <p> While this won't result in the maximum entropy solution, the result might still be good enough for many purposes. This method was used to incrementally add constraints to models in <ref> [3] </ref> and [40]. One item that is lacking from maximum entropy models is the notion of hidden data, previously seen in section 2.3.3. In the next section we will discuss a framework for constructing probabilistic automata using maximum entropy distributions. 5.3.
Reference: [4] <author> E. Black, F. Jelinek, J. Lafferty, D. M. Magerman, R. Mercer, and S. Roukos. </author> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of DARPA Speech and Natural Language Workshop, </booktitle> <month> Feb </month> <year> 1992. </year>
Reference-contexts: In [35] a similar model was examined and the results reported were not as good. Black et. al. <ref> [4] </ref> described history-based grammars (HGBs) in which rich histories are used to aid in prediction. Essentially, any information previously seen is allowed to influence the expansion of a non-terminal node. Decision trees were used in this work to estimate the rule probabilities.
Reference: [5] <author> E. Black, F. Jelinek, J. Lafferty, and S. Roukos. </author> <title> Decision tree models applied to the labeling of text with parts-of-speech. </title> <booktitle> In Proceedings of DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 117-121, </pages> <month> Feb </month> <year> 1992. </year>
Reference-contexts: One reason is that the part-of-speech categories used were quite limited. Other, more general problems with part-of-speech based clustering include: 1. It might be difficult to acquire training data with part-of-speech assignments for certain domains. While there are automatic methods to derive part-of-speech assignments [19, appendix C], [11], <ref> [5] </ref>, these systems are not perfect and on certain domains such as continuous conversational speech, they perform quite poorly. 2. It is unclear whether clustering by part-of-speech is suited for language modeling [19]. Lan guage modeling might benefit more by other groupings of words.
Reference: [6] <author> P. Brown, V. Della Pietra, P. de Souza, J. Lai, and R. Mercer. </author> <title> Class based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: It is unclear whether clustering by part-of-speech is suited for language modeling [19]. Lan guage modeling might benefit more by other groupings of words. Instead of using preassigned classes, words can be placed into classes based on other criteria such as minimizing loss of mutual information [19, appendix D] <ref> [6] </ref>. A trigram class model that grouped words in this manner did not perform quite as well as a word-based trigram model, but the decrease in the percentage of unseen n-grams in previously unseen data was substantial, from 14.7% to 3.8%. <p> The amount of storage needed for the class-based model was also substantially (1/3) lower than needed for the word-based trigram model <ref> [6] </ref>. While class based models do alleviate some of the problems with n-gram models at the expense of slightly poorer performance, they still do not address the issue of making predictions using longer range and structural information. 5 2.3.3. <p> The percentage of new tuples of the form (w; L; R; l; r) appearing in the test data would probably be much higher than the percentages reported in [19] <ref> [6] </ref> for trigrams. While a deleted interpolation algorithm for smoothing is given in [39], we still prefer to begin with simpler models. 2. It is unclear how to incorporate some elements of link grammars such as multi-connectors and conjunctions into the framework of [28]. 3.
Reference: [7] <author> P. E. Brown, S. A. Della Pietra, V. J. Della Pietra, J. C. Lai, and R. L. Mercer. </author> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: Shannon [43] reported results for a variation of this game where humans predicted letters using an alphabet of 27 symbols (26 letters and the space). The result was an upper-bound estimate of the entropy of English of 1.30 bits per character. In <ref> [7] </ref>, a language model was used to estimate the entropy of mixed-case English and an upper bound of 1.75 bits per character was reported.
Reference: [8] <author> N. Chomsky. </author> <title> Three models for the description of language. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> 2(3) </volume> <pages> 113-124, </pages> <year> 1956. </year>
Reference-contexts: We will propose using a robust version of link grammar as our grammatical formalism and explain how it overcomes the shortcomings of context-free grammars. 4.1. Probabilistic context-free grammars Context-free grammars <ref> [8] </ref> [9] [10] enjoy widespread use in the study of natural language. CFGs are able to model hierarchical data and the formalism has been used to create syntactic parsers for natural language.
Reference: [9] <author> N. Chomsky. </author> <title> On certain formal properties of grammars. </title> <journal> Information and Control, </journal> <volume> 2(2) </volume> <pages> 137-167, </pages> <year> 1959. </year>
Reference-contexts: We will propose using a robust version of link grammar as our grammatical formalism and explain how it overcomes the shortcomings of context-free grammars. 4.1. Probabilistic context-free grammars Context-free grammars [8] <ref> [9] </ref> [10] enjoy widespread use in the study of natural language. CFGs are able to model hierarchical data and the formalism has been used to create syntactic parsers for natural language.
Reference: [10] <author> N. Chomsky. </author> <title> Formal properties of grammars. </title> <editor> In L. D. Luce, R. R. Bush, and E. Galanter, editors, </editor> <booktitle> Handbook of Mathematical Psychology, </booktitle> <pages> pages 323-418. </pages> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: We will propose using a robust version of link grammar as our grammatical formalism and explain how it overcomes the shortcomings of context-free grammars. 4.1. Probabilistic context-free grammars Context-free grammars [8] [9] <ref> [10] </ref> enjoy widespread use in the study of natural language. CFGs are able to model hierarchical data and the formalism has been used to create syntactic parsers for natural language. Given a CFG for a language, one can build a stochastic grammar that can be used to model that language.
Reference: [11] <author> K. W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 695-698, </pages> <year> 1989. </year>
Reference-contexts: One reason is that the part-of-speech categories used were quite limited. Other, more general problems with part-of-speech based clustering include: 1. It might be difficult to acquire training data with part-of-speech assignments for certain domains. While there are automatic methods to derive part-of-speech assignments [19, appendix C], <ref> [11] </ref>, [5], these systems are not perfect and on certain domains such as continuous conversational speech, they perform quite poorly. 2. It is unclear whether clustering by part-of-speech is suited for language modeling [19]. Lan guage modeling might benefit more by other groupings of words.
Reference: [12] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: We will take the logarithms to the base 2 and we will measure entropy in units called bits. It is a well known theorem of information theory <ref> [12] </ref> that an average of H bits is needed to represent a symbol from a source with entropy H. Entropy is a measure of the uncertainty of the source; it measures how much information, on average, is needed to describe a symbol.
Reference: [13] <author> J. N. Darroch and D. Ratcliff. </author> <title> Generalized iterative scaling for log-linear models. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 43 </volume> <pages> 1470-1480, </pages> <year> 1972. </year> <month> 32 </month>
Reference-contexts: The principle is simple, intuitive and easily allows one to combine multiple information sources. The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2. A number of numerical algorithms (including generalized iterative scaling <ref> [13] </ref> and improved versions of it [3] [40]) exist for computing the parameters i of the maximum entropy solution The maximum entropy solution is unique, and these algorithms are guaranteed to converge to it. 3.
Reference: [14] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: GMMs are trained by using an expectation-maximization (EM) algorithm <ref> [14] </ref> where the M-step uses generalized iterative scaling for conditional maximum entropy models. 6.
Reference: [15] <author> J. Godfrey, E. Holliman, and J. McDaniel. </author> <title> Switchboard: Telephone speech corpus for research development. </title> <booktitle> In Proc. ICASSP-92, </booktitle> <pages> pages I-517-520, </pages> <year> 1992. </year>
Reference-contexts: In section 6.1 we will describe the corpus we will be using in our experiments and in section 6.2 we will discuss measures of goodness for language models. 6.1. Corpus We will be using the Switchboard <ref> [15] </ref> corpus of conversational English divided into training and testing data. This corpus is comprised of approximately three million words of text, corresponding to more than 150 hours of transcribed speech collected from telephone conversations restricted to 70 different topics.
Reference: [16] <author> I. J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3, </volume> 4):237-264, Dec 1953. 
Reference-contexts: The probability of observing an unknown n-gram in previously unseen data can be estimated by the Good-Turing estimate <ref> [16] </ref> as n 1 =N where n 1 is the number of n-grams that appeared only once in a sample corpus and N is the size of the corpus. This sparse data problem can be quite acute.
Reference: [17] <author> D. Grinberg, J. Lafferty, and D. Sleator. </author> <title> A robust parsing algorithm for link grammars. </title> <booktitle> In Proceedings of the Fourth International Workshop on Parsing Technologies, </booktitle> <address> Prague/Karlovy Vary, Czech Republic, </address> <year> 1995. </year> <note> Also issued as Technical Report CMU-CS-95-125, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: By this measure, the cost of a string of N words can be no greater than N 1, and it is zero only if the string is grammatical. In <ref> [17] </ref> we describe an efficient parsing algorithm that finds the minimal cost generalized parses of a sentence. This algorithm is able to extract relevant grammatical structure even in domains where very few of the sentences are "grammatical" by standard criteria. <p> A training algorithm is given in [28] that determines the parameters of the probabilistic model given a training corpus by maximum likelihood estimation using the EM algorithm. 4.2.5. Initial approaches Given the grammar introduced in [44] and the robust parser of <ref> [17] </ref> we could use the probabilistic model and training algorithm of [28] to create a link grammar based language model. Instead, we choose to begin with simple models such as bigrams or trigrams and to slowly add grammatical structure and additional information sources to these models. <p> In order to use link grammar as a source of information about the Switchboard corpus, we need to be able to parse the corpus. With this objective, we built a robust link grammar parser, discussed in section 4.2.3 and described in greater detail in <ref> [17] </ref>. The parser is quite adept at extracting relevant structure for a large portion of the corpus even though only a small fraction of the sentences are "grammatical" by standard criteria.
Reference: [18] <author> E. T. Jaynes. </author> <title> Brandeis lectures (1963). </title> <editor> In R. D. Rosenkrantz, editor, </editor> <title> Papers on Probability, </title> <journal> Statistics and Statistical Physics, </journal> <pages> pages 39-76. </pages> <address> D. </address> <publisher> Reidel Publishing Company, Dordrecht, Holland, </publisher> <year> 1983. </year>
Reference-contexts: While deleted interpolation can be used to combine our language model components, we will be using a different method, which uses a maximum entropy approach [42] [3] [40]. 5.2. Maximum entropy principle The maximum entropy method <ref> [18] </ref> chooses a model that agrees with certain explicit constraints yet avoids assuming anything that has not been specified. For example, we might know that in our training data, the noun "toad" is modified by adjective "slimy" 10 times. <p> real numbers, one for each constraint, it is easy to show that the maximum entropy solution to the system of equations (6) is 19 a member of the exponential family E = P j P (wjh) = 1 P o where Z (h) = w P is used for normalization <ref> [18] </ref>. The maximum entropy solution has a property that is intuitively appealing. Often in language modeling one seeks the probability distribution that maximizes the likelihood of the training data.
Reference: [19] <author> F. Jelinek. </author> <title> Self-organizing language models for speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 450-506. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: This sparse data problem can be quite acute. For example, Jelinek states in <ref> [19] </ref> that for a training corpus of patent descriptions whose size was 1,500,000 words, when tested against 300,000 words of previously unseen data from the same corpus, 23% of the trigrams in the unseen corpus had never appeared in the training corpus. <p> Of course class-based systems need not be limited to n-gram class models: conditioning on combinations of words and assigning classes probabilisticly rather than deterministically is quite common. In <ref> [19] </ref>, part-of-speech assignments are used to assign classes in a trigram class model and it appears that when combined with a regular trigram model by linear interpolation, the results are an improvement over the combined model's individual components. <p> One reason is that the part-of-speech categories used were quite limited. Other, more general problems with part-of-speech based clustering include: 1. It might be difficult to acquire training data with part-of-speech assignments for certain domains. While there are automatic methods to derive part-of-speech assignments <ref> [19, appendix C] </ref>, [11], [5], these systems are not perfect and on certain domains such as continuous conversational speech, they perform quite poorly. 2. It is unclear whether clustering by part-of-speech is suited for language modeling [19]. Lan guage modeling might benefit more by other groupings of words. <p> While there are automatic methods to derive part-of-speech assignments [19, appendix C], [11], [5], these systems are not perfect and on certain domains such as continuous conversational speech, they perform quite poorly. 2. It is unclear whether clustering by part-of-speech is suited for language modeling <ref> [19] </ref>. Lan guage modeling might benefit more by other groupings of words. Instead of using preassigned classes, words can be placed into classes based on other criteria such as minimizing loss of mutual information [19, appendix D] [6]. <p> It is unclear whether clustering by part-of-speech is suited for language modeling [19]. Lan guage modeling might benefit more by other groupings of words. Instead of using preassigned classes, words can be placed into classes based on other criteria such as minimizing loss of mutual information <ref> [19, appendix D] </ref> [6]. A trigram class model that grouped words in this manner did not perform quite as well as a word-based trigram model, but the decrease in the percentage of unseen n-grams in previously unseen data was substantial, from 14.7% to 3.8%. <p> The percentage of new tuples of the form (w; L; R; l; r) appearing in the test data would probably be much higher than the percentages reported in <ref> [19] </ref> [6] for trigrams. While a deleted interpolation algorithm for smoothing is given in [39], we still prefer to begin with simpler models. 2. It is unclear how to incorporate some elements of link grammars such as multi-connectors and conjunctions into the framework of [28]. 3.
Reference: [20] <author> F. Jelinek and J. D. Lafferty. </author> <title> Computation of the probability of initial substring generation by stochastic context-free grammars. </title> <journal> Computational Linguistics, </journal> <volume> 17(3) </volume> <pages> 315-323, </pages> <year> 1991. </year>
Reference-contexts: Algorithms exist to modify rule probabilities in order to maximize the probability of gener ating a corpus [2] [21]. This allows us to use a corpus to train the model's parameters. 8. Algorithms exist to compute prefix probabilities, the probabilities of initial word strings <ref> [20] </ref>. This might have some advantages when the language model is used in systems such as speech recognition systems where left-to-right processing of the input is useful. Some might argue that some of these features are limited in usefulness. <p> As we are not changing grammars and the probability function should be clear from the context, we choose to simplify notation. 10 probability of an initial word string requires the inversion of what can be a very large matrix <ref> [20] </ref>. 2 Also, some of the features above might not have analogues for robust PCFG-based parsers. Still, these features certainly give us good reason to consider using PCFGs to provide us with the syntactic structure we expect will lead to improved language models.
Reference: [21] <author> F. Jelinek, J. D. Lafferty, and R. L. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <editor> In P. Laface and R. De Mori, editors, </editor> <booktitle> Speech Recognition and Understanding: Recent Advances, </booktitle> <pages> pages 345-360. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <booktitle> NATO ASI Series, </booktitle> <volume> Vol. </volume> <month> F75. </month>
Reference-contexts: Except in extremely restricted domains, any input is possible. While some inputs might be highly unlikely, they are still possible and language models must still consider them as possibilities. 5. Algorithms exist to compute P (W ), the probability that the grammar generates word string W <ref> [21] </ref>. This allows us to rank hypothesized sentences as being more or less likely. 6. Algorithms exist to find the k most likely parse trees that correspond to a sentence [21]. 7. Algorithms exist to modify rule probabilities in order to maximize the probability of gener ating a corpus [2] [21]. <p> Algorithms exist to compute P (W ), the probability that the grammar generates word string W <ref> [21] </ref>. This allows us to rank hypothesized sentences as being more or less likely. 6. Algorithms exist to find the k most likely parse trees that correspond to a sentence [21]. 7. Algorithms exist to modify rule probabilities in order to maximize the probability of gener ating a corpus [2] [21]. This allows us to use a corpus to train the model's parameters. 8. Algorithms exist to compute prefix probabilities, the probabilities of initial word strings [20]. <p> <ref> [21] </ref>. This allows us to rank hypothesized sentences as being more or less likely. 6. Algorithms exist to find the k most likely parse trees that correspond to a sentence [21]. 7. Algorithms exist to modify rule probabilities in order to maximize the probability of gener ating a corpus [2] [21]. This allows us to use a corpus to train the model's parameters. 8. Algorithms exist to compute prefix probabilities, the probabilities of initial word strings [20].
Reference: [22] <author> F. Jelinek and R. L. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <editor> In E. S. Gelsema and L. N. Kanal, editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 381-397. </pages> <publisher> North-Holland Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: These probabilities can be estimated by using data collected by parsing a corpus with a link grammar and can be combined to estimate P (w i jw i1 ; c) by any number of methods such as deleted interpolation <ref> [22] </ref>. We can calculate P (cjw i1 ; c i1 ) in a similar fashion. <p> How should these components be combined to form one integrated estimate? 5.1. Deleted interpolation Deleted interpolation <ref> [22] </ref> has traditionally been used to alleviate sparse data problems. More detailed models (such as trigrams) are usually combined with less detailed ones (such as bigrams and unigrams) by linear interpolation. The coefficients of the combined model are trained using the forward-backward algorithm on unseen data.
Reference: [23] <author> F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. </author> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 293-295, </pages> <month> Feb </month> <year> 1991. </year>
Reference-contexts: Caches can be used to dynamically modify the parameters of a language model to make it more specific to the text it is given. A number of cache-based techniques have been used such as creating caches of recently used trigrams <ref> [23] </ref>, creating unigram caches based on part-of-speech information [25] [26], and creating selective unigram and bigram caches for words [42]. All of these cache-based systems improved on their non-cache counterparts.
Reference: [24] <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <year> 1987. </year>
Reference-contexts: Table 2 lists the perplexities of a test corpus for three language models. The first model is a bigram model using a backoff scheme <ref> [24] </ref>. The second is our simple maximum entropy model using only the 100 most common connectors. The third is a maximum entropy model that ignores all connector information. It is included to aid in determining the benefit of connectors in the language model.
Reference: [25] <author> R. Kuhn and R. De Mori. </author> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-12(6):570-583, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Caches can be used to dynamically modify the parameters of a language model to make it more specific to the text it is given. A number of cache-based techniques have been used such as creating caches of recently used trigrams [23], creating unigram caches based on part-of-speech information <ref> [25] </ref> [26], and creating selective unigram and bigram caches for words [42]. All of these cache-based systems improved on their non-cache counterparts. The use of caches results in longer range dependencies than n-gram models by allowing words to enter the cache and affect the likelihood of future words.
Reference: [26] <author> R. Kuhn and R. De Mori. </author> <title> Correction to a cache-based natural language model for speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-14(6):691-692, </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: Caches can be used to dynamically modify the parameters of a language model to make it more specific to the text it is given. A number of cache-based techniques have been used such as creating caches of recently used trigrams [23], creating unigram caches based on part-of-speech information [25] <ref> [26] </ref>, and creating selective unigram and bigram caches for words [42]. All of these cache-based systems improved on their non-cache counterparts. The use of caches results in longer range dependencies than n-gram models by allowing words to enter the cache and affect the likelihood of future words.
Reference: [27] <author> J. Lafferty. </author> <title> Gibbs-Markov models. </title> <booktitle> In Computing Science and Statistics: Proceedings of the 27th Symposium on the Interface. Interface Foundation, </booktitle> <year> 1995. </year>
Reference-contexts: One item that is lacking from maximum entropy models is the notion of hidden data, previously seen in section 2.3.3. In the next section we will discuss a framework for constructing probabilistic automata using maximum entropy distributions. 5.3. Gibbs-Markov models Gibbs-Markov models (GMMs) <ref> [27] </ref> are a framework that can be used to build what can be viewed as extended hidden Markov models that allow context-dependent probabilities, thus overcoming one of the severe limitations of HMMs.
Reference: [28] <author> J. Lafferty, D. Sleator, and D. Temperley. </author> <title> Grammatical trigrams: A probabilistic model of link grammar. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <address> Cambridge, MA, </address> <month> October </month> <year> 1992. </year> <note> Also issued as Technical Report CMU-CS-92-181, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1992. </year> <month> 33 </month>
Reference-contexts: We previously saw that n-grams are used in practically all language models today and that we believe it difficult to create a good language model without an n-gram component. Fortunately, the lexicality of link grammar allows n-grams to be easily incorporated into its framework <ref> [28] </ref>. For example, Figure 3 represents a bigram linkage of a sentence. The simple link grammar corresponding to linkages of this form can be trained [28] into a probabilistic model that corresponds with the regular bigram language model. <p> Fortunately, the lexicality of link grammar allows n-grams to be easily incorporated into its framework <ref> [28] </ref>. For example, Figure 3 represents a bigram linkage of a sentence. The simple link grammar corresponding to linkages of this form can be trained [28] into a probabilistic model that corresponds with the regular bigram language model. <p> Another possibility would be to use the robust parser in conjunction with the probabilistic model of link grammars developed in <ref> [28] </ref> and reviewed in the next section. 4.2.4. Previous work on probabilistic link grammars In [28] it is observed that in the link grammar parsing algorithm a link depends on a left connector l of a word L and a right connector r of a word R. <p> Another possibility would be to use the robust parser in conjunction with the probabilistic model of link grammars developed in <ref> [28] </ref> and reviewed in the next section. 4.2.4. Previous work on probabilistic link grammars In [28] it is observed that in the link grammar parsing algorithm a link depends on a left connector l of a word L and a right connector r of a word R. <p> A training algorithm is given in <ref> [28] </ref> that determines the parameters of the probabilistic model given a training corpus by maximum likelihood estimation using the EM algorithm. 4.2.5. Initial approaches Given the grammar introduced in [44] and the robust parser of [17] we could use the probabilistic model and training algorithm of [28] to create a link <p> algorithm is given in <ref> [28] </ref> that determines the parameters of the probabilistic model given a training corpus by maximum likelihood estimation using the EM algorithm. 4.2.5. Initial approaches Given the grammar introduced in [44] and the robust parser of [17] we could use the probabilistic model and training algorithm of [28] to create a link grammar based language model. Instead, we choose to begin with simple models such as bigrams or trigrams and to slowly add grammatical structure and additional information sources to these models. There are a number of reasons to begin with this approach, including the following: 1. <p> There are a number of reasons to begin with this approach, including the following: 1. Basing a language model on a relatively large grammar and parameterizing it as described in <ref> [28] </ref> might lead to a sparse data problem. Grammars can contain large numbers of disjuncts, many of them rare. Obtaining reliable estimates of likelihoods might not be possible. <p> While a deleted interpolation algorithm for smoothing is given in [39], we still prefer to begin with simpler models. 2. It is unclear how to incorporate some elements of link grammars such as multi-connectors and conjunctions into the framework of <ref> [28] </ref>. 3. Given the success of n-gram based language models we would like n-grams to be incorporated into our language models. <p> For example, "trash" would have quite a different set of triggered words when used as a noun than as a verb. Not only do we not have to utilize the probabilistic model of <ref> [28] </ref> in its entirety, our model does not even have to produce valid linkages of its input text. We can utilize the data produced from parsing a training corpus as an information source and use as much or as little of it as we please. <p> We plan on exploring the use of this algorithm to judge the relative worth of features and to incrementally build language models. 7.2.3. Stochastic grammars We would like to design a probabilistic model of link grammars in the same general vein as <ref> [28] </ref> and use it on the Switchboard corpus. We feel that this will be the ultimate test of the our abilities to build better language models by incorporating grammatical information.
Reference: [29] <author> J. Lafferty and B. Suhm. </author> <title> Cluster expansions and iterative scaling of maximum entropy lan-guage models. </title> <booktitle> In Proceedings of the Fifteenth International Workshop on Maximum Entropy and Bayesian Methods. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: The connector information also slightly helped the topic dependent models; in all cases but one, topic dependent models with no connector information had (slightly) higher perplexity than ones with connector information. Models similar to these "connectorless" models were used in a speech recognition system in <ref> [29] </ref> with mixed results. In the next section we will see how we can try to exploit this relationship between topic and connector information.
Reference: [30] <author> A. Lavie and M. Tomita. </author> <title> GLR fl : An efficient noise-skipping parsing algorithm for context free grammars. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies, </booktitle> <pages> pages 123-134, </pages> <year> 1993. </year>
Reference-contexts: The CFG formalism is well understood and for simple grammars it is often easy to see the correspondence between a parse tree and the syntax of a sentence. 4. Robust parsers based on the CFG formalism exist [32] [37] <ref> [30] </ref> [31]. Robust parsing is necessary as language models must assign a probability to any input string. Except in extremely restricted domains, any input is possible. While some inputs might be highly unlikely, they are still possible and language models must still consider them as possibilities. 5. <p> Some might argue that some of these features are limited in usefulness. For example, some of the robust parsers have limitations in speed [32] [37] [31] or language <ref> [30] </ref>, and computing the 1 We are slightly abusing notation here. The probabilities should really be conditioned on the grammar G and in addition, we are using P to denote a number of different probability functions.
Reference: [31] <author> K. J. Lee, C. J. Kweon, J. Seo, and G. C. Kim. </author> <title> A robust parser based on syntactic information. </title> <booktitle> In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <year> 1995. </year>
Reference-contexts: The CFG formalism is well understood and for simple grammars it is often easy to see the correspondence between a parse tree and the syntax of a sentence. 4. Robust parsers based on the CFG formalism exist [32] [37] [30] <ref> [31] </ref>. Robust parsing is necessary as language models must assign a probability to any input string. Except in extremely restricted domains, any input is possible. While some inputs might be highly unlikely, they are still possible and language models must still consider them as possibilities. 5. <p> Some might argue that some of these features are limited in usefulness. For example, some of the robust parsers have limitations in speed [32] [37] <ref> [31] </ref> or language [30], and computing the 1 We are slightly abusing notation here. The probabilities should really be conditioned on the grammar G and in addition, we are using P to denote a number of different probability functions.
Reference: [32] <author> G. Lyon. </author> <title> Syntax-directed least-errors analysis for context-free languages. </title> <journal> Communications of the ACM, </journal> <volume> 17(1) </volume> <pages> 3-14, </pages> <year> 1974. </year>
Reference-contexts: The CFG formalism is well understood and for simple grammars it is often easy to see the correspondence between a parse tree and the syntax of a sentence. 4. Robust parsers based on the CFG formalism exist <ref> [32] </ref> [37] [30] [31]. Robust parsing is necessary as language models must assign a probability to any input string. Except in extremely restricted domains, any input is possible. While some inputs might be highly unlikely, they are still possible and language models must still consider them as possibilities. 5. <p> This might have some advantages when the language model is used in systems such as speech recognition systems where left-to-right processing of the input is useful. Some might argue that some of these features are limited in usefulness. For example, some of the robust parsers have limitations in speed <ref> [32] </ref> [37] [31] or language [30], and computing the 1 We are slightly abusing notation here. The probabilities should really be conditioned on the grammar G and in addition, we are using P to denote a number of different probability functions.
Reference: [33] <author> D. M. Magerman. </author> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <month> Feb </month> <year> 1994. </year>
Reference-contexts: Decision trees were used in this work to estimate the rule probabilities. The authors tested a HBG and found it to be more accurate than a PCFG it was compared to. Another decision tree based model was used by Magerman in <ref> [33] </ref> with good results. While some of these systems seem quite adept at finding good parses of input sentences, they can not be directly used as language models as they don't calculate the prior probabilities we are interested in.
Reference: [34] <author> D. M. Magerman and M. P. Marcus. Pearl: </author> <title> A probabilistic chart parser. </title> <booktitle> In Proceedings of the European ACL Conference, </booktitle> <address> Berlin, Germany, </address> <month> Mar </month> <year> 1991. </year>
Reference-contexts: A number of CFG-based parsers that incorporate additional context have been built. For example, in <ref> [34] </ref>, the probability of a rule was conditioned on the immediate parent of the rule and on limited part-of-speech information. The grammar used in this work differs from a PCFG in the method it uses to score parses.
Reference: [35] <author> D. M. Magerman and C. Weir. </author> <title> Efficiency, robustness, and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <pages> pages 40-47, </pages> <address> Newark, Delaware, </address> <year> 1992. </year>
Reference-contexts: In <ref> [35] </ref> a similar model was examined and the results reported were not as good. Black et. al. [4] described history-based grammars (HGBs) in which rich histories are used to aid in prediction. Essentially, any information previously seen is allowed to influence the expansion of a non-terminal node. <p> As it is difficult to decide which elements from the history to condition on, one might be inclined to condition on a large history. Even relatively small histories, however, can lead to the sparse data problem. For example, in <ref> [35] </ref>, the probability of a rule A ! ff is conditioned on A's parent and the part-of-speech trigram centered on the leftmost word that A dominates. Parts-of-speech rather than words were used for prediction because of the sparse data problem. 3. <p> When conditioned on elements from the history, non-terminals can not always be treated the same and this dependence no longer holds. While 12 work has been done to overcome this limitation <ref> [35] </ref>, new restrictions need to be placed on the model to make parsing practical. 4. All of the models that we are aware of that attempt to incorporate non-trivial historic information were trained from treebank data.
Reference: [36] <author> K. Mark, M. Miller, U. Grenander, and S. Abney. </author> <title> Parameter estimation for constrained context-free language models. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, </booktitle> <pages> pages 146-149, </pages> <address> Harriman, New York, </address> <year> 1992. </year>
Reference-contexts: But in PCFGs, the previous words have no influence on the choice of rules at all! While we believe that syntactic structure will improve language models, the structure must not come at the expense of ignoring restrictions imposed by the lexical components in the sentence. In <ref> [36] </ref>, a language model that combines both bigrams and a stochastic context-free grammar is presented. The relative frequencies of the bigrams are imposed as constraints on the stochastic context-free trees. This approach is important as it attempts to integrate lexical considerations with grammatical structure.
Reference: [37] <author> C. S. Mellish. </author> <title> Some chart-based techniques for parsing ill-formed input. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <pages> pages 102-109, </pages> <year> 1989. </year>
Reference-contexts: The CFG formalism is well understood and for simple grammars it is often easy to see the correspondence between a parse tree and the syntax of a sentence. 4. Robust parsers based on the CFG formalism exist [32] <ref> [37] </ref> [30] [31]. Robust parsing is necessary as language models must assign a probability to any input string. Except in extremely restricted domains, any input is possible. While some inputs might be highly unlikely, they are still possible and language models must still consider them as possibilities. 5. <p> Some might argue that some of these features are limited in usefulness. For example, some of the robust parsers have limitations in speed [32] <ref> [37] </ref> [31] or language [30], and computing the 1 We are slightly abusing notation here. The probabilities should really be conditioned on the grammar G and in addition, we are using P to denote a number of different probability functions.
Reference: [38] <author> M. I. Miller and J. A. O'sullivan. </author> <title> Entropies and combinatorics of random branching processes and context-free languages. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(4), </volume> <month> Mar </month> <year> 1992. </year>
Reference: [39] <author> S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. </author> <title> Inference and estimation of a long-range trigram model. </title> <booktitle> In Proceedings of the Second International Colloquium on Grammatical Inference and Applications. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <pages> 862. </pages>
Reference-contexts: The percentage of new tuples of the form (w; L; R; l; r) appearing in the test data would probably be much higher than the percentages reported in [19] [6] for trigrams. While a deleted interpolation algorithm for smoothing is given in <ref> [39] </ref>, we still prefer to begin with simpler models. 2. It is unclear how to incorporate some elements of link grammars such as multi-connectors and conjunctions into the framework of [28]. 3.
Reference: [40] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty. </author> <title> Inducing features of random fields. </title> <type> Technical Report CMU-CS-95-144, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> 5000 Forbes Avenue, Pittsburgh, PA 15213, </address> <year> 1995. </year>
Reference-contexts: While deleted interpolation can be used to combine our language model components, we will be using a different method, which uses a maximum entropy approach [42] [3] <ref> [40] </ref>. 5.2. Maximum entropy principle The maximum entropy method [18] chooses a model that agrees with certain explicit constraints yet avoids assuming anything that has not been specified. For example, we might know that in our training data, the noun "toad" is modified by adjective "slimy" 10 times. <p> Among all of the elements of the exponential family E, the maximum entropy distribution is also the maximum likelihood solution. The maximum entropy principle has been successfully used as a method to combine information sources in natural language applications [42] [3] <ref> [40] </ref> and has a number of appealing properties: 1. The principle is simple, intuitive and easily allows one to combine multiple information sources. The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2. <p> The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2. A number of numerical algorithms (including generalized iterative scaling [13] and improved versions of it [3] <ref> [40] </ref>) exist for computing the parameters i of the maximum entropy solution The maximum entropy solution is unique, and these algorithms are guaranteed to converge to it. 3. <p> While this won't result in the maximum entropy solution, the result might still be good enough for many purposes. This method was used to incrementally add constraints to models in [3] and <ref> [40] </ref>. One item that is lacking from maximum entropy models is the notion of hidden data, previously seen in section 2.3.3. In the next section we will discuss a framework for constructing probabilistic automata using maximum entropy distributions. 5.3. <p> The robust link grammar parser will allow us to capture many grammatical relations and we expect that a number of them will be beneficial to language modeling. 7.2.2. Feature induction In <ref> [40] </ref> a field induction algorithm is given to incrementally construct random fields and it is applied to discover morphological features of words. We plan on exploring the use of this algorithm to judge the relative worth of features and to incrementally build language models. 7.2.3.
Reference: [41] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 267-296. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: This is illustrated in Figure 2. Note that this corresponds to our definition of HMMs in that the output symbols depend only on the current (hidden) state and the state transitions depend only on the previous states. There are efficient algorithms <ref> [41] </ref> that attempt to determine the parameters of the HMM (a s;t , b (s), and (s)) that maximize the likelihood of a given observed sequence. Unfortunately the algorithms can get stuck in local maxima.
Reference: [42] <author> R. Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Apr </month> <year> 1994. </year> <note> Available as Technical Report CMU-CS-94-138. </note>
Reference-contexts: A number of cache-based techniques have been used such as creating caches of recently used trigrams [23], creating unigram caches based on part-of-speech information [25] [26], and creating selective unigram and bigram caches for words <ref> [42] </ref>. All of these cache-based systems improved on their non-cache counterparts. The use of caches results in longer range dependencies than n-gram models by allowing words to enter the cache and affect the likelihood of future words. <p> In the next section we will look at triggers, which can be viewed as an extended notion of a cache. 2.3.5. Trigger models In <ref> [42] </ref>, Rosenfeld demonstrated that triggers can be a useful component of a language model. He defined (A ! B) to be a trigger pair if the word sequence A is significantly correlated with the word sequence B. <p> Grammatical information will allow us to weed out certain words we should not cache (such as function words) and help us to focus on words that are more likely to benefit from caching (such as nouns and objects). * Triggers: In <ref> [42] </ref>, triggers were found to be a useful information bearing element to incorporate into language models and it would probably be beneficial to include them in ours as well. By utilizing link information we can examine the possible benefits of utilizing part-of-speech or other grammatical considerations when choosing trigger pairs. <p> Not limited to solving sparse data problems, deleted interpolation can also be used to combine any number of language models into one integrated scheme. While deleted interpolation can be used to combine our language model components, we will be using a different method, which uses a maximum entropy approach <ref> [42] </ref> [3] [40]. 5.2. Maximum entropy principle The maximum entropy method [18] chooses a model that agrees with certain explicit constraints yet avoids assuming anything that has not been specified. For example, we might know that in our training data, the noun "toad" is modified by adjective "slimy" 10 times. <p> Among all of the elements of the exponential family E, the maximum entropy distribution is also the maximum likelihood solution. The maximum entropy principle has been successfully used as a method to combine information sources in natural language applications <ref> [42] </ref> [3] [40] and has a number of appealing properties: 1. The principle is simple, intuitive and easily allows one to combine multiple information sources. The maximum likelihood dual seems to lend additional evidence that the maxi mum likelihood solution is a natural one to use. 2.
Reference: [43] <author> C. E. Shannon. </author> <title> Prediction and entropy of printed english. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: Trials of this game indicate that a number of different aspects of the history such as topic, tense, number, and structure were found to be used with both syntactic and semantic considerations playing important roles in the humans' predictions. Shannon <ref> [43] </ref> reported results for a variation of this game where humans predicted letters using an alphabet of 27 symbols (26 letters and the space). The result was an upper-bound estimate of the entropy of English of 1.30 bits per character.
Reference: [44] <author> D. D. K. Sleator and D. Temperley. </author> <title> Parsing English with a link grammar. </title> <institution> Technical Re--port CMU-CS-91-196, School of Computer Science, Carnegie Mellon University, </institution> <address> 5000 Forbes Avenue, Pittsburgh, PA 15213, </address> <year> 1991. </year> <month> 35 </month>
Reference-contexts: In addition they must contain syntactic structure we can utilize to place restrictions on word choices. The models must be based on a formalism that lends itself to extracting relevant information from the data its grammars generate. We propose basing our language models on link grammars <ref> [44] </ref>. 4.2. Link grammar A formal grammatical system called link grammar was introduced by Sleator and Temperley in [44]. Although the expressive power of link grammars is equivalent to that of context-free grammars, Sleator and Temperley believe that the encoding of natural language grammars is much easier in their system. <p> The models must be based on a formalism that lends itself to extracting relevant information from the data its grammars generate. We propose basing our language models on link grammars <ref> [44] </ref>. 4.2. Link grammar A formal grammatical system called link grammar was introduced by Sleator and Temperley in [44]. Although the expressive power of link grammars is equivalent to that of context-free grammars, Sleator and Temperley believe that the encoding of natural language grammars is much easier in their system. <p> The links attached to each word in a linkage must satisfy that word's formula. That is, the connectors must make the logical expression represented by the formula true, with the or operator understood to be exclusive. 4.2.2. Link grammar for language modeling In <ref> [44] </ref> it was shown that link grammars have the same expressive power as context-free grammars. Yet in spite of our rejection of PCFGs as our basis for language modeling, we choose to go forward with link grammars. <p> A training algorithm is given in [28] that determines the parameters of the probabilistic model given a training corpus by maximum likelihood estimation using the EM algorithm. 4.2.5. Initial approaches Given the grammar introduced in <ref> [44] </ref> and the robust parser of [17] we could use the probabilistic model and training algorithm of [28] to create a link grammar based language model. <p> Given the success of n-gram based language models we would like n-grams to be incorporated into our language models. We could add n-gram disjuncts to the grammar given in <ref> [44] </ref>, but that would introduce numerous linkages of the sentences in the training corpus, considerably increasing training time. 17 4.
References-found: 44

