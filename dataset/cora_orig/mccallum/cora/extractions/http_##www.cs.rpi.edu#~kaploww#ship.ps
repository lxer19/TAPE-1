URL: http://www.cs.rpi.edu/~kaploww/ship.ps
Refering-URL: http://www.cs.rpi.edu/~kaploww/research.html
Root-URL: http://www.cs.rpi.edu
Email: fkaploww,szymansk,tannenpg@cs.rpi.edu  vdecyk@pepper.physics.ucla.edu  
Title: Run-Time Reference Clustering for Cache Performance Optimization  
Author: Wesley K. Kaplow Boleslaw K. Szymanski Peter Tannenbaum Viktor K. Decyk and 
Address: Troy, N.Y. 12180-3590, USA  Los Angeles, CA. 90024, USA  Pasadena, CA. 91109, USA  
Affiliation: Department of Computer Science Scientific Computation Research Center Rensselaer Polytechnic Institute  Physics Department, UCLA  Jet Propulsion Laboratory California Institute of Technology  
Abstract: We introduce a method for improving the cache performance of irregular computations in which data are referenced through run-time defined indirection arrays. Such computations often arise in scientific problems. The presented method, called Run-Time Reference Clustering (RTRC), is a run-time analog of a compile-time blocking used for dense matrix problems. RTRC uses the data partitioning and re-mapping techniques that are a part of distributed memory multi-processor codes designed to minimize interprocessor communication. Re-mapping each set of local data decreases cache-misses the same way remapping the global data decreases off-processor references. We demonstrate the applicability and performance of the RTRC technique on several prevalent applications: Sparse Matrix-Vector Multiply, Particle-In-Cell, and CHARMM-like codes. Performance results on SPARC-20, SP-2, and T3-D processors show that single node execution performance can be improved by as much as 35%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Birdsall and A. B. Langdon. </author> <title> Plasma Physics via Computer Simulation. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: For static irregular codes, such as sparse matrix-vector multiplication, the number of clusters and their sizes are determined statically (once during run-time), similar to [11]. For dynamic scientific applications, such as Particle-In-Cell <ref> [1] </ref> and CHARMM [2], the clusters' scopes can change. Run-time management used to maintain the reference clusters is similar to local memory data-reallocation done for load balancing. The remainder of the paper is structured as follows.
Reference: [2] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: For static irregular codes, such as sparse matrix-vector multiplication, the number of clusters and their sizes are determined statically (once during run-time), similar to [11]. For dynamic scientific applications, such as Particle-In-Cell [1] and CHARMM <ref> [2] </ref>, the clusters' scopes can change. Run-time management used to maintain the reference clusters is similar to local memory data-reallocation done for load balancing. The remainder of the paper is structured as follows. Section 2 describes the RTRC model itself and application types that are amenable to it.
Reference: [3] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Pon-nusamy. </author> <title> Design and implementation of a parallel unstructured euler solver using software primitives. </title> <journal> AIAA, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Another approach is to modify the reference order to improve locality. For certain applications, such as finite-element methods, the performance of the cache can be improved by applying algorithms that narrow the bandwidth of the sparse-matrix constructed increasing reference locality <ref> [3] </ref>. However, this does not address the details of reuse, nor is it generally applicable to irregular problems. To our knowledge, [11] is the only paper that applies data repartitioning explicitly for cache optimization.
Reference: [4] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communic-tion optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 462-478, </pages> <year> 1994. </year>
Reference-contexts: However, these methods introduce the problems of determin ing the run-time dependent remote access requirements of the application, and providing efficient facilities to perform the communication. These problems are addressed in the CHAOS/PARTI run-time and compilation methods <ref> [4, 12, 10] </ref>. The essential technique is the inspector/executor model in which the inspector is used to determine which references are required for execution, and the executor performs the communication and the actual computation.
Reference: [5] <author> V. K. Decyk, S. R. Karmesin, A. de Boer, and P. C. Liewer. </author> <title> Optimization of particle-in-cell codes on risc processors. </title> <booktitle> Computers in Physics, </booktitle> <year> 1995. </year> <note> Submitted for Publication. </note>
Reference-contexts: Cache performance can be poor because references to the FX and FY arrays are dependent on the run-time values of the PART array which change during execution. Over time, the reference pattern to FX and FY become essentially random, leading to as much as a 40% performance decrease <ref> [5] </ref> over an initially sorted order. The creation of RCs in the PIC code directly follows the approach that supports multi-processor execution. In this case the same data structures that provide the mechanism for interprocessor communication are reused to supply inter-RC object movement.
Reference: [6] <author> K. Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, </publisher> <year> 1993. </year>
Reference-contexts: During compilation of such programs it is impossible to determine a loop structure that will confine references to the current contents of the cache. It is exactly this type of problem that this paper addresses. Several methods have been explored for improving cache performance of irregular computation. Multithreading <ref> [6] </ref> attempts to hide memory latency by creating many parallel threads of computation that can be finely scheduled with respect to the availability of data to process. Another approach is to modify the reference order to improve locality.
Reference: [7] <author> W. K. Kaplow and B. K. Szymanski. </author> <title> Program optimization based on compile-time cache performance prediction. </title> <journal> Parallel Processing Letters, </journal> <volume> 6(1) </volume> <pages> 173-184, </pages> <year> 1996. </year>
Reference-contexts: There are several methods that can be used to determine the size of the arrays allocated to the virtual processors. An execution or benchmark method can be used, or a cache performance estimation technique can be employed. We describe a method in <ref> [7] </ref> that can be used to determine quickly the optimal range values by partial simulated execution of the application code on an architecturally correct model of the target processor's cache. Reference Cluster Creation and Code Modification.
Reference: [8] <author> D. R. Kincaid, J. R. Respess, D. M. Young, and R. G. Grimes. </author> <title> Itpack 2c: A fortran package for solving large sparse linear systems by adaptive accelerated iterative methods. </title> <type> Technical report, </type> <institution> University of Texas at Austin, </institution> <year> 1992. </year>
Reference-contexts: Techniques for improving the performance of sparse matrix-vector multiplication on parallel architectures have focused on improving processor partitioning to reduce remote processor communication costs [13]. Figure 2 shows the code for sparse matrix-vector multiplication taken from ITPACK <ref> [8] </ref> with RTRC applied (The code multiplies A by X to find W .) In general, a multi-processor implementation of this algorithm would assign to each processor a contiguous number of rows of A,X, and W .
Reference: [9] <author> C. D. Norton, B. K. Szymanski, and V. K. Decyk. </author> <title> Object-oriented parallel computation for plasma simulation. </title> <journal> Communications of the ACM, </journal> <volume> 38(10), </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: Particle interactions are not modeled directly, but through the fields which they produce. Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. The General Concurrent Particle in Cell (GCPIC) Algorithm <ref> [9] </ref> partitions the particles and grid points among the processors of the MIMD (multiple-instruction, multiple-data) distributed-memory machine. A secondary decomposition partitions the simulation space evenly among processors, which makes solving the field equations on the grid efficient.
Reference: [10] <author> R. Ponnusamy, J. Saltz, A. Choudhary, Y.-S. Hwang, and G. Fox. </author> <title> Runtime support and compilation methods for user-specified irregular data distributions. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(8) </volume> <pages> 815-831, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: However, these methods introduce the problems of determin ing the run-time dependent remote access requirements of the application, and providing efficient facilities to perform the communication. These problems are addressed in the CHAOS/PARTI run-time and compilation methods <ref> [4, 12, 10] </ref>. The essential technique is the inspector/executor model in which the inspector is used to determine which references are required for execution, and the executor performs the communication and the actual computation.
Reference: [11] <author> K. A. Tomko and S. G. Abraham. </author> <title> Data and program restructuring of irregular applications for cache-coherent multiprocessors. </title> <booktitle> In 8th ACM International Conference on Supercomputing, </booktitle> <address> Manchester, England. </address> <publisher> ACM, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: However, this does not address the details of reuse, nor is it generally applicable to irregular problems. To our knowledge, <ref> [11] </ref> is the only paper that applies data repartitioning explicitly for cache optimization. <p> The references of an irregular program are clustered in such a way that the data referenced in each cluster fits within the processor's cache. For static irregular codes, such as sparse matrix-vector multiplication, the number of clusters and their sizes are determined statically (once during run-time), similar to <ref> [11] </ref>. For dynamic scientific applications, such as Particle-In-Cell [1] and CHARMM [2], the clusters' scopes can change. Run-time management used to maintain the reference clusters is similar to local memory data-reallocation done for load balancing. The remainder of the paper is structured as follows.
Reference: [12] <author> J. Wu, R. Das, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(2) </volume> <pages> 737-753, </pages> <year> 1995. </year>
Reference-contexts: However, these methods introduce the problems of determin ing the run-time dependent remote access requirements of the application, and providing efficient facilities to perform the communication. These problems are addressed in the CHAOS/PARTI run-time and compilation methods <ref> [4, 12, 10] </ref>. The essential technique is the inspector/executor model in which the inspector is used to determine which references are required for execution, and the executor performs the communication and the actual computation.
Reference: [13] <author> L. H. Ziantz, C. C. Ozturan, and B. K. Szymanski. </author> <title> Run-time optimization of sparse matrix-vector multiplication on simd machines. </title> <editor> In C. Halatsis, D. Maritsas, G. Philokyprou, and S. Theodoridis, editors, </editor> <booktitle> PARLE 94 Parallel Architectures and Languages Europe, </booktitle> <address> Athens, Greece, </address> <booktitle> volume 817 of Lecture Notes in Computer Science, </booktitle> <pages> pages 313-322. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: Techniques for improving the performance of sparse matrix-vector multiplication on parallel architectures have focused on improving processor partitioning to reduce remote processor communication costs <ref> [13] </ref>. Figure 2 shows the code for sparse matrix-vector multiplication taken from ITPACK [8] with RTRC applied (The code multiplies A by X to find W .) In general, a multi-processor implementation of this algorithm would assign to each processor a contiguous number of rows of A,X, and W .
References-found: 13

