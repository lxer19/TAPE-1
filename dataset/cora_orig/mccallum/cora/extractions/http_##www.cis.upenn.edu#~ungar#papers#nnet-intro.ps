URL: http://www.cis.upenn.edu/~ungar/papers/nnet-intro.ps
Refering-URL: http://www.cis.upenn.edu/~ungar/papers.html
Root-URL: 
Abstract: Draft A Brief Introduction to Neural Networks Richard D. De Veaux Lyle H. Ungar Williams College University of Pennsylvania Abstract Artificial neural networks are being used with increasing frequency for high dimensional problems of regression or classification. This article provides a tutorial overview of neural networks, focusing on back propagation networks as a method for approximating nonlinear multivariable functions. We explain, from a statistician's vantage point, why neural networks might be attractive and how they compare to other modern regression techniques. KEYWORDS: nonparametric regression; function approximation; backpropagation. 1 Introduction Networks that mimic the way the brain works; computer programs that actually LEARN patterns; forecasting without having to know statistics. These are just some of the many claims and attractions of artificial neural networks. Neural networks (we will henceforth drop the term artificial, unless we need to distinguish them from biological neural networks) seem to be everywhere these days, and at least in their advertising, are able to do all that statistics can do without all the fuss and bother of having to do anything except buy a piece of software. Neural networks have been successfully used for many different applications including robotics, chemical process control, speech recognition, optical character recognition, credit card fraud detection, interpretation of chemical spectra and vision for autonomous navigation of vehicles. (Pointers to the literature are given at the end of this article.) In this article we will attempt to explain how one particular type of neural network, feedforward networks with sigmoidal activation functions ("backpropagation networks") actually works, how it is "trained", and how it compares with some more well known statistical techniques. As an example of why someone would want to use a neural network, consider the problem of recognizing hand written ZIP codes on letters. This is a classification problem, where the 1 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barron, A.R., Barron, R.L., and Wegman, E.J. </author> <year> (1992), </year> <title> "Statistical learning networks: A unifying view", </title> <booktitle> in Computer Science and Statistics: Proceedings of the 20th Symposium on the Interface, edited by E.J. Wegman, </booktitle> <pages> 192-203. </pages>
Reference: <author> Broomhead, </author> <title> D.S., and D.Lowe (1988), "Multivarible functional interpolation and adap-tive networks." </title> <booktitle> Complex Systems 2, </booktitle> <pages> 321-355. </pages>
Reference: <author> Cheng, B. and D.M. </author> <month> Titterington </month> <year> (1994), </year> <title> "Neural Networks: a review from a statistical perspective, with discussion." </title> <journal> Stat. </journal> <note> Science 9(1) 2-54 De Veaux, </note> <author> R. D., Psichogios, D. C., and Ungar, L. H. </author> <title> (1993a) "A Tale of Two NonParametric Estimation Schemes: MARS and Neural Networks." </title> <booktitle> in Fourth International Workshop on Artificial Intelligence and Statistics. </booktitle>
Reference: <author> De Veaux, R. D., Psichogios, D. C., and Ungar, L. H. </author> <title> (1993b) "A Comparison of Two Non-Parametric Estimation Schemes: MARS and Neural Networks." </title> <journal> Computers and Chemical Engineering, </journal> <volume> 17(8), </volume> <pages> 819-837. </pages>
Reference-contexts: need to learn other more complex methods | in short, they promise to avoid the need for statistics! This is largely not true: for example, outliers should be removed before training neural nets, and users should pay attention to distributions of errors as well as summaries such as the mean <ref> (see De Veaux et al. 1993b) </ref>. However, the neural network promise of 16 Draft "statistics-free statistics" is partly true.
Reference: <author> DeWeerth, S.P., Nielsen, L., </author> <title> Mead, </title> <address> C.A., Astrom, K.J. </address> <year> (1991), </year> <title> "A Simple neuron servo," </title> <journal> IEEE Transactions on Neural Nets, </journal> <volume> 2(2), </volume> <pages> 248-251. </pages>
Reference-contexts: Besides more accurately reflecting biological neurons (an unclear advantage), continuous time networks are more naturally implemented in some types of computer chips <ref> (DeWeerth et al. 1991) </ref>. 4.1 Recurrent networks One of the more important variations on feedforward networks arises when the network is supplemented with delay lines, so that outputs from the hidden layers can be remembered and used as inputs at a later time. Such networks are said to be recurrent.
Reference: <author> Friedman, J.H. </author> <year> (1991), </year> <title> "Multivariate adaptive regression splines" The Annals of Statistics 19(1), </title> <type> 1-141. </type>
Reference-contexts: Many are limited to linear systems or systems with a single dependent and independent variable, and as such, do not address the same problem as neural networks. Others such as multivariate regression splines <ref> (MARS, Friedman 1991) </ref>, generalized additive models (Hastie and Tibshirani, 1986) projection pursuit regression (Friedman and Stutzle 1981), compete directly with neural nets. It is informative to compare these methods in their philosophy and their effectiveness on different types of problems. <p> However, with many predictors, the computational burden of choosing which pairs to consider can soon become overwhelming. Partially as an attempt to alleviate this problem, Friedman devised the multivariate adaptive regression splines (MARS) algorithm <ref> (Friedman 1991) </ref>. MARS selects both the amount of smoothing for each predictor and the interaction order of the predictors automatically. <p> The final model is of the form: E (Y jX 1 ; : : : ; X p ) = ff + m=1 K m Y [s km (x -(k;m) t km )] + (12) Here s km = 1 and x -(k;m) is one of the original predictors. <ref> (For more information see Friedman 1991) </ref>. One of the main advantages of MARS is the added interpretability gained by variable selection, and the graphical displays of low order main effect and interaction terms.
Reference: <author> Friedman, J.H. and Silverman, B., </author> <title> (1987) "Flexible parsimonious smoothing and additive modeling ",Stanford Technical Report, </title> <month> Sept. </month> <year> 1987. </year>
Reference: <author> Friedman, J.H., and Stuetzle W. </author> <title> (1981) "Projection pursuit regression," </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76, </volume> <pages> 817-823. </pages>
Reference-contexts: Many are limited to linear systems or systems with a single dependent and independent variable, and as such, do not address the same problem as neural networks. Others such as multivariate regression splines (MARS, Friedman 1991), generalized additive models (Hastie and Tibshirani, 1986) projection pursuit regression <ref> (Friedman and Stutzle 1981) </ref>, compete directly with neural nets. It is informative to compare these methods in their philosophy and their effectiveness on different types of problems. <p> linear combinations: E (Y jX 1 ; : : : ; X p ) = ff + f im ( ff im X j ): (13) The `directions' are found iteratively by numerically minimizing the fraction of variance unexplained by a smooth of y versus P (j) im X j <ref> (Friedman and Stuetzle 1981) </ref>. As linear combinations are added to the predictors, the residuals from the smooth are substituted for the response.
Reference: <author> Geman, S., and Bienenstock E. </author> <year> (1992), </year> <title> "Neural Networks and the Bias/Variance Dilemma." </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-58. </pages>
Reference: <author> Haykin, S., </author> <title> Neural Networks: A comprehensive Foundation, </title> <publisher> Macmillan, </publisher> <address> NY, </address> <note> 1994 Haesloop, </note> <author> D., and Holt, B. R. </author> <title> (1990) "A Neural Network Structure for System Identification." </title> <booktitle> Proceedings of the American Control Conference 2460-2465. </booktitle>
Reference: <author> Hastie, T. and Tibshirani, R., </author> <title> (1986),"Generalized Additive Models." </title> <journal> Statistical Science 1:3, </journal> <pages> 297-318. </pages>
Reference-contexts: Many are limited to linear systems or systems with a single dependent and independent variable, and as such, do not address the same problem as neural networks. Others such as multivariate regression splines (MARS, Friedman 1991), generalized additive models <ref> (Hastie and Tibshirani, 1986) </ref> projection pursuit regression (Friedman and Stutzle 1981), compete directly with neural nets. It is informative to compare these methods in their philosophy and their effectiveness on different types of problems. <p> side of equation 10: g (E (Y jX 1 ; : : : ; X p )) = ff + i The function g () is, as with generalized linear models, referred to as the link function, and the model in equation 11 is known as a generalized additive model <ref> (Hastie and Tibshirani, 1986, 1990) </ref>. one can add terms f jk (X j ; X k ) to equation 11. However, with many predictors, the computational burden of choosing which pairs to consider can soon become overwhelming.
Reference: <author> Hastie, T.J. and Tibshirani, </author> <title> R.J., Generalized Additive Models, </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <note> 1990 18 Draft Hertz,J., </note> <author> Krogh A., and Palmer, R.G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation, </title> <address> Addison-Welsey, Reading, MA. </address>
Reference: <author> Holcomb, T. R., and Morari, M. </author> <title> (1992) "PLS/Neural Networks." </title> <journal> Computers and Chemical Engineering 16:4, </journal> <pages> 393-411. </pages>
Reference-contexts: They can take data which have been processed with a smoothing filter or can work with the loading vectors produced by linear projection methods such as PCA or PLS <ref> (Holcomb and Morari 1992, Qin and McAvoy 1992) </ref>. 12 Draft In another set of variations, artificial neural network can be made which are continuous in time.
Reference: <author> Kulkarni, A.D. </author> <year> (1994), </year> <title> "Artificial neural networks for image understanding" Van Nostrand Renhold, </title> <address> New York. </address>
Reference: <author> Leonard J., Kramer, M., and Ungar, </author> <title> L.H. (1992) "Using radial basis functions to ap-proximate a function and its error bounds." </title> <journal> IEEE Transactions on Neural Nets, </journal> <volume> 3(4), </volume> <pages> 624-627. </pages>
Reference-contexts: This property of RBFs can be used to build a network which warns when it is extrapolating <ref> (Leonard, Kramer and Ungar 1992) </ref>. Again, many variations have been studied: the Gaussians can be selected to be elliptical, rather than spherical, or other forms of local basis function can be used. 5 Comparison with other methods Statisticians and engineers have developed many methods for nonparametric multiple regression.
Reference: <author> Lippmann, </author> <title> R.P., (1987) "An introduction to computing with neural nets." </title> <journal> IEEE ASSP Magazine, </journal> <pages> 4-22. </pages>
Reference: <author> MacKay, D.J.C., </author> <title> (1992) "A Practical Bayesian Framework for Backpropagation Networks." </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 448-472. </pages>
Reference-contexts: In particular, in recent years, a number of regularization methods have been proposed to control the smoothness, and hence the degree of overfitting, in neural networks. As just one example, one can use Bayesian techniques to select network structures and to penalize large weights or reduce overfitting <ref> (MacKay, 1992) </ref>. 3.1 Example Continuing with our regression example, we consider using the hidden layer sigmoidal network with 5 nodes in the hidden layer, as before.
Reference: <author> Mammone, </author> <title> R.J., editor, (1994) "Artificial neural networks for speech and vision" Chapman and Hall, </title> <address> New York. </address>
Reference: <author> Miller, III, W.T, Sutton, </author> <title> R.S., </title> <editor> and Werbos. P.J., eds. </editor> <title> (1990) Neural Networks for Control MIT Press, </title> <address> Cambridge, Mass.. </address>
Reference: <author> Moody, J. and Darken, C.J., </author> <title> (1989) "Fast learning in networks of locally tuned processing units." </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-329. </pages>
Reference: <author> Narendra, K. S., and Parthasarathy, K. </author> <title> (1990) "Identification and Control of Dynamical Systems Using Neural Networks." </title> <journal> IEEE Transactions on Neural Networks 1, </journal> <pages> 4-27. </pages>
Reference: <author> Poggio, T., and Girosi, F. </author> <title> (1990) "Regularization algorithms for learning that are equiv-alent to multilayer networks." </title> <booktitle> Science 247, </booktitle> <pages> 978-982. </pages>
Reference: <author> Pomerleau, D.A. </author> <title> (1990) "Neural network based autonomous navigation." Vision and Navigation: </title> <address> The CMU Navlab Charles Thorpe, </address> <publisher> (Ed.) Kluwer Academic Publishers. </publisher>
Reference: <author> Psichogios, </author> <title> D.C. and Ungar, L.H. (1992) "A Hybrid Neural Network | First Principles Approach to Process Modeling." </title> <journal> AIChE Journal, </journal> <volume> 38(10), </volume> <pages> 1499-1511. </pages>
Reference-contexts: Neural networks can be built which have only partial connectivity, when one believes that not all input variables interact. Networks can also incorporate first principles equations such as mass and energy balances <ref> (Psichogios and Ungar 1992) </ref>, or known linear approximations to the process being modeled (Haesloop and Holt 1990).
Reference: <author> Qin, S. J., and McAvoy, T. J. </author> <title> (1992) "Nonlinear PLS Modeling Using Neural Networks." </title> <journal> Computers and Chemical Engineering 16:4, </journal> <pages> 379-391. </pages>
Reference-contexts: They can take data which have been processed with a smoothing filter or can work with the loading vectors produced by linear projection methods such as PCA or PLS <ref> (Holcomb and Morari 1992, Qin and McAvoy 1992) </ref>. 12 Draft In another set of variations, artificial neural network can be made which are continuous in time.
Reference: <author> Ripley, </author> <title> B.D. (1993) "Statistical Aspects of Neural Networks." In Networks and Chaos - Statistical and Probabilistic Aspects (eds. </title> <editor> O.E. Barndorff-Nielsen, J. L. Jensen and W.S. </editor> <publisher> Kendall),40-123,Chapman and Hall, London. </publisher>
Reference: <author> Ripley, B.D. </author> <year> (1994), </year> <title> "Neural networks and related methods for classification." Journal of the Royal Statistical Society Series B 56(3) 409-437 19 Draft Rumelhart, </title> <editor> D., Hinton, G., and Williams, R. </editor> <title> (1986) "Learning Internal Representations by Error Propagation." Parallel Distributed Processing: </title> <journal> Explorations in the Microstructures of Cognition, </journal> <volume> Vol 1: </volume> <booktitle> Foundations Cambridge: </booktitle> <publisher> MIT Press, </publisher> <pages> 318-362. </pages>
Reference: <author> Sanner, R.M. and Slotine J.-J.E., </author> <title> (1991) "Direct Adaptive Control with Gaussian Networks." </title> <booktitle> Proc. 1991 Automatic Control Conference, </booktitle> <volume> 3, </volume> <pages> 2153-2159. </pages>
Reference-contexts: Equally importantly, unlike sigmoidal networks, new data will not affect predictions in other regimes of the input space for these networks. This has been used to good effect in adaptive process control applications <ref> (Sanner and Slotine 1991, Narendra and Parthasarathy 1990) </ref>. Radial basis functions also have the advantage that their output provides a local measure of data density.
Reference: <author> White, H., </author> <title> (1989) "Learning in neural networks: a statistical perspective." </title> <journal> Neural Computation, </journal> <volume> 1(4), 425- 464. </volume>
Reference: <author> Wythoff, B.J. </author> <title> (1993) "Backpropagation neural networks a tutorial." Chemometrics and Intelligent laboratory systems 18(2) 115-155 Zupan, </title> <editor> J. and Gasteiger, J. </editor> <title> (1991) "Neural networks: a new method for solving chemical problems or just a passing phase?" Analytica Chimica Acta, </title> <type> 248, 1-30. 20 Draft 21 Draft 22 Draft 23 Draft 24 Draft 25 Draft 26 Draft 27 Draft 28 </type>
Reference-contexts: A good place to find more basic information is in textbooks such as (Haykin 1994; Herz, Krogh and Palmer 1991) and in review articles by statisticians such as (Cheng and Titterington 1994; Ripley 1994) and for chemometri- cians, <ref> (Wythoff 1993) </ref>. A particularly nice set of references is available in the Neural Network FAQ (FAQ.NN) available at the web site http://wwwipd.ira.uka.de/prechelt/FAQ/neural- net-faq-html and on the newsgroup "comp.ai.neural-nets". For more up-to-date research, see recent proceedings of the NIPS or IJCNN conferences.
References-found: 30

