URL: ftp://ftp.cs.ucsd.edu/pub/sfink/pub/iscope97.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~sfink/pub.html
Root-URL: http://www.cs.ucsd.edu
Title: Runtime Support for Multi-Tier Programming of Block-Structured Applications on SMP Clusters  
Author: Stephen J. Fink and Scott B. Baden 
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: December 1997  
Address: (Marina Del Rey, CA),  La Jolla, CA 92093-0114  
Note: To appear at ISCOPE '97  
Abstract: We present a small set of programming abstractions to simplify efficient implementations for block-structured scientific calculations on SMP clusters. We have implemented these abstractions in KeLP 2.0, a C++ class library. KeLP 2.0 provides hierarchical SMPD control flow to manage two levels of parallelism and locality. Additionally, to tolerate slow inter-node communication costs, KeLP 2.0 combines inspector/executor communication analysis with overlap of communication and computation. We illustrate how these programming abstractions hide the low-level details of thread management, scheduling, synchronization, and message-passing, but allow the programmer to express efficient al gorithms with intuitive geometric primitives.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. R. Woodward, </author> <title> "Perspectives on supercomputing: Three decades of change," </title> <journal> IEEE Computer, </journal> <volume> vol. 29, </volume> <pages> pp. 99-111, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Multi-tier parallel computers, such as clusters of symmetric multiprocessors (SMPs), have emerged as important platforms for high-performance computing <ref> [1] </ref>. A multi-tier computer, with several levels of locality and parallelism, presents a more complex non-uniform memory hierarchy than a single-tier mul-ticomputer with uniprocessor nodes. In order to use multi-tier platforms efficiently, the programmer or compiler must orchestrate parallelism and locality to match the hardware capabilities.
Reference: 2. <author> Message-Passing Interface Standard, </author> <title> "MPI: A message-passing interface standard," </title> <institution> University of Tennessee, Knoxville, TN, </institution> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: In order to use multi-tier platforms efficiently, the programmer or compiler must orchestrate parallelism and locality to match the hardware capabilities. On single-tier parallel computers, MPI <ref> [2] </ref> and HPF [3] have emerged as standard approaches to portable parallel programming. However, the proper programming model for multi-tier parallel computers remains an unresolved issue. At present, the programmer faces myriad options regarding the coordination of heavyweight processes, lightweight threads, shared memory, message-passing, synchronization, scheduling, and load balancing [4].
Reference: 3. <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: In order to use multi-tier platforms efficiently, the programmer or compiler must orchestrate parallelism and locality to match the hardware capabilities. On single-tier parallel computers, MPI [2] and HPF <ref> [3] </ref> have emerged as standard approaches to portable parallel programming. However, the proper programming model for multi-tier parallel computers remains an unresolved issue. At present, the programmer faces myriad options regarding the coordination of heavyweight processes, lightweight threads, shared memory, message-passing, synchronization, scheduling, and load balancing [4].
Reference: 4. <author> W. W. Gropp and E. L. Lusk, </author> <title> "A taxonomy of programming models for symmetric multiprocessors and SMP clusters," </title> <booktitle> in Proceedings 1995: Programming models for massively parallel computers, </booktitle> <pages> pp. 2-7, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: However, the proper programming model for multi-tier parallel computers remains an unresolved issue. At present, the programmer faces myriad options regarding the coordination of heavyweight processes, lightweight threads, shared memory, message-passing, synchronization, scheduling, and load balancing <ref> [4] </ref>. This daunting array of low-level programming detail hinders efficient implementations for multi-tier platforms. ? Stephen Fink was supported by the DOE Computational Science Graduate Fellowship Program, Scott Baden by NSF contract ASC-9520372.
Reference: 5. <author> S. J. Fink, S. B. Baden, and S. R. Kohn, </author> <title> "Flexible communication mechanisms for dynamic structured applications," </title> <booktitle> in Proc. 3rd Int'l Workshop IRREGULAR '96, </booktitle> <pages> pp. 203-215, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: To appear at ISCOPE '97 (Marina Del Rey, CA), December 1997 We present a small set of programming abstractions to simplify implementation of efficient algorithms for block-structured scientific calculations on SMP clusters. This paper extends previous work <ref> [5] </ref> with two contributions specifically targeted for multi-tier architectures: hierarchical SPMD control flow, and overlap of communication and computation. We show how high level abstractions hide tedious low-level implementation details, but allow the programmer to express efficient algorithms with intuitive geometric primitives. <p> Under structural abstraction, first-class meta-data objects represent the geometric structure of a calculation. Previous work describes KeLP abstractions to manage irregular block data decompositions and communication on single-tier multicomputers <ref> [5] </ref>. To discuss multi-tier KeLP programming, we first introduce the following meta-data abstractions: Region, Map, FloorPlan, and MotionPlan. KeLP 2.0 implements each of these abstractions as a first-class C++ object. The Region represents a rectangular subset of Z n ; i.e., a regular section with stride one. <p> The Mover, a first-class executor object, performs the data motion represented by a MotionPlan as an atomic collective operation. For a more complete discussion, see <ref> [5] </ref>. To achieve good performance on SMP clusters, it is vital to tolerate slow inter-node communication delays. To facilitate overlap of communication and computation, multi-tier KeLP Movers provide asynchronous execution. For a Mover M, M.start () begins asynchronous execution of a data motion pattern.
Reference: 6. <author> S. R. Kohn, </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations. </title> <type> PhD thesis, </type> <institution> University of CA at San Diego, </institution> <year> 1995. </year>
Reference-contexts: Furthermore, the results illustrate the benefits of overlapping communication and computation to tolerate inter-node message-passing costs. 2 Programming Abstractions 2.1 Structural Abstraction The KeLP programming abstractions extend structural abstraction, a programming model introduced in the LPARX programming system <ref> [6] </ref>. Under structural abstraction, first-class meta-data objects represent the geometric structure of a calculation. Previous work describes KeLP abstractions to manage irregular block data decompositions and communication on single-tier multicomputers [5]. To discuss multi-tier KeLP programming, we first introduce the following meta-data abstractions: Region, Map, FloorPlan, and MotionPlan. <p> Performance results on three codes indicate that the multi-tier approach outperforms pure message-passing codes. Moreover, the results show that overlap of communication and computation effectively improves performance. The KeLP programming abstractions extend the structual abstraction model introduced in the LPARX programming system <ref> [6] </ref>. KeLP's communication model combines structural abstraction with inspector/executor communication analysis as introduced in Multiblock PARTI [7]. In the Phase Abstractions programming model, Snyder [13] advocated separation of programs into levels corresponding to collective (Y) and node (X) levels.
Reference: 7. <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> "An integrated runtime and compile-time approach for parallelizing structured and block structured applications," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 6, </volume> <month> Jul. </month> <year> 1995. </year>
Reference-contexts: The FloorPlan consists of a Map along with an Array of Regions. The FloorPlan can represent a potentially irregular block data decomposition. Alternatively, a FloorPlan can represent distribution of work among processors of a single SMP. The MotionPlan implements a first-class, user-level block communication schedule <ref> [7] </ref>. The programmer builds and manipulates MotionPlans using geometric Region calculus operations. 2.2 Hierarchical Control Flow The multi-tier KeLP abstractions support three levels of control: a collective level, a node level, and a processor level. <p> An XArray is an array of Grids, whose structure is represented by a Floor-Plan. For an XArray X, X (i) denotes the ith Grid in X. 2.4 Data Motion KeLP performs inspector/executor analysis <ref> [7] </ref> with the MotionPlan and Mover classes. The Mover, a first-class executor object, performs the data motion represented by a MotionPlan as an atomic collective operation. For a more complete discussion, see [5]. To achieve good performance on SMP clusters, it is vital to tolerate slow inter-node communication delays. <p> Moreover, the results show that overlap of communication and computation effectively improves performance. The KeLP programming abstractions extend the structual abstraction model introduced in the LPARX programming system [6]. KeLP's communication model combines structural abstraction with inspector/executor communication analysis as introduced in Multiblock PARTI <ref> [7] </ref>. In the Phase Abstractions programming model, Snyder [13] advocated separation of programs into levels corresponding to collective (Y) and node (X) levels. Control flow in multi-tier KeLP programming model extends Snyder's XYZ program levels to multi-tier machines.
Reference: 8. <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum, </author> <title> "A high-performance, portable implementation of the MPI message passing interface standard," </title> <type> tech. rep., </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1997. </year> <note> http://www.mcs.anl.gov/mpi/mpich/. </note>
Reference-contexts: Each SMP has four Alpha 21064A processors, and each processor has a 4MB direct-mapped L2 cache. For inter-node communication, we rely on MPICH 1.0.12 <ref> [8] </ref> over an OC-3 ATM switch. Using a simple ring test we observe a message start time of 745 s and a peak bandwidth of 12 MB/sec. Unfortunately, we encountered severe problems with Digital UNIX 4.0 scheduling of lightweight threads.
Reference: 9. <author> R. van de Geign and J. Watts, "SUMMA: </author> <title> Scalable universal matrix multiplication algorithm," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 9, </volume> <pages> pp. 255-74, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: The first application, redblack3D, performs a 7-point relaxation to solve Poisson's equation over a unit cube as described in Section 3. The MPI version of this code uses BLOCK data decomposition on all three axes. The second application, SUMMA, implements dense matrix multiplication using the SUMMA algorithm <ref> [9] </ref>. The MPI code for SUMMA is listed in [9] and was made publicly available by the authors. The serial matrix multiply kernel calls vendor-provided BLAS. The multi-tier KeLP code for SUMMA uses a two-level decomposition of the algorithm. The outer level uses the SUMMA algorithm between SMP nodes. <p> The MPI version of this code uses BLOCK data decomposition on all three axes. The second application, SUMMA, implements dense matrix multiplication using the SUMMA algorithm <ref> [9] </ref>. The MPI code for SUMMA is listed in [9] and was made publicly available by the authors. The serial matrix multiply kernel calls vendor-provided BLAS. The multi-tier KeLP code for SUMMA uses a two-level decomposition of the algorithm. The outer level uses the SUMMA algorithm between SMP nodes.
Reference: 10. <author> S. J. Fink, </author> <title> "Hierarchical programming for block-structured scientific calculations." </title> <note> In preparation. </note>
Reference-contexts: The outer level uses the SUMMA algorithm between SMP nodes. Within each SMP node, we parallelize the matrix multiplication with a simple domain decomposition. To overlap communication and computation, we developed a multi-tier pipelined version of the SUMMA algorithm, which will be described in detail elsewhere <ref> [10] </ref>. The third application is the NAS-FT benchmark, which solves a 3D diffusion equation using Fast Fourier Transform. We obtained MPI code for FT from the NAS Parallel Benchmarks v2.1 [11]. The multi-tier KeLP versions add a second level of parallelism to node-level kernels with domain decomposition.
Reference: 11. <author> D. Bailey, T. Harris, W. Saphir, R. van der Wijngaart, A. Woo, and M. Yarrow, </author> <title> "The NAS parallel benchmarks 2.0," </title> <type> Tech. Rep. </type> <institution> NAS-95-020, NASA Ames Research Center, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The third application is the NAS-FT benchmark, which solves a 3D diffusion equation using Fast Fourier Transform. We obtained MPI code for FT from the NAS Parallel Benchmarks v2.1 <ref> [11] </ref>. The multi-tier KeLP versions add a second level of parallelism to node-level kernels with domain decomposition. To overlap communication and computation, we pipeline the FFTs across iterations with the algorithm described in [12]. Fig. 3 reports performance of these codes, scaling the problem size with the number of nodes.
Reference: 12. <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> "An efficient parallel algorithm for the 3-d FFT NAS parallel benchmark," </title> <booktitle> in Proc. of SHPCC `94, </booktitle> <pages> pp. 129-133, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: We obtained MPI code for FT from the NAS Parallel Benchmarks v2.1 [11]. The multi-tier KeLP versions add a second level of parallelism to node-level kernels with domain decomposition. To overlap communication and computation, we pipeline the FFTs across iterations with the algorithm described in <ref> [12] </ref>. Fig. 3 reports performance of these codes, scaling the problem size with the number of nodes. The results show that on eight SMP nodes, the multi-tier KeLP code without overlap outperforms the MPI code by a factor of 2 for redblack3D and a factor of 4.6 for NAS-FT.
Reference: 13. <author> L. Snyder, </author> <title> "Foundations of practical parallel programming languages," in Portability and Performance of Parallel Processing (T. </title> <editor> Hey and J. Ferrante, eds.), </editor> <publisher> John Wiley and Sons, </publisher> <year> 1993. </year>
Reference-contexts: The KeLP programming abstractions extend the structual abstraction model introduced in the LPARX programming system [6]. KeLP's communication model combines structural abstraction with inspector/executor communication analysis as introduced in Multiblock PARTI [7]. In the Phase Abstractions programming model, Snyder <ref> [13] </ref> advocated separation of programs into levels corresponding to collective (Y) and node (X) levels. Control flow in multi-tier KeLP programming model extends Snyder's XYZ program levels to multi-tier machines. Alpern, Carter, and Ferrante's PMH model [14] provides an elegant framework for multi-tier parallel architectures.
Reference: 14. <author> B. Alpern, L. Carter, and J. Ferrante, </author> <title> "Modeling parallel computers as memory hierarchies," in Programming Models for Massively Parallel Computers (W. </title> <editor> K. Giloi, S. Jahnichen, and B. D. Shriver, </editor> <booktitle> eds.), </booktitle> <pages> pp. 116-23, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: In the Phase Abstractions programming model, Snyder [13] advocated separation of programs into levels corresponding to collective (Y) and node (X) levels. Control flow in multi-tier KeLP programming model extends Snyder's XYZ program levels to multi-tier machines. Alpern, Carter, and Ferrante's PMH model <ref> [14] </ref> provides an elegant framework for multi-tier parallel architectures. The Cedar Fortran language [15] was perhaps the first language to incorporate two levels of parallelism in order to match a hierarchical parallel architecture. Some more recent systems explicitly target SMP clusters.
Reference: 15. <author> R. Eigenmann, J. Hoeflinger, G. Jaxson, and D. Padua, </author> <title> "Cedar Fortran and its compiler," </title> <booktitle> in CONPAR 90-VAPP IV. Joint International Conference on Vector and Parallel Parocessing, </booktitle> <pages> pp. 288 - 99, </pages> <year> 1990. </year>
Reference-contexts: Control flow in multi-tier KeLP programming model extends Snyder's XYZ program levels to multi-tier machines. Alpern, Carter, and Ferrante's PMH model [14] provides an elegant framework for multi-tier parallel architectures. The Cedar Fortran language <ref> [15] </ref> was perhaps the first language to incorporate two levels of parallelism in order to match a hierarchical parallel architecture. Some more recent systems explicitly target SMP clusters. Sawdey and O'Keefe [16] have applied the Fortran-P programming model to grid-based applications on SMP clusters.
Reference: 16. <author> A. C. Sawdey, M. T. O'Keefe, and W. B. Jones, </author> <title> "A general programming model for developing scalable ocean circulation applications," </title> <booktitle> in Proceedings of the ECMWF Workshop on the Use of Parallel Processors in Meteorology, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: The Cedar Fortran language [15] was perhaps the first language to incorporate two levels of parallelism in order to match a hierarchical parallel architecture. Some more recent systems explicitly target SMP clusters. Sawdey and O'Keefe <ref> [16] </ref> have applied the Fortran-P programming model to grid-based applications on SMP clusters. In Fortran-P, a compiler translates serial grid-based code to explicitly threaded parallel code.
Reference: 17. <author> D. A. Bader and J. JaJa, </author> <title> "SIMPLE: A methodology for programming high performance algorithms on clusters of symmetric multiprocessors." Preliminary Version, http://www.umiacs.umd.edu/research/EXPAR/papers/3798.html. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In Fortran-P, a compiler translates serial grid-based code to explicitly threaded parallel code. KeLP 2.0 presents a more complex, explicitly parallel model, but allows the programmer to express a wider class of algorithms and exert more control over the implementation. Bader and JaJa have developed SIMPLE <ref> [17] </ref>, a set of collective communication operations for SMP clusters. SIMPLE provides more general, lower-level primitives than KeLP 2.0, and does not help with data decomposition or overlap of communication and computation. The KeLP 2.0 implementation relies on collective operations similar to those provided by SIMPLE.
References-found: 17

