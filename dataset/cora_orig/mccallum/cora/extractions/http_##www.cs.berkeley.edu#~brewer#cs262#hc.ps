URL: http://www.cs.berkeley.edu/~brewer/cs262/hc.ps
Refering-URL: http://www.cs.berkeley.edu/~brewer/cs262.html
Root-URL: 
Title: Application-Controlled Physical Memory using External Page-Cache Management  
Author: Kieran Harty and David R. Cheriton 
Address: CA 94305  
Affiliation: Computer Science Department Stanford University,  
Abstract: Next generation computer systems will have gigabytes of physical memory and processors in the 200 MIPS range or higher. While this trend suggests that memory management for most programs will be less of a concern, memory-bound applications such as scientific simulations and database management systems will require more sophisticated memory management support, especially in a multiprogramming environment. Furthermore, new architectures are introducing new complexities between the processor and memory, requiring techniques such as page coloring, variable page sizes and physical placement control. We describe the design, implementation and evaluation of a virtual memory system that provides application control of physical memory using external page-cache management. In this approach, a sophisticated application is able to monitor and control the amount of physical memory it has available for execution, the exact contents of this memory, and the scheduling and nature of page-in and page-out using the abstraction of a page frame cache provided by the kernel. It is also able to handle multiple page sizes and control the specific physical pages it uses. We claim that this approach can significantly improve performance for many memory-bound applications while reducing kernel complexity, yet does not complicate other applications or reduce their performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska and Henry M. </author> <title> Levy Scheduler Activations; Effective Kernel Support for the User-Level Management of Parallelism ACM Transactions on Computer Systems, </title> <type> 10(1), </type> <month> February </month> <year> 1992. </year>
Reference-contexts: For example, Tucker and Gupta [20] show significant improvements in simultaneous parallel application execution if the applications are informed of changes in the numbers of available processors and thereby allowed to adapt, as compared to the conventional transparent, oblivious approach. Anderson et al. <ref> [1] </ref> and Black [4] have proposed kernel mechanisms for exporting more control of processor management to applications. Just as in our work, this processor-focused work is targeted to the demanding applications whose requirements exceed what are, by normal standards, plentiful hardware resources.
Reference: [2] <institution> Andrew Appel and Kai Li Virtual Memory Primitives for User Programs In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </institution> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Low overhead page fault handling allows efficient implementation of user level algorithms that use page protection hardware, like those described in <ref> [2] </ref>. Examples of these algorithms include mechanisms for concurrent garbage collection and concurrent checkpointing. In ULTRIX 4.1 on a DECstation 5000/200, the cost of a user level fault handler 6 for a protected page that simply changes the protection of the page is 152 microseconds. <p> This is over 50% higher than the cost of handling a full fault using external page-cache management. ULTRIX is competitive at user level fault handling with other systems like Mach or SunOS. For example, in Appel and Li's measurements for the DECstation 3100 <ref> [2] </ref> the overhead of Mach fault handling operations was over twice the overhead of ULTRIX for similar operations. The final measurements in the table are the costs of reading and writing a 4KB block in a cached file.
Reference: [3] <editor> A. Bensoussan, C. T. Clingen and R. C. </editor> <booktitle> Daley The Multics Virtual Memory In Proceedings of the 2nd ACM Symposium on Operating Systems Principles, </booktitle> <address> Princeton, New Jersey, </address> <month> Oc-tober </month> <year> 1969. </year>
Reference-contexts: A segment is a variable-size address range of zero or more pages, similar to the conventional virtual memory notion of segment <ref> [3] </ref>. Pages can be added, removed, mapped, unmapped, read and written using segment operations. A parameter to the segment creation call optionally specifies the page size to support machines such as those using the Alpha microprocessor [10] that support multiple page sizes.
Reference: [4] <institution> David Black Scheduler Support for Concurrency and Parallelism in the Mach Operating System IEEE Computer Magazine, </institution> <month> 23(5) </month> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, Tucker and Gupta [20] show significant improvements in simultaneous parallel application execution if the applications are informed of changes in the numbers of available processors and thereby allowed to adapt, as compared to the conventional transparent, oblivious approach. Anderson et al. [1] and Black <ref> [4] </ref> have proposed kernel mechanisms for exporting more control of processor management to applications. Just as in our work, this processor-focused work is targeted to the demanding applications whose requirements exceed what are, by normal standards, plentiful hardware resources.
Reference: [5] <editor> David R. </editor> <booktitle> Cheriton The V Distributed System Communications of the ACM, </booktitle> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Thus there is ample time to overlap prefetching and writeback if the data does not fit entirely in memory. Extensions to virtual memory systems, such as page pinning, external pagers <ref> [21, 5] </ref> and application-program advisory system calls like the Unix 2 madvise attempt to address some of these issues, but incompletely and with significant increase in kernel complexity. <p> The MigratePages operation is also used to reclaim pages frames from segments as part of a page reclamation strategy. Cached files, implemented as segments, can be accessed using a kernel-provided file-like block read/write interface, specifically the Uniform Input/Output Object (UIO) protocol <ref> [5] </ref>. A file read to a segment page that does not have an associated page frame causes a page fault event to be com-municated to the manager of the segment, as for a regular page fault. File write operations requiring page allocation are handled similarly. <p> In V++, the default segment manager is currently created as part of the "first team", a memory-resident set of systems servers started immediately after kernel initialization. Thus, the default manager does not itself page-fault. In the V++ implementation, the UIO Cache Directory Server (UCDS) <ref> [5] </ref> has been extended to act as default segment manager. This server manages the V++ virtual memory system effectively as a file page cache. All address spaces are realized as bindings to open files, as in SunOS 5 . <p> We expect that, with the appropriate generic segment manager software, developing an application-specific segment manager should be no harder than developing a "pin" manager module. However, further experience is required in this area before firmer conclusions can be drawn. The external pagers in Mach [21] and V <ref> [5] </ref> provide the ability to implement application-specific read-ahead and writeback using backing servers or external pages. However, these extensions do not address application control of the page cache and are primarily focused on the handling of backing storage.
Reference: [6] <author> David R. Cheriton, Gregory R. Whitehead and Ed-ward W. </author> <title> Sznyter Binary Emulation of Unix using the V Kernel Usenix Summer Conference, </title> <month> June, </month> <year> 1990. </year>
Reference-contexts: With our primary focus on batch processing, results to date have been promising. The external page cache management approach develops further a principle of operating system design we call efficient completeness, described previously in the context of supporting emulation <ref> [6] </ref>. The operating system kernel, in providing an abstraction of hardware resources, should provide efficient and complete access to the functionality and performance of the hardware. In the context of memory management, the complete and efficient abstraction of this hardware resource is that of a page-cache.
Reference: [7] <author> David R. Cheriton, Hendrik A. </author> <title> Goosen and Philip Ma-chanick Restructuring a Parallel Simulation to Improve Shared Memory Multiprocessor Cache Behavior: A First Experience Shared Memory Multiprocessor Symposium, </title> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Addressing these problems has significant performance benefits for applications, as argued below. With knowledge of the amount of available physical memory, an application may be able to make an intelligent space-time tradeoff between different algorithms or modes of execution that achieve its desired computation. For example, MP3D <ref> [7] </ref>, a large scale parallel particle simulation based on the Monte-Carlo method, generates a final result based on the averaging of a number of simulation runs. The simulation can be run for a shorter amount of time if it uses many runs with a large number of particles.
Reference: [8] <author> David R. Cheriton, Hendrik A. Goosen and Patrick D. </author> <title> Boyle ParaDiGM: A Highly Scalable Shared-Memory Multi-Computer Architecture IEEE Computer 24 (2), </title> <month> February, </month> <year> 1991. </year>
Reference-contexts: In that vein, we expect that other considerations, such as page coloring, physical placement control and and cache line software control, as in ParaDiGM <ref> [8] </ref>, to place further demands on memory management software in the future. Finally, we have exploited the new external page cache management kernel operations to further reduce the size of the V++ kernel by implementing system page cache management and a default segment manager outside the kernel.
Reference: [9] <author> David R. </author> <note> Cheriton </note>
Reference-contexts: Further extensions can easily be provided for future architectures by modifying the SPCM, rather than complicating and destabilizing the kernel. A "memory market" model of system memory allocation has been developed for the SPCM, and is explored in depth in a separate report <ref> [9] </ref>. In brief, the SPCM imposes a charge on a process for the memory that it uses over a given period of time in an artificial monetary unit we call a dram.
References-found: 9

