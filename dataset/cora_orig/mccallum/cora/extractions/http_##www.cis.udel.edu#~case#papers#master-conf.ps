URL: http://www.cis.udel.edu/~case/papers/master-conf.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Title: Learning To Win Process-Control Games Watching Game-Masters  
Author: John Case Matthias Ott Arun Sharma Frank Stephan 
Address: Heidelberg  
Affiliation: University of Delaware  Universitat Karlsruhe  University of New South Wales  Universitat  
Abstract: The present paper focuses on some interesting classes of process-control games, where winning essentially means successfully controlling the process. A master for one of these games is an agent who plays a winning-strategy. Learners try to find programs for winning strategies for such games from a program for (or model of) the process to be controlled and by watching masters play winning strategies. Studied are successful learning from arbitrary masters and from hopefully pedagogically useful selected masters. It is shown that selected masters are strictly more helpful for learning than are arbitrary masters. Both for learning from arbitrary masters and for learning from selected masters, though, there are cases where one can learn programs for winning strategies from masters but not if one is required to learn a program for the master's strategy itself! Both for learning from arbitrary masters and for learning from selected masters, surprisingly, one can learn strictly more watching m + 1 masters than one can learn watching only m. Lastly a simulation result is presented where the presence of a selected master reduces the complexity from infinitely many semantic mind changes to finitely many syntactic ones.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Arnold and K. </author> <title> Jantke. Therapy plan generation as program synthesis. </title> <booktitle> In Proceeedings of the Fifth International Workshop on Algorithmic Learning Theory, </booktitle> <pages> pages 40-55. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The results about learning from masters explicitly set off and labeled as theorems in the present paper all follow this model. This classical (complete model) approach is realistic in some situations, but not in situations such as learning to maintain the safety and productivity of a chemical plant <ref> [1] </ref>, robotics and manufacturing [7, 11, 30], or teaching a machine to operate efficiently a free-swinging shipyard crane [2, 29]. In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . <p> In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, <ref> [1, 7, 11, 29, 30] </ref>). A corresponding theoretical approach has been introduced by Kummer and Ott [12] (see also [20]), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree). <p> Thus, this notion in [5] corresponds to ArbMaId, and for trees which contain exactly one infinite computable branch, it coincides with SelectMaId-learning by teams. We say a class C Tree is in <ref> [1; m] </ref>SelectMa, if there are m Turing machines such that for each T 2 C one of the m Turing machines learns a branch for T from one selected master. <p> In our terminology this means: Theorem 3.3 [5] For all m 1: <ref> [1; m + 1] </ref>SelectMa 6 [1; m]SelectMa. Theorem 3.4 For all m 1: SelectMa m SelectMa m+1 . 17 The term team in the context of learning was introduced in [27]. <p> In our terminology this means: Theorem 3.3 [5] For all m 1: [1; m + 1]SelectMa 6 <ref> [1; m] </ref>SelectMa. Theorem 3.4 For all m 1: SelectMa m SelectMa m+1 . 17 The term team in the context of learning was introduced in [27]. <p> Proof. The inclusion SelectMa m SelectMa m+1 is immediate from the definition. Let an arbitrary m 1 be given and let C 2 m be a class in ([1; 2 m + 1]SelectMa <ref> [1; 2 m ] </ref>SelectMa), such that each T 2 C 2 m contains exactly one infinite computable branch. C 2 m exists by Theorem 3.3. For each T 2 C 2 m we define new trees U (T ) = 0 T [ 1 T . <p> Then C 2 m is in <ref> [1; 2 m ] </ref>SelectMa via the team of machines M 1 ; : : : ; M 2 m where M i works as follows for i = 1; : : : ; 2 m : ' M i (e;f [n]) = x:' h i (e;f [n]) (x + 1); To <p> As soon as M converges to a (program for an) infinite branch u of U (T ), M i will output a program for x:u (x + 1) which is an infinite branch of T . Since C 2 m is not in <ref> [1; 2 m ] </ref>SelectMa by Theorem 3.3 we get a contradiction. <p> Since C 2 m is not in [1; 2 m ]SelectMa by Theorem 3.3 we get a contradiction. But C is in SelectMa m+1 : Let C be in <ref> [1; 2 m + 1] </ref>SelectMa as witnessed by M 1 ; : : : ; M 2 m +1 and let an arbitrary tree U (T ) 2 C be given. <p> Then the sequence (0 h n ) n2! will converge to an infinite branch of U (T ) since (h n ) n2! converges to a (program for an) infinite branch of T . 2 Corollary 3.5 (8m 1)[Tree 62 SelectMa m ]. Besides <ref> [1; m] </ref>-teams, as considered in Theorem 3.3, also the more general [l; m]SelectMa-teams are interesting, where it is required that l out of the m machines learn branches from the one selected master. 18 By using techniques from the proof of Theorem 2.4 one can prove that exactly those inclusions hold <p> If one considers [l; m]-teams of SelectMaId-learner then one gets the same team hierarchy as for Ex-style function identification [27]: every [l; m]-team is equivalent to exactly one <ref> [1; k] </ref>-team, namely to that with 1 m 1 k .
Reference: [2] <author> M. Bain and C. Sammut. </author> <title> A framework for behavioural cloning. </title> <editor> In K. F. S. Muggleton and D. Michie, editors, </editor> <booktitle> Machine Intelligence 15. </booktitle> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: For example, it is likely better to study the behavior of both Kasparov and Deep Blue 2 than to study only one of them. In machine learning, the behavioral cloning approach to process-control, surveyed in <ref> [2] </ref>, involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator [2, 4, 16, 25, 26] and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> In machine learning, the behavioral cloning approach to process-control, surveyed in [2], involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator <ref> [2, 4, 16, 25, 26] </ref> and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> For example, it has been used successfully to teach an autopilot to fly an aircraft simulator [2, 4, 16, 25, 26] and to teach a machine to operate efficiently a free-swinging shipyard crane <ref> [2, 29] </ref>. Behavioral cloning partly motivates the present paper. fl Department of CIS, University of Delaware, Newark, DE 19716, USA, Email: case@cis.udel.edu y Institut fur Logik, Komplexitat und Deduktionssysteme, Universitat Karlsruhe, 76128 Karlsruhe, Germany, Email: m ott@ira.uka.de. <p> Nonetheless, some of the parallels we describe, in the rest of this subsection, between these experimental machine learning results and our main theorems are very interesting and, we hope, instructive for the future. For pedagogical purposes, some masters may be better to watch than others. In <ref> [2, 4, 16, 25, 26] </ref> it is noted that better results were obtained using the data from some pilots rather than others. <p> Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In <ref> [2, 4, 16, 25, 26] </ref> the learning program employed, C4.5 [23], did not merely learn to copy identically each pilot modeled. <p> ! 5 An interestingly contrasting theorem in the same section (Theorem 2.2) implies that, if a class of process-control games can be learned incrementally, i.e., after finitely many trial and error rounds, from arbitrary masters, then it can be incrementally learned by copycatting selected masters. 6 In the learning-to-fly project <ref> [2, 4, 16, 25, 26] </ref> it was discovered that C4.5 got confused if it received data from more than one pilot at a time. <p> For the theorems, all masters considered (by definition) play perfect winning strategies. In the learning-to-fly project, data from medium-skilled pilots provided better learning <ref> [2, 4, 16, 25, 26] </ref>. 4 Formally, this distinction is handled definitionally by universal versus existential quantifiers over masters in positive assertions (see Definition 2.1 in Section 2 below). 5 The learning from masters in the present paper is off-line learning, learning programs for winning strategies. <p> This classical (complete model) approach is realistic in some situations, but not in situations such as learning to maintain the safety and productivity of a chemical plant [1], robotics and manufacturing [7, 11, 30], or teaching a machine to operate efficiently a free-swinging shipyard crane <ref> [2, 29] </ref>. In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control.
Reference: [3] <author> J. R. Buchi and L. H. </author> <title> Landweber. Solving sequential conditions by finite-state strategies. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 138 </volume> <pages> 295-311, </pages> <year> 1969. </year>
Reference-contexts: From such a model one tries to compute or synthesize (perhaps incrementally) the corresponding controller. The synthesis can be modeled in terms of inputting a program for a corresponding game tree and eventually outputting (a program for) a winning strategy <ref> [3, 15, 28] </ref>. The results about learning from masters explicitly set off and labeled as theorems in the present paper all follow this model. <p> Is there a connection between the quality of the input masters and the number of plays which an on-line learner loses? The one-player immortality games given by a deterministic finite automaton, as described in Section 1.2, are just a special case of the well known two-player finite-state games <ref> [3, 15, 28] </ref>. In such games there always exist winning strategies which can be executed by a finite automaton.
Reference: [4] <author> C. Sammut, S. Hurst, K. Kedzier and D. Michie. </author> <title> Learning to fly. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In machine learning, the behavioral cloning approach to process-control, surveyed in [2], involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator <ref> [2, 4, 16, 25, 26] </ref> and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> Nonetheless, some of the parallels we describe, in the rest of this subsection, between these experimental machine learning results and our main theorems are very interesting and, we hope, instructive for the future. For pedagogical purposes, some masters may be better to watch than others. In <ref> [2, 4, 16, 25, 26] </ref> it is noted that better results were obtained using the data from some pilots rather than others. <p> Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In <ref> [2, 4, 16, 25, 26] </ref> the learning program employed, C4.5 [23], did not merely learn to copy identically each pilot modeled. <p> ! 5 An interestingly contrasting theorem in the same section (Theorem 2.2) implies that, if a class of process-control games can be learned incrementally, i.e., after finitely many trial and error rounds, from arbitrary masters, then it can be incrementally learned by copycatting selected masters. 6 In the learning-to-fly project <ref> [2, 4, 16, 25, 26] </ref> it was discovered that C4.5 got confused if it received data from more than one pilot at a time. <p> For the theorems, all masters considered (by definition) play perfect winning strategies. In the learning-to-fly project, data from medium-skilled pilots provided better learning <ref> [2, 4, 16, 25, 26] </ref>. 4 Formally, this distinction is handled definitionally by universal versus existential quantifiers over masters in positive assertions (see Definition 2.1 in Section 2 below). 5 The learning from masters in the present paper is off-line learning, learning programs for winning strategies.
Reference: [5] <author> J. Case, S. Kaufmann, E. Kinber, and M. Kummer. </author> <title> Learning recursive functions from approximations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 25 </volume> <pages> 59-78, </pages> <year> 1997. </year>
Reference-contexts: For each m 1, actually, as in the case of arbitrary masters, one can show that watching m + 1 selected masters is also strictly more powerful than watching m. In order to prove this we use a team learning result from <ref> [5] </ref>. Basically, a team of m machines is successful, if at least one of them is (and we may not know which one). 17 In [5] the identification of computable functions is investigated when a tree is given as additional input to the learner, which contains the function as an infinite <p> In order to prove this we use a team learning result from <ref> [5] </ref>. Basically, a team of m machines is successful, if at least one of them is (and we may not know which one). 17 In [5] the identification of computable functions is investigated when a tree is given as additional input to the learner, which contains the function as an infinite computable branch. [5] studied the team size which is necessary to identify all infinite computable branches of some naturally defined tree classes. <p> team of m machines is successful, if at least one of them is (and we may not know which one). 17 In <ref> [5] </ref> the identification of computable functions is investigated when a tree is given as additional input to the learner, which contains the function as an infinite computable branch. [5] studied the team size which is necessary to identify all infinite computable branches of some naturally defined tree classes. Thus, this notion in [5] corresponds to ArbMaId, and for trees which contain exactly one infinite computable branch, it coincides with SelectMaId-learning by teams. <p> identification of computable functions is investigated when a tree is given as additional input to the learner, which contains the function as an infinite computable branch. <ref> [5] </ref> studied the team size which is necessary to identify all infinite computable branches of some naturally defined tree classes. Thus, this notion in [5] corresponds to ArbMaId, and for trees which contain exactly one infinite computable branch, it coincides with SelectMaId-learning by teams. <p> Note that each machine of the team gets the same selected master of T as input. <ref> [5] </ref> separated teams of size m + 1 from teams of size m, by a class of trees which each contain exactly one infinite computable branch. In our terminology this means: Theorem 3.3 [5] For all m 1: [1; m + 1]SelectMa 6 [1; m]SelectMa. <p> Note that each machine of the team gets the same selected master of T as input. <ref> [5] </ref> separated teams of size m + 1 from teams of size m, by a class of trees which each contain exactly one infinite computable branch. In our terminology this means: Theorem 3.3 [5] For all m 1: [1; m + 1]SelectMa 6 [1; m]SelectMa. Theorem 3.4 For all m 1: SelectMa m SelectMa m+1 . 17 The term team in the context of learning was introduced in [27]. <p> Besides TreeInf there are many other natural classes of trees. It would be interesting to fit these classes into the SelectMa m and ArbMa m hierarchies. For example, trees of bounded variation, trees of bounded width and trees of bounded rank are natural candidates <ref> [5] </ref>. 21 SynthLim is equivalent to the learning criterion called Uni [K] in [12, 20]. 9
Reference: [6] <author> D. Cenzer and J. Remmel. </author> <title> Recursively presented games and strategies. </title> <journal> Mathematical Social Sciences, </journal> <volume> 24 </volume> <pages> 117-139, </pages> <year> 1992. </year>
Reference-contexts: If we model finite environments as deterministic finite automata [24], then, in these cases, the one-player immortality game can be modeled as follows: Given a finite automaton, a winning strat 8 For more formal treatment, see <ref> [6, 12, 14, 28] </ref>. 9 This definition of a master's behavior is somewhat of a mathematical idealization. In real games one might never get to watch how a master would react to situations which, for whatever reason, don't happen to occur.
Reference: [7] <author> J. H. Connell and S. Mahadevan, </author> <title> editors. Robot Learning, </title> <address> Boston, 1993. </address> <publisher> Kluwer. </publisher>
Reference-contexts: This classical (complete model) approach is realistic in some situations, but not in situations such as learning to maintain the safety and productivity of a chemical plant [1], robotics and manufacturing <ref> [7, 11, 30] </ref>, or teaching a machine to operate efficiently a free-swinging shipyard crane [2, 29]. In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . <p> In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, <ref> [1, 7, 11, 29, 30] </ref>). A corresponding theoretical approach has been introduced by Kummer and Ott [12] (see also [20]), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree).
Reference: [8] <author> O. Follinger. Regelungstechnik. Huthig, </author> <note> Heidelberg, 8th edition, </note> <year> 1994. </year>
Reference-contexts: to synthesize (a program for) the master's strategy: sometimes one may learn more from studying another's road to success by finding ones own road to success. 1.3 Summary of Some Interesting Further Results Classical approaches to learning process-control strategies assume one has a complete program or model for the process <ref> [8] </ref> (equivalently, all the rules of the game). From such a model one tries to compute or synthesize (perhaps incrementally) the corresponding controller.
Reference: [9] <author> R. V. Freivalds and R. Wiehagen. </author> <title> Inductive inference with additional information. </title> <journal> Elektronische Informationsverarbeitung und Kybernetik, </journal> <volume> 15 </volume> <pages> 179-185, </pages> <year> 1979. </year>
Reference-contexts: Proof Sketch. This simulation proof is related to the proof of Freivalds and Wiehagen that every computable function can be learned from an upper bound of any of its indices 16 <ref> [9, 10] </ref>. Let M be an ArbMa-learner. Furthermore, for any given tree select that infinite branch which has among all computable infinite branches the smallest minimal index, say e.
Reference: [10] <author> S. Jain and A. Sharma. </author> <title> Learning with the knowledge of an upper bound on program size. </title> <journal> Information and Computation, </journal> <volume> 102(1) </volume> <pages> 118-166, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Proof Sketch. This simulation proof is related to the proof of Freivalds and Wiehagen that every computable function can be learned from an upper bound of any of its indices 16 <ref> [9, 10] </ref>. Let M be an ArbMa-learner. Furthermore, for any given tree select that infinite branch which has among all computable infinite branches the smallest minimal index, say e.
Reference: [11] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Jornal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: This classical (complete model) approach is realistic in some situations, but not in situations such as learning to maintain the safety and productivity of a chemical plant [1], robotics and manufacturing <ref> [7, 11, 30] </ref>, or teaching a machine to operate efficiently a free-swinging shipyard crane [2, 29]. In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . <p> In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, <ref> [1, 7, 11, 29, 30] </ref>). A corresponding theoretical approach has been introduced by Kummer and Ott [12] (see also [20]), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree).
Reference: [12] <author> M. Kummer and M. Ott. </author> <title> Learning branches and learning to win closed games. </title> <booktitle> In Proceedings of Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 280-291, </pages> <address> New York, 1996. </address> <publisher> ACM. </publisher>
Reference-contexts: The two kinds turn out to be, for all our purposes, mathematically equivalent <ref> [12] </ref>! The second kind is mathematically elegantly simple, so we state/prove our results in terms of it, but, although, this second kind is interesting in its own right, more of our motivation comes from the first kind. <p> In this context on-line learning from masters is trivial and can always be done by copycatting directly the masterly control actions. <ref> [12] </ref> studies both on-line and off-line learning without masters, a context where each is non-trivial. 6 This copycatting is still off-line. <p> If we model finite environments as deterministic finite automata [24], then, in these cases, the one-player immortality game can be modeled as follows: Given a finite automaton, a winning strat 8 For more formal treatment, see <ref> [6, 12, 14, 28] </ref>. 9 This definition of a master's behavior is somewhat of a mathematical idealization. In real games one might never get to watch how a master would react to situations which, for whatever reason, don't happen to occur. <p> In real games one might never get to watch how a master would react to situations which, for whatever reason, don't happen to occur. Furthermore, winning strategies in general may have to depend on the whole history of the opponent's moves, not just the latest move <ref> [12] </ref>. 10 Of course, the impractical variant in which the controller is only expected to maintain control from some, unspecified time onwards is not closed. <p> In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, [1, 7, 11, 29, 30]). A corresponding theoretical approach has been introduced by Kummer and Ott <ref> [12] </ref> (see also [20]), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree). <p> The situation for Section 4 below is very different: we know that all but exactly one of the results mentioned there fails if totally restated in terms of the enumeration model | so, next we will describe the main result in Section 4. <ref> [12] </ref> considers several master-free learning criteria other than simply incremental. <p> The analogy goes further: we can also show [l; m]SelectMaId is equivalent to probablistic SelectMaId-learning with probability of success l m . 19 4 Master Learning versus Branch Learning How is ordinary branch learning <ref> [12, 20] </ref>, where the learner can only inspect the tree but has no access to a master, related to master learning? The most powerful branch learning notion from [12] is called weak Bc-learning. 20 A class C Tree is in BranchWBc, if there exists a Turing machine 18 Pitt and Smith <p> to probablistic SelectMaId-learning with probability of success l m . 19 4 Master Learning versus Branch Learning How is ordinary branch learning [12, 20], where the learner can only inspect the tree but has no access to a master, related to master learning? The most powerful branch learning notion from <ref> [12] </ref> is called weak Bc-learning. 20 A class C Tree is in BranchWBc, if there exists a Turing machine 18 Pitt and Smith [22] showed that l out of m incremental team function learning is equivalent to probablistic incremental learning (by a single machine) achieving probability of success l m . <p> The corresponding class is denoted by SynthLim. 21 By definition, SynthLim is a subset of ArbMa. One can show that this inclusion is proper. The classes SynthLim and BranchWBc are incomparable <ref> [12] </ref>. Thus, there are classes in BranchWBc for which one cannot synthesize branches in the limit. We know that SynthLim ArbMa SelectMaId SelectMa. <p> For example, one may consider masters which are playing finite variants of winning strategies. Or one may assume, that one of m input masters, or a majority of them, knows the best move in each situation. Moreover, for imperfect masters the problem of on-line learning is no longer trivial <ref> [12] </ref>! It would be interesting to investigate probabilistic learning from imperfect masters. The performance of an on-line learner can be measured by the number of lost plays, until it is eventually playing perfectly. <p> It would be interesting to fit these classes into the SelectMa m and ArbMa m hierarchies. For example, trees of bounded variation, trees of bounded width and trees of bounded rank are natural candidates [5]. 21 SynthLim is equivalent to the learning criterion called Uni [K] in <ref> [12, 20] </ref>. 9
Reference: [13] <author> M. Kummer and F. Stephan. </author> <title> On the structure of degrees of inferability. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(2) </volume> <pages> 214-238, </pages> <month> Apr. </month> <year> 1996. </year>
Reference: [14] <author> O. Maler, A. Pnueli, and J. Sifakis. </author> <title> On the synthesis of discrete controllers for timed systems. </title> <booktitle> In Proceedings of the Annual Symposium on the Theoretical Aspects of Computer Science, volume 900 of LNCS, </booktitle> <pages> pages 229-242. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: If we model finite environments as deterministic finite automata [24], then, in these cases, the one-player immortality game can be modeled as follows: Given a finite automaton, a winning strat 8 For more formal treatment, see <ref> [6, 12, 14, 28] </ref>. 9 This definition of a master's behavior is somewhat of a mathematical idealization. In real games one might never get to watch how a master would react to situations which, for whatever reason, don't happen to occur.
Reference: [15] <author> R. McNaughton. </author> <title> Infinite games played on finite graphs. </title> <journal> Annals of Pure and Applied Logic, </journal> <volume> 65 </volume> <pages> 149-184, </pages> <year> 1993. </year>
Reference-contexts: From such a model one tries to compute or synthesize (perhaps incrementally) the corresponding controller. The synthesis can be modeled in terms of inputting a program for a corresponding game tree and eventually outputting (a program for) a winning strategy <ref> [3, 15, 28] </ref>. The results about learning from masters explicitly set off and labeled as theorems in the present paper all follow this model. <p> Is there a connection between the quality of the input masters and the number of plays which an on-line learner loses? The one-player immortality games given by a deterministic finite automaton, as described in Section 1.2, are just a special case of the well known two-player finite-state games <ref> [3, 15, 28] </ref>. In such games there always exist winning strategies which can be executed by a finite automaton.
Reference: [16] <author> D. Michie and C. Sammut. </author> <title> Machine learning from real-time input-output behaviour. </title> <booktitle> In Proceedings of the International Conference on Design to Manufacture in Modern Industry, </booktitle> <pages> pages 363-369, </pages> <year> 1993. </year>
Reference-contexts: In machine learning, the behavioral cloning approach to process-control, surveyed in [2], involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator <ref> [2, 4, 16, 25, 26] </ref> and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> Nonetheless, some of the parallels we describe, in the rest of this subsection, between these experimental machine learning results and our main theorems are very interesting and, we hope, instructive for the future. For pedagogical purposes, some masters may be better to watch than others. In <ref> [2, 4, 16, 25, 26] </ref> it is noted that better results were obtained using the data from some pilots rather than others. <p> Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In <ref> [2, 4, 16, 25, 26] </ref> the learning program employed, C4.5 [23], did not merely learn to copy identically each pilot modeled. <p> ! 5 An interestingly contrasting theorem in the same section (Theorem 2.2) implies that, if a class of process-control games can be learned incrementally, i.e., after finitely many trial and error rounds, from arbitrary masters, then it can be incrementally learned by copycatting selected masters. 6 In the learning-to-fly project <ref> [2, 4, 16, 25, 26] </ref> it was discovered that C4.5 got confused if it received data from more than one pilot at a time. <p> For the theorems, all masters considered (by definition) play perfect winning strategies. In the learning-to-fly project, data from medium-skilled pilots provided better learning <ref> [2, 4, 16, 25, 26] </ref>. 4 Formally, this distinction is handled definitionally by universal versus existential quantifiers over masters in positive assertions (see Definition 2.1 in Section 2 below). 5 The learning from masters in the present paper is off-line learning, learning programs for winning strategies.
Reference: [17] <author> P. Odifreddi. </author> <title> Classical Recursion Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: A total function f : ! ! ! is an infinite branch of T if f [n] 2 T for all n 2 !. For background from inductive inference see, e.g., [18]. Remaining computability theoretic notation is from <ref> [17] </ref>. We are interested only in the class Tree of all computable trees which contain at least one infinite computable branch. If f 2 REC is an infinite computable branch of T we also say that f is on T .
Reference: [18] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: We often will define trees by specifying only such a set A. A total function f : ! ! ! is an infinite branch of T if f [n] 2 T for all n 2 !. For background from inductive inference see, e.g., <ref> [18] </ref>. Remaining computability theoretic notation is from [17]. We are interested only in the class Tree of all computable trees which contain at least one infinite computable branch. If f 2 REC is an infinite computable branch of T we also say that f is on T .
Reference: [19] <author> M. Ott and F. Stephan. </author> <title> The complexity of learning branches and strategies from queries. </title> <booktitle> In Proceedings of the Eighth Annual International Symposium on Algorithms and Computation, </booktitle> <year> 1997. </year>
Reference-contexts: In such games there always exist winning strategies which can be executed by a finite automaton. In <ref> [19] </ref> it is investigated whether one can efficiently learn strategies for one- and two-player closed finite-state games from membership and play queries, where membership queries involve asking whether a certain position is already a loss, and play queries involve asking whether a certain finite automaton implements a winning strategy.
Reference: [20] <author> M. Ott and F. Stephan. </author> <title> Structural measures for games and process control in the branch learning model. </title> <booktitle> In Proceedings of the Third European Conference on Computational Learning Theory, </booktitle> <pages> pages 94-108. </pages> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, [1, 7, 11, 29, 30]). A corresponding theoretical approach has been introduced by Kummer and Ott [12] (see also <ref> [20] </ref>), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree). <p> Thus, to identify a master is a proper mentioned in Section 4 which still holds if we employ instead the enumeration model for learning from masters. 13 The theory remains the same if it is based on trees over a finite alphabet, e.g., on binary trees T f0; 1g fl <ref> [20] </ref>. 14 Note that the sequence M (e; f [n]) converges syntactically to a (program for an) infinite computable branch, i.e., our notion of learning corresponds to the version of learning in the limit, which is called Ex-style (or incremental) learning in the literature. 5 restriction for both learning from arbitrary <p> The analogy goes further: we can also show [l; m]SelectMaId is equivalent to probablistic SelectMaId-learning with probability of success l m . 19 4 Master Learning versus Branch Learning How is ordinary branch learning <ref> [12, 20] </ref>, where the learner can only inspect the tree but has no access to a master, related to master learning? The most powerful branch learning notion from [12] is called weak Bc-learning. 20 A class C Tree is in BranchWBc, if there exists a Turing machine 18 Pitt and Smith <p> It would be interesting to fit these classes into the SelectMa m and ArbMa m hierarchies. For example, trees of bounded variation, trees of bounded width and trees of bounded rank are natural candidates [5]. 21 SynthLim is equivalent to the learning criterion called Uni [K] in <ref> [12, 20] </ref>. 9
Reference: [21] <author> L. Pitt. </author> <title> Probabilistic inductive inference. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 383-433, </pages> <year> 1989. </year>
Reference-contexts: Pitt in <ref> [21] </ref> showed the surprising result that one out of m incremental team function learning is equivalent to probablistic incremental learning (by a single machine) achieving probability of success 1 m . Proof. The inclusion SelectMa m SelectMa m+1 is immediate from the definition.
Reference: [22] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: 20], where the learner can only inspect the tree but has no access to a master, related to master learning? The most powerful branch learning notion from [12] is called weak Bc-learning. 20 A class C Tree is in BranchWBc, if there exists a Turing machine 18 Pitt and Smith <ref> [22] </ref> showed that l out of m incremental team function learning is equivalent to probablistic incremental learning (by a single machine) achieving probability of success l m . 19 We also know that, for [l; m]SelectMa-learning the connection to probabilistic learning holds only in one direc tion: if a class is
Reference: [23] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In [2, 4, 16, 25, 26] the learning program employed, C4.5 <ref> [23] </ref>, did not merely learn to copy identically each pilot modeled.
Reference: [24] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: The robot's job is to keep exploring its environment yet not get trapped or destroyed. To help it, it has a model of its environment, from which it can generate, for example, a map showing the dangerous spots. If we model finite environments as deterministic finite automata <ref> [24] </ref>, then, in these cases, the one-player immortality game can be modeled as follows: Given a finite automaton, a winning strat 8 For more formal treatment, see [6, 12, 14, 28]. 9 This definition of a master's behavior is somewhat of a mathematical idealization.
Reference: [25] <author> C. Sammut. </author> <title> Acquiring expert knowledge by learning from recorded behaviours. In Japanese Knowledge Acquisition Workshop, </title> <year> 1992. </year>
Reference-contexts: In machine learning, the behavioral cloning approach to process-control, surveyed in [2], involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator <ref> [2, 4, 16, 25, 26] </ref> and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> Nonetheless, some of the parallels we describe, in the rest of this subsection, between these experimental machine learning results and our main theorems are very interesting and, we hope, instructive for the future. For pedagogical purposes, some masters may be better to watch than others. In <ref> [2, 4, 16, 25, 26] </ref> it is noted that better results were obtained using the data from some pilots rather than others. <p> Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In <ref> [2, 4, 16, 25, 26] </ref> the learning program employed, C4.5 [23], did not merely learn to copy identically each pilot modeled. <p> ! 5 An interestingly contrasting theorem in the same section (Theorem 2.2) implies that, if a class of process-control games can be learned incrementally, i.e., after finitely many trial and error rounds, from arbitrary masters, then it can be incrementally learned by copycatting selected masters. 6 In the learning-to-fly project <ref> [2, 4, 16, 25, 26] </ref> it was discovered that C4.5 got confused if it received data from more than one pilot at a time. <p> For the theorems, all masters considered (by definition) play perfect winning strategies. In the learning-to-fly project, data from medium-skilled pilots provided better learning <ref> [2, 4, 16, 25, 26] </ref>. 4 Formally, this distinction is handled definitionally by universal versus existential quantifiers over masters in positive assertions (see Definition 2.1 in Section 2 below). 5 The learning from masters in the present paper is off-line learning, learning programs for winning strategies.
Reference: [26] <author> C. Sammut. </author> <title> Automatic construction of reactive control systems using symbolic machine learning. </title> <journal> Knowledge Engineering Review, </journal> <volume> 11(1) </volume> <pages> 27-42, </pages> <year> 1996. </year>
Reference-contexts: In machine learning, the behavioral cloning approach to process-control, surveyed in [2], involves using data from the behavior of master or expert human controllers. For example, it has been used successfully to teach an autopilot to fly an aircraft simulator <ref> [2, 4, 16, 25, 26] </ref> and to teach a machine to operate efficiently a free-swinging shipyard crane [2, 29]. <p> Nonetheless, some of the parallels we describe, in the rest of this subsection, between these experimental machine learning results and our main theorems are very interesting and, we hope, instructive for the future. For pedagogical purposes, some masters may be better to watch than others. In <ref> [2, 4, 16, 25, 26] </ref> it is noted that better results were obtained using the data from some pilots rather than others. <p> Theorem 2.6 in Section 2 below implies that some masters are strictly more helpful than others. 3 Hence, we distinguish between whether we are using arbitrary or carefully selected masters. 4 In <ref> [2, 4, 16, 25, 26] </ref> the learning program employed, C4.5 [23], did not merely learn to copy identically each pilot modeled. <p> ! 5 An interestingly contrasting theorem in the same section (Theorem 2.2) implies that, if a class of process-control games can be learned incrementally, i.e., after finitely many trial and error rounds, from arbitrary masters, then it can be incrementally learned by copycatting selected masters. 6 In the learning-to-fly project <ref> [2, 4, 16, 25, 26] </ref> it was discovered that C4.5 got confused if it received data from more than one pilot at a time. <p> For the theorems, all masters considered (by definition) play perfect winning strategies. In the learning-to-fly project, data from medium-skilled pilots provided better learning <ref> [2, 4, 16, 25, 26] </ref>. 4 Formally, this distinction is handled definitionally by universal versus existential quantifiers over masters in positive assertions (see Definition 2.1 in Section 2 below). 5 The learning from masters in the present paper is off-line learning, learning programs for winning strategies.
Reference: [27] <author> C. Smith. </author> <title> The power of pluralism for automatic program synthesis. </title> <journal> J. of the ACM, </journal> <volume> 29 </volume> <pages> 1144-1165, </pages> <year> 1982. </year>
Reference-contexts: In our terminology this means: Theorem 3.3 [5] For all m 1: [1; m + 1]SelectMa 6 [1; m]SelectMa. Theorem 3.4 For all m 1: SelectMa m SelectMa m+1 . 17 The term team in the context of learning was introduced in <ref> [27] </ref>. There the first author of the present paper introduced motivation for its independent interest: if we send a group of m exploratory robots to a hostile planet, we don't need to know which one learns how to survive to send back data (just so one of them does). <p> If one considers [l; m]-teams of SelectMaId-learner then one gets the same team hierarchy as for Ex-style function identification <ref> [27] </ref>: every [l; m]-team is equivalent to exactly one [1; k]-team, namely to that with 1 m 1 k .
Reference: [28] <author> W. Thomas. </author> <title> On the synthesis of strategies in infinite games. </title> <booktitle> In Proceedings of the Annual Symposium on the Theoretical Aspects of Computer Science, volume 900 of LNCS, </booktitle> <pages> pages 1-13. </pages> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: If we model finite environments as deterministic finite automata [24], then, in these cases, the one-player immortality game can be modeled as follows: Given a finite automaton, a winning strat 8 For more formal treatment, see <ref> [6, 12, 14, 28] </ref>. 9 This definition of a master's behavior is somewhat of a mathematical idealization. In real games one might never get to watch how a master would react to situations which, for whatever reason, don't happen to occur. <p> From such a model one tries to compute or synthesize (perhaps incrementally) the corresponding controller. The synthesis can be modeled in terms of inputting a program for a corresponding game tree and eventually outputting (a program for) a winning strategy <ref> [3, 15, 28] </ref>. The results about learning from masters explicitly set off and labeled as theorems in the present paper all follow this model. <p> Is there a connection between the quality of the input masters and the number of plays which an on-line learner loses? The one-player immortality games given by a deterministic finite automaton, as described in Section 1.2, are just a special case of the well known two-player finite-state games <ref> [3, 15, 28] </ref>. In such games there always exist winning strategies which can be executed by a finite automaton.
Reference: [29] <author> T. Urbancic and I. Bratko. </author> <title> Reconstructing human skill with machine learning. </title> <editor> In A. Cohn, editor, </editor> <booktitle> Proceedings of the Eleventh European Conference on Artificial Intelligence. </booktitle> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: For example, it has been used successfully to teach an autopilot to fly an aircraft simulator [2, 4, 16, 25, 26] and to teach a machine to operate efficiently a free-swinging shipyard crane <ref> [2, 29] </ref>. Behavioral cloning partly motivates the present paper. fl Department of CIS, University of Delaware, Newark, DE 19716, USA, Email: case@cis.udel.edu y Institut fur Logik, Komplexitat und Deduktionssysteme, Universitat Karlsruhe, 76128 Karlsruhe, Germany, Email: m ott@ira.uka.de. <p> This classical (complete model) approach is realistic in some situations, but not in situations such as learning to maintain the safety and productivity of a chemical plant [1], robotics and manufacturing [7, 11, 30], or teaching a machine to operate efficiently a free-swinging shipyard crane <ref> [2, 29] </ref>. In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. <p> In many of these situations not everything is easily completely modeled, accessible to measurement, . . . . In such cases one, nonetheless, has access to salient portions of the behavior of the process to control. Machine learning techniques have been applied to control incompletely-modeled processes (see, for example, <ref> [1, 7, 11, 29, 30] </ref>). A corresponding theoretical approach has been introduced by Kummer and Ott [12] (see also [20]), where the learning mechanism has available at any time only a portion of the behavior of the process to control (equivalently, a portion of the corresponding game tree).

References-found: 29

