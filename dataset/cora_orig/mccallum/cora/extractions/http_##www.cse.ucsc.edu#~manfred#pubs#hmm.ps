URL: http://www.cse.ucsc.edu/~manfred/pubs/hmm.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: singer@research.att.com  manfred@cse.ucsc.edu  
Title: Training Algorithms for Hidden Markov Models Using Entropy Based Distance Functions  
Author: Yoram Singer Manfred K. Warmuth 
Address: 600 Mountain Avenue Murray Hill, NJ 07974  Santa Cruz, CA 95064  
Affiliation: AT&T Laboratories  Computer Science Department University of California  
Abstract: We present new algorithms for parameter estimation of HMMs. By adapting a framework used for supervised learning, we construct iterative algorithms that maximize the likelihood of the observations while also attempting to stay close to the current estimated parameters. We use a bound on the relative entropy between the two HMMs as a distance measure between them. The result is new iterative training algorithms which are similar to the EM (Baum-Welch) algorithm for training HMMs. The proposed algorithms are composed of a step similar to the expectation step of Baum-Welch and a new update of the parameters which replaces the maximization (re-estimation) step. The algorithm takes only negligibly more time per iteration and an approximated version uses the same expectation step as Baum-Welch. We evaluate experimentally the new algorithms on synthetic and natural speech pronunciation data. For sparse models, i.e. models with relatively small number of non-zero parameters, the proposed algorithms require significantly fewer iterations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Baldi and Y. Chauvin. </author> <title> Smooth on-line learning algorithms for Hidden Markov Models. </title> <journal> Neural Computation, </journal> <volume> 6(2), </volume> <year> 1994. </year>
Reference-contexts: In contrast, the on-line versions of our updates can be easily derived using only one observation sequence at a time. Also, there are alternative gradient descent based methods for estimating the parameters of HMMs. Such methods usually employ an exponential parameterization (such as soft-max) of the parameters (see <ref> [1] </ref>). For the case of learning one set of mixture coefficients an exponential parameterization led to an algorithm with a slower convergence rate compared to algorithms derived using entropic distances [5]. However, it is not clear whether this is still the case for HMMs.
Reference: [2] <author> L.E. Baum and T. Petrie. </author> <title> Statistical inference for probabilisticfunctionsof finite state markovchains. </title> <journal> Annals of Mathematical Statisitics, </journal> <volume> 37, </volume> <year> 1966. </year>
Reference-contexts: These val ues are calculated in the expectation step of the Expectation-Maximization (EM) training algorithm for HMMs [7], also known as the Baum-Welch <ref> [2] </ref> or the Forward-Backward algorithm. In the next sections we use the additional following expectations, n i () = x;s n i (x; s)P (x; sj) and n [i] () = j2 [i] n j ().
Reference: [3] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: To avoid the computational difficulties and the non-convexity of d RE we upper bound the relative entropy using the log sum inequality <ref> [3] </ref>: d RE ( ~ ; ) b d RE ( ~ ; ) = x;s P (x; sj ~ ) = x;s Q I i i=1 i = x;s I X n i (x; s) ln i I X ln i x;s I X n i ( ~ ) ln <p> To do so, we approximate the relative entropy by the 2 distance (see <ref> [3] </ref>), d RE (~p; p) d 2 (~p; p) = 1 P ( p i p i ) 2 p i , and use this distance to approximate b d RE ( ~ ; ): b d RE ( ~ ; ) b d 2 ( ~ ; ) = [i]
Reference: [4] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: Therefore, these updates belong to the family of Generalized EM (GEM) algorithms which are guaranteed to converge to a local maximum given some additional conditions <ref> [4] </ref>.
Reference: [5] <author> D. P. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. </author> <title> A comparison of new and old algorithms for a mixture estimation problem. </title> <booktitle> In Proceedingsof the Eighth Annual Workshop on ComputationalLearning Theory, </booktitle> <pages> pages 69-78, </pages> <year> 1995. </year>
Reference-contexts: Thus, instead of maximizing the log-likelihood we maximize, U ( ~ ) = LL (X j ~ ) d ( ~ ; ) (see <ref> [6, 5] </ref> for further motivation). Here d measures the distance between the old and new parameters and &gt; 0 is a trade-off factor. Maximizing U ( ~ ) is usually difficult since both the distance function and the log likelihood depend on ~ . As in [6, 5], we approximate the <p> ~ ; ) (see <ref> [6, 5] </ref> for further motivation). Here d measures the distance between the old and new parameters and &gt; 0 is a trade-off factor. Maximizing U ( ~ ) is usually difficult since both the distance function and the log likelihood depend on ~ . As in [6, 5], we approximate the log-likelihood by a first order Taylor expansion around ~ = and add Lagrange multipliers for the constraints that the parameters of each class must sum to one: U ( ~ ) LL (X j) + ( ~ )r LL (X j) d ( ~ ; ) <p> In our experiments we mainly used sparse models, that is, models with many parameters clamped to zero. Previous work (e.g., <ref> [5, 6] </ref>) might suggest that the entropic updates will perform better on sparse models. (Indeed, when we used dense models to generate the data, the algorithms showed almost the same performance). The training algorithms, however, were started from a randomly chosen dense model. <p> Such methods usually employ an exponential parameterization (such as soft-max) of the parameters (see [1]). For the case of learning one set of mixture coefficients an exponential parameterization led to an algorithm with a slower convergence rate compared to algorithms derived using entropic distances <ref> [5] </ref>. However, it is not clear whether this is still the case for HMMs. Our future goals is to perform a comparative study of the different updates with emphasis on the on-line versions.
Reference: [6] <author> J. Kivinen and M. K. Warmuth. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <journal> Informationa and Computation, </journal> <note> 1997. To appear. </note>
Reference-contexts: summation here is over all legal x and s of arbitrary length and n [i] () is the expected number of times the state [i] was visited. 2 Entropic distance functions for HMMs Our training algorithms are based on the following framework of Kivinen and Warmuth for motivating iterative updates <ref> [6] </ref>. Assume we have already done a number of iterations and our current parameters are . Assume further that X is the set of observations to be processed in the current iteration. <p> Thus, instead of maximizing the log-likelihood we maximize, U ( ~ ) = LL (X j ~ ) d ( ~ ; ) (see <ref> [6, 5] </ref> for further motivation). Here d measures the distance between the old and new parameters and &gt; 0 is a trade-off factor. Maximizing U ( ~ ) is usually difficult since both the distance function and the log likelihood depend on ~ . As in [6, 5], we approximate the <p> ~ ; ) (see <ref> [6, 5] </ref> for further motivation). Here d measures the distance between the old and new parameters and &gt; 0 is a trade-off factor. Maximizing U ( ~ ) is usually difficult since both the distance function and the log likelihood depend on ~ . As in [6, 5], we approximate the log-likelihood by a first order Taylor expansion around ~ = and add Lagrange multipliers for the constraints that the parameters of each class must sum to one: U ( ~ ) LL (X j) + ( ~ )r LL (X j) d ( ~ ; ) <p> In our experiments we mainly used sparse models, that is, models with many parameters clamped to zero. Previous work (e.g., <ref> [5, 6] </ref>) might suggest that the entropic updates will perform better on sparse models. (Indeed, when we used dense models to generate the data, the algorithms showed almost the same performance). The training algorithms, however, were started from a randomly chosen dense model. <p> EU 2440 2389 2426 25.5 35.0 37.0 Entropic Update 2418 2352 2405 23.1 30.9 32.6 Table 1: Comparison of the entropic updates and Baum-Welch on speech pronunciation data. 6 Conclusions and future research In this paper we have showed how the framework of Kivinen and Warmuth <ref> [6] </ref> can be used to derive parameter updates algorithms for HMMs. We view an HMM as a joint distribution between the observation sequences and hidden state sequences and use a bound on relative entropy as a distance between the new and old parameter settings.
Reference: [7] <author> L.R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <year> 1986. </year>
Reference-contexts: These val ues are calculated in the expectation step of the Expectation-Maximization (EM) training algorithm for HMMs <ref> [7] </ref>, also known as the Baum-Welch [2] or the Forward-Backward algorithm. In the next sections we use the additional following expectations, n i () = x;s n i (x; s)P (x; sj) and n [i] () = j2 [i] n j ().
Reference: [8] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> On the learnability and usage of acyclic probabilistic finite automata. </title> <booktitle> In Proc. of the Eighth Annual Workshop on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: A common practice is to construct a set of stochastic models in order to capture the variability of the possible pronunciations. alternative pronunciations of a given word. This problem was studied previously in [9] using a state merging algorithm for HMMs and in <ref> [8] </ref> using a subclass of probabilistic finite automata. The purpose of the experiments discussed here is not to compare the above algorithms to the entropic updates but rather compare the entropic updates to Baum-Welch. Nevertheless, the resulting HMM pronunciation models are usually sparse. <p> Typically, only two or three phonemes have a non zero output probability at a given state and the average number of states that in practice can follow a states is about 2. Therefore, the entropic updates may provide a good alternative to the algorithms presented in <ref> [8, 9] </ref>. We used the TIMIT (Texas Instruments-MIT) database as in [8, 9]. This database contains the acoustic waveforms of continuous speech with phone labels from an alphabet of 62 phones which constitute a temporally aligned phonetic transcription to the uttered words. <p> Therefore, the entropic updates may provide a good alternative to the algorithms presented in <ref> [8, 9] </ref>. We used the TIMIT (Texas Instruments-MIT) database as in [8, 9]. This database contains the acoustic waveforms of continuous speech with phone labels from an alphabet of 62 phones which constitute a temporally aligned phonetic transcription to the uttered words.
Reference: [9] <author> A. Stolcke and S. Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In natural speech, a word might be pronounced differently by different speakers. A common practice is to construct a set of stochastic models in order to capture the variability of the possible pronunciations. alternative pronunciations of a given word. This problem was studied previously in <ref> [9] </ref> using a state merging algorithm for HMMs and in [8] using a subclass of probabilistic finite automata. The purpose of the experiments discussed here is not to compare the above algorithms to the entropic updates but rather compare the entropic updates to Baum-Welch. <p> Typically, only two or three phonemes have a non zero output probability at a given state and the average number of states that in practice can follow a states is about 2. Therefore, the entropic updates may provide a good alternative to the algorithms presented in <ref> [8, 9] </ref>. We used the TIMIT (Texas Instruments-MIT) database as in [8, 9]. This database contains the acoustic waveforms of continuous speech with phone labels from an alphabet of 62 phones which constitute a temporally aligned phonetic transcription to the uttered words. <p> Therefore, the entropic updates may provide a good alternative to the algorithms presented in <ref> [8, 9] </ref>. We used the TIMIT (Texas Instruments-MIT) database as in [8, 9]. This database contains the acoustic waveforms of continuous speech with phone labels from an alphabet of 62 phones which constitute a temporally aligned phonetic transcription to the uttered words.
Reference: [10] <author> L. Xu and M.I. Jordan. </author> <title> On convergence properties of the EM algorithm for Gaussian mixtures. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1996. </year>
Reference-contexts: Therefore, these updates belong to the family of Generalized EM (GEM) algorithms which are guaranteed to converge to a local maximum given some additional conditions [4]. Furthermore, using infinitesimal analysis and second order approximation of the likelihood function at the (local) maximum similar to <ref> [10] </ref>, it can be shown that the approximated 2 update is a contraction mapping and close to the local maximum there exists a learning rate &gt; 1 which results in a faster rate of convergence than when using = 1. 5 Experiments with Artificial and Natural Data In order to test
References-found: 10

