URL: ftp://iridia.ulb.ac.be/pub/dorigo/conferences/IC.15-MLC95.ps.gz
Refering-URL: http://iridia.ulb.ac.be/dorigo/pub_x_subj.html
Root-URL: 
Email: luca@idsia.ch  mdorigo@ulb.ac.be  
Title: Ant-Q: A Reinforcement Learning approach to the traveling salesman problem  
Author: Luca M. Gambardella Marco Dorigo 
Web: http://www.idsia.ch  http://iridia.ulb.ac.be/dorigo/dorigo.html  
Address: Corso Elvezia 36 6900 Lugano Switzerland  Avenue Franklin Roosevelt 50 CP 194/6 1050 Bruxelles, Belgium, EU  
Affiliation: IDSIA  IRIDIA, Universit Libre de Bruxelles  
Date: 1995, 252260.  
Note: Appeared in: Proceedings of ML-95, Twelfth Intern. Conf. on Machine Learning, Morgan Kaufmann,  
Abstract: In this paper we introduce Ant-Q, a family of algorithms which present many similarities with Q-learning (Watkins, 1989), and which we apply to the solution of symmetric and asymmetric instances of the traveling salesman problem (TSP). Ant-Q algorithms were inspired by work on the ant system (AS), a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in (Dorigo, 1992; Dorigo, Maniezzo and Colorni, 1996). We show that AS is a particular instance of the Ant-Q family, and that there are instances of this family which perform better than AS. We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP's are competitive with those obtained by other heuristic approaches based on neural networks or local search. Finally, we apply Ant-Q to some difficult asymmetric TSP's obtaining very good results: Ant-Q was able to find solutions of a quality which usually can be found only by very specialized algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Balas E., S. Ceria and G. Cornujols, </author> <year> 1993. </year> <title> A lift-and-project cutting plane algorithm for mixed 0-1 programs, </title> <booktitle> Mathematical Programming 58, </booktitle> <pages> 295324. </pages>
Reference-contexts: We also applied Ant-Q to some difficult ATSP problems finding very good results. For example, Ant-Q was able to find in 119 iterations 1 (238 seconds on a Pentium PC) the optimal solution for 43X2, a 43-city asymmetric problem <ref> (Balas, Ceria and Cornujols, 1993) </ref>.
Reference: <author> Colorni A., M. Dorigo and V. Maniezzo, </author> <year> 1991. </year> <title> Distributed Optimization by Ant Colonies. </title> <booktitle> Proceedings of ECAL91 - European Conference on Artificial Life, Paris, France, </booktitle> <editor> F.Varela and P.Bourgine (Eds.), </editor> <publisher> Elsevier Publishing, </publisher> <pages> 134142. </pages>
Reference: <author> Colorni A., M. Dorigo and V. Maniezzo, </author> <year> 1992. </year> <title> An Investigation of some Properties of an Ant Algorithm. </title> <booktitle> Proceedings of the Parallel Problem Solving from Nature Conference (PPSN 92), </booktitle> <address> Brussels, </address> <publisher> Belgium, </publisher> <editor> R.Mnner and B.Manderick (Eds.), </editor> <publisher> Elsevier Publishing, </publisher> <pages> 509520. </pages>
Reference: <author> Dorigo M., </author> <year> 1992. </year> <title> Optimization, Learning and Natural Algorithms. </title> <institution> Ph.D.Thesis, Politecnico di Milano, Italy, EU. </institution> <note> (In Italian.) </note> <editor> Dorigo M. and L.M. Gambardella, </editor> <year> 1995. </year> <title> Ant-Q: A Reinforcement Learning Approach to Combinatorial Optimization. </title> <type> Tech. Rep. </type> <institution> IRIDIA/95-01, Universit Libre de Bruxelles, Belgium. </institution>
Reference: <author> Dorigo M., V.Maniezzo and A.Colorni, </author> <year> 1996. </year> <title> The Ant System: Optimization by a colony of cooperating agents. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 26, 2, </volume> <publisher> in press. </publisher>
Reference: <author> Durbin R. and D. Willshaw, </author> <year> 1987. </year> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326, </volume> <pages> 689-691. </pages>
Reference-contexts: good for a set of benchmark problems: grid problems 3 , Oliver30 (a 30-city symmetric problem, see for example Whitley, Starkweather and Fuquay, 1989), ry48p (a 48-city asymmetric problem, see TSPLIB, in Reinelt, 1994), and for a set of five 50-city symmetric problems in which cities coordinates were randomly generated <ref> (Durbin and Willshaw, 1987) </ref>. 3.1 THE ACTION CHOICE RULE We tested Ant-Q algorithms with the following action choice rules: pseudorandom, pseudo-random-proportional, and random-proportional. They are all obtained from formula (1) as follows.
Reference: <author> Fischetti M. and P.Toth, </author> <year> 1992. </year> <title> An Additive Bounding Procedure for the Asymmetric Travelling Salesman Problem. </title> <journal> Mathematical Programming, </journal> <volume> 53, </volume> <pages> 173197. </pages>
Reference-contexts: The same problem could not be solved to optimality within 32 hours of computation on a workstation by the best published code available for the ATSP based on the Assignment Problem relaxation <ref> (Fischetti and Toth, 1992) </ref> of the ATSP, and was only very recently solved to optimality by (Fischetti and Toth, 1994) with an algorithm based on polyhedral cuts (branch-and-cut scheme).
Reference: <author> Fischetti M. and P.Toth, </author> <year> 1994. </year> <title> A polyhedral approach for the exact solution of hard ATSP instances. </title> <type> Technical Report OR-94, </type> <institution> DEIS, Universit di Bologna, Italy, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The same problem could not be solved to optimality within 32 hours of computation on a workstation by the best published code available for the ATSP based on the Assignment Problem relaxation (Fischetti and Toth, 1992) of the ATSP, and was only very recently solved to optimality by <ref> (Fischetti and Toth, 1994) </ref> with an algorithm based on polyhedral cuts (branch-and-cut scheme). We conclude in Section 6 briefly discussing related and future work. 2 THE ANT-Q FAMILY OF ALGO RITHMS We introduce the Ant-Q algorithm by its application to the traveling salesman problem.
Reference: <author> Lin S., </author> <year> 1965. </year> <title> Computer solutions of the traveling salesman problem. </title> <journal> Bell Syst. Journal, </journal> <volume> 44, 2245 2269. </volume>
Reference-contexts: We also compared the best result obtained by Ant-Q with those obtained using improved versions of the previous heuristics: SOM+, an improved version of SOM consisting of over 4,000 different runs of SOM processing the cities in various orders, and SA and FI plus local optimization by 2-opt and 3-opt <ref> (Lin, 1965) </ref> 5 . The comparison was run on the set of five 50-city problems of Durbin and Willshaw (1987). Table 4 reports the average results obtained by each heuristic (the best average result found by all heuristics is in bold font).
Reference: <author> Padberg M. and G. Rinaldi, </author> <year> 1990. </year> <title> Facet identification for the symmetric traveling salesman problem. </title> <booktitle> Mathematical programming 47, 219257. </booktitle> <address> Potvin JY., </address> <year> 1993. </year> <title> The traveling salesman problem: A neural network Perpective. </title> <journal> ORSA Journal of Computing, </journal> <volume> 5, 4, </volume> <pages> 328347. </pages>
Reference-contexts: fact, Ant-Q iteration complexity (order of mn 2 ) makes quickly infeasible its application to big TSP problems, for which there exist good heuristic and exact methods. (Using exact methods optimal solutions can be found for instances of many hundreds of cities; the largest TSP solved optimally has 2392 cities <ref> (Padberg and Rinaldi, 1990) </ref>.) On the other hand, ATSP problems are much more difficult than the TSP (using exact methods optimal solutions have been found for instances of no more than 200 cities), and Ant-Q maintains the same iteration complexity as when applied to the TSP.
Reference: <author> Reinelt G., </author> <year> 1994. </year> <title> The traveling salesman: Computational solutions for TSP applications. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 423.74 0.9 515.19 10 493.20 ry48p 0.3 15602 440 14848 0.3 14690 175 14422 0.9 19495 797 17921 These values were found to be very good for a set of benchmark problems: grid problems 3 , Oliver30 (a 30-city symmetric problem, see for example Whitley, Starkweather and Fuquay, 1989), ry48p <ref> (a 48-city asymmetric problem, see TSPLIB, in Reinelt, 1994) </ref>, and for a set of five 50-city symmetric problems in which cities coordinates were randomly generated (Durbin and Willshaw, 1987). 3.1 THE ACTION CHOICE RULE We tested Ant-Q algorithms with the following action choice rules: pseudorandom, pseudo-random-proportional, and random-proportional. <p> In Table 6 we report the best result and the time needed to find it by the two exact algorithms and the mean and the best result obtained by Ant-Q on 15 trials of 600 iterations each. The two test problems are available in the TSPLIB 7 <ref> (Reinelt, 1994) </ref>.
Reference: <author> Siegel S. and N.J. Castellan, </author> <year> 1956. </year> <title> Nonparametric statistics for the behavioral sciences. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: We report means, variances and, when necessary, the significance of comparisons between means was computed by Mann-Whitney t-tests and Kruskal-Wallis ANOVA <ref> (Siegel and Castellan, 1956) </ref>. In tables we report average and best performances. The average performance is computed by taking the best result obtained in each of the 15 trials and computing the mean. The best performance is given by the best result obtained in the 15 trials.
Reference: <author> Watkins C.J.C.H., </author> <year> 1989. </year> <title> Learning with delayed rewards. </title>
Reference: <author> Ph. D. </author> <type> dissertation, </type> <institution> Psychology Department, University of Cambridge, </institution> <address> England. </address>
Reference: <author> Whitley D., T. Starkweather and D. Fuquay, </author> <year> 1989. </year> <title> Scheduling problems and travelling salesman: the genetic edge recombination operator. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 133140. </pages>
References-found: 15

