URL: ftp://ftp.mbfys.kun.nl/snn/pub/reports/Heskes.lognips.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Email: tom@mbfys.kun.nl  
Title: Selecting weighting factors in logarithmic opinion pools  
Author: Tom Heskes 
Address: Geert Grooteplein 21, 6525 EZ Nijmegen, The Netherlands  
Affiliation: University of Nijmegen  
Note: To appear in Advances in Neural Information Processing Systems 10, 1998.  Foundation for Neural Networks,  
Abstract: A simple linear averaging of the outputs of several networks as e.g. in bagging [3], seems to follow naturally from a bias/variance decomposition of the sum-squared error. The sum-squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble [7], suggesting a quadratic programming algorithm for finding the "optimal" weighting factors. If we interpret the output of a network as a probability statement, the sum-squared error corresponds to minus the loglikelihood or the Kullback-Leibler divergence, and linear averaging of the outputs to logarithmic averaging of the probability statements: the logarithmic opinion pool. The crux of this paper is that this whole story about model averaging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum-squared error, but applies to the combination of probability statements of any kind in a logarithmic opinion pool, as long as the Kullback-Leibler divergence plays the role of the error measure. As examples we treat model averaging for classification models under a cross-entropy error measure and models for estimating variances.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Benediktsson and P. Swain. </author> <title> Consensus theoretic classification methods. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22 </volume> <pages> 688-704, </pages> <year> 1992. </year>
Reference-contexts: Selecting weighting factors in any combination of probability statements is known to be a difficult problem for which several suggestions have been made. These suggestions range from rather involved supra-Bayesian methods to simple heuristics (see e.g. <ref> [1, 6] </ref> and references therein). The method that follows from our analysis is probably somewhere in the middle: easier to compute than the supra-Bayesian methods and more elegant than simple heuristics. To stress the generality of our results, the presentation in the next section will be rather formal.
Reference: [2] <author> R. Bordley. </author> <title> A multiplicative formula for aggregating probability assessments. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1137-1148, </pages> <year> 1982. </year>
Reference-contexts: In fact, logarithmic opinion pools have been proposed to overcome some of the weaknesses of the linear opinion pool. For example, the logarithmic opinion pool is "externally Bayesian", i.e., can be derived from joint probabilities using Bayes' rule <ref> [2] </ref>. A drawback of the logarithmic opinion pool is that if any of the experts assigns probability zero to a particular outcome, the complete pool assigns probability zero, no matter what the other experts claim.
Reference: [3] <author> L. Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 123-140, </pages> <year> 1996. </year>
Reference-contexts: Suppose that we have available a data set consisting of P combinations fx ; y g. As suggested in <ref> [3] </ref>, we construct different models by training them on different bootstrap replicates of the available data set. Optimizing nonlinear models is often an unstable process: small differences in initial parameter settings or two almost equivalent bootstrap replicates can result in completely different models.
Reference: [4] <author> T. Heskes. </author> <title> Balancing between bagging and bumping. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 466-472, </pages> <address> Cambridge, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Neural networks, for example, are notorious for local minima and plateaus in weight space where models might get stuck. Therefore, the incorporation of weighting factors, even when models are constructed using the same procedure, can yield a better generalizing opinion pool. In <ref> [4] </ref> good results have been reported on several regression problems. Balancing clearly outperformed bagging, which corresponds to w ff = 1=n with n the number of experts, and bumping, which proposes to keep a single expert. <p> In <ref> [4] </ref> we suggest how to remove this bias for regression models minimizing sum-squared errors. Similar compensations can be found for other probability models.
Reference: [5] <author> T. Heskes. </author> <title> Practical confidence and prediction intervals. </title> <editor> In M. Mozer, M. Jor-dan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 176-182, </pages> <address> Cambridge, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In fact, one can use the probability density (6) with input-dependent (x). We will consider the simpler situation in which an input-dependent model is fitted to residuals y, after a regression model has been fitted to estimate the mean (see also <ref> [5] </ref>). The probability model of expert ff can be written p ff (yjx) = z ff (x) exp 2 ; where 1=z ff (x) is the experts' estimate of the residual variance given input x.
Reference: [6] <author> R. Jacobs. </author> <title> Methods for combining experts' probability assessments. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 867-888, </pages> <year> 1995. </year>
Reference-contexts: Selecting weighting factors in any combination of probability statements is known to be a difficult problem for which several suggestions have been made. These suggestions range from rather involved supra-Bayesian methods to simple heuristics (see e.g. <ref> [1, 6] </ref> and references therein). The method that follows from our analysis is probably somewhere in the middle: easier to compute than the supra-Bayesian methods and more elegant than simple heuristics. To stress the generality of our results, the presentation in the next section will be rather formal.
Reference: [7] <author> A. Krogh and J. Vedelsby. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 231-238, </pages> <address> Cambridge, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: From a bias/variance decomposition of the sum--squared error it follows that the error of the so obtained average model is always smaller or equal than the average error of the individual models. In <ref> [7] </ref> simple linear averaging is generalized to weighted linear averaging, with different weighting factors for the different networks in the ensemble. A slightly more involved bias/variance decomposition suggests a rather straightforward procedure for finding "optimal" weighting factors. <p> In <ref> [7] </ref>, this has all been derived starting from a sum-squared error measure. Variance estimation. There has been some recent interest in using neural networks not only to estimate the mean of the target distribution, but also its variance (see e.g. [9] and references therein).
Reference: [8] <author> P. Smyth and D. Wolpert. </author> <title> Stacked density estimation. </title> <booktitle> These proceedings, </booktitle> <year> 1998. </year>
Reference-contexts: We suppose that there is a "true" conditional probability model q (yjx) and have a whole ensemble (also called pool or committee) of experts, each supplying a probability model p ff (yjx). (x) is the unconditional probability distribution of inputs. An unsupervised scenario, as for example treated in <ref> [8] </ref>, is obtained if we simply neglect the inputs x or consider them constant.
Reference: [9] <author> P. Williams. </author> <title> Using neural networks to model conditional multivariate densities. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 843-854, </pages> <year> 1996. </year>
Reference-contexts: In [7], this has all been derived starting from a sum-squared error measure. Variance estimation. There has been some recent interest in using neural networks not only to estimate the mean of the target distribution, but also its variance (see e.g. <ref> [9] </ref> and references therein). In fact, one can use the probability density (6) with input-dependent (x). We will consider the simpler situation in which an input-dependent model is fitted to residuals y, after a regression model has been fitted to estimate the mean (see also [5]).
Reference: [10] <author> D. Wolpert. </author> <title> On bias plus variance. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 1211-1243, </pages> <year> 1997. </year>
Reference-contexts: The expression for the ambiguity, defined as the difference between these two, is much more involved and more difficult to interpret (see e.g. <ref> [10] </ref>). The ambiguity of the logarithmic opinion pool depends on the weighting factors w ff , not only directly as expressed in (3), but also through p (yjx).
References-found: 10

