URL: http://www.cs.ucsb.edu/~murat/ICSLP96.textOnly.ps
Refering-URL: http://www.cs.ucsb.edu/~murat/
Root-URL: http://www.cs.ucsb.edu
Title: feasibility of an interpreting video phone system using currently available recognition, synthesis, translation, and communication
Keyword: 8.2. Experiments and Demonstrations  
Note: 9. CONCLUSION The demonstrations have been very valuable in demonstrating the  REFERENCES  
Abstract: nary. While both of these approaches add additional complexity to the task-grammar and add to recognition time, for our application's task size the speed loss was unnoticeable. The IVP electronic shopping demonstrations have been shown to live audiences in Japan and U.S. at three exhibits lasting a total of nine days. We also had many days of experiments at various stages of the IVP system's development for testing and fine-tuning. A total of ten male and female speakers participated in the experiments and final demonstrations. At the experimentation stage we continually enhanced the dialogues and fine tuned system settings. The confirmation-step was added after realizing that it helped for a smoother dialogue by eliminating the need to send correction messages in case of misrecognitions. The likelihood of misrecognition is higher in a dialogue setting than in an isolated sentence recognition task because the speaker may get distracted and say sentences that are outside of the task-grammar. Incomplete sentences, long pauses between words, repetitions, false starts, restarts, or omissions are typical in human dialogues and still easily understandable by humans, however, extremely difficult to model using machines. Additionally, the voice detection is harder and recognition accuracy is negatively affected in the presence of showroom noise. Adding the confirmation step gave the speaker greater control and confidence over the system's accuracy and reduced stress. During the shows we had 8 hours of continuous demonstrations each day, and experienced recognition-to-synthesis times of 2-3 seconds quite consistently, however, about twice a day we would experience 1 to 10 minute delays due to Internet which required us to pause the on-going skits until transmission has cleared. Otherwise, the reliability of the Internet was high, and frequency of restarts due to network or software crashes was low, at less than once per day. The real challenge of building the IVP system has been integrating all the independent technologies for recognition, synthesis, translation, and communication to facilitate a real-time and natural interpreted dialogue while using reasonable and standard hardware resources. To a great extent our goals have been met. to its success. We were able to use a closed task dictionary and grammar, and use high-end PC and workstation equipment. The speakers were familiar with language used in the dialogues. Future enhancements for the IVP should include futher research on more flexible language modelling for recognition and translation of spontaneous speech with open vocabulary and grammar rules and more advanced speaker and microphone adaptation techniques. We would like to acknowledge the continuous support received from Brian Hanson, Shoji Hiraoka, Antoine Lefloch, Philippe Morin, and Michael Galler throughout this project. 1. Zhao, Y. A Speaker-Independent Continuous Speech Recognition System Using Continuous Mixture Gaussian Density HMM of Phoneme-Sized Units, IEEE Trans.SAP, pp. 345-361, 1993. 2. Moran, H., K. Hata, and S. Pearson, The Use of Sampled Consonants for Improved Intelligibility in Formant Synthesizers, JASA, Pt. 2, 2816, 1994. 3. Zhao, Y., An Acoustic-Phonetic Based Speaker Adaptation Technique for Improving Speaker-Independent Continuous Speech Recognition. IEEE Trans. SAP 2:380-394, 1994. 4. Sato, S. Example-Based Translation, Journal of Information Processing Society of Japan, Vol.33, No.6, pp. 673-681,1992. 5. Kamai, T., Matsui, K. Hybrid Synthesis Method using Pre-windowed Waveform Segments, Proc. Spring Meet. Acoust. Soc. Jpn., 3-4-7, pp287-288, 1995. 6. Miyata, M., et al. Speaker Independent Speech Recognition Using Sub-Word Units of Model Speech Uttered by a Small Number of Speakers,IEICE Technical Report SP91-83. 7. Endo, M., et al., A Study on Sentence Recognition Technique Using Linguistic Constraints Between Separate Word Pairs and Between Consecutive Word Pair, Proc. Spring Meet. Acoust. Soc. Jpn., 3-P-8, pp177-178, 1995. 8. Endo, M., et al., A Study on Fast Algorithm for A* Search with Cooccurrent-Word Model, Proc. Autumn Meet. Acoust. Soc. Jpn., 2-2-11, pp59-60, 1995. 9. Morimoto, T., et al.,ATR's Speech Translation System: ASURA, EUROSPEECH, pp.1291-1294, 1993 10. Roe, D.B., et al., Efficient Grammar Processing for a Spoken Language Translation System, ICASSP 1992, Vol.1, pp.213. 11. Suhm, B., et al.,JANUS: Towards Multi-Lingual Spoken Language Translation, ARPA Workshop on Spoken Language Technology, Austin, TX, 1995, V.1,pp.221-226 12. Hatazaki, K. et al.,INTERTALKER: An Experimental Automatic Interpretation System Using Conceptual Representation. ICSLP 1992. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Zhao, Y. </author> <title> A Speaker-Independent Continuous Speech Recognition System Using Continuous Mixture Gaussian Density HMM of Phoneme-Sized Units, </title> <journal> IEEE Trans.SAP, </journal> <pages> pp. 345-361, </pages> <year> 1993. </year>
Reference: 2. <author> Moran, H., K. Hata, and S. Pearson, </author> <title> The Use of Sampled Consonants for Improved Intelligibility in Formant Synthesizers, </title> <journal> JASA, </journal> <volume> Pt. 2, 2816, </volume> <year> 1994. </year>
Reference: 3. <author> Zhao, Y., </author> <title> An Acoustic-Phonetic Based Speaker Adaptation Technique for Improving Speaker-Independent Continuous Speech Recognition. </title> <journal> IEEE Trans. </journal> <volume> SAP 2 </volume> <pages> 380-394, </pages> <year> 1994. </year>
Reference: 4. <author> Sato, S. </author> <title> Example-Based Translation, </title> <journal> Journal of Information Processing Society of Japan, Vol.33, </journal> <volume> No.6, </volume> <pages> pp. </pages> <year> 673-681,1992. </year>
Reference: 5. <author> Kamai, T., Matsui, K. </author> <title> Hybrid Synthesis Method using Pre-windowed Waveform Segments, </title> <booktitle> Proc. Spring Meet. Acoust. Soc. Jpn., </booktitle> <address> 3-4-7, pp287-288, </address> <year> 1995. </year>
Reference: 6. <author> Miyata, M., et al. </author> <title> Speaker Independent Speech Recognition Using Sub-Word Units of Model Speech Uttered by a Small Number of Speakers,IEICE Technical Report SP91-83. </title>
Reference: 7. <author> Endo, M., et al., </author> <title> A Study on Sentence Recognition Technique Using Linguistic Constraints Between Separate Word Pairs and Between Consecutive Word Pair, </title> <booktitle> Proc. Spring Meet. Acoust. Soc. Jpn., </booktitle> <address> 3-P-8, pp177-178, </address> <year> 1995. </year>
Reference: 8. <author> Endo, M., et al., </author> <title> A Study on Fast Algorithm for A* Search with Cooccurrent-Word Model, </title> <booktitle> Proc. Autumn Meet. Acoust. Soc. Jpn., </booktitle> <address> 2-2-11, pp59-60, </address> <year> 1995. </year>
Reference: 9. <author> Morimoto, T., </author> <title> et al.,ATR's Speech Translation System: </title> <booktitle> ASURA, EUROSPEECH, </booktitle> <address> pp.1291-1294, </address> <year> 1993 </year>
Reference: 10. <editor> Roe, D.B., et al., </editor> <title> Efficient Grammar Processing for a Spoken Language Translation System, </title> <booktitle> ICASSP 1992, Vol.1, </booktitle> <address> pp.213. </address>
Reference: 11. <author> Suhm, B., et al.,JANUS: </author> <title> Towards Multi-Lingual Spoken Language Translation, </title> <booktitle> ARPA Workshop on Spoken Language Technology, </booktitle> <address> Austin, TX, </address> <year> 1995, </year> <month> V.1,pp.221-226 </month>





Reference: 5. <editor> SYNTHESIS Two different and independently developed text-to-speech (TTS) synthesizers are used, </editor> <title> one for Japanese [5] and another for English [2]. Both TTS systems are highly intelligible, hybrid systems combining formant-synthesis method and wave-concatanation methods. The TTS synthesizers can be dynamically switched between a male and female voice. Both TTS synthesizers run in real-time on PCs without additional hardware support and provide a user-lexicon capability. </title>



References-found: 12

