URL: http://www.robotics.stanford.edu/~koller/papers/fsymp96.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/fsymp96.html
Root-URL: http://www.robotics.stanford.edu
Email: koller@cs.stanford.edu  
Title: Evidence-directed belief network simplification  
Author: Daphne Koller 
Address: Gates Building 1A Stanford, CA 94305-9010  
Affiliation: Computer Science Department  
Abstract: The cost of exact probabilistic reasoning in a belief network increases rapidly with the structural complexity of the network: the number of nodes, the set of values taken by each, and the the density of edges in the network. We can therefore speed up the inference process in a belief network by conducting our inference on a simplified network that approximates the original one. In this paper, we consider the issue of what it means for one network to approximate another, and show how the answer can be used to guide the network simplification process. We also consider the effect of evidence on the quality of our approximation. In this paper, we focus on the application of these ideas to one particular type of structural simplification: the elimination of weak dependencies from the network. However, we believe that these ideas apply in a variety of other contexts. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boutilier, C.; Friedman, N.; Goldszmidt, M.; and Koller, D. </author> <year> 1996. </year> <title> Context-specific independence in Bayesian networks. </title> <booktitle> In Proceedings of the Twelfth Annual Conference on Uncertainty in Artificial Intelligence (UAI '96), </booktitle> <volume> 115 123. </volume>
Reference-contexts: The semantics of this tree is that the conditional distribution P (X j y) is the distribution at the unique leaf whose path is consistent with the assignment y to Parents (X). In recent work <ref> (Boutilier et al. 1996) </ref>, we ascribed formal semantics to these CPT-trees in terms of the notion of context-specific independence, and showed how this representation can be used to speed up the computation in two standard belief network inference algorithms: clustering (Lauritzen & Spiegelhalter 1988) and cutset conditioning (Pearl 1988). <p> Acknowledgements Some of these ideas were developed during the course of my work with Craig Boutilier, Nir Friedman, and Moises Goldszmidt on <ref> (Boutilier et al. 1996) </ref>. I am very grateful to them for their input. I would also like to thank Daishi Harada for useful discussions and his help on testing out some of these insights in practice.
Reference: <author> Change, K.-C., and Fung, R. </author> <year> 1991. </year> <title> Refinement and coarsening of Bayesian networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference on Uncertainty in Artificial Intelligence (UAI '91), </booktitle> <pages> 475482. </pages>
Reference-contexts: This phenomenon suggests that we can attempt to speed up the inference process for a belief network by simplifying the network structure and then executing probabilistic inference on the resulting network. A number of approaches in this spirit have been proposed, including the works of <ref> (Change & Fung 1991) </ref> and (Wellman & Liu 1994) on abstracting multiple values of the same node, the work of (Poh & Horvitz 1993) on the decision-theoretic value of certain network refinements, the work of (Draper & Hanks 1994) on the evaluation of a local fragment of the network, and the
Reference: <author> Cooper, G. F. </author> <year> 1990. </year> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence 42:393405. </journal>
Reference-contexts: 1 Introduction Probabilistic inference in Bayesian belief networks is known to be NP-hard <ref> (Cooper 1990) </ref>. Therefore, we cannot expect our algorithms to require less than exponential time in the worst case. However, the performance of typical belief network inference algorithms varies widely over different networks. <p> with an elegant solution to this problem: 1 Theorem 2.1: Let B 1 and B 2 be two belief networks with the same structure, and let X 1 ; : : : ; X n represent the nodes 1 The same decomposition was also used by Cooper and Her-skovits in <ref> (Herskovits & Cooper 1990) </ref>. in this structure, and for each i, let Y i represent the parents of X i . X X P B 1 (y) (1) where val (Y ) denotes the set of possible assignments of values to the a set of variables Y .
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: Both the original Bayesian network and the simplified one are simply (compact) representations of distributions. Thus, we should use a metric tar-getted for evaluating the distance between two distributions over the same joint robability space. Arguably, the metric best suited for this task is cross-entropy <ref> (Cover & Thomas 1991) </ref>, which is defined as follows: For two distributions ; over the same probability space , the cross-entropy distance is D (jj) = P This measure can be understood as follows: log ( (!) is an estimate of the difference between the probability assigned to ! by the <p> A priori, it seems hard to find a distribution that approximates a distribution . In fact, practical considerations seem to prevent us from even evaluating D (jj) given that this requires the summation over the entire joint probability space. Luckily, cross-entropy has some very desirable decomposition properties <ref> (Cover & Thomas 1991) </ref> when applied to a decomposable distribution derived from a belief network.
Reference: <author> Draper, D., and Hanks, S. </author> <year> 1994. </year> <title> Localized partial evaluation of belief networks. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> 170177. </pages>
Reference-contexts: A number of approaches in this spirit have been proposed, including the works of (Change & Fung 1991) and (Wellman & Liu 1994) on abstracting multiple values of the same node, the work of (Poh & Horvitz 1993) on the decision-theoretic value of certain network refinements, the work of <ref> (Draper & Hanks 1994) </ref> on the evaluation of a local fragment of the network, and the work of (Kjaerulff 1994) on the elimination of weak intercausal dependencies from the moral graph.
Reference: <author> Herskovits, E., and Cooper, G. </author> <year> 1990. </year> <title> Kutato: An entropy-driven system for construction of probabilistic expert systems from databases. </title> <booktitle> In Proc. of UAI-90, </booktitle> <pages> 5462. </pages>
Reference-contexts: with an elegant solution to this problem: 1 Theorem 2.1: Let B 1 and B 2 be two belief networks with the same structure, and let X 1 ; : : : ; X n represent the nodes 1 The same decomposition was also used by Cooper and Her-skovits in <ref> (Herskovits & Cooper 1990) </ref>. in this structure, and for each i, let Y i represent the parents of X i . X X P B 1 (y) (1) where val (Y ) denotes the set of possible assignments of values to the a set of variables Y .
Reference: <author> Kjaerulff, U. </author> <year> 1994. </year> <title> Reduction of computational complexity in Bayesian networks through removal of weak dependences. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> 375382. </pages>
Reference-contexts: 1991) and (Wellman & Liu 1994) on abstracting multiple values of the same node, the work of (Poh & Horvitz 1993) on the decision-theoretic value of certain network refinements, the work of (Draper & Hanks 1994) on the evaluation of a local fragment of the network, and the work of <ref> (Kjaerulff 1994) </ref> on the elimination of weak intercausal dependencies from the moral graph. In this abstract, we consider the general problem of approximating one network by another, and its implementation in the context of a specific approach to doing the approximation.
Reference: <author> Lauritzen, S. L., and Spiegelhalter, D. J. </author> <year> 1988. </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B 50(2):157224. </journal>
Reference-contexts: In recent work (Boutilier et al. 1996), we ascribed formal semantics to these CPT-trees in terms of the notion of context-specific independence, and showed how this representation can be used to speed up the computation in two standard belief network inference algorithms: clustering <ref> (Lauritzen & Spiegelhalter 1988) </ref> and cutset conditioning (Pearl 1988). In general, however, networks are described in terms of full tabular CPTs, preventing us from utilizing these techniques. Even in those cases where a CPT is given as a tree, the tree might be too large to allow efficient probabilistic inference.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Sys--tems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In recent work (Boutilier et al. 1996), we ascribed formal semantics to these CPT-trees in terms of the notion of context-specific independence, and showed how this representation can be used to speed up the computation in two standard belief network inference algorithms: clustering (Lauritzen & Spiegelhalter 1988) and cutset conditioning <ref> (Pearl 1988) </ref>. In general, however, networks are described in terms of full tabular CPTs, preventing us from utilizing these techniques. Even in those cases where a CPT is given as a tree, the tree might be too large to allow efficient probabilistic inference.
Reference: <author> Poh, K. L., and Horvitz, E. J. </author> <year> 1993. </year> <title> Reasoning about the value of decision-model refinement: Methods and application. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Uncertainty in Artificial Intelligence (UAI '93), </booktitle> <pages> 174182. </pages>
Reference-contexts: A number of approaches in this spirit have been proposed, including the works of (Change & Fung 1991) and (Wellman & Liu 1994) on abstracting multiple values of the same node, the work of <ref> (Poh & Horvitz 1993) </ref> on the decision-theoretic value of certain network refinements, the work of (Draper & Hanks 1994) on the evaluation of a local fragment of the network, and the work of (Kjaerulff 1994) on the elimination of weak intercausal dependencies from the moral graph.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C45: Programs for Machince Learning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It remains only to choose the structure of the tree. Fortunately, this can be done using techniques from the machine learning literature <ref> (Quinlan 1993) </ref>. These methods are com-putationally efficient, and are known to lead to good results in practice. The standard approach to learning decision trees for classifying a given data set involves learning a large tree and then pruning it. The growing stage is fairly straightforward. <p> We then recursively learn the subtree corresponding to each y on D y . In the second stage the learned tree is pruned using variety of techniques <ref> (Quinlan 1993) </ref>. We suggest a similar process for learning tree structure in BNs. Assume we are given a BN B, and an error bound *. Our goal is to produce a compact belief network B 0 such that D (P B ; P B 0 ) *.
Reference: <author> Wellman, M. P., and Liu, C.-L. </author> <year> 1994. </year> <title> State-space abstraction for anytime evaluation of probabilistic networks. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> 567574. </pages>
Reference-contexts: A number of approaches in this spirit have been proposed, including the works of (Change & Fung 1991) and <ref> (Wellman & Liu 1994) </ref> on abstracting multiple values of the same node, the work of (Poh & Horvitz 1993) on the decision-theoretic value of certain network refinements, the work of (Draper & Hanks 1994) on the evaluation of a local fragment of the network, and the work of (Kjaerulff 1994) on
References-found: 12

