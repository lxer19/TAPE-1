URL: http://www.cs.washington.edu/homes/sgberg/quals/quals.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/sgberg/quals/index.html
Root-URL: http://www.cs.washington.edu
Email: sgberg@cs.washington.edu  
Title: A Framework for Evaluating Software and Hardware Mechanisms for Reducing False Sharing  
Author: Stefan G. Berg 
Date: October 17, 1996  
Address: Box 352350 Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science Engineering University of Washington,  
Abstract: We present a framework for studying the effectiveness of different approaches to reducing false sharing. Our framework includes the implementation of a distributed shared memory multiprocessor simulator that has been designed to work well with several currently existing cache coherency protocols known to reduce false sharing. A suite of seven benchmarks has been ported. These benchmarks exist in two versions, one of them optimized by Jeremiassen's compiler analysis for reducing false sharing. We show the results of running these benchmarks on the simulated architecture and give some possible reasons for why the framework in its current form is not suitable for the study we had designed it for. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> "Limits on Interconnection Network Performance." </title> <journal> IEEE Transactions on Parallel and Distributed Systems. </journal> <pages> pp. 398-412. </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: While this method gives exact results, it is resource intensive, programmer intensive, and error prone. Instead we chose to use an analytical model described and validated by Agarwal <ref> [1] </ref>. It is computationally very inexpensive and gives good results. The model uses the probabilities for routing packets in a switch to compute an expected value and a variance for the number of packets that leave a switch during some cycle.
Reference: [2] <author> F. J. Carrasco. </author> <title> "A Parallel Maxflow Implementation." </title> <type> CS411 Project Report. </type> <institution> Stanford University. </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Radiosity computes the distribution of light in a scene based on the iterative hierarchical diffuse radiosity method. Raytrace renders a three dimensional teacup using ray tracing. The other benchmarks are Pverify [16], Maxflow <ref> [2] </ref>, Topopt [6], and Water [19]: Pverify reads logical descriptions of two circuits and verifies that they are functionally equivalent; Maxflow computes the maximum flow from the distinguished source to sink in a directed graph with edge capacities; Topopt uses simulated annealing to do topological compaction of MOS circuits.
Reference: [3] <editor> M. Cekleov, et al. </editor> <booktitle> "SPARCcenter 2000: Multiprocessing for the 90's." IEEE COM-PCON. </booktitle> <pages> pp. 345-53. </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The first major change was to replace the bus with a 2-dimensional mesh of interconnected processors. This was necessary, because a bus-based architecture typically does not support more than four processors <ref> [10, 3] </ref>. False sharing, however, often does not become noticeable until four or more processors are used. Second, the bus-snooping cache protocol was replaced by a distributed directory-based cache coherency protocol.
Reference: [4] <author> L. M. Censier and P. Feautrier. </author> <title> "A New Solution to Coherence Problems in Multic-ache Systems." </title> <journal> IEEE Transactions on Computers. </journal> <volume> Vol C-27. No 12. </volume> <pages> pp. 1112-8. </pages> <month> Decem-ber </month> <year> 1978. </year>
Reference-contexts: Both hardware schemes for reducing false sharing were designed for directory-based protocols; therefore when comparing them to the compiler-based alternative, the latter must use a coherency mechanism in the same family. We decided to implement the Censier & Feautrier directory-based protocol <ref> [4] </ref>, because it is a simple protocol with no false sharing support and it can serve well as a comparison basis to the different hardware false sharing solutions. To allow easy verification of the simulator, a global monitor was written to detect inconsistencies in the caches. <p> Our optimiza 5 tion was to prioritized the queue and move unlock requests ahead of any other requests. 3.2 Censier & Feautrier With the replacement of the bus with a multipath interconnect, the bus-snooping protocol had to go. We decided to implement the Censier & Feau-trier cache protocol <ref> [4, 5] </ref>, because it is a simple, traditional directory protocol. As such, it can server as a baseline against which the cache protocols that reduce false sharing can be compared and on which compiler-restructured benchmarks can be used. The simulator closely models the behavior of the cache protocol.
Reference: [5] <author> Y.S. Chen and M. Dubois. </author> <title> "Cache Protocols with Partial Block Invalidations." </title> <booktitle> International Parallel Processing Symposium. </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Not surprisingly attempts have been made to reduce the number of false sharing misses. Some do so by changing the hardware, others by changing the software. The hardware solutions all introduce new cache coherency protocols which try to prevent false sharing misses. For example, the Partial Block Invalidation protocol <ref> [5] </ref> uses multiple coherency blocks per cache block to reduce false sharing. By using a release consistent memory model, the Send and Receive Delayed [7] 1 protocol delays invalidations, therefore combining false sharing operations. <p> Section four will describe how the simulator was verified and discuss the feasibility of applying the compiler optimizations in this framework. We will conclude the paper in section five. 2 Solutions to False Sharing 2.1 Partial Block Invalidation The Partial Block Invalidation protocol <ref> [5] </ref> reduces false sharing by dividing each block into multiple coherency blocks and associating a valid bit with each. On each processor update, only the coherency block that contains the updated word is invalidated in other processors. <p> Our optimiza 5 tion was to prioritized the queue and move unlock requests ahead of any other requests. 3.2 Censier & Feautrier With the replacement of the bus with a multipath interconnect, the bus-snooping protocol had to go. We decided to implement the Censier & Feau-trier cache protocol <ref> [4, 5] </ref>, because it is a simple, traditional directory protocol. As such, it can server as a baseline against which the cache protocols that reduce false sharing can be compared and on which compiler-restructured benchmarks can be used. The simulator closely models the behavior of the cache protocol.
Reference: [6] <author> S. Devadas and A. R. </author> <title> Newton. "Topological Optimization of Multiple Level Array Logic." </title> <journal> IEEE Transactions on Computer-Aided Design. </journal> <pages> pp. 915-942. </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Radiosity computes the distribution of light in a scene based on the iterative hierarchical diffuse radiosity method. Raytrace renders a three dimensional teacup using ray tracing. The other benchmarks are Pverify [16], Maxflow [2], Topopt <ref> [6] </ref>, and Water [19]: Pverify reads logical descriptions of two circuits and verifies that they are functionally equivalent; Maxflow computes the maximum flow from the distinguished source to sink in a directed graph with edge capacities; Topopt uses simulated annealing to do topological compaction of MOS circuits.
Reference: [7] <author> M. Dubois, J. C. Wang, L. A. Barroso, K. Lee, Y.S. Chen. </author> <title> "Delayed Consistency and Its Effects on the Miss Rate of Parallel Programs." </title> <booktitle> Proceedings of the 1991 Supercomputing Conference. </booktitle> <pages> pp. 197-207, </pages> <month> November. </month> <year> 1991. </year>
Reference-contexts: The hardware solutions all introduce new cache coherency protocols which try to prevent false sharing misses. For example, the Partial Block Invalidation protocol [5] uses multiple coherency blocks per cache block to reduce false sharing. By using a release consistent memory model, the Send and Receive Delayed <ref> [7] </ref> 1 protocol delays invalidations, therefore combining false sharing operations. An invalidation received can be delayed until the next lock instruction; an invalidation issued can be held back until the next unlock instruction. The obvious drawback of these methods is the need for new hardware. <p> The costs of this approach are additional state bits for the valid bit for each coherency block and more complicated control logic because more states have been added to the cache. 2.2 Send and Receive Delayed The Send and Receive Delayed protocol <ref> [7] </ref> reduces false sharing by delaying invalidations as long as possible. Delaying invalidations helps especially well in the common case where two pro cessors invalidate each other's cache line in rapid alternation. <p> When the processor executes an unlock instruction, its invalidation buffer is flushed. Dubois et al. have shown miss rate reductions between 8% and 94% for a block size of 128 bytes when comparing the Send and Receive Delayed protocol to a regular non-delayed protocol <ref> [7] </ref>. The best results were seen in a picture interpolation program. This program uses virtually no synchronization. Processes pass over the pixel values, reading each, computing the unknowns, and writing the value back.
Reference: [8] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> "Eliminating False Sharing." </title> <booktitle> International Conference on Parallel Processing. </booktitle> <pages> pp. 377-81. </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: False sharing can be a sizable impediment to decent parallel performance. One study using coarse-grained applications showed that with block sizes from 8 bytes to 256 bytes false sharing made up 40% to 90% of all cache misses <ref> [8] </ref>. The misses increase traffic on the interconnect and generally increase the execution time. False sharing tends to be a more serious problem for large cache block sizes and when many processors are present. Due to that, false sharing is also a limiting factor for the scalability of parallel programs.
Reference: [9] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> "Static Analysis of Barrier Synchronization in Explicitly Parallel Programs." </title> <booktitle> International Conference on Parallel Architectures and Compilation Techniques. </booktitle> <pages> pp. 171-80. </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The control-flow graph is annotated accordingly and processes with identical annotation patterns in the control-flow are grouped into process families. In the second stage, an in-terprocedural non-concurrency analysis examines barrier synchronization patterns within the program to find a control-flow between phases that are guaranteed not to executed concurrently <ref> [9] </ref>. The last stage performs an enhanced interproced-ural, flow-insensitive, summary side-effect analysis and static profiling on all combinations of process families and phases. Both stages one and three give information about per-process references to shared data. Stage one identifies reference patterns due to differences in the control-flow.
Reference: [10] <author> M. Galles and E. Williams. </author> <title> "Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor." </title> <booktitle> Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences. Volume I: Architecture. </booktitle> <pages> pp. 134-43. </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The first major change was to replace the bus with a 2-dimensional mesh of interconnected processors. This was necessary, because a bus-based architecture typically does not support more than four processors <ref> [10, 3] </ref>. False sharing, however, often does not become noticeable until four or more processors are used. Second, the bus-snooping cache protocol was replaced by a distributed directory-based cache coherency protocol.
Reference: [11] <author> T. E. Jeremiassen. </author> <title> "LOKI A Multiprocessor Cache Simulator." </title> <institution> AT&T Bell Laboratories. </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The purpose of the work described in this paper is to design and verify a framework for doing such a study. The framework is based on Mint [18], a program-driven memory reference generator that permits easy building and modification of a target system simulator (back-end). We used Loki <ref> [11] </ref> as a starting point for the target system simulator. It simulates a Power-PC architecture using a split-transaction bus for communication between the processors. Loki implements Spur [14], a bus-snooping cache protocol. <p> We use Mint [18] as the memory reference generator (front-end), because it uses executables as its input (instead of traces), and it has an easy interface to the target system simulator (back-end). We use a modified version of Loki <ref> [11] </ref> as our back-end. Loki simulates a shared memory multiprocessor with split I & D first-level caches and a unified copy-back second 4 level cache. The first-level cache is a write--through cache with a write-buffer for the data section of the cache.
Reference: [12] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> "Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations." </title> <booktitle> Conference on Principals and Practices on Parallel Programming. </booktitle> <pages> pp. 179-88. </pages> <year> 1995 </year>
Reference-contexts: While true sharing is an essential part of a program, false sharing is not and can always be reduced (if not completely eliminated) by changing the data layout or explicitly detecting it at run-time <ref> [12] </ref>. False sharing can be a sizable impediment to decent parallel performance. One study using coarse-grained applications showed that with block sizes from 8 bytes to 256 bytes false sharing made up 40% to 90% of all cache misses [8]. <p> The compiler approach reduces between 4% and 85% of the cache misses for coherency block sizes of 16 and 128 bytes. The speedups often show little improvement for few number of processors, but the maximum speedups reach values up to three times that of the unoptimized program <ref> [12] </ref>. The drawback of the compiler optimization is that it bases its decision to restructure shared data on an approximation of dynamic cross-processor memory accesses. In addition, spatial locality may be reduced in some instances, where data is moved apart.
Reference: [13] <author> T. E. Jeremiassen. </author> <title> "Using Compile-Time Analysis and Transformations to Reduce False Sharing on Shared-Memory Multiprocessors." </title> <type> PhD Dissertation. </type> <year> 1995. </year>
Reference-contexts: The software approaches either restructure the shared data or change the code to alter the cross-processor memory pattern. One such approach optimizes the layout of the program data structures based on a compile-time analysis <ref> [13] </ref>. The analysis computes an approximation of the memory access pattern for each processor, tries to pinpoint the data susceptible to false sharing, and suggests a data transformation to reduce it. The appeal of this method is that it works with no hardware modifications in a sequentially consistent memory model. <p> Some of this mismatch can be corrected by restructuring the program data. The data layout is known by simply examining the shared data declarations, but the memory accesses of each processor require an in-depth analysis of the program. The analysis to compute per-process memory accesses consists of three stages <ref> [13] </ref>. First, an interprocedural control-flow analysis determines which sections of code are executed by each process. The control-flow graph is annotated accordingly and processes with identical annotation patterns in the control-flow are grouped into process families. <p> Stage one made use of the original versions of the benchmarks and the optimized and unoptim-ized versions of the benchmarks used in the work done by Jeremiassen <ref> [13] </ref>. Jeremiassen's versions of the benchmarks were all designed to compile and run on the KSR-2 shared memory multiprocessor. <p> For brevity we do not show the exact outputs in this paper. The second step in verifying the framework involved running compiler-optimized and unoptim-ized versions of the benchmarks on the Censier & Feautrier protocol. We expected to see similar results as those that were published in the original study <ref> [13] </ref> of the compiler optimization that was conducted using the Spur cache protocol. <p> The results of this step are discussed next. 4.1 Framework Verification To demonstrate that the simulator reports believable results, we ran optimized and unoptimized versions of each benchmark and compared the results to those obtained by Jeremiassen's study on the KSR-2 <ref> [13] </ref>. While we did not expect the results to be comparable in absolute terms, we expected the relative differences of the optimized and unoptimized version to be comparable.
Reference: [14] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, R. G. Sheldon. </author> <title> "Implementing a Cache Consistency Protocol." </title> <booktitle> International Symposium on Computer Architecture. </booktitle> <pages> pp. 276-83. </pages> <month> June </month> <year> 1985. </year> <month> 15 </month>
Reference-contexts: We used Loki [11] as a starting point for the target system simulator. It simulates a Power-PC architecture using a split-transaction bus for communication between the processors. Loki implements Spur <ref> [14] </ref>, a bus-snooping cache protocol. Several changes had to be made to the simulated base to allow for an interesting and unbiased comparison between the hardware and software schemes. The first major change was to replace the bus with a 2-dimensional mesh of interconnected processors. <p> We use a 64 KB split, direct-mapped, first-level cache, a 2 MB unified, direct-mapped, second-level cache with 6 cycle minimum latency, and a write buffer capable of holding four words. Memory is distributed across processors in 4 KByte pages. The simulated cache coherency protocol is Spur <ref> [14] </ref>, a bus-snooping protocol. Loki simulates a split-transaction bus and has support for counting false sharing misses. To support the study, several features of Loki had to be modified, resulting in a rewrite of about 30% of the back-end.
Reference: [15] <author> C. P. Kruskal and M. Snir. </author> <title> "The Perform--ance of Multistage Interconnection Networks for Multiprocessors." </title> <journal> IEEE Transactions on Computers. </journal> <volume> Vol. C-32. </volume> <pages> pp. 1091-8. </pages> <month> Dec. </month> <year> 1983. </year>
Reference: [16] <author> H-K. T. Ma, S. Devadas, R-S. Wei, A. Sangiovanni-Vincentelli. </author> <title> "Logic Verification Algorithms and Their Parallel Implementation." </title> <journal> IEEE Transactions on Computer-Aided Design. </journal> <pages> pp. 181-189. </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Fmm simulates a two-dimensional system of bodies over a predefined time period, using the adaptive Fast Multipole Method. Radiosity computes the distribution of light in a scene based on the iterative hierarchical diffuse radiosity method. Raytrace renders a three dimensional teacup using ray tracing. The other benchmarks are Pverify <ref> [16] </ref>, Maxflow [2], Topopt [6], and Water [19]: Pverify reads logical descriptions of two circuits and verifies that they are functionally equivalent; Maxflow computes the maximum flow from the distinguished source to sink in a directed graph with edge capacities; Topopt uses simulated annealing to do topological compaction of MOS circuits.
Reference: [17] <author> R. Thekkath. </author> <title> "Design and Performance of Multithreaded Architectures." </title> <type> PhD Dissertation. </type> <year> 1995. </year>
Reference: [18] <author> J. E. Veenstra and R. J. Fowler. </author> <title> "MINT Tutorial and User Manual." </title> <type> Technical Report 452. </type> <month> August </month> <year> 1994. </year>
Reference-contexts: The purpose of the work described in this paper is to design and verify a framework for doing such a study. The framework is based on Mint <ref> [18] </ref>, a program-driven memory reference generator that permits easy building and modification of a target system simulator (back-end). We used Loki [11] as a starting point for the target system simulator. It simulates a Power-PC architecture using a split-transaction bus for communication between the processors. <p> In addition, spatial locality may be reduced in some instances, where data is moved apart. This could increase cache misses and decrease the efficiency of the cache. 3 Implementation This section will describe in detail the implementation of the framework. We use Mint <ref> [18] </ref> as the memory reference generator (front-end), because it uses executables as its input (instead of traces), and it has an easy interface to the target system simulator (back-end). We use a modified version of Loki [11] as our back-end.
Reference: [19] <author> J. P. Singh, W-D. Weber, A. Gupta. </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory." </title> <type> Stanford Technical Report No. </type> <institution> CSL-TR-91-469. </institution>
Reference-contexts: Radiosity computes the distribution of light in a scene based on the iterative hierarchical diffuse radiosity method. Raytrace renders a three dimensional teacup using ray tracing. The other benchmarks are Pverify [16], Maxflow [2], Topopt [6], and Water <ref> [19] </ref>: Pverify reads logical descriptions of two circuits and verifies that they are functionally equivalent; Maxflow computes the maximum flow from the distinguished source to sink in a directed graph with edge capacities; Topopt uses simulated annealing to do topological compaction of MOS circuits.
Reference: [20] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, A. Gupta. </author> <title> "The SPLASH-2 Programs: Characterization and Methodological Considerations." </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture. </booktitle> <pages> pp. 24-36. </pages> <month> June </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: For example, while an owned cache line is modified, it is checked that the cache controller making the modifications is indeed the owner indicated by the directory state bits. 3.4 Benchmarks Seven benchmarks were selected for inclusion in the framework. Four of the benchmarks are Splash-2 benchmarks <ref> [20] </ref>. The choice of benchmarks was guided by the availability of compiler-optimized versions of the benchmarks. The benchmarks have different speedup curves and different sensitivities to false sharing. In this section we will describe each benchmark and the work required to port them to run correctly on the framework.
References-found: 20

