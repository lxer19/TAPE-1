URL: ftp://ftp.csd.abdn.ac.uk/pub/pedwards/PUBS/daviessv.ps.Z
Refering-URL: http://www.csd.abdn.ac.uk/~pedwards/res/publs.html
Root-URL: 
Email: -wdavies, pedwards-@csd.abdn.ac.uk  
Title: The Communication of Inductive Infe rences  
Author: Winton Davies and Peter Edwards 
Address: Aberdeen, Scotland, UK, AB24 3UE  
Affiliation: Department of Computing Science, Kings College, University of Aberdeen,  
Abstract: We propose a new approach to communication between agents that perform inductive inference. Consider a community of agents where each agent has a limited view of the overall world. When an agent in this community induces a hypothesis about the world, it necessarily reflects that agents partial view of the world. If an agent communicates a hypothesis to another agent, and that hypothesis is in conflict with the receiving agents view of the world, then the receiving agent has to modify or discard the hypothesis. Previous systems have used voting methods or theory refinement techniques t o integrate these partial hypotheses. However, these mechanisms risk destroying parts of the hypothesis that are correct. Our proposal is that an agent should communicate the bounds of an induced hypothesis, along with the hypothesis itself. These bounds allow the hypotheses to be judged in the context from which they were formed. This paper examines using version space boundary sets to represent these bounds. Version space boundary sets may be manipulated using set operations. These operations can be used to evaluate and integrate multiple partial hypotheses. We describe a simple implementation of this approach, and draw some conclusions o n its practicality. Finally, we describe a tentative set of KQML operators for communicating hypotheses and their bounds.
Abstract-found: 1
Intro-found: 1
Reference: <editor> P. Brazdil, M. Gams, S. Sian, L. Torgo, and W. Van de Velde (1991). </editor> <booktitle> Learning in Distributed Systems and Multi-Agent Environments, In Proceedings of the European Working Session on Learning (EWSL91), </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 424-439, </pages> <address> Porto, Portugal. </address>
Reference-contexts: 1. Introduction In this paper we address the question: How should agents communicate inductive inferences? This question is important because of a growing interest in systems that distribute a learning task amongst a community of agents (Weiss and Sen, 1995). Such systems are often referred to as multi-agent learning systems <ref> (Brazdil, 1991) </ref>. The question is also relevant to the development of the Knowledge Query and Manipulation Language (Finin, 1993). KQML currently does not define standards for the communication of inductive inferences. Inductive inference differs from deductive inference in that it is logically unsound.
Reference: <author> P. Brazdil and L. </author> <month> Torgo </month> <year> (1990). </year> <title> Knowledge Acquisition via Knowledge Integration, </title> <booktitle> in Current Trends in AI, </booktitle> <editor> B. Wielenga et al.(eds.), </editor> <publisher> IOS Press, Amsterdam. </publisher>
Reference-contexts: ID5 (Utgoff, 1989) and theory refinement systems, e.g. EITHER (Mooney & Ourston, 1991). The second class of systems integrate individual agents hypotheses into a single theory. One method is to simply order the hypotheses (Gams, 1989). Another method is for the agents to vote for the best hypothesis <ref> (Brazdil & Torgo, 1990) </ref>. The final class of system discussed by Brazdil et al. are hybrids, in which agents both refine and vote on hypotheses during the inductive process; see for example (Sian, 1991).
Reference: <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, D. Haussler, R. E. Schapire, and M. </author> <title> K. </title>
Reference: <author> Warmuth (1995). </author> <title> How to Use Expert Advice, </title> <type> Technical Report UCSC-CRL-95-19, </type> <institution> University of California, </institution> <address> Santa Cruz, CA. </address>
Reference: <author> P. K. Chan and S. J. </author> <title> Stolfo (1995). A Comparative Evaluation of Voting and Meta-Learning on Partitioned Data, </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (ML95), </booktitle> <publisher> Morgan-Kaufmann, </publisher> <pages> pages 90-98, </pages> <address> Lake Tahoe, CA. </address>
Reference: <author> W. </author> <title> Davies (1993). ANIMALS, An Integrated Multi-Agent Learning System, M.Sc. </title> <type> Thesis, </type> <institution> Department of Computing Science, University of Aberdeen, UK. </institution>
Reference-contexts: It should also be noted that agents may communicate knowledge induced by one agent which is then used as background knowledge for another learning goal; see, for example <ref> (Davies, 1993) </ref>. Since this initial attempt to survey activity in the area, a number of additional systems and methods have been developed: Provost & Hennessey, 1995; Silver et al., 1991; Svatek, 1994; Chan & Stolfo, 1995.
Reference: <author> M. Des Jardins and Diana F. </author> <title> Gordon (1995). Evaluation and Selection of Biases in Machine Learning, </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 1-17. </pages>
Reference-contexts: The specific concerns so far are: 1. The approach adopted so far involves an NP-Complete algorithm. 2. We have not investigated how to handle noise and uncertainty, nor have investigated how to accommodate shifts in bias <ref> (Des Jardins and Gordon, 1995) </ref>. 3. Our implementation is an attribute-value learner. 4. We do not have a mechanism for either integrating version spaces of different concept description languages or manipulating different reference or descriptor sets. Future work will seek to address these issues in part.
Reference: <author> T. Finin, J. Weber, G. Wiederhold, M. Geneserth, R. Fritzson, D. MacKay, </author> <note> J. </note>
Reference: <author> McGuire, R. Pelavin, S. Shapiro, and C. </author> <title> Beck (1993). Draft Specification of the KQML Agent-Communication Language, </title> <type> Unpublished Draft. </type>
Reference: <author> R. W. Floyd and R. </author> <title> Beigel (1994). The Language of Machines, </title> <publisher> Computer Science Press, </publisher> <address> NY. </address>
Reference-contexts: However it scales poorly. An analysis (prompted by initial experiments) of sizeof (X) suggests it is NP-Hard. This is based on the observation that the algorithm is attempting to count the number of examples that would satisfy the G or S description. This is effectively the SAT problem <ref> (Floyd and Beigel, 1994) </ref> which is known to be NP-Complete. This is a disappointing result, but should have been expected. However, it is not grounds for giving up the approach. Firstly, learning descriptions that are 3-DNF and above is known to be NP-Complete. <p> In many cases there will be two reference sets, representing positive and negative training examples. 3. &lt;descriptor-set&gt;: This defines the representational biases. As the representational bias is a language, we assume we will specify this using a grammar of an appropriate class. For a finite language a regular expression <ref> (Floyd and Beigel, 1994) </ref> will suffice. 4. &lt;version-space&gt;: The operators imply the generation of a version space, either enumerated fully, or defined as a boundary set. Therefore the reply to an operator request consists of a version space.
Reference: <author> M. </author> <title> Gams (1989). New Measurements Highlight the Importance of Redundant Knowledge, </title> <booktitle> In Proceedings of the 4th European Working Session on Learning (EWSL89), </booktitle> <publisher> Pitman-Morgan Kaufmann, </publisher> <pages> pages 71-80, </pages> <address> Montpellier, France. </address>
Reference-contexts: ID5 (Utgoff, 1989) and theory refinement systems, e.g. EITHER (Mooney & Ourston, 1991). The second class of systems integrate individual agents hypotheses into a single theory. One method is to simply order the hypotheses <ref> (Gams, 1989) </ref>. Another method is for the agents to vote for the best hypothesis (Brazdil & Torgo, 1990). The final class of system discussed by Brazdil et al. are hybrids, in which agents both refine and vote on hypotheses during the inductive process; see for example (Sian, 1991).
Reference: <author> M. </author> <title> Genesereth (1995). Epilog for Lisp 2.0 Manual. </title> <publisher> Epistemics Inc., </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Its basic learning algorithm is an attribute value version of FOIL (Quinlan, 1990). That is, it uses training examples in the form attribute (example-id, value), and generates horn clauses that describe the given concept. We use the EPILOG theorem prover <ref> (Genesereth, 1995) </ref> to provide deductive inference capabilities. Our intended model of distributed learning is as follows: Agents generate the hypothesis (F) that best fits their view of the training examples and boundary sets G & S that describe the version space of the training examples (and of course the CDL).
Reference: <author> H. </author> <title> Hirsh (1989). Incremental Version Space Merging: A General Framework for Concept Learning, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: However the agent should never communicate a selected candidate hypothesis as if it had been soundly deduced until this point is reached. It is also possible to obtain the same convergence effect by intersecting version spaces <ref> (Hirsh, 89) </ref>. For a given learning goal, the examples and representational biases can be partitioned between several agents. Each agent then learns a version space consistent with the available knowledge. The resulting version spaces can then be intersected to form a new version space.
Reference: <author> S. Jain and A. </author> <title> Sharma (1993). Computational Limits on Team Identification of Languages, </title> <type> Technical Report 9301, </type> <institution> School of Computer Science and Engineering, University of New South Wales, Australia. </institution>
Reference: <author> M. Kearns and H. S. </author> <title> Seung (1995). Learning from a Population of Hypotheses, </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 255-276. </pages>
Reference: <author> N. Lavrac and S. </author> <title> Dzeroski (1994). Inductive Logic Programming: Techniques and Applications, </title> <publisher> Ellis Horwood, </publisher> <address> Herts, UK. </address>
Reference-contexts: Future work will seek to address these issues in part. The first two points may well be jointly solved by approximating the version space boundary sets, using statistical information. The third point will be pursued by first taking the approach employed in LINUS <ref> (Lavrac and Dzeroski, 1994) </ref>, and then examining the possibility of modifying an ILP algorithm such as GOLEM (Muggleton, 1992) to generate and use version spaces. Recent work by Nienhuys-Cheng and De Wolf (1996) suggests that certain classes of ILP problems are representable with boundary sets.
Reference: <author> R. S. </author> <title> Michalski (1993). Inferential Theory of Learning as a Conceptual Basis for Multistrategy Learning, </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 111-151. </pages>
Reference-contexts: We also review the Inferential Theory of Learning which we plan to use as the basis of our communication language <ref> (Michalski, 1993) </ref>. In Section 3 we describe our initial implementation, and discuss the practicality of the approach. We also discuss a number of features of boundary set representations which may prove useful. Section 4 attempts to outline the KQML performatives to be used in communicating inductive inferences.
Reference: <author> T. M. </author> <title> Mitchell (1978). Version spaces: An Approach to Concept Learning, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: However, the following question arose: How could inductive inferences be communicated, whilst simultaneously maintaining a logically sound distributed knowledge-base? This led us to propose a solution based on the communication of the bounds to the inductive inference." Version spaces <ref> (Mitchell, 1978) </ref> seem the natural candidate to describe these bounds, as they represent all possible hypotheses that are consistent with an agents view of the world. In Section 2.2 we provide an overview of Mitchells work and review more recent extensions to it. <p> This partial ordering defines a lattice which allows the version space to be represented using two boundary sets (G and S), which contain the most general and most specific hypotheses. As training examples are presented, the boundary sets are updated using the Candidate Elimination Algorithm <ref> (Mitchell 1978) </ref>. To complete the inductive inference a selection bias is applied to the version space. This imposes a preference order over the candidate hypotheses. An example of a selection bias is Occams Razor, where hypotheses with shorter descriptions are preferred.
Reference: <author> T. M. Mitchell, R. M. Keller, and S. T. </author> <month> Kedar-Cabell </month> <year> (1986). </year> <title> Explanation-Based Generalization: A Unifying View, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 1-33. </pages>
Reference-contexts: He also demonstrates how background knowledge can be incorporated, by generating a version space consistent with the knowledge, which can then be intersected with the version space being learned from the examples. This allows version spaces to be used for explanation-based learning <ref> (Mitchell et.al. 1986) </ref>, as well as for concept learning. It is important to note here that any two version spaces can be intersected. By proving this property, Hirsh allows us to distribute the task of inductive inference, as described earlier.
Reference: <author> R. J. Mooney and D. </author> <title> Ourston (1991). A Multistrategy Approach to Theory Refinement, </title> <booktitle> In Proceedings of the International Workshop on Multistrategy Learning, </booktitle> <pages> pages 115-130, </pages> <address> Harpers Ferry, WV. </address>
Reference-contexts: Given that this trivially requires an algorithm to modify a concept with respect to new examples, then this class can be thought of as abstractly including all incremental learning algorithms, e.g. ID5 (Utgoff, 1989) and theory refinement systems, e.g. EITHER <ref> (Mooney & Ourston, 1991) </ref>. The second class of systems integrate individual agents hypotheses into a single theory. One method is to simply order the hypotheses (Gams, 1989). Another method is for the agents to vote for the best hypothesis (Brazdil & Torgo, 1990).
Reference: <author> S. </author> <title> Muggleton (1992). Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, UK. </address>
Reference-contexts: The third point will be pursued by first taking the approach employed in LINUS (Lavrac and Dzeroski, 1994), and then examining the possibility of modifying an ILP algorithm such as GOLEM <ref> (Muggleton, 1992) </ref> to generate and use version spaces. Recent work by Nienhuys-Cheng and De Wolf (1996) suggests that certain classes of ILP problems are representable with boundary sets.
Reference: <author> S. H. Nienhuys-Cheng and R. </author> <title> De Wolf (1996). Least Generalizations and Greatest Specializations of Sets of Clauses, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <editor> 341-363 M. Pazzani and D. </editor> <title> Kibler (1992). The Utility of Knowledge in Inductive Learning, </title> <journal> Machine Learning, </journal> <volume> 9: </volume> <pages> 57-94. </pages>
Reference: <author> F. J. Provost and D. N. </author> <title> Hennessy (1995). Distributed Machine Learning: Scaling up with Coarse Grained Parallelism, </title> <booktitle> In Proceedings of the Second International Conference on Intelligent Systems for Molecular Biology (ISMB94), </booktitle> <publisher> AAAI Press, </publisher> <pages> pages 340-348, </pages> <address> Stanford, CA. </address>
Reference: <author> J. R. </author> <title> Quinlan (1986). Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <editor> 81-106 J. R. </editor> <title> Quinlan (1990). Learning Logical Definitions from Relations, </title> <journal> Machine Learning, </journal> 5:239-266 J. R. Quinlan (1994). The Minimum Description Length Principle and Categorical Theories, In Machine Learning, Proceedings of the 11th International Workshop (ML94), Morgan Kaufmann, pages 233-241, New Brunswick, NJ. 
Reference-contexts: This basis for this guess is the selection bias. If Occams razor is used, then we would select the shortest one, in which case is a randomly choice between p (X) or q (X). Traditionally, a supervised concept formation learning algorithm such as ID3 <ref> (Quinlan, 1986) </ref>, FOIL (Quinlan 1990), or FOCL (Pazzani, 1992), directly calculates a hypothesis, without considering G & S. However, by definition, even a directly calculated hypothesis has a version space. <p> Therefore to simulate an Occams Razor selection bias, we need only try combinations of Gs disjuncts. Note that we do not yet say that it is identical to the F that would be produced by an approach employing an Information Gain metric <ref> (Quinlan, 1986) </ref> or Minimum Description Length principle (Quinlan, 1994 ) Our agents basic inductive learning algorithm is as follows: 1. Initialise two sets P and N according to whether we are generating G, S, F from examples, or using G and S to generate F. 2.
Reference: <author> S. S. </author> <month> Sian </month> <year> (1991). </year> <title> Learning in Distributed Artificial Intelligence Systems, </title> <type> Ph.D. Thesis, </type> <institution> Imperial College, UK. </institution>
Reference-contexts: Another method is for the agents to vote for the best hypothesis (Brazdil & Torgo, 1990). The final class of system discussed by Brazdil et al. are hybrids, in which agents both refine and vote on hypotheses during the inductive process; see for example <ref> (Sian, 1991) </ref>. It should also be noted that agents may communicate knowledge induced by one agent which is then used as background knowledge for another learning goal; see, for example (Davies, 1993).
Reference: <author> B. Silver, W. Frawley, G. Iba, J. Vittal, and K. </author> <title> Bradford (1990). ILS: A Framework for Multi-Paradigmatic Learning, </title> <booktitle> In Machine Learning, Proceedings of the 7th International Workshop (ML90), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pages 348-356, </pages> <address> Austin, TX. </address>
Reference: <author> B. D. </author> <title> Smith (1995). Induction as Knowledge Integration, </title> <type> Ph.D. Thesis, </type> <institution> University of Southern California. </institution>
Reference-contexts: It is also used as the input to and output from an intersect performative. 5. &lt;selection-bias&gt;: It is not yet certain how we can represent this. It is generally a procedural bias, therefore we could just name a procedure (e.g. most general). However, it could also be specified declaratively <ref> (Smith, 1995) </ref>. These performatives and parameters are still under investigation, and will be more refined as we implement learning agents. 5. Discussion & Future Work This is very much work in progress, and to date we have not fully expanded all the details. Nor have we evaluated the approach empirically. <p> It is also perceivable that this approach can be specialised (for example, a neural network learning algorithm could send weight vectors that correspond to G and S), or can be generalised along the lines of <ref> (Smith 95) </ref> where all the biases are declarative. The specific concerns so far are: 1. The approach adopted so far involves an NP-Complete algorithm. 2. We have not investigated how to handle noise and uncertainty, nor have investigated how to accommodate shifts in bias (Des Jardins and Gordon, 1995). 3.
Reference: <author> V. </author> <month> Svatek </month> <year> (1994). </year> <title> Integration of Rules from Expert and Rules Discovered in Data, </title> <type> Unpublished Draft, </type> <institution> Prague University of Economics, </institution> <address> Czech Republic. </address>
Reference: <author> P. E. </author> <title> Utgoff (1989). Incremental Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 161-186. </pages>
Reference-contexts: Their simplest class of system is when distributed data-gathering agents communicate with a single learning agent. Given that this trivially requires an algorithm to modify a concept with respect to new examples, then this class can be thought of as abstractly including all incremental learning algorithms, e.g. ID5 <ref> (Utgoff, 1989) </ref> and theory refinement systems, e.g. EITHER (Mooney & Ourston, 1991). The second class of systems integrate individual agents hypotheses into a single theory. One method is to simply order the hypotheses (Gams, 1989).
References-found: 29

