URL: http://www.aic.nrl.navy.mil/papers/1994/AIC-94-026.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Email: aha@aic.nrl.navy.mil  bankert@nrlmry.navy.mil  
Title: A Comparative Evaluation of Sequential Feature Selection Algorithms  
Author: David W. Aha Richard L. Bankert 
Address: Washington, DC 20375  Monterey, CA 93943  
Affiliation: Navy AI Center Naval Research Laboratory  Marine Meteorology Division Naval Research Laboratory  
Abstract: Several recent machine learning publications demonstrate the utility of using feature selection algorithms in supervised learning tasks. Among these, sequential feature selection algorithms are receiving attention. The most frequently studied variants of these algorithms are forward and backward sequential selection. Many studies on supervised learning with sequential feature selection report applications of these algorithms, but do not consider variants of them that might be more appropriate for some performance tasks. This paper reports positive empirical results on such variants, and argues for their serious consideration in similar learning tasks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 1-10). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is well known that nearest neighbor algorithms perform poorly under such situations <ref> (e.g., Aha, 1992) </ref>. However, such non-parametric classifiers are excellent choices for evaluation functions since manual parameter-tuning is not feasible whenever a large number of (different) 1 Bankert (1994b) describes feature selection experiments with a variant of this dataset, but the differences in these datasets prevent a direct comparison. <p> While (queue is not empty) do: A. states = Select states (queue,m) B. evaluations = Evaluate states (S,E,states,n) C. queue = Update queue (states,evaluations,queue,q) D. best = Update best (best,queue) 4. Output: best feature subsets must be evaluated. Therefore, we chose IB1 <ref> (Aha, 1992) </ref> as the classifier; it is an implementation of the nearest neighbor classifier. The features used to describe each instance consist of shape, size, spectral, and textural measures for each cloud pattern region.
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <editor> In D. W. Aha (Ed.) </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: How this is best done remains an open issue. Sequential search algorithms have polynomial complexity (i.e., O (d 2 )); they add or subtract features and use a hill-climbing search strategy. We are investigating frequently used variants of these algorithms <ref> (Aha & Bankert, 1994) </ref>. The most common sequential search algorithms for feature selection are forward sequential selection (FSS) and backward sequential selection (BSS), and we focus on these algorithms in this paper. <p> This also suggests that BSS's performance can be enhanced in these situations when it is biased towards using small-sized subsets of features. We show evidence for this in <ref> (Aha & Bankert, 1994) </ref>.
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1995). </year> <title> A comparative evaluation of sequential feature selection algorithms. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics (pp. </booktitle> <pages> 1-7). </pages> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. </note>
Reference-contexts: Performance function The performance task studied in this paper is classification. Given the subset found to perform best by the evaluation function, the classifier is used to classify instances in the dataset. fl This 1995 AI & Statistics Workshop paper is <ref> (Aha & Bankert, 1995) </ref> in the references. Doak (1992) identified three categories of search algorithms: exponential, randomized, and sequen-tial.
Reference: <author> Almuallim, H., & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 547-552). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Bankert, R. L. </author> <year> (1994a). </year> <title> Cloud classification of AVHRR imagery in maritime regions using a probabilistic neural network. </title> <journal> Journal of Applied Meteorology, </journal> <volume> 33, </volume> <pages> 909-918. </pages>
Reference: <author> Bankert, R., L. </author> <year> (1994b). </year> <title> Cloud pattern identification as part of an automated image analysis. </title> <booktitle> Proceedings of the Seventh Conference on Satellite Meteorology and Oceanography (pp. </booktitle> <pages> 441-443). </pages> <address> Boston, MA: </address> <publisher> American Meteorological Society. </publisher>
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 28-36). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cover, T. M., & van Campenhout, J. M. </author> <year> (1977). </year> <title> On the possible orderings in the measurement selection problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 7, </volume> <pages> 657-661. </pages>
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security (Technical Report CSE-92-18). </title> <institution> Davis, CA: University of California, Department of Computer Science. </institution>
Reference: <author> Fu, K. S. </author> <year> (1968). </year> <title> Sequential methods in pattern recognition and machine learning. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 121-129). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> In Proceedings of the 1994 European Conference on Machine Learning (pp. </booktitle> <pages> 171-182). </pages> <address> Catania, Italy: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994). </year> <title> Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Milligan, G. W., & Cooper, M. C. </author> <year> (1985). </year> <title> An examination of procedures for determining the number of clusters in a data set. </title> <journal> Psychometrika, </journal> <volume> 50, </volume> <pages> 159-179. </pages>
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 190-198). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mucciardi, A. N., & Gose, E. E. </author> <year> (1971). </year> <title> A comparison of seven techniques for choosing subsets of pattern recognition properties. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 20, </volume> <pages> 1023-1031. </pages>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 293-301). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Townsend-Weber, T., & Kibler, D. </author> <year> (1994). </year> <title> Instance-based prediction of continuous values. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Vafaie, H., & De Jong, K. </author> <year> (1993). </year> <title> Robust feature selection algorithms. </title> <booktitle> In Proceedings of the Fifth Conference on Tools for Artificial Intelligence (pp. </booktitle> <pages> 356-363). </pages> <address> Boston, MA: </address> <publisher> IEEE Computer Society Press. </publisher>
References-found: 19

