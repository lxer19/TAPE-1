URL: http://www.cs.uni-bonn.de/II/ag-anlauf/papers/ferscha.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/II/ag-anlauf/seminar-eventsim.html
Root-URL: http://cs.uni-bonn.de
Email: Email: ferscha@ani.univie.ac.at Email: tripathi@cs.umd.edu  
Title: Parallel and Distributed Simulation of Discrete Event Systems  
Author: Alois Ferscha Satish K. Tripathi 
Address: Lenaugasse 2/8, A-1080 Vienna, AUSTRIA College Park, MD 20742, U.S.A.  
Affiliation: Institut fur Angewandte Informatik Computer Science Department Universitat Wien University of Maryland  
Abstract: The achievements attained in accelerating the simulation of the dynamics of complex discrete event systems using parallel or distributed multiprocessing environments are comprehensively presented. While parallel discrete event simulation (DES) governs the evolution of the system over simulated time in an iterative SIMD way, distributed DES tries to spatially decompose the event structure underlying the system, and executes event occurrences in spatial subregions by logical processes (LPs) usually assigned to different (physical) processing elements. Synchronization protocols are necessary in this approach to avoid timing inconsistencies and to guarantee the preservation of event causalities across LPs. Included in the survey are discussions on the sources and levels of parallelism, synchronous vs. asynchronous simulation and principles of LP simulation. In the context of conservative LP simulation (Chandy/Misra/Bryant) deadlock avoidance and deadlock detection/recovery strategies, Conservative Time Windows and the Carrier Nullmessage protocol are presented. Related to optimistic LP simulation (Time Warp), Optimistic Time Windows, memory management, GVT computation, probabilistic optimism control and adaptive schemes are investigated. CR Categories and Subject Descriptors: C.1.0 [Processor Architectures:] General; C.2 [Computer Communication Networks:] Distributed Systems | Distributed Applications; C.4 [Computer Systems Organization:] Performance of Systems | Modeling techniques; D.4.1 [Operating Systems:] Process Management | Concur-rency, Deadlocks, Synchronization; I.6.0 [Simulation and Modeling:] General; I.6.8 [Simulation and Modeling:] Types of Simulation Distributed, Parallel General Terms: Algorithms, Performance Additional Key Words and Phrases: Parallel Simulation, Distributed Simulation, Conservative Simulation, Optimistic Simulation, Synchronization Protocols, Memory Management fl This work was conducted while Alois Ferscha was visiting the University of Maryland, Computer Science Department, supported by a grant from the Academic Senate of the University of Vienna.
Abstract-found: 1
Intro-found: 1
Reference: [Akyi 93] <author> I. F. Akyildiz, L. Chen, R. Das, R. M. Fujimoto, and R. F. Serfozo. </author> <title> "The Effect of Memory Capacity on Time Warp Performance". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 18, No. 4, </volume> <pages> pp. 411-422, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: An obvious implementation of Cancelback is therefore for shared memory environments and making use of system level interrupts. A Markov chain model of Cancelback <ref> [Akyi 93] </ref> predicting speedup as the amount of available memory beyond M seq is varied, revealed that even with small fractions of additional memory the protocol performs about as well as with unlimited memory. <p> Analytical performance models of Time Warp with Cancelback <ref> [Akyi 93] </ref> for homogeneous (artificial) workloads have shown that at a certain amount of available free memory fossil collection is sufficient to allocate enough memory. With a decreasing amount of available memory, absolute execution performance decreases due to more frequent cancelbacks until it becomes frozen at some point. <p> Optimistic Protocols? The question on the relative qualities of conservative and optimistic protocols has often been raised. General rules of superiority cannot be formulated, since performance due to a very high degree of interweaving of influencing factors cannot be sufficiently characterized by models, although exceptions do exist <ref> [Akyi 93] </ref>. Even full implementations often prohibit performance comparisons if different implementation strategies were followed (a performance comparable implementation design is worked out in [Chio 93c]) or different traget platforms were selected.
Reference: [Bagr 91] <author> R. Bagrodia, K. M. Chandy, and W. T. Liao. </author> <title> "A Unifying Framework for Distributed Simulation". </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 4, </volume> <pages> pp. 348-385, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Opposed to the approaches that "virtualize" time as presented in this work, the "virtualization" of space ambitiously studied at this time [Luba 93] promises a shift of conventional parallel and distributed simulation paradigms. Further challenges are seen in the hierarchical combination [Raja 93] or even the uniformization of protocols <ref> [Bagr 91] </ref>, the uniformization of continuous and discrete event simulation, the integration of real time constraints into protocols (distributed interactive simulation), etc.
Reference: [Bagr 94] <author> R. L. Bagrodia, V. Jha, and J. Waldorf. </author> <title> "The Maisie Environment for Parallel Simulation". </title> <booktitle> In: Proc. of the 27 th Annual Simulation Symposium, </booktitle> <address> La Jolla, California, </address> <month> April 11-15, </month> <year> 1994, </year> <pages> pp. 4 - 12, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1994. </year>
Reference-contexts: Issues with more practical relevance [Fuji 93] are the design of simulation languages and the development of tools to support a simulation model description independently of the sequential, parallel or distributed DES algorithm or protocol to execute it <ref> [Bagr 94] </ref>, and to automate the parallelization process as far as possible.
Reference: [Baik 85] <author> D. Baik and B. P. Zeigler. </author> <title> "Perfromance Evaluation of Hierarchical Distributed Simulators". </title> <booktitle> In: Proc. of the 1985 Winter Simulation Conference, </booktitle> <pages> pp. 421 - 427, </pages> <publisher> SCS, </publisher> <year> 1985. </year>
Reference-contexts: For a distributed implementation of a global clock [Peac 79], a structured (hierarchical) LP organization can be used [Conc 85] to determine the minimum next event time. A parallel min-reduction operation can bring this timestamp to the root of a process tree <ref> [Baik 85] </ref>, which can then be propagated down the tree. Another possibility is to apply a distributed snapshot algorithm [Chan 85] in order to avoid the bottleneck of a centralized global clock coordinator. Combinations of synchronous LP simulation with event-driven global clock progression have also been studied.
Reference: [Bain 88] <author> W. L. Bain and D. S. Scott. </author> <title> "An algorithm for time synchronization in distributed discrete event simulation". </title> <editor> In: B. Unger and D. Jefferson, Eds., </editor> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19 (3), </volume> <pages> pp. 30-33, </pages> <publisher> SCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: To recover from deadlock, this LP is 17 invoked to process its first event. Obviously message lengths in this algorithm grow proportionally to the number of nodes in GLP. Bain and Scott <ref> [Bain 88] </ref> propose an algorithm for demand driven deadlock free synchronization in conservative LP simulation that avoids message lengths to grow with the size of GLP.
Reference: [Ball 90] <author> D. Ball and S. Hoyt. </author> <title> "The Adaptive Time-Warp Concurrency Control Algorithm". </title> <editor> In: D. Nicol, Ed., </editor> <booktitle> Distributed Simulation. Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pp. 174 - 177, </pages> <institution> Society for Computer Simulation, </institution> <address> San Diego, California, </address> <year> 1990. </year> <booktitle> Simulation Series, </booktitle> <volume> Volume 22, Number 1. </volume>
Reference-contexts: Furthermore, a natural difficulty is the determination of the admitting enough events to make the simulation efficient. The latter is addressed with the adaptive Time Warp concurrency control algorithm (ATW) proposed by Ball and Hyot <ref> [Ball 90] </ref>, allowing the window size (t) be adapted at any point t in simulation time. ATW aims to temporarily suspend event processing if it has observed a certain amount of lcc violations in the past.
Reference: [Bell 90] <author> S. Bellenot. </author> <title> "Global virtual time algorithms". </title> <booktitle> In: Proceedings of the Multiconference on Distributed Simulation., </booktitle> <pages> pp. 122 - 127, </pages> <year> 1990. </year>
Reference-contexts: LP j receiving smallest sequence numbers from other LPs can determine the messages still in transit and compute a lower bound on their timestamps. To reduce communication complexity, Bellenot's algorithm <ref> [Bell 90] </ref> embeds GLP in a Message Routing Graph MRG, which is mainly a composition of two binary trees with arcs interconnecting their leaves. <p> The MRG for a GLP with N = 10 LPs e.g. would be a three level binary tree mirrored along its four node leaf base (a MRG construction procedure for arbitrary N is given in <ref> [Bell 90] </ref>). The algorithm efficiently utilizes the static MRG topology and operates in three steps: (1) (MRG forward phase) LP 0 (GVT manager) sends a GVT-start to the (one or) two successor LPs on the MRG.
Reference: [Broc 91] <author> P. J. Brockwell and R. A. Davis. </author> <title> Time Series: Theory and Methods. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Autoregressive moving average (ARMA) process models <ref> [Broc 91] </ref> are a reasonable method to characterize that process by the relationship among a series of empirical non-independent observations fX i g = (X 1 ; X 2 ; : : :X n ) (in our case = (ffi 1 ; ffi 2 ; : : :ffi n )). <p> X t is usually called the centered response, and OE i are parameters that can be be estimated from the realizations in various different ways <ref> [Broc 91] </ref>, e.g. maximum likelihood or the (recursive) Yule-Walker method.
Reference: [Brya 84] <author> R. E. Bryant. </author> <title> "A Switch-Level Model and Simulator for MOS Digital Systems". </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-33, No. 2, </volume> <pages> pp. 160-177, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: in real implementations on certain platforms, but the results might help to rank the two approaches at least from a statistical viewpoint. 2 "Classical" LP Simulation Protocols 2.1 Conservative Logical Processes LP simulations following a conservative strategy date back to original works by Chandy and Misra [Chan 79] and Bryant <ref> [Brya 84] </ref>, and are often referred to as the Chandy-Misra-Bryant (CMB) protocols.
Reference: [Cai 90] <author> W. Cai and S. J. Turner. </author> <title> "An Algorithm for Distributed Discrete-Event Simulation - The `Carrier Null Message' Approach". </title> <booktitle> In: Proceedings of the SCS Multiconference on Distributed Simulation Vol. </booktitle> <volume> 22 (1), </volume> <pages> pp. 3-8, </pages> <publisher> SCS, </publisher> <month> January </month> <year> 1990. </year>
Reference-contexts: An approach where additional information (essentially the routing path as observed during traversal) is attached to the nullmessage, the carrier nullmessage protocol <ref> [Cai 90] </ref> will be investigated in more detail later. One problem that still remains with conservative LPs is the determination of when it is safe to process an event. <p> The carrier null message protocol <ref> [Cai 90] </ref> uses nullmessages to advance CC [i]'s and acquire/propagate knowledge global to the participating LPs, with the goal of improving the ability of lookahead to reduce the message traffic.
Reference: [Chan 79] <author> K. M. Chandy and J. Misra. </author> <title> "Distributed Simulation: A Case Study in Design and Verfification of Distributed Programs". </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-5, No. 5, </volume> <pages> pp. 440-452, </pages> <month> Sep. </month> <year> 1979. </year>
Reference-contexts: what would be observed in real implementations on certain platforms, but the results might help to rank the two approaches at least from a statistical viewpoint. 2 "Classical" LP Simulation Protocols 2.1 Conservative Logical Processes LP simulations following a conservative strategy date back to original works by Chandy and Misra <ref> [Chan 79] </ref> and Bryant [Brya 84], and are often referred to as the Chandy-Misra-Bryant (CMB) protocols.
Reference: [Chan 81] <author> K. M. Chandy and J. Misra. </author> <title> "Asynchronous Distributed Simulation via a Sequence of Parallel Computations". </title> <journal> Communications ACM, </journal> <volume> Vol. 24, No. 11, </volume> <pages> pp. 198-206, </pages> <month> Nov. </month> <year> 1981. </year>
Reference-contexts: In the diagram in Figure 8, this is at points where the arrow denoting a token move from T1 (T2) to T2 (T1) has the opposite direction that the previous one. 2.1.3 Deadlock Detection/Recovery An alternative to the Chandy-Misra-Bryant protocol avoiding nullmessages has also been proposed by Chandy and Misra <ref> [Chan 81] </ref>, allowing deadlocks to occur, but providing a mechanism to detect it and recover from it. Their algorithm runs in two phases: (i) parallel phase, in which the simulation runs until it deadlocks, and (ii) phase interface, which initiates a computation allowing some LP to advance LVT.
Reference: [Chan 83] <author> K. M. Chandy, J. Misra, and L. M. Haas. </author> <title> "Distributed Deadlock Detection". </title> <journal> ACM Transactions On Computer Systems, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 144-156, </pages> <month> May </month> <year> 1983. </year> <month> 47 </month>
Reference-contexts: A central controller is assumed in their algorithm, thus violating a distributed computing principle. To avoid a single resource (controller) to become a communication performance bottleneck during deadlock detection, any general distributed termination detection algorithm [Matt 87] or distributed deadlock detection algorithm <ref> [Chan 83] </ref> could be used instead. In an algorithm described by Misra [Misr 86], a special message called marker circulates through GLP to detect and correct deadlock. A cyclic path for traversing all ch i;j 2 CH is precomputed and LPs are initially colored white.
Reference: [Chan 85] <author> K. M. Chandy and J. Lamport. </author> <title> "Distributed Snapshots: Determining Global States of Distributed Systems.". </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 63-75, </pages> <year> 1985. </year>
Reference-contexts: A parallel min-reduction operation can bring this timestamp to the root of a process tree [Baik 85], which can then be propagated down the tree. Another possibility is to apply a distributed snapshot algorithm <ref> [Chan 85] </ref> in order to avoid the bottleneck of a centralized global clock coordinator. Combinations of synchronous LP simulation with event-driven global clock progression have also been studied. <p> Distributed GVT Computation A distributed GVT estimation procedure does not rely on the availability of common memory shared among LPs, neither is a centralized GVT manager required. Although distributed snapshot algorithms <ref> [Chan 85] </ref> find a straightforward application, solutions more efficient than message ackowledging, the delaying of sending event messages while awaiting control messages or piggybacking control information onto event messages are desired.
Reference: [Chio 93a] <author> G. Chiola and A. Ferscha. </author> <title> "Distributed Simulation of Petri Nets". </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> Vol. 1, No. 3, </volume> <pages> pp. 33 - 50, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The latter is closely related to the problem of partitioning the simulation model into regions [Chio 93b] which can be conducted at least semi-automatically if model specifications are made in a formalisms abstract enough to support a structural analysis (e.g. Petri Nets) <ref> [Chio 93a, Fers 94b, Fers 94a] </ref>. The management and balancing of dynamic distributed simulation workloads is becoming more and more important with the shift from parallel processors and supercomputers to distributed computing environments (powerful workstations interconnected with high speed networks and switches) as the preferred target architecture.
Reference: [Chio 93b] <author> G. Chiola and A. Ferscha. </author> <title> "Distributed Simulation of Timed Petri Nets: Exploiting the Net Structure to Obtain Efficiency". </title> <editor> In: M. Ajmone Marsan, Ed., </editor> <booktitle> Proc. of the 14 th Int. Conf. on Application and Theory of Petri Nets 1993, </booktitle> <address> Chicago, </address> <month> June </month> <year> 1993, </year> <pages> pp. 146 - 165, </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: The latter is closely related to the problem of partitioning the simulation model into regions <ref> [Chio 93b] </ref> which can be conducted at least semi-automatically if model specifications are made in a formalisms abstract enough to support a structural analysis (e.g. Petri Nets) [Chio 93a, Fers 94b, Fers 94a].
Reference: [Chio 93c] <author> G. Chiola and A. Ferscha. </author> <title> "Performance Comparable Implementation Design of Synchronization Protcols for Distributed Simulation". </title> <type> Tech. Rep. </type> <institution> ACPC-TR 93-20, Aus-trian Center for Parallel Computation, </institution> <year> 1993. </year>
Reference-contexts: Even full implementations often prohibit performance comparisons if different implementation strategies were followed (a performance comparable implementation design is worked out in <ref> [Chio 93c] </ref>) or different traget platforms were selected.
Reference: [Conc 85] <author> A. I. </author> <title> Concepcion. "Mapping Distributed Simulators onto the Hierarchical Multibus Multiprocessor Architecture". </title> <editor> In: P. Reynolds, Ed., </editor> <booktitle> Proc. of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pp. 8-13, </pages> <institution> Society for Computer Simulation, </institution> <year> 1985. </year>
Reference-contexts: Once the minimum timestamp of possible next external events is determined, the global clock can be advanced by (S), i.e. an amount which depends on the particular state S. For a distributed implementation of a global clock [Peac 79], a structured (hierarchical) LP organization can be used <ref> [Conc 85] </ref> to determine the minimum next event time. A parallel min-reduction operation can bring this timestamp to the root of a process tree [Baik 85], which can then be propagated down the tree.
Reference: [Das 94] <author> S. R. Das and R. M. Fujimoto. </author> <title> "An Adaptive Memory Management Protocol for Time Warp Parallel Simulation". </title> <booktitle> In: Proc. of the 1994 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Nashville, </address> <year> 1994, </year> <pages> pp. 201-210, </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: Adaptive Memory Management The adaptive memory management (AMM) scheme proposed by Das and Fujimoto <ref> [Das 94] </ref> attempts a combination of controling optimism in Time Warp and an automatic adjustment of the amount of memory in order to optimize fossil collection, Can-celback and rollback overheads.
Reference: [Dick 90] <author> P. M. Dickens and P. F. Reynolds. </author> <title> "SRADS with Local Rollback". </title> <booktitle> In: Proceedings of the SCS Multiconference on Distributed Simulation Vol. </booktitle> <volume> 22 (1), </volume> <pages> pp. 161-164, </pages> <publisher> SCS, </publisher> <month> January </month> <year> 1990. </year>
Reference-contexts: Causality violations can only affect the time period in between the window edge and the lag bound, thus limiting (the relative) length of rollback chains. The SRADS protocol by Dickens and Reynolds <ref> [Dick 90] </ref>, although allowing optimistic simulation progression, prohibits the propagation of uncommitted events to other LPs. Therefore, rollback can only be local to some LP and cascades of rollback can never occur. <p> BTB is an optimistic windowing mechanism with a pessimistic message sendout policy to avoid the necessity of any antimessage by restricting potential rollback to affect only local history records (as in SRADS <ref> [Dick 90] </ref>). BTB basically processes events in time buckets of different size as determined by the event horizon (Figure 16). Each bucket contains the maximum amount of causally independent events which can be executed concurrently.
Reference: [Dick 94] <author> P. M. Dickens, P. Heidelberger, and D. M. Nicol. </author> <title> "Parallelized Direct Execution Simulation of Message-Passing Parallel Programs". </title> <type> Tech. Rep., </type> <institution> ICASE, NASA Langley Research Center, Hampton, VA, </institution> <year> 1994. </year> <note> submitted for publication. </note>
Reference-contexts: For example, simulated executions of SIMD programs in asynchronous environments can accelerate their execution [Shen 92], and parallel simulations executing parallel programs with message passing communication have already been shown to be possible <ref> [Dick 94] </ref>. Other work has shown that an intrusion free monitoring and trace collection of distributed memory parallel program executions is possible by superimposing the execution with a distributed DES protocol [Turn 93].
Reference: [DSou 94] <author> L. M. D'Souza, X. Fan, and P. A. Wilsey. "pGVT: </author> <title> An Algorithm for Accurate GVT Estimation". </title> <editor> In: , Ed., </editor> <booktitle> Proc. of the 8 th Workshop on Parallel and Distributed Simulation, </booktitle> <address> p. </address> , <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: Bellenot's algorithm sends less than 4N messages and uses overall O (log (N )) time per GVT prediction epoch after an O (log (N )) time for the initial MRG embedding. It requires a FIFO, fault free CS. The passive response GVT (pGVT) algorithm <ref> [DSou 94] </ref> copes with faulty communication channels, while at the same time relaxing (i) the FIFO requirement to CS and (ii) the "centralized invocation" of the GVT computation.
Reference: [Feld 90] <author> R. E. Felderman and L. Kleinrock. </author> <title> "An Upper Bound on the Improvement of Asynchronous versus Synchronous Distributed Processing". </title> <editor> In: D. Nicol, Ed., </editor> <booktitle> Proc. of the SCS Multiconf. on Dist. Sim., </booktitle> <pages> pp. 131 - 136, </pages> <month> Jan </month> <year> 1990. </year>
Reference-contexts: The variety of mechanisms around these schemes will be the main body of this review. In a comparison of synchronous and asynchronous LP simulation schemes it has been shown <ref> [Feld 90] </ref>, that the potential performance improvement of an asynchronous LP simulation strategy over the time-stepped variant is at most O (logP ), P being the number of LPs executing concurrently on independent processors.
Reference: [Fers 94a] <author> A. Ferscha. </author> <title> "Concurrent Execution of Time Petri Nets". </title> <editor> In: J. D. Tew and S. Mani-vannan, Eds., </editor> <booktitle> Proceedings of the 1994 Winter Simulation Conference, </booktitle> <year> 1994. </year>
Reference-contexts: The latter is closely related to the problem of partitioning the simulation model into regions [Chio 93b] which can be conducted at least semi-automatically if model specifications are made in a formalisms abstract enough to support a structural analysis (e.g. Petri Nets) <ref> [Chio 93a, Fers 94b, Fers 94a] </ref>. The management and balancing of dynamic distributed simulation workloads is becoming more and more important with the shift from parallel processors and supercomputers to distributed computing environments (powerful workstations interconnected with high speed networks and switches) as the preferred target architecture.
Reference: [Fers 94b] <author> A. Ferscha and G. Chiola. </author> <title> "Accelerating the Evaluation of Parallel Program Performance Models using Distributed Simulation". </title> <booktitle> In: Proc. of. the 7 th Int. Conf. on Mod-elling Techniques and Tools for Computer Performance Evaluation., </booktitle> <year> 1994. </year> <month> 48 </month>
Reference-contexts: The latter is closely related to the problem of partitioning the simulation model into regions [Chio 93b] which can be conducted at least semi-automatically if model specifications are made in a formalisms abstract enough to support a structural analysis (e.g. Petri Nets) <ref> [Chio 93a, Fers 94b, Fers 94a] </ref>. The management and balancing of dynamic distributed simulation workloads is becoming more and more important with the shift from parallel processors and supercomputers to distributed computing environments (powerful workstations interconnected with high speed networks and switches) as the preferred target architecture.
Reference: [Fers 94c] <author> A. Ferscha and G. Chiola. </author> <title> "Self-Adaptive Logical Processes: the Probabilistic Dis--tributed Simulation Protocol". </title> <booktitle> In: Proc. of the 27 th Annual Simulation Symposium, LaJolla, 1994, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: h 1; P2; 0.37 i 0.56 1 T2@0.79 | Table 4: Motivation for Probabilistic LP Simulation 2.2.10 Probabilistic Optimism A communication interface CI that considers the CMB protocol and Time Warp as two extremes in a spectrum of possibilities to synchronize LP's LVT advancements is the probabilistic distributed DES protocol <ref> [Fers 94c] </ref>.
Reference: [Fuji 90] <author> R. M. Fujimoto. </author> <title> "Parallel Discrete Event Simulation". </title> <journal> Communications of the ACM, </journal> <volume> Vol. 33, No. 10, </volume> <pages> pp. 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: It is intuitively convincing and has been shown in [Misr 86] that no causality error can ever occur in an asynchronous LP simulation if and only if every LP adheres to processing events in nondecreasing timestamp order only (local causality constraint (lcc) as formulated in <ref> [Fuji 90] </ref>). Although sufficient, it is not always necessary to obey the lcc, because two events occuring within one and the same LP may be concurrent (independent of each other) and could thus be processed in any order. <p> A serious liability of this optimization is again additional memory and bookkeeping overhead, but also (and mainly) the considerable complication of the Time Warp code <ref> [Fuji 90] </ref>. To verify equivalence of IQ's the protocol must draw and log copies of the IQ in every state saving step (S3.7 ). In a weaker lazy re-evaluation strategy one could allow jumping forward only if no message has arrived since rollback. <p> The difficult problem of debugging parallel programs finds a high chance to be tackled by similar ideas. 45 4 Sources of Literature and Further Reading Comprehensive overviews on the field of parallel and distributed simulation are the surveys by Richter and Walrand [Rich 89], Fujimoto <ref> [Fuji 90] </ref>, and most recently Nicol and Fujimoto [Nico 94]. The primary reading for Time Warp is [Jeff 85a], for conservative protocols it is [Misr 86].
Reference: [Fuji 93] <author> R. M. Fujimoto. </author> <title> "Parallel Discrete Event Simulation: Will the Field Survive?". </title> <journal> ORSA Journal of Computing, </journal> <volume> Vol. 5, No. 3, </volume> <pages> pp. 218-230, </pages> <year> 1993. </year>
Reference-contexts: Issues with more practical relevance <ref> [Fuji 93] </ref> are the design of simulation languages and the development of tools to support a simulation model description independently of the sequential, parallel or distributed DES algorithm or protocol to execute it [Bagr 94], and to automate the parallelization process as far as possible.
Reference: [Gafn 88a] <author> A. Gafni. </author> <title> "Rollback Mechanisms for Optimistic Distributed Simulation Systems". </title> <booktitle> In: Proc. of the SCS Multiconference on Distributed Simulation 19, </booktitle> <pages> pp. 61-67, </pages> <year> 1988. </year>
Reference-contexts: Instead, it delays its propagation until the resimulation after rollback has progressed to LVT = ts (m + ) producing m + 0 6= m + . If the resimulation produced m + 0 = m + , no antimessage has to be sent all <ref> [Gafn 88a] </ref>. Lazy cancellation thus avoids unnecessary cancelling of correct messages, but has the liability of additional memory and bookkeeping overhead (potential antimessages must be maintained in a rollback queue) and delaying the annihilation of actually wrong simulations.
Reference: [Gafn 88b] <author> A. Gafni. </author> <title> "Rollback Mechanisms for Optimistic Distributed Simulation Systems". </title> <editor> In: B. Unger and D. Jefferson, Eds., </editor> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19 (3), </volume> <pages> pp. 61-67, </pages> <publisher> SCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: An indirect effect of the sendback could also be storage release in remote LPs due to annihilation of messages triggered by the original sender's rollback procedure. Gafni's Protocol In a message traffic study of aggressive and lazy cancellation, Gafni <ref> [Gafn 88b] </ref> notes that past (RT (m) &lt; GVT) and present messages (ST (m) &lt; GVT &lt; RT (m)) and events 29 accumulate in IQ, OQ, SS for the two annihilation mechanisms at the same rate, pointing out also the interweaving of messages and events in memory consumption.
Reference: [Gros 88] <author> B. Groselj and C. Tropper. </author> <title> "The time-of-next-event algorithm". </title> <editor> In: B. Unger and D. Jefferson, Eds., </editor> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19 (3), </volume> <pages> pp. 25-29, </pages> <publisher> SCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: If no was received in an LP initiating a request, the LP has to restart the time request with lower channel clocks. The time-of-next-event algorithm as proposed by Groselj and Tropper <ref> [Gros 88] </ref> assumes more than one LP mapped onto a single physical processor, and computes the greatest lower bound of the timestamps of the event messages expected to arrive next at all empty links on the LPs located at that processor.
Reference: [Jeff 85a] <author> D. A. Jefferson. </author> <title> "Virtual Time". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 7, No. 3, </volume> <pages> pp. 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: The critical problem, however, which asynchronous LP simulation poses is the chance of causality errors. Indeed, an asynchronous LP simulation insures correctness if the (total) event ordering as produced by a sequential DES is consistent with the (partial) event ordering as generated by the distributed execution. Jefferson <ref> [Jeff 85a] </ref> recognized this problem to be the inverse of Lamport's logical clock problem [Lamp 78], i.e. providing clock values for events occuring in a distributed system such that all events appear ordered in logical time. <p> possible, without warranty that the set of generated (internal and external) events is consistent with lcc, and regardless to the possibility of the arrival of an external event with a timestamp in the local past. 2.2.1 Time Warp Pioneering work in optimistic LP simulation was done by Jefferson and Sowizral <ref> [Jeff 85b, Jeff 85a] </ref> in the definition of the Time Warp (TW) mechanism, which like the Chandy-Misra-Bryant protocol uses the sending of messages for synchronization. Time Warp employs a rollback (in time) mechanism to take care of proper synchronization with respect to lcc. <p> completely allocated, only additional techniques, mostly based on returning messages to senders or artificially initiating rollback, can help to escape from deadlocks due to waiting for free memory: Message Sendback The first approach to recover from memory overflow in Time Warp was proposed by the message sendback mechanism by Jefferson <ref> [Jeff 85a] </ref>. <p> The primary reading for Time Warp is <ref> [Jeff 85a] </ref>, for conservative protocols it is [Misr 86]. The most relevant literature appears in the frame of the * Workshop on Parallel and Distributed Simulation (PADS), formerly (while being held as part of the SCS Multiconference) published as the Proceedings of the SCS Multiconference on Distributed Simulation.
Reference: [Jeff 85b] <author> D. Jefferson and H. Sowizral. </author> <title> "Fast Concurrent Simulation Using the Time Warp Mechanism". </title> <editor> In: P. Reynolds, Ed., </editor> <booktitle> Distributed Simulation 1985, </booktitle> <pages> pp. 63-69, </pages> <institution> SCS-The Society for Computer Simulation, Simulation Councils, Inc., La Jolla, California, </institution> <year> 1985. </year>
Reference-contexts: possible, without warranty that the set of generated (internal and external) events is consistent with lcc, and regardless to the possibility of the arrival of an external event with a timestamp in the local past. 2.2.1 Time Warp Pioneering work in optimistic LP simulation was done by Jefferson and Sowizral <ref> [Jeff 85b, Jeff 85a] </ref> in the definition of the Time Warp (TW) mechanism, which like the Chandy-Misra-Bryant protocol uses the sending of messages for synchronization. Time Warp employs a rollback (in time) mechanism to take care of proper synchronization with respect to lcc.
Reference: [Jeff 90] <author> D. Jefferson. </author> <title> "Virtual Time II: the cancelback protocol for storage management in Time Warp". </title> <booktitle> In: Proc. of the 9th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 75-90, </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Cancelback is the first optimal memory management protocol <ref> [Jeff 90] </ref>, and was developed targeting Time Warp implementations on shared memory systems. As opposed to Gafni's protocol, in Cancelback elements can be canceled in any LP i (not necessarily in the one that observed memory overflow), whereas the element selection scheme is the same.
Reference: [Jeff 91] <author> D. Jefferson and P. Reiher. </author> <title> "Supercritical Speedup". </title> <editor> In: A. H. Rutan, Ed., </editor> <booktitle> Proceedings of the 24 th Annual Simulation Symposium, </booktitle> <address> New Orleans, Louisiana, USA, </address> <month> April 1-5, </month> <year> 1991., </year> <pages> pp. 159-168, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Message annihilation and rollback can be avoided due to the exploitation of the lookahead from the next random variate in the future list, 0.39. The effect of the straggler is in the future of LP 2 (0.56). It has been shown <ref> [Jeff 91] </ref> that Time Warp with lazy cancellation can produce so called "super-critical speedup", i.e. surpass the simulations critical path by the chance of having wrong compu 24 tations produce correct results. By immediately discarding rolled back computations this chance is lost for the aggressive cancellation policy. <p> Indeed, it has been shown that the length of the critical path is a lower bound on the execution time of any conservative protocol, but some optimistic protocols do exist (Time Warp with lazy cancellation, Time Warp with lazy rollback, Time Warp with phase decomposition, and the Chandy-Sherman Space-Time Method <ref> [Jeff 91] </ref>, which can surpass the critical path.
Reference: [Lamp 78] <author> L. Lamport. </author> <title> "Time, clocks, and the ordering of events in distributed systems". </title> <journal> Communications of the ACM, </journal> <volume> Vol. 21, No. 7, </volume> <pages> pp. 558 - 565, </pages> <month> Jul </month> <year> 1978. </year>
Reference-contexts: Indeed, an asynchronous LP simulation insures correctness if the (total) event ordering as produced by a sequential DES is consistent with the (partial) event ordering as generated by the distributed execution. Jefferson [Jeff 85a] recognized this problem to be the inverse of Lamport's logical clock problem <ref> [Lamp 78] </ref>, i.e. providing clock values for events occuring in a distributed system such that all events appear ordered in logical time. <p> Let further be `!' Lamport's happens before relation <ref> [Lamp 78] </ref> defining a partial ordering of e 2 E as follows: (1) if e; e 0 2 IE EE and e 0 is the next after e, then e ! e 0 , (2) if e 2 SE and e 0 2 RE is the corresponding receive event, the e
Reference: [Lin 90] <author> Y.-B. Lin and E. Lazowska. </author> <title> "Determining the Global Virtual Time in a Distributed Simulation". </title> <booktitle> In: 1990 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> III-201-III-209, </address> <year> 1990. </year>
Reference-contexts: The resulting possibility of so called supercritical speedup, and as a consequence its nonsuitability as an absolute lower bound reference, however, has made critical path less attractive. 33 An improvement of Samadi's algorithm by Lin and Lazowska <ref> [Lin 90] </ref> does not acknowledge every single message. Instead, to every message a sequence number is piggybacked, such that LP i can identify missing messages as gaps in the arriving sequence numbers.
Reference: [Lin 91] <author> Y.-B. Lin and B. R. Preiss. </author> <title> "Optimal Memory Management for Time Warp Parallel Simulation". </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 4, </volume> <pages> pp. 283-307, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Time Warp always consumes more memory than sequential simulation <ref> [Lin 91] </ref>, and a memory limitation imposes a performance decrease on Time Warp: providing just the minimum of memory necessary 26 may cause the protocol to execute fairly slow, such that the memory/performance tradeoff becomes an issue. <p> An obvious consequence is that any processed event e with ts (e) &lt; GVT (T ) can never (at no instant T ) be rolled back, and can therefore be considered as (irrevocably) committed <ref> [Lin 91] </ref> (Figure 13). <p> So, given a certain minimum but limited amount of memory, both protocols make Time Warp "operable". Cancelback An LP simulation memory management scheme is considered to be storage optimal iff it consumes O (M seq ) constant bounded memory <ref> [Lin 91] </ref>. The worst case space complexity of Gafni's protocol is O (N M seq ) = O (N 2 ) (irrespective of whether memory is shared or distributed), the reason for this being that it can only cancel elements within the individual LPs. <p> Lin <ref> [Lin 91] </ref> describes a Time Warp management scheme that is in turn memory optimal (there exists a shared memory implementation of Time Warp with space complexity O (M seq ) 1 ), but has a simpler implementation. <p> In the implementation proposed by Lin and Preiss <ref> [Lin 91] </ref>, the two other issues are coupled to a processor scheduling policy in order to guarantee a certain amount of free memory (called salvage parameter in [Nico 94]), while following the "cancel-furthest-ahead" principle.
Reference: [Lin 93] <author> Y.-B. Lin, B. Preiss, W. Loucks, and E. Lazowska. </author> <title> "Selecting the Checkpoint Interval in Time Warp Simulation". </title> <editor> In: R. Bagrodia and D. Jefferson, Eds., </editor> <booktitle> Proc. of the 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pp. 3-10, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year> <month> 49 </month>
Reference-contexts: In a study of the optimal checkpointing interval explicitly considering state saving and restoration costs while assuming does neither affect the number of rollbacks nor the number of rolled back events in <ref> [Lin 93] </ref>, an algorithm is developed that, integrated into the protocol, "on the-fly", within a few iterations, automatically adjusts to fl .
Reference: [Luba 88] <author> B. D. Lubachevsky. </author> <title> "Bounded lag distributed discrete event simulation". </title> <editor> In: B. Unger and D. Jefferson, Eds., </editor> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> 19 (3), </volume> <pages> pp. 183-191, </pages> <publisher> SCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: Combinations of synchronous LP simulation with event-driven global clock progression have also been studied. Although the global clock is advanced to the minimum next event time as in the event driven scheme, LPs are only allowed to simulate within a -tick of time, called a bounded lag by Lubachevsky <ref> [Luba 88] </ref> or a Moving Time Window by [Soko 88]. 1.5.2 Asynchronous LP Simulation Asynchronous LP simulation relies on the presence of events occuring at different simulated times that do not affect one another. Concurrent processing of those events thus effectively accelerates sequential simulation execution time. <p> in multiprocessors is to introduce a window W i in simulated time for each LP i , such that events within this time window are safe (events in W i are independent of events in W j , i 6= j) and can be processed concurrently across all LP i <ref> [Luba 88] </ref>, [Nico 91]. A conservative time window (CTW) parallel LP simulation synchronously operates in two phases. <p> In this case the corresponding LPs would idle for that cycle. A considerable overhead can be imposed on the algorithm by the identification of when it is safe to process an event within LP i (window identification phase). Lubachevsy <ref> [Luba 88] </ref> proposes to reduce the complexity of this operation by restricting the lag on the LP simulation, i.e. the difference in occurrence time of events being processed concurrently is bounded from above by a know finite constant (bounded lag protocol).
Reference: [Luba 91] <author> B. Lubachevsky, A. Weiss, and A. Shwartz. </author> <title> "An Analysis of Rollback-Based Simulation". </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 154-193, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This information allows LPs to filter out messages based on preempted (obsolete) states to be eventually annihilated by chasing antimessages currently in transit. Related to the (conservative) bounded lag algorithm, Lubachevsky, Shwartz and Weiss have developed a filtered rollback protocol <ref> [Luba 91] </ref> that allows optimistically crossing the lag bound, but only up to a time window upper edge. Causality violations can only affect the time period in between the window edge and the lag bound, thus limiting (the relative) length of rollback chains.
Reference: [Luba 93] <author> B. D. Lubachevsky. </author> <title> "Relaxation for Massively Parallel Discrete Event Simulation". </title> <editor> In: L. Donatiello and R. Nelson, Eds., </editor> <title> Performance Evaluation of Computer and Communication Systems, </title> <journal> pp. </journal> <volume> 307 - 329, </volume> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: In this context, protocols with the possibility of dynamic LP creation and migration (dynamic rescheduling) will have to be developed. Opposed to the approaches that "virtualize" time as presented in this work, the "virtualization" of space ambitiously studied at this time <ref> [Luba 93] </ref> promises a shift of conventional parallel and distributed simulation paradigms. Further challenges are seen in the hierarchical combination [Raja 93] or even the uniformization of protocols [Bagr 91], the uniformization of continuous and discrete event simulation, the integration of real time constraints into protocols (distributed interactive simulation), etc.
Reference: [Matt 87] <author> F. Mattern. </author> <title> "Algorithms for distributed termination detection". </title> <journal> Distributed Computing, </journal> <volume> Vol. 2, </volume> <pages> pp. 161-175, </pages> <year> 1987. </year>
Reference-contexts: A central controller is assumed in their algorithm, thus violating a distributed computing principle. To avoid a single resource (controller) to become a communication performance bottleneck during deadlock detection, any general distributed termination detection algorithm <ref> [Matt 87] </ref> or distributed deadlock detection algorithm [Chan 83] could be used instead. In an algorithm described by Misra [Misr 86], a special message called marker circulates through GLP to detect and correct deadlock.
Reference: [Matt 93] <author> F. Mattern. </author> <title> "Efficient Algorithms for Distributed Snapshots and Global Virtual Time Approximation". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 18, No. 4, </volume> <pages> pp. 423-434, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Although distributed snapshot algorithms [Chan 85] find a straightforward application, solutions more efficient than message ackowledging, the delaying of sending event messages while awaiting control messages or piggybacking control information onto event messages are desired. Mattern <ref> [Matt 93] </ref> uses a "parallel" distributed snapshot algorithm to approximate GVT, that is not related to any specific control topology like a ring or the MRG topology. Moreover, it does not rely on FIFO channels. <p> The "parallel" snapshot and GVT computation based on the ideas above (coloring messages and LPs, and establishing a GVT estimate based on the distributed computation of two snapshots) is sketched in <ref> [Matt 93] </ref>. 2.2.9 Limiting the Optimism to Time Buckets Quite similar to the optimistic time windows approach, the Breathing Time Bucket (BTB) protocol addresses the antimessage dilemma which exhibits instabilities in the performance of Time Warp.
Reference: [Misr 86] <author> J. Misra. </author> <title> "Distributed Discrete-Event Simulation". </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Jefferson [Jeff 85a] recognized this problem to be the inverse of Lamport's logical clock problem [Lamp 78], i.e. providing clock values for events occuring in a distributed system such that all events appear ordered in logical time. It is intuitively convincing and has been shown in <ref> [Misr 86] </ref> that no causality error can ever occur in an asynchronous LP simulation if and only if every LP adheres to processing events in nondecreasing timestamp order only (local causality constraint (lcc) as formulated in [Fuji 90]). <p> As described by <ref> [Misr 86] </ref>, in CMB causality of events across LPs is preserved by sending timestamped (external) event messages of type hee@ti, where ee denotes the event and t is a copy of LVT of the sending LP at (@) the instant when the message was created and sent. t = ts (ee) <p> Several methods have been proposed to overcome the vulnerability of the CMB protocol to deadlock, falling into the two principle categories: deadlock avoidance and deadlock detection/recory. 2.1.1 Deadlock Avoidance Deadlock as in Figure 6 can be prevented by modifying the communication protocol based on the sending of nullmessages <ref> [Misr 86] </ref> of the form h0@ti, where 0 denotes a nullevent (event without effect). A nullmessages is not related to the simulated model and only serves for synchronization purposes. <p> Optimizations of the protocol to reduce the frequency and amount of nullmessages, e.g. sending them only on demand (upon request), delayed until some timeout, or only when an LP becomes blocked have been proposed <ref> [Misr 86] </ref>. An approach where additional information (essentially the routing path as observed during traversal) is attached to the nullmessage, the carrier nullmessage protocol [Cai 90] will be investigated in more detail later. <p> To avoid a single resource (controller) to become a communication performance bottleneck during deadlock detection, any general distributed termination detection algorithm [Matt 87] or distributed deadlock detection algorithm [Chan 83] could be used instead. In an algorithm described by Misra <ref> [Misr 86] </ref>, a special message called marker circulates through GLP to detect and correct deadlock. A cyclic path for traversing all ch i;j 2 CH is precomputed and LPs are initially colored white. <p> The primary reading for Time Warp is [Jeff 85a], for conservative protocols it is <ref> [Misr 86] </ref>. The most relevant literature appears in the frame of the * Workshop on Parallel and Distributed Simulation (PADS), formerly (while being held as part of the SCS Multiconference) published as the Proceedings of the SCS Multiconference on Distributed Simulation.
Reference: [Nico 88] <author> D. M. Nicol. </author> <title> "Parallel Discrete-Event Simulation OF FCFS Stochastic Queueing Networks". </title> <booktitle> In: Proceedings of the ACM/SIGPLAN PPEALS 1988, </booktitle> <pages> pp. 124 - 137, </pages> <year> 1988. </year>
Reference-contexts: Lookahead must come directly from the underlying simulation model and enhances the prediction of future events, which is as seen necessary to determine when it is safe to process an event. The ability to exploit lookahead from FCFS queueing network simulations was originally demonstrated by Nicol <ref> [Nico 88] </ref>, the basic idea being that the simulation of a job arriving at a FCFS queue will certainly increment LVT by the service time, which can already be determined, e.g. by random variate presampling, upon arrival since the number of queued jobs is known and preemption is not possible. 2.1.2 <p> net into two LPs, such that the individual PN regions of LP 1 and LP 2 are: R 1 = (fT1g; fP1g; f (P1; T1)g; o (T1) exp ( = 0:5)); and R 2 = (fT2g; fP2g; f (P2; T2)g; o (T2) exp ( = 0:5)): Let the future list <ref> [Nico 88] </ref>, a sequence of exponentially distributed random firing times (random variates), for T1 and T2 be as in the table of Figure 7.
Reference: [Nico 91] <author> D. M. Nicol. </author> <title> "Performance bounds on parallel self-initiating discrete event simulations". </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 1, </volume> <pages> pp. 24 - 50, </pages> <month> Jan </month> <year> 1991. </year>
Reference-contexts: is to introduce a window W i in simulated time for each LP i , such that events within this time window are safe (events in W i are independent of events in W j , i 6= j) and can be processed concurrently across all LP i [Luba 88], <ref> [Nico 91] </ref>. A conservative time window (CTW) parallel LP simulation synchronously operates in two phases.
Reference: [Nico 94] <author> D. Nicol and R. Fujimoto. </author> <title> "Parallel Simulation Today". </title> <note> to appear in: Operations Research, </note> <year> 1994. </year>
Reference-contexts: The tradeoff between state saving costs and the coast forward overhead has been studied (as reported by <ref> [Nico 94] </ref> in reference to Lin and Lazowska) based on expected event processing time (* = E [exec (e)]) and state saving costs (oe), giving an optimal interleaving factor fl as b (ff 1)fic &lt; fl &lt; d (2ff + 1)fie where ff is the average number of rollbacks with = <p> In the implementation proposed by Lin and Preiss [Lin 91], the two other issues are coupled to a processor scheduling policy in order to guarantee a certain amount of free memory (called salvage parameter in <ref> [Nico 94] </ref>), while following the "cancel-furthest-ahead" principle. <p> of debugging parallel programs finds a high chance to be tackled by similar ideas. 45 4 Sources of Literature and Further Reading Comprehensive overviews on the field of parallel and distributed simulation are the surveys by Richter and Walrand [Rich 89], Fujimoto [Fuji 90], and most recently Nicol and Fujimoto <ref> [Nico 94] </ref>. The primary reading for Time Warp is [Jeff 85a], for conservative protocols it is [Misr 86].
Reference: [Peac 79] <author> J. K. Peacock, J. W. Wong, and E. G. Manning. </author> <title> "Distributed Simulation using a Network of Processors.". </title> <journal> Computer Networks, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 44-56, </pages> <year> 1979. </year>
Reference-contexts: is highly related to the effective technology used in target multiprocessor architectures. (We shall avoid presenting the achievements of research in the light of readily available technology, permanently being subject to change.) For the representation and advancement of simulated time (VT) in an LP simulation we can devise two possibilities <ref> [Peac 79] </ref>: a synchronous LP simulation implements VT as a global clock, which is either represented explicitly as a centralized data structure, or implicitly implemented by a time-stepped execution procedure the key characteristic being that each LP (at any point in real time) faces the same VT. <p> This restriction is relaxed in an asynchronous LP simulation, where every LP maintains a local VT (LVT) with generally different clock values at a given point in real time. 1.5.1 Synchronous LP Simulation In a time-stepped LP simulation <ref> [Peac 79] </ref>, all the LPs' local clocks are kept at the same value at every point in real time, i.e. every local clock evolves on a sequence of discrete values (0; ; 2; 3; : : :). <p> Once the minimum timestamp of possible next external events is determined, the global clock can be advanced by (S), i.e. an amount which depends on the particular state S. For a distributed implementation of a global clock <ref> [Peac 79] </ref>, a structured (hierarchical) LP organization can be used [Conc 85] to determine the minimum next event time. A parallel min-reduction operation can bring this timestamp to the root of a process tree [Baik 85], which can then be propagated down the tree.
Reference: [Prak 91] <author> A. Prakash and R. Subramanian. </author> <title> "Filter: An Algorithm for Reducing Cascaded Rollbacks in Optimistic Distributed Simulation". </title> <editor> In: A. H. Rutan, Ed., </editor> <booktitle> Proceedings of the 24 th Annual Simulation Symposium, </booktitle> <address> New Orleans, Louisiana, USA, </address> <month> April 1-5, </month> <year> 1991., </year> <pages> pp. 123-132, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Breaking/Preventing Rollback Chains Besides the postponing of erroneous message and state annihilation until it turns out that they are not reproduced in the repeated simulation, other techniques have been studied to break cascades of rollbacks as early as possible. Prakash and Subramanian <ref> [Prak 91] </ref>, comparable to the carrier null message approach, attach a limited amount of state information to messages to prevent recursive rollbacks in cyclic GLPs. This information allows LPs to filter out messages based on preempted (obsolete) states to be eventually annihilated by chasing antimessages currently in transit.
Reference: [Raja 93] <author> H. Rajaei, R. Ayani, and L. E. Thorelli. </author> <title> "The Local Time Warp Approach to Parallel Simulation". </title> <editor> In: R. Bagrodia and D. Jefferson, Eds., </editor> <booktitle> Proc. of the 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pp. 119-126, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: Opposed to the approaches that "virtualize" time as presented in this work, the "virtualization" of space ambitiously studied at this time [Luba 93] promises a shift of conventional parallel and distributed simulation paradigms. Further challenges are seen in the hierarchical combination <ref> [Raja 93] </ref> or even the uniformization of protocols [Bagr 91], the uniformization of continuous and discrete event simulation, the integration of real time constraints into protocols (distributed interactive simulation), etc.
Reference: [Reih 90] <author> P. L. Reiher, R. M. Fujimoto, S. Bellenot, and D. Jefferson. </author> <title> "Cancellation Strategies in Optimistic Execution Systems". </title> <booktitle> In: Proceedings of the SCS Multiconference on Distributed Simulation Vol. </booktitle> <volume> 22 (1), </volume> <pages> pp. 112-121, </pages> <publisher> SCS, </publisher> <month> January </month> <year> 1990. </year> <month> 50 </month>
Reference-contexts: By immediately discarding rolled back computations this chance is lost for the aggressive cancellation policy. A performance comparison of the two, however, is related to the simulation model. Analysis by Reiher and Fujimoto <ref> [Reih 90] </ref> shows that lazy cancellation can arbitrarily outperform aggressive cancellation and vice versa, i.e. one can construct extreme cases for lazy and aggressive cancellation such that if one protocol executes in ff time using N processors, the other uses ffN time.
Reference: [Rich 89] <author> R. Richter and J. C. Walrand. </author> <title> "Distributed Simulation of Discrete Event Systems". </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 1, </volume> <pages> pp. 99 - 113, </pages> <month> Jan </month> <year> 1989. </year>
Reference-contexts: The difficult problem of debugging parallel programs finds a high chance to be tackled by similar ideas. 45 4 Sources of Literature and Further Reading Comprehensive overviews on the field of parallel and distributed simulation are the surveys by Richter and Walrand <ref> [Rich 89] </ref>, Fujimoto [Fuji 90], and most recently Nicol and Fujimoto [Nico 94]. The primary reading for Time Warp is [Jeff 85a], for conservative protocols it is [Misr 86].
Reference: [Sama 85] <author> B. Samadi. </author> <title> Distributed Simulation algorithms and performance analysis. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1985. </year>
Reference-contexts: T fl , thus is an instant of real time that happens to lie within every LP's interval. Samadi's algorithm <ref> [Sama 85] </ref> follows the idea of GVT triggering via a central GVT manager sending out a GVT-start message to announce a new GVT computation epoch. After all LPs have prompted the request, the manager computes and broadcasts the new GVT value and completes the GVT epoch.
Reference: [Shen 92] <author> S. Shen and L. Kleinrock. </author> <title> "The Virtual-Time Data-Parallel Machine". </title> <booktitle> In: Proc. of the 4 th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 46-53, </pages> <publisher> IEEE Compute Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Indeed, parallel and distributed simulation protocol research has just started to ferment developments in computer science disciplines other than "simulation" in the classical sense. For example, simulated executions of SIMD programs in asynchronous environments can accelerate their execution <ref> [Shen 92] </ref>, and parallel simulations executing parallel programs with message passing communication have already been shown to be possible [Dick 94].
Reference: [Soko 88] <author> L. M. Sokol, D. P. Briscoe, and A. P. Wieland. "MTW: </author> <title> a strategy for scheduling discrete simulation events for concurrent execution.". </title> <booktitle> In: Proc. of the SCS Multiconf. on Distributed Simulation, </booktitle> <pages> pp. 34 - 42, </pages> <year> 1988. </year>
Reference-contexts: Although the global clock is advanced to the minimum next event time as in the event driven scheme, LPs are only allowed to simulate within a -tick of time, called a bounded lag by Lubachevsky [Luba 88] or a Moving Time Window by <ref> [Soko 88] </ref>. 1.5.2 Asynchronous LP Simulation Asynchronous LP simulation relies on the presence of events occuring at different simulated times that do not affect one another. Concurrent processing of those events thus effectively accelerates sequential simulation execution time. <p> In the original work of Sokol, Briscoe and Wieland <ref> [Soko 88] </ref>, the moving time window (MTW) protocol, neither internal nor external events e with ts (e) &gt; t + are allowed to be simulated in the time window [t; t +), but are postponed for the next time window [t + ; t +2).
Reference: [Stei 93] <author> J. S. Steinmann. </author> <title> "Breathing Time Warp". </title> <editor> In: R. Bagrodia and D. Jefferson, Eds., </editor> <booktitle> Proc. of the 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pp. 109-118, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: Once the last LP has issued the nonblocking sync, all the other LPs are interrupted and requested to send their event messages. Clearly, BTB can only work efficiently if a sufficient amount of events is processed on average in one bucket. The Breathing Time Warp (BTW) <ref> [Stei 93] </ref> combines features of Time Warp with BTB aiming to eliminate shortcomings of the two protocols. The underlying idea again is the belief that the likelihood of an optimistically processed event being subject to a future correction decreases with the distance of its timestamp to GVT.
Reference: [Turn 93] <author> S. J. Turner and W. Cai. </author> <title> "The "Logical Clocks" Approach to the Visualization of Parallel Programs". </title> <editor> In: G. Haring and G. Kotsis, Eds., </editor> <booktitle> Performance Measurement and Visualization of Parallel Systems, </booktitle> <pages> pp. 45-66, </pages> <publisher> North Holland, </publisher> <year> 1993. </year>
Reference-contexts: Other work has shown that an intrusion free monitoring and trace collection of distributed memory parallel program executions is possible by superimposing the execution with a distributed DES protocol <ref> [Turn 93] </ref>.
Reference: [Venk 86] <author> K. Venkatesh, T. Radhakrishnan, and H. F. Li. </author> <title> "Discrete Event Simulation in a Distributed System". </title> <booktitle> In: IEEE COMPSAC, </booktitle> <pages> pp. 123 - 129, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference-contexts: The imbalance of work across the LPs in certain time steps on the other hand naturally leads to idle times and thus represents a source of inefficiency. 9 Both centralized and decentralized approaches of implementing global clocks have been followed. In <ref> [Venk 86] </ref>, a centralized implementation with one dedicated processor controlling the global clock is proposed. To overcome stepping the time at instances where no events are occuring, algorithms to determine for every LP at what point in time the next interaction with another LP shall occur have been developed.
References-found: 59

