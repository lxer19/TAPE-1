URL: http://www.cs.washington.edu/research/jair/volume8/moore98a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/moore98a.html
Root-URL: 
Email: awm@cs.cmu.edu  mslee@cs.cmu.edu  
Title: Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets  
Author: Andrew Moore Mary Soon Lee 
Address: Pittsburgh PA 15213  
Affiliation: School of Computer Science and Robotics Institute Carnegie Mellon University,  
Note: Journal of Artificial Intelligence Research 8 (1998) 67-91 Submitted 7/97; published 3/98  
Abstract: This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., & Verkamo, A. I. </author> <year> (1996). </year> <title> Fast discovery of association rules. </title> <editor> In Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., & Uthurusamy, R. (Eds.), </editor> <title> Advances in Knowledge Discovery and Data Mining. </title> <note> AAAI Press. </note> <author> 89 Moore & Lee Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: This work is also applicable to Online Analytical Processing (OLAP) applications in data mining, where operations on large datasets such as multidimensional database access, DataCube operations (Harinarayan, Rajaraman, & Ullman, 1996), and association rule learning <ref> (Agrawal, Mannila, Srikant, Toivonen, & Verkamo, 1996) </ref> could be accelerated by fast counting. Let us begin by establishing some notation. We are given a data set with R records and M attributes. The attributes are called a 1 ; a 2 ; : : : a M . <p> i (2) : : : a i (n) g; ADN) (4) and so the missing conditional contingency table in the algorithm can be computed by the following row-wise subtraction: CT MCV := MakeContab (fa i (2) : : : a i (n) g; ADN) X CT k (5) Frequent Sets <ref> (Agrawal et al., 1996) </ref>, which are traditionally used for learning association rules, can also be used for computing counts. A recent paper (Mannila & Toivonen, 1996), which also employs a similar subtraction trick, calculates counts from Frequent Sets. <p> Another possibility, R-trees (Guttman, 1984; Roussopoulos & Leifker, 1985), store databases of M -dimensional geometric objects. However, in this context, they offer no advantages over kd-trees. 8.2 Why not use a Frequent Set finder? Frequent Set finders <ref> (Agrawal et al., 1996) </ref> are typically used with very large databases of millions of records containing very sparse binary attributes. Efficient algorithms exist for finding all subsets of attributes that co-occur with value TRUE in more than a fixed number (chosen by the user, and called the support) of records.
Reference: <author> Clark, P., & Niblett, R. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference-contexts: If the ADtree is used for multiple purposes then its build-time is amortized and the resulting relative efficiency gains over traditional counting are the same for both exhaustive searches and non-exhaustive searches. Algorithms that use non-exhaustive searches include hill-climbing Bayes net learners, greedy rule learners such as CN2 <ref> (Clark & Niblett, 1989) </ref> and decision tree learners (Quinlan, 1983; Breiman et al., 1984). Acknowledgements This work was sponsored by a National Science Foundation Career Award to Andrew Moore.
Reference: <author> Cover, T. M., & Thomas, J. A. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: There are many ways of scoring a set of features, but a particularly simple one is information gain <ref> (Cover & Thomas, 1991) </ref>. Let a out be the attribute we wish to predict and let a i (1) : : : a i (n) be the set of attributes used as inputs.
Reference: <author> Fayyad, U., Mannila, H., & Piatetsky-Shapiro, G. </author> <year> (1997). </year> <title> Data Mining and Knowledge Discovery. </title> <publisher> Kluwer Academic Publishers. A new journal. </publisher>
Reference-contexts: A second possibility is to exploit secondary storage and store deep, rarely visited nodes of the ADtree on disk. This would doubtless best be achieved by integrating the machine learning algorithms with current database management tools|a topic of considerable interest in the data mining community <ref> (Fayyad et al., 1997) </ref>.
Reference: <author> Fayyad, U., & Uthurusamy, R. </author> <year> (1996). </year> <journal> Special issue on Data Mining. Communications of the ACM, </journal> <volume> 39 (11). </volume>
Reference: <author> Friedman, N., & Yakhini, Z. </author> <year> (1996). </year> <title> On the sample complexity of learning Bayesian networks. </title> <booktitle> In Proceedings of the 12th conference on Uncertainty in Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This provides a dataset with fairly sparse values and with many interdependencies. network). The penalized log-likelihood score <ref> (Friedman & Yakhini, 1996) </ref> is N params log (R)=2 + R M X X Asgn2X j n j X P (a j = v ^ Asgn) log P (a j = v j Asgn) (12) where N params is the total number of probability table entries in the network. <p> The latter operation is necessary to allow the search algorithm to choose the best ordering of nodes in the Bayes net. Stochastic searches such as this are a popular method for finding Bayes net structures <ref> (Friedman & Yakhini, 1996) </ref>.
Reference: <author> Guttman, A. </author> <year> (1984). </year> <title> R-trees: A dynamic index structure for spatial searching. </title> <booktitle> In Proceedings of the Third ACM SIGACT-SIGMOD Symposium on Principles of Database Systems. </booktitle> <institution> Assn for Computing Machinery. </institution>
Reference: <author> Harinarayan, V., Rajaraman, A., & Ullman, J. D. </author> <year> (1996). </year> <title> Implementing Data Cubes Efficiently. </title> <booktitle> In Proceedings of the Fifteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems : PODS 1996, </booktitle> <pages> pp. 205-216. </pages> <institution> Assn for Computing Machinery. </institution>
Reference-contexts: Many machine learning algorithms operating on datasets of symbolic attributes need to do frequent counting. This work is also applicable to Online Analytical Processing (OLAP) applications in data mining, where operations on large datasets such as multidimensional database access, DataCube operations <ref> (Harinarayan, Rajaraman, & Ullman, 1996) </ref>, and association rule learning (Agrawal, Mannila, Srikant, Toivonen, & Verkamo, 1996) could be accelerated by fast counting. Let us begin by establishing some notation. We are given a data set with R records and M attributes. <p> The cost of the initial tree building is then amortized over all the times it is used. In database terminology, the process is known as materializing <ref> (Harinarayan et al., 1996) </ref> and has been suggested as desirable for datamining by several researchers (John & Lent, 1997; Mannila & Toivonen, 1996).
Reference: <author> John, G. H., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the Subset Selection Problem. </title> <editor> In Cohen, W. W., & Hirsh, H. (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This depends on whether the cost of initially building the ADtree can be amortized over many runs of the decision tree algorithm. Repeated runs of decision tree building can occur if one is using the wrapper model of feature selection <ref> (John, Kohavi, & Pfleger, 1994) </ref>, or if one is using a more intensive search over tree structures than the traditional greedy search (Quinlan, 1983; Breiman et al., 1984) 75 Moore & Lee matching 3 or fewer records is not expanded, but simply records a set of pointers into the dataset (shown
Reference: <author> John, G. H., & Lent, B. </author> <year> (1997). </year> <title> SIPping from the data firehose. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> The Power of Decision Tables. </title> <editor> In Lavrae, N., & Wrobel, S. (Eds.), </editor> <booktitle> Machine Learning : ECML-95 : 8th European Conference on Machine Learning, </booktitle> <address> Heraclion, Crete, Greece. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Feature Selection Given M attributes, of which one is an output that we wish to predict, it is often interesting to ask "which subset of n attributes, (n &lt; M ), is the best predictor of the output on the same distribution of datapoints that are reflected in this dataset?" <ref> (Kohavi, 1995) </ref>. There are many ways of scoring a set of features, but a particularly simple one is information gain (Cover & Thomas, 1991).
Reference: <author> Kohavi, R. </author> <year> (1996). </year> <title> Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid. </title> <editor> In E. Simoudis and J. Han and U. Fayyad (Ed.), </editor> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: This is a simple example designed to test 76 Cached Sufficient Statistics for Efficient Machine Learning Name R = Num. M = Num. Records Attributes ADULT1 15,060 15 The small "Adult Income" dataset placed in the UCI repository by Ron Kohavi <ref> (Kohavi, 1996) </ref>. Contains census data related to job, wealth, and nationality. Attribute arities range from 2 to 41. In the UCI repository this is called the Test Set. Rows with missing values were removed. ADULT2 30,162 15 The same kinds of records as above but with different data.
Reference: <author> Madala, H. R., & Ivakhnenko, A. G. </author> <year> (1994). </year> <title> Inductive Learning Algorithms for Complex Systems Modeling. </title> <publisher> CRC Press Inc., </publisher> <address> Boca Raton. </address>
Reference-contexts: How can machine learning and statistical algorithms take advantage of this? Here we provide three examples: Feature Selection, Bayes net scoring and rule learning. But it seems likely that many other algorithms can also benefit, for example stepwise logistic regression, GMDH <ref> (Madala & Ivakhnenko, 1994) </ref>, and text classification. Even decision tree (Quinlan, 1983; Breiman, Friedman, Olshen, & Stone, 1984) learning may benefit. 2 In future work we will also examine ways to speed up nearest neighbor and other memory-based queries using ADtrees. 2.
Reference: <author> Mannila, H., & Toivonen, H. </author> <year> (1996). </year> <title> Multiple uses of frequent sets and condensed representations. </title> <editor> In E. Simoudis and J. Han and U. Fayyad (Ed.), </editor> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <editor> AAAI Press. </editor> <title> 90 Cached Sufficient Statistics for Efficient Machine Learning Moore, </title> <editor> A. W., Schneider, J., & Deng, K. </editor> <year> (1997). </year> <title> Efficient Locally Weighted Polynomial Regression Predictions. </title> <editor> In D. Fisher (Ed.), </editor> <booktitle> Proceedings of the 1997 International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This work is also applicable to Online Analytical Processing (OLAP) applications in data mining, where operations on large datasets such as multidimensional database access, DataCube operations (Harinarayan, Rajaraman, & Ullman, 1996), and association rule learning <ref> (Agrawal, Mannila, Srikant, Toivonen, & Verkamo, 1996) </ref> could be accelerated by fast counting. Let us begin by establishing some notation. We are given a data set with R records and M attributes. The attributes are called a 1 ; a 2 ; : : : a M . <p> A recent paper <ref> (Mannila & Toivonen, 1996) </ref>, which also employs a similar subtraction trick, calculates counts from Frequent Sets. <p> Efficient algorithms exist for finding all subsets of attributes that co-occur with value TRUE in more than a fixed number (chosen by the user, and called the support) of records. Recent research <ref> (Mannila & Toivonen, 1996) </ref> suggests that such Frequent Sets can be used to perform efficient counting. <p> But if that is inadequate at least three possibilities remain. First, we could build approximate ADtrees that do not store any information for nodes that match fewer than a threshold number of records. Then approximate contingency tables (complete with error bounds) can be produced <ref> (Mannila & Toivonen, 1996) </ref>. A second possibility is to exploit secondary storage and store deep, rarely visited nodes of the ADtree on disk.
Reference: <author> Omohundro, S. M. </author> <year> (1987). </year> <title> Efficient Algorithms with Neural Network Behaviour. </title> <journal> Journal of Complex Systems, </journal> <volume> 1 (2), </volume> <pages> 273-347. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning Efficient Classification Procedures and their Application to Chess End Games. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning|An Artificial Intelligence Approach (I). </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference: <author> Roussopoulos, N., & Leifker, D. </author> <year> (1985). </year> <title> Direct spatial search on pictorial databases using packed R-trees. </title> <editor> In Navathe, S. (Ed.), </editor> <booktitle> Proceedings of ACM-SIGMOD 1985 International Conference on Management of Data. </booktitle> <institution> Assn for Computing Machinery. </institution>
Reference: <author> Rymon, R. </author> <year> (1993). </year> <title> An SE-tree based Characterization of the Induction Problem. </title> <note> In P. </note>
Reference-contexts: Our initial simplified description is an obvious tree representation that does not yield any immediate memory savings, 1 but will later provide several opportunities 1. The SE-tree <ref> (Rymon, 1993) </ref> is a similar data structure. 69 Moore & Lee for cutting off zero counts and redundant counts. This structure is shown in Figure 3. An ADtree node (shown as a rectangle) has child nodes called "Vary nodes" (shown as ovals).
Reference: <editor> Utgoff (Ed.), </editor> <booktitle> Proceedings of the 10th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher> <pages> 91 </pages>
References-found: 20

