URL: ftp://speech.cse.ogi.edu/pub/docs/tsap_jan95.ps.Z
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/cole.html
Root-URL: http://www.cse.ogi.edu
Title: The Challenge of Spoken Language Systems: Research Directions for the Nineties  
Author: Ron Cole Lynette Hirschman Les Atlas Hynek Hermansky Patti Price Mary Beckman Steve Levinson Harvey Silverman Alan Biermann Kathy McKeown Judy Spitz Marcia Bush Nelson Morgan Alex Waibel Mark Clements David G. Novick Clifford Weinstein Jordan Cohen Mari Ostendorf Steve Zahorian Oscar Garcia Sharon Oviatt Victor Zue Brian Hanson 
Affiliation: Oregon Graduate Institute MITRE  Univ. of Washington Oregon Graduate Inst. SRI International  Ohio State Univ. AT&T Bell Labs Brown Univ.  Duke Univ. Columbia Univ. NYNEX AI Speech Group  Xerox Palo Alto Research ICSI, UC Berkeley Carnegie-Mellon Univ.  Georgia Inst. of Technology Oregon Graduate Inst. MIT Lincoln Laboratory  IDA Center for Comm. Research Boston Univ. Old Dominion Univ.  George Washington Univ. SRI International MIT  Speech Technology Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Acero and R.M. Stern. </author> <title> Environmental robustness in automatic speech recognition. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 849-852. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference: [2] <author> J. Allen, S. Guez, L. Hoebel, E. Hinkelman, K. Jackson, A. Kyburg, and D. Traum. </author> <title> The discourse system project. </title> <type> Technical Report 317, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: True speech understanding requires that individual utterance meanings be understood in the context of the larger dialogue structure <ref> [2, 40] </ref>. This structure must co-ordinate a variety of information, including the ultimate goals of the interaction, the subgoals being attempted, the status of the system knowledge base, models of user knowledge, and a history of the interaction.
Reference: [3] <author> J. F. Allen and C. R. Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 3(15) </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: Discovering the Structure of Dialogue. Typical dialogues are usually organized into a series of subdialogues each of which is aimed at solving a particular subgoal [39, 73, 104]. The individual subdialogues provide what is called "focus" [39], and the tracking of subdialogues is called "plan recognition" <ref> [3] </ref>. The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth. This nesting affects not only content and referential structure, but prosodic structure as well [54].
Reference: [4] <author> D. Appelt and E. Jackson. </author> <title> SRI International February 1992 ATIS benchmark test results. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: These systems operate in near real-time, accepting spontaneous, continuous speech from speakers with no prior enrollment; they have vocabularies of 1000-2000 words, and an overall correct understanding rate of almost 90% <ref> [96, 5, 110, 4, 26] </ref>. Although progress over the past decade has been impressive, there are significant obstacles to be overcome before spoken language systems can reach their full potential.
Reference: [5] <author> M. Bates, R. Bobrow, P. Fung, R. Ingria, F. Kubala, J. Makhoul, L. Nguyen, R. Schwartz, and D. Stallard. </author> <title> The BBN/HARC spoken language understanding system. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech and Signal Processing. IEEE, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: These systems operate in near real-time, accepting spontaneous, continuous speech from speakers with no prior enrollment; they have vocabularies of 1000-2000 words, and an overall correct understanding rate of almost 90% <ref> [96, 5, 110, 4, 26] </ref>. Although progress over the past decade has been impressive, there are significant obstacles to be overcome before spoken language systems can reach their full potential.
Reference: [6] <author> M. Berger and H. F. Silverman. </author> <title> Microphone array optimization by stochastic region contraction (SRC). </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(11) </volume> <pages> 2377-2386, </pages> <year> 1991. </year>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [7] <author> G. Brown, K. Currie, and J. Kenworthy. </author> <title> Questions of Intonation. </title> <publisher> Croom Helm, </publisher> <year> 1980. </year>
Reference-contexts: of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., <ref> [8, 7, 55] </ref>) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences. Speaker differences arise from two different sources: a) differences in anatomy of speech production organs, b) differences in acquired speech production skills.
Reference: [8] <author> B. </author> <title> Butterworth. Hesitation and semantic planning in speech. </title> <journal> Journal of Psycholinguistic Research, </journal> <volume> 4 </volume> <pages> 75-87, </pages> <year> 1975. </year>
Reference-contexts: of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., <ref> [8, 7, 55] </ref>) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences. Speaker differences arise from two different sources: a) differences in anatomy of speech production organs, b) differences in acquired speech production skills.
Reference: [9] <author> R. Carlson and B. </author> <title> Granstrom. Speech synthesis development and phonetic research - a personal introduction. </title> <journal> Journal of Phonetics, </journal> <volume> 19 </volume> <pages> 3-8, </pages> <year> 1991. </year>
Reference-contexts: Possibilities range from simply increasing the quality to modeling discourse structure to active dialogue control. 4. Adaptation. Adaptation is an issue which is only recently being addressed <ref> [9] </ref>. Adaptation technology and, more generally, models that can be trained automatically are important for adjusting a synthesis system to different situational demands, different speaker characteristics and style, and different languages, all of which will be important for more general applicability of speech synthesis.
Reference: [10] <author> F. Chen and M. Withgott. </author> <title> The use of emphasis to automatically summarize a spoken discourse. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages I.229-232, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For example, phrasal prominence may help to detect high-information regions of a discourse, for use in automatic gisting and summarization <ref> [10] </ref>. However, this is a very young research area and more research is needed to understand how people use prosodic information, and to understand how prosodic information can improve the performance of spoken language systems, both for recognition and for generation (discussed below). 3. Understanding Conversational Dynamics.
Reference: [11] <author> J. Cheshire. </author> <title> English Around the World: Sociolinguistic Perspectives. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [12] <author> L. A. Chistovich and V.V. Lublinskaya. </author> <title> The `center of gravity' effect in vowel spectra and critical distance between the formants: psychoacoustical study of the perception of vowel-like stimuli. </title> <journal> Hearing Research, </journal> <volume> 1 </volume> <pages> 185-195, </pages> <year> 1979. </year>
Reference: [13] <author> J. R. Cohen. </author> <title> Application of an auditory model to speech recognition. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 85 </volume> <pages> 2623-2629, </pages> <year> 1989. </year>
Reference-contexts: These techniques can provide significant improvement of recognition robustness by alleviating some of non-linguistic sources of variability in speech, such as differences due to talkers, differences in the acoustic environment and in background noise, or overall spectral differences due to a change of microphone or microphone position <ref> [48, 52, 13] </ref>. Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., [72, 126, 120]).
Reference: [14] <author> P. R. Cohen and S. L. Oviatt. </author> <title> The role of voice in human-machine communication, chapter 1. </title> <publisher> National Academy Press, </publisher> <address> Washington, D. C., </address> <year> 1994. </year> <month> 36 </month>
Reference-contexts: However, the role that spoken language ultimately should play in future multimodal systems is not well understood <ref> [14] </ref>. In addition, since multimodal systems are relatively complex, the problem of how to design successful configurations is unlikely to be solved through a simple intuitive approach.
Reference: [15] <author> D. Van Compernolle, W. Ma, F. Xie, and M. Van Diest. </author> <title> Speech recognition in noisy environments with the aid of microphone arrays. </title> <journal> Speech Communication, </journal> 9(5/6):433-442, December 1990. 
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [16] <author> N. Daly and V. Zue. </author> <title> Statistical and linguistic analyses of F0 in read and spontaneous speech. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <pages> pages I:763-766, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Prosodics We know that prosody can provide information that helps humans understand speech (e.g., in read speech by radio announcers [103]). We also know that prosody of read speech differs from spontaneous speech <ref> [16] </ref>. Recent results are beginning to show that use of prosody can aid automated understanding of spontaneous speech [134] provided that the system can detect prosodic phenomena reliably [139, 140] and can correlate these prosodic cues with higher level syntactic, semantic, pragmatic, and conversational structures.
Reference: [17] <author> L. Deng and D. Sun. </author> <title> Phonetic classification and recognition using HMM representation of overlapping articulatory features for all classes of English sounds. </title> <booktitle> In Proceedings of the 1994 International Conference on Acoustics, Speech and Signal Processing, pages I:45-48, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Standard tools from system identification are then used in an attempt to derive a mathematical description of this process. On the other hand, the known properties of human articulatory dynamics can be used to constrain the mathematical description of the system dynamics <ref> [111, 107, 17] </ref>. In both classes of systems one attempts to achieve more robust speech recognition by first deducing a certain amount of information regarding the dynamics underlying speech production. 2. Modeling Speech Rate. Effects of tempo are poorly understood.
Reference: [18] <author> J. DiPaolo and A. </author> <title> Faber. Phonation differences and the phonetic context of the tense-lax contrast in Utah English. Language Variation and Change, </title> <booktitle> 2 </booktitle> <pages> 155-204, </pages> <year> 1991. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [19] <author> J. Edwards, M. Beckman, and J. Fletcher. </author> <title> The articulatory kinematics of final lengthening. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 89 </volume> <pages> 369-382, </pages> <year> 1991. </year>
Reference-contexts: What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., <ref> [122, 19, 103] </ref>); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences.
Reference: [20] <author> Y. Ephraim. </author> <title> Gain-adapted hidden markov models for recognition of clean and noisy speech. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 40 </volume> <pages> 1303-1316, </pages> <month> June </month> <year> 1992. </year>
Reference: [21] <author> A. Erell and M. Weintraub. </author> <title> Recognition of noisy speech: Using minimum-mean log-spectral distance estimation. </title> <booktitle> In DARPA Workshop on Spoken Language Systems, </booktitle> <pages> pages 341-345. DARPA, </pages> <month> June </month> <year> 1990. </year>
Reference: [22] <author> L. D. Erman, F. Hayes-Roth, V. R. Lesser, and D. R. Reddy. </author> <title> The Hearsay II speech understanding system. </title> <journal> ACM Computing Surveys, </journal> <pages> pages 213-253, </pages> <year> 1980. </year>
Reference-contexts: The currently active subgoal will make very strong predictions, and other locally nonactive subgoals will make weaker predictions. The combination of all the information from the dialogue level can substantially sharpen estimates at for improved recognition <ref> [22, 136, 138] </ref>. This leads to a new formulation of the speech understanding problem. Instead of receiving an acoustic input and passing a meaning to the higher level, the recognizer could receive both the acoustic input and a representation of expected meanings.
Reference: [23] <author> L. Hirschman et al. </author> <title> Multi-site data collection for a spoken language corpus. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: For speech recognition, highly effective automated training procedures have been developed, but these require large amounts of task-specific data for reasonable performance. For example, in the ARPA Air Travel (ATIS) domain, joint data collection activity across five sites resulted in the collection of over 14,000 utterances <ref> [23] </ref>. <p> This may require new ways of doing system evaluation, since the current evaluation methods measure "understanding" and are expensive to implement for a single domain (cf. MUC [123] and ATIS <ref> [23] </ref>), let alone for multiple domains.
Reference: [24] <author> L. Hirschman et al. </author> <title> Multi-site data collection and evaluation in spoken language understanding. </title> <editor> In M. Bates, editor, </editor> <booktitle> Proceedings of the Human Language Technology Workshop, </booktitle> <address> Princeton, NJ, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: These corpora have also been associated with standardized evaluations of the component technologies <ref> [96, 24] </ref>. Although the use of regular "common evaluations" originated in the ARPA community, it has spread to the broader community and provides periodic measurements of progress of the field over time, as well as making it possible for individual systems to evaluate their internal progress.
Reference: [25] <author> M. Rayner et al. </author> <title> Spoken language translation with mid-90's technology: A case study. </title> <booktitle> In Proceedings of the 3rd European Conference on Speech Communication and Technology, </booktitle> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: a new language often places an additional burden of "language independence" on all components of the system [36], in addition to the need for new training data for both speech and language, although there are architectures that can make the port less difficult than it would otherwise be (see, e.g., <ref> [25] </ref>). <p> Spoken Language Translation. This is the grandest of the challenges, encompassing all the above challenges plus a machine translation capability. Initial advances in this direction are indeed in progress <ref> [135, 106, 25] </ref>, but considerable additional research will be necessary to 21 achieve complete widely usable and robust speech translation systems.
Reference: [26] <author> W. Ward et al. </author> <title> Speech recognition in open tasks. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: These systems operate in near real-time, accepting spontaneous, continuous speech from speakers with no prior enrollment; they have vocabularies of 1000-2000 words, and an overall correct understanding rate of almost 90% <ref> [96, 5, 110, 4, 26] </ref>. Although progress over the past decade has been impressive, there are significant obstacles to be overcome before spoken language systems can reach their full potential.
Reference: [27] <author> G. Fant. </author> <title> What can basic research contribute to speech synthesis? Journal of Phonetics, </title> <booktitle> 19 </booktitle> <pages> 75-90, </pages> <year> 1991. </year>
Reference-contexts: few particularly important research areas are: models of the physics of sound generation in the human vocal apparatus, models of articulation for synthesizing phonetic segments, theories of the relationship between prosody and syntax/semantics for predicting abstract prosodic patterns, and models of intonation and duration for interpreting those prosodic patterns acoustically <ref> [66, 27] </ref>. 2. Computational Models of Variability. Explicit models of variability are needed in synthesis to avoid monotony, an issue both for synthesis of long monologues and long human-computer interactive sessions.
Reference: [28] <author> M. Fanty, J. Pochmara, and R. A. Cole. </author> <title> An interactive environment for speech recognition research. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October 12-16 </month> <year> 1992. </year> <month> 37 </month>
Reference-contexts: Commercially available speech tools are available, but they are expensive. In the U.S., as part of their Software Capitalization Program, NSF has funded the development of a set of portable tools for speech recognition research for Unix workstations running X windows. These tools <ref> [28] </ref> are available to interested researchers 3 . Similarly, ARPA has funded an effort to make standard speech recognition components available, through Entropic Research Laboratory.
Reference: [29] <author> T. W. Finin. GUMS: </author> <title> A general user modelling shell. </title> <editor> In A. Kobsa and W. Wahlster, editors, </editor> <booktitle> User Models in Dialogue Systems, </booktitle> <pages> pages 411-430. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Incorporating a Model of the User. A key aspect of a dialogue system is its model of the user <ref> [29, 67, 97] </ref>. Processes of input recognition, output generation, and internal decision making all depend on user modelling. Word usage, grammatical constructions, and transmitted meanings will differ for users of different backgrounds and different levels of expertise.
Reference: [30] <author> J. L. Flanagan. </author> <title> Use of acoustic filtering to control the beamwidth of steered microphone arrays. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 78(2) </volume> <pages> 423-428, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [31] <author> J. L. Flanagan, D. A. Berkley, G. W. Elko, J. E. West, and M. M. Sondhi. </author> <title> Autodirective microphone systems. </title> <journal> Acoustica, </journal> <volume> 73 </volume> <pages> 58-71, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [32] <author> J. E. Flege. </author> <title> Laryngeal timing and phonation onset in utterance-initial English stops. </title> <journal> Journal of Phonetics, </journal> <volume> 10 </volume> <pages> 177-192, </pages> <year> 1982. </year>
Reference: [33] <author> T. Gay. </author> <title> Mechanisms in the control of speech rate. </title> <journal> Phonetica, </journal> <volume> 38 </volume> <pages> 148-158, </pages> <year> 1981. </year>
Reference-contexts: Modeling Speech Rate. Effects of tempo are poorly understood. What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., <ref> [33, 93, 133] </ref>), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances
Reference: [34] <author> O. Ghitza. </author> <title> Temporal non-place information in the auditory-nerve firing patterns as a front end for speech recognition in a noisy environment. </title> <journal> Journal of Phonetics, </journal> <volume> 16(1) </volume> <pages> 109-124, </pages> <year> 1988. </year>
Reference-contexts: The development of auditory models for speech processing is still at an early stage, but 6 current work in the application of some of these models to automated speech recognition appears quite promising. Some recently developed speech analysis techniques attempt to model the basic properties of human speech perception <ref> [109, 34, 75, 46, 50] </ref>.
Reference: [35] <author> H. Gish, Y.L. Chow, and J.R. Rohlicek. </author> <title> Probabilistic vector mapping of noisy speech parameters for hmm word spotting. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 117-120. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference: [36] <author> J. Glass, D. Goodine, M. Phillips, S. Sakai, S. Seneff, and V. Zue. </author> <title> A bilingual voyager system. </title> <booktitle> In Workshop on Human Language Technology, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: Multi-lingual systems represent a significant challenge for portability as well. Porting a system to a new language often places an additional burden of "language independence" on all components of the system <ref> [36] </ref>, in addition to the need for new training data for both speech and language, although there are architectures that can make the port less difficult than it would otherwise be (see, e.g., [25]). <p> These challenges include: 1. Multi-lingual Spoken Language Interfaces. Systems and techniques are needed which will allow users to speak to the systems in a variety of languages, and which will understand the speech well enough to efficiently carry out tasks such as interactive database retrieval <ref> [36] </ref> or command and control of complex systems. 2. Language Identification.
Reference: [37] <author> J. Godfrey, E. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 517-520. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: Specifically, this will require spontaneous speech corpora with detailed transcriptions (including prosodic annotation), at least some of it collected in "two-party" conversation settings (like the SWITCHBOARD corpus being collected at Texas Instruments <ref> [37] </ref>). Efforts are currently underway to create a standard [20 notation for prosody, so that training and evaluation materials can be prepared [117], but this is only a first towards a corpus of prosodically labelled spontaneous speech. 2.4 Dialogue Models Dialogue processing is the enabling technology for spoken language systems.
Reference: [38] <author> A. J. Goldschen. </author> <title> Continuous Automatic Speech Recognition by Lipreading. </title> <type> PhD thesis, </type> <institution> George Washington University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Goldschen has obtained 25 percent recognition in continuous speech using optical recognition exclusively, without the use of acoustic data or syntax <ref> [38] </ref>. Such results support the belief that multimodal systems may display more desirable properties, especially under realistic field conditions, than stand-alone spoken language systems.
Reference: [39] <author> B. J. Grosz. </author> <title> Discourse analysis. </title> <editor> In D.E. Walker, editor, </editor> <booktitle> Understanding Spoken Language, </booktitle> <pages> pages 235-268. </pages> <publisher> North Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Discovering the Structure of Dialogue. Typical dialogues are usually organized into a series of subdialogues each of which is aimed at solving a particular subgoal <ref> [39, 73, 104] </ref>. The individual subdialogues provide what is called "focus" [39], and the tracking of subdialogues is called "plan recognition" [3]. The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth. <p> Discovering the Structure of Dialogue. Typical dialogues are usually organized into a series of subdialogues each of which is aimed at solving a particular subgoal [39, 73, 104]. The individual subdialogues provide what is called "focus" <ref> [39] </ref>, and the tracking of subdialogues is called "plan recognition" [3]. The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth. This nesting affects not only content and referential structure, but prosodic structure as well [54].
Reference: [40] <author> B. J. Grosz and C. L. Sidner. </author> <title> Attentions, intentions, and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 3(12) </volume> <pages> 175-204, </pages> <year> 1986. </year>
Reference-contexts: True speech understanding requires that individual utterance meanings be understood in the context of the larger dialogue structure <ref> [2, 40] </ref>. This structure must co-ordinate a variety of information, including the ultimate goals of the interaction, the subgoals being attempted, the status of the system knowledge base, models of user knowledge, and a history of the interaction.
Reference: [41] <author> G. Guy. </author> <title> Variation in the group and in the individual: the case of final stop deletion. </title> <editor> In W. Labov, editor, </editor> <title> Locating Language in Time and Space. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [42] <author> M. A. K. Halliday. </author> <title> Language as a Social Semiotic: The Social Interpretation of Language and Meaning. </title> <publisher> University Park Press, </publisher> <address> Baltimore, MD, </address> <year> 1978. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [43] <author> S. Hamlet. </author> <title> Handedness and articulatory asymmetries in /s/ and /l/. </title> <journal> Journal of Phonetics, </journal> <volume> 15 </volume> <pages> 191-195, </pages> <year> 1987. </year> <month> 38 </month>
Reference: [44] <author> J. Hampshire and A. Waibel. </author> <title> The meta-pi network: Connectionist rapid adaptation for high-performance multi-speaker phoneme recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Until quite recently, modeling of speaker-dependencies was largely ignored and speaker-independent recognition was accomplished by training on large amounts of data from many different talkers. Recently, some systems have begun to model specific talker characteristics in a more explicit way, in the context of research on speaker adaptation <ref> [44, 141, 108] </ref>. Progress in modeling speaker differences can benefit from basic research on talker characteristics arising from anatomical differences from fields as diverse as speech physiology, phonetics, psychoacoustics, and speech synthesis (e.g., [12, 81, 129, 32, 91, 43, 130, 87, 90, 102, 61, 65, 64], etc.).
Reference: [45] <author> B.A. Hanson and H. Wakita. </author> <title> Spectral slope distance measures with linear prediction analysis for word recognition in noise. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 35(7) </volume> <pages> 968-973, </pages> <year> 1987. </year>
Reference: [46] <author> H. Hermansky. </author> <title> Perceptual linear predictive PLP analysis for speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87(4) </volume> <pages> 1738-1752, </pages> <year> 1990. </year>
Reference-contexts: Incorporating these results into recognition systems could lead to new models and representations, with implications not only for speech recognition but also for talker verification and identification. Recent works indicate that even a simple engineering models of human speech perception <ref> [46, 49] </ref> could alleviate some of the dependent variability. <p> The development of auditory models for speech processing is still at an early stage, but 6 current work in the application of some of these models to automated speech recognition appears quite promising. Some recently developed speech analysis techniques attempt to model the basic properties of human speech perception <ref> [109, 34, 75, 46, 50] </ref>.
Reference: [47] <author> H. Hermansky, N. Morgan, A. Bayya, and P. Kohn. </author> <title> Compensation for the effects of the communication channel in auditory-like analysis of speech. </title> <booktitle> In Proceedings of the 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1367-1370, </pages> <address> Genova, Italy, </address> <month> September </month> <year> 1991. </year>
Reference: [48] <author> H. Hermansky, N. Morgan, A. Bayya, and P. Kohn. </author> <title> RASTA-PLP speech analysis technique. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech, and Signal Processing, pages I:121-124. IEEE, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: At present, speech recognition systems are not very robust. Their performance degrades suddenly and significantly with modifications as minor as a change in microphone or telecommunication channel <ref> [48] </ref>. Systems trained in the laboratory fail when exposed to operating conditions in the field [146]. <p> This is a most difficult problem due to the spectral similarity of the interference. Many current recognizers require a specific microphone for good performance. Research is now underway to understand how to adapt a system trained on one microphone to perform at full capability using a different microphone/environment <ref> [48, 52, 119] </ref>. 5. Communication channel. Speech recognition over telephone channels is imperative. <p> These techniques can provide significant improvement of recognition robustness by alleviating some of non-linguistic sources of variability in speech, such as differences due to talkers, differences in the acoustic environment and in background noise, or overall spectral differences due to a change of microphone or microphone position <ref> [48, 52, 13] </ref>. Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., [72, 126, 120]).
Reference: [49] <author> Hynek Hermansky and David J. </author> <title> Broad. The effective second formant f2' and the vocal tract front-cavity. </title> <booktitle> In Proceedings of the 1989 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 480-483. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: Incorporating these results into recognition systems could lead to new models and representations, with implications not only for speech recognition but also for talker verification and identification. Recent works indicate that even a simple engineering models of human speech perception <ref> [46, 49] </ref> could alleviate some of the dependent variability.
Reference: [50] <author> Hynek Hermansky, Nelson Morgan, and Hans-Gunter Hirsch. </author> <title> Recognition of speech in additive and convolutional noise based on rasta processing. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech, and Signal Processing, pages II:83-86. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: When communicating over current telephone channels, many previously ignored phenomena such as echoes, noise, nonlinearities, and spectral distortions arise and need to be addressed. Recent work indicates that at least partial alleviation of some of these effects during speech feature extraction is possible (see e.g. <ref> [50] </ref>). 6. Models of human speech perception. It is reasonable to assume that the properties of human auditory perception have influenced the coding of linguistic information in the speech signal. <p> The development of auditory models for speech processing is still at an early stage, but 6 current work in the application of some of these models to automated speech recognition appears quite promising. Some recently developed speech analysis techniques attempt to model the basic properties of human speech perception <ref> [109, 34, 75, 46, 50] </ref>.
Reference: [51] <author> D. Hindle. </author> <title> The Social and Situational Conditioning of Phonetic Variation. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1980. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [52] <author> H. G. Hirsch, P. Meyer, and H. W. Ruehl. </author> <title> Improved speech recognition using high-pass filtering of subband envelopes. </title> <booktitle> In Proceedings of 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 413-416, </pages> <address> Genova, Italy, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: This is a most difficult problem due to the spectral similarity of the interference. Many current recognizers require a specific microphone for good performance. Research is now underway to understand how to adapt a system trained on one microphone to perform at full capability using a different microphone/environment <ref> [48, 52, 119] </ref>. 5. Communication channel. Speech recognition over telephone channels is imperative. <p> These techniques can provide significant improvement of recognition robustness by alleviating some of non-linguistic sources of variability in speech, such as differences due to talkers, differences in the acoustic environment and in background noise, or overall spectral differences due to a change of microphone or microphone position <ref> [48, 52, 13] </ref>. Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., [72, 126, 120]).
Reference: [53] <author> J. Hirschberg. </author> <title> Towards a redefinition of yes/no question. </title> <booktitle> In 22nd Annual Meeting of the ACL, </booktitle> <institution> Stanford University, Stanford, CA, 1983. Association for Computational Linguistics. </institution>
Reference-contexts: These efforts ignore the results of natural language research in the early 80's which showed why such an approach is inadequate <ref> [63, 53, 80, 83, 85] </ref>. In any interactive situation, a system must be able to interpret input and take some action that achieves what the speaker intended. Without a response generation component, this must be an action that the underlying back-end application system can carry out.
Reference: [54] <author> J. Hirschberg and B. Gross. </author> <title> Intonational features of local and global discourse. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth. This nesting affects not only content and referential structure, but prosodic structure as well <ref> [54] </ref>. In order to understand and participate in conversational interaction, the dialogue/subdialogue structure must be correctly understood and modeled. 2. Using Dialogue Structure in Speech Recognition. The dialogue model provides, at each instant of time, a powerful expectation of what is to be said next.
Reference: [55] <author> J. Hirschberg and B. Grosz. </author> <title> Intonational features of local and global discourse structure. </title> <booktitle> In Proceedings of the Fifth DARPA Workshop on Speech and Natural Language, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., <ref> [8, 7, 55] </ref>) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences. Speaker differences arise from two different sources: a) differences in anatomy of speech production organs, b) differences in acquired speech production skills.
Reference: [56] <author> J. Hirschberg and G. Ward. </author> <title> The influence of pitch range, duration, amplitude, and spectral features on the interpretation of rise-fall-rise intonation contour in English. </title> <journal> Journal of Phonetics, </journal> <volume> 20 </volume> <pages> 241-251, </pages> <year> 1992. </year> <month> 39 </month>
Reference: [57] <author> L. Hirschman and C. Pao. </author> <title> The cost of errors in a spoken language system. </title> <booktitle> In Eurospeech '93 Proceedings, </booktitle> <volume> volume 2, </volume> <pages> pages 1419-1422, </pages> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: When the system does make a mistake, how should the system present its response, to help the user diagnose a possible system misunderstanding? What is the cost of an error <ref> [57] </ref>? Graceful error handling, clarification dialogue and detection and correction of presupposition failures are critical features for a spoken language system. 6. Generation of Appropriate Output. Another important part of a dialogue system is its output generation facility [78, 76, 84].
Reference: [58] <author> H.-W. Hon and K.-F. Lee. </author> <title> Vocabulary learning and environment normalization in vocabulary-independent speech recognition. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages I 485-488. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: Better Use of Training Data. One approach to the problem of collecting new training data for each application is task-independent vocabulary modeling <ref> [58] </ref>. This approach could be extended to task-independent language modeling, and rapid adaptation to new domains using task-independent data supplemented by only a small sample of task-specific data, as suggested by recent work in cache-based language modeling [68]. 2. The "New Word" Problem.
Reference: [59] <author> A. Hughes and P. Trudgill. </author> <title> English Accents and Dialects: An Introduction to Social and Regional Varieties of British English. </title> <editor> E. Arnold, </editor> <address> London, </address> <year> 1979. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [60] <author> K.-I. Iso. </author> <title> Speech recognition using dynamical model of speech production. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech and Signal Processing, pages II:283-286, </booktitle> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Two classes of approaches have been developed along these lines. On the one hand, researchers have modeled speech production as a generic dynamic system <ref> [60] </ref>. Standard tools from system identification are then used in an attempt to derive a mathematical description of this process. On the other hand, the known properties of human articulatory dynamics can be used to constrain the mathematical description of the system dynamics [111, 107, 17].
Reference: [61] <author> K. Johnson. </author> <title> The role of perceived speaker identity in F0 normalization of vowels. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 88 </volume> <pages> 642-654, </pages> <year> 1990. </year>
Reference: [62] <author> B. H. Juang and L. R. Rabiner. </author> <title> Signal restoration by spectral mapping. </title> <booktitle> In Proceedings of the 1987 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 2368-2371. </pages> <publisher> IEEE, </publisher> <year> 1987. </year>
Reference: [63] <author> S. J. Kaplan. </author> <title> Cooperative responses from a portable natural language query system. </title> <journal> Artificial Intelligence, </journal> <volume> 19(2), </volume> <year> 1982. </year>
Reference-contexts: These efforts ignore the results of natural language research in the early 80's which showed why such an approach is inadequate <ref> [63, 53, 80, 83, 85] </ref>. In any interactive situation, a system must be able to interpret input and take some action that achieves what the speaker intended. Without a response generation component, this must be an action that the underlying back-end application system can carry out.
Reference: [64] <author> I. Karlsson. </author> <title> Female voices in speech synthesis. </title> <journal> Journal of Phonetics, </journal> <volume> 19 </volume> <pages> 111-120, </pages> <year> 1991. </year>
Reference: [65] <author> D. Klatt and L. Klatt. </author> <title> Analysis, synthesis and perception of voice quality variations among male and female talkers. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87 </volume> <pages> 820-857, </pages> <year> 1990. </year>
Reference: [66] <author> D. H. Klatt. </author> <title> Review of text-to-speech conversion for English. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 3(82) </volume> <pages> 737-793, </pages> <year> 1987. </year>
Reference-contexts: In human-machine interactions, prosodic cues may provide valuable information for computational models with limited semantic knowledge, even though the cues may be only redundant information for human listeners with a detailed knowledge of the world. In 10 addition, prosody is a limiting factor in speech synthesis applications <ref> [66] </ref>. There are three major reasons for the problems of current models: first, the hand-crafting of language understanding systems leads to a competence-based model rather than a performance-based model. Second, we do not understand well how to treat various phenomena as information rather than noise. <p> few particularly important research areas are: models of the physics of sound generation in the human vocal apparatus, models of articulation for synthesizing phonetic segments, theories of the relationship between prosody and syntax/semantics for predicting abstract prosodic patterns, and models of intonation and duration for interpreting those prosodic patterns acoustically <ref> [66, 27] </ref>. 2. Computational Models of Variability. Explicit models of variability are needed in synthesis to avoid monotony, an issue both for synthesis of long monologues and long human-computer interactive sessions.
Reference: [67] <author> A. Kobsa and Eds W. Wahlster. </author> <title> User Models in Dialogue Systems. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Incorporating a Model of the User. A key aspect of a dialogue system is its model of the user <ref> [29, 67, 97] </ref>. Processes of input recognition, output generation, and internal decision making all depend on user modelling. Word usage, grammatical constructions, and transmitted meanings will differ for users of different backgrounds and different levels of expertise.
Reference: [68] <author> R. Kuhn and R. DeMori. </author> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 12(6) </volume> <pages> 570-583, </pages> <month> Jun </month> <year> 1990. </year>
Reference-contexts: This approach could be extended to task-independent language modeling, and rapid adaptation to new domains using task-independent data supplemented by only a small sample of task-specific data, as suggested by recent work in cache-based language modeling <ref> [68] </ref>. 2. The "New Word" Problem. The occurrence of unknown or out-of-vocabulary words is one of the major problems frustrating the use of automatic speech understanding systems in real world tasks.
Reference: [69] <author> W. Labov. </author> <title> Language in the Inner City; Studies in the Black English Vernacular. </title> <publisher> University of Pennsylvania Press, </publisher> <address> Philadelphia, </address> <year> 1972. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [70] <author> W. Labov. </author> <title> Sources of inherent variation in speech. </title> <editor> In J. S. Perkell and D. H. Klatt, editors, </editor> <booktitle> Invariance and Variability in Speech Processes, </booktitle> <pages> pages 402-423. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: Recent works indicate that even a simple engineering models of human speech perception [46, 49] could alleviate some of the dependent variability. Additional research is also required to understand dialect differences and other effects of social context on speech production (e.g., <ref> [69, 42, 41, 51, 70, 59, 11, 18] </ref>) Incorporating explicit representations of phonological differences among dialects could vastly reduce the amount of training data required for training of a robust recognizer.
Reference: [71] <author> W. Levelt. </author> <title> Monitoring and self-repair in speech. </title> <journal> Cognition, </journal> <volume> 14 </volume> <pages> 41-104, </pages> <year> 1983. </year> <month> 40 </month>
Reference-contexts: For example, repairs occur with some frequency (around 6 percent of sentences in rather planned spontaneous data such as ATIS [113], and in 34 percent of sentences in a human-human dialogue corpus <ref> [71] </ref>), but occur more rarely in read material. Such repairs are easily recognized by humans, but our current spoken language models are not rich enough to handle them. Prosody is another important component of spoken language that is not well represented in written language.
Reference: [72] <author> A.M. Liberman and I.G. Mattingly. </author> <title> The motor theory of speech perception revised. </title> <journal> Cognition, </journal> <volume> 21 </volume> <pages> 1-36, </pages> <year> 1985. </year>
Reference-contexts: Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., <ref> [72, 126, 120] </ref>). Understanding human speech perception is an important step in the development of spoken language systems. 7. Confidence and Rejection. In real applications, a speech recognition system must deal with unexpected or unusual input.
Reference: [73] <author> C. Linde and J. Goguen. </author> <title> Structure of planning discourse. </title> <journal> Journal of Social Biol. Structure, </journal> <volume> 1 </volume> <pages> 219-251, </pages> <year> 1978. </year>
Reference-contexts: Discovering the Structure of Dialogue. Typical dialogues are usually organized into a series of subdialogues each of which is aimed at solving a particular subgoal <ref> [39, 73, 104] </ref>. The individual subdialogues provide what is called "focus" [39], and the tracking of subdialogues is called "plan recognition" [3]. The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth.
Reference: [74] <author> J. S. Logan, B. G. Greene, and D. B. Pisoni. </author> <title> Segmental intelligibility of synthetic speech produced by ten text-to-speech systems. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 86 </volume> <pages> 566-581, </pages> <year> 1986. </year>
Reference-contexts: Speech synthesis is not a solved problem. Synthetic speech is not as intelligible or "acceptable" as natural speech, particularly for cases where language redundancy plays less of a role (e.g., in difficult material or unfamiliar names) or in lower quality audio environments <ref> [74] </ref>. The quality of current text-to-speech systems is a limiting factor in many applications, especially those where extensive output is required. As for industry funding, the results are not generally in the public domain, and consequently speech research has suffered.
Reference: [75] <author> R. F. Lyon and C. Mead. </author> <title> An analog electronic cochlea. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 36(7) </volume> <pages> 1119-1134, </pages> <year> 1988. </year>
Reference-contexts: The development of auditory models for speech processing is still at an early stage, but 6 current work in the application of some of these models to automated speech recognition appears quite promising. Some recently developed speech analysis techniques attempt to model the basic properties of human speech perception <ref> [109, 34, 75, 46, 50] </ref>.
Reference: [76] <author> D. D. MacDonald. </author> <title> Natural language generation as a computational problem: An introduction. </title> <editor> In M. Brady and R.C. Berwick, editors, </editor> <title> Computational Models of Discourse. </title> <editor> M. I. T. </editor> <publisher> Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: Generation of Appropriate Output. Another important part of a dialogue system is its output generation facility <ref> [78, 76, 84] </ref>. This may be in a typed, voiced, or graphic mode, and its purpose is to enunciate the machine's portion of the interaction as dictated by the dialogue processor.
Reference: [77] <author> A. Malhotra. </author> <title> Design criteria for a knowledge-based English language system for management: an experimental analysis. </title> <type> Technical Report MAC TR-146, </type> <institution> MIT, </institution> <year> 1975. </year>
Reference-contexts: For example, it has been shown that users would like to ask questions about the type of information available in the underlying database, or questions requesting the definition of terms, or questions about the differences between concepts <ref> [77, 128] </ref>. These questions cannot be answered unless the system includes facilities to determine what information to include. Given that this information does not directly mirror the user's question, the system also needs to determine how to phrase the information in language.
Reference: [78] <author> W. C. Mann and J. A. Moore. </author> <title> Computer generation of multiparagraph English text. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 1(7), </volume> <year> 1981. </year>
Reference-contexts: Generation of Appropriate Output. Another important part of a dialogue system is its output generation facility <ref> [78, 76, 84] </ref>. This may be in a typed, voiced, or graphic mode, and its purpose is to enunciate the machine's portion of the interaction as dictated by the dialogue processor.
Reference: [79] <author> D. Mansour and B.H. Juang. </author> <title> A family of distortion measures based upon projection operation for robust speech recognition. </title> <booktitle> In Proceedings of the 1988 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 36-39. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference: [80] <author> K. F. McCoy. </author> <title> The ROMPER system: Responding to object-related misconceptions using perspective. </title> <booktitle> In 24th Annual Meeting of the ACL. Association of Computational Linguistics, </booktitle> <address> New York City, NY, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: These efforts ignore the results of natural language research in the early 80's which showed why such an approach is inadequate <ref> [63, 53, 80, 83, 85] </ref>. In any interactive situation, a system must be able to interpret input and take some action that achieves what the speaker intended. Without a response generation component, this must be an action that the underlying back-end application system can carry out.
Reference: [81] <author> M. McCutcheon, A. Hasegawa, and S. Fletcher. </author> <title> Effects of palatal morphology on [s,z] articulation. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 67:S94, </volume> <year> 1980. </year>
Reference: [82] <author> H. McGurk and J. MacDonald. </author> <title> Hearing lips and seeing voices. </title> <journal> Nature, </journal> <volume> 264 </volume> <pages> 746-748, </pages> <year> 1976. </year>
Reference-contexts: In other cases, adequate recognition of spoken language could be supported with handwriting, graphics, or contextual information in virtual environments. One clear experimental demonstration of how visual cues are integrated with auditory ones during speech recognition is provided by the "McGurk effect <ref> [82] </ref>." During this effect, a person observes a videotaped face saying "ga" while listening to "ba" on a soundtrack.
Reference: [83] <author> K. R. McKeown. </author> <title> Text Generation. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1985. </year>
Reference-contexts: These efforts ignore the results of natural language research in the early 80's which showed why such an approach is inadequate <ref> [63, 53, 80, 83, 85] </ref>. In any interactive situation, a system must be able to interpret input and take some action that achieves what the speaker intended. Without a response generation component, this must be an action that the underlying back-end application system can carry out.
Reference: [84] <author> K.R. McKeown. </author> <title> Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1985. </year>
Reference-contexts: Generation of Appropriate Output. Another important part of a dialogue system is its output generation facility <ref> [78, 76, 84] </ref>. This may be in a typed, voiced, or graphic mode, and its purpose is to enunciate the machine's portion of the interaction as dictated by the dialogue processor.
Reference: [85] <author> K.R. McKeown and W. R. Swartout. </author> <title> Language generation and explanation. </title> <editor> In J.F. Traub et al., editor, </editor> <booktitle> Annual Review of Computer Science. </booktitle> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: These efforts ignore the results of natural language research in the early 80's which showed why such an approach is inadequate <ref> [63, 53, 80, 83, 85] </ref>. In any interactive situation, a system must be able to interpret input and take some action that achieves what the speaker intended. Without a response generation component, this must be an action that the underlying back-end application system can carry out.
Reference: [86] <author> G.A. Miller, C. Leacock, R. Tengi, </author> <title> and R.T. Bunker. A semantic concordance. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, NJ, </address> <month> March </month> <year> 1993. </year> <month> 41 </month>
Reference-contexts: To decrease the cost of portability, we must find ways to utilize existing repositories of "expert information", such as thesauri and lexicons, and semantic representations such as WordNet <ref> [86] </ref>. Under the Consortium for Lexical Research 1 and the Linguistic Data Consortium 2 , these resources are being made widely available, but we need further research on how to extract and utilize the information contained in these resources.
Reference: [87] <author> J. M. Mullenix, D. B. Pisoni, and C. S. Martin. </author> <title> Some effects of talker variability on spoken word recognition. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 85 </volume> <pages> 365-378, </pages> <year> 1989. </year>
Reference: [88] <author> A. Nadas, D. Nahamoo, and M. Picheny. </author> <title> Adaptive labeling: Normalization of speech by adaptive transformations based on vector quantization. </title> <booktitle> In Proceedings of the 1988 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 521-524. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference: [89] <author> A. Nadas, D. Nahamoo, and M.A. Picheny. </author> <title> Speech recognition using noise-adaptive prototypes. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(10) </volume> <pages> 1495-1503, </pages> <year> 1989. </year>
Reference: [90] <author> T. M. Neary. </author> <title> Static, dynamic and relational properties in vowel perception. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 85 </volume> <pages> 2088-2113, </pages> <year> 1989. </year>
Reference: [91] <author> F. Nolan. </author> <title> The Phonetic Basis of Speaker Recognition. </title> <publisher> Cambridge University Press, </publisher> <year> 1983. </year>
Reference: [92] <author> F. Nolan and P. E. Kerswill. </author> <title> The description of connected speech processes. </title> <editor> In S. Ramsaran, editor, </editor> <booktitle> Studies in the Pronunciation of English: A Commemorative Volume in Honour of A. </booktitle> <address> C. </address> <publisher> Gibson. Routledge, </publisher> <year> 1990. </year>
Reference-contexts: Explicit modeling of such dialect variation at the phonetic level will be particularly important if it turns out, as suggested by <ref> [92] </ref>, that patterns of 5 coarticulatory variation across word boundaries can differ from one dialect to another. 4. Acoustic Environment and Microphone. It is widely accepted that users do not want to be encumbered by a headmounted or hand held microphone when communicating with a machine.
Reference: [93] <author> D.J. Ostry and K. G. Munhall. </author> <title> Control of rate and duration of speech movements. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 77 </volume> <pages> 640-648, </pages> <year> 1985. </year>
Reference-contexts: Modeling Speech Rate. Effects of tempo are poorly understood. What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., <ref> [33, 93, 133] </ref>), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances
Reference: [94] <author> S. L. Oviatt, P. R. Cohen, M. W. Fong, and M. P. Frank. </author> <title> A rapid semi-automatic simulation technique for investigating interactive speech and handwriting. </title> <editor> In J. Ohala et al., editor, </editor> <booktitle> Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume II, </volume> <pages> pages 1351-1354, </pages> <institution> University of Alberta, Canada, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition, since multimodal systems are relatively complex, the problem of how to design successful configurations is unlikely to be solved through a simple intuitive approach. Instead, determining optimal designs and appropriate applications for different types of multimodal systems will require interdisciplinary research, preferably based on advance simulations <ref> [94, 95] </ref>. There are many potential advantages of well designed multimodal systems. One is the support of robust system performance under adverse conditions. For example, adequate recognition of spoken language could be maintained in a noisy environment with supplementary visual information about corresponding lip movements.
Reference: [95] <author> S. L. Oviatt, P. R. Cohen, M. Wang, and J. Gaston. </author> <title> A simulation-based research strategy for designing complex NL systems. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, NJ, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: In addition, since multimodal systems are relatively complex, the problem of how to design successful configurations is unlikely to be solved through a simple intuitive approach. Instead, determining optimal designs and appropriate applications for different types of multimodal systems will require interdisciplinary research, preferably based on advance simulations <ref> [94, 95] </ref>. There are many potential advantages of well designed multimodal systems. One is the support of robust system performance under adverse conditions. For example, adequate recognition of spoken language could be maintained in a noisy environment with supplementary visual information about corresponding lip movements.
Reference: [96] <author> D. Pallett, J. Fiscus, W. Fisher, and J. Garofolo. </author> <title> Benchmark tests for the DARPA spoken language program. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: These systems operate in near real-time, accepting spontaneous, continuous speech from speakers with no prior enrollment; they have vocabularies of 1000-2000 words, and an overall correct understanding rate of almost 90% <ref> [96, 5, 110, 4, 26] </ref>. Although progress over the past decade has been impressive, there are significant obstacles to be overcome before spoken language systems can reach their full potential. <p> These corpora have also been associated with standardized evaluations of the component technologies <ref> [96, 24] </ref>. Although the use of regular "common evaluations" originated in the ARPA community, it has spread to the broader community and provides periodic measurements of progress of the field over time, as well as making it possible for individual systems to evaluate their internal progress.
Reference: [97] <author> C. L. Paris. </author> <title> Tailoring object descriptions to a user's level of expertise. </title> <journal> Computational Linguistics, </journal> <volume> 3(14), </volume> <year> 1988. </year>
Reference-contexts: Incorporating a Model of the User. A key aspect of a dialogue system is its model of the user <ref> [29, 67, 97] </ref>. Processes of input recognition, output generation, and internal decision making all depend on user modelling. Word usage, grammatical constructions, and transmitted meanings will differ for users of different backgrounds and different levels of expertise.
Reference: [98] <author> A. Pentland and K. Mase. </author> <title> Lip reading: Automatic visual recognition of spoken words. </title> <booktitle> In Proceedings of Image Understanding and Machine Vision. Optical Society of America, </booktitle> <month> June 12-14 </month> <year> 1989. </year>
Reference-contexts: Inspired by these empirical results, computationalists have begun attempting to integrate auditory and visual information to improve the accuracy and robustness of speech recognition, with encouraging results <ref> [99, 100, 98, 145, 144] </ref>. For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only [121].
Reference: [99] <author> E. D. Petajan. </author> <title> Automatic lipreading to enhance speech recognition. </title> <booktitle> In Proceedings of the IEEE Communication Society Global Telecommunications Conference, </booktitle> <pages> pages 26-29, </pages> <address> Atlanta, GA, </address> <month> November </month> <year> 1984. </year> <month> 42 </month>
Reference-contexts: Inspired by these empirical results, computationalists have begun attempting to integrate auditory and visual information to improve the accuracy and robustness of speech recognition, with encouraging results <ref> [99, 100, 98, 145, 144] </ref>. For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only [121].
Reference: [100] <author> E. D. Petajan, B. Bischoff, and D. Bodoff. </author> <title> An improved automatic lipreading system to enhance speech recognition. </title> <booktitle> In Proceedings of the ACM SIGCHI-88, </booktitle> <pages> pages 19-25, </pages> <year> 1988. </year>
Reference-contexts: Inspired by these empirical results, computationalists have begun attempting to integrate auditory and visual information to improve the accuracy and robustness of speech recognition, with encouraging results <ref> [99, 100, 98, 145, 144] </ref>. For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only [121].
Reference: [101] <author> J. E. Porter and S. F. Boll. </author> <title> Optimal estimators for spectral restoration of noisy speech. </title> <booktitle> In Proceedings of the 1984 International Conference on Acoustics, Speech and Signal Processing, pages 18A.2.1-2.4. IEEE, </booktitle> <year> 1984. </year>
Reference: [102] <author> P. Price. </author> <title> Male and female voice source characteristics: Inverse filtering results. </title> <journal> Speech Communication, </journal> <volume> 8 </volume> <pages> 261-277, </pages> <year> 1989. </year>
Reference: [103] <author> P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and C. Fong. </author> <title> The use of prosody in syntactic disambiguation. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 90 </volume> <pages> 2956-2970, </pages> <year> 1991. </year>
Reference-contexts: What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., <ref> [122, 19, 103] </ref>); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences. <p> Finally, such 11 models need to be integrated into architectures for spoken language processing, in both speech understanding and generation components. 2. Prosodics We know that prosody can provide information that helps humans understand speech (e.g., in read speech by radio announcers <ref> [103] </ref>). We also know that prosody of read speech differs from spontaneous speech [16].
Reference: [104] <author> R. Reichman. </author> <title> Getting Computers to Talk Like You and Me. </title> <editor> M. I. T. </editor> <publisher> Press, </publisher> <year> 1985. </year>
Reference-contexts: Discovering the Structure of Dialogue. Typical dialogues are usually organized into a series of subdialogues each of which is aimed at solving a particular subgoal <ref> [39, 73, 104] </ref>. The individual subdialogues provide what is called "focus" [39], and the tracking of subdialogues is called "plan recognition" [3]. The relationships between the subdialogues are often quite complex, some being nested within others, some being functionally disjoint from others, and so forth.
Reference: [105] <author> B. Repp. </author> <title> Phonetic trading relations and context effects: New experimental evidence for a speech mode of perception. </title> <journal> Psychological Bulletin, </journal> <volume> 92 </volume> <pages> 81-110, </pages> <year> 1982. </year>
Reference-contexts: Modeling Coarticulation and Phonetic Context. The spectral characteristics of a sound segment vary tremendously from one linguistic context to another (see <ref> [105] </ref> for a partial overview). At present, coarticulatory variability due to segmental context is mostly accounted for by context-dependent Hidden Markov Models; this vastly increases the number of classes on which a recognizer must be trained. More explicit models of coarticulation could improve this situation.
Reference: [106] <author> D. Roe, F. Pereira, and R. Sproat. </author> <title> Efficient grammar processing for a spoken language translation system. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <publisher> page I.213. IEEE, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: Spoken Language Translation. This is the grandest of the challenges, encompassing all the above challenges plus a machine translation capability. Initial advances in this direction are indeed in progress <ref> [135, 106, 25] </ref>, but considerable additional research will be necessary to 21 achieve complete widely usable and robust speech translation systems.
Reference: [107] <author> J. Schroeter and M. M. Sondhi. </author> <title> Speech coding based on physiological models of speech production. </title> <editor> In S. Furui and M. M. Sondhi, editors, </editor> <booktitle> Advances in Speech Signal Processing, </booktitle> <pages> pages 231-268, </pages> <address> New York, 1991. </address> <publisher> Dekker. </publisher>
Reference-contexts: Standard tools from system identification are then used in an attempt to derive a mathematical description of this process. On the other hand, the known properties of human articulatory dynamics can be used to constrain the mathematical description of the system dynamics <ref> [111, 107, 17] </ref>. In both classes of systems one attempts to achieve more robust speech recognition by first deducing a certain amount of information regarding the dynamics underlying speech production. 2. Modeling Speech Rate. Effects of tempo are poorly understood.
Reference: [108] <author> R. Schwartz, Y. Chow, and F. Kubala. </author> <title> Rapid speaker adaption using a probabilistic spectral mapping. </title> <booktitle> In Proceedings of the 1987 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 633-636. </pages> <publisher> IEEE, </publisher> <year> 1987. </year>
Reference-contexts: Until quite recently, modeling of speaker-dependencies was largely ignored and speaker-independent recognition was accomplished by training on large amounts of data from many different talkers. Recently, some systems have begun to model specific talker characteristics in a more explicit way, in the context of research on speaker adaptation <ref> [44, 141, 108] </ref>. Progress in modeling speaker differences can benefit from basic research on talker characteristics arising from anatomical differences from fields as diverse as speech physiology, phonetics, psychoacoustics, and speech synthesis (e.g., [12, 81, 129, 32, 91, 43, 130, 87, 90, 102, 61, 65, 64], etc.).
Reference: [109] <author> S. Seneff. </author> <title> A joint synchrony/mean-rate model of auditory speech processing. </title> <journal> Journal of Phonetics, </journal> <volume> 16(1) </volume> <pages> 55-76, </pages> <year> 1988. </year>
Reference-contexts: The development of auditory models for speech processing is still at an early stage, but 6 current work in the application of some of these models to automated speech recognition appears quite promising. Some recently developed speech analysis techniques attempt to model the basic properties of human speech perception <ref> [109, 34, 75, 46, 50] </ref>.
Reference: [110] <author> S. Seneff. </author> <title> A relaxation method for understanding spontaneous utterances. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: These systems operate in near real-time, accepting spontaneous, continuous speech from speakers with no prior enrollment; they have vocabularies of 1000-2000 words, and an overall correct understanding rate of almost 90% <ref> [96, 5, 110, 4, 26] </ref>. Although progress over the past decade has been impressive, there are significant obstacles to be overcome before spoken language systems can reach their full potential.
Reference: [111] <author> K. Shirai and T. Kobayashi. </author> <title> Estimating articulatory motion from speech wave. </title> <journal> Speech Communication, </journal> <volume> 5(2) </volume> <pages> 159-170, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Standard tools from system identification are then used in an attempt to derive a mathematical description of this process. On the other hand, the known properties of human articulatory dynamics can be used to constrain the mathematical description of the system dynamics <ref> [111, 107, 17] </ref>. In both classes of systems one attempts to achieve more robust speech recognition by first deducing a certain amount of information regarding the dynamics underlying speech production. 2. Modeling Speech Rate. Effects of tempo are poorly understood.
Reference: [112] <author> E. Shortliffe. </author> <title> Computer-Based Medical Consultations. </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Similarly, expert system explanation is another application where it has been shown [127] that a simple "translation" of the underlying inference trace (as is often done using templates <ref> [112] </ref>), does not produce a satisfactory explanation of the system's reasoning.
Reference: [113] <author> E. Shriberg, J. Bear, and J. Dowding. </author> <title> Automatic detection and correction of repairs in computer-human dialog. </title> <booktitle> In DARPA Workshop on Speech and Natural Language Processing, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: We know that there are regularities associated with spontaneous speech phenomena that do not appear as frequently in read speech. For example, repairs occur with some frequency (around 6 percent of sentences in rather planned spontaneous data such as ATIS <ref> [113] </ref>, and in 34 percent of sentences in a human-human dialogue corpus [71]), but occur more rarely in read material. Such repairs are easily recognized by humans, but our current spoken language models are not rich enough to handle them.
Reference: [114] <author> H. F. Silverman. </author> <title> Some analysis of microphone arrays for speech data acquisition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(2):1699-1712, </volume> <month> December </month> <year> 1987. </year> <month> 43 </month>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [115] <author> H. F. Silverman and S. E. Kirtman. </author> <title> A two-stage algorithm for determining talker location from linear microphone-array data. </title> <booktitle> Computer, Speech, and Language, </booktitle> <volume> 6(2) </volume> <pages> 129-152, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [116] <author> H. F. Silverman, S. E. Kirtman, J. E. Adcock, and P. C. Meuse. </author> <title> Experimental results for baseline speech recognition performance using input acquired from a linear microphone array. </title> <booktitle> In Proceedings of the Fifth DARPA Workshop on Speech and Natuaral Language, </booktitle> <address> Arden House, Harriman, NY, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Thus, it is highly desirable to have a remote microphone attached to the system, which can track a talker and maintain consistently high signal quality. Multiple microphone systems offer the prospect of being able of tracking a remote talker <ref> [30, 114, 15, 6, 31, 115, 116] </ref>. This approach has yielded some success, but considerable work is still necessary to handle speech acquisition in a free space. Some of the hardest problems arise from reverberation.
Reference: [117] <author> K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, C. Wightman, P. Price, J. Pierrehumbert, and J. Hirschberg. </author> <title> TOBI: A standard for labeling english prosody. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Systems, </booktitle> <volume> volume II, </volume> <pages> pages 867-870, </pages> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Efforts are currently underway to create a standard [20 notation for prosody, so that training and evaluation materials can be prepared <ref> [117] </ref>, but this is only a first towards a corpus of prosodically labelled spontaneous speech. 2.4 Dialogue Models Dialogue processing is the enabling technology for spoken language systems.
Reference: [118] <author> R. W. Smith, D. R. Hipp, and A. W. Biermann. </author> <title> A dialogue control algorithm and its performance. </title> <booktitle> Third Conference on Applied Natural Language Processing, </booktitle> <address> March 31 - April 3 1992. Trento, Italy. </address>
Reference-contexts: This model of speech understanding could reduce perplexity and provide improved error correction [143]. 3. Building a Variable Initiative Capability Into the Processor. The possibility of moving from subdialogue to subdialogue in nearly arbitrary ways leads to the question of who controls these transitions <ref> [118] </ref>. The answer is that an efficient dialogue capability requires that either participant be able to take control. If one participant, machine or human, has most of the knowledge related to a subtopic, efficiency may require that that entity dictate dialogue transitions to properly guide the interaction to success.
Reference: [119] <author> R. M. Stern, F. Liu, Y. Ohshima, T. M. Sullivan, and A. Acero. </author> <title> Multiple approaches to robust speech recognition. </title> <booktitle> In Proceedings of the Fifth DARPA Workshop on Speech and Natuaral Language, </booktitle> <address> Arden House, Harriman, NY, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: This is a most difficult problem due to the spectral similarity of the interference. Many current recognizers require a specific microphone for good performance. Research is now underway to understand how to adapt a system trained on one microphone to perform at full capability using a different microphone/environment <ref> [48, 52, 119] </ref>. 5. Communication channel. Speech recognition over telephone channels is imperative.
Reference: [120] <author> K.N. Stevens. </author> <title> Phonetic Linguistics, chapter Evidence for the role of acoustic boundaries in the perception of speech sounds, </title> <address> pages 243-255. </address> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., <ref> [72, 126, 120] </ref>). Understanding human speech perception is an important step in the development of spoken language systems. 7. Confidence and Rejection. In real applications, a speech recognition system must deal with unexpected or unusual input.
Reference: [121] <author> D. G. Stork, G. Wolff, and E. Levine. </author> <title> Neural network lipreading system for improved speech recognition. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages II:286-295, </pages> <year> 1992. </year>
Reference-contexts: For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only <ref> [121] </ref>. Goldschen has obtained 25 percent recognition in continuous speech using optical recognition exclusively, without the use of acoustic data or syntax [38]. Such results support the belief that multimodal systems may display more desirable properties, especially under realistic field conditions, than stand-alone spoken language systems.
Reference: [122] <author> W. V. Summers. </author> <title> Effects of stress and final consonant voicing on vowel production: articulatory and acoustic analyses. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 82 </volume> <pages> 847-863, </pages> <year> 1987. </year>
Reference-contexts: What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., [33, 93, 133]), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., <ref> [122, 19, 103] </ref>); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances ([56]). 3. Modeling Speaker Differences.
Reference: [123] <author> B. Sundheim. </author> <title> Overview of the third message understanding evaluation and conference. </title> <booktitle> In Proceedings of the Third Message Understanding Conference MUC-3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In order to measure progress in portability, it is important to find some reasonable metrics that are themselves fairly cheap to implement. This may require new ways of doing system evaluation, since the current evaluation methods measure "understanding" and are expensive to implement for a single domain (cf. MUC <ref> [123] </ref> and ATIS [23]), let alone for multiple domains.
Reference: [124] <author> P. Suppes. </author> <title> Current trends in computer assisted instruction. </title> <editor> In M.C. Yovits, editor, </editor> <booktitle> In Advances in Computers. </booktitle> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: The benefits are perhaps most clear in applications involving information access via telephone, computer training, and aids for the handicapped. In computer training, for example, research has shown that interactions via spoken responses resulted in better learning performance than visual presentation alone in a computerized course for teaching algebra <ref> [124, 125] </ref>. 18 For remote access to computers via telephone, or for telephone information services, spoken responses are currently the only means of communication.
Reference: [125] <author> P. Suppes. </author> <title> University-level computer-assisted instruction at Stanford: </title> <type> 1968-1980. </type> <institution> Publication of Institute for Mathematical Studies in the Social Sciences, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1981. </year>
Reference-contexts: The benefits are perhaps most clear in applications involving information access via telephone, computer training, and aids for the handicapped. In computer training, for example, research has shown that interactions via spoken responses resulted in better learning performance than visual presentation alone in a computerized course for teaching algebra <ref> [124, 125] </ref>. 18 For remote access to computers via telephone, or for telephone information services, spoken responses are currently the only means of communication.
Reference: [126] <author> H.M. Sussman, H.A. McCaffrey, and S.A. Matthews. </author> <title> An investigation of locus equations as a source of relational invariance for stop place categorization. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 90 </volume> <pages> 1309-1325, </pages> <year> 1991. </year>
Reference-contexts: Models of speech perception for processing stages beyond the periphery have been proposed, but little attempt has been made to incorporate concepts of these models into systems for speech recognition (e.g., <ref> [72, 126, 120] </ref>). Understanding human speech perception is an important step in the development of spoken language systems. 7. Confidence and Rejection. In real applications, a speech recognition system must deal with unexpected or unusual input.
Reference: [127] <author> W.R. Swartout. XPLAIN: </author> <title> a system for creating and explaining expert consulting systems. </title> <journal> Artificial Intelligence, </journal> <volume> 3(2) </volume> <pages> 285-325, </pages> <year> 1983. </year> <month> 44 </month>
Reference-contexts: Given that this information does not directly mirror the user's question, the system also needs to determine how to phrase the information in language. Similarly, expert system explanation is another application where it has been shown <ref> [127] </ref> that a simple "translation" of the underlying inference trace (as is often done using templates [112]), does not produce a satisfactory explanation of the system's reasoning.
Reference: [128] <author> H. Tennant. </author> <title> Experience with the evaluation of natural language question answerers. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: For example, it has been shown that users would like to ask questions about the type of information available in the underlying database, or questions requesting the definition of terms, or questions about the differences between concepts <ref> [77, 128] </ref>. These questions cannot be answered unless the system includes facilities to determine what information to include. Given that this information does not directly mirror the user's question, the system also needs to determine how to phrase the information in language.
Reference: [129] <author> H. Traunmuller. </author> <title> Perceptual dimension of openness in vowels. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 69 </volume> <pages> 1465-1475, </pages> <year> 1981. </year>
Reference: [130] <author> D. van Bergem, L. Pols, and F. Koopmans van Beinum. </author> <title> Perceptual normalization of the vowels of a man and a child. </title> <journal> Speech Communication, </journal> <volume> 7 </volume> <pages> 1-20, </pages> <year> 1988. </year>
Reference: [131] <author> A. Varga, R. Moore, J. Bridle, K. Ponting, and M. Russell. </author> <title> Noise compensation algorithms for use with hidden Markov model based speech recognition. </title> <booktitle> In Proceedings of the 1988 International Conference on Acoustics, Speech and Signal Processing. IEEE, </booktitle> <year> 1988. </year>
Reference: [132] <author> A.P. Varga and R.K. Moore. </author> <title> Hidden Markov model decomposition of speech and noise. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 845-848. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference: [133] <author> E. Vatikiotis-Bateson and J. A. S. </author> <title> Kelso. Rhythm type and articulatory dynamics in English, French, and Japanese. </title> <journal> Journal of Phonetics, </journal> <volume> 21, </volume> <year> 1992. </year>
Reference-contexts: Modeling Speech Rate. Effects of tempo are poorly understood. What factors cause speakers to speak more quickly or more slowly? If the effects of tempo on the production process were better understood (see e.g., <ref> [33, 93, 133] </ref>), local changes in speaking rate might be used to recognize such prosodic patterns as stress or phrase-final lengthening (e.g., [122, 19, 103]); more global changes might help parse changes in topic or conversational turn (see e.g., [8, 7, 55]) and even some more intricate pragmatic differences among utterances
Reference: [134] <author> N. Veilleux and M. Ostendorf. </author> <title> Prosody/parse scoring in ATIS. </title> <booktitle> In Proceedings of the Workshop on Human Language Technology, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: We also know that prosody of read speech differs from spontaneous speech [16]. Recent results are beginning to show that use of prosody can aid automated understanding of spontaneous speech <ref> [134] </ref> provided that the system can detect prosodic phenomena reliably [139, 140] and can correlate these prosodic cues with higher level syntactic, semantic, pragmatic, and conversational structures. For example, phrasal prominence may help to detect high-information regions of a discourse, for use in automatic gisting and summarization [10]. <p> Explicit models of variability are needed in synthesis to avoid monotony, an issue both for synthesis of long monologues and long human-computer interactive sessions. In addition, models that can account for variability are more likely to also be useful in speech understanding applications, as demonstrated in <ref> [134, 137] </ref>. 3. Integration of Synthesis and Language Generation. Little work has been done on this problem, and there are many opportunities for exploiting the linguistic information that is a by-product of language generation. Possibilities range from simply increasing the quality to modeling discourse structure to active dialogue control. 4.
Reference: [135] <author> A. Waibel, A. Jain, A. McNair, H. Saito, A. Hauptmann, and J. Tebelskis. </author> <title> JANUS: A speech-to-speech translation system using connectionist and symbolic processing strategies. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Spoken Language Translation. This is the grandest of the challenges, encompassing all the above challenges plus a machine translation capability. Initial advances in this direction are indeed in progress <ref> [135, 106, 25] </ref>, but considerable additional research will be necessary to 21 achieve complete widely usable and robust speech translation systems.
Reference: [136] <author> D. E. Walker, </author> <title> editor. Understanding Spoken Language. </title> <publisher> North Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The currently active subgoal will make very strong predictions, and other locally nonactive subgoals will make weaker predictions. The combination of all the information from the dialogue level can substantially sharpen estimates at for improved recognition <ref> [22, 136, 138] </ref>. This leads to a new formulation of the speech understanding problem. Instead of receiving an acoustic input and passing a meaning to the higher level, the recognizer could receive both the acoustic input and a representation of expected meanings.
Reference: [137] <author> M. Wang and J. Hirschberg. </author> <title> Automatic classification of intonational phrase boundaries. </title> <type> unpublished, </type> <year> 1992. </year>
Reference-contexts: Explicit models of variability are needed in synthesis to avoid monotony, an issue both for synthesis of long monologues and long human-computer interactive sessions. In addition, models that can account for variability are more likely to also be useful in speech understanding applications, as demonstrated in <ref> [134, 137] </ref>. 3. Integration of Synthesis and Language Generation. Little work has been done on this problem, and there are many opportunities for exploiting the linguistic information that is a by-product of language generation. Possibilities range from simply increasing the quality to modeling discourse structure to active dialogue control. 4.
Reference: [138] <author> W. Ward and S. Young. </author> <title> Flexible use of semantic constraints in speech recognition. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech and Signal Processing, pages II:49-50, </booktitle> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: The currently active subgoal will make very strong predictions, and other locally nonactive subgoals will make weaker predictions. The combination of all the information from the dialogue level can substantially sharpen estimates at for improved recognition <ref> [22, 136, 138] </ref>. This leads to a new formulation of the speech understanding problem. Instead of receiving an acoustic input and passing a meaning to the higher level, the recognizer could receive both the acoustic input and a representation of expected meanings.
Reference: [139] <author> C. Wightman and M. Ostendorf. </author> <title> Automatic recognition of prosodic phrases. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 321-324. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: We also know that prosody of read speech differs from spontaneous speech [16]. Recent results are beginning to show that use of prosody can aid automated understanding of spontaneous speech [134] provided that the system can detect prosodic phenomena reliably <ref> [139, 140] </ref> and can correlate these prosodic cues with higher level syntactic, semantic, pragmatic, and conversational structures. For example, phrasal prominence may help to detect high-information regions of a discourse, for use in automatic gisting and summarization [10].
Reference: [140] <author> C. Wightman and M. Ostendorf. </author> <title> Automatic recognition of intonational features. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: We also know that prosody of read speech differs from spontaneous speech [16]. Recent results are beginning to show that use of prosody can aid automated understanding of spontaneous speech [134] provided that the system can detect prosodic phenomena reliably <ref> [139, 140] </ref> and can correlate these prosodic cues with higher level syntactic, semantic, pragmatic, and conversational structures. For example, phrasal prominence may help to detect high-information regions of a discourse, for use in automatic gisting and summarization [10].
Reference: [141] <author> M. Witbrock and P. Haffner. </author> <title> Rapid connectionist speaker adaptation. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech and Signal Processing, pages I:453-456. IEEE, </booktitle> <month> March </month> <year> 1992. </year> <month> 45 </month>
Reference-contexts: Until quite recently, modeling of speaker-dependencies was largely ignored and speaker-independent recognition was accomplished by training on large amounts of data from many different talkers. Recently, some systems have begun to model specific talker characteristics in a more explicit way, in the context of research on speaker adaptation <ref> [44, 141, 108] </ref>. Progress in modeling speaker differences can benefit from basic research on talker characteristics arising from anatomical differences from fields as diverse as speech physiology, phonetics, psychoacoustics, and speech synthesis (e.g., [12, 81, 129, 32, 91, 43, 130, 87, 90, 102, 61, 65, 64], etc.).
Reference: [142] <author> U. Wolz, K. R. McKeown, and G. Kaiser. </author> <title> Automated tutoring in interactive environments: A task centered approach. </title> <journal> Journal of Machine Mediated Learning, </journal> <year> 1989. </year>
Reference-contexts: In fact, spoken language interfaces have not often been attempted for these applications. Typical help systems provide much more information than is needed to solve the problem at hand and often make it difficult to find the bit of information needed to complete a task <ref> [142] </ref>. Response generation would allow for a concise answer addressing user problems.
Reference: [143] <author> S. R. Young, A. G. Hauptmann, W. H. Ward, E. T. Smith, and P. Werner. </author> <title> High level knowledge sources in usable speech recognition systems. </title> <journal> Communications of the ACM, </journal> <volume> 32(2) </volume> <pages> 183-194, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The output of the recognizer should be a best guess of which of the expected meanings was, in fact, received. This model of speech understanding could reduce perplexity and provide improved error correction <ref> [143] </ref>. 3. Building a Variable Initiative Capability Into the Processor. The possibility of moving from subdialogue to subdialogue in nearly arbitrary ways leads to the question of who controls these transitions [118]. The answer is that an efficient dialogue capability requires that either participant be able to take control.
Reference: [144] <author> B. P. Yuhas, Jr. M. H. Goldstein, and T. J. Sejnowski. </author> <title> Integration of acoustic and visual speech signals using neural networks. </title> <journal> IEEE Communications Magazine, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: Inspired by these empirical results, computationalists have begun attempting to integrate auditory and visual information to improve the accuracy and robustness of speech recognition, with encouraging results <ref> [99, 100, 98, 145, 144] </ref>. For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only [121].
Reference: [145] <author> B. P. Yuhas, Jr. M. H. Goldstein, T. J. Sejnowski, and R. E. Jenkins. </author> <title> Neural network models of sensory integration for improved vowel recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1658-1668, </pages> <year> 1988. </year>
Reference-contexts: Inspired by these empirical results, computationalists have begun attempting to integrate auditory and visual information to improve the accuracy and robustness of speech recognition, with encouraging results <ref> [99, 100, 98, 145, 144] </ref>. For example, neural networks trained with combined visual and acoustic features have been shown to perform more 23 accurately and degrad more gracefully as ambient noise levels are increased, compared to networks trained with acoustic features only [121].
Reference: [146] <author> V. W. Zue. </author> <title> The use of speech knowledge in automatic speech recognition. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <pages> pages 1602-1615. </pages> <publisher> IEEE, </publisher> <year> 1985. </year> <month> 46 </month>
Reference-contexts: At present, speech recognition systems are not very robust. Their performance degrades suddenly and significantly with modifications as minor as a change in microphone or telecommunication channel [48]. Systems trained in the laboratory fail when exposed to operating conditions in the field <ref> [146] </ref>. Users will naturally be reluctant to rely on automatic speech recognition if they have to talk in a highly constrained way, if it fails on a day when they have a cold, or if performance drops severely when there is a reasonable level of background noise.
References-found: 146

