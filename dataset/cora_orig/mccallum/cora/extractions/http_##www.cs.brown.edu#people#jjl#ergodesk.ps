URL: http://www.cs.brown.edu/people/jjl/ergodesk.ps
Refering-URL: http://www.cs.brown.edu/people/jjl/pubs.html
Root-URL: http://www.cs.brown.edu/
Title: ErgoDesk: A Framework for Two- and Three-Dimensional Interaction at the ActiveDesk  
Author: Andrew S. Forsberg, Joseph J. LaViola Jr., Robert C. Zeleznik 
Address: PO Box 1910, Providence, RI 02912 USA  
Affiliation: Brown University Site of the NSF Science and Technology Center for Computer Graphics and Scientific Visualization  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bier, E. A., Stone, M. C., Pier, K., Buxton, W., and DeRose, T., Toolglass and Magic Lenses: </author> <title> The See-through Interface, </title> <booktitle> In Proceedings of SIGGRAPH '93, </booktitle> <pages> pp. 73-80, </pages> <month> August, </month> <year> 1993. </year>
Reference-contexts: In particular, the the discussion of ErgoDesk will cover specific ErgoSketch tasks including: 3D modeling with 2D gesture lines (as in Sketch [18]), non-dominant hand camera control, stereoscopic model examination in 3D (using an object-in-hand metaphor), stereoscopic model annotation in 3D, toolglasses and magic lens interaction <ref> [1] </ref>. In addition, we will discuss the calibration procedure we are using with our magnetic trackers and how we transform from the ActiveDesk's coordinate system to the virtual environment's coordinate system. <p> Second, some physical tools serve multiple purposes; in these cases the selection of logical tools is also seamless. For example, a 3D tracker can act as a 3D camera manipulator, a 3D annotation tool, or as a magic lens <ref> [1] </ref>. The transition between logical tools is accomplished seamlessly by speaking to the tool or automatically based on context. Third, virtual tools are activated and deactivated by a drawn gesture, a spoken command, by selection of a different virtual object, or, less naturally, through a 2D menu selection.
Reference: [2] <author> Bryson, S., </author> <title> Measurement and Calibration of Static Error for Three-Dimensional Electromagnetic Trackers, </title> <booktitle> SPIE Conference on Stereoscopic Displays and Applications, </booktitle> <pages> pp. 244-255, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: There are several approaches to re ducing static error such as <ref> [2] </ref>. 4.2 Transforming Between Desk and Virtual Environment Coordinates ErgoSketch uses another transformation between desk and VE coordinates to support object placement within the desk viewing volume and to navigate within the VE.
Reference: [3] <author> Buxton, W., and Myers, B. A., </author> <title> A Study in Two-Handed Input. </title> <booktitle> In Proceedings of CHI'86 Human Factors in Computing Systems, </booktitle> <pages> pp. 321-326, </pages> <year> 1986. </year>
Reference: [4] <author> Cohen, P. R., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith I., Chen, L., and Clow, J., Quick-Set: </author> <title> Multimodal Interaction for Distributed Applications, </title> <booktitle> In Proceedings of the 5th ACM International Multimedia Conference, </booktitle> <address> Seattle, WA, </address> <pages> pp. 31-40, </pages> <month> November, </month> <year> 1997. </year>
Reference: [5] <author> Forsberg, A. S., LaViola, J. J., Markosian, L., Zeleznik, R. C. </author> <year> (1997), </year> <title> Seamless Interaction in Virtual Reality, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 17(6), </volume> <pages> pp. 6-9, </pages> <month> November/December </month> <year> 1997. </year>
Reference: [6] <author> Hark Recognizer Reference Manual, </author> <title> BBN, </title> <publisher> Inc. </publisher> <address> Cam-bridge, MA 1994. </address>
Reference-contexts: Recent advances in speech recognition technology are resulting in robust speaker independent speech recognition for small vocabularies which is nearly adequate for our requirements at this time. We are using a speaker dependent system, [9], in our framework, but are working on integrating a speaker independent system <ref> [6] </ref>. In most situations, speech is not an exclusive means for performing an operation. The colorpicker widget can also be activated through a drawn gesture (by drawing a C). Spoken commands can be used in the context of operations performed by virtual tools.
Reference: [7] <author> Hinkley, K., Pausch, R., Proffitt, D., Patten, J., and Kassell, N. </author> <title> Cooperative Bimanual Action. </title> <booktitle> In Proceedings of CHI'97 Human Factors in Computing Systems, </booktitle> <pages> pp. 27-34, </pages> <year> 1997. </year>
Reference: [8] <author> Hinkley, K., Tullio, J., Pausch, R., Kassell, N. </author> <title> Usability Analysis of 3D Rotation Techniques. </title> <booktitle> In Proceedings of User Interface Software and Technology '97, </booktitle> <pages> pp. 1-10, </pages> <year> 1997. </year>
Reference-contexts: A user might perform this task when examining an object or discussing it with another person. We support this type of interaction through the use of a physical prop that acts as a proxy for a virtual object (similar to <ref> [8] </ref>). The physical prop is a 6 DOF tracker that normally is attached with Velcro to the edge of the desk surface.
Reference: [9] <institution> The In-Cube User Guide, available from Command Corporation, Inc., </institution> <address> Atlanta, GA, </address> <year> 1997. </year>
Reference-contexts: Recent advances in speech recognition technology are resulting in robust speaker independent speech recognition for small vocabularies which is nearly adequate for our requirements at this time. We are using a speaker dependent system, <ref> [9] </ref>, in our framework, but are working on integrating a speaker independent system [6]. In most situations, speech is not an exclusive means for performing an operation. The colorpicker widget can also be activated through a drawn gesture (by drawing a C).
Reference: [10] <institution> Input Technologies, Incorporated., </institution> <note> http://www.iti-world.com. </note>
Reference-contexts: Further, two-handed interaction was limited to essentially sequential operations (e.g., camera manipulation with the non-dominant hand followed by drawing with the dominant hand). 2D drawing was hindered by the use of a lightpen instead of a cordless device although cordless solutions exist from ITI <ref> [10] </ref>. Finally, subjects expected to be able to use their hands and fingers to directly interact with objects on the display surface. In terms of the ErgoSketch modeling application, users had difficulty creating interesting 3D models. This is in part due to the nature of learning a gestural user interface.
Reference: [11] <author> Johnston, M., Cohen, P. R., McGee D., Oviatt, S. L., Pittman, J. A., Smith I., </author> <title> Unification-based Multimodal Integration, </title> <booktitle> 35th Annual Meeting of the Assoc. for Computational Linguistics and 8th Conference of the European Chapter of the Assoc. for Computational Linguistics, </booktitle> <address> Madrid, Spain, </address> <month> July 7-12, </month> <year> 1997. </year>
Reference: [12] <author> Khuner, H., </author> <title> Personal communication and in print. </title>
Reference-contexts: While magnetic trackers could be attached to the user's head in some way, we would prefer to use an unobtrusive camera-based solution. <ref> [12] </ref> demonstrates such a system for head tracking is robust in various lighting conditions and that no calibration is necessary after the tracking algorithm has begun running. The main advantage of the camera-based solution is that no cabling or method for mounting a tracker on a user is required.
Reference: [13] <author> Krueger, W. and Frohlich, B., </author> <title> The Responsive Workbench, </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pp. 12-15, </pages> <year> 1994. </year>
Reference-contexts: Keywords: ActiveDesk, Sketch, 3D Interaction, 3D Modeling, Stereoscopic Viewing, Two-handed Interaction 2 Introduction ErgoSketch is a conceptual modeling system that adapts the Sketch interface [18][19] to the ErgoDesk framework. The ErgoDesk framework is based on the concept of an Ac-tiveDesk (see Figure 1), a variant of the responsive workbench <ref> [13] </ref>, that minimally supports 2D pen-based gestural interaction, 3D interaction, and stereoscopic 3D viewing. The ErgoSketch name derives from the design goal to extend the 2D gestural Sketch application to a more ergonomic physical setup.
Reference: [14] <author> Mapes, D. J., and Moshell, M. J., </author> <title> A Two-Handed Interface for Object Manipulation in Virtual Environments. </title> <booktitle> PRESENSE Teleoperators and Virtual Environments, </booktitle> <volume> 4(4), </volume> <pages> pp. 403-416, </pages> <month> Fall </month> <year> 1995. </year>
Reference: [15] <author> Rekimoto, J., Pick-and-Drop: </author> <title> A Direct Manipulation Technique for Multiple Computer Environments, </title> <booktitle> In Proceedings of User Interface Software and Technology '97, </booktitle> <pages> pp. 31-39, </pages> <address> Banff, </address> <year> 1997. </year>
Reference-contexts: In addition, tablet technology, in theory, supports the tracking of multiple devices that can have unique identifiers which would support a range of interaction possibilities such as personal pens, Pick-and-Drop user interface (see <ref> [15] </ref>), and simplifying the implementation and use of specialized physical tools. In summary, while the ErgoSketch application was difficult for many users to operate, the underlying ErgoDesk framework appears to be more promising.
Reference: [16] <author> Shaw, C. and Green M., THRED: </author> <title> A Two-Handed Design System., </title> <journal> In Multimedia Systems Journal, </journal> <volume> Volume 5, Number 2, </volume> <publisher> ACM/Springer Verlag, </publisher> <year> 1997. </year>
Reference: [17] <author> Sowizral, H., Rushforth, K., Deering, M., </author> <title> The Java 3D API Specification, </title> <publisher> Addison Wesley Longman, Inc.: </publisher> <address> Reading, MA, </address> <year> 1998. </year>
Reference-contexts: In addition, the desk to VE transformation simplifies the programmers interface to moving through the VE. This is similar to the platform concept presented in <ref> [17] </ref>. system. 5 Usability Discussion Over the past few months we have gathered information about the usability of the basic ErgoDesk framework and ErgoSketch modeling application.
Reference: [18] <author> Zeleznik, R.C., Herndon, K., Hughes, J. </author> <title> Sketch: An Interface for Sketching 3D Scenes. </title> <booktitle> In proceedings of SIGGRAPH'96, </booktitle> <pages> pp. 163-170, </pages> <year> 1996. </year>
Reference-contexts: In particular, the the discussion of ErgoDesk will cover specific ErgoSketch tasks including: 3D modeling with 2D gesture lines (as in Sketch <ref> [18] </ref>), non-dominant hand camera control, stereoscopic model examination in 3D (using an object-in-hand metaphor), stereoscopic model annotation in 3D, toolglasses and magic lens interaction [1]. <p> Therefore, we support a seamless, dynamic transition between monoscopic and stereoscopic viewing depending on which type of tool is being used. In general, environmental transitions are automatically triggered when a particular tool is activated by the user. 3.1 2D Pen-based Input The Sketch interface <ref> [18] </ref> is an effective gestural interface for creating and editing 3D conceptual models. Sketch's first implementation used a conventional workstation setup (a three-button mouse, a monitor, and a keyboard) for input and output.
Reference: [19] <author> Zeleznik, </author> <title> R.C., Forsberg, A.S., and Strauss, P.S., Two Pointer Input for 3D Interaction. </title> <booktitle> In Proceedings of the 1997 Symposium on Interactive 3D Graphics, </booktitle> <address> Providence, RI, </address> <month> April, </month> <year> 1997. </year>
References-found: 19

