URL: http://www.csd.uwo.ca/~dsilver/phd-prop.ps
Refering-URL: http://www.csd.uwo.ca/~dsilver/
Root-URL: 
Email: dsilver@csd.uwo.ca  
Title: Consolidation and Transfer of Neural Network Task Knowledge A PhD Proposal networks, learning to learn,
Author: Daniel L. Silver 
Note: artificial neural  
Date: June 2, 1996  
Address: Ontario London, Ontario, Canada N6A 5B7  
Affiliation: Doctoral Candidate Department of Computer Science University of Western  
Abstract: This paper proposes a PhD research program in an area of Machine Learning and Artificial Intelligence, which is currently referred to as "learning to learn". Theoretical and empirical artificial neural network (ANN) research has focussed on the "tabula rasa" form of induction; given a set of training examples a randomly initialized ANN must construct a model, of the function (task), which originally produced the examples. The theories and systems do not take into account the accumulation and use of previously learned task domain knowledge. In particular, there has been little work on the sequential storage and integration (consolidation) of task knowledge after it has been induced, and its recall (transfer) to facilitate the learning of a new task. This proposal outlines the background material on learning to learn, and task knowledge consolidation and transfer in the context of ANNs, describes the motivation for study in this area, and defines the objectives and scope of the research. The progress to date is then presented followed by a research plan to complete the program. 
Abstract-found: 1
Intro-found: 1
Reference: [Akay93] <author> M. Akay and W. Welkowitz, </author> <title> Acoustical detection of coronary occlusions using neural networks, </title> <journal> J Biomed Eng, </journal> <volume> Vol. 15, </volume> <pages> pp. 469-473, </pages> <year> 1993. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see <ref> [Akay93, Boon93, Asti92] </ref>). Of particular importance to this thesis are those articles which involved diagnostic medical images [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a].
Reference: [AM93] <author> Yaser S. Abu-Mostafa, </author> <title> A method for learning from Hints, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 73-80, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: KBANN promotes a hybrid architecture composed of a ANN inductive learning component and a symbolic domain knowledge database. Learning from Hints Yaser Abu-Mostafa is credited with introducing an alternative method of supplying domain knowledge to a neural network referred to as hints <ref> [AM93] </ref>. Others who have used variants of this method are [Sudd90, Fawc92]. Mostafa generalizes the concept of learning from examples to one of learning from hints. A hint is defined to be any property of the target function which can be expressed as one or more pieces of training data.
Reference: [AM95] <author> Yaser S. Abu-Mostafa, </author> <title> Hints, Neural Computation, </title> <journal> Massachusetts Institute of Technology, </journal> <volume> Vol. 7, </volume> <pages> pp. 639-671, </pages> <year> 1995. </year>
Reference-contexts: A learning schedule is used to balance the impact of learning various hints. The generation of hints can be difficult. It requires sufficient external knowledge of the task and its domain. Mostafa presents a systematic method for developing examples for a number of different hint types. Furthermore, in <ref> [AM95] </ref> he formalizes how hint examples can be used to reduce the effective Vapnik-Chervonenkis (VC) dimension which measures the size of the hypothesis search space [Anth92]. <p> In contrast to representational transfer is a form defined as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples <ref> [Sudd90, AM95] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Ande88] <author> J.A. Anderson and E. Rosenfeld, </author> <title> Neurocomputing Foundations of research, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1988. </year>
Reference-contexts: The history of ANN research is as old as that of the modern computer. In 1890, Dr. Willian James defined the first theories of the neuronal process of learning which inspired researchers in the early part of the century <ref> [Ande88] </ref>. During the second world war researchers such as McCulloch, Pitts, and Hebb developed the first mathematical models of neural networks and subsequently ran the first analog and digital computer simulations.
Reference: [Anth92] <author> Martin Anthony and Norman Biggs, </author> <title> Computational Learning Theory, </title> <publisher> Cambridge University Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Such domains are said to be efficiently PAC, or EPAC, learnable. 7 of Kolmogorov, Solomonoff, and Levin <ref> [Li92, Anth92] </ref>, Blumer et al. [Blum87], and Rissanen [Riss78, Riss89] have given formal support to this intuition by showing that the optimal method of representing a series of examples is based on a minimization of the description length of the hypothesis. <p> Mostafa presents a systematic method for developing examples for a number of different hint types. Furthermore, in [AM95] he formalizes how hint examples can be used to reduce the effective Vapnik-Chervonenkis (VC) dimension which measures the size of the hypothesis search space <ref> [Anth92] </ref>. Thus, through the use of hints, the hypothesis space is constrained in an appropriate manner. 2.6.3 Direct, literal methods of transfer In [Prat93a] two methods of direct transfer are defined; literal and non-literal.
Reference: [Asti92] <author> M.L. Astion and P. Wilding, </author> <title> Application of neural networks to interpretation of laboratory data in cancer diagnosis, </title> <journal> Clin Chem, </journal> <volume> Vol. 38, </volume> <pages> pp. 34-38, </pages> <year> 1992. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see <ref> [Akay93, Boon93, Asti92] </ref>). Of particular importance to this thesis are those articles which involved diagnostic medical images [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a].
Reference: [Bard94a] <author> John A. Barden and Keith J. Holyoak, </author> <title> Advances in Connectionist and Neural Computation Theory Volume 2: Analogical Connections, </title> <publisher> Ablex Publishing, </publisher> <address> Norwood, NJ, </address> <year> 1994. </year>
Reference-contexts: This suggests that the abilities of a neural network might be well suited for producing elaborated source knowledge. The formulation of analogical learning in ANNs is very new and, currently, there are more questions than answers <ref> [Bard94a, Bard94b] </ref>.
Reference: [Bard94b] <author> John A. Barden and Keith J. Holyoak, </author> <title> Advances in Connectionist and Neural Computation Theory Volume 3: Analogy, Metaphor, and Reminding, </title> <publisher> Ablex Publishing, </publisher> <address> Norwood, NJ, </address> <year> 1994. </year>
Reference-contexts: This suggests that the abilities of a neural network might be well suited for producing elaborated source knowledge. The formulation of analogical learning in ANNs is very new and, currently, there are more questions than answers <ref> [Bard94a, Bard94b] </ref>.
Reference: [Baxt91a] <author> W.G. Baxt, </author> <title> Use of an artificial neural network for data analysis in clinical decision-making: the diagnosis of acute coronary occlusion, </title> <journal> Neural Computation, </journal> <volume> Vol. 2, </volume> <pages> pp. 480-489, </pages> <year> 1991. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Baxt91b] <author> W.G. Baxt, </author> <title> Use of an artificial neural network for diagnosis of myocardial infarction, </title> <journal> Ann Intern Med, </journal> <volume> Vol. 115, </volume> <pages> pp. 843-848, </pages> <year> 1991. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Baxt95a] <author> Jonathan Baxter, </author> <title> The evolution of learning algorithms for artificial neural networks, Complex Systems, Complex Systems, </title> <publisher> IOS Press, </publisher> <year> 1995. </year>
Reference-contexts: The common component remains available for use in subsequent learning. This concept has recently been formalized by Baxter in <ref> [Baxt95a, Baxt95b] </ref> and demonstrated by Caruana in [Caru95] by a method called parallel learning using a Multi-task Learning (MTL) network. A MTL network uses a feed-forward multi-layer ANN with an output for each task to be learned (observe Figure 7). <p> The common portion of the MTL network is the lower section which maps the input attributes to internal representation at the hidden nodes. This differs from Kehoe's proposal. Baxter <ref> [Baxt95a] </ref> has proven that the number of examples required for learning any one task using a MTL network decreases as a function of the total number of tasks being learned in parallel. <p> Generate empirical results and analyse in light of the initial theory. * Refine the hypothesis of functional transfer and task relatedness. Generate proofs where possible. Relate theory to previous work, particularly to Pratt's DBT method [Prat93a], and to parallel Multi-task Learning <ref> [Baxt95a, Caru95] </ref>. 74 * Develop a method of task rehearsal which utilizes previously learned task representations to generate parallel target values while learning a new task. * Develop an extended version of MTL, XMTL, which is capable of sequentially learning a sequence of tasks from a domain and employs the task
Reference: [Baxt95b] <author> Jonathan Baxter, </author> <title> Learning Internal Representations, </title> <type> Phd Thesis, </type> <institution> Department of Mathematics and Staistics, The Flinders University of South Australia, Australia, </institution> <year> 1995. </year>
Reference-contexts: The common component remains available for use in subsequent learning. This concept has recently been formalized by Baxter in <ref> [Baxt95a, Baxt95b] </ref> and demonstrated by Caruana in [Caru95] by a method called parallel learning using a Multi-task Learning (MTL) network. A MTL network uses a feed-forward multi-layer ANN with an output for each task to be learned (observe Figure 7).
Reference: [Baxt95c] <author> Jonathan Baxter, </author> <title> Learning internal representations, </title> <booktitle> Proceedings of the Eigth International Conference on Computational Learning Theory, </booktitle> <publisher> (to appear) ACM Press, </publisher> <address> Santa Cruz, CA, </address> <year> 1995. </year>
Reference-contexts: analysis of the theory and the prototype systems against a backdrop of informative labo ratory problem domains. * A demonstration of the system on challenging, real-world problems in medical decision making (and possibly other domains). * A comparison of the similarities and differences of the theory with others such as <ref> [Baxt95c, Caru93b, Prat93a] </ref>. 29 5 Progress to Date Two fundamentally different approaches have been applied to the problem: a representational form of consolidation and transfer and a functional form. <p> Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93b, Caru95, Baxt95c] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b]. <p> Certain methods of functional transfer have also been found to reduce training time (measured in number of training iterations). Chief among these methods is the parallel Multiple Task Learning (MTL) paradigm explored recently by Caruana and Baxter <ref> [Caru95, Baxt95c] </ref>. 5.2 Task Relatedness Critical to the transfer of task knowledge from one or more source tasks to a primary task is some measure of relatedness between those tasks. <p> This can be consider a method of indexing into a functional form of domain knowledge. 5.4.2 Review of MTL Network Learning An MTL network <ref> [Caru95, Baxt95c] </ref> uses a feed-forward multi-layer network with an output for each task to be learned. Training examples contain a set of input attributes as well as a target output for each task. The standard back-propagation learning algorithm is used to train all tasks in parallel. <p> source task will provide a positive transfer of inductive knowledge to the primary task and the standard MTL algorithm has no method of escaping the pressures of unrelated tasks. 5.4.3 Prototype 2 - MTL Parallel Learning System This section presents a modified version of the Multiple Task Learning (MTL) methodology <ref> [Caru95, Baxt95c] </ref>, called MTL, which dynamically selects the developing hypotheses for tasks most closely related to a primary task 8 . The transfer is completely functional, relying only on a common initial representation for each of the parallel hypotheses.
Reference: [Blum87] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> Occma's razor, </title> <journal> Information Processing Letters, </journal> <volume> Vol. 24, </volume> <pages> pp. 377-380, </pages> <year> 1987. </year>
Reference-contexts: Such domains are said to be efficiently PAC, or EPAC, learnable. 7 of Kolmogorov, Solomonoff, and Levin [Li92, Anth92], Blumer et al. <ref> [Blum87] </ref>, and Rissanen [Riss78, Riss89] have given formal support to this intuition by showing that the optimal method of representing a series of examples is based on a minimization of the description length of the hypothesis.
Reference: [Boon93] <author> M.E. Boon and L.P. Kok, </author> <title> Neural network processing can provide means to catch errors that slip thorugh human screening of pap smears, </title> <journal> Diagn Cytopathol, </journal> <volume> Vol. 9, </volume> <pages> pp. 411-416, </pages> <year> 1993. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see <ref> [Akay93, Boon93, Asti92] </ref>). Of particular importance to this thesis are those articles which involved diagnostic medical images [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a].
Reference: [Burk95] <author> Harry B. Burke, </author> <booktitle> The importance of artificial neural networks in biomedicine, Proceedings of the INNS World Congress on Neural Networks, </booktitle> <editor> Lawrence Erlbaun Assosciates, </editor> <volume> Vol. II, </volume> <pages> pp. 725-730, </pages> <year> 1995. </year>
Reference-contexts: At that session 24 a major point was made that the complexities of medical science today are forcing the use of automated tools for clinical diagnosis <ref> [Burk95] </ref>. Furthermore, the non-linear interaction of various diagnostic attributes requires the use of sophisticated modelling systems just to understand them. This is bringing about a shift in data-based analysis to one of model-based analysis.
Reference: [Caru93a] <author> Richard A. Caruana, </author> <title> Multitask Connectionist Learning, </title> <booktitle> Proceedings of the 1993 Con--nectionist Models Summer School, </booktitle> <institution> School of Computer Science, Carnegie Mellon University, </institution> <note> pp. 372-379, </note> <year> 1993. </year>
Reference-contexts: In fact, there is empirical evidence to indicate that hidden node weight values favoured by several tasks in a domain are favoured by another task from the same domain. The work on parallel learning of multiple tasks has been significant <ref> [Caru93b, Caru93a] </ref>, however, from a learning to learn perspective it has a major limitation. Parallel transfer in an MTL network occurs due to the pressures of learning several related tasks given the constraint that the majority of the connection weights of each task are shared.
Reference: [Caru93b] <author> Richard A. Caruana, </author> <title> Multitask Learning: A Knowledge-Based Source of Inductive Bias, </title> <booktitle> Proceedings of the tenth international conference on machine learning, </booktitle> <publisher> University of Massachusetts, </publisher> <pages> pp. 41-48, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In fact, there is empirical evidence to indicate that hidden node weight values favoured by several tasks in a domain are favoured by another task from the same domain. The work on parallel learning of multiple tasks has been significant <ref> [Caru93b, Caru93a] </ref>, however, from a learning to learn perspective it has a major limitation. Parallel transfer in an MTL network occurs due to the pressures of learning several related tasks given the constraint that the majority of the connection weights of each task are shared. <p> analysis of the theory and the prototype systems against a backdrop of informative labo ratory problem domains. * A demonstration of the system on challenging, real-world problems in medical decision making (and possibly other domains). * A comparison of the similarities and differences of the theory with others such as <ref> [Baxt95c, Caru93b, Prat93a] </ref>. 29 5 Progress to Date Two fundamentally different approaches have been applied to the problem: a representational form of consolidation and transfer and a functional form. <p> Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93b, Caru95, Baxt95c] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Caru95] <author> Richard A. Caruana, </author> <title> Learning many related tasks at the same time with backpropagation, </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 7, pp. 657-664, </pages> <address> San Mateo, CA, </address> <year> 1995. </year>
Reference-contexts: The common component remains available for use in subsequent learning. This concept has recently been formalized by Baxter in [Baxt95a, Baxt95b] and demonstrated by Caruana in <ref> [Caru95] </ref> by a method called parallel learning using a Multi-task Learning (MTL) network. A MTL network uses a feed-forward multi-layer ANN with an output for each task to be learned (observe Figure 7). Training examples contain a set of input attributes as well as a target output for each task. <p> Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation <ref> [Caru93b, Caru95, Baxt95c] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b]. <p> Certain methods of functional transfer have also been found to reduce training time (measured in number of training iterations). Chief among these methods is the parallel Multiple Task Learning (MTL) paradigm explored recently by Caruana and Baxter <ref> [Caru95, Baxt95c] </ref>. 5.2 Task Relatedness Critical to the transfer of task knowledge from one or more source tasks to a primary task is some measure of relatedness between those tasks. <p> This can be consider a method of indexing into a functional form of domain knowledge. 5.4.2 Review of MTL Network Learning An MTL network <ref> [Caru95, Baxt95c] </ref> uses a feed-forward multi-layer network with an output for each task to be learned. Training examples contain a set of input attributes as well as a target output for each task. The standard back-propagation learning algorithm is used to train all tasks in parallel. <p> source task will provide a positive transfer of inductive knowledge to the primary task and the standard MTL algorithm has no method of escaping the pressures of unrelated tasks. 5.4.3 Prototype 2 - MTL Parallel Learning System This section presents a modified version of the Multiple Task Learning (MTL) methodology <ref> [Caru95, Baxt95c] </ref>, called MTL, which dynamically selects the developing hypotheses for tasks most closely related to a primary task 8 . The transfer is completely functional, relying only on a common initial representation for each of the parallel hypotheses. <p> Generate empirical results and analyse in light of the initial theory. * Refine the hypothesis of functional transfer and task relatedness. Generate proofs where possible. Relate theory to previous work, particularly to Pratt's DBT method [Prat93a], and to parallel Multi-task Learning <ref> [Baxt95a, Caru95] </ref>. 74 * Develop a method of task rehearsal which utilizes previously learned task representations to generate parallel target values while learning a new task. * Develop an extended version of MTL, XMTL, which is capable of sequentially learning a sequence of tasks from a domain and employs the task
Reference: [Chan94] <author> K. Chan, K. Johnson, and J. Becker, </author> <title> A neural network classifier for cerebral perfusion imaging, </title> <journal> J Nucl Med, </journal> <volume> Vol. 35, </volume> <pages> pp. 771-774, 94. </pages>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Dany89] <author> Andrea P. Danyluk, </author> <title> Finding new rules for incomplete theories: Explicit biases for induction with contextual information, </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Cornell University, </publisher> <pages> pp. 34-36, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The basic EBL method can be advanced by combining it with an inductive learning component. The domain rules can be used to impose additional constraints which serve to reduce the effective hypothesis space from which a model will be selected by the inductive learning component <ref> [Dany89, Moon89] </ref>. Cooperatively, the inductive component is able to generate new rules which augment the domain knowledge. 9 Davies and Russel define a related form of domain knowledge influence called determinations [Davi87].
Reference: [Datz94] <author> F.L. Datz, </author> <title> et al.,The use of artificial intelligence to interpret cardiac and pulmonary nuclear medicine images, </title> <booktitle> Nucl Med Annual, </booktitle> <pages> pp. 141-179, </pages> <year> 1994. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Davi87] <author> T.R. Davies and S.J. Russel, </author> <title> A logical approach to reasoning by analogy, </title> <booktitle> Proceedings of the Tenth International Conference on Artificial Intelligence (IJCAI-87), </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 1, pp. 264-270, </pages> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: Cooperatively, the inductive component is able to generate new rules which augment the domain knowledge. 9 Davies and Russel define a related form of domain knowledge influence called determinations <ref> [Davi87] </ref>. The intent is to acquire the knowledge of the problem domain in the form of functional dependencies which act as relevant guidelines for future inductive learning.
Reference: [Daws94] <author> M. Dawson, A. Dobbs, H. Hooper, A. McEwan, J. Triscott, and J. Conney, </author> <title> Artificial neural networks that use single-photon emission tomography to identify patients with probable Alzheimer's disease, </title> <journal> Eur J Nucl Med, </journal> <volume> Vol. 21, </volume> <pages> pp. 1303-1311, </pages> <year> 1994. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Dets91] <author> A.S. Detsky and M.R.J. Guerriere, </author> <title> Neural networks: What are they? (Editorial), </title> <journal> Ann Intern Med, </journal> <volume> Vol. 115, </volume> <pages> pp. 906-707, </pages> <year> 1991. </year>
Reference-contexts: The field of medicine is ripe for the application of machine learning technology [Scot93b]. There have already been a number of successes using inductive decision trees, instance based learning methods, case based reasoning systems, and ANNs <ref> [Dets91] </ref>. A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]).
Reference: [Elli65] <author> H. Ellis, </author> <title> Transfer of Learning, </title> <publisher> MacMillan, </publisher> <address> New York, NY, </address> <year> 1965. </year>
Reference-contexts: Most ANN machine learning theories and systems do not take into account the accumulation and use of previously learned task domain knowledge. This can be considered a major aspect of the problem of learning to learning <ref> [Elli65] </ref> and a fundamental component of human intelligence. In contrast, a significant amount of work has been compiled in symbolic machine learning, psychology, and linguistics on knowledge based inductive inference, reasoning by analogy, and learning to learn.
Reference: [Fahl90] <author> S.E. Fahlman and C. Lebiere, </author> <booktitle> The cascade-correlation learning architecture, Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 2, pp. 524-532, </pages> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: In this case there is an attempt to incrementally build upon previously acquired task knowledge, starting with simple concepts and moving on to more complex problems. The Cascade Correlation Network (CCN) is perhaps the best example of this <ref> [Fahl90] </ref>. The learning algorithm associated with this network iteratively adds hidden nodes and connections for learning preselected subsets of the training examples which may represent sub-tasks. <p> Since 1990 numerous authors have discussed methods of representational transfer. [Shar92, Prat93a, Prat94c] introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in <ref> [Fahl90, Sing92, Ring93] </ref> various incremental or compositional methods of weight representation are explored. The intent of most of these methods is to bias the learning process by initializing the weight representation to an area of weight space conducive for learning a new task.
Reference: [Fawc92] <author> Tom Fawcett and Paul Utgoff, </author> <title> Automatic feature generation for Problem Solving Systems, </title> <address> COINS Tech-Report 92-9, </address> <year> 1992. </year>
Reference-contexts: Learning from Hints Yaser Abu-Mostafa is credited with introducing an alternative method of supplying domain knowledge to a neural network referred to as hints [AM93]. Others who have used variants of this method are <ref> [Sudd90, Fawc92] </ref>. Mostafa generalizes the concept of learning from examples to one of learning from hints. A hint is defined to be any property of the target function which can be expressed as one or more pieces of training data. Thus, training examples, in the traditional sense, are hints.
Reference: [Fodo88] <author> J.A. Fodor and Z.W. Pylyshyn, </author> <title> Connectionism and cognitive architectures: A critical analysis, </title> <journal> Cognition, </journal> <volume> Vol. 28, </volume> <pages> pp. 3-71, </pages> <year> 1988. </year>
Reference-contexts: At a recent international conference Gentner and Markman [Gent93] posed a challenge to the connectionist community: How can neural networks be used to develop models of analogy? They suggested, along the lines of Fodor and Pylyshyn <ref> [Fodo88] </ref>, that neural networks lack the systematicity to provide the structural alignment, structural projection, and flexibility evidenced in human analogy and symbolic models of analogy.
Reference: [Fren91] <author> Robert M. </author> <title> French, Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks, </title> <type> CRCC Technical Report 51-1991, </type> <institution> Center for research on Concepts and Cognition, Indiana Univeristy, </institution> <year> 1991. </year>
Reference-contexts: One approach to solving the problem of catastrophic interference when sequentially learning the categories of a single classification task is to reduce the overlap in hidden unit representation by trading off distributed representation for local representation <ref> [Fren91, Shar94b, Shar94a, McRa93, Gros87] </ref>. This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs.
Reference: [Fren94a] <author> Robert M. </author> <title> French, Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference, </title> <booktitle> Proceedings of the 16th Annual Cognitive Science Society Conference, </booktitle> <year> 1994. </year>
Reference-contexts: This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs. In <ref> [Fren94a] </ref> there is a effort to make the best of both distribution and orthogonality of representation through a method referred to as context biasing. Context biasing influences the internal representation of each new association in terms of the average Hamming distance between class examples. <p> He describes an Interactive Tandem Network (ITN) composed of two back-propagation tandem networks one referred to as short term memory, or STM, and the other referred to as long term memory, or LTM. Building on earlier work presented in <ref> [Fren94a] </ref> the author uses the LTM network to meta-learn prototypes of orthogonally distributed internal representations (hidden node activations) of categories learned in the STM network.
Reference: [Fren94b] <author> Robert M. </author> <title> French, Interactive tandem networks and the sequential learning problem, </title> <type> CRCC Technical Report, </type> <institution> Center for Research on Concepts and Cognition, Indiana Uni-veristy, </institution> <year> 1994. </year> <month> 84 </month>
Reference-contexts: It should be noted that Pratt's original work has been somewhat improved upon and reported in [Prat94d, Prat94c]. Robert French is interested in methods of preventing catastrophic interference when a network is used to learn various classification categories (eg., types of furniture) <ref> [Fren94b] </ref>. His work has been inspired by the writing of McClelland et al. [McCl94] on psychological and physiological studies regarding the interaction between the hippocampal system and the neocortex of the brain.
Reference: [Fuji92] <author> H. Fujita, T. Katafuchi, T. Uehara, and T Mishimura, </author> <title> Application of artificial neural network to computer-aided diagnosis of coronary artery disease in myocardial SPECT bull's eye images, </title> <journal> J Nucl Med, </journal> <volume> Vol. 33, </volume> <pages> pp. 272-276, </pages> <year> 1992. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Gent93] <author> Dedre Gentner and Arthur B. Markman, </author> <title> Analogy Watershed or Waterloo? Structural alignment and the development of connectionist models of analogy, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 855-862, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This suggests that the abilities of a neural network might be well suited for producing elaborated source knowledge. The formulation of analogical learning in ANNs is very new and, currently, there are more questions than answers [Bard94a, Bard94b]. At a recent international conference Gentner and Markman <ref> [Gent93] </ref> posed a challenge to the connectionist community: How can neural networks be used to develop models of analogy? They suggested, along the lines of Fodor and Pylyshyn [Fodo88], that neural networks lack the systematicity to provide the structural alignment, structural projection, and flexibility evidenced in human analogy and symbolic models
Reference: [Gold67] <author> E. Mark Gold, </author> <title> Language Identification in the Limit, </title> <journal> Information and Control, </journal> <volume> Vol. 10, </volume> <pages> pp. 447-474, </pages> <year> 1967. </year>
Reference-contexts: This means that the desired level of confidence and error can be set even though 3 This can be viewed as a probabilistic extension of E.M. Gold's identification in the limit paradigm <ref> [Gold67] </ref>. 4 For greater detail the reader is referrer to Appendix B. 6 the target task and the distribution of examples is unknown. <p> Gold's identification in the limit paradigm <ref> [Gold67] </ref>. 77 Definition of a PAC Learning Algorithm.
Reference: [Gros87] <author> Stephen Grossberg, </author> <title> Competitive learning: From interactive activation to adaptive resonance, </title> <journal> Cognitive Science, </journal> <volume> Vol. 11, </volume> <pages> pp. 23-64, </pages> <year> 1987. </year>
Reference-contexts: The earlier association can be completely "forgotten" by the neural network. McCloskey used the term catastrophic interference for this phenomenon. Previously, Grossberg referred to the same as the stability-plasticity problem of ANNs <ref> [Gros87] </ref>. The phenomenon holds true after learning one classification task and then attempting to learn another using the same network. The network will show poor generalization performance on the original task after learning the second. <p> One approach to solving the problem of catastrophic interference when sequentially learning the categories of a single classification task is to reduce the overlap in hidden unit representation by trading off distributed representation for local representation <ref> [Fren91, Shar94b, Shar94a, McRa93, Gros87] </ref>. This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs.
Reference: [Hall89] <author> Rogers P. Hall, </author> <title> Computational approaches to analogical reasoning: A comparative analysis, </title> <journal> Arificial Intelligence, Elseivier Sience Publishers B.V., </journal> <volume> Vol. 39, </volume> <pages> pp. 39-120, </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: In artificial inteligence and machine learning, symbolic methods of analogical reasoning have been researched since the 1960's. An excellent survey of the subject is given by Hall <ref> [Hall89] </ref>. Much of the following discussion relates to that article. Hall uses the term analogical mapping to describe the transfer of task knowledge from a source domain to a target domain.
Reference: [Hamm95] <author> Leonard Hammey, </author> <title> Analysis of the error surface of the XOR network with two hidden nodes, </title> <institution> Department of Computer Science Computing Report 95/167C, Macquarie University, Australia, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Such a hypothesis can be said to be a local minimum and therefore sub-optimal. It has been debated in <ref> [Hamm95] </ref> and [Shar95] that local minima are a problem which appears to be worse in theory than it is in practice. Networks which are initialized to small random values tend to do well on average.
Reference: [Harl49] <author> H.F. Harlow, </author> <title> Foundation of learning sets, </title> <journal> Psychological Review, </journal> <volume> Vol. 56, </volume> <pages> pp. 51-65, </pages> <year> 1949. </year>
Reference-contexts: However, as tasks are learned successfully, knowledge of the domain increases and the learning system should no longer pursue naive models that clearly fly in the face of previous experience. Psychological and physiological evidence. Psychological evidence <ref> [Harl49, Marx44, Ward37] </ref> indicates that the effectiveness and efficiency of the mammalian brain to learn a new task is closely related to knowledge of similar or dissimilar tasks. If the task is similar to previous tasks then a positive transfer of task knowledge will occur.
Reference: [Hert91] <author> J. Hertz, A. Krogh, and R.G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Adddison-Wesley Pub. Co., </publisher> <address> Redwood City, CA., </address> <year> 1991. </year>
Reference-contexts: Networks which are initialized to small random values tend to do well on average. However, when a network is initialized to high magnitude weight values as a consequence of literal transfer the probability of "falling" into a local minimum increases <ref> [Hert91] </ref>. Consolidation and indexing of task knowledge. If learned tasks are to be used as domain knowledge then there must be a systematic method of accumulating, or consolidating, the knowledge from those tasks. Integral with consolidation is a method of indexing prior knowledge that facilitates its use in future learning. <p> During the training of a new target function, the meta-network generates a slope prediction and an estimate of its accuracy for each training example. This information provides a further constraint to the weight update equation employed in the modified back-propagation neural network (which resembles the Tangent Prop Network of <ref> [Hert91] </ref>) used to learn the target task. In [Thru94b] the authors discuss a specialized version of the system for meta-learning the spatial invariances required to recognize the same object in several different images. The work of Mitchell and Thrun has been very encouraging for researchers in learning to learn. <p> This cycle of interaction is intended to force a structure on the function of functions at the meta-knowledge level. The Networks. Both networks employ the standard back-propagation of error algorithm (each node uses a sigmoid activation function) with a momentum term <ref> [Hert91] </ref>. The task network (TN) is used to learn each new boolean function. The TN accepts function example inputs (I T ) and produces an estimate of the function output (O T ). <p> It produces an estimate of the converged weights (O E ) which we will refer to as the set of generated weights. The back-propagation learning algorithm for this network includes a weight cost term <ref> [Hert91] </ref> which forces an amount of generalization to occur. The Query Phase: Step 1 - TN produces signature weights. The TN is initialized with a known set of weights and then trained on the I T for a new function.
Reference: [Hick90] <author> Angela K. Hickman and Jill H. Larkin, </author> <title> Internal analogy: A model of transfer with problems, </title> <booktitle> Program of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> pp. 53-60, </pages> <address> Hilsdale, NJ, </address> <year> 1990. </year>
Reference-contexts: In <ref> [Hick90] </ref> a fifth component is added, that of gathering feedback on the success of the analogical process and the associated effort (cost). This can be used to modify the other components.
Reference: [Holl89] <author> John H. Holland, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard, </author> <title> Induction: Processes of Inference, Learning, and Discovery, </title> <publisher> The MIT Press, </publisher> <address> Cambridge. MA, </address> <year> 1989. </year>
Reference-contexts: If a machine learning system is to emulate the ability of human learning to use domain knowledge, then it must have a method of acquiring such knowledge in the first place. It makes sense that this method be, itself, a learning process, or, if you like, a meta-learning phenomenon <ref> [JL83, Holl89] </ref>. As in the case of a child, a learning system should start as a pure inductive learner having no background knowledge of the problem domain. This makes for a slow and unsure beginning that is typified by trial and error.
Reference: [Jaco88] <author> R.A. Jacobs, </author> <title> Increased rates of convergence through learning rate adaptation, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: promote a form of generalization equivalent to mathematical interpolation we feel this is an important characteristic of consolidated domain knowledge; Both training examples and task representation (connection weights) can be analysed analytically as well as visually using various graphical tools; Previous research suggests methods of dynamically adjusting ANN learning parameters <ref> [Jaco88, Vogl88, Naik92] </ref> as a method of inductive bias. * Multi-layer feed-forward neural networks and the back-propagation of error learning algorithm will be used because of their wide range of practical deployment, their use in related work on task consolidation and transfer, availability of simulation software, familiarity with the learning paradigm, <p> It has been used for various purposes by other authors such as <ref> [Jaco88, Vogl88, Naik92] </ref>. 60 * Let d k be the current weight space distance between the primary hypothesis, h 0 , and the hypothesis, h k . The rationale behind the choice of these two factors is not immediately obvious.
Reference: [JL83] <author> Phillip N. Johnson-Laird, </author> <title> Mental Models, </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: If a machine learning system is to emulate the ability of human learning to use domain knowledge, then it must have a method of acquiring such knowledge in the first place. It makes sense that this method be, itself, a learning process, or, if you like, a meta-learning phenomenon <ref> [JL83, Holl89] </ref>. As in the case of a child, a learning system should start as a pure inductive learner having no background knowledge of the problem domain. This makes for a slow and unsure beginning that is typified by trial and error.
Reference: [Kear90] <author> Michael J. Kearns, </author> <title> The Computational Complexity of Machine Learning, </title> <publisher> MIT Press, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: And lastly, it is independent of any specific implementation and has been shown to be equivalent to a large number of other models. Furthermore, Valiant's model departs from traditional methods of inductive inference and statistical pattern recognition in one or more of three basic directions <ref> [Kear90] </ref>: * the requirement that a learning algorithm identify the unknown hypothesis exactly is relaxed to approximately; * computational efficiency is an explicit and central concern; and * the emphasis is on developing general learning algorithms which perform well against any probability distribution on the training data; for this reason the
Reference: [Keho88] <author> E. James Kehoe, </author> <title> A layered network model of associative learning: Learning to learn and configuration, </title> <journal> Psychological Review, </journal> <volume> Vol. 95, No. 4, </volume> <pages> pp. 411-433, </pages> <year> 1988. </year>
Reference-contexts: Given the choice of category prototype representation is arbitrary, no systematic consolidation method seems to have been employed. Parallel transfer of internal representations. Kehoe points out in <ref> [Keho88] </ref> that psychological studies of human and animal learning suggest that, besides the development of a specific discriminate function which satisfies the task at hand, there is the acquisition of general knowledge of the structural relationship between input attributes. <p> Furthermore, there is an abundance of evidence, that during the learning process, humans and animals develop not only specific 25 discriminate models but also a sensitivity to similar structural relations among the input stimuli <ref> [Keho88] </ref>. Recent work under the leadership of James McClelland has influenced the research direction presented in this report.
Reference: [Kole90] <author> John F. Kolen and Jordan Pollack, </author> <title> Scenes from Exclusive-Or: Back Propagation is Sensitive to Initial Conditions, </title> <booktitle> Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <address> Cambridge, MA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Within the context of the back-propagation algorithm, Pratt has demonstrated that inappropriately transferred weight values of high magnitudes will cause prolonged training times [Prat93a]. Sensitivity to initial conditions. A related problem of ANNs is their sensitivity to initial conditions <ref> [Kole90] </ref>. An ANN learning algorithm is a non-linear dynamical system, and as such it is possible for the search process to "fall" into a hypothesis which does not have the lowest possible generalization error. Such a hypothesis can be said to be a local minimum and therefore sub-optimal.
Reference: [Li92] <author> Ming Li and Paul M.B. Vitanyi, </author> <title> Kolmogorov Complexity and Its Applications, </title> <institution> Center for Mathematics and Computer Science, </institution> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Such domains are said to be efficiently PAC, or EPAC, learnable. 7 of Kolmogorov, Solomonoff, and Levin <ref> [Li92, Anth92] </ref>, Blumer et al. [Blum87], and Rissanen [Riss78, Riss89] have given formal support to this intuition by showing that the optimal method of representing a series of examples is based on a minimization of the description length of the hypothesis.
Reference: [Mart88] <author> Gale Martin, </author> <title> The effect of old learning on new in hopfield and back-propagation nets, </title> <type> Technical Report ACA-HI-019, </type> <institution> Microelectronics and Computer Technology Corporation (MCC), </institution> <year> 1988. </year> <month> 85 </month>
Reference-contexts: Unfortunately, this is often not the case. It has been shown that the degree of catastrophic interference between the representation of two tasks can prolong the learning process <ref> [Mart88] </ref>. Within the context of the back-propagation algorithm, Pratt has demonstrated that inappropriately transferred weight values of high magnitudes will cause prolonged training times [Prat93a]. Sensitivity to initial conditions. A related problem of ANNs is their sensitivity to initial conditions [Kole90].
Reference: [Marx44] <author> M.H. Marx, </author> <title> The effects of cumulative training upon retroactive inhibition and transfer, </title> <journal> Comp. Psychol. Monographs, </journal> <volume> Vol. 94, </volume> , <year> 1944. </year>
Reference-contexts: However, as tasks are learned successfully, knowledge of the domain increases and the learning system should no longer pursue naive models that clearly fly in the face of previous experience. Psychological and physiological evidence. Psychological evidence <ref> [Harl49, Marx44, Ward37] </ref> indicates that the effectiveness and efficiency of the mammalian brain to learn a new task is closely related to knowledge of similar or dissimilar tasks. If the task is similar to previous tasks then a positive transfer of task knowledge will occur.
Reference: [Math95] <author> MathWorks, </author> <title> The Student Edition of MATLAB, Version 4, Users Guide, </title> <publisher> The MathWorks Inc and Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: The Protoype System. A general back-propagation of error system has been developed using MATLAB 4.2 <ref> [Math95] </ref>. It can simulate a single task learning (STL) method, a multi-task learning (MTL) method, or the MTL method. In MTL or MTL mode the system is currently capable of parallel learning up to 3 tasks.
Reference: [McCl89] <author> Michael McCloskey and Neal J. Cohen, </author> <title> Catastrophic interference in connectionist networks: the sequential learning problem, </title> <journal> The Psychology of Learning and Motivation, </journal> <volume> Vol. 24, </volume> , <year> 1989. </year>
Reference-contexts: In 1989, McCloskey wrote of the difficulty a neural network has in maintaining the knowledge of one association after the learning of another <ref> [McCl89] </ref>. The earlier association can be completely "forgotten" by the neural network. McCloskey used the term catastrophic interference for this phenomenon. Previously, Grossberg referred to the same as the stability-plasticity problem of ANNs [Gros87].
Reference: [McCl94] <author> James L. McClelland, Bruce L. McNaughton, and Randall C. O'Reilly, </author> <title> Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory, </title> <type> Technical Report PDP.CNS.94.1, </type> <institution> Department of Psychology, Carnegie Mellon University, </institution> <address> Pittsurgh, PA 15213, </address> <year> 1994. </year>
Reference-contexts: Robert French is interested in methods of preventing catastrophic interference when a network is used to learn various classification categories (eg., types of furniture) [Fren94b]. His work has been inspired by the writing of McClelland et al. <ref> [McCl94] </ref> on psychological and physiological studies regarding the interaction between the hippocampal system and the neocortex of the brain. <p> Recent work under the leadership of James McClelland has influenced the research direction presented in this report. In <ref> [McCl94] </ref>, McClelland and his team discuss the process of memory consolidation: "... we suggest that the neocortex may be optimized for the gradual discovery of the shared structure of events and experiences, and that the hippocampal system is there to provide a mechanism for the rapid acquisition of new information without
Reference: [McRa93] <author> Ken McRae and P. A. Hetherington, </author> <title> Catastrophic interference is eliminated in pre-trained networks, </title> <booktitle> Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Erlbaum, </publisher> <pages> pp. 723-728, </pages> <address> Hillsdale, NJ, </address> <year> 1993. </year>
Reference-contexts: One approach to solving the problem of catastrophic interference when sequentially learning the categories of a single classification task is to reduce the overlap in hidden unit representation by trading off distributed representation for local representation <ref> [Fren91, Shar94b, Shar94a, McRa93, Gros87] </ref>. This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs.
Reference: [Mich93] <author> R.S. Michalski, </author> <title> Learning = Inferencing + Memorizing, </title> <booktitle> Foundations of Knowledge Ac-quistion: Machine Learning, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 1-41, </pages> <address> Boston, MA, </address> <year> 1993. </year>
Reference-contexts: In turn, new information is added to the domain knowledge database following its discovery. Michalski in his Inferential Theory of Learning refers to this as constructive inductive learning <ref> [Mich93] </ref>. In the extreme, where the new classification task to be learned is exactly the same as one learned at some earlier time, the inductive bias should provide rapid convergence on the optimal hypothesis with very few examples.
Reference: [Mitc80] <author> Tom. M. Mitchell, </author> <title> The need for biases in learning generalizations, </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 184-191, </pages> <address> San Mateo, CA, </address> <year> 1980. </year>
Reference-contexts: Acquiring and using prior knowledge is one of the unsolved problems in learning theory. 2.2 Inductive Bias and Prior Knowledge Mitchell points out in <ref> [Mitc80] </ref> that inductive bias is essential for the development of a hypothesis with good generalization in a tractable amount of time and with a practical number of examples. <p> Recent theoretical developments point to the need for inductive bias during learning <ref> [Mitc80] </ref>. In fact, without a source of guidance during the learning process, there is little hope of ever producing a machine which can learn most real-world classification tasks. Domain knowledge has been recognized as a major source of inductive bias.
Reference: [Mitc86] <author> T. Mitchell, R. Keller, and S. Kedar-Cabelli, </author> <title> Explaination-based generalization: A unifying view, </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 47-80, </pages> <year> 1986. </year>
Reference-contexts: One of the most widely studied is Explanation Based Learning (EBL) popularized by <ref> [Mitc86] </ref>. EBL systems extract general, explanatory rules from examples and record them in a domain knowledge database. During the learning of a new function, EBL rules are used to generate candidate hypotheses. This is not so much an inductive method as it is a derivational approach.
Reference: [Mitc93] <author> Tom Mitchell and Sebastian Thrun, </author> <title> Explanation based neural network learning for robot control, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 287-294, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The results of simulations on four-bit boolean logic functions shows positive results in terms of reduced training time and insensitivity to initial random weights when compared to conventional back-propagation. No experiments were conducted with real-world training sets and generalization accuracy was not reported. In <ref> [Mitc93] </ref> Mitchell and Thrun introduce the Explanation-based Neural Network (EBNN) method for robot learning which is a neural network analogue to the symbolic EBL framework for knowledge based learning initiated by Mitchell. <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Moon89] <author> Raymond J. Mooney and Dirk Ourston, </author> <title> Induction over the unexplained: Integrated learning of concepts with both explainable and conventional aspects, </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Cornell University, </publisher> <pages> pp. 5-7, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The basic EBL method can be advanced by combining it with an inductive learning component. The domain rules can be used to impose additional constraints which serve to reduce the effective hypothesis space from which a model will be selected by the inductive learning component <ref> [Dany89, Moon89] </ref>. Cooperatively, the inductive component is able to generate new rules which augment the domain knowledge. 9 Davies and Russel define a related form of domain knowledge influence called determinations [Davi87].
Reference: [Mugg91] <author> Stephen Muggleton, </author> <title> Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Such dependencies are learned by using a search algorithm to select the smallest but consistent set of attribute conditions which hold for a particular target class. Currently, Inductive Logic Programming (ILP) is a major area of research for domain knowledge based inductive learning <ref> [Mugg91, Quin90] </ref>. One of the most promising methods used in ILP is inverse resolution which takes the statement domain knowledge ^ hypothesis ^ input attributes j= target class and creates a backwards proof in search of an appropriate hypothesis.
Reference: [Naik92] <author> D. K. Naik, R. J. Mammone, and A. Agarwal, </author> <title> Meta-Neural Network approach to learning by learning, </title> <journal> Intelligence Engineering Systems through Artificial Neural Networks, ASME Press, </journal> <volume> Vol. 2, </volume> <pages> pp. 245-252, </pages> <year> 1992. </year>
Reference-contexts: A number of researchers have taken an approach that advocates the meta-learning of neural network search control parameters such as the learning-rate (also referred to as the step-size) or the momentum coefficient. In <ref> [Naik92, Naik93] </ref> Meta-Neural Networks (MNN) are used to record aspects of back-propagation training on a source task. A MNN is associated with each hidden node in the source network and works in a observing mode during source training and in a guiding mode during target training. <p> promote a form of generalization equivalent to mathematical interpolation we feel this is an important characteristic of consolidated domain knowledge; Both training examples and task representation (connection weights) can be analysed analytically as well as visually using various graphical tools; Previous research suggests methods of dynamically adjusting ANN learning parameters <ref> [Jaco88, Vogl88, Naik92] </ref> as a method of inductive bias. * Multi-layer feed-forward neural networks and the back-propagation of error learning algorithm will be used because of their wide range of practical deployment, their use in related work on task consolidation and transfer, availability of simulation software, familiarity with the learning paradigm, <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance. <p> It has been used for various purposes by other authors such as <ref> [Jaco88, Vogl88, Naik92] </ref>. 60 * Let d k be the current weight space distance between the primary hypothesis, h 0 , and the hypothesis, h k . The rationale behind the choice of these two factors is not immediately obvious.
Reference: [Naik93] <author> D.K. Naik and Richard J. Mammone, </author> <title> Learning by learning in neural networks, Artificial Neural Networks for Speech and Vision; ed: </title> <editor> Richard J. Mammone, </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: A number of researchers have taken an approach that advocates the meta-learning of neural network search control parameters such as the learning-rate (also referred to as the step-size) or the momentum coefficient. In <ref> [Naik92, Naik93] </ref> Meta-Neural Networks (MNN) are used to record aspects of back-propagation training on a source task. A MNN is associated with each hidden node in the source network and works in a observing mode during source training and in a guiding mode during target training. <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Pati34] <author> S. Patil, J.W. Henry, M. Rubenfire, and P.D. Stein., </author> <title> RNeural network in the clinical diagnosis of acute pulmonary embolism, </title> <journal> Chest, </journal> <volume> Vol. 104, </volume> <pages> pp. 1685-1689, </pages> <month> 199r34. </month>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Prat91] <author> Lorien Y. Pratt, Jack Mostow, and Candance A. Kamm, </author> <title> Direct transfer of learned information among neural networks, </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI'91), </booktitle> <publisher> AAAI Pres / The MIT Press, Vol. </publisher> <pages> 2, pp. 584-589, </pages> <address> Menlo Park, CA, </address> <year> 1991. </year> <month> 86 </month>
Reference-contexts: Each method of 17 direct literal transfer must supply a solution for overcoming the problem of catastrophic interference discussed earlier. Compositional methods. Divide and conquer methods are common in AI. In ANN learning, approaches based on this paradigm have been referred to as modular [Waib89] or compositional learning <ref> [Sing92, Sing94, Prat91] </ref>. The emphasis is on decomposing a large task into a set of smaller tasks. A set of small networks each learn a component or sub-task using several of the input attributes.
Reference: [Prat93a] <author> Lorien Y. Pratt, </author> <title> Discriminability-Based transfer between neural networks, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 204-211, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: It has been shown that the degree of catastrophic interference between the representation of two tasks can prolong the learning process [Mart88]. Within the context of the back-propagation algorithm, Pratt has demonstrated that inappropriately transferred weight values of high magnitudes will cause prolonged training times <ref> [Prat93a] </ref>. Sensitivity to initial conditions. A related problem of ANNs is their sensitivity to initial conditions [Kole90]. An ANN learning algorithm is a non-linear dynamical system, and as such it is possible for the search process to "fall" into a hypothesis which does not have the lowest possible generalization error. <p> Thus, through the use of hints, the hypothesis space is constrained in an appropriate manner. 2.6.3 Direct, literal methods of transfer In <ref> [Prat93a] </ref> two methods of direct transfer are defined; literal and non-literal. Direct literal transfer is the placement of neural network parameters (typically weight values) from a source network into a target network with no intermediate modification of those parameters. <p> We will discuss two researchers who have examined methods of using previously learned ANN task representation to facilitate the learning of a new task or a previously learned task. Lorien Pratt <ref> [Prat93a] </ref> has done extensive research on the dynamics of the back-propagation learning algorithm and on methods of transferring the weights of a source task so as to generate a good initial representation for a target task. In [Prat93a, Prat94a, Prat94b] she describes the Descriminability-Based Transfer (DBT) method. <p> Lorien Pratt [Prat93a] has done extensive research on the dynamics of the back-propagation learning algorithm and on methods of transferring the weights of a source task so as to generate a good initial representation for a target task. In <ref> [Prat93a, Prat94a, Prat94b] </ref> she describes the Descriminability-Based Transfer (DBT) method. The method selects good initial weight values for transfer from a source network over poor ones by examining the information theoretic value of the hidden node hyperplanes. <p> analysis of the theory and the prototype systems against a backdrop of informative labo ratory problem domains. * A demonstration of the system on challenging, real-world problems in medical decision making (and possibly other domains). * A comparison of the similarities and differences of the theory with others such as <ref> [Baxt95c, Caru93b, Prat93a] </ref>. 29 5 Progress to Date Two fundamentally different approaches have been applied to the problem: a representational form of consolidation and transfer and a functional form. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer. <ref> [Shar92, Prat93a, Prat94c] </ref> introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in [Fahl90, Sing92, Ring93] various incremental or compositional methods of weight representation are explored. <p> What other characteristics of the developing hypotheses could be used in more sophisticated measures of relatedness? In particular, it will be important to take into consideration more subtle characteristics of the back-propagation learning algorithm such as the impact of high magnitude weight values <ref> [Prat93a] </ref> and the energy involved in moving from one point in weight space to another. * As mentioned above, there is the possibility of developing a rehearsal mechanism which will use the MTL method to sequentially learn a set of tasks. <p> Generate empirical results and analyse in light of the initial theory. * Refine the hypothesis of functional transfer and task relatedness. Generate proofs where possible. Relate theory to previous work, particularly to Pratt's DBT method <ref> [Prat93a] </ref>, and to parallel Multi-task Learning [Baxt95a, Caru95]. 74 * Develop a method of task rehearsal which utilizes previously learned task representations to generate parallel target values while learning a new task. * Develop an extended version of MTL, XMTL, which is capable of sequentially learning a sequence of tasks from
Reference: [Prat93b] <author> Lorien Y. Pratt, </author> <title> Transferring previously learned back-propagation neural networks to new learning tasks, </title> <type> PhD Thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1993. </year>
Reference-contexts: There are three limitations which must be pointed out in the above work. First, the contribution made by the hidden to output node weights are not considered in Pratt's research. In <ref> [Prat93b] </ref> this is acknowledged and a reference is made to [Shar92] where the impact of varying the degree to which hidden to output weights are transferred is determined to be significant for some of the functions examined. <p> Examples produced by most of these tools have been presented in the preceding pages. The HP-ANIMATOR <ref> [Prat93b] </ref> has also been used to animate the movement of 1-dimensional hyperplanes (line separators) for 2-dimensional input spaces. The graphical tools have proven crucial to the formulation and testing of theories.
Reference: [Prat94a] <author> L. Y. Pratt and A. N. Christensen, </author> <title> Relaxing the hyperplane assumption in the analysis and modification of back-propagation networks, </title> <editor> In Robert Trappl, ed., </editor> <booktitle> Cybernetics and Systems '94, World Scientific, </booktitle> <pages> pp. 1711-1718, </pages> <address> Singapore, </address> <year> 1994. </year>
Reference-contexts: Lorien Pratt [Prat93a] has done extensive research on the dynamics of the back-propagation learning algorithm and on methods of transferring the weights of a source task so as to generate a good initial representation for a target task. In <ref> [Prat93a, Prat94a, Prat94b] </ref> she describes the Descriminability-Based Transfer (DBT) method. The method selects good initial weight values for transfer from a source network over poor ones by examining the information theoretic value of the hidden node hyperplanes.
Reference: [Prat94b] <author> L. Y. Pratt and V. I. Gough, </author> <title> Improving discriminability based transfer by modifying the IM metric to use sigmoidal activations, </title> <editor> In Robert Trappl, ed., </editor> <booktitle> Cybernetics and Systems '94, World Scientific, </booktitle> <pages> pp. 1719-1726, </pages> <address> Singapore, </address> <year> 1994. </year>
Reference-contexts: Lorien Pratt [Prat93a] has done extensive research on the dynamics of the back-propagation learning algorithm and on methods of transferring the weights of a source task so as to generate a good initial representation for a target task. In <ref> [Prat93a, Prat94a, Prat94b] </ref> she describes the Descriminability-Based Transfer (DBT) method. The method selects good initial weight values for transfer from a source network over poor ones by examining the information theoretic value of the hidden node hyperplanes.
Reference: [Prat94c] <author> Lorien Y. Pratt, </author> <title> Experiments on the transfer of knowledge between neural networks, </title> <editor> In S. Hanson, G. Drastal, and R. Rivest, </editor> <title> Computational Learning Theory and Natural Learning Systems, Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> pp. 523-560, </pages> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: Lastly, the research does not address the issue of knowledge consolidation, but instead relies on the manual selection of a related source task. It should be noted that Pratt's original work has been somewhat improved upon and reported in <ref> [Prat94d, Prat94c] </ref>. Robert French is interested in methods of preventing catastrophic interference when a network is used to learn various classification categories (eg., types of furniture) [Fren94b]. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer. <ref> [Shar92, Prat93a, Prat94c] </ref> introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in [Fahl90, Sing92, Ring93] various incremental or compositional methods of weight representation are explored.
Reference: [Prat94d] <author> Lorien Y. Pratt, </author> <title> Non-literal transfer Among Neural Network Learners, In R.J. Mam-mone (ed) Artificial Neural Networks for Speech and Vision, </title> <publisher> Chapman & Hall, </publisher> <pages> pp. 143-169, </pages> <year> 1994. </year>
Reference-contexts: Lastly, the research does not address the issue of knowledge consolidation, but instead relies on the manual selection of a related source task. It should be noted that Pratt's original work has been somewhat improved upon and reported in <ref> [Prat94d, Prat94c] </ref>. Robert French is interested in methods of preventing catastrophic interference when a network is used to learn various classification categories (eg., types of furniture) [Fren94b].
Reference: [Quin90] <author> J.R. Quinlan, </author> <title> Learning logical definitions from relations, </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1990. </year>
Reference-contexts: Such dependencies are learned by using a search algorithm to select the smallest but consistent set of attribute conditions which hold for a particular target class. Currently, Inductive Logic Programming (ILP) is a major area of research for domain knowledge based inductive learning <ref> [Mugg91, Quin90] </ref>. One of the most promising methods used in ILP is inverse resolution which takes the statement domain knowledge ^ hypothesis ^ input attributes j= target class and creates a backwards proof in search of an appropriate hypothesis.
Reference: [Ring93] <author> Mark Ring, </author> <title> Learning sequential tasks by incrementally adding higher orders, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 155-122, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Since 1990 numerous authors have discussed methods of representational transfer. [Shar92, Prat93a, Prat94c] introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in <ref> [Fahl90, Sing92, Ring93] </ref> various incremental or compositional methods of weight representation are explored. The intent of most of these methods is to bias the learning process by initializing the weight representation to an area of weight space conducive for learning a new task.
Reference: [Riss78] <author> Jorma Rissanen, </author> <title> Modeling by the shortest data description, </title> <journal> Automatica, </journal> <volume> Vol. 14, </volume> <pages> pp. 465-471, </pages> <year> 1978. </year>
Reference-contexts: Such domains are said to be efficiently PAC, or EPAC, learnable. 7 of Kolmogorov, Solomonoff, and Levin [Li92, Anth92], Blumer et al. [Blum87], and Rissanen <ref> [Riss78, Riss89] </ref> have given formal support to this intuition by showing that the optimal method of representing a series of examples is based on a minimization of the description length of the hypothesis. For machine learning, Occam's Razor, suggests an a priori strategy for selecting an hypothesis of greatest generalization.
Reference: [Riss89] <author> Jorma Rissanen, </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <publisher> World Scientific, </publisher> <address> Signa-pore, </address> <year> 1989. </year>
Reference-contexts: Such domains are said to be efficiently PAC, or EPAC, learnable. 7 of Kolmogorov, Solomonoff, and Levin [Li92, Anth92], Blumer et al. [Blum87], and Rissanen <ref> [Riss78, Riss89] </ref> have given formal support to this intuition by showing that the optimal method of representing a series of examples is based on a minimization of the description length of the hypothesis. For machine learning, Occam's Razor, suggests an a priori strategy for selecting an hypothesis of greatest generalization.
Reference: [Roma94] <author> Steve G. Romaniuk, </author> <title> Learning to learn: Automatic adaptation of learning bias, </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI'94), </booktitle> <publisher> AAAI Pres / The MIT Press, Vol. </publisher> <pages> 2, pp. 871-876, </pages> <address> Menlo Park, CA, </address> <year> 1994. </year>
Reference-contexts: In this way there is a literal transfer of previously learned sub-task representation on to the next sub-task and finally on to the major task. Romaniuk expands upon the CCN in <ref> [Roma94] </ref> by using a genetic algorithm to optimally select training subsets for each task. This he calls a Evolutionary Growth Perceptron (EGP) network. Romaniuk is specifically interested in construction of domain knowledge and the ability to use EGP networks to sequentially learn various tasks in no particular order.
Reference: [Rume86] <author> D.E. Rumelhart, J.L. McClelland, </author> <title> and the PDP Research Group, Parallel distributed process: Explorations in the microstructure of cognition, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1986. </year>
Reference-contexts: In 1985 and 1986 several researchers independently discovered the solution to training more complex networks of Perceptrons. The dominant group was composed of Rumelhart, McClelland, Williams, and Hinton who published an influential set of books on their ground-breaking research entitled Parallel Distributed Processing or PDP <ref> [Rume86] </ref>. The hallmark of the PDP Group was their interdisciplinary approach involving mathematics and theory of computation, psychology, philosophy, neuro-physiology, and traditional AI. Over the last 10 years, ANN research and development has exploded all over the world with interest coming from educational institutions, industry, business, and the military. <p> To train the ANN in Figure 5, the weights of the connections must be adjusted so as to produce the hypothesis with greatest generalization. The most widely used learning algorithm for this type 13 of network is the back-propagation of error algorithm <ref> [Rume86] </ref>. Although the differential calculus of the back-propagation algorithm is complex, the concept is simple.
Reference: [Scot93a] <author> J.A. Scott and E.L. Palmer, </author> <title> Neural network analysis of ventilation-perfusion lung scans, </title> <journal> Radiology, </journal> <volume> Vol. 186, </volume> <pages> pp. 661-664, </pages> <year> 1993. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Scot93b] <author> R. Scott, </author> <title> Artificial Intelligence: Its use in medical diagnosis, </title> <journal> J Nucl Med, </journal> <volume> Vol. 34, No. 3, </volume> <pages> pp. 510-514, </pages> <year> 1993. </year>
Reference-contexts: The field of medicine is ripe for the application of machine learning technology <ref> [Scot93b] </ref>. There have already been a number of successes using inductive decision trees, instance based learning methods, case based reasoning systems, and ANNs [Dets91].
Reference: [Shar92] <author> Noel E. Sharkey and Amanda J.C. Sharkey, </author> <title> Adaptive generalization and the transfer of knowledge, </title> <institution> Working paper Center for Connection Science, University of Exeter, UK, </institution> <year> 1992. </year>
Reference-contexts: There are three limitations which must be pointed out in the above work. First, the contribution made by the hidden to output node weights are not considered in Pratt's research. In [Prat93b] this is acknowledged and a reference is made to <ref> [Shar92] </ref> where the impact of varying the degree to which hidden to output weights are transferred is determined to be significant for some of the functions examined. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer. <ref> [Shar92, Prat93a, Prat94c] </ref> introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in [Fahl90, Sing92, Ring93] various incremental or compositional methods of weight representation are explored.
Reference: [Shar94a] <author> Noel E. Sharkey and Amanda J.C. Sharkey, </author> <title> Interference and discrimination in neural net memory, </title> <institution> Department of Computer Science Research Report CS-94-?, University of Sheffield, UK, </institution> <year> 1994. </year>
Reference-contexts: One approach to solving the problem of catastrophic interference when sequentially learning the categories of a single classification task is to reduce the overlap in hidden unit representation by trading off distributed representation for local representation <ref> [Fren91, Shar94b, Shar94a, McRa93, Gros87] </ref>. This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs.
Reference: [Shar94b] <author> Noel E. Sharkey and Amanda J.C. Sharkey, </author> <title> Understanding catastrophic interference in neural nets, </title> <institution> Department of Computer Science Research Report CS-94-4, University of Sheffield, UK, </institution> <year> 1994. </year>
Reference-contexts: One approach to solving the problem of catastrophic interference when sequentially learning the categories of a single classification task is to reduce the overlap in hidden unit representation by trading off distributed representation for local representation <ref> [Fren91, Shar94b, Shar94a, McRa93, Gros87] </ref>. This can be accomplished by forcing the internal representations to be as orthogonal to one another as possible. In the extreme, of course, one hidden node is used to represent each 15 training example and little or no generalization occurs.
Reference: [Shar95] <editor> Noel Sharkey, John Neary, and Amanda Sharkey, </editor> <title> Searching weight space for backpropagation solution types, </title> <institution> Department of Computer Science Research Report CS-95-?, University of Sheffield, UK, </institution> <year> 1995. </year>
Reference-contexts: Such a hypothesis can be said to be a local minimum and therefore sub-optimal. It has been debated in [Hamm95] and <ref> [Shar95] </ref> that local minima are a problem which appears to be worse in theory than it is in practice. Networks which are initialized to small random values tend to do well on average.
Reference: [Shav89] <author> Jude W. Shavlik, </author> <title> Acquiring Recursive Concepts with Explanation-Based Learning, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> 2929 Campus Drive, Suite 260, San Mateo, CA 94403, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Thus, indirect methods of consolidation and transfer promote a hybrid architecture. KBANN Knowledge-based ANNs Some of the first efforts to transfer knowledge into a ANN were by AI researchers with a background in Explanation-based Learning (EBL). Shavlik and Towell made the first advances in this area with their EBL-ANNs <ref> [Shav89, Shav90] </ref> and later their knowledge-based networks (KBANN) [Towe90].
Reference: [Shav90] <author> Jude W. Shavlik and Geoffrey G. Towell, </author> <title> An appraoch to combining explanation-based and neural learning algorithms, </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 828-839, </pages> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Thus, indirect methods of consolidation and transfer promote a hybrid architecture. KBANN Knowledge-based ANNs Some of the first efforts to transfer knowledge into a ANN were by AI researchers with a background in Explanation-based Learning (EBL). Shavlik and Towell made the first advances in this area with their EBL-ANNs <ref> [Shav89, Shav90] </ref> and later their knowledge-based networks (KBANN) [Towe90]. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer. [Shar92, Prat93a, Prat94c] introduce methods of literal and non-literal transfer of weight values, <ref> [Shav90, Towe90] </ref> demonstrate indirect methods of transfer via an intermediate symbolic representation, while in [Fahl90, Sing92, Ring93] various incremental or compositional methods of weight representation are explored.
Reference: [Silv95a] <author> Daniel L. Silver, </author> <title> A Survey of Machine Learning Theory, </title> <institution> Department of Computer Science, University of Western Ontario, </institution> <address> London, Ontario, </address> <year> 1995. </year>
Reference-contexts: For details on the formal model which will be used in the dissertation consult <ref> [Silv95a] </ref>. 2 Clearly, if all examples in X were seen by the learning system, given sufficient memory it should have the ability to construct a perfect h, such that h (x i ) = f (x i ). 5 or hypothesis ^ input attributes j= target class: The degree of approximation <p> This is in conflict with the fact that most complex real-world problems require the use of large hypothesis spaces. Some strategy, or heuristic, must be employed to "intelligently" restrict the hypothesis space, H, thereby making the search process computationally efficient <ref> [Silv95a] </ref>. The selection of one hypothesis over another, beyond the criterion of consistency, is called inductive bias, that is the learning system must utilize a source of prior knowledge of the problem to facilitate the search for the correct h 2 H .
Reference: [Silv95b] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge, </title> <booktitle> Proceedings of the INNS World Congress on Neural Networks, </booktitle> <editor> Lawrence Erlbaun Assosciates, </editor> <volume> Vol. III, </volume> <pages> pp. 164-169, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The article was based on the working hypothesis and the results from preliminary experiments conducted with the Consolidation System. The paper was accepted for presentation at the World Congress on Neural Networks (WCNN*95) in Washington, DC, in July, and included in the conference proceedings 1995 <ref> [Silv95b] </ref>.
Reference: [Silv96] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> The parallel transfer of task knowledge using dynamic learning rates based on a measure of relatedness, Connection Science Special Issue: Transfer in Inductive Systems, </title> <editor> Lorien Pratt (Editor), </editor> <publisher> Carfax Publishing Company, </publisher> <address> Cambridge, MA, </address> <note> In Press 1996. </note>
Reference-contexts: This section discusses the theories behind the two approaches, their differences, the prototype systems which have been tested, and the problems which have been encountered. This material sets the stage for the remainder of the thesis effort. 5.1 Representational vs. Functional Transfer In <ref> [Silv96] </ref> the difference between two forms of task knowledge transfer is defined: representational and functional. The representational form of transfer involves the direct or indirect assignment of known task representation (weight values) to a new task. <p> Robert Mercer entitled "The Parallel Transfer of Task Knowledge Using Dynamic Learning Rates Based on a Measure of Relatedness" <ref> [Silv96] </ref> was submitted to the Connection Science special issue on "Transfer in Inductive Systems". This documented the functional transfer hypothesis and preliminary experiments with the MTL prototype system. The manuscript was reviewed by three independent individuals knowledgeable in the field and accepted with minor revisions.
Reference: [Sing92] <author> Satinder P. Singh, </author> <title> Transfer of learning by composing solutions for elemental sequential tasks, </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: Each method of 17 direct literal transfer must supply a solution for overcoming the problem of catastrophic interference discussed earlier. Compositional methods. Divide and conquer methods are common in AI. In ANN learning, approaches based on this paradigm have been referred to as modular [Waib89] or compositional learning <ref> [Sing92, Sing94, Prat91] </ref>. The emphasis is on decomposing a large task into a set of smaller tasks. A set of small networks each learn a component or sub-task using several of the input attributes. <p> Since 1990 numerous authors have discussed methods of representational transfer. [Shar92, Prat93a, Prat94c] introduce methods of literal and non-literal transfer of weight values, [Shav90, Towe90] demonstrate indirect methods of transfer via an intermediate symbolic representation, while in <ref> [Fahl90, Sing92, Ring93] </ref> various incremental or compositional methods of weight representation are explored. The intent of most of these methods is to bias the learning process by initializing the weight representation to an area of weight space conducive for learning a new task.
Reference: [Sing94] <author> Satinder P. Singh, </author> <title> The efficient learning of multiple task sequences, </title> <booktitle> Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: Each method of 17 direct literal transfer must supply a solution for overcoming the problem of catastrophic interference discussed earlier. Compositional methods. Divide and conquer methods are common in AI. In ANN learning, approaches based on this paradigm have been referred to as modular [Waib89] or compositional learning <ref> [Sing92, Sing94, Prat91] </ref>. The emphasis is on decomposing a large task into a set of smaller tasks. A set of small networks each learn a component or sub-task using several of the input attributes.
Reference: [Sudd90] <author> Steven Suddarth and Y Kergoisien, </author> <title> Rule injection hints as a means of improving network performance and learning time, </title> <booktitle> Proceedings of the EURASIP workshop on Neural Networks, </booktitle> <year> 1990. </year>
Reference-contexts: Learning from Hints Yaser Abu-Mostafa is credited with introducing an alternative method of supplying domain knowledge to a neural network referred to as hints [AM93]. Others who have used variants of this method are <ref> [Sudd90, Fawc92] </ref>. Mostafa generalizes the concept of learning from examples to one of learning from hints. A hint is defined to be any property of the target function which can be expressed as one or more pieces of training data. Thus, training examples, in the traditional sense, are hints. <p> In contrast to representational transfer is a form defined as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the use of implicit pressures from supplemental training examples <ref> [Sudd90, AM95] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b].
Reference: [Syst93] <author> Research Systems, </author> <title> IDL Interactive Data Language User & Reference Guides (Ver. </title> <type> 3), </type> <institution> Research Systems, Inc, Boulder, CO, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Might this provide the basis for a functional form of sequential transfer of task knowledge? 72 5.5 Visualization Software Several graphical tools have been developed using xgraph, MicroSoft Excel, MATLAB, XLISPSTAT [Tier90], and IDL <ref> [Syst93] </ref> for visualizing 2-dimensional and 3-dimensional attribute and weight spaces as well as hidden node hyperplanes. Examples produced by most of these tools have been presented in the preceding pages. The HP-ANIMATOR [Prat93b] has also been used to animate the movement of 1-dimensional hyperplanes (line separators) for 2-dimensional input spaces.
Reference: [Thru93] <author> Sebastian Thrun and Tom M.Mitchell, </author> <title> Lifelong Robot Learning, </title> <type> Technical Report IAI-TR-93-7, </type> <institution> Institute for Informatics III, University of Bonn, Bonn, Germany, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: The EBNN research is important, as it provides a method by which impoverished training sets can be used along with domain knowledge to learn a new task. In <ref> [Thru93, Thru94a, Thru94b] </ref> the authors expand slightly on the initial effort and propose the life long learning paradigm. For an autonomous agent to be successful it must ultimately have the ability to acquire domain knowledge dynamically from its environment. <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru94a] <author> Sebastian Thrun, </author> <title> A Lifelong Learning Perspective for Mobile Robot Control, </title> <booktitle> Proceedings of the IEEE Conference on Intelligent Robots and Systems, IEEE, </booktitle> <month> September 12-16, </month> <year> 1994. </year>
Reference-contexts: The EBNN research is important, as it provides a method by which impoverished training sets can be used along with domain knowledge to learn a new task. In <ref> [Thru93, Thru94a, Thru94b] </ref> the authors expand slightly on the initial effort and propose the life long learning paradigm. For an autonomous agent to be successful it must ultimately have the ability to acquire domain knowledge dynamically from its environment. <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru94b] <author> Sebastian Thrun and Tom M.Mitchell, </author> <title> Learning one more thing, </title> <type> Technical Report CMU-CS-94-184, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year>
Reference-contexts: The EBNN research is important, as it provides a method by which impoverished training sets can be used along with domain knowledge to learn a new task. In <ref> [Thru93, Thru94a, Thru94b] </ref> the authors expand slightly on the initial effort and propose the life long learning paradigm. For an autonomous agent to be successful it must ultimately have the ability to acquire domain knowledge dynamically from its environment. <p> This information provides a further constraint to the weight update equation employed in the modified back-propagation neural network (which resembles the Tangent Prop Network of [Hert91]) used to learn the target task. In <ref> [Thru94b] </ref> the authors discuss a specialized version of the system for meta-learning the spatial invariances required to recognize the same object in several different images. The work of Mitchell and Thrun has been very encouraging for researchers in learning to learn. <p> of implicit pressures from supplemental training examples [Sudd90, AM95], the parallel learning of related tasks constrained to use a common internal representation [Caru93b, Caru95, Baxt95c], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Naik92, Naik93, Mitc93, Thru93, Thru94a, Thru94b] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Tier90] <author> Luke Tierney, LISP-STAT: </author> <title> An object-oriented environemnt for statistical computing and dynamic graphics, </title> <publisher> John Wiley & Sons, </publisher> <address> Inc, </address> <institution> University of Minnesota, Minneaplois, Minnesota, </institution> <year> 1990. </year>
Reference-contexts: Might this provide the basis for a functional form of sequential transfer of task knowledge? 72 5.5 Visualization Software Several graphical tools have been developed using xgraph, MicroSoft Excel, MATLAB, XLISPSTAT <ref> [Tier90] </ref>, and IDL [Syst93] for visualizing 2-dimensional and 3-dimensional attribute and weight spaces as well as hidden node hyperplanes. Examples produced by most of these tools have been presented in the preceding pages.
Reference: [Tour93] <author> G. Tourassi, C. Floyd, H. Sostman, and R. Coleman, Acute pulmoary embolism: </author> <title> artificial neural network approach for diagnosis, </title> <journal> Radiology, </journal> <volume> Vol. 192, </volume> <pages> pp. 739-742, </pages> <year> 1993. </year>
Reference-contexts: A survey of medical and bio-medical journals since 1989 produced over 700 articles on the application of neural networks in biomedical research and clinical diagnosis (for examples see [Akay93, Boon93, Asti92]). Of particular importance to this thesis are those articles which involved diagnostic medical images <ref> [Baxt91b, Baxt91a, Fuji92, Daws94, Tour93, Chan94, Pati34, Datz94, Scot93a] </ref>. Recently, at the 1995 World Congress on Neural Networks in Washington, DC, there was a two-day session dedicated to the use and regulation of neural networks in medicine.
Reference: [Towe90] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier, </author> <title> Refinement of approximate domain theories by knowledge-based neural networks, </title> <booktitle> Proceedings of the Eigth National Conference on Artificial Intelligence (AAAI-90), AAAI Press/MIT Press, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 861-866, </pages> <address> Menlo Park, CA, </address> <year> 1990. </year>
Reference-contexts: Integral with consolidation is a method of indexing prior knowledge that facilitates its use in future learning. Consolidation and indexing have been accomplished in a syntactic manner using symbolic learning systems such as EBL <ref> [Towe90] </ref>, however, it has not been carefully studied using ANN learning systems. One of the perceived difficulties is that for any one ANN configuration there is more than one, and potentially many, weight representations which approximate a particular function. <p> KBANN Knowledge-based ANNs Some of the first efforts to transfer knowledge into a ANN were by AI researchers with a background in Explanation-based Learning (EBL). Shavlik and Towell made the first advances in this area with their EBL-ANNs [Shav89, Shav90] and later their knowledge-based networks (KBANN) <ref> [Towe90] </ref>. These systems demonstrate the mutual advantage of using the symbolic data of the EBL domain knowledge to overcome, on the one hand, the ANN's need for large numbers of examples, while on the other hand, the EBL requirement for complete domain knowledge. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer. [Shar92, Prat93a, Prat94c] introduce methods of literal and non-literal transfer of weight values, <ref> [Shav90, Towe90] </ref> demonstrate indirect methods of transfer via an intermediate symbolic representation, while in [Fahl90, Sing92, Ring93] various incremental or compositional methods of weight representation are explored.
Reference: [Towe91] <author> Geoffrey G. Towell and Jude W. Shavlik, </author> <title> Interpretation of Artificial Neural Networks: Mapping Knowledge-Based Neural Networks into Rules, </title> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 4, pp. 977-984, </pages> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: A major part of this effort is mapping the symbolic EBL rules to the appropriate ANN configuration and weight values. Recent work by Towel and Shavlik have explored methods of extracting symbolic rules from the refined ANN <ref> [Towe91, Towe93] </ref>; the objective being to consolidate the EBL domain knowledge with new rules produced by the ANN. KBANN promotes a hybrid architecture composed of a ANN inductive learning component and a symbolic domain knowledge database. <p> into the relatedness of tasks; and * Advances in understanding systems of interacting neural networks; * Advances in understanding learning by analogy in the context of neural networks; * Construction of useful domain knowledge in the field of medicine (there is currently work on the automatic extraction of symbolic rules <ref> [Towe91] </ref> from neural networks). 4 Objectives and Scope 4.1 Objectives Theory. The first objective is to develop a theoretical model and a prototype system which sequentially retains task domain knowledge (task representations) and uses it to bias the learning of a new task in an efficient and effective manner.
Reference: [Towe93] <author> Geoffrey G. Towell and Jude W. Shavlik, </author> <title> Extracting Refined Rules from Knowledge-Based Neural Networks, </title> <journal> Machine Learning, </journal> <volume> Vol. 13, </volume> <pages> pp. 71-101, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A major part of this effort is mapping the symbolic EBL rules to the appropriate ANN configuration and weight values. Recent work by Towel and Shavlik have explored methods of extracting symbolic rules from the refined ANN <ref> [Towe91, Towe93] </ref>; the objective being to consolidate the EBL domain knowledge with new rules produced by the ANN. KBANN promotes a hybrid architecture composed of a ANN inductive learning component and a symbolic domain knowledge database.
Reference: [Vali84] <author> Leslie G. Valiant, </author> <title> A Theory of the Learnable, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 27, No. 11, </volume> <pages> pp. 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: An answer to this was provided by Leslie G. Valiant in 1984 when he proposed the probably approximately correct, or PAC theory of learning <ref> [Vali84] </ref> 3 . The PAC model of learning characterizes training examples by their statistical properties, and measures the error in the hypothesis produced by a learning system in light of the same statistical properties. <p> How can the system know that any selected hypothesis h is sufficiently close to the desired function f ? An answer to the above question was provided in 1984 by Leslie G. Valiant who proposed the probably approximately correct, or PAC theory of learning <ref> [Vali84] </ref> 11 . The PAC model of learning characterizes training examples by their statistical properties, and measure the error in the hypothesis produced by a learning system in light of the same statistical properties. Valiant's PAC model continues to gain acceptance as a fundamental model of learning.
Reference: [vG90] <author> T. van Gelder, </author> <title> Compositionality: A connectionist variation on a classical theme, </title> <journal> Cognitive Science, </journal> <volume> Vol. 14, No. 3, </volume> <pages> pp. 355-384, </pages> <year> 1990. </year>
Reference-contexts: In this way the problem of multiple task representations will be reduced. 31 The hypothesis is congruent with aspects of functional compositionality first presented in <ref> [vG90] </ref>. The theory relies on the geometric position of task representations as points in weight space and the ability to bias learning based on an interpolation over these points.
Reference: [Vogl88] <author> T.P. Vogl, J.K. Mangis, A.K. Rigler, W.T. Zink, and D.L. Alkon, </author> <title> Accelerating the convergence of the back-propagation method, </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 59, </volume> <pages> pp. 257-263, </pages> <year> 1988. </year>
Reference-contexts: promote a form of generalization equivalent to mathematical interpolation we feel this is an important characteristic of consolidated domain knowledge; Both training examples and task representation (connection weights) can be analysed analytically as well as visually using various graphical tools; Previous research suggests methods of dynamically adjusting ANN learning parameters <ref> [Jaco88, Vogl88, Naik92] </ref> as a method of inductive bias. * Multi-layer feed-forward neural networks and the back-propagation of error learning algorithm will be used because of their wide range of practical deployment, their use in related work on task consolidation and transfer, availability of simulation software, familiarity with the learning paradigm, <p> It has been used for various purposes by other authors such as <ref> [Jaco88, Vogl88, Naik92] </ref>. 60 * Let d k be the current weight space distance between the primary hypothesis, h 0 , and the hypothesis, h k . The rationale behind the choice of these two factors is not immediately obvious.
Reference: [Waib89] <author> Alexander Waibel, Hidefumi Sawai, and Kiyoshiro Shikano, </author> <title> Modularity and scaling in large phonemic neural networks, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, IEEE, </journal> <volume> Vol. 37, No. 12, </volume> <pages> pp. 1888-1898, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Each method of 17 direct literal transfer must supply a solution for overcoming the problem of catastrophic interference discussed earlier. Compositional methods. Divide and conquer methods are common in AI. In ANN learning, approaches based on this paradigm have been referred to as modular <ref> [Waib89] </ref> or compositional learning [Sing92, Sing94, Prat91]. The emphasis is on decomposing a large task into a set of smaller tasks. A set of small networks each learn a component or sub-task using several of the input attributes.
Reference: [Ward37] <author> L.B. Ward, </author> <title> Reminiscence and rote learning, </title> <journal> Psychol. Monographs, </journal> <volume> Vol. 49, No. 220, </volume> , <year> 1937. </year>
Reference-contexts: However, as tasks are learned successfully, knowledge of the domain increases and the learning system should no longer pursue naive models that clearly fly in the face of previous experience. Psychological and physiological evidence. Psychological evidence <ref> [Harl49, Marx44, Ward37] </ref> indicates that the effectiveness and efficiency of the mammalian brain to learn a new task is closely related to knowledge of similar or dissimilar tasks. If the task is similar to previous tasks then a positive transfer of task knowledge will occur.
References-found: 104

