URL: http://www.cs.rice.edu/~hofri/Kn_prob1.ps
Refering-URL: http://www.cs.rice.edu/~hofri/
Root-URL: 
Email: e-mail: hofri@cs.rice.edu e-mail: Philippe.Jacquet@inria.fr  
Title: Saddle Points in Random Matrices: Analysis of Knuth Search Algorithms  
Author: Micha Hofri Philippe Jacquet 
Address: Rocquencourt B.P. 105 Houston TX 77005 78153 Le Chesnay Cedex, France  
Affiliation: Dept. of Computer Science INRIA, Domaine de Voluceau Rice University  
Abstract: We present an analysis of algorithms for finding saddle points in a random matrix, presented by Donald E. Knuth as exercise 1.3.2-12 in The Art of Computer Programming. We estimate the average computing costs of three saddle point search algorithms. Amusingly, the asymptotic results in this analysis about matrix saddle points uses the same approach that leads to the celebrated saddle point method in complex analysis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bienstock, F. Chung, M. Fredman, A. Schaffer, P. Shor, and S. Suri, </author> <title> A note on finding a strict saddle-point, </title> <journal> American Math. </journal> <volume> Monthly 98, </volume> <month> 418419 </month> <year> (1991). </year>
Reference-contexts: Note: We discuss some of these aspects briefly in the last section. A more sophisticated approach to searching for saddle points is described in <ref> [1] </ref>. 3.1 Algorithm I: row-oriented At the level of detail needed here, this algorithm does the following: 1. For rows i = 1 through m, 2. Locate minima R i , (only one is found in the current scenario) 3 3. Compare R i against elements in its column: 4. <p> Interestingly, here the worst case may exceed substantially the average one. With more possible values, special algorithms will not bring nearly as much savings, and are probably not cost effective. The next point is relevant here too. (6) As noted in <ref> [1] </ref>, not much can be done for matrices with repeated values (when known to have more than two levels) to obtain a more efficient algorithm. In particular, there is no escaping the term bmn.
Reference: [2] <author> P. Jacquet, W. Szpankowski, </author> <title> Analytical depoissonization and its applications, submitted for Theor. Comp. Science, as a Fundamental Study, </title> <year> 1997. </year>
Reference-contexts: Note: while we selected here a direct approach to derive the asymptotics, by a curious coincidence they also follow from recent depoissonization results of the second author and Wojciech Szpankowski <ref> [2] </ref>, results which were obtained via the saddle point method of complex analysis. This is shown in an Appendix. The calculations below concern the random variables one needs in order to compute expected processing times. <p> The appendix shows how to obtain for it an expansion using depoissonization, according to the results in <ref> [2] </ref>, which are based on the complex saddle-point method. 8 The ratio G (n)=G (n + 1 + r=m) is developed asymptotically as in [4, x4.5]. <p> We employ the Poisson generating function G m (z) = n n! e z and use the depoissonization lemmas presented by Jacquet and Szpankowski in 12 <ref> [2] </ref>. Corollary 1 there states that if a sequence of Poisson generating functions G m (z) are all entire, and satisfy the following two properties, uniformly in m: 1.
Reference: [3] <author> D. Knuth, </author> <booktitle> The Art of Computer Programming, Addison-Wesley, volume 1: Fundamental Algorithms, 1968. 3rd. </booktitle> <editor> Ed. </editor> <year> 1997. </year>
Reference-contexts: 1. Introduction A matrix saddle point is defined in <ref> [3] </ref> as an element of the matrix which is the smallest in its row, and the largest in its column. Exercise 1.3.2-10 there shows two algorithms to find such saddle points, and suggests a third. <p> This is shown in an Appendix. The calculations below concern the random variables one needs in order to compute expected processing times. In particular, unlike the custom in <ref> [3] </ref>, we do not count the instructions and their execution times in a MIX program, to determine the multipliers of these variables. We consider matrices with distinct values. The scenario where values may be repeated introduces additional difficulties, which are not addressed here, except in the last section. <p> In our model they are random variables. The operations of finding row-minima or column-maxima in each of the algorithms are independent of each other, and each takes a time as discussed in <ref> [3, x1.2.10] </ref>, over n or m entries. These durations are denoted by T n and T m respectively. It is known that E [T n ] = a + bn + n ln n, where the Greek letters represent instruction counts and times that depend on the implementation. <p> Keeping the symbol F makes some of the calculations more transparent. An arbitrary matrix term is denoted by X . 2.1 The minmax and maxmin inequality The following theorem is shown in <ref> [3] </ref> by elementary considerations: Theorem 2 : For an arbitrary numerical matrix let minmax be the minimum value of all columns maxima, and similarly, let maxmin be the maximum value of all row minima. <p> saddle point if and only if minmax = maxmin, and then this is the value of the saddle point. * A matrix with distinct terms can have one saddle point at most. 2.2 Saddle point probability The likelihood that a saddle point occurs in a distinct-values matrix is computed in <ref> [3] </ref>, and is shown there to be quite small: P mn Pr [An m fi n matrix has a saddle point] = m + n n This quantity is nm times the probability of occurrence of a saddle point at any given position in the matrix. <p> The last observation allows us to rederive the result in <ref> [3] </ref> from the equivalent model, by theorem 1. Let x be the value of a specific element. The probability that the other n 1 terms of its row are greater than x is (1 F (x)) n1 . <p> When we consider that careful programming (as demonstrated in the implementations of the first two algorithms presented in <ref> [3] </ref>) can result in nontrivial savings, by combining various components of each algorithm, we realize that much of these differences may be illusory. (2) The conclusions in comment (1) depend somewhat on our assumption that m and n are of similar order (see the discussion following equation (17)). <p> <ref> [3] </ref>that all terms are distinct. At the level of our analysis the differences do not play a role. (5) The performance of saddle point search algorithms when the terms may not be all distinct can be quite different, as some extreme experiments reported in the solution to exercise 1.3.2-10 in [3] show. Clearly it is possible to design specialized algorithms for some scenarios. An obvious case is when the matrix can have two distinct terms only; then we need only search for a row of high value terms, or a column of low values.
Reference: [4] <author> F. W. J. Olver, </author> <title> Asymptotics and Special Functions, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1974. </year> <month> 14 </month>
Reference-contexts: The appendix shows how to obtain for it an expansion using depoissonization, according to the results in [2], which are based on the complex saddle-point method. 8 The ratio G (n)=G (n + 1 + r=m) is developed asymptotically as in <ref> [4, x4.5] </ref>.
References-found: 4

