URL: http://www.cs.umd.edu/projects/dimsum/papers/semantic_caching.ps.gz
Refering-URL: http://www.cs.umd.edu/projects/dimsum/siumei/references.html
Root-URL: 
Email: dar@dtl.co.il franklin@cs.umd.edu bthj@cs.umd.edu  divesh@research.att.com mdtanx@cs.umd.edu  
Title: Semantic Data Caching and Replacement  
Author: Shaul Dar Michael J. Franklin Bjorn T. Jonsson Divesh Srivastava Michael Tan 
Affiliation: Data Technologies Ltd. University of Maryland University of Maryland  AT&T Research University of Maryland  
Abstract: We propose a semantic model for client-side caching and replacement in a client-server database system and compare this approach to page caching and tuple caching strategies. Our caching model is based on, and derives its advantages from, three key ideas. First, the client maintains a semantic description of the data in its cache,which allows for a compact specification, as a remainder query, of the tuples needed to answer a query that are not available in the cache. Second, usage information for replacement policies is maintained in an adaptive fashion for semantic regions, which are associated with collections of tuples. This avoids the high overheads of tuple caching and, unlike page caching, is insensitive to bad clustering. Third, maintaining a semantic description of cached data enables the use of sophisticated value functions that incorporate semantic notions of locality, not just LRU or MRU, for cache replacement. We validate these ideas with a detailed performance study that includes traditional workloads as well as a workload motivated by a mobile navigation application. 
Abstract-found: 1
Intro-found: 1
Reference: [Bro92] <author> K. Brown. PRPL: </author> <title> A database workload specification lan guage, v1.3. M.S. </title> <type> thesis, </type> <institution> Univ. of WI, Madison, </institution> <year> 1992. </year>
Reference-contexts: Disks are modeled using a detailed characterization adapted from the ZetaSim model <ref> [Bro92] </ref>. The disk model includes an elevator scheduling policy, a controller cache, and read-ahead prefetching. There are many parameters to the disk model (not shown) including: rotational speed, seek factor, settle time, track and cylinder sizes, controller cache size, etc.
Reference: [C+94] <author> M. Carey, et al. </author> <title> Shoring up persistent applications. </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <year> 1994. </year>
Reference-contexts: We focus on systems with a single server, but all of the approaches studied here can be easily extended to a multiple server or even a peer-to-peer architecture, such as SHORE <ref> [C+94] </ref>. The database is stored on disk at the server, and is organized in terms of pages. Pages are physical units they are fixed length. The database contains index as well as data pages. We assume that tuples are fixed-length and that pages contain multiple tuples. <p> It models a heterogeneous, peer-to-peer database system such as SHORE <ref> [C+94] </ref>, and provides a detailed model of query processing costs in such a system. For this study, the simulator was configured to model a system with a single client and a single server. Table 1 shows the main parameters of the model.
Reference: [CFZ94] <author> M. Carey, M. Franklin, M. Zaharioudakis, </author> <title> Fine-grained sharing in page server database systems, </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <year> 1994. </year>
Reference-contexts: We address these factors briefly below. 2.1.1 Data Granularity In any system that uses data-shipping, the granularity of data management is a key performance concern. As described in <ref> [CFZ94, Fra96] </ref>, the granularity decisions that must be made include: (1) client-server transfer, (2) consistency maintenance, and (3) cache management. In this study (in contrast to [DFMV90]), all architectures ship data in page-sized units. Also, we examine the architectures in the context of read-only queries. <p> Semantic locality differs from spatial locality in that it is not dependent on the static clustering of tuples to pages; rather it dynamically adapts to the pattern of query accesses. 2.2 Page Caching Architecture In page caching architectures (also referred to as page-server systems <ref> [DFMV90, CFZ94] </ref>), the unit of transfer between servers and clients is a page. Queries are posed at clients, and processed locally down to the level of requests for individual pages.
Reference: [CKPS95] <author> S. Chaudhuri, R. Krishnamurthy, S. Potamianos, K. Shim. </author> <title> Optimizing queries with materialized views. </title> <booktitle> Proc. of IEEE Conf. on Data Engineering, </booktitle> <year> 1995. </year>
Reference-contexts: Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. This topic has been the subject of considerable study in the literature (e.g., <ref> [YL87, CR94, CKPS95, LMSS95] </ref>). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture.
Reference: [CKSV86] <author> G. P. Copeland, S. N. Khosafian, M. G. Smith, P. Val duriez. </author> <title> Buffering schemes for permanent data. </title> <booktitle> Proc. of IEEE Conf. on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: These approaches aim to balance the tradeoff between overhead and sensitivity to cluster ing. Semantic caching takes the different approach of using predicates to dynamically group tuples. The caching of results based on projections (rather than selections) was studied in <ref> [CKSV86] </ref>. However, the work most closely related to ours is the predicate caching approach of Keller and Basu [KB96], which uses a collection of possibly overlapping constraint formulas, derived from queries, to describe client cache contents. Our work differs from [KB96] in three significant respects.
Reference: [CR94] <author> C. Chen, N. Roussopoulos. </author> <title> Implementation and perfor mance evaluation of the ADMS query optimizer: Integrating query result caching and matching. </title> <booktitle> Proc. EDBT Conf. </booktitle> <year> 1994. </year>
Reference-contexts: Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. This topic has been the subject of considerable study in the literature (e.g., <ref> [YL87, CR94, CKPS95, LMSS95] </ref>). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture. <p> This topic has been the subject of considerable study in the literature (e.g., [YL87, CR94, CKPS95, LMSS95]). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture. ADMS <ref> [CR94, R+95] </ref> caches the results of subquery expressions corresponding to join nodes in the evaluation tree of each user query. Subsequent queries are optimized by using previously cached views, so query matching plays an important role. Cache replacement is performed by tossing out entire views.
Reference: [DFMV90] <author> D. DeWitt, P. Futtersack, D. Maier, F. Velez. </author> <title> A study of three alternative workstation-server architectures for object-oriented database systems, </title> <booktitle> Proc. VLDB Conf., </booktitle> <year> 1990. </year>
Reference-contexts: We then describe the approaches in light of these dimensions. We focus on the particular instantia-tions of the architectures that are studied in this paper, rather than on an analysis of all possible design choices. More detailed discussions of the traditional architectures can be found in, among other places, <ref> [DFMV90, KK94, Fra96] </ref>. 2.1 Overview of the Architectures In this paper, we assume a client-server architecture in which client machines have significant processing and storage resources, and are capable of executing queries. <p> As described in [CFZ94, Fra96], the granularity decisions that must be made include: (1) client-server transfer, (2) consistency maintenance, and (3) cache management. In this study (in contrast to <ref> [DFMV90] </ref>), all architectures ship data in page-sized units. Also, we examine the architectures in the context of read-only queries. Thus, the main impact of granularity in this study is on cache management. <p> Semantic locality differs from spatial locality in that it is not dependent on the static clustering of tuples to pages; rather it dynamically adapts to the pattern of query accesses. 2.2 Page Caching Architecture In page caching architectures (also referred to as page-server systems <ref> [DFMV90, CFZ94] </ref>), the unit of transfer between servers and clients is a page. Queries are posed at clients, and processed locally down to the level of requests for individual pages. <p> Caching at the granularity of a single item allows maximal flexibility in the tuning of cache contents to the access locality properties of applications <ref> [DFMV90] </ref>. As described in [DFMV90], however, the faulting in of individual tuples (assuming that tuples are substantially smaller than pages) can lead to performance problems due to the expense of sending large numbers of small messages. <p> Caching at the granularity of a single item allows maximal flexibility in the tuning of cache contents to the access locality properties of applications <ref> [DFMV90] </ref>. As described in [DFMV90], however, the faulting in of individual tuples (assuming that tuples are substantially smaller than pages) can lead to performance problems due to the expense of sending large numbers of small messages. <p> The tradeoffs between page caching (called page servers) and tuple caching (called object servers) were initially studied in <ref> [DFMV90] </ref>. That work demonstrated the sensitivity of page caching to static clustering, and also the message overhead that results from sending tuples from the server one-at-a-time. In our implementation of tuple caching, we took care to group tuples into pages before transferring them from the server.
Reference: [D+96] <author> S. Dar, et al. </author> <title> Columbus: Providing information and navigation services to mobile users. </title> <note> Submitted, </note> <year> 1996. </year>
Reference-contexts: In this section, we further examine the benefits of semantic locality by exploring a workload that has more semantic content than the selection-based workloads studied so far. The workload models mobile clients accessing remotely-stored map data through a low-bandwidth wireless communication network (see, e.g., <ref> [D+96] </ref>). Each tuple in the database represents a road segment in the map, and each page is a collection of such tuples.
Reference: [Fra96] <author> M. Franklin, </author> <title> Client data caching: A foundation for high performance object database systems, </title> <publisher> Kluwer, </publisher> <year> 1996. </year>
Reference-contexts: We then describe the approaches in light of these dimensions. We focus on the particular instantia-tions of the architectures that are studied in this paper, rather than on an analysis of all possible design choices. More detailed discussions of the traditional architectures can be found in, among other places, <ref> [DFMV90, KK94, Fra96] </ref>. 2.1 Overview of the Architectures In this paper, we assume a client-server architecture in which client machines have significant processing and storage resources, and are capable of executing queries. <p> We address these factors briefly below. 2.1.1 Data Granularity In any system that uses data-shipping, the granularity of data management is a key performance concern. As described in <ref> [CFZ94, Fra96] </ref>, the granularity decisions that must be made include: (1) client-server transfer, (2) consistency maintenance, and (3) cache management. In this study (in contrast to [DFMV90]), all architectures ship data in page-sized units. Also, we examine the architectures in the context of read-only queries. <p> The directional Manhattan distance function is always better than LRU, and hence is the clear winner when the square is traversed many times. 7 Related Work Data-shipping systems have been studied primarily in the context of object-oriented database systems, and are discussed in detail in <ref> [Fra96] </ref>. The tradeoffs between page caching (called page servers) and tuple caching (called object servers) were initially studied in [DFMV90]. That work demonstrated the sensitivity of page caching to static clustering, and also the message overhead that results from sending tuples from the server one-at-a-time.
Reference: [FJK96] <author> M. Franklin, B. Jonsson, D. Kossmann. </author> <title> Performance tradeoffs for client-server query processing. </title> <booktitle> Proc. ACM SIG-MOD Conf., </booktitle> <year> 1996. </year>
Reference-contexts: values based on S 0 i , R (Q i ; V i1 ), the discarded semantic regions S 1 ; : : : ; S k , and the caching/replacement policy. 4 Simulation Environment 4.1 Resources and Model Parameters Our simulator is an extension of the one used in <ref> [FJK96] </ref>, written in C++ using CSIM. It models a heterogeneous, peer-to-peer database system such as SHORE [C+94], and provides a detailed model of query processing costs in such a system. For this study, the simulator was configured to model a system with a single client and a single server.
Reference: [GR93] <author> J. Gray, A. Reuter. </author> <title> Transaction processing: Concepts and techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: relation (tuples) TupleSize 200 Size of each tuple (bytes) QuerySize 1-10% % of relation selected by each query Skew 90% % of queries within a hot region HotSpot 10% Size of the hot region (% of relation) Table 2: Workload Parameters and Default Settings used the Buffer Control Block of <ref> [GR93] </ref>. After removing all attributes pertaining to updates and concurrency control, we were left with 28 bytes per page. To model the storage cost of indexes, we assume that the primary index takes up negligible space, as also the upper levels of the secondary index.
Reference: [Jag90] <author> H.V. Jagadish. </author> <title> Linear clustering of objects with multiple attributes. </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <year> 1990. </year>
Reference-contexts: This pair of attributes forms a dense key of the relation; there is a tuple for every possible pair of values. These two attributes can be viewed as the X and Y co-ordinates in a 2-dimensional space. The relation is clustered using the Z-ordering <ref> [Jag90] </ref> on these two attributes. Each tuple is 200 bytes long. We use a benchmark of simple selections of tuples, which is characteristic of map data accesses in a navigation application.
Reference: [KB96] <author> A. Keller, J. Basu. </author> <title> A predicate-based caching scheme for client-server database architectures. </title> <journal> VLDB J, </journal> <volume> 5(1), </volume> <year> 1996. </year>
Reference-contexts: Semantic caching takes the different approach of using predicates to dynamically group tuples. The caching of results based on projections (rather than selections) was studied in [CKSV86]. However, the work most closely related to ours is the predicate caching approach of Keller and Basu <ref> [KB96] </ref>, which uses a collection of possibly overlapping constraint formulas, derived from queries, to describe client cache contents. Our work differs from [KB96] in three significant respects. First, in [KB96] there is no concept analogous to a semantic region. <p> However, the work most closely related to ours is the predicate caching approach of Keller and Basu <ref> [KB96] </ref>, which uses a collection of possibly overlapping constraint formulas, derived from queries, to describe client cache contents. Our work differs from [KB96] in three significant respects. First, in [KB96] there is no concept analogous to a semantic region. Recall that maintaining semantic regions allows, in particular, the use of sophisticated value functions incorporating semantic notions of locality. <p> However, the work most closely related to ours is the predicate caching approach of Keller and Basu <ref> [KB96] </ref>, which uses a collection of possibly overlapping constraint formulas, derived from queries, to describe client cache contents. Our work differs from [KB96] in three significant respects. First, in [KB96] there is no concept analogous to a semantic region. Recall that maintaining semantic regions allows, in particular, the use of sophisticated value functions incorporating semantic notions of locality. <p> Recall that maintaining semantic regions allows, in particular, the use of sophisticated value functions incorporating semantic notions of locality. For discarding cached tuples, Keller and Basu use instead, a reference counting approach based on the number of predicates satisfied by the tuple. Second, the focus of <ref> [KB96] </ref> is largely on the effects of database updates. Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. <p> For discarding cached tuples, Keller and Basu use instead, a reference counting approach based on the number of predicates satisfied by the tuple. Second, the focus of <ref> [KB96] </ref> is largely on the effects of database updates. Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. This topic has been the subject of considerable study in the literature (e.g., [YL87, CR94, CKPS95, LMSS95]).
Reference: [KK94] <author> A. Kemper, D. Kossmann. </author> <title> Dual-buffering strategies in object bases. </title> <booktitle> Proc. VLDB Conf., </booktitle> <year> 1994. </year>
Reference-contexts: We then describe the approaches in light of these dimensions. We focus on the particular instantia-tions of the architectures that are studied in this paper, rather than on an analysis of all possible design choices. More detailed discussions of the traditional architectures can be found in, among other places, <ref> [DFMV90, KK94, Fra96] </ref>. 2.1 Overview of the Architectures In this paper, we assume a client-server architecture in which client machines have significant processing and storage resources, and are capable of executing queries. <p> In our implementation of tuple caching, we took care to group tuples into pages before transferring them from the server. Alternative approaches to making page caching less sensitive to static clustering have been proposed <ref> [KK94, OTS94] </ref>. These schemes, known as Dual Buffering and Hybrid Caching respectively, keep a mixture of pages and objects in the cache based on heuristics.
Reference: [LMSS95] <author> A. Y. Levy, A. O. Mendelzon, Y. Sagiv, D. Srivastava. </author> <title> Answering queries using views. </title> <booktitle> Proc. PODS Conf., </booktitle> <year> 1995. </year>
Reference-contexts: Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. This topic has been the subject of considerable study in the literature (e.g., <ref> [YL87, CR94, CKPS95, LMSS95] </ref>). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture.
Reference: [OTS94] <author> J. O'Toole, L. Shrira. </author> <title> Hybrid caching for large scale object systems. </title> <booktitle> Proc. 6th Wkshp on Pers. Object Sys., </booktitle> <year> 1994. </year>
Reference-contexts: In our implementation of tuple caching, we took care to group tuples into pages before transferring them from the server. Alternative approaches to making page caching less sensitive to static clustering have been proposed <ref> [KK94, OTS94] </ref>. These schemes, known as Dual Buffering and Hybrid Caching respectively, keep a mixture of pages and objects in the cache based on heuristics.
Reference: [R+95] <author> N. Roussopoulos, et al. </author> <title> The ADMS project: Views R Us. </title> <journal> IEEE Data Engineering Bulletin, </journal> <month> June </month> <year> 1995. </year>
Reference-contexts: This topic has been the subject of considerable study in the literature (e.g., [YL87, CR94, CKPS95, LMSS95]). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture. ADMS <ref> [CR94, R+95] </ref> caches the results of subquery expressions corresponding to join nodes in the evaluation tree of each user query. Subsequent queries are optimized by using previously cached views, so query matching plays an important role. Cache replacement is performed by tossing out entire views.
Reference: [RK86] <author> N. Roussopoulos, H. Kang. </author> <booktitle> Principles and techniques in the design of ADMS+-. IEEE Computer, </booktitle> <month> December, </month> <year> 1986. </year>
Reference: [YL87] <author> H. Z. Yang, P.-A. Larson. </author> <title> Query transformation for PSJ queries. </title> <booktitle> Proc. VLDB Conf., </booktitle> <year> 1987. </year>
Reference-contexts: Third, [KB96] does not present any performance results to validate their heuristics. Making use of the tuples in the cache can be viewed as a simple case of using materialized views to answer queries. This topic has been the subject of considerable study in the literature (e.g., <ref> [YL87, CR94, CKPS95, LMSS95] </ref>). None of these studies, however, considered the issue of which views to cache/materialize given a limited sized cache, or the performance implications of view usability in a client-server architecture.
References-found: 19

