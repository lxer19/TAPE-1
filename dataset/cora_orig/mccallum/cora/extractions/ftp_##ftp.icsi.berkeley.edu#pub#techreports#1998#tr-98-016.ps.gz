URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-016.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Scatter-partitioning RBF network for function regression and image  
Keyword: RBF networks, supervised and unsupervised learning from data, prototype vectors, synaptic links, Gestaltist theory, image segmentation, low-level vision.  
Note: TR-98-016  ICSI, 1947  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Street, Suite 600, Berkeley, CA 94704-1198,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Center  
Email: Email: baraldi@icsi.berkeley.edu  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: June 1998  
Abstract: segmentation: Preliminary results Abstract. Scatter-partitioning Radial Basis Function (RBF) networks increase their number of degrees of freedom with the complexity of an input-output mapping to be estimated on the basis of a supervised training data set. Due to its superior expressive power a scatter-partitioning Gaussian RBF (GRBF) model, termed Supervised Growing Neural Gas (SGNG), is selected from the literature. SGNG employs a one-stage error-driven learning strategy and is capable of generating and removing both hidden units and synaptic connections. A slightly modified SGNG version is tested as a function estimator when the training surface to be fitted is an image, i.e., a 2-D signal whose size is finite. The relationship between the generation, by the learning system, of disjointed maps of hidden units and the presence, in the image, of pictorially homogeneous subsets (segments) is investigated. Unfortunately, the examined SGNG version performs poorly both as function estimator and image segmenter. This may be due to an intrinsic inadequacy of the one-stage error-driven learning strategy to adjust structural parameters and output weights simultaneously but consistently. In the framework of RBF networks, further studies should investigate the combination of two-stage error-driven learning strategies with synapse generation and removal criteria. y Internal report of the paper entitled "Image segmentation with scatter-partitioning RBF networks: A feasibility study," to be presented at the conference Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation, part of SPIE's International Symposium on Optical Science, Engineering and Instrumentation, 19-24 July 1998, San Diego, CA. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. Bishop, </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <address> Oxford (UK), </address> <year> 1995. </year>
Reference-contexts: Among possible RBFs, the Gaussian function, g (), is preferred due to the fact that it is localized and factorizable <ref> [1] </ref>. <p> simple architectures to perform complex mappings; ii) they train faster than multi layer perceptrons; iii) they may be particularly attractive for applications where input patterns are readily available but input-output sample pairs are difficult to be gathered; iv) they are easily interpretable when they employ RBFs that are well localized <ref> [1] </ref>; and v) the use of unsupervised learning methods can be quite successful in practice when the input distribution is highly nonuniform [3]. <p> ii) the distribution of RBFs in the input space as it is computed by the unsupervised technique may be poor for the classification or regression problem at hand, e.g., unsupervised methods may form clusters of input vectors that are closely spaced in the input space but belong to different classes <ref> [1] </ref>, [4]-[6].
Reference: 2. <author> J. Moody, and C. Darken, </author> <title> "Fast learning in networks of locally-tuned processing units," </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> pp. 281-294, </pages> <year> 1989. </year>
Reference-contexts: This parameter adaptation process is called learning from data. For the class of RBF networks the learning task is typically implemented as a two-stage procedure, termed hybrid learning <ref> [2] </ref>. A hybrid learning algorithm combines unsupervised with supervised learning as described below.
Reference: 3. <author> V. Cherkassky, and F. Mulier, </author> <title> Learning from data, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1998. </year>
Reference-contexts: where input patterns are readily available but input-output sample pairs are difficult to be gathered; iv) they are easily interpretable when they employ RBFs that are well localized [1]; and v) the use of unsupervised learning methods can be quite successful in practice when the input distribution is highly nonuniform <ref> [3] </ref>.
Reference: 4. <author> B. Fritzke, </author> <title> "Incremental neuro-fuzzy systems," </title> <booktitle> Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Applications of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Growing RBF networks employ either scatter- [6]-[8], or grid-partitioning mechanisms [9], such that the input space is covered with localized basis functions, also termed small patches <ref> [4] </ref>, or data windows [10]. Unlike grid-partitioning systems, scatter-partitioning systems position small patches at locations which are not known a priori. <p> If this inter-pattern distance is measured as the Euclidean distance, then receptive fields centered on prototype vectors are equivalent to Voronoi polyhedra <ref> [4] </ref>, [13], [15]. 4. According to CHR, if a synaptic link between units w 1 and w 2 , identified as s w 1 ;w 2 , does not exist already, create it.
Reference: 5. <author> E. Alpaydn, </author> <title> Soft vector quantization and the EM algorithm. Neural Networks, </title> <publisher> in press, </publisher> <year> 1998. </year>
Reference: 6. <author> N. Karayiannis, </author> <title> "Growing radial basis neural networks: Merging supervised and unsupervised learning with network growth techniques," </title> <journal> IEEE Trans. on Neural Neworks, </journal> <note> in press, </note> <year> 1998. </year>
Reference-contexts: is increased by the user, there is no guarantee of improving the system's performance on a test set, consisting of unobserved ii examples, because the unsupervised algorithm may locate basis functions in regions of the input space where they are either useless or harmful for implementing the desired input-output mapping <ref> [6] </ref>. To overcome this limitation, growing RBF networks based on error-driven learning techniques have been recently proposed in the literature [6]-[9]. <p> The Karayiannis learning scheme is presented below <ref> [6] </ref>: 1. Set c = 2. Initialize prototypes j , j = 1; :::; c. 2.
Reference: 7. <author> B. Fritzke, </author> <title> "Growing cell structures A self-organizing network for unsupervised and supervised learning," </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9), </volume> <pages> pp. 1441-1460, </pages> <year> 1994. </year>
Reference-contexts: If the number of input presentations is a multiple of parameter (equivalent to a number of processing steps), then split the hidden unit selected as the one featuring the highest value of its neuron-based (local) error counter (for more details about this splitting criterion, refer to the literature) <ref> [7] </ref>, [8]. The output weights of the generated unit are initialized. <p> The requantization error can be computed as the squared Euclidean distance between the current input pattern and the prototype vector of the winner unit (for more details, refer to the literature) <ref> [7] </ref>, [8]. If parameter c max is set to 24 (equal to the number of input patterns) and GNG is input with the data set shown in Fig. 1, then Fig. 2 is generated when convergence is reached.
Reference: 8. <author> B. Fritzke, </author> <title> "Some competitive learning methods," draft document, </title> <address> http://www. neuroinformatik.ruhr-uni-bochum.de/ini/VDM/research/gsn/DemoGNG, </address> <year> 1998. </year>
Reference-contexts: If the number of input presentations is a multiple of parameter (equivalent to a number of processing steps), then split the hidden unit selected as the one featuring the highest value of its neuron-based (local) error counter (for more details about this splitting criterion, refer to the literature) [7], <ref> [8] </ref>. The output weights of the generated unit are initialized. <p> The requantization error can be computed as the squared Euclidean distance between the current input pattern and the prototype vector of the winner unit (for more details, refer to the literature) [7], <ref> [8] </ref>. If parameter c max is set to 24 (equal to the number of input patterns) and GNG is input with the data set shown in Fig. 1, then Fig. 2 is generated when convergence is reached.
Reference: 9. <author> N. A. Borghese, and S. Ferrari, </author> <title> "Hierarchical RBF networks and local parameter estimate," Neurocomputing, </title> <publisher> in press, </publisher> <year> 1998. </year>
Reference-contexts: have been recently proposed in the literature [6]-<ref> [9] </ref>. In these systems localized basis functions are selectively positioned at those locations of the input space where it is difficult to approximate the target input-output mapping at the desired level of precision. Growing RBF networks employ either scatter- [6]-[8], or grid-partitioning mechanisms [9], such that the input space is covered with localized basis functions, also termed small patches [4], or data windows [10]. Unlike grid-partitioning systems, scatter-partitioning systems position small patches at locations which are not known a priori. <p> How to insert in such a scheme the on-line CHR or CCHR policy capable of augmenting the expressive power of the model by introducing a competitive mechanism among synaptic links may be the subject of further research. Additional interest may be focused on hierarchical grid-partitioning RBF networks <ref> [9] </ref>, that should be related to wavelets and filter banks theory [21].
Reference: 10. <author> T. </author> <title> Masters, Signal and image processing with neural networks A C++ sourcebook, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Growing RBF networks employ either scatter- [6]-[8], or grid-partitioning mechanisms [9], such that the input space is covered with localized basis functions, also termed small patches [4], or data windows <ref> [10] </ref>. Unlike grid-partitioning systems, scatter-partitioning systems position small patches at locations which are not known a priori.
Reference: 11. <author> J. Buhmann, </author> <title> "Learning and data clustering," in Handbook of Brain Theory and Neural Networks, </title> <editor> M. Arbib, Ed., </editor> <publisher> Bradford Books / MIT Press, </publisher> <year> 1995. </year> <pages> ix </pages>
Reference-contexts: In batch learning parameters are estimated once for every processing epoch, i.e., on the basis of the full training data set. This implies that batch learning can be employed only when the data set of observed examples is finite and small <ref> [11] </ref>.
Reference: 12. <author> B. Fritzke, </author> <title> "A self-organizing network that can follow non-stationary dis-tributions," </title> <booktitle> Proc. of the International Conference on Artificial Neural Networks '97, </booktitle> <publisher> Springer, </publisher> <year> 1997, </year> <pages> pp. 613-618. </pages>
Reference-contexts: contrary, in on-line learning a steady (infinite) stream of data is assumed to be presented to the system; in this case, it may be that the input-output distribution is non-stationary, i.e., it may change with time, so that specific rules for relocating basis functions within the input space are enforced <ref> [12] </ref>.
Reference: 13. <author> T. Martinetz, G. Berkovich, and K. Schulten, </author> <title> "Topology representing networks," </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3), </volume> <pages> pp. 507-522, </pages> <year> 1994. </year>
Reference-contexts: Additional major differences between SGNG and the Karayannis growing system are that: i) while Karayannis' system is incremental, i.e., the number of hidden units increases monotonically, SGNG may generate as well as delete hidden units dynamically; ii) by adopting the Competitive Hebbian Rule (CHR) <ref> [13] </ref>, SGNG is capable of generating synaptic links between pairs of hidden units (intra-layer connections); and iii) according to a heuristic, SGNG may also remove synaptic links. <p> SGNG employs CHR to generate intra-layer connections. It has been proved that CHR guarantees topological preserving mapping <ref> [13] </ref>. The SGNG algorithm is presented below: 1. Set c = 2. Initialize prototypes j and output weights w j , j = 1; :::; c. 2. Select at random an input-output vector pair from the supervised training set. 3. <p> If this inter-pattern distance is measured as the Euclidean distance, then receptive fields centered on prototype vectors are equivalent to Voronoi polyhedra [4], <ref> [13] </ref>, [15]. 4. According to CHR, if a synaptic link between units w 1 and w 2 , identified as s w 1 ;w 2 , does not exist already, create it. <p> It is important to stress that synaptic links generated by CHR belong to an output graph or network of neural units; both synapses and neural units can be projected back onto the input space as a set of template vectors and inter-template connections <ref> [13] </ref>. 5. Move prototype w 1 toward the input pattern by a quantity equal to learning rate * a times the Euclidean distance between template w 1 and the input vector. 6.
Reference: 14. <author> D. Marr, </author> <title> Vision, </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: In this paper we discuss the application of a slightly modified version of SGNG to approximate images, which are 2-D signals. In the mammalian low-level visual system an image is partitioned into segments that are perceived as pictorially uniform <ref> [14] </ref>. The goal of this paper is to assess the ability of SGNG to approximate images while generating maps of hidden units somehow correlated to image segments considered uniform by a human photointerpreter.
Reference: 15. <author> T. Mitchell, </author> <title> Machine learning, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: If this inter-pattern distance is measured as the Euclidean distance, then receptive fields centered on prototype vectors are equivalent to Voronoi polyhedra [4], [13], <ref> [15] </ref>. 4. According to CHR, if a synaptic link between units w 1 and w 2 , identified as s w 1 ;w 2 , does not exist already, create it.
Reference: 16. <author> M. Wertheimer, </author> <title> "Laws of organization in perceptual forms" (partial translation), </title> <booktitle> in A Source-book of Gestalt Psychology, </booktitle> <pages> pp. 71-88, </pages> <publisher> Harcourt, Brace and Company, </publisher> <year> 1938. </year>
Reference-contexts: with the size of the network rather than with the size of the training set. 3 Constrained CHR based on perceptual grouping The Gestaltist theory has long ago revealed the existence of innate perceptual mechanisms capable of organizing visual stimuli (observations) into perceived objects which are separated from their background <ref> [16] </ref>. For our purpose the problem of perceptual grouping can be described by considering the set of points shown in Fig. 1, hereafter referred to as Simpson's data set, consisting of 24 vectors [17]. Typically, different human observers would provide different partitions of the Simpson data set.
Reference: 17. <author> P. Simpson, </author> <title> "Fuzzy min-max neural networks Part 2: clustering," </title> <journal> IEEE Trans. Fuzzy Systems, </journal> <volume> 1(1), </volume> <pages> pp. 32-45, </pages> <year> 1993. </year>
Reference-contexts: For our purpose the problem of perceptual grouping can be described by considering the set of points shown in Fig. 1, hereafter referred to as Simpson's data set, consisting of 24 vectors <ref> [17] </ref>. Typically, different human observers would provide different partitions of the Simpson data set. This means that perceptual grouping is an ill-posed problem which allows different (subjective) solutions depending on vi the state of (subjective) prior world knowledge. This view is consistent with a Bayesian interpretation of the grouping percept.
Reference: 18. <author> J. Shi, and J. Malik, </author> <title> "Normalized cuts and image segmentation," </title> <booktitle> Proc. IEEE Conf. on Comp. Vision and Pattern Recognition, </booktitle> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: The difficulty, of course, is in specifying the prior world knowledge; some of it relates to low level visual processing based on obervations about intensity, color and texture features, but some of it also relates to higher processing levels involving symmetries of objects or object models <ref> [18] </ref>. Let us consider the GNG clustering algorithm, which is the unsupervised version of SGNG.
Reference: 19. <author> A. Baraldi, and F. Parmiggiani, </author> <title> "Novel neural network model combining radial basis function, competitive Hebbian learning rule, and fuzzy simplified adaptive resonance theory," </title> <booktitle> Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Applications of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1997, </year> <journal> vol. </journal> <volume> 3165, </volume> <pages> pp. 98-112. </pages>
Reference-contexts: This is tantamount to saying that the ratio between the length of the longest and shortest connection emanating from any template must always be k. This connection generation policy is termed Constrained CHR (CCHR) <ref> [19] </ref>. When CCHR is adopted, slightly modified versions of GNG and SGNG must be developed, such that a new criterion for removing processing units replaces that described at Step 8 in Section 2.2. Our choice is to employ the following heuristic.
Reference: 20. <author> D. Burr, and M. C. Morrone, </author> <title> "A nonlinear model of feature detection," in Nonlinear Vision: Determination of Neural Receptive Fields, Functions, and Networks, </title> <editor> R. B. Pinter and N. Bahram, </editor> <booktitle> Eds., </booktitle> <pages> pp. 309-327, </pages> <publisher> CRC Press, </publisher> <address> Boca Raton, </address> <year> 1992. </year>
Reference-contexts: To verify how well SGNG performs as image segmenter we consider some synthetic images for testing. It is known that contour pixels belong to: edges (step or ramp edges), ridges (e.g., a line represents a narrow ridge), roofs, or to a combination of these structures <ref> [20] </ref>. These image features are shown in Figs. 5 to 8, where images consist of 50 fi 50 pixels. <p> According to the psychophysical phenomenon of the Mach bands, which is one of the best known brightness illusions, when a luminance (radiance, intensity) ramp meets a plateau there is a spike of brightness (i.e., perceived luminance), although there is continuity in the luminance profile <ref> [20] </ref>. This phenomenon should be consistent with an ideal behavior of SGNG provided with CCHR in approximating the surfaces depicted in Figs. 5 to 8.
Reference: 21. <author> G. Strang, and T. Nguyen, </author> <title> Wavelets and Filter Banks, </title> <publisher> Wellesley-Cambridge Press, </publisher> <address> Wellesley (MA), </address> <year> 1997. </year> <title> x 24 points. data set when CHR is employed. data set when CCHR is employed: parameter k = 1:2. data set when CCHR is employed: parameter k = 1:6. xi edge. ridge. xii ramp. edge. ridge. ramp. roof. xiii step edge. ridge. ramp. roof. </title> <type> xiv </type>
Reference-contexts: Additional interest may be focused on hierarchical grid-partitioning RBF networks [9], that should be related to wavelets and filter banks theory <ref> [21] </ref>.
References-found: 21

