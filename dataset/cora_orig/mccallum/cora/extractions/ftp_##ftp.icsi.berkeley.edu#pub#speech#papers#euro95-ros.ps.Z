URL: ftp://ftp.icsi.berkeley.edu/pub/speech/papers/euro95-ros.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/papers.html
Root-URL: http://www.icsi.berkeley.edu
Email: morgang@icsi.berkeley.edu  
Title: FAST SPEAKERS IN LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION: ANALYSIS ANTIDOTES  
Author: Nikki Mirghafori, Eric Fosler, and Nelson Morgan 
Address: fnikki, fosler,  
Affiliation: International Computer Science Institute and University of California, Berkeley  
Abstract: The performance of automatic speech recognizers (ASR) typically degrades for test speakers with "outlier" characteristics, for example, speakers with foreign accent and fast speaking rate. In this work, we concentrate on the latter. Consistent with other researchers, we have observed that for speakers with exceptionally high speaking rate, the word recognition error is significantly higher. We have investigated two possible causes for this effect. Inherent spectral differences may cause the extracted features for these outliers to be significantly different from that of normal speech. Also, due to phone omissions and duration reduction, the normal word-models may not be suitable for fast speech. Based on our exploratory experiments on TIMIT and WSJ corpora, we believe the spectral differences and duration reduction are both significant sources of the increased error. By adapting our MLP phonetic probability estimator to fast speech, and employing fast speaker word-models, we have been able to eliminate about 16% of the fast speaker word recognition errors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hermansky, H. </author> <title> Perceptual Linear Predictive (PLP) Analysis of Speech, </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> Vol 87, </volume> <pages> pp. 1738-1752, </pages> <year> 1990. </year>
Reference-contexts: We chose 400 sentences from the SX training set, 100 for each combination of ff astest; slowestg fl fmale; f emaleg. Then we calculated the PLP12 & energy features and their deltas <ref> [1] </ref> (a total of 26 features) for each 20 msec window of speech, overlapped every 10 msec. We trained a two-layer neural network (26 input, 50 hidden, and 2 output units) for each phone on fast and slow speakers' extracted features.
Reference: [2] <author> Kaisse, E. </author> <title> Connected Speech: the Interaction of Syntax and Phonology. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: In both cases, the overall recognition suffered slightly due to increase in slow speaker error. Finally, we have introduced alternate pronunciations into our word models which represent the phone reduction and deletion effects often seen in fast speech <ref> [8, 9, 10, 2] </ref>. These pronunciations were generated by twenty surface-phonological rules applied to the base (single pronunciation) lexicon. These rules provided an average of 2.41 pronunciations per word for the 5k WSJ test set lexicon.
Reference: [3] <author> Lindblom, B. </author> <title> Spectrographic Study of Vowel Reduction. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> Vol 35, </volume> <pages> pp. 1773-1781, </pages> <year> 1963. </year>
Reference-contexts: For some phones, such as /uw/, /uh/, /en/, /oy/, /aw/, /ux/, /y/, /ao/, /ow/, /hh/, and /ay/ (mostly diphthongs and glides) the classification score was between 80-90%. This makes particular sense in the light of psycho-acoustical studies that suggest diphthongs and glides are most affected by ROS variations <ref> [3] </ref>. The most difficult phones for speed discrimination were, unsurprisingly, the silence phones, closures, stops, and some fricatives. It is evident that features for fast and slow sounds are different. The next question is whether this difference is causing the higher recognition error rate for fast speakers.
Reference: [4] <author> Pallett, D.S., Fiscus, J.G., Fisher, W.M., Garofolo, J.S., Lund, B.A., Przybocki, M.A. </author> <title> 1993 WSJ-CSR Benchmark Test Results, </title> <booktitle> ARPA's Spoken Language Systems Technology Workshop, </booktitle> <address> Princeton, New Jersey, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION In a recent NIST WSJ evaluation (Nov 93) all participating systems had about 2-3 times higher word error rates on the two fastest speakers <ref> [4] </ref> (see Figure 1). In an earlier NIST Resource Management (RM) Sep 92 evaluation, this strong effect was also observed, as all participating systems had 2-4 times more error on the fastest (and one of the slowest 1 ) speakers [5].
Reference: [5] <author> Pallett, D.S., Fiscus, J.G., and Garofolo, J.S. </author> <title> Resource Management Corpus: September 1992 Test Set Benchmark Test Results, ARPA's Continuous Speech Recognition Workshop, </title> <publisher> Stanford, </publisher> <address> California, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: In an earlier NIST Resource Management (RM) Sep 92 evaluation, this strong effect was also observed, as all participating systems had 2-4 times more error on the fastest (and one of the slowest 1 ) speakers <ref> [5] </ref>. This observation naturally inspires the following question: "why do the ASR systems perform significantly worse on fast speakers?". We have considered two reasons for the higher error rate of faster speakers.
Reference: [6] <author> Siegler, M.A., and Stern, </author> <title> R.M., On The Effects Of Speech Rate In Large Vocabulary Speech Recognition Systems, </title> <booktitle> Proceedings of ICASSP '95, </booktitle> <pages> pp. 612-615, </pages> <address> Detroit, Michigan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: For WSJ0 training set ROS = 13:24 and ROS = 1:80. In our experiments, we use ICSI's hybrid HMM/MLP speech recognition system, which participated in the WSJ 93 (5K bigram task) and RM 92 NIST evaluations. As observed in Figure 1 and discussed in the work of Siegler <ref> [6] </ref>, similar rate of speech (ROS) effects have been observed for mixture of Gaussian systems, so it is hoped that the conclusions of our work are useful in those systems as well. 2. <p> The system overall performed best on all sentences at x = 0:7, giving 15.7% error, but assuming an ideal ROS detector the system would have improved to 15.0% error. Such a detector could be approximated by approaches discussed in <ref> [6] </ref>, perhaps in combination with local detectors as described earlier in section 2.1. An alternative would be to simply reduce the minimum phone durations. We tried this in both phone-independent (Figure 5.c) and phone-specific (Figure 5.d) duration scaling experiments. <p> The merged system improved the word recognition error rate of fast speakers (i.e., speakers with ROS &gt; + 1:65) by 16% relative to the baseline system. In all cases, the error of the slower sentences was increased. Assuming an ideal ROS detector (an approximation of which is discussed in <ref> [6] </ref>), the overall error of our system on WSJ-93 evaluation set would be 14.9%, which is an improvement over 16.1% of our baseline system. More importantly, the ROS-tuned system is more robust to fast speakers, for whom the system might fail seriously.
Reference: [7] <author> Stolcke, A., and Omohundro, S., </author> <title> Best-first Model Merging for Hidden Markov Model Induction, </title> <booktitle> TR-94-003, ICSI, </booktitle> <address> Berkeley, CA, </address> <month> January </month> <year> 1994. </year>
Reference: [8] <author> Zwicky, A. </author> <title> Auxiliary Reduction in English. </title> <journal> Linguistic Inquiry 1.323-336, </journal> <year> 1970. </year>
Reference-contexts: In both cases, the overall recognition suffered slightly due to increase in slow speaker error. Finally, we have introduced alternate pronunciations into our word models which represent the phone reduction and deletion effects often seen in fast speech <ref> [8, 9, 10, 2] </ref>. These pronunciations were generated by twenty surface-phonological rules applied to the base (single pronunciation) lexicon. These rules provided an average of 2.41 pronunciations per word for the 5k WSJ test set lexicon.
Reference: [9] <author> Zwicky, A. </author> <title> Note on a Phonological Hierarchy in English. Linguistic Change and Generative Theory, </title> <editor> ed. by R Stockwell & R. Macaulay. </editor> <publisher> Indiana University Press, </publisher> <year> 1972. </year>
Reference-contexts: In both cases, the overall recognition suffered slightly due to increase in slow speaker error. Finally, we have introduced alternate pronunciations into our word models which represent the phone reduction and deletion effects often seen in fast speech <ref> [8, 9, 10, 2] </ref>. These pronunciations were generated by twenty surface-phonological rules applied to the base (single pronunciation) lexicon. These rules provided an average of 2.41 pronunciations per word for the 5k WSJ test set lexicon.
Reference: [10] <editor> Zwicky, A. </editor> <booktitle> On Casual Speech. In Eighth Regional Meeting of the Chicago Linguistic Society, </booktitle> <pages> pp. 607-615, </pages> <year> 1972. </year>
Reference-contexts: In both cases, the overall recognition suffered slightly due to increase in slow speaker error. Finally, we have introduced alternate pronunciations into our word models which represent the phone reduction and deletion effects often seen in fast speech <ref> [8, 9, 10, 2] </ref>. These pronunciations were generated by twenty surface-phonological rules applied to the base (single pronunciation) lexicon. These rules provided an average of 2.41 pronunciations per word for the 5k WSJ test set lexicon.
References-found: 10

