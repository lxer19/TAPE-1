URL: http://www.tc.cornell.edu/UserDoc/Software/Num/matlab/docs/help/pdf_doc/otherdocs/sorensen.ps
Refering-URL: http://www.tc.cornell.edu/UserDoc/Software/Num/matlab/docs/help/fulldocset.html
Root-URL: http://www.tc.cornell.edu
Title: IMPLICITLY RESTARTED ARNOLDI/LANCZOS METHODS FOR LARGE SCALE EIGENVALUE CALCULATIONS  
Author: D. C. Sorensen 
Keyword: Key words and phrases: Large scale eigenvalue problems, Arnoldi methods, Lanczos methods, Krylov subspace pro jection, Implicit restarting,  
Note: AMS classification: Primary 65F15; Secondary  This work was supported in part by the National Science Foundation contract ASC-9408795, National Science Foundation cooperative agreement CCR-9120008 and by DARPA through U.S. Army ORA7453.01.  
Address: Houston, Texas 77251-1829  
Affiliation: Department of Computational and Applied Mathematics Rice University  
Date: October 25, 1995  
Web: 65G05  
Abstract: This report provides an introductory overview of the numerical solution of large scale algebraic eigenvalue problems. The main focus is on a class of methods called Krylov subspace projection methods. The Lanczos method is the premier member of this class and the Arnoldi method is a generalization to the nonsymmetric case. A recently developed and very promising variant of the Arnoldi/Lanczos scheme called the Implicitly Restarted Arnoldi Method is presented here in detail. This method is highlighted because of its suitability as a basis for software development. It may be viewed as a truncated form of the implicitly shifted QR-algorithm that is appropriate for very large problems. Based on this technique, a public domain software package called ARPACK has been developed in Fortran 77 for finding a few eigenvalues and eigenvectors of large scale symmetric, nonsymmetric, standard or generalized problems. This package has performed well on workstations, parallel-vector supercomputers, distributed memory parallel systems and clusters of workstations. The important features of this package are presented along with a discussion some applications and performance indicators. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W.E. </author> <title> Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue problem, </title> <journal> Quart. Appl. Math. </journal> <month> 9 , 17-29 </month> <year> (1951) </year> . 
Reference-contexts: The second observation leads to the Lanczos/Arnoldi process <ref> [23, 1] </ref>. 4 The Arnoldi Factorization Definition : If A 2 C nfin then a relation of the form AV k = V k H k + f k e T where V k 2 C nfik has orthonormal columns, V H k f k = 0 and H k 2
Reference: [2] <author> Z. Bai, D. Day and Q. Ye, ABLE: </author> <title> an Adaptive Block Lanczos Method for Non-Hermitian Eigenvalue Problems, </title> <type> Tech. </type> <address> Rept. 95-04, U. Kentucky, Lexington (1995). </address>
Reference-contexts: Certainly in the nonsymmetric case there is no "black box" technique and it is questionable that there is one in the symmetric case either. A block method called ABLE based upon two-sided nonsymmetric Lanczos is being developed by Bai, Day and Ye <ref> [2] </ref>. Software based upon subspace iteration with Chbeychev acceleration has been developed by Duff and Scott [12]. Jennifer Scott has also developed software based upon an explicitly restarted Chebyshev-Arnoldi method [42].
Reference: [3] <author> Z.Bai and G.W. Stewart, </author> <title> SRRIT A FORTRAN subroutine to calculate the dominant invariant subspace of a nonsymmetric matrix, </title> <type> Tech. </type> <institution> Rept. 2908, Dept of Computer Science, U. Maryland (1992). </institution>
Reference-contexts: One alternative is to introduce a block form of the simple power method which is often called subspace iteration. This important class of algorithms has been developed and investigated in [46]. Several software efforts have been based upon this approach <ref> [3, 47, 12] </ref>. However, there is another class of algorithms called Krylov subspace projection methods that are based upon the intricate structure of the sequence of vectors naturally produced by the power method.
Reference: [4] <author> A. Baliga, D. Trifedi, N.G. Anderson, </author> <title> Tensile-strain effects in quantum-well and su-perlattice band structures Phys. </title> <journal> Rev. B (1994). </journal>
Reference-contexts: ARPACK was used to find highly accurate solutions to these nonsymmetric problems which couldn't be solved by other means. See [25] for details. Researchers at U. Massachusetts have used ARPACK to solve the eignvalue problems arising in their FEM quantum well Kp model for strained layer superlattices <ref> [4] </ref>. A final example of non-symmetric eigenproblems to be discussed here arises in magneto-hydrodynamics (MHD) involving the study of the interaction of a plasma and a magnetic field. The MHD equations describe the macroscopic behavior of the plasma in the magnetic field.
Reference: [5] <author> F. Chatelin and D. Ho, </author> <title> Arnoldi-Tchebychev procedure for large scale nonsymmetric matrices, </title> <journal> Math. Modeling and Num. Analysis , 24,53-65 (1990). </journal> <volume> 32 </volume>
Reference-contexts: Both of these are based upon well known theory of polynomial approximation. The problem of constructing an optimal ellipse for this problem has been studied by Chatelin and Ho. The reader is referred to <ref> [5] </ref> for details of constructing these polynomials. The reasoning behind this type of algorithm is that that if v 1 is a linear combination of precisely k eigenvectors of A then Arnoldi factorization terminates in k steps (i.e. f k = 0).
Reference: [6] <author> J. Cullum, </author> <title> The simultaneous computation of a few of the algebraically largest and smallest eigenvalues of a large, symmetric, sparse matrix, </title> <type> BIT 18, </type> <month> 265-275 </month> <year> (1978). </year>
Reference-contexts: This last option introduces the almost certain possibility of introducing spurious eigenvalues. Various techniques have been developed to detect and deal with the presence of spurious eigenvalues <ref> [6, 8] </ref>. The appearance of spurious eigenvalues may be avoided through complete orthogonal-ization of the Arnoldi (or Lanczos) vectors using the DGKS correction. Computational cost has been cited as the reason for not employing this option .
Reference: [7] <author> J. Cullum and W.E. Donath, </author> <title> A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace for large, sparse symmetric matrices, </title> <booktitle> in Proc. 1974 IEEE Conference on Decision and Control, </booktitle> <publisher> IEEE Press, </publisher> <address> New York, </address> <month> 505-509 </month> <year> (1974). </year>
Reference-contexts: However, the cost will be reasonable if one is able to fix k at a modest size and then update the starting vector v 1 = V k e 1 while repeatedly doing k-Arnoldi steps. This approach was introduced in [21] and developed further by <ref> [7] </ref> for the symmetric case. Saad [38, 39, 40] has developed explicit restarting for the nonsymmetric case.
Reference: [8] <author> J. Cullum and R.A. Willoughby, </author> <title> Computing eigenvalues of very large symmetric matrices an implementation of a Lanczos algorithm with no reorthogonalization, </title> <journal> J. Comput. Phys. </journal> <volume> 434, </volume> <month> 329-358 </month> <year> (1981). </year>
Reference-contexts: This last option introduces the almost certain possibility of introducing spurious eigenvalues. Various techniques have been developed to detect and deal with the presence of spurious eigenvalues <ref> [6, 8] </ref>. The appearance of spurious eigenvalues may be avoided through complete orthogonal-ization of the Arnoldi (or Lanczos) vectors using the DGKS correction. Computational cost has been cited as the reason for not employing this option .
Reference: [9] <author> J. Daniel, W.B. Gragg, L. Kaufman, G.W. Stewart, </author> <title> Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization, </title> <journal> Math. </journal> <volume> Comp.,30, </volume> <month> 772-795 </month> <year> (1976). </year>
Reference-contexts: In finite precision arithmetic, care must be taken to assure that the computed vectors are orthogonal to working precision. The method proposed by Daniel, Gragg, Kaufman and Stewart (DGKS) in <ref> [9] </ref> provides an excellent way to construct a vector f j+1 that is numerically orthogonal to V j+1 . It amounts to computing a correction s = V T just after Step (a4.4) if necessary. <p> There have been several approaches to overcome this problem in the symmetric case. They include: (1) complete re-orthogonalization, which may be accomplished through maintaining V in product Householder form [50, 17] or through the Modified Gram-Schmidt processes with re-orthogonalization <ref> [9] </ref>. (2) Selective re-orthogonalization, which has been proposed by Parlett and has been heavily researched by him and his students.
Reference: [10] <author> J.J. Dongarra, J. Du Croz, S. Hammarling, and R.J. Hanson, </author> <title> Algorithm 656 An extended set of fortran basic linear algebra subprograms: Model implementation and test programs, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 14, </volume> <month> 18-32 </month> <year> (1988). </year>
Reference-contexts: This is quite important for performance on vector, and parallel-vector supercomputers. The BLAS operation GEMV is easily parallelized and vectorized and has a much better ratio of floating point computation to data movement <ref> [10, 11] </ref>. The Modified Gram-Schmidt Process (MGS) is often used in the construction of Arnoldi factorizations. However, MGS will definitely not produce numerically orthogonal basis vectors in practice.
Reference: [11] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen and H.A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia (1991). </address>
Reference-contexts: This is quite important for performance on vector, and parallel-vector supercomputers. The BLAS operation GEMV is easily parallelized and vectorized and has a much better ratio of floating point computation to data movement <ref> [10, 11] </ref>. The Modified Gram-Schmidt Process (MGS) is often used in the construction of Arnoldi factorizations. However, MGS will definitely not produce numerically orthogonal basis vectors in practice.
Reference: [12] <author> I.S. Duff and J Scott, </author> <title> Computing selected eigenvalues of large sparse unsymmetric matrices using subspace iteration, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 19, </volume> <month> 137-159 </month> <year> (1993). </year>
Reference-contexts: One alternative is to introduce a block form of the simple power method which is often called subspace iteration. This important class of algorithms has been developed and investigated in [46]. Several software efforts have been based upon this approach <ref> [3, 47, 12] </ref>. However, there is another class of algorithms called Krylov subspace projection methods that are based upon the intricate structure of the sequence of vectors naturally produced by the power method. <p> A block method called ABLE based upon two-sided nonsymmetric Lanczos is being developed by Bai, Day and Ye [2]. Software based upon subspace iteration with Chbeychev acceleration has been developed by Duff and Scott <ref> [12] </ref>. Jennifer Scott has also developed software based upon an explicitly restarted Chebyshev-Arnoldi method [42].
Reference: [13] <editor> W.S. Edwards, L.S. Tuckerman, R.A. Friesner and D.C. Sorensen, </editor> <title> Krylov Methods for the Incompressible Navier-Stokes Equations, </title> <journal> Journal of Computational Physics, </journal> <month> 110,82-102 </month> <year> (1994). </year>
Reference-contexts: One interesting facet of this application is that the matrices are not available explicitly and are logically dense. The particular discretization provides efficient matrix-vector products through Fourier transform. Details may be found in <ref> [13] </ref>. Very large symmetric generalized eigenproblems arise in structural analysis. One example that we have worked with at Cray Research through the courtesy of Ford Motor Company involves an automobile engine model constructed from 3D solid elements.
Reference: [14] <author> T. Ericsson and A. Ruhe, </author> <title> The spectral transformation Lanczos method for the numerical solution of large sparse generalized symmetric eigenvalue problems, </title> <journal> Math. Comp. </journal> <volume> 35, </volume> <month> 1251-1268 </month> <year> (1980). </year>
Reference-contexts: Spectral transformations were studied extensively by Ericsson and Ruhe <ref> [14] </ref> and the first eigenvector purification strategy was developed in [32]. Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon. The many nuances of this technique in practical applications is discussed thoroughly in [19].
Reference: [15] <author> L. Feinswog, M. Sherman, W. Chiu, </author> <title> D.C. Sorensen, Improved Computational Methods for 3-Dimensional Image Reconstruction, </title> <type> CRPC Tech. </type> <institution> Rept., Rice University (in preparation). </institution>
Reference-contexts: Our initial effort has been to replace the existing algorithm for computing the SVD with ARPACK which has increased the speed of the analysis by a factor of 7 on an Iris workstation. The accuracy of the results were also increased dramatically. Details are reported in <ref> [15] </ref>. Computational chemistry provides a rich source of problems. ARPACK is being used in two applications currently and holds promise for a variety of challenging problems in this area. We are collaborating with researchers at Ohio State on large scale three-dimensional reactive scattering problems.
Reference: [16] <author> J.G.F. Francis, </author> <title> The QR transformation: A unitary analogue to the LR transformation, Parts I and II, </title> <journal> Comp. J. </journal> <volume> 4, </volume> <pages> 265-272, </pages> <month> 332-345 </month> <year> (1961). </year>
Reference-contexts: The columns of Q are called Schur vectors in general and these are eigenvectors of A if and only if A is normal. For purposes of algorithmic development this structure is fundamental. In fact, the well known Implicitly Shifted QR-Algorithm <ref> [16] </ref> is designed to produce a sequence of unitary similarity transformations Q j that iteratively reduce A to upper triangular form.
Reference: [17] <author> G.H. Golub, R. Underwood, and J.H. Wilkinson, </author> <title> The Lanczos algorithm for the symmetric Ax = Bx problem, </title> <type> Report STAN-CS-72-270, </type> <institution> Department of Computer Science, Stanford U. Stanford, California ,(1972). </institution>
Reference-contexts: The identification of this phenomenon in the symmetric case and the first rigorous numerical treatment is due to Paige [31]. There have been several approaches to overcome this problem in the symmetric case. They include: (1) complete re-orthogonalization, which may be accomplished through maintaining V in product Householder form <ref> [50, 17] </ref> or through the Modified Gram-Schmidt processes with re-orthogonalization [9]. (2) Selective re-orthogonalization, which has been proposed by Parlett and has been heavily researched by him and his students.
Reference: [18] <author> G.H. Golub and C.F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland (1983). </address> <month> 33 </month>
Reference-contexts: The elegant details of an efficient and stable implementation would be too much of a digression here. They may be found in <ref> [18] </ref>. The convergence behavior of this iteration is fascinating. The columns of V converge to Schur vectors at various rates. These rates are fundamentally linked to the simple power method and its rapidly convergent variant, inverse iteration (see [51]). <p> This method, called Rayleigh Quotient Iteration, has very impressive convergence rates indeed. Rayleigh Quotient Iteration converges at a quadratic rate in general and at a cubic rate on Hermitian problems. For a more detailed discussion of the eigenvalue problem and basic algorithms see <ref> [52, 46, 18] </ref>. 3 Krylov Subspaces and Projection Methods Although the rate of convergence can be improved to an acceptable level through spectral transformations, power iterations are only able to find one eigenvector at a time.
Reference: [19] <author> R.G. Grimes, J.G. Lewis and H.D. Simon, </author> <title> A shifted block Lanczos algorithm for solving sparse symmetric generalized eigenproblems, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 15, </volume> <month> 228-272 </month> <year> (1994). </year>
Reference-contexts: Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon. The many nuances of this technique in practical applications is discussed thoroughly in <ref> [19] </ref>. The development presented here and the eigenvector purification through implicit restarting is due to Meerbergen and Spence [29]. 6.3 AN EXAMPLE This discussion is illustrated with the following example.
Reference: [20] <author> P. Pendergast. Z. Darakjian, E. F. Hayes, </author> <title> D.C. Sorensen, Scalable Algorithms for Three-dimensional Reactive Scattering: Evaluation of a New Algorithm for Obtaining Surface Functions, </title> <journal> J. Comp. Phys. </journal> , <month> 113,201-214 </month> <year> (1994). </year>
Reference-contexts: For details about the method and experimental results, see <ref> [20] </ref>, [45]. Nonsymmetric problems also arise in quantum chemistry. Researchers at University of Washington have used the code to investigate the effects of the electric field on InAs/GaSb and GaAs/Al x Ga 1x as quantum wells.
Reference: [21] <author> W. Karush, </author> <title> An iterative method for finding characteristic vectors of a symmetric matrix, </title> <journal> Pacific J. Math. </journal> <volume> 1, </volume> <month> 233-248 </month> <year> (1951). </year>
Reference-contexts: However, the cost will be reasonable if one is able to fix k at a modest size and then update the starting vector v 1 = V k e 1 while repeatedly doing k-Arnoldi steps. This approach was introduced in <ref> [21] </ref> and developed further by [7] for the symmetric case. Saad [38, 39, 40] has developed explicit restarting for the nonsymmetric case.
Reference: [22] <author> M.N. Kooper, H.A. van der Vorst, S. Poedts, and J.P. Goedbloed, </author> <title> Application of the Implicitly Updated Arnoldi Method with a Complex Shift and Invert Strategy in MHD, </title> <type> Tech. </type> <institution> Rept., Institute for Plasmaphysics, FOM Rijnhuizen, Nieuwegin, The Netherlands (Sep. </institution> <note> 1993) (submitted to Journal of Computational Physics). </note>
Reference-contexts: The code is not only computes extremely accurate solutions, it does so very efficiently in comparison to other methods that have been tried. See <ref> [22] </ref> for details. There are many other applications.
Reference: [23] <author> C. </author> <title> Lanczos, An iteration method for the solution of the eigenvalue problem of linear differential and integral operators, </title> <institution> J. Res. Nat. Bur. Stand. </institution> , <month> 45, 255-282 </month> <year> (1950). </year>
Reference-contexts: The second observation leads to the Lanczos/Arnoldi process <ref> [23, 1] </ref>. 4 The Arnoldi Factorization Definition : If A 2 C nfin then a relation of the form AV k = V k H k + f k e T where V k 2 C nfik has orthonormal columns, V H k f k = 0 and H k 2
Reference: [24] <author> R.B. Lehoucq, </author> <title> Analysis and Implementation of an Implicitly Restarted Arnoldi Iteration Ph.D. </title> <type> Thesis, </type> <institution> Rice U. </institution> <note> (1995) (Available as CAAM Tech. </note> <institution> Rept. TR95-13, Rice U., Houston) </institution>
Reference-contexts: However, these details are extremely important to the success of this iteration in difficult cases. Complete details of these numerical refinements may be found in <ref> [26, 24] </ref> The above iteration can be used to apply any known polynomial restart. If the roots of the polynomial are not known there is an alternative implementation that only requires one to compute q 1 = (H)e 1 where is the desired degree p polynomial. <p> The first property was established in <ref> [24] </ref> along with an extensive analysis of the numerical properties of implicit restarting.
Reference: [25] <author> T.L. Li, K.J. </author> <title> Kuhn FEM solution to quantum wells by irreducible formulation Dept. </title> <institution> Elec. Eng. Tech. </institution> <address> Rept. U. Wash. </address> <year> (1993). </year>
Reference-contexts: Researchers at University of Washington have used the code to investigate the effects of the electric field on InAs/GaSb and GaAs/Al x Ga 1x as quantum wells. ARPACK was used to find highly accurate solutions to these nonsymmetric problems which couldn't be solved by other means. See <ref> [25] </ref> for details. Researchers at U. Massachusetts have used ARPACK to solve the eignvalue problems arising in their FEM quantum well Kp model for strained layer superlattices [4].
Reference: [26] <author> R.B. Lehoucq and D.C. Sorensen, </author> <title> Deflation Techniques for an Implicitly Re-started Arnoldi Iteration, </title> <type> CAAM-TR 94-13, </type> <institution> Rice U. </institution> , <address> Houston (1994). </address>
Reference-contexts: However, these details are extremely important to the success of this iteration in difficult cases. Complete details of these numerical refinements may be found in <ref> [26, 24] </ref> The above iteration can be used to apply any known polynomial restart. If the roots of the polynomial are not known there is an alternative implementation that only requires one to compute q 1 = (H)e 1 where is the desired degree p polynomial.
Reference: [27] <author> R. Lehoucq, </author> <title> D.C. Sorensen, P.A. Vu, ARPACK: Fortran subroutines for solving large scale eigenvalue problems, </title> <note> Release 2.1, available from netlib@ornl.gov in the scalapack directory (1994). </note>
Reference-contexts: This software, called ARPACK <ref> [27] </ref>, provides several features which are not present in other codes based upon a single-vector Arnoldi process. One of the most important features from the software standpoint is the reverse communication interface.
Reference: [28] <author> T.A. Manteuffel, </author> <title> Adaptive procedure for estimating parameters for the nonsymmetric Tchebychev iteration, </title> <journal> Numer. Math. </journal> <volume> 31, </volume> <month> 183-208 </month> <year> (1978). </year>
Reference-contexts: in the non-Hermitian case, not only do the basis vectors have to be stored, but the cost of the Hessenberg eigenvalue subproblem is O (k 3 ) at the k-th step. 5.1 EXPLICIT RESTARTING An alternative has been proposed by Saad based upon the polynomial acceleration scheme developed by Manteuffel <ref> [28] </ref> for the iterative solution of linear systems. Saad [39] proposed to restart the iteration with a vector that has been preconditioned so that it is more nearly in a k-dimensional invariant subspace of interest.
Reference: [29] <author> K. Meerbergen and A. Spence, </author> <title> Implicitly restarted Arnoldi with purification for the shift-invert transformation, </title> <type> Tech. </type> <institution> Rept. TW225, Katholieke Universitet Leuven, </institution> <month> Bel-gium </month> <year> (1995). </year>
Reference-contexts: Another recent suggestion due to Meerbergen and Spence is to use implicit restarting with a zero shift <ref> [29] </ref>. Recall that implicit restarting with ` zero shifts is equivalent to starting the M -Arnoldi process with a starting vector of S ` v 1 and all the resulting Ritz vectors will be multiplied by S ` as well. <p> The many nuances of this technique in practical applications is discussed thoroughly in [19]. The development presented here and the eigenvector purification through implicit restarting is due to Meerbergen and Spence <ref> [29] </ref>. 6.3 AN EXAMPLE This discussion is illustrated with the following example. A = K C # " 0 0 ; with A an order 225 matrix approximation to a convection-diffusion operator and C a structured random matrix. <p> A = K C # " 0 0 ; with A an order 225 matrix approximation to a convection-diffusion operator and C a structured random matrix. This example was chosen because it has the block structure of a typical steady-state Navier-Stokes linear stability analysis (see <ref> [29] </ref>).
Reference: [30] <author> R.B. Morgan, </author> <title> On restarting the Arnoldi method for large scale eigenvalue problems, </title> <journal> Math. of Comp. </journal> <note> (to appear). </note>
Reference-contexts: The first property was established in [24] along with an extensive analysis of the numerical properties of implicit restarting. The surprising second property was established by Morgan in <ref> [30] </ref> along with some compelling numerical results indicating superior performance of implicit over explicit restarting. 17 6 The Generalized Eigenvalue Problem A typical source of large scale eigenproblems is through a discrete form of a continuous problem.
Reference: [31] <author> C.C. Paige, </author> <title> The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices, </title> <type> Ph.D. thesis, </type> <institution> Univ. of London (1971). </institution>
Reference-contexts: The identification of this phenomenon in the symmetric case and the first rigorous numerical treatment is due to Paige <ref> [31] </ref>. There have been several approaches to overcome this problem in the symmetric case.
Reference: [32] <author> B. Nour-Omid, B.N. Parlett, T. Ericsson, and P.S. Jensen, </author> <title> How to implement the spectral transformation, </title> <journal> Math. of Comp., </journal> <volume> 48, </volume> <month> 663-673 </month> <year> (1987). </year> <month> 34 </month>
Reference-contexts: One is to note that if x = V y with Hy = y then Sx = V Hy + f e T k y = x + f e T k y and this is the correction suggested by <ref> [32] </ref>. Another recent suggestion due to Meerbergen and Spence is to use implicit restarting with a zero shift [29]. <p> Spectral transformations were studied extensively by Ericsson and Ruhe [14] and the first eigenvector purification strategy was developed in <ref> [32] </ref>. Shift and invert techniques play an essential role in the block Lanczos code developed by Grimes, Lewis, and Simon. The many nuances of this technique in practical applications is discussed thoroughly in [19]. <p> The + symbol denotes the updated basis after purging. The next table shows the residual norms for the two approximate eigenvalues that are closest to the shift before and after purging. Clearly, there is considerable merit to doing this purging. This generalizes the purging proposed by <ref> [32] </ref> and seems to be quite promising.
Reference: [33] <author> B.N. Parlett and D. S. Scott, </author> <title> The Lanczos algorithm with selective orthogonalization, </title> <journal> Math. Comp. </journal> <volume> 33, </volume> <month> 311-328 </month> <year> (1979). </year>
Reference-contexts: Most notably, the theses and subsequent papers and computer codes of Scott and of Simon have developed this idea <ref> [34, 33, 43] </ref>. (3) No re-orthogonalization, which has been developed by Cullum and her colleagues. This last option introduces the almost certain possibility of introducing spurious eigenvalues. Various techniques have been developed to detect and deal with the presence of spurious eigenvalues [6, 8].
Reference: [34] <author> B.N. Parlett, </author> <title> The Symmetric Eigenvalue Problem , Prentice-Hall, </title> <address> Englewood Cliffs, NJ. </address> <year> (1980). </year>
Reference-contexts: = 1) and the associated Rayleigh Quotient residual r (x) = Ax x satisfies kr (x)k = jfi k e T When A is Hermitian, this relation may be used to provide computable rigorous bounds on the accuracy of the eigenvalues of H as approximations to eigenvalues of A (see <ref> [34] </ref>). When A is non-Hermitian the possibility of non-normality precludes such bounds and one can only say that the RQ-residual is small if jfi k e T k yj is small. However, in either case, if 9 f k = 0 these the Ritz pairs become exact eigenpairs of A. <p> Most notably, the theses and subsequent papers and computer codes of Scott and of Simon have developed this idea <ref> [34, 33, 43] </ref>. (3) No re-orthogonalization, which has been developed by Cullum and her colleagues. This last option introduces the almost certain possibility of introducing spurious eigenvalues. Various techniques have been developed to detect and deal with the presence of spurious eigenvalues [6, 8].
Reference: [35] <author> T.D. Romo, J.B. Clarage, D.C. Sorensen, and G.N. Phillips, Jr., </author> <title> Automatic Identification of Discrete Substates in Proteins: Singular Value Decomposition Analysis of Time Averaged Crystallographic Refinements, </title> <type> CRPC-TR 94481, </type> <institution> Rice University (Oct. </institution> <year> 1994). </year>
Reference-contexts: The second is an application to molecular dynamical simulation of the motions of proteins. The SVD may be used to compress the data required to represent the simulation and more importantly to provide an analytical tool to help in understanding the function of the protean. See <ref> [35] </ref> for further details of the molecular dynamics application. The underlying algorithm for reconstructing 3-D image reconstruction of biological macromolecules from 2-D projections [48] is based upon the statistical technique of principal component analysis [49].
Reference: [36] <author> A. Ruhe, </author> <title> Rational Krylov sequence methods for eigenvalue computation, Linear Algebra Apps., </title> <type> 58, </type> <month> 391-405 </month> <year> (1984). </year>
Reference-contexts: Software based upon subspace iteration with Chbeychev acceleration has been developed by Duff and Scott [12]. Jennifer Scott has also developed software based upon an explicitly restarted Chebyshev-Arnoldi method [42]. Finally, the Rational Krylov method being developed by Ruhe <ref> [36, 37] </ref> is very promising for the nonsymmetric problem when a factorization of the matrix is possible. 9 Acknowledgements The computational results presented in Section 7 are due to Zdenko Tomasic and Dan Hu.
Reference: [37] <author> A. Ruhe, </author> <title> Rational Krylov sequence methods for eigenvalue computation II, Linear Algebra Apps., </title> <address> 197,198, </address> <month> 283-295 </month> <year> (1994). </year>
Reference-contexts: Software based upon subspace iteration with Chbeychev acceleration has been developed by Duff and Scott [12]. Jennifer Scott has also developed software based upon an explicitly restarted Chebyshev-Arnoldi method [42]. Finally, the Rational Krylov method being developed by Ruhe <ref> [36, 37] </ref> is very promising for the nonsymmetric problem when a factorization of the matrix is possible. 9 Acknowledgements The computational results presented in Section 7 are due to Zdenko Tomasic and Dan Hu.
Reference: [38] <author> Y. Saad, </author> <title> Variations on Arnoldi's method for computing eigenelements of large unsym-metric matrices, Linear Algebra Apps., </title> <type> 34, </type> <month> 269-295 </month> <year> (1980). </year>
Reference-contexts: This approach was introduced in [21] and developed further by [7] for the symmetric case. Saad <ref> [38, 39, 40] </ref> has developed explicit restarting for the nonsymmetric case. <p> Again, the motivation here is that the Arnoldi residual f k would vanish if these k Ritz vectors were actually eigenvectors of A and the Ritz vectors are the best available approximations to these eigenvectors. A heuristic choice for the coefficients fl j has also been suggested by Saad <ref> [38] </ref>. It is to weight the j-th Ritz vector with the value of its Ritz estimate and then normalize so that the new starting vector has norm 1. This has the effect of favoring the Ritz vectors that have least converged.
Reference: [39] <author> Y. Saad, </author> <title> Chebyshev acceleration techniques for solving nonsymmetric eigenvalue problems, </title> <journal> Math. Comp., </journal> <volume> 42, </volume> <month> 567-588 </month> <year> (1984). </year>
Reference-contexts: This approach was introduced in [21] and developed further by [7] for the symmetric case. Saad <ref> [38, 39, 40] </ref> has developed explicit restarting for the nonsymmetric case. <p> Saad <ref> [39] </ref> proposed to restart the iteration with a vector that has been preconditioned so that it is more nearly in a k-dimensional invariant subspace of interest. This preconditioning takes the form of a polynomial applied to the starting vector that is constructed to damp unwanted components from the eigenvector expansion.
Reference: [40] <author> Y. Saad, </author> <title> Numerical Methods for Large Eigenvalue Problems, </title> <publisher> Halsted Press-John Wiley & Sons Inc., </publisher> <address> New York (1992). </address>
Reference-contexts: If (x; ) is any Ritz-pair for A with respect to K k then Ax x = fl ^p (A)v 1 for some scalar fl. This discussion follows the treatment given by Saad in <ref> [40] </ref> and in his earlier papers. While these facts may seem esoteric, they have important algorithmic consequences. <p> This approach was introduced in [21] and developed further by [7] for the symmetric case. Saad <ref> [38, 39, 40] </ref> has developed explicit restarting for the nonsymmetric case. <p> This has the effect of favoring the Ritz vectors that have least converged. Additional aspects of explicit restarting are developed thoroughly in Chapter VII of <ref> [40] </ref>. In any case, this restarting mechanism is actually polynomial restarting in disguise.
Reference: [41] <author> Y. Saad and M. Schultz, </author> <title> GMRES: A generalized minimum residual algorithm for solving nonsymmetric linear systems, </title> <journal> SIAM J. Scientific and Stat. Comp., </journal> <volume> 7, </volume> <month> 856-869 </month> <year> (1986). </year>
Reference-contexts: v k+1 ) H k k where fi k = kf k k and v k+1 = 1 f k : This factorization may be used to obtain approximate solutions to a linear system Ax = b if b = v 1 fi o and this underlies the GMRES method <ref> [41] </ref>. However, the purpose here is to investigate the use of this factorization to obtain approximate eigenvalues and eigenvec-tors. The discussion of the previous section implies that Ritz pairs satisfying the Galerkin condition are immediately available from the eigenpairs of the small projected matrix H.
Reference: [42] <author> J.A. Scott. </author> <title> An Arnoldi code for computing selected eigenvalues of sparse real unsym-metric matrices, </title> <type> Tech. </type> <institution> Rept. RAL-93-097, Rutherford Appleton Laboratory (1993). </institution>
Reference-contexts: A block method called ABLE based upon two-sided nonsymmetric Lanczos is being developed by Bai, Day and Ye [2]. Software based upon subspace iteration with Chbeychev acceleration has been developed by Duff and Scott [12]. Jennifer Scott has also developed software based upon an explicitly restarted Chebyshev-Arnoldi method <ref> [42] </ref>. Finally, the Rational Krylov method being developed by Ruhe [36, 37] is very promising for the nonsymmetric problem when a factorization of the matrix is possible. 9 Acknowledgements The computational results presented in Section 7 are due to Zdenko Tomasic and Dan Hu.
Reference: [43] <author> H. Simon, </author> <title> Analysis of the symmeteric Lanczos algorithm with reorthogonalization methods, Linear Algebra and Its Applications 61, </title> <month> 101-131 </month> <year> (1984). </year>
Reference-contexts: Most notably, the theses and subsequent papers and computer codes of Scott and of Simon have developed this idea <ref> [34, 33, 43] </ref>. (3) No re-orthogonalization, which has been developed by Cullum and her colleagues. This last option introduces the almost certain possibility of introducing spurious eigenvalues. Various techniques have been developed to detect and deal with the presence of spurious eigenvalues [6, 8].
Reference: [44] <author> D. C. Sorensen, </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13, </volume> <pages> pp. 357-385, </pages> <year> 1992. </year>
Reference-contexts: The well known Lanczos method is the premier member of this class. The Arnoldi method generalizes the Lanczos method to the non-symmetric case. A recently developed variant of the Arnoldi/Lanczos scheme called the Implicitly Restarted Arnoldi Method <ref> [44] </ref> is presented here in some depth. This method is highlighted because of its suitability as a basis for software development. The discussion begins with a brief synopsis of the theory and the basic iterations suitable for large scale problems to motivate the introduction of Krylov subspaces. <p> Full details may be found in <ref> [44] </ref>. The basic iteration is given here in Algorithm 6 and the diagrams in Figures 1-3 describe how this iteration proceeds schematically. In Algorithm 6 and in the discussion below, the notation M (1:n;1:k) denotes the leading n fi k submatrix of M .
Reference: [45] <author> D.C. Sorensen, P.A. Vu, Z. Tomasic, </author> <title> Algorithms and Software for Large Scale Eigen-problems on High Performance Computers, High Performance Computing 1993 - Grand Challenges in Computer Simulation,Adrian Tentner ed. </title> , <booktitle> Proceedings 1993 Simulation Multiconference, Society for Computer Simulation, </booktitle> <month> 149-154 </month> <year> (1993). </year>
Reference-contexts: This model has over 250,000 degrees of freedom. The smallest eigenvalues are of interest and the ARPACK code appears to be very competitive with the best commercially available codes on problems of this size. For details see <ref> [45] </ref>. The Singular Value Decomposition (SVD) may also be computed using ARPACK and the SVD has a many large scale applications. Two SVD applications occur in computational biology. The first of these is the 3-D image reconstruction of biological macromolecules from 2-D projections obtained through electron micrographs. <p> For details about the method and experimental results, see [20], <ref> [45] </ref>. Nonsymmetric problems also arise in quantum chemistry. Researchers at University of Washington have used the code to investigate the effects of the electric field on InAs/GaSb and GaAs/Al x Ga 1x as quantum wells.
Reference: [46] <author> G.W. Stewart, </author> <title> Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: This method, called Rayleigh Quotient Iteration, has very impressive convergence rates indeed. Rayleigh Quotient Iteration converges at a quadratic rate in general and at a cubic rate on Hermitian problems. For a more detailed discussion of the eigenvalue problem and basic algorithms see <ref> [52, 46, 18] </ref>. 3 Krylov Subspaces and Projection Methods Although the rate of convergence can be improved to an acceptable level through spectral transformations, power iterations are only able to find one eigenvector at a time. <p> One alternative is to introduce a block form of the simple power method which is often called subspace iteration. This important class of algorithms has been developed and investigated in <ref> [46] </ref>. Several software efforts have been based upon this approach [3, 47, 12]. However, there is another class of algorithms called Krylov subspace projection methods that are based upon the intricate structure of the sequence of vectors naturally produced by the power method.
Reference: [47] <author> W.J. Stewart and A. Jennings, </author> <title> ALGORITHM 570: LOPSI a simultaneous iteratin method for real matrices [F2], </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7, </volume> <month> 184-198 </month> <year> (1981). </year>
Reference-contexts: One alternative is to introduce a block form of the simple power method which is often called subspace iteration. This important class of algorithms has been developed and investigated in [46]. Several software efforts have been based upon this approach <ref> [3, 47, 12] </ref>. However, there is another class of algorithms called Krylov subspace projection methods that are based upon the intricate structure of the sequence of vectors naturally produced by the power method.
Reference: [48] <author> M. Van Heel, J. Frank, </author> <title> Use of Multivariate Statistics in Analysing the Images of Biological Macromolecules, </title> <address> Ultramicroscopy, </address> <month> 6 187-194 </month> <year> (1981). </year>
Reference-contexts: See [35] for further details of the molecular dynamics application. The underlying algorithm for reconstructing 3-D image reconstruction of biological macromolecules from 2-D projections <ref> [48] </ref> is based upon the statistical technique of principal component analysis [49]. In this algorithm, a singular value decomposition (SVD) of the data set is performed to extract the largest singular vectors which are then used in a classification procedure.
Reference: [49] <author> S. Van Huffel and J. Vandewalle, </author> <title> The Total Least Squares Provblem: </title> <booktitle> Computational Aspects and Analysis , Frontiers in Applied Mathematics 9, </booktitle> <publisher> SIAM Press, </publisher> <address> Philadelphia (1991). </address>
Reference-contexts: See [35] for further details of the molecular dynamics application. The underlying algorithm for reconstructing 3-D image reconstruction of biological macromolecules from 2-D projections [48] is based upon the statistical technique of principal component analysis <ref> [49] </ref>. In this algorithm, a singular value decomposition (SVD) of the data set is performed to extract the largest singular vectors which are then used in a classification procedure.
Reference: [50] <author> H.F. Walker, </author> <title> Implementation of the GMRES method using Householder transformations, </title> <journal> SIAM J. Scientific and Stat. Comp. </journal> <month> 9,152-163 </month> <year> (1988). </year>
Reference-contexts: The identification of this phenomenon in the symmetric case and the first rigorous numerical treatment is due to Paige [31]. There have been several approaches to overcome this problem in the symmetric case. They include: (1) complete re-orthogonalization, which may be accomplished through maintaining V in product Householder form <ref> [50, 17] </ref> or through the Modified Gram-Schmidt processes with re-orthogonalization [9]. (2) Selective re-orthogonalization, which has been proposed by Parlett and has been heavily researched by him and his students.
Reference: [51] <author> D.S. Watkins and L. Elsner, </author> <title> Convergence of algorithms of decomposition type for the eigenvalue problem, Linear Algebra and Its Applications, </title> <type> 143, </type> <month> 19-47 </month> <year> (1991). </year>
Reference-contexts: They may be found in [18]. The convergence behavior of this iteration is fascinating. The columns of V converge to Schur vectors at various rates. These rates are fundamentally linked to the simple power method and its rapidly convergent variant, inverse iteration (see <ref> [51] </ref>). Despite the extremely fast rate of convergence and the efficient use of storage, the implicitly shifted QR method is not suitable for large scale problems and it has proved to be extremely difficult to parallelize.
Reference: [52] <author> J.H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Claredon Press, Oxford, </publisher> <address> England (1965). </address>
Reference-contexts: This method, called Rayleigh Quotient Iteration, has very impressive convergence rates indeed. Rayleigh Quotient Iteration converges at a quadratic rate in general and at a cubic rate on Hermitian problems. For a more detailed discussion of the eigenvalue problem and basic algorithms see <ref> [52, 46, 18] </ref>. 3 Krylov Subspaces and Projection Methods Although the rate of convergence can be improved to an acceptable level through spectral transformations, power iterations are only able to find one eigenvector at a time.
References-found: 52

