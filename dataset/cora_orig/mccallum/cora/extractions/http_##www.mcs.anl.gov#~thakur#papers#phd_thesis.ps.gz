URL: http://www.mcs.anl.gov/~thakur/papers/phd_thesis.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Title: RUNTIME SUPPORT FOR IN-CORE AND OUT-OF-CORE DATA-PARALLEL PROGRAMS  
Author: by RAJEEV THAKUR 
Degree: B.E., University of Bombay, India, 1990 M.S., Syracuse University, 1992 DISSERTATION Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Engineering in the Graduate  Approved Professor Alok Choudhary  
Note: Date  
Date: May 1995  
Affiliation: School of Syracuse University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> Compiler and Runtime Support for Structured and Block Structured Applications. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 578-587, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das [34] and Hanxleden [125]. Agrawal et al. <ref> [1] </ref> describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution.
Reference: [2] <author> I. Ahmad, R. Bordawekar, Z. Bozkus, A. Choudhary, G. Fox, K. Parasuram, R. Ponnusamy, S. Ranka, and R. Thakur. </author> <title> Implementation and Scalability of Fortran 90D Intrinsic Functions on Distributed Memory Machines. </title> <type> Technical Report SCCS-256, </type> <institution> NPAC, Syracuse University, </institution> <year> 1992. </year>
Reference-contexts: It translates HPF programs to Fortran 77 programs with calls to a runtime library <ref> [2, 14, 119] </ref>. The compiler only exploits the parallelism expressed in data parallel constructs such as FORALL, WHERE and array assignment statements. It does not attempt to parallelize other constructs such as DO loops and WHILE loops, since they are used only as naturally sequential control constructs in HPF. <p> But, if two or more processors communicate with each other, they are implicitly synchronized during the communication. Communication Library The HPF compiler described above relies on a very powerful runtime support system which includes a library of collective communication routines [13], a library of intrinsic functions <ref> [2, 14] </ref> and other runtime routines such as for array redistribution [119]. The HPF compiler produces calls to collective communication routines instead of generating individual send and receive calls inside the compiled code. This is done CHAPTER 2. ISSUES IN RUNTIME SUPPORT 19 for the following reasons: 1.
Reference: [3] <author> M. Barnett, R. Littlefield, D. Payne, and R. van de Geijn. </author> <title> Global Combine on Mesh Architectures with Wormhole Routing. </title> <booktitle> In Proceedings of the 7 th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Thus, if L f &lt;< L, the path length D will not significantly affect the network latency unless it is very large. Further details of wormhole routing can be found in [87]. 4.1.3 Performance Models Barnett et. al. <ref> [3] </ref> have proposed algorithms and performance models for global combine operations on a wormhole routed mesh. We use similar models for our all-to-all communication algorithms, which take into account link conflicts and other characteristics of the underlying communication system. <p> Hence, we use fi ex or fi sr depending on the algorithm. We assume that the time taken is independent of distance, a property of both CM-5 and Delta <ref> [95, 3] </ref>. Thus, the time CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 63 required for an exchange step i is given by T = ff + L max (fi ex ; f (i)fi sat ) We assume that conflicting messages share the bandwidth of a network link. <p> We have adopted this approach of allowing a small amount of link contention to exist, thereby reducing the number of steps and keeping all processors active at every step. This approach also takes advantage of the fact that the communication links in the Delta have excess bandwidth <ref> [3] </ref>, so that a small number of contending messages will not affect the communication time. We consider six algorithms on the Delta, for power-of-two and non-power-of-two meshes. For the analysis of the algorithms, we assume that the mesh has r rows and c columns. <p> RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 88 0 0.4 0.8 1.2 1.6 Time (s) Number of Processors PEX 3 3 GEN + + + REX 2 2 2 IPEX fi fi fi fi this purpose, we use typical values for the communication costs on the Delta <ref> [80, 3] </ref>, namely for unforced messages ff = 75s, fi ex = 0:35s and for forced messages ff = 150s, fi ex = 0:2s. <p> We assume that fi sr fi ex and 2fi sat fi ex , as done in <ref> [3] </ref>, ie. two messages can travel on a link in the same direction without conflict. Figures 4.20 and 4.21 show that the observed and predicted times agree very closely.
Reference: [4] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. Jovian: </author> <title> A Framework for Optimizing Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library <ref> [4] </ref>, are being developed. Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12].
Reference: [5] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, and S. Yang. </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language. </title> <booktitle> In Proceedings of the First Annual Object-Oriented Numerics Conference, </booktitle> <pages> pages 1-24, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: INTRODUCTION 9 irregular problems. A compiler for Split-C is being developed at the University of Cal-ifornia, Berkeley, with a runtime support system which uses Active Messages [124]. pC++, a data-parallel extension to C++, has been developed at Indiana University <ref> [5] </ref>. A Fortran D compiler is being developed at Rice University [122]. Some research has been done in developing compilers which can automatically determine a good distribution and alignment of arrays instead of having the user specify them. <p> Latency or startup cost is reduced by packing small messages intended for the same destination into one large message. CHAPTER 2. ISSUES IN RUNTIME SUPPORT 15 2.3 Runtime Support for Compilers Data-parallel languages like HPF [57] and pC++ <ref> [5] </ref> have recently been developed to provide support for high performance programming on parallel machines. These languages provide a framework for writing portable parallel programs independent of the underlying architecture and other idiosyncrasies of the machine.
Reference: [6] <author> S. Bokhari. </author> <title> Complete Exchange on the iPSC/860. </title> <type> Technical Report 91-4, </type> <institution> ICASE, NASA Langley Research Center, </institution> <year> 1991. </year> <note> 143 BIBLIOGRAPHY 144 </note>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari <ref> [6] </ref>, and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. <p> 6 4 $ 7 2 $ 6 2 $ 7 2 $ 4 2 $ 5 4.2.2 Pairwise Exchange (PEX) We consider the Pairwise Exchange (PEX) algorithm which has been shown to be the best algorithm for a hypercube network, on which it guarantees no link contention at any step <ref> [6, 103] </ref>. The algorithm is described in Figure 4.3. It requires P 1 steps and the communication schedule is as follows. At step i, 1 i P 1, each processor exchanges a message with the processor determined by taking the exclusive-or of its processor number with i.
Reference: [7] <author> S. Bokhari and H. Berryman. </author> <title> Complete Exchange on a Circuit Switched Mesh. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 300-306, </pages> <year> 1992. </year>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in <ref> [103, 7, 113, 51, 54] </ref>. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108]. <p> So, it is necessary to develop algorithms which work even on non power-of-two meshes. Bokhari and Berryman describe two algorithms for a circuit-switched mesh, which assume that the number of processors is a power-of-two <ref> [7] </ref>. Scott has shown that a 3 =4 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 72 is the lower bound on the number of phases required to perform complete exchange on an a fi a mesh such that there is no link contention in any phase [103].
Reference: [8] <author> R. Bordawekar and A. Choudhary. </author> <title> Language and Compiler Support for Parallel I/O. </title> <booktitle> In Proceedings of IFIP Working Conference on Programming Environments for Massively Parallel Distributed Systems, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar <ref> [11, 8] </ref> is developing a compiler for out-of-core HPF programs which uses the runtime library [116, 25, 115, 118] discussed in Chapters 5 and 6 of this thesis. <p> Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. [18] are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in <ref> [8, 17, 109, 18] </ref>. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability.
Reference: [9] <author> R. Bordawekar, A. Choudhary, and J. del Rosario. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the 7 th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: A poor I/O capability can severely degrade the performance of the entire program. Almost all present generation parallel computers provide some kind of hardware and system software support for parallel I/O <ref> [29, 92, 9, 36] </ref>. But, the I/O performance observed at the application level is usually much lower than what the hardware can 90 CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 91 support. There are several reasons for this. <p> The CFS provides the user with a Unix-like interface and the parallel reads and writes are handled transparently. The performance of the CFS on the Touchstone Delta has been studied in detail in <ref> [9] </ref>. The CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 94 CFS supports four modes of file access. In Mode 0, each processor has an independent file pointer. In Modes 1 - 3, processors have a common file pointer.
Reference: [10] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A Model and Compilation Strategy for Out-of-Core Data Parallel Programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in <ref> [10] </ref>. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library [116, 25, 115, 118] discussed in Chapters 5 and 6 of this thesis.
Reference: [11] <author> R. Bordawekar, A. Choudhary, and R. Thakur. </author> <title> Data Access Reorganizations in Compiling Out-of-core Data Parallel Programs on Distributed Memory Machines. </title> <type> Technical Report SCCS-622, </type> <institution> NPAC, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar <ref> [11, 8] </ref> is developing a compiler for out-of-core HPF programs which uses the runtime library [116, 25, 115, 118] discussed in Chapters 5 and 6 of this thesis.
Reference: [12] <author> R. Bordawekar, J. del Rosario, and A. Choudhary. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library <ref> [37, 12] </ref>. Disk-directed I/O is a technique for performing collective I/O at the file system level [69, 70, 71]. 1.7 Organization of this Thesis The rest of this thesis is organized as follows. <p> The PASSION Runtime Library provides such an interface [26]. Given an appropriate collective I/O interface, the collective I/O operation can be implemented either as a library on top of the file system, or at the file system level itself. A technique called Two-Phase I/O <ref> [37, 12] </ref> has been proposed for doing collective I/O at the library level. In this method, I/O is done in two phases. In the first phase, processors cooperate to read data in large chunks, and in the second phase they do an in-core redistribution of the data. <p> In disk-directed I/O, a collective I/O request is sent to all I/O nodes which determine the order and timing of the flow of data. CHAPTER 6. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (II) 117 6.3 Extended Two-Phase Method for Collective I/O The Two-Phase Method <ref> [37, 12] </ref> is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely to write a distributed in-core array to a file. I/O is done in two phases. <p> In the second phase, data is redistributed among processors to whatever is the desired distribution. Since I/O cost is orders of magnitude more than communication cost, the cost incurred by the second phase is negligible. This Two-Phase approach is found to give consistently good performance for all distributions <ref> [37, 12] </ref>. We have extended the basic Two-Phase Method to access arbitrary sections of out-of-core arrays.
Reference: [13] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF Compiler for Distributed Memory MIMD Computers: Design, Implementation, and Performance Results. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Another such language is Fortran 90D [128]. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. The Fortran 90D compiler developed at Syracuse University <ref> [13] </ref> translates Fortran 90D programs to Fortran 77 plus message passing node programs. Vienna Fortran [21, 130] and CM Fortran [121] also allow the user to write programs in global address space. <p> This runtime support can also be used together with a compiler to translate programs written in a high-level data-parallel language like HPF to node programs for distributed memory parallel computers. In fact it forms an essential part of the Fortran 90D/HPF compiler developed at Syracuse University <ref> [13] </ref>. Runtime support helps to separate the machine dependent aspects of compilation from the machine independent aspects. The compiler can do all the machine independent transformations and the runtime system can be optimized for each different machine. <p> Irregular distributions are not allowed in version 1.0 of HPF. CHAPTER 2. ISSUES IN RUNTIME SUPPORT 17 2.3.2 Compiler and Runtime Support for HPF An HPF compiler which translates in-core HPF programs to message passing node programs for distributed memory parallel computers has been developed at Syra-cuse University <ref> [13, 15] </ref>. It translates HPF programs to Fortran 77 programs with calls to a runtime library [2, 14, 119]. The compiler only exploits the parallelism expressed in data parallel constructs such as FORALL, WHERE and array assignment statements. <p> But, if two or more processors communicate with each other, they are implicitly synchronized during the communication. Communication Library The HPF compiler described above relies on a very powerful runtime support system which includes a library of collective communication routines <ref> [13] </ref>, a library of intrinsic functions [2, 14] and other runtime routines such as for array redistribution [119]. The HPF compiler produces calls to collective communication routines instead of generating individual send and receive calls inside the compiled code. This is done CHAPTER 2. <p> This involves a number of tests on the relationship among subscripts of various arrays in a FORALL statement. These tests also include information about array alignments and distributions. The compiler uses pattern matching techniques to detect communication patterns <ref> [13, 15] </ref>. Intrinsic Library Intrinsic functions form an important feature of Fortran 90 and HPF. They directly support many of the basic data-parallel operations on arrays and provide a means for expressing operations on arrays concisely.
Reference: [14] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, R. Thakur, and J. Wang. </author> <title> Scalable Libraries for High Performance Fortran. </title> <booktitle> In Proceedings of the Scalable BIBLIOGRAPHY 145 Parallel Libraries Conference, </booktitle> <pages> pages 67-75. </pages> <institution> Mississippi State University, </institution> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: It translates HPF programs to Fortran 77 programs with calls to a runtime library <ref> [2, 14, 119] </ref>. The compiler only exploits the parallelism expressed in data parallel constructs such as FORALL, WHERE and array assignment statements. It does not attempt to parallelize other constructs such as DO loops and WHILE loops, since they are used only as naturally sequential control constructs in HPF. <p> But, if two or more processors communicate with each other, they are implicitly synchronized during the communication. Communication Library The HPF compiler described above relies on a very powerful runtime support system which includes a library of collective communication routines [13], a library of intrinsic functions <ref> [2, 14] </ref> and other runtime routines such as for array redistribution [119]. The HPF compiler produces calls to collective communication routines instead of generating individual send and receive calls inside the compiled code. This is done CHAPTER 2. ISSUES IN RUNTIME SUPPORT 19 for the following reasons: 1. <p> UNPACK * Vector and Matrix Multiplication Functions: DOT PRODUCT, MATMUL. * Bit Manipulation functions: IAND, IOR, POPCNT, POPPAR, LEADZ * Elemental Intrinsics functions: SIN, COS, TAN It is necessary to have a library of these intrinsic functions which can be called from the node programs of a distributed memory machine <ref> [14] </ref>. The HPF compiler detects calls to intrinsic functions in the HPF program and replaces them with calls to these routines.
Reference: [15] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M. Wu. </author> <title> Compiling Fortran 90D/HPF for Distributed Memory MIMD Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Irregular distributions are not allowed in version 1.0 of HPF. CHAPTER 2. ISSUES IN RUNTIME SUPPORT 17 2.3.2 Compiler and Runtime Support for HPF An HPF compiler which translates in-core HPF programs to message passing node programs for distributed memory parallel computers has been developed at Syra-cuse University <ref> [13, 15] </ref>. It translates HPF programs to Fortran 77 programs with calls to a runtime library [2, 14, 119]. The compiler only exploits the parallelism expressed in data parallel constructs such as FORALL, WHERE and array assignment statements. <p> This involves a number of tests on the relationship among subscripts of various arrays in a FORALL statement. These tests also include information about array alignments and distributions. The compiler uses pattern matching techniques to detect communication patterns <ref> [13, 15] </ref>. Intrinsic Library Intrinsic functions form an important feature of Fortran 90 and HPF. They directly support many of the basic data-parallel operations on arrays and provide a means for expressing operations on arrays concisely.
Reference: [16] <author> Z. Bozkus, S. Ranka, and G. Fox. </author> <title> Modeling the CM-5 Multicomputer. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Computation 92, </booktitle> <pages> pages 100-107, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This allows the switches to perform load balancing on the fly. Once the message has reached the necessary height in the tree, it must follow a particular path down. A detailed discussion of the interprocessor communication overhead on the CM-5 is given in <ref> [96, 16, 94] </ref>. CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 61 4.1.2 Touchstone Delta The Intel Touchstone Delta is a 16 fi 32 mesh of computational nodes, each of which is an Intel i860/XR microprocessor. The two-dimensional mesh interconnection network has bidirectional links and uses wormhole routing.
Reference: [17] <author> P. Brezany, M. Gerndt, P. Mehrotra, and H. Zima. </author> <title> Concurrent File Operations in a High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-238, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. [18] are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in <ref> [8, 17, 109, 18] </ref>. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability.
Reference: [18] <author> P. Brezany, T. Mueck, and E. Schikuta. </author> <title> Language, Compiler and Parallel Database Support for I/O Intensive Applications. </title> <booktitle> In Proceedings of High Performance Computing and Networking 1995, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. <ref> [18] </ref> are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in [8, 17, 109, 18]. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. <p> Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. [18] are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in <ref> [8, 17, 109, 18] </ref>. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability.
Reference: [19] <author> R. Butler and E. Lusk. </author> <title> User's Guide to the P4 Programming System. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: There have been several attempts to provide some measure of portability in parallel programs. There are number of portable communication libraries like EXPRESS [90], PVM [114], PICL [47], P4 <ref> [19] </ref> etc. which provide a communication layer above the native message passing library of the system. These libraries provide a well-defined set of communication routines which remain the same for any system.
Reference: [20] <author> A. Carle, K. Kennedy, U. Kremer, and J. Mellor-Crummey. </author> <title> Automatic Data Layout for Distributed Memory Machines in the D Programming Environment. </title> <type> Technical Report CRPC-TR93298, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: At the end of the subroutine, the array (or array section) needs to be redistributed to the original distribution. * In many applications such as 2D FFT and the ADI method for solving multidimensional PDEs, dynamic redistribution can result in significant performance improvement <ref> [20] </ref>. An example of an HPF program using redistribution is shown in Figure 3.2. This is a two-dimensional FFT program in which the array is first block distributed along CHAPTER 3.
Reference: [21] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <type> Technical Report 92-9, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Another such language is Fortran 90D [128]. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. The Fortran 90D compiler developed at Syracuse University [13] translates Fortran 90D programs to Fortran 77 plus message passing node programs. Vienna Fortran <ref> [21, 130] </ref> and CM Fortran [121] also allow the user to write programs in global address space. Recently, the High Performance Fortran Forum, comprising a group of researchers from industry, academia and research laboratories, developed a standard language called High Performance Fortran (HPF) [57, 67].
Reference: [22] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Automatic Array Alignment in Data-Parallel Programs. </title> <booktitle> In Proceedings of Principles of Programming Languages (POPL) '93, </booktitle> <pages> pages 16-28, </pages> <month> January </month> <year> 1993. </year> <note> BIBLIOGRAPHY 146 </note>
Reference-contexts: The PARADIGM project at the University of Illi-nois aims at developing a fully automated compiler to translate sequential programs to parallel programs for distributed memory computers. The problem of automatic alignment of arrays has been studied by Chatterjee et al. <ref> [22] </ref> and Li et al. [79]. There has been some research in runtime support for applications with irregular data access patterns. The PARTI/CHAOS toolkit is a collection of runtime library routines to handle irregular computations [35, 102].
Reference: [23] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating Local Addresses and Communication Sets for Data Parallel Programs. </title> <booktitle> In Proceedings of Principles and Practices of Parallel Programming (PPoPP) '93, </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: some research on the closely related problem of determining the local addresses and communication sets for array assignment statements like A (l 1 : h 1 : s 1 ) = B (l 2 : h 2 : s 2 ) where A and B have different cyclic (m) distributions <ref> [23, 110, 111, 52] </ref>. Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. <p> Several complex schemes have been proposed for performing this redistribution <ref> [110, 111, 23, 99] </ref>. Since redistribution has to be done at runtime, a simple and efficient algorithm with minimum runtime overhead is necessary. We propose a new method for performing the general cyclic (x) to cyclic (y) redistribution, which has low runtime overhead.
Reference: [24] <author> P. Chen, E. Lee, G. Gibson, R. Katz, and D. Patterson. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability. An excellent overview of RAID concepts is given in <ref> [24] </ref>. Disk striping where data is distributed across disks at a small granularity so that each block is distributed across all disks is proposed in [101]. File declustering, where different blocks of a file are stored on distinct disks is suggested in [81].
Reference: [25] <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur. </author> <title> PASSION: Parallel and Scalable Software for Input-Output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, Syracuse University, </institution> <month> September </month> <year> 1994. </year> <note> Also available as CRPC Technical Report CRPC-TR94483-S. </note>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library <ref> [116, 25, 115, 118] </ref> discussed in Chapters 5 and 6 of this thesis. Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. <p> This makes it difficult for the programmer to perform optimizations at the application level, for example prefetching to overlap I/O with computation, because of the complexities involved in managing buffers and file pointers. We have developed a runtime system called PASSION (Parallel and Scalable Software for Input-Output) <ref> [116, 25] </ref> which aims to alleviate many of these problems and provide better software support for out-of-core programs. We believe that high-level interfaces that facilitate the use of semantic knowledge about the accesses from parallel programs are necessary for simple and portable application programming.
Reference: [26] <author> A. Choudhary, R. Bordawekar, S. More, K. Sivaram, and R. Thakur. </author> <title> A User's Guide for the PASSION Runtime Library Version 1.0. </title> <type> Technical Report SCCS-702, </type> <institution> NPAC, Syracuse University, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Hence, a group of processors may need to perform I/O at the same time. An appropriate interface needs to be provided for the user to specify a collective I/O request. The PASSION Runtime Library provides such an interface <ref> [26] </ref>. Given an appropriate collective I/O interface, the collective I/O operation can be implemented either as a library on top of the file system, or at the file system level itself. A technique called Two-Phase I/O [37, 12] has been proposed for doing collective I/O at the library level.
Reference: [27] <author> P. Corbett and D. Feitelson. </author> <title> Overview of the Vesta Parallel File System. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files <ref> [29, 27] </ref>. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed.
Reference: [28] <author> P. Corbett, D. Feitelson, Y. Hsu, J. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: A Parallel I/O Interface for MPI, Version 0.3. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Since the I/O latency is very high, this results is poor performance. Second, the interface to any parallel file system does not currently allow a programmer to specify strided accesses using a single read or write call; though there are some recent proposals to rectify this <ref> [28, 88] </ref>. Third, the interface does not provide support for processors to make a collective I/O request. So file systems cannot perform any optimizations based on the knowledge of the access requests of all processors. <p> The Extended Two-Phase Method is not specific to any particular machine, file system or architecture. It can be easily implemented on top of any of the existing file system interfaces or portable interfaces such as the proposed MPI-IO interface <ref> [28] </ref>, resulting in portable implementations. It can also be easily modified and tuned for any particular system. This only requires defining the file domains appropriately, and possibly using a different algorithm for interprocessor communication.
Reference: [29] <author> P. Corbett, D. Feitelson, J. Prost, and S. Baylor. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files <ref> [29, 27] </ref>. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed. <p> A poor I/O capability can severely degrade the performance of the entire program. Almost all present generation parallel computers provide some kind of hardware and system software support for parallel I/O <ref> [29, 92, 9, 36] </ref>. But, the I/O performance observed at the application level is usually much lower than what the hardware can 90 CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 91 support. There are several reasons for this. <p> We call this the Direct Method. The Vesta file system <ref> [29] </ref> and the nCUBE file system [36] do provide support for the user to specify a logical view of the data to be read, and use a single call to read data. <p> It would be an interesting project to port it to the IBM SP-2 using the PIOFS file system. A unique feature of PIOFS (and its predecessor Vesta) is that it supports logical partitioning of files <ref> [29] </ref>. It is not entirely clear how widely this logical partitioning can be used. Implementing a general runtime system like PASSION using PIOFS can provide some insight into the usefulness of logical partitioning.
Reference: [30] <author> T. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1993. </year> <note> BIBLIOGRAPHY 147 </note>
Reference-contexts: The algorithm for this currently operates on a 3 Tbyte database with single runs producing 100Mbytes to several Gbytes of output. These figures are expected to increase by orders of magnitude in the near future. Cormen <ref> [30] </ref> has also compiled a list of several applications which deal with huge data sets.
Reference: [31] <author> T. Cormen and A. Colvin. </author> <title> ViC*: A Preprocessor for Virtual-Memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library [116, 25, 115, 118] discussed in Chapters 5 and 6 of this thesis. Cormen and Colvin <ref> [31] </ref> are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs.
Reference: [32] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The SUPERB tool transforms sequential Fortran programs with data distribution annotations into parallel programs. Compilers for functional languages like Id Nouveau and Crystal have been developed for distributed memory machines. The Crystal compiler generates communication statements by studying the access patterns of the arrays in a statement. Split-C <ref> [32] </ref> is a parallel extension of C intended for high performance programming on distributed memory multiprocessors. It provides a global address space and allows a mixture of shared memory, message passing and data-parallel programming styles for both regular and CHAPTER 1. INTRODUCTION 9 irregular problems.
Reference: [33] <author> W. Dally and C. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: In other words, at most one turn is allowed, and that turn must be from the X dimension to the Y dimension. For a 2D mesh, the XY routing algorithm is guaranteed to be deadlock free <ref> [33] </ref>. In wormhole routing, a packet is divided into a number of flits (flow control digits) for transmission. The size of a flit is typically the same as the channel width. The header flit of a packet determines the route.
Reference: [34] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed Memory Compiler Methods for Irregular Problems Data Copy Reuse and Runtime Partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Run-time Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-220. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das <ref> [34] </ref> and Hanxleden [125]. Agrawal et al. [1] describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution. <p> It depends on the values of the indirection arrays, which may not be known until runtime. Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers <ref> [93, 102, 34, 35, 68] </ref>. One way of detecting and performing the communication is by using an inspector-executor [68, 102] approach. A parallel loop is transformed into two constructs called an inspector and an executor.
Reference: [35] <author> R. Das, J. Saltz, and H. Berryman. </author> <title> A Manual for PARTI Runtime Primitives. </title> <type> Interim Report 17, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The problem of automatic alignment of arrays has been studied by Chatterjee et al. [22] and Li et al. [79]. There has been some research in runtime support for applications with irregular data access patterns. The PARTI/CHAOS toolkit is a collection of runtime library routines to handle irregular computations <ref> [35, 102] </ref>. These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das [34] and Hanxleden [125]. <p> It depends on the values of the indirection arrays, which may not be known until runtime. Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers <ref> [93, 102, 34, 35, 68] </ref>. One way of detecting and performing the communication is by using an inspector-executor [68, 102] approach. A parallel loop is transformed into two constructs called an inspector and an executor. <p> The executor loop then uses the information from the inspector to perform the actual communication and computation. PARTI <ref> [35] </ref> is a library of runtime routines for solving irregular problems on distributed memory computers. PARTI primitives can be directly used to generate the inspector/executor pairs. Each inspector produces a communication schedule, which is essentially a pattern of communication for gathering or scattering data.
Reference: [36] <author> E. DeBenedictis and J. del Rosario. </author> <title> nCUBE Parallel I/O Software. </title> <booktitle> In Proceed ings of 11 th International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 117-124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: A poor I/O capability can severely degrade the performance of the entire program. Almost all present generation parallel computers provide some kind of hardware and system software support for parallel I/O <ref> [29, 92, 9, 36] </ref>. But, the I/O performance observed at the application level is usually much lower than what the hardware can 90 CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 91 support. There are several reasons for this. <p> We call this the Direct Method. The Vesta file system [29] and the nCUBE file system <ref> [36] </ref> do provide support for the user to specify a logical view of the data to be read, and use a single call to read data. But each processor's request is serviced independently and there is no collective optimization based on the requests of all processors.
Reference: [37] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Runtime Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library <ref> [37, 12] </ref>. Disk-directed I/O is a technique for performing collective I/O at the file system level [69, 70, 71]. 1.7 Organization of this Thesis The rest of this thesis is organized as follows. <p> The PASSION Runtime Library provides such an interface [26]. Given an appropriate collective I/O interface, the collective I/O operation can be implemented either as a library on top of the file system, or at the file system level itself. A technique called Two-Phase I/O <ref> [37, 12] </ref> has been proposed for doing collective I/O at the library level. In this method, I/O is done in two phases. In the first phase, processors cooperate to read data in large chunks, and in the second phase they do an in-core redistribution of the data. <p> In disk-directed I/O, a collective I/O request is sent to all I/O nodes which determine the order and timing of the flow of data. CHAPTER 6. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (II) 117 6.3 Extended Two-Phase Method for Collective I/O The Two-Phase Method <ref> [37, 12] </ref> is a collective I/O technique for reading an entire in-core array from a file into a distributed array in main memory, and conversely to write a distributed in-core array to a file. I/O is done in two phases. <p> In the second phase, data is redistributed among processors to whatever is the desired distribution. Since I/O cost is orders of magnitude more than communication cost, the cost incurred by the second phase is negligible. This Two-Phase approach is found to give consistently good performance for all distributions <ref> [37, 12] </ref>. We have extended the basic Two-Phase Method to access arbitrary sections of out-of-core arrays.
Reference: [38] <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-68, </pages> <month> March </month> <year> 1994. </year> <note> BIBLIOGRAPHY 148 </note>
Reference-contexts: Large scale scientific computations, in addition to requiring a great deal of computational power, also deal with large quantities of data. At present, a typical Grand Challenge Application could require 1Gbyte to 4Tbytes of data per run <ref> [38] </ref>. These figures are expected to increase by orders of magnitude as teraflop machines make their appearance. Although supercomputers have very large main memories, the memory is not large enough to hold this much amount of data. <p> These applications exist in diverse areas such as large scale scientific computations, database and information processing, hypertext and multimedia systems, information retrieval and many others. del Rosario and Choudhary <ref> [38] </ref> have done a survey of the I/O requirements of several Grand Challenge applications. They find that the data requirements of these applications range from 1 Gbyte to 4 Tbytes per run. <p> Although supercomputers have very large main memories, the memory is not large enough to hold all the data required by these applications. For example, a typical Grand Challenge Application at present could require 1Gbyte to 4Tbytes of data per run <ref> [38] </ref>. These figures are expected to increase by orders of magnitude as teraflop machines make their appearance. Hence, data needs to be stored on disk and the performance of the program depends on how fast processors can access data from disks.
Reference: [39] <author> P. Dibble, M. Scott, and C. Ellis. </author> <title> Bridge: A High-Performance File System for Parallel Processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: File declustering, where different blocks of a file are stored on distinct disks is suggested in [81]. This is used in the Bridge File System <ref> [39] </ref>, in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout.
Reference: [40] <author> I. Foster, M. Henderson, and R. Stevens. </author> <title> Workshop Introduction. </title> <booktitle> In Proceedings of the Workshop on Data Systems for Parallel Climate Models at Argonne National Laboratory, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: For a 1000-year coupled atmosphere-ocean run with a 150 km 2 resolution, the atmospheric simulation takes about 30 weeks, while ocean simulation takes 27 weeks; the process produces 40 Mbytes of data per simulation minute, or a total of 20 Tbytes of data for the entire simulation <ref> [40] </ref>. CHAPTER 2. ISSUES IN RUNTIME SUPPORT 23 * Four-Dimensional Data Assimilation: This application from NASA incorporates actual space-time observations into mathematical and computational models in order to create a unified, complete description of the atmosphere.
Reference: [41] <author> G. Fox. </author> <title> Domain Decomposition in Distributed and Shared Memory Environments I: A Uniform Decomposition and Performance Analysis for the nCUBE and JPL Mark IIIfp Hypercubes, </title> <booktitle> volume 297 of Lecture Notes in Computer Science, </booktitle> <pages> pages 1042-1073. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year> <title> Supercomputing, </title> <editor> ed. E. Houstis, T. Papatheodorou, and C. </editor> <publisher> Polychronopoulos. </publisher>
Reference-contexts: The Applications Working Group of the Scalable I/O Initiative has provided a description of the I/O requirements of several applications in biology, chemistry, physics, earth sciences, engineering and graphics [97]. Fox <ref> [41] </ref> presents a performance analysis of applications on a hypercube machine with a hierarchical memory at each node, consisting of a fast cache and slower main memory. The applications considered are the N-body problem, Poisson's equation solver, matrix multiplication, LU Decomposition, Fast Fourier Transform and neural networks. <p> Let us assume that arrays A and B are very large and hence out-of-core. We note that this may not be the best algorithm for solving Laplace's equation with an out-of-core data set as discussed in <ref> [41] </ref>, but we only use it for the purpose of explanation. The arrays A and B are distributed as (block,block) on a 4 fi 4 grid of processors as shown in Figure 5.4.
Reference: [42] <author> G. Fox and W. Furmanski. </author> <title> Optimal Communication Algorithms for Regular Decompositions on the Hypercube. </title> <booktitle> In The Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> Volume 1, </volume> <pages> pages 648-713, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski <ref> [42] </ref>. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108]. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area.
Reference: [43] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D Language Specifications. </title> <type> Technical Report COMP TR90-141, CRPC, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: Researchers have found it very hard to build compilers which can parallelize sequential programs written in standard C or Fortran 77. Hence, standard sequential languages have been augmented with directives and constructs to aid the compiler in generating message passing code. Fortran D <ref> [43, 58] </ref> is one such language. Fortran D consists of a set of extensions to Fortran 77 which specify how data is to be distributed among the processors of a distributed memory machine. <p> Chapter 3 Runtime Support for Array Redistribution 3.1 Introduction In distributed memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic or block-cyclic as in Fortran D <ref> [43] </ref> and High Performance Fortran (HPF) [57, 67]; or irregular in which there is no function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program. <p> In a cyclic distribution, array elements are distributed among processors in a round-robin fashion. In a cyclic (m) distribution, blocks of size m are distributed cyclically. The cyclic (m) distribution with 1 &lt; m &lt; dN=P e is commonly referred to as a block-cyclic distribution with block size m <ref> [43] </ref>. Block and cyclic distributions are special cases of the general cyclic (m) distribution. A cyclic (m) distribution with m = dN=P e is a block distribution and a cyclic (m) distribution with m = 1 is a cyclic distribution.
Reference: [44] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, Vol. I. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1988. </year>
Reference-contexts: Array operations and FORALL statements in the original program are transformed into DO loops. The communication module detects communication requirements and inserts appropriate communication primitives. Finally, the code generator produces loosely synchronous <ref> [44, 46] </ref> SPMD code. The generated code is structured as alternating phases of local computation and communication. Local computations consist of operations by each processor on the data in its own memory.
Reference: [45] <author> G. Fox, S. Ranka, M. Scott, A. Malony, J. Browne, M. Chen, A. Choud-hary, T. Cheatham, J. Cuny, R. Eigenmann, A. Fahmy, I. Foster, D. Gan-non, T. Haupt, M. Karr, C. Kesselman, C. Koelbel, W. Li, M. Lam, T. LeBlanc, J. Openshaw, D. Padua, C. Polychronopoulos, J. Saltz, A. Sussman, G. Weigand, and K. Yelick. </author> <title> Common Runtime Support for High Performance Parallel Languages | Parallel Compiler Runtime Consortium. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 752-757, </pages> <month> November </month> <year> 1993. </year> <note> BIBLIOGRAPHY 149 </note>
Reference-contexts: A dynamic scheme is used for partitioning the I/O workload among processors, depending on the access requests. Performance results obtained using this runtime support for out-of-core CHAPTER 1. INTRODUCTION 8 programs on the Touchstone Delta are presented. A Parallel Compiler Runtime Consortium (PCRC) <ref> [45] </ref> has recently been formed with the goal of developing a common runtime support for high level parallel languages. We believe that the research presented in this thesis would be very useful to this consortium.
Reference: [46] <author> G. Fox, R. Williams, and P. Messina. </author> <title> Parallel Computing Works. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <year> 1994. </year>
Reference-contexts: Array operations and FORALL statements in the original program are transformed into DO loops. The communication module detects communication requirements and inserts appropriate communication primitives. Finally, the code generator produces loosely synchronous <ref> [44, 46] </ref> SPMD code. The generated code is structured as alternating phases of local computation and communication. Local computations consist of operations by each processor on the data in its own memory. <p> We observe that there is considerable improvement in performance by reusing the information. Chapter 4 Runtime Support for All-to-All Collective Communication Programs on distributed memory parallel computers typically require interprocessor communication. In loosely synchronous parallel programs <ref> [46] </ref>, all processors perform similar operations but on different data sets. Hence it is very likely that a group of processors or even all processors may need to perform communication at the same time. <p> This chapter discusses the basic design of PASSION and the various models, techniques and optimizations used in it. 5.2 PASSION Runtime Library The PASSION Runtime Library provides routines to efficiently perform the I/O required in programs involving out-of-core multidimensional arrays. It provides support for loosely synchronous <ref> [46] </ref> out-of-core computations which use a Single Program CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 92 Interface Interface Parallel File System PASSION RUNTIME SYSTEM MPI I/O Node + MPHPC++HPF/ Multiple Data (SPMD) Model. <p> If each processor directly tries to read the data it needs, it may result in a large number of low granularity requests and multiple requests for the same data. In loosely synchronous SPMD programs, all processors perform similar operations but on different data sets <ref> [46] </ref>. Hence, if one processor needs to read data from disks, it is very likely that a group of processors or maybe all processors need to read data from disks at about the same time. This makes it possible for the requesting 114 CHAPTER 6.
Reference: [47] <author> G. Geist, M. Heath, B. Peyton, and P. Worley. </author> <title> A User's Guide to PICL, A Portable Instrumented Communication Library. </title> <type> Technical Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: There have been several attempts to provide some measure of portability in parallel programs. There are number of portable communication libraries like EXPRESS [90], PVM [114], PICL <ref> [47] </ref>, P4 [19] etc. which provide a communication layer above the native message passing library of the system. These libraries provide a well-defined set of communication routines which remain the same for any system.
Reference: [48] <author> M. Grossman. </author> <title> Modeling Reality. </title> <journal> IEEE Spectrum, </journal> <pages> pages 56-60, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: To produce a three-dimensional rendering of the surface of Venus at 200 Mbytes of data per frame would require over 13 Gbytes/sec. of I/O throughput at 50 frames per sec ond <ref> [48] </ref>. * Climate Prediction: A climate prediction code using the General Circulation Model (GCM) has the following requirements on the Intel Touchstone Delta.
Reference: [49] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomput-ers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Gupta [50] has proposed a constraint based approach to automatically determine a good data distribution. This method has been incorporated in the PARADIGM compiler <ref> [49, 112] </ref>. The PARADIGM project at the University of Illi-nois aims at developing a fully automated compiler to translate sequential programs to parallel programs for distributed memory computers. The problem of automatic alignment of arrays has been studied by Chatterjee et al. [22] and Li et al. [79].
Reference: [50] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 179-193, </pages> <month> mar </month> <year> 1992. </year>
Reference-contexts: A Fortran D compiler is being developed at Rice University [122]. Some research has been done in developing compilers which can automatically determine a good distribution and alignment of arrays instead of having the user specify them. Gupta <ref> [50] </ref> has proposed a constraint based approach to automatically determine a good data distribution. This method has been incorporated in the PARADIGM compiler [49, 112].
Reference: [51] <author> S. Gupta, S. Hawkinson, and B. Baxter. </author> <title> A Binary Interleaved Algorithm for Complete Exchange on a Mesh Architecture. </title> <type> Technical report, </type> <institution> Intel Supercomputer Systems Division, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in <ref> [103, 7, 113, 51, 54] </ref>. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108].
Reference: [52] <author> S. Gupta, S. Kaushik, C. Huang, and P. Sadayappan. </author> <title> On Compiling Array Expressions for Efficient Execution on Distributed Memory Machines. </title> <type> Technical Report OSU-CISRC-4/94-TR19, </type> <institution> Computer and Information Science Research Center, The Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: some research on the closely related problem of determining the local addresses and communication sets for array assignment statements like A (l 1 : h 1 : s 1 ) = B (l 2 : h 2 : s 2 ) where A and B have different cyclic (m) distributions <ref> [23, 110, 111, 52] </ref>. Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54].
Reference: [53] <author> S. Gupta, S. Kaushik, S. Mufti, S. Sharma, C. Huang, and P. Sadayappan. </author> <title> On the Generation of Efficient Data Communication for Distributed Memory Machines. </title> <booktitle> In Proceedings of International Computing Symposium, Taiwan, </booktitle> <pages> pages 504-513, </pages> <year> 1992. </year>
Reference-contexts: Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das [34] and Hanxleden [125]. Agrawal et al. [1] describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution. Gupta et al. <ref> [53] </ref> and Koelbel [66] provide closed form expressions for determining the send and receive processor sets and data sets for redistributing arrays between block and cyclic distributions. An analytical model for evaluating the communication cost of data redistribution is presented in [63]. <p> We consider block (m) to cyclic, cyclic to block (m) and the general cyclic (x) to cyclic (y) type redistributions. For the general cyclic (x) to cyclic (y) redistribution, there is no direct algebraic formula to calculate the source and destination processor and index sets <ref> [53] </ref>. We use a novel approach for doing the general cyclic (x) to cyclic (y) redistribution, where we first 25 CHAPTER 3. RUNTIME SUPPORT FOR ARRAY REDISTRIBUTION 26 develop optimized algorithms for two special cases | when x is a multiple of y, or y is a multiple of x. <p> RUNTIME SUPPORT FOR ARRAY REDISTRIBUTION 38 3.5 Cyclic (x) to Cyclic (y) Redistribution For a general cyclic (x) to cyclic (y) redistribution, there is no direct algebraic formula to calculate the set of elements to send to a destination processor and the local addresses of these elements at the destination <ref> [53] </ref>. Several complex schemes have been proposed for performing this redistribution [110, 111, 23, 99]. Since redistribution has to be done at runtime, a simple and efficient algorithm with minimum runtime overhead is necessary.
Reference: [54] <author> S. Hambrusch, F. Hameed, and A. Khokhar. </author> <title> Communication Operations on Coarse-Grained Mesh Architectures. </title> <booktitle> Parallel Computing, </booktitle> <year> 1995. </year> <note> to appear. BIBLIOGRAPHY 150 </note>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in <ref> [103, 7, 113, 51, 54] </ref>. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108].
Reference: [55] <author> M. Harry, J. del Rosario, and A. Choudhary. </author> <title> VIP-FS: A Virtual, Parallel File System for High Performance Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed. Portable parallel file systems such as VIP-FS <ref> [55] </ref>, PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12].
Reference: [56] <institution> High Performance Computing and Communications: </institution> <note> Grand Challenges 1993 Report. A Report by the Committee on Physical, </note> <institution> Mathematical and Engineering Sciences, Federal Coordinating Council for Science, Engineering and Technology, </institution> <year> 1993. </year>
Reference-contexts: As a result, parallel computers are increasingly being used in many applications which require a great deal of computational power. Examples of such applications include many large scale computations in physics, chemistry, biology, engineering, medicine and other sciences, which have been identified as Grand Challenge Applications <ref> [56, 131] </ref>. Many applications dealing with information technology, such as multimedia systems, video on demand, video compression and decompression, also require a large amount of compute power.
Reference: [57] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Vienna Fortran [21, 130] and CM Fortran [121] also allow the user to write programs in global address space. Recently, the High Performance Fortran Forum, comprising a group of researchers from industry, academia and research laboratories, developed a standard language called High Performance Fortran (HPF) <ref> [57, 67] </ref>. HPF was designed to provide a CHAPTER 1. INTRODUCTION 5 portable extension to Fortran 90 for writing data-parallel applications. It includes features for mapping data to processors, specifying data-parallel operations, and methods for interfacing HPF programs to other programming paradigms. <p> Latency or startup cost is reduced by packing small messages intended for the same destination into one large message. CHAPTER 2. ISSUES IN RUNTIME SUPPORT 15 2.3 Runtime Support for Compilers Data-parallel languages like HPF <ref> [57] </ref> and pC++ [5] have recently been developed to provide support for high performance programming on parallel machines. These languages provide a framework for writing portable parallel programs independent of the underlying architecture and other idiosyncrasies of the machine. <p> Chapter 3 Runtime Support for Array Redistribution 3.1 Introduction In distributed memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic or block-cyclic as in Fortran D [43] and High Performance Fortran (HPF) <ref> [57, 67] </ref>; or irregular in which there is no function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program. <p> Arrays are first aligned to a template or index space. The DISTRIBUTE directive specifies how the template is to be distributed among the processors. In HPF, an array (or template) can be distributed as BLOCK (m) or CYCLIC (m) <ref> [57] </ref>.
Reference: [58] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An Overview of the Fortran D Programming System. </title> <type> Technical Report COMP TR91-154, CRPC, </type> <institution> Rice University, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Researchers have found it very hard to build compilers which can parallelize sequential programs written in standard C or Fortran 77. Hence, standard sequential languages have been augmented with directives and constructs to aid the compiler in generating message passing code. Fortran D <ref> [43, 58] </ref> is one such language. Fortran D consists of a set of extensions to Fortran 77 which specify how data is to be distributed among the processors of a distributed memory machine.
Reference: [59] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler Support for Machine-Independent Parallel Programming in Fortran D. In Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: Fortran D [43, 58] is one such language. Fortran D consists of a set of extensions to Fortran 77 which specify how data is to be distributed among the processors of a distributed memory machine. The Fortran D compiler developed at Rice University <ref> [122, 59] </ref> can translate a Fortran D program into a Fortran 77 plus message passing node program. Another such language is Fortran 90D [128]. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D.
Reference: [60] <author> J. Huber, C. Elford, D. Reed, A. Chien, and D. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <type> Technical Report UIUCDCS-R-95-1903, </type> <institution> University of Illinois at Urbana Champaign, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed. Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS <ref> [60] </ref> have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12].
Reference: [61] <author> S. L. Johnsson and C. T. </author> <title> Ho. </title> <journal> Optimum Broadcasting and Personalized Communication in Hypercubes . IEEE Transactions on Computers, </journal> <pages> pages 1249-1268, </pages> <month> Sep </month> <year> 1989. </year>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho <ref> [61] </ref>. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42].
Reference: [62] <author> E. Kalns and L. Ni. </author> <title> Processor Mapping Techniques Toward Efficient Data Re distribution. </title> <booktitle> In Proceedings of the 8 th International Parallel Processing Symposium, </booktitle> <pages> pages 469-476, </pages> <month> April </month> <year> 1994. </year> <note> BIBLIOGRAPHY 151 </note>
Reference-contexts: The reason for doing CHAPTER 1. INTRODUCTION 10 this is to try to overlap the communication involved in redistribution with some of the computation in the program. Kalns and Ni <ref> [62] </ref> present a technique for mapping data to processors so as to minimize the total amount of data that must be communicated during redistribution. Ramaswamy and Banerjee [99] discuss algorithms for redistribution based on a mathematical representation for regular distributions called PITFALLS.
Reference: [63] <author> S. Kaushik, C. Huang, R. Johnson, and P. Sadayappan. </author> <title> An Approach to Communication-Efficient Data Redistribution. </title> <booktitle> In Proceedings of the 8 th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Gupta et al. [53] and Koelbel [66] provide closed form expressions for determining the send and receive processor sets and data sets for redistributing arrays between block and cyclic distributions. An analytical model for evaluating the communication cost of data redistribution is presented in <ref> [63] </ref>. A multiphase approach to redistribution is discussed in [64]. Wakatani and Wolfe [126] describe a method of array redistribution called Strip Mining Redistribution in which parts of an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole.
Reference: [64] <author> S. Kaushik, C. Huang, J. Ramanujam, and P. Sadayappan. </author> <title> Multi-phase Array Redistribution: Modeling and Evaluation. </title> <type> Technical Report OSU-CISRC-9/94-52, </type> <institution> Computer and Information Science Research Center, The Ohio State University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: An analytical model for evaluating the communication cost of data redistribution is presented in [63]. A multiphase approach to redistribution is discussed in <ref> [64] </ref>. Wakatani and Wolfe [126] describe a method of array redistribution called Strip Mining Redistribution in which parts of an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole. The reason for doing CHAPTER 1.
Reference: [65] <author> C. Koelbel. </author> <title> Compiling Programs for Nonshared Memory Machines. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: The concept of defining processor arrays and distributing data to them was first introduced in the programming language BLAZE [82] in the context of shared memory systems with non-uniform access times. This research was continued in the Kali <ref> [65] </ref> programming language for distributed memory machines. The Kali compiler uses the inspector/executor strategy to parallelize irregular computations. The compilation system SUPERB [129] parallelizes sequential programs semi-automatically for distributed memory machines. The SUPERB tool transforms sequential Fortran programs with data distribution annotations into parallel programs.
Reference: [66] <author> C. Koelbel. </author> <title> Compile-Time Generation of Regular Communication Patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Agrawal et al. [1] describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution. Gupta et al. [53] and Koelbel <ref> [66] </ref> provide closed form expressions for determining the send and receive processor sets and data sets for redistributing arrays between block and cyclic distributions. An analytical model for evaluating the communication cost of data redistribution is presented in [63]. A multiphase approach to redistribution is discussed in [64].
Reference: [67] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Vienna Fortran [21, 130] and CM Fortran [121] also allow the user to write programs in global address space. Recently, the High Performance Fortran Forum, comprising a group of researchers from industry, academia and research laboratories, developed a standard language called High Performance Fortran (HPF) <ref> [57, 67] </ref>. HPF was designed to provide a CHAPTER 1. INTRODUCTION 5 portable extension to Fortran 90 for writing data-parallel applications. It includes features for mapping data to processors, specifying data-parallel operations, and methods for interfacing HPF programs to other programming paradigms. <p> However once good compilers are available, the modern features and powerful capabilities of HPF are expected to make it the new popular version of Fortran for scientists and engineers solving complex large-scale problems <ref> [67] </ref>. 1.4 Need for High Performance I/O Another important issue in high performance computing is providing support for high performance parallel input-output (I/O). <p> Chapter 3 Runtime Support for Array Redistribution 3.1 Introduction In distributed memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic or block-cyclic as in Fortran D [43] and High Performance Fortran (HPF) <ref> [57, 67] </ref>; or irregular in which there is no function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program.
Reference: [68] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 440-451, </pages> <month> oct </month> <year> 1991. </year>
Reference-contexts: It depends on the values of the indirection arrays, which may not be known until runtime. Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers <ref> [93, 102, 34, 35, 68] </ref>. One way of detecting and performing the communication is by using an inspector-executor [68, 102] approach. A parallel loop is transformed into two constructs called an inspector and an executor. <p> Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers [93, 102, 34, 35, 68]. One way of detecting and performing the communication is by using an inspector-executor <ref> [68, 102] </ref> approach. A parallel loop is transformed into two constructs called an inspector and an executor. During program execution, the inspector examines the data references made by a processor, and calculates what off-processor data needs to be fetched and where that data will be stored once it is received.
Reference: [69] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994. </year> <note> Revised as Technical Report PCS-TR94-226, </note> <institution> Dept. of Computer Science, Dartmouth College. </institution>
Reference-contexts: Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12]. Disk-directed I/O is a technique for performing collective I/O at the file system level <ref> [69, 70, 71] </ref>. 1.7 Organization of this Thesis The rest of this thesis is organized as follows. Chapter 2 gives an overview of some of the issues in providing runtime support for in-core and out-of-core data-parallel programs. Runtime support for array redistribution is discussed in Chapter 3. <p> In this method, I/O is done in two phases. In the first phase, processors cooperate to read data in large chunks, and in the second phase they do an in-core redistribution of the data. Disk-directed I/O <ref> [69] </ref> is a technique which proposes to do collective I/O at the file system level. In disk-directed I/O, a collective I/O request is sent to all I/O nodes which determine the order and timing of the flow of data. CHAPTER 6.
Reference: [70] <author> D. Kotz. </author> <title> Disk-directed I/O for an Out-of-Core Computation. </title> <type> Technical Report PCS-TR95-251, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12]. Disk-directed I/O is a technique for performing collective I/O at the file system level <ref> [69, 70, 71] </ref>. 1.7 Organization of this Thesis The rest of this thesis is organized as follows. Chapter 2 gives an overview of some of the issues in providing runtime support for in-core and out-of-core data-parallel programs. Runtime support for array redistribution is discussed in Chapter 3.
Reference: [71] <author> D. Kotz. </author> <title> Expanding the Potential for Disk-directed I/O. </title> <type> Technical Report PCS-TR95-254, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1995. </year> <note> BIBLIOGRAPHY 152 </note>
Reference-contexts: Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12]. Disk-directed I/O is a technique for performing collective I/O at the file system level <ref> [69, 70, 71] </ref>. 1.7 Organization of this Thesis The rest of this thesis is organized as follows. Chapter 2 gives an overview of some of the issues in providing runtime support for in-core and out-of-core data-parallel programs. Runtime support for array redistribution is discussed in Chapter 3.
Reference: [72] <author> D. Kotz and T. Cai. </author> <title> Exploring the Use of I/O Nodes for Computation in a MIMD Multiprocessor. </title> <type> Technical Report PCS-TR94-232, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <note> Revised February 1995. </note>
Reference-contexts: The system is assumed to be provided with a set of disks and I/O nodes. The I/O nodes can either be dedicated processors or some of the compute nodes may also serve as I/O nodes <ref> [72] </ref>. Each processor may either have its own local disk or all processors may share the set of disks.
Reference: [73] <author> D. Kotz and C. Ellis. </author> <title> Prefetching in File Systems for MIMD Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: A simple way of achieving this is to issue a non-blocking I/O read request for the next slab immediately after the current slab has been read. This is called Data Prefetching. Kotz and Ellis <ref> [73] </ref> discuss in detail the effects of prefetching in parallel file systems. Since the read request is non-blocking, the reading of the next slab can be overlapped with the computation being performed on the current slab. If the computation time is comparable to the I/O time, this can CHAPTER 5.
Reference: [74] <author> D. Kotz and N. Nieuwejaar. </author> <title> Dynamic File Access Characteristics of a Production Parallel Scientific Workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This performance analysis can be extended to out-of-core applications in which the memory hierarchy includes a much slower secondary memory. There have also been studies of the file-access characteristics of application programs on multiprocessor systems. Kotz and Nieuwejaar <ref> [74] </ref> present the results of a three week tracing study in which all file-related activity on an Intel iPSC/860 running production, parallel scientific applications at NASA Ames Research Center was recorded. <p> RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 91 support. There are several reasons for this. First, the data access patterns of many parallel programs are such that they result in a large number of small requests to the file system <ref> [74] </ref>. Since the I/O latency is very high, this results is poor performance. Second, the interface to any parallel file system does not currently allow a programmer to specify strided accesses using a single read or write call; though there are some recent proposals to rectify this [28, 88]. <p> CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 101 Dimension 1 2 3 4 5 6 7 Incore lb Incore ub Incore lbo Incore ubo Global sz OCLA size Procs OOC storage x y Distribution Block sz 5.5.1 Data Sieving Studies of file access characteristics by Kotz and Nieuwejaar <ref> [74] </ref> have shown that many scientific applications actually make strided accesses to the file. Hence, all the PASSION runtime routines for reading or writing data from/to files support the reading/writing of regular sections of arrays with strides.
Reference: [75] <author> H. Kung. </author> <title> Memory Requirements for Balanced Computer Architectures. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 49-54, </pages> <year> 1986. </year>
Reference-contexts: Hence, data needs to be stored on disk and the performance of the program depends on how fast the processors can access data from disks. In order to have a balanced system <ref> [75] </ref>, it is essential that the I/O bandwidth is comparable to the CPU and communication bandwidth. Unfortunately, the performance of the I/O subsystems of MPPs has not kept pace with their CPU and communication capabilities. It is still CHAPTER 1.
Reference: [76] <author> C. Leiserson. Fat-Trees: </author> <title> Universal Networks for Hardware-Efficient Supercomputing. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 952-958, </pages> <year> 1984. </year>
Reference-contexts: CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 60 Processors Switches Switches of each datum, such as global reduction operations, parallel prefix operations and processor synchronization. The data network supports point-to-point communication and has a fat tree topology <ref> [76, 77] </ref> as shown in Figures 4.1 and 4.2. It is actually a 4-ary fat tree or quad tree, where each node has four children. Each internal node of the fat tree is implemented as a set of switches.
Reference: [77] <author> C. Leiserson, Z. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, W. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <year> 1992. </year>
Reference-contexts: CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 60 Processors Switches Switches of each datum, such as global reduction operations, parallel prefix operations and processor synchronization. The data network supports point-to-point communication and has a fat tree topology <ref> [76, 77] </ref> as shown in Figures 4.1 and 4.2. It is actually a 4-ary fat tree or quad tree, where each node has four children. Each internal node of the fat tree is implemented as a set of switches.
Reference: [78] <author> J. Li and M. Chen. </author> <title> Compiling Communication-Efficient Programs for Massively Parallel Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: ISSUES IN RUNTIME SUPPORT 13 distributed using either a block, cyclic, or block-cyclic distribution [119]. The array subscripts used in the program are usually linear functions of the loop indices. Many different communication patterns are possible depending on the array access pattern in the program. Li and Chen <ref> [78] </ref> characterize many of the commonly occurring communication patterns. A library of collective communication routines is needed for carrying out communication efficiently. A particular type of communication pattern which occurs frequently is the all-to-all communication pattern. Efficient algorithms for all-to-all communication are presented in Chapter 4 of this thesis.
Reference: [79] <author> J. Li and M. Chen. </author> <title> Index Domain Alignment: Minimizing Cost of Cross-References Between Distributed Arrays. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Computation 92, </booktitle> <month> October </month> <year> 1992. </year> <note> BIBLIOGRAPHY 153 </note>
Reference-contexts: The PARADIGM project at the University of Illi-nois aims at developing a fully automated compiler to translate sequential programs to parallel programs for distributed memory computers. The problem of automatic alignment of arrays has been studied by Chatterjee et al. [22] and Li et al. <ref> [79] </ref>. There has been some research in runtime support for applications with irregular data access patterns. The PARTI/CHAOS toolkit is a collection of runtime library routines to handle irregular computations [35, 102].
Reference: [80] <author> R. Littlefield. </author> <title> Tuning Communication. Proceedings of the Delta Advanced User Training Class Notes, </title> <type> CCSF Technical Report CCSF-25-92, </type> <pages> pages 99-119, </pages> <month> jul </month> <year> 1992. </year>
Reference-contexts: As suggested in <ref> [80] </ref>, we use forced messages (which provide higher bandwidth but also higher startup cost) if the message size is greater than or equal to 1.5 Kbytes and unforced messages if the message size is less than 1.5 Kbytes. The performance of PEX is shown in Table 4.4. <p> RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 88 0 0.4 0.8 1.2 1.6 Time (s) Number of Processors PEX 3 3 GEN + + + REX 2 2 2 IPEX fi fi fi fi this purpose, we use typical values for the communication costs on the Delta <ref> [80, 3] </ref>, namely for unforced messages ff = 75s, fi ex = 0:35s and for forced messages ff = 150s, fi ex = 0:2s.
Reference: [81] <author> M. Livny, S. Khoshafian, and H. Boral. </author> <title> Multi-Disk Management Algorithms. </title> <booktitle> In Proceedings of the 1987 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 69-77, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Disk striping where data is distributed across disks at a small granularity so that each block is distributed across all disks is proposed in [101]. File declustering, where different blocks of a file are stored on distinct disks is suggested in <ref> [81] </ref>. This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files [29, 27].
Reference: [82] <author> P. Mehrotra and J. Van Rosendale. </author> <title> The BLAZE Language: A Parallel Language for Scientific Programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: The concept of defining processor arrays and distributing data to them was first introduced in the programming language BLAZE <ref> [82] </ref> in the context of shared memory systems with non-uniform access times. This research was continued in the Kali [65] programming language for distributed memory machines. The Kali compiler uses the inspector/executor strategy to parallelize irregular computations. The compilation system SUPERB [129] parallelizes sequential programs semi-automatically for distributed memory machines.
Reference: [83] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <note> Version 1.0, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: The EXPRESS or PVM routines make calls to the message passing library provided on the system. However, this portability is at the cost of slightly lower performance because of this additional layer of communication software. There has also been an effort to develop a standard Message Passing Interface (MPI) <ref> [83] </ref>. The Message Passing Interface Forum, a group of researchers from industry, academia and research laboratories, has defined a set of library interface standards CHAPTER 1. INTRODUCTION 4 for message passing called MPI. <p> It is not necessary that all processors must call the Extended Two-Phase read/write routine. It is possible to define a group of processors involving only those processors that need to access data from the file. This is similar to process groups in MPI <ref> [83] </ref>. CHAPTER 6. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (II) 139 Only the processors in the group need to call the Extended Two-Phase routine and participate in the two-phase process. The I/O workload can be divided among the processors in this group.
Reference: [84] <author> E. Miller and R. Katz. </author> <title> RAMA: Easy Access to a High-Bandwidth Massively Parallel File System. </title> <booktitle> In Proceedings of the 1995 Winter USENIX Conference, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA <ref> [84] </ref> file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed.
Reference: [85] <author> S. Moyer and V. Sunderam. </author> <title> PIOUS: A Scalable Parallel I/O System for Distributed Computing Environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: Runtime libraries for parallel I/O, such as the Panda Library [104, 105] and the Jovian Library [4], are being developed. Portable parallel file systems such as VIP-FS [55], PIOUS <ref> [85] </ref> and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed. Two-phase I/O is a technique for performing collective I/O using a runtime library [37, 12].
Reference: [86] <institution> National Center for Supercomputing Applications, University of Illinois. </institution> <note> NCSA HDF Reference Manual. Version 3.3, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: A good research problem is to determine the best way to define file domains depending on CHAPTER 7. CONCLUSIONS 142 the pattern of access requests as well as the distribution of the file on disks. Scientific data is very often stored in standard data formats such as HDF <ref> [86] </ref> or NetCDF [123]. It would be useful to design and implement efficient runtime support for data stored in files using these formats. The PASSION Runtime Library has currently been implemented on the Intel Touchstone Delta, Paragon and iPSC/860 systems.
Reference: [87] <author> L. Ni and P. McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 62-76, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The network latency for wormhole routing is (L f =B)D + L=B, where L f is the length of each flit, B is the channel bandwidth, D is the path length, and L is the length of the message <ref> [87] </ref>. Thus, if L f &lt;< L, the path length D will not significantly affect the network latency unless it is very large. Further details of wormhole routing can be found in [87]. 4.1.3 Performance Models Barnett et. al. [3] have proposed algorithms and performance models for global combine operations on <p> B is the channel bandwidth, D is the path length, and L is the length of the message <ref> [87] </ref>. Thus, if L f &lt;< L, the path length D will not significantly affect the network latency unless it is very large. Further details of wormhole routing can be found in [87]. 4.1.3 Performance Models Barnett et. al. [3] have proposed algorithms and performance models for global combine operations on a wormhole routed mesh. We use similar models for our all-to-all communication algorithms, which take into account link conflicts and other characteristics of the underlying communication system.
Reference: [88] <author> N. Nieuwejaar and D. Kotz. </author> <title> Low-level Interfaces for High-level Parallel I/O. </title> <type> Technical Report PCS-TR95-253, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Since the I/O latency is very high, this results is poor performance. Second, the interface to any parallel file system does not currently allow a programmer to specify strided accesses using a single read or write call; though there are some recent proposals to rectify this <ref> [28, 88] </ref>. Third, the interface does not provide support for processors to make a collective I/O request. So file systems cannot perform any optimizations based on the knowledge of the access requests of all processors.
Reference: [89] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler Support for Out-of-Core Arrays on Data Parallel Machines. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> February </month> <year> 1995. </year> <note> BIBLIOGRAPHY 154 </note>
Reference-contexts: Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. Paleczny et al. <ref> [89] </ref> are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. [18] are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in [8, 17, 109, 18].
Reference: [90] <author> Parasoft Corporation. </author> <title> EXPRESS Fortran User's Guide, </title> <year> 1990. </year>
Reference-contexts: There have been several attempts to provide some measure of portability in parallel programs. There are number of portable communication libraries like EXPRESS <ref> [90] </ref>, PVM [114], PICL [47], P4 [19] etc. which provide a communication layer above the native message passing library of the system. These libraries provide a well-defined set of communication routines which remain the same for any system.
Reference: [91] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks. </title> <booktitle> In Proceedings of ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Language extensions for out-of-core data-parallel programs are proposed in [8, 17, 109, 18]. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in <ref> [91] </ref> for better performance, reliability, power consumption and scalability. An excellent overview of RAID concepts is given in [24]. Disk striping where data is distributed across disks at a small granularity so that each block is distributed across all disks is proposed in [101]. <p> File declustering, where different blocks of a file are stored on distinct disks is suggested in [81]. This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes <ref> [91] </ref>. Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout.
Reference: [92] <author> P. Pierce. </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsys tem. </title> <booktitle> In Proceedings of 4 th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: File declustering, where different blocks of a file are stored on distinct disks is suggested in [81]. This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) <ref> [92] </ref> and in various RAID schemes [91]. Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. <p> A poor I/O capability can severely degrade the performance of the entire program. Almost all present generation parallel computers provide some kind of hardware and system software support for parallel I/O <ref> [29, 92, 9, 36] </ref>. But, the I/O performance observed at the application level is usually much lower than what the hardware can 90 CHAPTER 5. RUNTIME SUPPORT FOR OUT-OF-CORE PROGRAMS (I) 91 support. There are several reasons for this. <p> The I/O system on the Delta consists of 32 dedicated I/O nodes, each an Intel 80386 microprocessor. Each I/O node is connected to two disks, resulting in a total of 64 disks. A Concurrent File System (CFS) <ref> [92] </ref> is provided for parallel access to files. By default, a file is striped across all 64 disks in a round-robin fashion in blocks of size 4 Kbytes.
Reference: [93] <author> R. Ponnusamy. </author> <title> Runtime Support and Compilation Methods for Irregular Computations on Distributed Memory Parallel Machines. </title> <type> PhD thesis, </type> <institution> School of Computer and Information Science, Syracuse University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy <ref> [93] </ref>, Das [34] and Hanxleden [125]. Agrawal et al. [1] describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution. <p> It depends on the values of the indirection arrays, which may not be known until runtime. Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers <ref> [93, 102, 34, 35, 68] </ref>. One way of detecting and performing the communication is by using an inspector-executor [68, 102] approach. A parallel loop is transformed into two constructs called an inspector and an executor. <p> We briefly describe some of the issues related to providing runtime support for an HPF compiler. We do not discuss runtime support for compiling irregular problems; that is explained in detail in <ref> [93] </ref>. We first outline the salient features of HPF in order to explain the runtime support needed for an HPF compiler. 2.3.1 Overview of HPF HPF was designed to be a standard portable programming language for writing efficient computationally intensive parallel programs.
Reference: [94] <author> R. Ponnusamy, A. Choudhary, and G. Fox. </author> <title> Communication Overhead on CM-5 : An Experimental Performance Evaluation. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Computation 92, </booktitle> <pages> pages 108-115, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This allows the switches to perform load balancing on the fly. Once the message has reached the necessary height in the tree, it must follow a particular path down. A detailed discussion of the interprocessor communication overhead on the CM-5 is given in <ref> [96, 16, 94] </ref>. CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 61 4.1.2 Touchstone Delta The Intel Touchstone Delta is a 16 fi 32 mesh of computational nodes, each of which is an Intel i860/XR microprocessor. The two-dimensional mesh interconnection network has bidirectional links and uses wormhole routing.
Reference: [95] <author> R. Ponnusamy, R. Thakur, A. Choudhary, and G. Fox. </author> <title> Scheduling Regular and Irregular Communication Patterns on the CM-5. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 394-402, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Redistribution requires all-to-many personalized communication in general and in many cases it requires all-to-all personalized communication. These communication algorithms are described in detail in Chapter 4 of this thesis and in <ref> [117, 95, 120] </ref>. The performance results presented in this chapter have been obtained using these algorithms. We do assume that all the data to be sent from any processor i to processor j has to be CHAPTER 3. <p> Hence, we use fi ex or fi sr depending on the algorithm. We assume that the time taken is independent of distance, a property of both CM-5 and Delta <ref> [95, 3] </ref>. Thus, the time CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 63 required for an exchange step i is given by T = ff + L max (fi ex ; f (i)fi sat ) We assume that conflicting messages share the bandwidth of a network link.
Reference: [96] <author> R. Ponnusamy, R. Thakur, A. Choudhary, K. Velamakanni, and G. Fox. </author> <title> Experimental Performance Evaluation of the CM-5. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 192-202, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This allows the switches to perform load balancing on the fly. Once the message has reached the necessary height in the tree, it must follow a particular path down. A detailed discussion of the interprocessor communication overhead on the CM-5 is given in <ref> [96, 16, 94] </ref>. CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 61 4.1.2 Touchstone Delta The Intel Touchstone Delta is a 16 fi 32 mesh of computational nodes, each of which is an Intel i860/XR microprocessor. The two-dimensional mesh interconnection network has bidirectional links and uses wormhole routing.
Reference: [97] <author> J. Poole. </author> <title> Preliminary Survey of I/O Intensive Applications. Scalable I/O Initiative Working Paper Number 1, </title> <year> 1994. </year>
Reference-contexts: The Applications Working Group of the Scalable I/O Initiative has provided a description of the I/O requirements of several applications in biology, chemistry, physics, earth sciences, engineering and graphics <ref> [97] </ref>. Fox [41] presents a performance analysis of applications on a hypercube machine with a hierarchical memory at each node, consisting of a fast cache and slower main memory. The applications considered are the N-body problem, Poisson's equation solver, matrix multiplication, LU Decomposition, Fast Fourier Transform and neural networks.
Reference: [98] <author> A. Purakayastha, C. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best. Characterizing Parallel File-Access Patterns on a Large-Scale Multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year> <note> BIBLIOGRAPHY 155 </note>
Reference-contexts: Another study of the file-access characteristics CHAPTER 2. ISSUES IN RUNTIME SUPPORT 24 of production applications on the CM-5 at the National Center for Supercomputing Applications, University of Illinois, also found similar results of a large number of small requests <ref> [98] </ref>. This shows that although it is well-known that I/O should be done in large chunks to minimize the effect of high I/O latency, many parallel out-of-core applications actually access small strided data sets. Hence, it is necessary to provide runtime support for accessing small strided data efficiently.
Reference: [99] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic Generation of Efficient Array Redistribution Routines for Distributed Memory Multicomputers. </title> <type> Technical Report CRHC-94-09, </type> <institution> CRHC, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: Kalns and Ni [62] present a technique for mapping data to processors so as to minimize the total amount of data that must be communicated during redistribution. Ramaswamy and Banerjee <ref> [99] </ref> discuss algorithms for redistribution based on a mathematical representation for regular distributions called PITFALLS. <p> Several complex schemes have been proposed for performing this redistribution <ref> [110, 111, 23, 99] </ref>. Since redistribution has to be done at runtime, a simple and efficient algorithm with minimum runtime overhead is necessary. We propose a new method for performing the general cyclic (x) to cyclic (y) redistribution, which has low runtime overhead.
Reference: [100] <author> S. Ranka, J. Wang, and G. Fox. </author> <title> Static and Runtime Algorithms for All-to-Many Personalized Communication on Permutation Networks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 1266-1274, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka <ref> [100, 127] </ref> and also by Shankar and Ranka [106, 107, 108]. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. <p> Efficient algorithms are needed to implement these communication patterns on different network topologies. In this chapter, we consider the all-to-all communication pattern in detail. The other communication patterns, namely all-to-many, many-to all and many-to-many, have been studied in <ref> [100, 127, 106, 107, 108] </ref>. 58 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 59 The all-to-all communication pattern (also called complete exchange 1 ) occurs frequently in many important parallel computing applications such as array redistribution, parallel quicksort, some implementations of two-dimensional Fast Fourier Transform, matrix transpose etc.
Reference: [101] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk Striping. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability. An excellent overview of RAID concepts is given in [24]. Disk striping where data is distributed across disks at a small granularity so that each block is distributed across all disks is proposed in <ref> [101] </ref>. File declustering, where different blocks of a file are stored on distinct disks is suggested in [81]. This is used in the Bridge File System [39], in Intel's Concurrent File System (CFS) [92] and in various RAID schemes [91].
Reference: [102] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and Runtime Compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The problem of automatic alignment of arrays has been studied by Chatterjee et al. [22] and Li et al. [79]. There has been some research in runtime support for applications with irregular data access patterns. The PARTI/CHAOS toolkit is a collection of runtime library routines to handle irregular computations <ref> [35, 102] </ref>. These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das [34] and Hanxleden [125]. <p> It depends on the values of the indirection arrays, which may not be known until runtime. Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers <ref> [93, 102, 34, 35, 68] </ref>. One way of detecting and performing the communication is by using an inspector-executor [68, 102] approach. A parallel loop is transformed into two constructs called an inspector and an executor. <p> Thus a runtime resolution scheme is needed to determine the communication required. Runtime support for irregular problems has been studied by a number of researchers [93, 102, 34, 35, 68]. One way of detecting and performing the communication is by using an inspector-executor <ref> [68, 102] </ref> approach. A parallel loop is transformed into two constructs called an inspector and an executor. During program execution, the inspector examines the data references made by a processor, and calculates what off-processor data needs to be fetched and where that data will be stored once it is received.
Reference: [103] <author> D. Scott. </author> <title> Efficient All-to-All Communication Patterns in Hypercube and Mesh Topologies. </title> <booktitle> In Proceedings of the 6 th Distributed Memory Computing Conference, </booktitle> <pages> pages 398-403, </pages> <year> 1991. </year>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in <ref> [103, 7, 113, 51, 54] </ref>. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108]. <p> 6 4 $ 7 2 $ 6 2 $ 7 2 $ 4 2 $ 5 4.2.2 Pairwise Exchange (PEX) We consider the Pairwise Exchange (PEX) algorithm which has been shown to be the best algorithm for a hypercube network, on which it guarantees no link contention at any step <ref> [6, 103] </ref>. The algorithm is described in Figure 4.3. It requires P 1 steps and the communication schedule is as follows. At step i, 1 i P 1, each processor exchanges a message with the processor determined by taking the exclusive-or of its processor number with i. <p> Scott has shown that a 3 =4 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 72 is the lower bound on the number of phases required to perform complete exchange on an a fi a mesh such that there is no link contention in any phase <ref> [103] </ref>. However, if we allow link contention to exist, the operation can be performed in fewer steps. We have adopted this approach of allowing a small amount of link contention to exist, thereby reducing the number of steps and keeping all processors active at every step.
Reference: [104] <author> K. Seamons and M. Winslett. </author> <title> An Efficient Abstract Interface for Multidimensional Array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library <ref> [104, 105] </ref> and the Jovian Library [4], are being developed. Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed.
Reference: [105] <author> K. Seamons and M. Winslett. </author> <title> A Data Management Approach for Handling Large Compressed Arrays in High Performance Computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Vesta is a parallel file system which supports logical partitioning of files [29, 27]. The RAMA [84] file system distributes file blocks across disks randomly using a hash function, instead of the usual striped layout. Runtime libraries for parallel I/O, such as the Panda Library <ref> [104, 105] </ref> and the Jovian Library [4], are being developed. Portable parallel file systems such as VIP-FS [55], PIOUS [85] and PPFS [60] have been developed recently. Techniques for improving I/O performance using collective I/O have also been proposed.
Reference: [106] <author> R. Shankar, K. Alsabti, and S. Ranka. </author> <title> The Transportation Primitive. </title> <type> Technical report, </type> <institution> Dept. of Computer and Information Science, Syracuse University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka <ref> [106, 107, 108] </ref>. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. <p> Efficient algorithms are needed to implement these communication patterns on different network topologies. In this chapter, we consider the all-to-all communication pattern in detail. The other communication patterns, namely all-to-many, many-to all and many-to-many, have been studied in <ref> [100, 127, 106, 107, 108] </ref>. 58 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 59 The all-to-all communication pattern (also called complete exchange 1 ) occurs frequently in many important parallel computing applications such as array redistribution, parallel quicksort, some implementations of two-dimensional Fast Fourier Transform, matrix transpose etc.
Reference: [107] <author> R. Shankar and S. Ranka. </author> <title> Random Data Accesses on a Coarse-grained Parallel Machine I. One-to-One Mappings. </title> <type> Technical report, </type> <institution> Dept. of Computer and Information Science, Syracuse University, </institution> <month> October </month> <year> 1994. </year> <note> BIBLIOGRAPHY 156 </note>
Reference-contexts: Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka <ref> [106, 107, 108] </ref>. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. <p> Efficient algorithms are needed to implement these communication patterns on different network topologies. In this chapter, we consider the all-to-all communication pattern in detail. The other communication patterns, namely all-to-many, many-to all and many-to-many, have been studied in <ref> [100, 127, 106, 107, 108] </ref>. 58 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 59 The all-to-all communication pattern (also called complete exchange 1 ) occurs frequently in many important parallel computing applications such as array redistribution, parallel quicksort, some implementations of two-dimensional Fast Fourier Transform, matrix transpose etc.
Reference: [108] <author> R. Shankar and S. Ranka. </author> <title> Random Data Accesses on a Coarse-grained Parallel Machine II. One-to-Many and Many-to-One Mappings. </title> <type> Technical report, </type> <institution> Dept. of Computer and Information Science, Syracuse University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka <ref> [106, 107, 108] </ref>. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. <p> Efficient algorithms are needed to implement these communication patterns on different network topologies. In this chapter, we consider the all-to-all communication pattern in detail. The other communication patterns, namely all-to-many, many-to all and many-to-many, have been studied in <ref> [100, 127, 106, 107, 108] </ref>. 58 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 59 The all-to-all communication pattern (also called complete exchange 1 ) occurs frequently in many important parallel computing applications such as array redistribution, parallel quicksort, some implementations of two-dimensional Fast Fourier Transform, matrix transpose etc.
Reference: [109] <author> M. Snir. </author> <title> Proposal for IO. </title> <note> Posted to HPFF I/O Forum by Marc Snir, </note> <month> July </month> <year> 1992. </year>
Reference-contexts: Paleczny et al. [89] are also developing techniques for compiling out-of-core data-parallel programs. Brezany et al. [18] are working on compilation techniques for out-of-core programs in the context of Vienna Fortran. Language extensions for out-of-core data-parallel programs are proposed in <ref> [8, 17, 109, 18] </ref>. There has been a lot of effort to improve parallel I/O performance both by hardware and software means. Various RAID schemes (Redundant Arrays of Inexpensive CHAPTER 1. INTRODUCTION 11 Disks) are proposed in [91] for better performance, reliability, power consumption and scalability.
Reference: [110] <author> J. Stichnoth. </author> <title> Efficient Compilation of Array Statements for Private Memory Multicomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: some research on the closely related problem of determining the local addresses and communication sets for array assignment statements like A (l 1 : h 1 : s 1 ) = B (l 2 : h 2 : s 2 ) where A and B have different cyclic (m) distributions <ref> [23, 110, 111, 52] </ref>. Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. <p> Several complex schemes have been proposed for performing this redistribution <ref> [110, 111, 23, 99] </ref>. Since redistribution has to be done at runtime, a simple and efficient algorithm with minimum runtime overhead is necessary. We propose a new method for performing the general cyclic (x) to cyclic (y) redistribution, which has low runtime overhead.
Reference: [111] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating Communication for Array Statements: Design, Implementation, and Evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: some research on the closely related problem of determining the local addresses and communication sets for array assignment statements like A (l 1 : h 1 : s 1 ) = B (l 2 : h 2 : s 2 ) where A and B have different cyclic (m) distributions <ref> [23, 110, 111, 52] </ref>. Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. <p> Several complex schemes have been proposed for performing this redistribution <ref> [110, 111, 23, 99] </ref>. Since redistribution has to be done at runtime, a simple and efficient algorithm with minimum runtime overhead is necessary. We propose a new method for performing the general cyclic (x) to cyclic (y) redistribution, which has low runtime overhead.
Reference: [112] <author> E. Su, D. Palermo, and P. Banerjee. </author> <title> Automatic Parallelization of Regular Computations For Distributed Memory Multicomputers in the PARADIGM Compiler. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages II-30|II-38, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Gupta [50] has proposed a constraint based approach to automatically determine a good data distribution. This method has been incorporated in the PARADIGM compiler <ref> [49, 112] </ref>. The PARADIGM project at the University of Illi-nois aims at developing a fully automated compiler to translate sequential programs to parallel programs for distributed memory computers. The problem of automatic alignment of arrays has been studied by Chatterjee et al. [22] and Li et al. [79].
Reference: [113] <author> N. Sundar, D. Jayasimha, D. Panda, and P. Sadayappan. </author> <title> Complete Exchange in 2D Meshes. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 406-413, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Algorithms for all-to-all communication (complete exchange) on a hypercube have been proposed by Bokhari [6], and also by Johnsson and Ho [61]. Complete exchange algorithms for a two-dimensional mesh are discussed in <ref> [103, 7, 113, 51, 54] </ref>. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka [100, 127] and also by Shankar and Ranka [106, 107, 108].
Reference: [114] <author> V. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2 </volume> <pages> 315-339, </pages> <month> dec </month> <year> 1991. </year>
Reference-contexts: There have been several attempts to provide some measure of portability in parallel programs. There are number of portable communication libraries like EXPRESS [90], PVM <ref> [114] </ref>, PICL [47], P4 [19] etc. which provide a communication layer above the native message passing library of the system. These libraries provide a well-defined set of communication routines which remain the same for any system.
Reference: [115] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs. </title> <booktitle> In Proceedings of the 8 th ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library <ref> [116, 25, 115, 118] </ref> discussed in Chapters 5 and 6 of this thesis. Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. <p> A number of optimizations such as Data Sieving, Data Prefetching, Data Reuse, and the Extended Two-Phase Method have been incorporated in the library <ref> [116, 115, 118] </ref>. CHAPTER 5.
Reference: [116] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION Runtime Library for Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year> <note> BIBLIOGRAPHY 157 </note>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library <ref> [116, 25, 115, 118] </ref> discussed in Chapters 5 and 6 of this thesis. Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. <p> This makes it difficult for the programmer to perform optimizations at the application level, for example prefetching to overlap I/O with computation, because of the complexities involved in managing buffers and file pointers. We have developed a runtime system called PASSION (Parallel and Scalable Software for Input-Output) <ref> [116, 25] </ref> which aims to alleviate many of these problems and provide better software support for out-of-core programs. We believe that high-level interfaces that facilitate the use of semantic knowledge about the accesses from parallel programs are necessary for simple and portable application programming. <p> A number of optimizations such as Data Sieving, Data Prefetching, Data Reuse, and the Extended Two-Phase Method have been incorporated in the library <ref> [116, 115, 118] </ref>. CHAPTER 5.
Reference: [117] <author> R. Thakur and A. Choudhary. </author> <title> All-to-All Communication on Meshes with Wormhole Routing. </title> <booktitle> In Proceedings of the 8 th International Parallel Processing Symposium, </booktitle> <pages> pages 561-565, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Redistribution requires all-to-many personalized communication in general and in many cases it requires all-to-all personalized communication. These communication algorithms are described in detail in Chapter 4 of this thesis and in <ref> [117, 95, 120] </ref>. The performance results presented in this chapter have been obtained using these algorithms. We do assume that all the data to be sent from any processor i to processor j has to be CHAPTER 3.
Reference: [118] <author> R. Thakur and A. Choudhary. </author> <title> Collective I/O Using an Extended Two-Phase Method with Dynamic Partitioning. </title> <type> Technical Report SCCS-704, </type> <institution> NPAC, Syra-cuse University, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. Bordawekar [11, 8] is developing a compiler for out-of-core HPF programs which uses the runtime library <ref> [116, 25, 115, 118] </ref> discussed in Chapters 5 and 6 of this thesis. Cormen and Colvin [31] are developing a compiler-like preprocessor for out-of-core C*, called ViC*, which translates out-of-core C* programs to standard C* programs with calls to a runtime library for I/O. <p> A number of optimizations such as Data Sieving, Data Prefetching, Data Reuse, and the Extended Two-Phase Method have been incorporated in the library <ref> [116, 115, 118] </ref>. CHAPTER 5.
Reference: [119] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime Array Redistribution in HPF Programs. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 309-316, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Thus all the necessary optimizations can be performed beforehand at compile time. In such applications, data is usually 12 CHAPTER 2. ISSUES IN RUNTIME SUPPORT 13 distributed using either a block, cyclic, or block-cyclic distribution <ref> [119] </ref>. The array subscripts used in the program are usually linear functions of the loop indices. Many different communication patterns are possible depending on the array access pattern in the program. Li and Chen [78] characterize many of the commonly occurring communication patterns. <p> It translates HPF programs to Fortran 77 programs with calls to a runtime library <ref> [2, 14, 119] </ref>. The compiler only exploits the parallelism expressed in data parallel constructs such as FORALL, WHERE and array assignment statements. It does not attempt to parallelize other constructs such as DO loops and WHILE loops, since they are used only as naturally sequential control constructs in HPF. <p> Communication Library The HPF compiler described above relies on a very powerful runtime support system which includes a library of collective communication routines [13], a library of intrinsic functions [2, 14] and other runtime routines such as for array redistribution <ref> [119] </ref>. The HPF compiler produces calls to collective communication routines instead of generating individual send and receive calls inside the compiled code. This is done CHAPTER 2. ISSUES IN RUNTIME SUPPORT 19 for the following reasons: 1. Improved performance: To achieve good performance, interprocessor communication must be minimized.
Reference: [120] <author> R. Thakur, R. Ponnusamy, A. Choudhary, and G. Fox. </author> <title> Complete Exchange on the CM-5 and Touchstone Delta. </title> <journal> The Journal of Supercomputing, </journal> <pages> pages 305-328, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Redistribution requires all-to-many personalized communication in general and in many cases it requires all-to-all personalized communication. These communication algorithms are described in detail in Chapter 4 of this thesis and in <ref> [117, 95, 120] </ref>. The performance results presented in this chapter have been obtained using these algorithms. We do assume that all the data to be sent from any processor i to processor j has to be CHAPTER 3.
Reference: [121] <author> Thinking Machines Corporation. </author> <title> CM Fortran Language Reference Manual. </title> <note> Version 2.1, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. The Fortran 90D compiler developed at Syracuse University [13] translates Fortran 90D programs to Fortran 77 plus message passing node programs. Vienna Fortran [21, 130] and CM Fortran <ref> [121] </ref> also allow the user to write programs in global address space. Recently, the High Performance Fortran Forum, comprising a group of researchers from industry, academia and research laboratories, developed a standard language called High Performance Fortran (HPF) [57, 67]. HPF was designed to provide a CHAPTER 1.
Reference: [122] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Fortran D [43, 58] is one such language. Fortran D consists of a set of extensions to Fortran 77 which specify how data is to be distributed among the processors of a distributed memory machine. The Fortran D compiler developed at Rice University <ref> [122, 59] </ref> can translate a Fortran D program into a Fortran 77 plus message passing node program. Another such language is Fortran 90D [128]. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. <p> A compiler for Split-C is being developed at the University of Cal-ifornia, Berkeley, with a runtime support system which uses Active Messages [124]. pC++, a data-parallel extension to C++, has been developed at Indiana University [5]. A Fortran D compiler is being developed at Rice University <ref> [122] </ref>. Some research has been done in developing compilers which can automatically determine a good distribution and alignment of arrays instead of having the user specify them. Gupta [50] has proposed a constraint based approach to automatically determine a good data distribution.
Reference: [123] <institution> Unidata Program Center, University Corporation for Atmospheric Research. </institution> <note> netCDF User's Guide. Version 2.0, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: CONCLUSIONS 142 the pattern of access requests as well as the distribution of the file on disks. Scientific data is very often stored in standard data formats such as HDF [86] or NetCDF <ref> [123] </ref>. It would be useful to design and implement efficient runtime support for data stored in files using these formats. The PASSION Runtime Library has currently been implemented on the Intel Touchstone Delta, Paragon and iPSC/860 systems.
Reference: [124] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: INTRODUCTION 9 irregular problems. A compiler for Split-C is being developed at the University of Cal-ifornia, Berkeley, with a runtime support system which uses Active Messages <ref> [124] </ref>. pC++, a data-parallel extension to C++, has been developed at Indiana University [5]. A Fortran D compiler is being developed at Rice University [122].
Reference: [125] <author> R. von Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler Analysis for Irregular Problems in Fortran D. </title> <booktitle> In Proceedings of the 5 th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year> <note> BIBLIOGRAPHY 158 </note>
Reference-contexts: These primitives have been integrated with the Fortran D compiler at Rice University, the Fortran 90D/HPF compiler at Syracuse University and the Vienna Fortran compiler at the University of Vienna. Compilation methods for irregular problems have been investigated by Pon-nusamy [93], Das [34] and Hanxleden <ref> [125] </ref>. Agrawal et al. [1] describe how runtime support can be integrated with a compiler to solve irregular block-structured problems. Research has also been done in the area of array redistribution.
Reference: [126] <author> A. Wakatani and M. Wolfe. </author> <title> A New Approach to Array Redistribution: Strip Mining Redistribution. </title> <booktitle> In Proceedings of Parallel Architectures and Languages Europe (PARLE 94), </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: An analytical model for evaluating the communication cost of data redistribution is presented in [63]. A multiphase approach to redistribution is discussed in [64]. Wakatani and Wolfe <ref> [126] </ref> describe a method of array redistribution called Strip Mining Redistribution in which parts of an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole. The reason for doing CHAPTER 1.
Reference: [127] <author> J. Wang. </author> <title> Load Balancing and Communication Support for Regular and Irregular Problems. </title> <type> PhD thesis, </type> <institution> School of Computer and Information Science, Syracuse University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Complete exchange algorithms for a two-dimensional mesh are discussed in [103, 7, 113, 51, 54]. Optimal communication algorithms on the hypercube have been developed by Fox and Fur-manski [42]. Algorithms for scheduling irregular communication patterns have been proposed by Wang and Ranka <ref> [100, 127] </ref> and also by Shankar and Ranka [106, 107, 108]. Compiling out-of-core data-parallel programs is a fairly new topic and there has been very little research in that area. A model and compilation strategy for out-of-core data-parallel programs is discussed in [10]. <p> Efficient algorithms are needed to implement these communication patterns on different network topologies. In this chapter, we consider the all-to-all communication pattern in detail. The other communication patterns, namely all-to-many, many-to all and many-to-many, have been studied in <ref> [100, 127, 106, 107, 108] </ref>. 58 CHAPTER 4. RUNTIME SUPPORT FOR ALL-TO-ALL COMMUNICATION 59 The all-to-all communication pattern (also called complete exchange 1 ) occurs frequently in many important parallel computing applications such as array redistribution, parallel quicksort, some implementations of two-dimensional Fast Fourier Transform, matrix transpose etc.
Reference: [128] <author> M. Wu and G. Fox. </author> <title> Compiling Fortran 90 Programs for Distributed Memory MIMD Parallel Computers. </title> <type> Technical Report SCCS-88, </type> <institution> NPAC, Syracuse University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The Fortran D compiler developed at Rice University [122, 59] can translate a Fortran D program into a Fortran 77 plus message passing node program. Another such language is Fortran 90D <ref> [128] </ref>. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. The Fortran 90D compiler developed at Syracuse University [13] translates Fortran 90D programs to Fortran 77 plus message passing node programs.
Reference: [129] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> SUPERB: A Tool for Semi-Automatic MIMD/SIMD Parallelization. </title> <booktitle> Parallel Computing, </booktitle> <pages> pages 1-18, </pages> <year> 1988. </year>
Reference-contexts: This research was continued in the Kali [65] programming language for distributed memory machines. The Kali compiler uses the inspector/executor strategy to parallelize irregular computations. The compilation system SUPERB <ref> [129] </ref> parallelizes sequential programs semi-automatically for distributed memory machines. The SUPERB tool transforms sequential Fortran programs with data distribution annotations into parallel programs. Compilers for functional languages like Id Nouveau and Crystal have been developed for distributed memory machines.
Reference: [130] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a Language Specification, Version 1.1. </title> <type> Interim Report 21, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Another such language is Fortran 90D [128]. Fortran 90D consists of a set of extensions to Fortran 90, similar to those used in Fortran D. The Fortran 90D compiler developed at Syracuse University [13] translates Fortran 90D programs to Fortran 77 plus message passing node programs. Vienna Fortran <ref> [21, 130] </ref> and CM Fortran [121] also allow the user to write programs in global address space. Recently, the High Performance Fortran Forum, comprising a group of researchers from industry, academia and research laboratories, developed a standard language called High Performance Fortran (HPF) [57, 67].
Reference: [131] <author> G. Zorpette. </author> <title> Teraflops Galore. </title> <journal> IEEE Spectrum, </journal> <pages> pages 26-76, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: As a result, parallel computers are increasingly being used in many applications which require a great deal of computational power. Examples of such applications include many large scale computations in physics, chemistry, biology, engineering, medicine and other sciences, which have been identified as Grand Challenge Applications <ref> [56, 131] </ref>. Many applications dealing with information technology, such as multimedia systems, video on demand, video compression and decompression, also require a large amount of compute power.
References-found: 131

