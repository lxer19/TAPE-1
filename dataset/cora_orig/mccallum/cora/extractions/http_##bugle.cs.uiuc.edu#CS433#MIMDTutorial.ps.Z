URL: http://bugle.cs.uiuc.edu/CS433/MIMDTutorial.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/CS433.html
Root-URL: http://www.cs.uiuc.edu
Title: Distributed-Memory MIMD Computing An Introduction  High Performance Scientific  
Author: E. R. Jessup 
Address: Boulder  
Affiliation: University of Colorado at  
Date: January 3, 1995  
Pubnum: Computing  
Abstract-found: 0
Intro-found: 1
Reference: [Almasi & Gottlieb 94] <author> ALMASI, GEORGE S. AND ALLAN GOTTLIEB. </author> <year> [1994]. </year> <note> Highly Parallel Computing. Benjamin/Cummings, 2nd edition. ISBN 0-8053-0443-6. </note>
Reference-contexts: Instead, all nodes were connected to a pair of common data buses. Using the data buses, any one node in the machine could communicate directly with any other node. The NonStop was a computer designed especially for database applications <ref> [Almasi & Gottlieb 94] </ref>. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 11 Other designs followed soon after, beginning with the introduction of the Erlangen General Purpose Architecture (EGPA) in 1977 by W. Haendler, F. Hofman, and H. Schneider at the University of Erlangen in Germany. <p> Each node held Intel 8086/8087 processors and 128 Kbytes of memory. The first prototype went into operation in 1982. The prototype Cosmic Cube was followed by improved versions culminating in the Mark III Cosmic Cube completed in 1987. The Mark III uses Motorola 68020 processors <ref> [Almasi & Gottlieb 94, Wilson 94] </ref>. The Cosmic Cube inspired the development of at least two successful commercial hypercube multiprocessors. The first of these was the Intel Personal Supercomputer (the iPSC, later called the iPSC/1). The first iPSC/1's were delivered in 1985 and have up to 128 nodes. <p> Each node had 64 Kbytes of memory and a 32-bit processor. Its host computer had an Intel 80286 processor and ran a special version of Unix that allowed the nCUBE to emulate a machine with a single distributed file system CUBoulder : HPSC Course Notes 12 Distributed-Memory MIMD Computing <ref> [Almasi & Gottlieb 94] </ref>. The iPSC/1 and the nCUBE/1 have both spawned new generations of hypercube multiprocessors with more powerful processors and more advanced communication. The iPSC/2 hypercube appeared in 1987 using 80386/80387 processors. The iPSC/2 also has separate computation and communication processors on each node. <p> The third generation hypercube, the nCUBE/3, was released in 1994. It offers even faster communication and computation and can have up to 65,536 nodes and up to 1 Gbyte of memory per node. The second and third generation nCUBEs have been geared toward the database market <ref> [Almasi & Gottlieb 94, Wyckoff 94] </ref>. The mesh multiprocessor has a more scalable design than does the hypercube. In a hypercube multiprocessor with p = 2 d nodes, a node has d nearest neighbors and so d communication wires. <p> The Cray T3D has DEC Alpha chips interconnected as a torus. The IBM SP1 and SP2 use IBM RS/6000 processors interconnected by means of a high-speed Omega switch <ref> [Almasi & Gottlieb 94, Hwang 93, Wilson 94] </ref>. A wholly different type of DM-MIMD multiprocessor is represented by a cluster of workstations. Workstation clusters are growing in popularity because of their affordability and the substantial computing power of individual workstations. <p> For example, in the tree-based Teradata DBC/1012 Data Base Computer, two types of processors at the leaves process queries and access databases. Processors located above the leaves are dedicated to sorting and broadcasting operations <ref> [Almasi & Gottlieb 94] </ref>. The general-purpose Thinking Machines CM-5 architecture is based on a quaternary tree with SPARC processors at the leaves and routing chips at the other tree nodes. Each leaf processor has four parent routing chips.
Reference: [Barnett et al. 91] <author> BARNETT, M., D.G. PAYNE, AND R. VAN DE GEIJN. </author> <year> [1991]. </year> <title> Optimal broadcasting in mesh-connected architectures. </title> <type> Technical Report TR-91-38, </type> <institution> Dept. of Computer Science, University of Texas at Austin. </institution>
Reference-contexts: On the hypercube, the alternate direction exchange (ADE) presented in section 3.3 was the most efficient means for such a gather operation. The ADE, however, relies completely on the recursive structure of the hypercube and CUBoulder : HPSC Course Notes 40 Distributed-Memory MIMD Computing contention-free linear array broadcast. Source: <ref> [Barnett et al. 91] </ref>. CUBoulder : HPSC Course Notes Distributed-Memory MIMD Computing 41 broadcast algorithm. so does not extend to the mesh architecture. Thus, a mesh spanning tree gather (MeshSTG) and broadcast must be combined to accumulate complete copies of distributed data on all nodes of the mesh. <p> These mesh operations can also be applied if p 1 and p 2 are not powers of 2. For details of the modifications to arbitrary mesh size as well as for the optimizations available under wormhole routing, see <ref> [Barnett et al. 91, Barnett et al. 94] </ref>.
Reference: [Barnett et al. 94] <author> BARNETT, M., D.G. PAYNE, R. VAN DE GEIJN, AND J. WATTS. </author> <year> [1994]. </year> <title> Broadcasting on meshes with worm-hole routing. </title> <note> Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: These mesh operations can also be applied if p 1 and p 2 are not powers of 2. For details of the modifications to arbitrary mesh size as well as for the optimizations available under wormhole routing, see <ref> [Barnett et al. 91, Barnett et al. 94] </ref>.
Reference: [Beguelin et al. 91] <author> BEGUELIN, A., J. DON-GARRA, A. GEIST, R. MANCHEK, AND V. SUNDER AM. </author> <year> [1991]. </year> <title> A user's guide to PVM: Parallel Virtual Machine. </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: A wholly different type of DM-MIMD multiprocessor is represented by a cluster of workstations. Workstation clusters are growing in popularity because of their affordability and the substantial computing power of individual workstations. Thanks to such networking software as PVM <ref> [Beguelin et al. 91] </ref> workstation clusters can be programmed in the same style as a more traditional multiprocessor. Because the nodes do not have to be identical, a workstation cluster is an example of a heterogeneous computing environment.
Reference: [Dewar & Smosna 90] <author> DEWAR, R. AND M. SMOSNA. </author> <year> [1990]. </year> <pages> Microprocessors: </pages>
Reference-contexts: This poor serial performance is due to memory access costs and to difficulties in programming the chip itself <ref> [Dewar & Smosna 90] </ref>. The main advantage of DM-MIMD multiprocessors is in their large distributed memories and cumulative computing power. The Highly Parallel Computing benchmark demonstrates the full potential of a parallel computer for solving linear systems.
References-found: 5

