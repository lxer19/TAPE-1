URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/tr1191r.ps
Refering-URL: http://www.cs.wisc.edu/~olvi/olvi.html
Root-URL: 
Title: Smoothing Methods for Convex Inequalities and Linear Complementarity Problems  
Author: Chunhui Chen O. L. Mangasarian 
Keyword: Key Words: Smoothing, convex inequalities, linear complementarity. Abbreviated Title: Smoothing Methods in Mathematical Programming  
Date: November 1993/ Revised November 1994  
Abstract: A smooth approximation p(x; ff) to the plus function: maxfx; 0g, is obtained by integrating the sigmoid function 1=(1 + e ffx ), commonly used in neural networks. By means of this approximation, linear and convex inequalities are converted into smooth, convex unconstrained minimization problems, the solution of which approximates the solution of the original problem to a high degree of accuracy for ff sufficiently large. In the special case when a Slater constraint qualification is satisfied, an exact solution can be obtained for finite ff. Speedup over MINOS 5.4 was as high as 1142 times for linear inequalities of size 2000 fi 1000, and 580 times for convex inequalities with 400 variables. Linear complementarity problems are converted into a system of smooth nonlinear equations and are solved by a quadrat-ically convergent Newton method. For monotone LCP's with as many as 10,000 variables, the proposed approach was as much as 63 times faster than Lemke's method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Let f (x) = 2 2 (21) We will show that under the assumption that M is a P 0 matrix, that is a matrix with nonnegative minors <ref> [1] </ref>, then all the stationary points of (21) are solutions of (20). First we will state a simple lemma for P 0 matrices. Lemma 4.2 Suppose M 2 R nfin is a P 0 matrix. <p> Therefore x i (M x) i = x i (d i x i ) = d i x 2 i , which is negative whenever x i 6= 0; i = 1; ; n. This contradicts Theorem 3.4.2 of <ref> [1] </ref>. Theorem 4.1 Consider LCP (M; q) with M 2 P 0 . Let x (ff) be a stationary point of min x2R n f (x), where f (x) is defined by (21). Then x (ff) is a solution of (20). <p> Note that the class of P 0 matrices contains the classes of P matrices, positive semi-definite matrices and row-sufficient matrices <ref> [1] </ref>. For this class of matrices, if f (x) defined by (21) has a stationary point, that point is also a solution of (20). Now we establish the existence of a solution to (20) for P 0 " R 0 matrices. <p> Now we establish the existence of a solution to (20) for P 0 " R 0 matrices. A matrix M is called an R 0 matrix if the only solution to LCP (M; 0) is the zero vector <ref> [1] </ref>. 11 Theorem 4.2 Consider LCP (M; q) with M 2 P 0 " R 0 . The system of nonlinear equations (20) always has a solution. Proof Let f (x) = 1 2 kxp (xM xq; ff)k 2 2 . <p> For monotone linear complementarity problems, we used a Newton method with a safeguarded linear search to solve the nonlinear equation (19). This approach is computationally more efficient 13 than solving the nonlinear equation (20) . We compared the smooth method with Lemke's method <ref> [1] </ref>. All problems were generated randomly. The matrix M was determined by: M = AA T + C, where A is an n fi r random matrix, r is a random number between 1 to n and C is a random skew-symmetric matrix. <p> The SOR method of Delone and Tork Roth [6] does not apply to this class of nonsymmetric LCP nor do other splitting methods described in <ref> [1] </ref>. In fact, the SOR method of [6] failed on all test problems. Figures 9 and 10 show the CPU times for the smooth algorithm and Lemke's method. The smooth algorithm is always better than Lemke's method for both dense as well as the sparse problems.
Reference: [2] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: For the cases m=2n and m=4n, the smooth algorithm is faster than the other two algorithms. For the case m=n, the smooth algorithm is faster than MINOS and comparable with the relaxation method. For convex inequalities, we use the BFGS algorithm <ref> [2] </ref> to solve the unconstrained minimization problem for variables up to 150, and the limited memory BFGS algorithm [16] for lager problems. Starting with ff = 5, we increased ff by a factor of 1.05 to 1.2 at each minor iteration.
Reference: [3] <author> S.P. Dirkse and M.C. Ferris. </author> <title> The path solver: A non-monotone stabilization scheme for mixed complementarity problems. </title> <institution> Computer Sciences Department Technical Report 1179, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1993. </year>
Reference-contexts: For dense problems, we compared the smooth algorithm with Lemke's method which was implemented in FORTRAN. For sparse problems with density between 0.012 and 0.15 percent, we compared the smooth algorithm with a sparse version of lemke's method <ref> [3] </ref>, which employs sparse basis updating techniques. The SOR method of Delone and Tork Roth [6] does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [1]. In fact, the SOR method of [6] failed on all test problems.
Reference: [4] <author> A. J. Hoffman. </author> <title> On approximate solutions of systems of linear inequalities. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 263-265, </pages> <year> 1952. </year>
Reference-contexts: In the following, we will prove that a solution of (5) gives an approximate solution of (2). First we will state an error bound lemma for linear inequalities. Lemma 2.1 Error bound <ref> [4] </ref> [8] Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [5] <author> M. Kojima, S. Mizuno, and A. Yoshise. </author> <title> A polynomial-time algorithm for a class of linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 44 </volume> <pages> 1-27, </pages> <year> 1989. </year>
Reference-contexts: In the following theorem, we assume that all the elements of matrix M and vector q are integers and n 2. Let L be the size of LCP (M; q) defined by <ref> [5] </ref> L = b i=1 j=1 i=n X log (jq i j) + log (n 2 )c + 1: Theorem 4.4 Suppose that LCP (M; q) is solvable. Let x (ff) be a solution of (20) with ff ff = p n2 L . <p> Hence x (ff)(M x (ff) + q) = ff 2 &lt; ff 2 2 2L p n2 L . By the purification procedure described in Appendix B <ref> [5] </ref>, x (ff) can be purified to a solution of LCP (M; q). 12 5 Numerical Results We now give a summary of our computational experience with the smooth algorithms described in this paper. The smooth algorithms were implemented in C.
Reference: [6] <author> R. De Leone and M.A. Tork Roth. </author> <title> Massively parallel solution of quadratic programs via successive overrelaxation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5 </volume> <pages> 623-634, </pages> <year> 1993. </year>
Reference-contexts: For sparse problems with density between 0.012 and 0.15 percent, we compared the smooth algorithm with a sparse version of lemke's method [3], which employs sparse basis updating techniques. The SOR method of Delone and Tork Roth <ref> [6] </ref> does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [1]. In fact, the SOR method of [6] failed on all test problems. Figures 9 and 10 show the CPU times for the smooth algorithm and Lemke's method. <p> The SOR method of Delone and Tork Roth <ref> [6] </ref> does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [1]. In fact, the SOR method of [6] failed on all test problems. Figures 9 and 10 show the CPU times for the smooth algorithm and Lemke's method. The smooth algorithm is always better than Lemke's method for both dense as well as the sparse problems.
Reference: [7] <author> K. Madsen and H.B. Nielsen. </author> <title> A finite smoothing algorithm for linear l 1 estimation. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3(2) </volume> <pages> 223-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The inverse function p 1 is well defined for x 2 (0; 1). 8. p (x; ff) &gt; p (x; fi), for ff &lt; fi, x 2 R. Smoothing techniques have been used for l 1 -minimization problems <ref> [7] </ref> and in multi-commodity flows problem [17] using a linear quadratic smoothing function with encouraging numerical results. We now summarize our results. In Section 2 we treat linear inequalities by converting them to unconstrained differentiable minimization problems.
Reference: [8] <author> O.L. Mangasarian. </author> <title> A condition number for linear inequalities and linear programs. </title> <editor> In G. Bamberg and O. Opitz, editors, </editor> <booktitle> Proceedings of 6. Symposium uber Operations Research, </booktitle> <address> Augsburg, </address> <month> 7-9 September </month> <year> 1981, </year> <pages> pages 3-15, </pages> <address> Konigstein, 1981. Verlagsgruppe Athenaum/Hain/Scriptor/Hanstein. </address>
Reference-contexts: In the following, we will prove that a solution of (5) gives an approximate solution of (2). First we will state an error bound lemma for linear inequalities. Lemma 2.1 Error bound [4] <ref> [8] </ref> Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [9] <author> O.L. Mangasarian. </author> <title> A condition number for differentiable convex inequalities. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 175-179, </pages> <year> 1985. </year>
Reference-contexts: (ff) and x 2 (ff) be solutions of (15) with f = f 1 and f = f 2 respectively. (i) Let X be bounded and let g satisfy Slater constraint qualification: g (^x) &lt; 0 or let g (x) be differentiable and satisfy the Slater and asymptotic constraint qualification <ref> [9] </ref>. <p> Then there exist x 1 (ff) and x 2 (ff), both in X, such that kx 1 (ff) x 1 (ff)k 1 ff and log 2 p where C 1 and C 2 are constants dependent on g (x) <ref> [19, 9] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0, then there exists an ff &gt; 0 such that for any ff ff, x 1 (ff) and x 2 (ff) solve the convex inequalities (12) exactly.
Reference: [10] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: Note that (x) + = R x 1 (y)dy, where (x) is the step function: (x) = 1 if x &gt; 0 In the extensive neural network literature <ref> [10] </ref>, the step function is very effectively approximated by the sigmoid function s (x; ff) = 1 + e ffx ; ff &gt; 0 See Figures 1 and 3.
Reference: [11] <author> O.L. Mangasarian. </author> <title> Error bounds for inconsistent linear inequalities and programs. </title> <journal> Operations Research Letters, </journal> <volume> 15 </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference-contexts: In fact a multiple of value of f (x) bounds the distance of x to the set of minimizers of k (Ax b) + k 1 for the case when f = f 1 , see <ref> [11] </ref>.
Reference: [12] <author> O.L. Mangasarian and J. Ren. </author> <title> New error bounds for the linear complementarity problem. </title> <type> Technical Report 1156, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> June </month> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: The vector q was then defined by: q = w M x. We chose the parameter ff inversely proportional to the 2-norm of the natural residual: k minfx; M x + qgk 2 <ref> [12] </ref> . The algorithm terminates when the infinity-norm of the natural residual is less than 1.0e-6. For dense problems, we compared the smooth algorithm with Lemke's method which was implemented in FORTRAN.
Reference: [13] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference-contexts: The smooth algorithms employ sparsity by evaluating the function and gradients using sparse matrix computation and solving linear equations by a sparse LU decomposition from MINOS. For linear inequalities, we compared the smooth algorithm with MINOS as well as with the relaxation method of Motzkin and Schoenberg <ref> [13] </ref>. The relaxation method was implemented in C. All the algorithms for linear inequalities were run on a Sun SPARCstation 10. We used the truncated Newton algorithm [15] to solve the smooth unconstrained minimization problem.
Reference: [14] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: The smooth algorithms were implemented in C. The CPU times for all the algorithms do not include the time to input data. The time of MINOS 5.4 <ref> [14] </ref> is the execution time for subroutine M5SOLV and also does not include the input time. MINOS is a pivot-based solver, which employs sparsity by updating the basis using sparse linear algebra.
Reference: [15] <author> S.G. Nash. </author> <title> Newton-type minimization via the Lanczos method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 21 </volume> <pages> 770-778, </pages> <year> 1984. </year>
Reference-contexts: For linear inequalities, we compared the smooth algorithm with MINOS as well as with the relaxation method of Motzkin and Schoenberg [13]. The relaxation method was implemented in C. All the algorithms for linear inequalities were run on a Sun SPARCstation 10. We used the truncated Newton algorithm <ref> [15] </ref> to solve the smooth unconstrained minimization problem. We started with ff = 1000:0 and increased it by a factor of 2 at each major iteration. The algorithms terminate when the infeasibilities are less than 1.0e-7. All problems were generated randomly.
Reference: [16] <author> J. Nocedal. </author> <title> Theory of algorithms for unconstrained optimization. </title> <journal> Acta Numerica, </journal> <pages> pages 199-242, </pages> <year> 1992. </year>
Reference-contexts: For the case m=n, the smooth algorithm is faster than MINOS and comparable with the relaxation method. For convex inequalities, we use the BFGS algorithm [2] to solve the unconstrained minimization problem for variables up to 150, and the limited memory BFGS algorithm <ref> [16] </ref> for lager problems. Starting with ff = 5, we increased ff by a factor of 1.05 to 1.2 at each minor iteration. The algorithm terminates when the infeasibilities are less than 1.0e-7. We compared the smooth algorithm with MINOS. Both algorithms were run on a DECstation 3100.
Reference: [17] <author> M. C. Pinar and S. A. Zenios. </author> <title> On smoothing exact penalty functions for convex constrained optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 </volume> <pages> 486-511, </pages> <year> 1994. </year>
Reference-contexts: The inverse function p 1 is well defined for x 2 (0; 1). 8. p (x; ff) &gt; p (x; fi), for ff &lt; fi, x 2 R. Smoothing techniques have been used for l 1 -minimization problems [7] and in multi-commodity flows problem <ref> [17] </ref> using a linear quadratic smoothing function with encouraging numerical results. We now summarize our results. In Section 2 we treat linear inequalities by converting them to unconstrained differentiable minimization problems.
Reference: [18] <author> J. Ren. </author> <title> Computable error bounds in mathematical programming. </title> <type> Technical Report 1173, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: By Lemma 5.2.1 of <ref> [18] </ref>, there exists a x 1 (ff) 2 X 1 such that kx 1 (ff) x 1 (ff)k 1 1 (A; b)(k (Ax 1 (ff) b) + k 1 k (Ax 1 (ff) b) + k 1 ) + 1 (A; b) ff Similarly, x 2 2 X 2 is equivalent <p> By Lemma 5.3.2 of <ref> [18] </ref> and some tedious computation, we get the desired conclusion. Remark 2.1 Suppose that the solution set of (2) is nonempty and bounded, then the level sets of f (x) are compact and f (x) is strongly convex on its level sets. <p> Let x (ff) be a solution of (20). Then there exists an x (x (ff)) which is a solution of LCP (M; q) such that kx (ff) x (x (ff))k 2 t (M; q) n log 2 ; where t (M; q) is a constant, see Theorem 2.2.1 <ref> [18] </ref>. The following theorem proves that if ff is sufficiently large, then a solution of (20) can be purified to a solution of LCP (M; q). In the following theorem, we assume that all the elements of matrix M and vector q are integers and n 2.
Reference: [19] <author> S.M. Robinson. </author> <title> Application of error bounds for convex programming in a linear space. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 13 </volume> <pages> 271-273, </pages> <year> 1975. </year>
Reference-contexts: Then there exist x 1 (ff) and x 2 (ff), both in X, such that kx 1 (ff) x 1 (ff)k 1 ff and log 2 p where C 1 and C 2 are constants dependent on g (x) <ref> [19, 9] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0, then there exists an ff &gt; 0 such that for any ff ff, x 1 (ff) and x 2 (ff) solve the convex inequalities (12) exactly.
Reference: [20] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year> <month> 19 </month>
Reference-contexts: Let rc (g) denote the recession cone of a proper convex function g, that is rc (g) = fyj sup x2dom g (g (x + y) g (x)) 0g, where dom g is the domain of g <ref> [20] </ref>. Now we will state a condition under which (15) has a solution. Theorem 3.1 Let g : R n ! R m be continuous and convex and let f (x) be defined as in (13) or (14). The following are equivalent: 1.
References-found: 20

