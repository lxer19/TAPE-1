URL: ftp://ftp.cs.washington.edu/tr/1993/11/UW-CSE-93-11-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Processor Allocation Policies for Message-Passing Parallel Computers  
Abstract: Cathy McCann and John Zahorjan Department of Computer Science and Engineering University of Washington Technical Report 93-11-01 Revised 2-28-94 To Appear in Proceedings of the ACM Sigmetrics Conference, May 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Ashok. Adhara: </author> <title> A Run-Time Support System for Space-Based Applications. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <note> In Preparation. </note>
Reference-contexts: We also assume that the application does not alter the number of threads in response to changes in processor allocation. Considerable effort is required to implement dynamic remapping, and determining when to remap can be complicated [11]. This effort may be justified only for very irregular and dynamic computations <ref> [1] </ref>. Our hardware and software models reflect an important class of system and applications, and capture the most central aspects of the processor allocation problem for other classes. However, we have not attempted to include all aspects of all parallel systems.
Reference: [2] <author> M.-S. Chen and K.G. Shin. </author> <title> Processor allocation in an N-cube multiprocessor using gray codes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1396-1407, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Pool-based scheduling may timeshare partitions. Setia et al. [13] examine the benefits of time-sharing and dynamic partitioning. They conclude that timesharing can be effective in reducting mean response time. Chen and Shin <ref> [2] </ref> examine processor allocation for cube-connected message passing machines. Like Feit-elson and Rudolph, they assume that arriving jobs declare the number of processors required for execution. The goal of their work is to describe efficient schemes for finding sub-cubes to satisfy arriving jobs.
Reference: [3] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientifc applications with explicit communication. </title> <booktitle> In Proceedings 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We use the following notation. The values in parentheses indicate the baseline setting for the experiments discussed throughout the remainder of the paper. These values are based, in part, on the workload characteristics described in <ref> [3] </ref>, and on the characteristics of the Intel Paragon hardware. * 2 M x2 N , the size of the mesh of processors (2 4 x2 4 ). * J , the (static) number of jobs in the system (vari able). * t, the average per-thread compute time of each ap
Reference: [4] <author> K. Dussa, B. Carlson, L. Dowdy, and K-H. Park. </author> <title> Dynamic partitioning in a transputer environment. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 203-213, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Chen and Shin [2] examine processor allocation for cube-connected message passing machines. Like Feit-elson and Rudolph, they assume that arriving jobs declare the number of processors required for execution. The goal of their work is to describe efficient schemes for finding sub-cubes to satisfy arriving jobs. Dussa et al. <ref> [4] </ref> examine the benefits of dynamically repartitioning processors on job arrival and departure, using experiments on a ring connected Transputer-based system, as well as a simple analytic model.
Reference: [5] <author> D.G. Feitelson. </author> <title> In Support of Gang Scheduling. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, The Hebrew University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: This effect forces processor allocators to operate on a per-job granularity, rather than a per-thread one. Feitelson and Rudolph <ref> [5, 6] </ref> build upon these observations, describing variants of gang scheduling. They propose a hierarchical scheme for assigning applications to processors, and examine the fragmentation it experiences when used in conjunction with gang scheduling for jobs whose sizes differ according to a number of distributions.
Reference: [6] <author> D.G. Feitelson and L. Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This effect forces processor allocators to operate on a per-job granularity, rather than a per-thread one. Feitelson and Rudolph <ref> [5, 6] </ref> build upon these observations, describing variants of gang scheduling. They propose a hierarchical scheme for assigning applications to processors, and examine the fragmentation it experiences when used in conjunction with gang scheduling for jobs whose sizes differ according to a number of distributions.
Reference: [7] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Mc-Cann et al. [10] further refine this policy, and report on results from a prototype implementation of it. Gupta et al. <ref> [7] </ref> and Vaswani and Zahorjan [19] examine the importance of cache affinity to the decisions made by the kernel processor allocator.
Reference: [8] <author> S. Leutenegger and M. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Their results agree in showing that cache affinity is not exploitable by the kernel, except in very special circumstances. (Squil-lante and Lazowska come to somewhat contradictory 3 conclusions, based on results from an analytic model [16].) Majumdar et al [9], Sevcik [14, 15], and Leutenegger and Vernon <ref> [8] </ref> examine the relationship between job characteristics, such as maximum parallelism and total service time, and the performance of various scheduling disciplines. <p> These policies assume that no a priori information is available on job characteristics, such as duration. Experience with uniprocessor systems, as well as prior work on parallel machine scheduling <ref> [8] </ref>, indicates that providing roughly equal allocation of resources to the jobs is appropriate in these circumstances. Our allocation policies apply to sets of jobs that fit simultaneously in memory and the 2 M+N processors of the mesh.
Reference: [9] <author> S. Majumdar, D.L. Eager, and R. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Their results agree in showing that cache affinity is not exploitable by the kernel, except in very special circumstances. (Squil-lante and Lazowska come to somewhat contradictory 3 conclusions, based on results from an analytic model [16].) Majumdar et al <ref> [9] </ref>, Sevcik [14, 15], and Leutenegger and Vernon [8] examine the relationship between job characteristics, such as maximum parallelism and total service time, and the performance of various scheduling disciplines.
Reference: [10] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed, shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They also describe a more aggressive form of co-scheduling than is used in process control, as well as a more aggressive approach to reallocating processors in response to transient changes in job parallelism. Mc-Cann et al. <ref> [10] </ref> further refine this policy, and report on results from a prototype implementation of it. Gupta et al. [7] and Vaswani and Zahorjan [19] examine the importance of cache affinity to the decisions made by the kernel processor allocator.
Reference: [11] <author> D.M. Nicol and J.C. Townsend. </author> <title> Accurate modeling of parallel scientific computations. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 165-170, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: After that, it is unable to alter the load distribution. We also assume that the application does not alter the number of threads in response to changes in processor allocation. Considerable effort is required to implement dynamic remapping, and determining when to remap can be complicated <ref> [11] </ref>. This effort may be justified only for very irregular and dynamic computations [1]. Our hardware and software models reflect an important class of system and applications, and capture the most central aspects of the processor allocation problem for other classes.
Reference: [12] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: An early and fundamental result on processor scheduling for parallel machines is due to Ousterhout <ref> [12] </ref>. He noted that because the threads of a parallel application synchronize frequently, the rate of progress of the application will be determined by the scheduling quantum unless all threads of the processor are "co-scheduled", that is, run at once.
Reference: [13] <author> S. Setia, M.S. Squillante, , and S. Tripathi. </author> <title> Processor scheduling on multiprogrammed, </title> <booktitle> distributed memory parallel systems. In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 158-170, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Additionally, in pool-based scheduling the kernel can effectively restrict a job's choice of parallelism by restricting its threads to only one or a few pools. Pool-based scheduling may timeshare partitions. Setia et al. <ref> [13] </ref> examine the benefits of time-sharing and dynamic partitioning. They conclude that timesharing can be effective in reducting mean response time. Chen and Shin [2] examine processor allocation for cube-connected message passing machines. Like Feit-elson and Rudolph, they assume that arriving jobs declare the number of processors required for execution.
Reference: [14] <author> K.C. Sevcik. </author> <title> Characterization of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Their results agree in showing that cache affinity is not exploitable by the kernel, except in very special circumstances. (Squil-lante and Lazowska come to somewhat contradictory 3 conclusions, based on results from an analytic model [16].) Majumdar et al [9], Sevcik <ref> [14, 15] </ref>, and Leutenegger and Vernon [8] examine the relationship between job characteristics, such as maximum parallelism and total service time, and the performance of various scheduling disciplines.
Reference: [15] <author> K.C. Sevcik. </author> <title> Application scheduling and processor allocation in multiprogrammed parallel processing systems. Performance Evaluation, </title> <note> To appear. </note>
Reference-contexts: Their results agree in showing that cache affinity is not exploitable by the kernel, except in very special circumstances. (Squil-lante and Lazowska come to somewhat contradictory 3 conclusions, based on results from an analytic model [16].) Majumdar et al [9], Sevcik <ref> [14, 15] </ref>, and Leutenegger and Vernon [8] examine the relationship between job characteristics, such as maximum parallelism and total service time, and the performance of various scheduling disciplines.
Reference: [16] <author> M.S. Squillante and E.D. Lazowska. </author> <title> Using processor-cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Their results agree in showing that cache affinity is not exploitable by the kernel, except in very special circumstances. (Squil-lante and Lazowska come to somewhat contradictory 3 conclusions, based on results from an analytic model <ref> [16] </ref>.) Majumdar et al [9], Sevcik [14, 15], and Leutenegger and Vernon [8] examine the relationship between job characteristics, such as maximum parallelism and total service time, and the performance of various scheduling disciplines.
Reference: [17] <author> C.A. Thekkath and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Existing systems employ this static form of partitioning because dynamic reassignment of processors among jobs has been thought to be too expensive, due to the communication costs associated with relocating code and data. However, hardware advances continue to increase the bandwidth of interconnection networks, and recent software advances <ref> [20, 17] </ref> show how to reduce the latency currently imposed by large message startup costs to a negligible level. <p> I, the inter-rotation time for the Folding policy (variable). * L, the limit on the number of threads per node, or equivalently, the number of jobs in the system (2 M+N ) While current production systems impose high message start-up costs that can have a significant performance impact, recent work, <ref> [20, 17] </ref> has shown that these costs can be almost entirely avoided.
Reference: [18] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Shared-Memory Systems. Much of the recent work on processor allocation policies has considered shared memory machines. Tucker and Gupta <ref> [18] </ref> describe "process-control", which is fundamentally a space-sharing approach. Under process control, an arriving job creates a thread per processor of the machine, but based on feedback from the kernel, idles threads in excess of its current processor allocation.
Reference: [19] <author> R. Vaswani and J. Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multipro-grammed, shared memory multiprocessors. </title> <booktitle> In Proceedings 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Mc-Cann et al. [10] further refine this policy, and report on results from a prototype implementation of it. Gupta et al. [7] and Vaswani and Zahorjan <ref> [19] </ref> examine the importance of cache affinity to the decisions made by the kernel processor allocator.
Reference: [20] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Existing systems employ this static form of partitioning because dynamic reassignment of processors among jobs has been thought to be too expensive, due to the communication costs associated with relocating code and data. However, hardware advances continue to increase the bandwidth of interconnection networks, and recent software advances <ref> [20, 17] </ref> show how to reduce the latency currently imposed by large message startup costs to a negligible level. <p> I, the inter-rotation time for the Folding policy (variable). * L, the limit on the number of threads per node, or equivalently, the number of jobs in the system (2 M+N ) While current production systems impose high message start-up costs that can have a significant performance impact, recent work, <ref> [20, 17] </ref> has shown that these costs can be almost entirely avoided.
Reference: [21] <author> J. Zahorjan and C. McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 214-225, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This allows a natural form of co-scheduling, which would not be possible if the number of active threads exceeded the processor allocation, even with space sharing. Processors are reallocated among jobs only on job arrival and departure. Zahorjan and McCann <ref> [21] </ref> compare time sharing to space sharing, and conclude that space sharing is preferable. They also describe a more aggressive form of co-scheduling than is used in process control, as well as a more aggressive approach to reallocating processors in response to transient changes in job parallelism.
Reference: [22] <author> S. Zhou and T. Brecht. </author> <title> Processor-pool-based scheduling for large-scale NUMA multiprocessors. </title> <booktitle> In Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 133-142, </pages> <month> May </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Because many jobs may be allocated to each processor, the processors must be time-shared among them. Zhou and Brecht <ref> [22] </ref> describe a pool-based scheduling mechanism. Pools are a logical construct, used by 1 Throughout this paper we will use "thread" to mean "kernel thread".
References-found: 22

