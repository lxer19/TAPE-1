URL: http://www.cc.gatech.edu/grads/k/Prince.Kohli/sigops_cm.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Mustaque.Ahamad/pubs.html
Root-URL: 
Title: Causal Memory Meets the Consistency and Performance Needs of Distributed Applications!  
Author: Mustaque Ahamad Ranjit John Prince Kohli Gil Neiger 
Address: Atlanta, GA U.S.A  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: In order to provide acceptable performance in large scale distributed systems, shared data must be cached at or close to nodes where it is accessed. Maintaining the consistency of such cached data is an important problem in distributed systems. We claim that causal memory, which defines consistency of shared data based on causal orderings between data accesses, provides strong enough consistency guarantees to be usable yet it allows efficient and scalable implementations. In this paper, we describe some results of our recent work that support this claim. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering|a new definition. </title> <booktitle> In Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <year> 1990. </year>
Reference-contexts: There are two main classes of such applications. First, a large class of applications employ explicit synchronization operations to ensure that a sequence of memory operations are executed atomically. Such applications are data-race-free <ref> [1] </ref>, and we have proved that, for these applications, code that executes correctly with sequentially consistent memory will also execute correctly with causal memory [2]. The other application domain is the general area of computer supported cooperative work (CSCW) where shared data is accessed by asynchronously interacting users.
Reference: [2] <author> Mustaque Ahamad, Gil Neiger, Jim Burns, Prince Kohli and Phillip Hutto. </author> <title> Causal Memory: Definitions, Implementation and Programming. </title> <type> Georgia Tech Technical Report 93/54. </type> <year> 1993. </year>
Reference-contexts: First, a large class of applications employ explicit synchronization operations to ensure that a sequence of memory operations are executed atomically. Such applications are data-race-free [1], and we have proved that, for these applications, code that executes correctly with sequentially consistent memory will also execute correctly with causal memory <ref> [2] </ref>. The other application domain is the general area of computer supported cooperative work (CSCW) where shared data is accessed by asynchronously interacting users. In these applications, processes executing at different nodes share a set of objects.
Reference: [3] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the Eleventh International Conference on Distributed Computing, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference: [4] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: A shared memory abstraction can be provided in distributed systems in software, but due to large latencies and high communication costs, efficient implementations cannot be developed because of the strong consistency guarantees of traditional shared memories. Recently several memory models have been developed both for multiprocessor and distributed systems <ref> [7, 4, 9] </ref> that weaken consistency guarantees provided by the memory system as a way to improve performance. We have explored a weakly consistent memory, called causal memory, which ensures that values returned by read operations are consistent with the causal orderings [5, 11] between data accesses. <p> These annotations are used to determine the best coherence protocol for the set of data items. LRC is implemented by the TreadMarks system [10] and Midway implements EC <ref> [4] </ref>. In addition to the sequentially consistent memory system, we have also compared causal memory with a memory system derived from the RC approach [8]. We found that causal memory outperforms RC because of two main reasons. <p> Its consistency guarantees are strong enough for building many practical distributed applications and, at the same time, it allows 4 (a) EP (b) MM (e) TSP (f) SOR 5 more efficient and scalable implementations. Furthermore, it is similar to lazy release consistency [9] and entry consistency <ref> [4] </ref> in that it can exploit information about synchronization operations in performing consistency maintenance. Additionally, it allows cooperative applications to be programmed where changes to shared state are propagated asynchronously.
Reference: [5] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: We have explored a weakly consistent memory, called causal memory, which ensures that values returned by read operations are consistent with the causal orderings <ref> [5, 11] </ref> between data accesses.
Reference: [6] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: We have compared the performance of the applications with sequentially consistent memory and message passing systems. A natural question is how causal memory compares with systems based on release consistency (RC), lazy release consistency (LRC) or entry consistency (EC). RC is implemented in the Munin system <ref> [6] </ref> but it also requires that users provide annotations that specify the sharing patterns for a set of data items. These annotations are used to determine the best coherence protocol for the set of data items. LRC is implemented by the TreadMarks system [10] and Midway implements EC [4].
Reference: [7] <author> Kourosh Gharchorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hen-nessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: A shared memory abstraction can be provided in distributed systems in software, but due to large latencies and high communication costs, efficient implementations cannot be developed because of the strong consistency guarantees of traditional shared memories. Recently several memory models have been developed both for multiprocessor and distributed systems <ref> [7, 4, 9] </ref> that weaken consistency guarantees provided by the memory system as a way to improve performance. We have explored a weakly consistent memory, called causal memory, which ensures that values returned by read operations are consistent with the causal orderings [5, 11] between data accesses.
Reference: [8] <author> Ranjit John, Mustaque Ahamad, Umakishore Ramachandran, Ajay Mohindra and R. Anantha-narayanan. </author> <title> An evaluation of state sharing techniques in distributed operating systems. </title> <type> Technical Report GIT-CC-93-73, </type> <institution> College of Computing, Georgia Institute of Technology. </institution>
Reference-contexts: LRC is implemented by the TreadMarks system [10] and Midway implements EC [4]. In addition to the sequentially consistent memory system, we have also compared causal memory with a memory system derived from the RC approach <ref> [8] </ref>. We found that causal memory outperforms RC because of two main reasons. First, although RC allows the system to asynchronously send coherence related messages, it still requires that all nodes that have copies of a data item be notified when the data item is written.
Reference: [9] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium of Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: A shared memory abstraction can be provided in distributed systems in software, but due to large latencies and high communication costs, efficient implementations cannot be developed because of the strong consistency guarantees of traditional shared memories. Recently several memory models have been developed both for multiprocessor and distributed systems <ref> [7, 4, 9] </ref> that weaken consistency guarantees provided by the memory system as a way to improve performance. We have explored a weakly consistent memory, called causal memory, which ensures that values returned by read operations are consistent with the causal orderings [5, 11] between data accesses. <p> Its consistency guarantees are strong enough for building many practical distributed applications and, at the same time, it allows 4 (a) EP (b) MM (e) TSP (f) SOR 5 more efficient and scalable implementations. Furthermore, it is similar to lazy release consistency <ref> [9] </ref> and entry consistency [4] in that it can exploit information about synchronization operations in performing consistency maintenance. Additionally, it allows cooperative applications to be programmed where changes to shared state are propagated asynchronously.
Reference: [10] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: These annotations are used to determine the best coherence protocol for the set of data items. LRC is implemented by the TreadMarks system <ref> [10] </ref> and Midway implements EC [4]. In addition to the sequentially consistent memory system, we have also compared causal memory with a memory system derived from the RC approach [8]. We found that causal memory outperforms RC because of two main reasons.
Reference: [11] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: We have explored a weakly consistent memory, called causal memory, which ensures that values returned by read operations are consistent with the causal orderings <ref> [5, 11] </ref> between data accesses.
Reference: [12] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4):321 359, </volume> <month> November </month> <year> 1989. </year> <month> 6 </month>
Reference-contexts: In all these applications, it can be seen that the completion times for causal memory are lower than sequentially consistent memory. In particular, causal memory provides 5-50% reduction in completion times compared to sequentially consistent memory implemented using a variant of the invalidation based protocol used in Ivy <ref> [12] </ref>. We also measured various components of the completion time of an application which include actual user computation time, communication time, and synchronization time. Communication time is what a process spends waiting for a page fault to be serviced or to receive a message.
References-found: 12

