URL: http://www.cs.ucsd.edu/users/mihir/papers/composition.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mihir/papers/complexity-papers.html
Root-URL: http://www.cs.ucsd.edu
Email: E-mail: russell@cs.ucsd.edu.  E-mail: naor@wisdom.weizmann.ac.il.  
Title: Parallel Repetition Lower the Error in Computationally Sound Protocols? The question of parallel error reduction
Author: Mihir Bellare Russell Impagliazzo Moni Naor succeeds. 
Address: San Diego, 9500 Gilman Drive, La Jolla, CA 92093, USA.  Rehovot 76100, Israel.  
Affiliation: Department of Computer Science Engineering, Mail Code 0114, University of California at  Dept. of Applied Mathematics and Computer Science, Weizmann Institute of Science,  
Date: August 17, 1997  
Note: Does  Incumbent of the Morris and Rose Goldman Career Development Chair,  Supported in part by BSF Grant 32-00032-1.  
Abstract: An extended abstract of this paper appears in Proceedings of the 38th Symposium on Foundations Abstract Whether or not parallel repetition lowers the error has been a fundamental question in the theory of protocols, with applications in many different areas. It is well known that parallel repetition reduces the error at an exponential rate in interactive proofs and Arthur-Merlin games. It seems to have been taken for granted that the same is true in arguments, or other proofs where the soundness only holds with respect to computationally bounded parties. We show that this is not the case. Surprisingly, parallel repetition can actually fail in this setting. We present four-round protocols whose error does not decrease under parallel repetition. This holds for any (polynomial) number of repetitions. These protocols exploit non-malleable encryption and can be based on any trapdoor permutation. On the other hand we show that for three-round protocols the error does go down exponentially fast. fl Department of Computer Science & Engineering, Mail Code 0114, University of California at San Diego, 9500 Gilman Drive, La Jolla, CA 92093, USA. E-mail: mihir@cs.ucsd.edu. Supported in part by NSF CAREER Award CCR-9624439 and a 1996 Packard Foundation Fellowship in Science and Engineering. of Computer Science, IEEE, 1997. This is the full version.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arora and C. </author> <title> Lund Hardness of Approximations. In Approximation algorithms for NP-hard problems, edited by Dorit Hochbaum, </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, </address> <year> 1997. </year>
Reference-contexts: They have turned out to be essential for cryptography but somewhat more surprisingly, they have also been key to complexity theory, in particular to the theory of hardness of approximation problems (see <ref> [1] </ref> for a survey). For many of these applications, the purpose of the protocol is for a "verifier" to distinguish between a "good" prover making a legitimate claim and a "bad" prover attempting to trick the verifier into accepting incorrectly. <p> Namely, to define B, we need just one strategy which can call A as a subroutine. Definition 2.2 Let V be a verifier strategy over a domain fl and input distribution I. Suppose *; ffi : fl ! <ref> [0; 1] </ref>. <p> Let k = k (n) be any polynomial. Let ffi &gt; 1=3 be a constant and let *: fl ! <ref> [0; 1] </ref> be arbitrary. Then there in no (k; *; ffi)-black-box error-reduction procedure for LC. (Actually it is enough that ffi () 1=3 + t () where t is a non-negligible function.) Note the communication complexity of LC does not depend on k, unlike DD k .
Reference: [2] <author> L. Babai. </author> <title> Trading Group Theory for Randomness. </title> <booktitle> Proceedings of the 17th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1985. </year>
Reference: [3] <author> L. Babai and S. Moran. </author> <title> Arthur-Merlin Games: A Randomized Proof System, and a Hierarchy of Complexity Classes. </title> <journal> J. Computer and System Sciences Vol. </journal> <volume> 36, </volume> <pages> 254-276, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Various notions of interactive protocols <ref> [23, 3, 6, 8] </ref> have found wide applicability over the last decade. They have turned out to be essential for cryptography but somewhat more surprisingly, they have also been key to complexity theory, in particular to the theory of hardness of approximation problems (see [1] for a survey). <p> However, this is an expensive solution, in that it increases the number of communication rounds of the protocol, which is undesirable for both practical and theoretical applications. Parallel repetition was shown to reduce the error probability of Arthur-Merlin games at an exponential rate <ref> [3] </ref>. (That is, k parallel repetitions of a protocol with error * results in a protocol with error * k for k poly (n).) It can be shown that the same is true for interactive proofs, although a formal proof does not seem to have appeared.
Reference: [4] <author> M. Bellare and O. Goldreich. </author> <title> On Defining Proofs of Knowledge. </title> <booktitle> Advances in Cryptology - Crypto 92 Proceedings, Lecture Notes in Computer Science Vol. </booktitle> <volume> 740, </volume> <editor> E. Brickell ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Our results are about soundness. What about proofs of knowledge <ref> [4] </ref>? Bellare, Halevi and Naor are investigating this question. By using the protocols here they have similar negative results for proofs of knowledge. 6 Acknowledgments We thank Oded Goldreich for helpful comments on an earlier version of this paper.
Reference: [5] <author> M. Bellare, M. Jakobsson and M. Yung. </author> <title> Round-optimal zero-knowledge arguments based on any one-way function. </title> <booktitle> Advances in Cryptology - Eurocrypt 97 Proceedings, Lecture Notes in Computer Science Vol. </booktitle> <volume> 1233, </volume> <editor> W. Fumy ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <month> 13 </month>
Reference-contexts: Our results say that a claim that these protocols have low error, if true, cannot rely on a general theorem but must be justified by proofs specific to the protocol at hand. For some constant round protocols, rigorous proofs of this sort have been provided <ref> [15, 5] </ref>. (Note the constructions there are not exactly parallel repetition.) More often, however, either no argument, or sketchy arguments which seem implicitly to assume parallel repetition works in general, are provided. The example protocols that establish our negative results have four rounds of interaction.
Reference: [6] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the 20th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: 1 Introduction Various notions of interactive protocols <ref> [23, 3, 6, 8] </ref> have found wide applicability over the last decade. They have turned out to be essential for cryptography but somewhat more surprisingly, they have also been key to complexity theory, in particular to the theory of hardness of approximation problems (see [1] for a survey).
Reference: [7] <author> M. Blum. </author> <title> Coin Flipping over the Telephone. </title> <booktitle> IEEE COMPCON 1982. </booktitle>
Reference: [8] <author> G. Brassard and C. Cr epeau. </author> <title> Non-transitive Transfer of Confidence: A perfect Zero-knowledge Interactive protocol for SAT and Beyond. </title> <booktitle> Proceedings of the 27th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1986. </year>
Reference-contexts: 1 Introduction Various notions of interactive protocols <ref> [23, 3, 6, 8] </ref> have found wide applicability over the last decade. They have turned out to be essential for cryptography but somewhat more surprisingly, they have also been key to complexity theory, in particular to the theory of hardness of approximation problems (see [1] for a survey). <p> However, the error can be reduced at an exponential rate depending on the communication complexity of the given protocol [29], and this is the best possible [16]. Soon after the appearance of interactive proofs, the notion of arguments (also called computa-tionally convincing protocols) was put forth by <ref> [8, 9] </ref>. <p> Remarks. As indicated above, this is a very general setup in that we allow a context and input distribution. The "arguments" model of <ref> [9, 8] </ref> is typically presented in terms of language recognition. That's a special case of our setup.
Reference: [9] <author> G. Brassard, D. Chaum and C. Cr epeau. </author> <title> Minimum Disclosure Proofs of Knowledge. </title> <journal> J. Computer and System Sciences, </journal> <volume> Vol. 37, </volume> <year> 1988, </year> <pages> pp. 156-189. </pages>
Reference-contexts: However, the error can be reduced at an exponential rate depending on the communication complexity of the given protocol [29], and this is the best possible [16]. Soon after the appearance of interactive proofs, the notion of arguments (also called computa-tionally convincing protocols) was put forth by <ref> [8, 9] </ref>. <p> Remarks. As indicated above, this is a very general setup in that we allow a context and input distribution. The "arguments" model of <ref> [9, 8] </ref> is typically presented in terms of language recognition. That's a special case of our setup.
Reference: [10] <author> G. Brassard, C. Cr epeau and M. Yung. </author> <title> Constant round perfect zero knowledge computationally convincing protocols. </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 84, No. 1, </volume> <year> 1991. </year>
Reference: [11] <author> D. Dolev, C. Dwork and M. Naor. </author> <title> Non-malleable cryptography. </title> <booktitle> Proceedings of the 23rd Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991. </year> <note> Full version available from authors. </note>
Reference-contexts: We present a protocol for which there is no "black-box" error-reduction theorem, meaning that standard techniques will be unable to show any reduction in error for even an arbitrarily large polynomial number of repetitions. These results exploit the notion and construction of non-malleable encryption schemes of <ref> [11] </ref>. We stress that this is independent of any zero-knowledge (ZK) concerns. As we indicated above, it is well known that zero-knowledge is not preserved under parallel repetition [19]. What we are saying is that even the error does not in general go down. These results are somewhat surprising. <p> In fact it is stronger. 3 Parallel repetition fails in general In this section we provide our negative results. Proofs of all claims here can be found in Appendix A. 3.1 Non-malleable encryption Our constructions exploit non-malleable encryption schemes as defined and constructed in <ref> [11] </ref>. Let (G; E; D) specify a public key encryption scheme. The key generator algorithm G takes input 1 n and produces a pair (pk; sk) of matching public and secret keys. <p> This is not guaranteed by semantic security (and in fact for many semantically secure cryptosystems it is easy given an encryption of a bit, to create an encryption of the complement bit). But it is guaranteed by non-malleability. We do not provide a formal definition here (see <ref> [11] </ref>). Our first protocol requires only "complement security," meaning it is hard, given an encryption C of a bit b, to come up with an encryption C 0 of 1 b. <p> Our third protocol actually uses non-malleability in its strongest form as per <ref> [11] </ref>. It is shown in [11] that non-malleable (and hence complement and copy secure) encryption schemes with unique decryptability exist given the existence of trapdoor permutations. <p> Our third protocol actually uses non-malleability in its strongest form as per <ref> [11] </ref>. It is shown in [11] that non-malleable (and hence complement and copy secure) encryption schemes with unique decryptability exist given the existence of trapdoor permutations. <p> Oracle O pk 1 ;pk 2 ((C 1 ; : : : ; C m )) (1) If any of C 1 ; : : : ; C m is invalid (meaning not the encryption of any bit under pk 1 ) then reject. (The ciphertexts in the cryptosystem of <ref> [11] </ref> are self-validating, so this step does not reveal any extra information to the caller of the oracle.) If not, we know there are bits x 1 ; : : : ; x m and strings t 1 ; : : : ; t m such that C i = E
Reference: [12] <author> U. Feige. </author> <title> On the success probability of two provers in one round proof systems. </title> <booktitle> Proceedings of the 6th Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: In multi-prover proof systems, whether or not parallel repetition reduces the error has been the subject of much research (see [13] for a survey). There are examples of protocols for which two parallel repetitions fail to reduce the error at all <ref> [12] </ref>, so a result as strong as for the single prover model does not hold. However, the error can be reduced at an exponential rate depending on the communication complexity of the given protocol [29], and this is the best possible [16].
Reference: [13] <author> U. Feige. </author> <title> Error reduction by parallel repetition the state of the art, </title> <type> Technical Report CS95-32, </type> <institution> Weiz-mann Institute. </institution>
Reference-contexts: Beyond that, parallel repetition is more problematic. In single prover proofs, Goldreich and Krawczyk [19] showed that parallel repetition does not preserve zero-knowledge. In multi-prover proof systems, whether or not parallel repetition reduces the error has been the subject of much research (see <ref> [13] </ref> for a survey). There are examples of protocols for which two parallel repetitions fail to reduce the error at all [12], so a result as strong as for the single prover model does not hold.
Reference: [14] <author> U. Feige, A. Fiat, and A. Shamir. </author> <title> Zero-Knowledge Proofs of Identity. </title> <journal> Journal of Cryptology, </journal> <volume> Vol. 1, </volume> <year> 1988, </year> <pages> pp. 77-94. </pages>
Reference-contexts: One natural task where this occurs is identification <ref> [14] </ref>. Suppose that one can show a protocol where an unauthorized player has probability ff of making the verifier accept (whereas an authorized player may know a strategy that is perfect).
Reference: [15] <author> U. Feige and A. Shamir. </author> <title> Zero-knowledge proofs of knowledge in two rounds. </title> <booktitle> Advances in Cryptology - Crypto 89 Proceedings, Lecture Notes in Computer Science Vol. </booktitle> <volume> 435, </volume> <editor> G. Brassard ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Our results say that a claim that these protocols have low error, if true, cannot rely on a general theorem but must be justified by proofs specific to the protocol at hand. For some constant round protocols, rigorous proofs of this sort have been provided <ref> [15, 5] </ref>. (Note the constructions there are not exactly parallel repetition.) More often, however, either no argument, or sketchy arguments which seem implicitly to assume parallel repetition works in general, are provided. The example protocols that establish our negative results have four rounds of interaction.
Reference: [16] <author> U. Feige and O. Verbitsky. </author> <title> Error reduction by parallel repetition- A negative result. </title> <booktitle> Proceedings of the 11th Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: However, the error can be reduced at an exponential rate depending on the communication complexity of the given protocol [29], and this is the best possible <ref> [16] </ref>. Soon after the appearance of interactive proofs, the notion of arguments (also called computa-tionally convincing protocols) was put forth by [8, 9]. <p> protocol consisting of k parallel repetitions of DD k , there is a polynomial time strategy for the prover to make the verifier accept with probability at least 1=2. 3.4 Failure of parallel error reduction with low communication In light of the results of Raz [29] and Feige and Verbitsky <ref> [16] </ref> a reasonable conjecture at this point is that the failure of error-reduction in protocol DD k is due to the fact that the communication complexity is proportional to k (the number times we are going to execute the protocol in parallel).
Reference: [17] <author> O. Goldreich. </author> <title> Foundations of cryptography: Fragments of a book. </title> <institution> Weizmann Institute of Science, </institution> <month> February </month> <year> 1995. </year>
Reference: [18] <author> O. Goldreich, S. Goldwasser and S. Micali. </author> <title> How to construct random functions. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 33, No. 4, </volume> <year> 1986, </year> <pages> pp. 210-217. </pages>
Reference-contexts: Since O is just a tool in proving Claim 3.6 this doesn't matter much, but in any case we note that 11 this state can be eliminated by specifying s as F K ((C 1 ; : : : ; C m )) where F is a pseudorandom function family <ref> [18] </ref> and the key K is chosen at random and made a part of the description of O. We first claim that given access to this oracle, it is possible to make the verifier V k of LC k accept 1=3 of the time.
Reference: [19] <author> O. Goldreich and H. Krawczyk. </author> <title> On the Composition of Zero Knowledge Proof Systems. </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 25, No. 1, </volume> <pages> pp. 169-192, </pages> <year> 1996. </year>
Reference-contexts: Beyond that, parallel repetition is more problematic. In single prover proofs, Goldreich and Krawczyk <ref> [19] </ref> showed that parallel repetition does not preserve zero-knowledge. In multi-prover proof systems, whether or not parallel repetition reduces the error has been the subject of much research (see [13] for a survey). <p> These results exploit the notion and construction of non-malleable encryption schemes of [11]. We stress that this is independent of any zero-knowledge (ZK) concerns. As we indicated above, it is well known that zero-knowledge is not preserved under parallel repetition <ref> [19] </ref>. What we are saying is that even the error does not in general go down. These results are somewhat surprising.
Reference: [20] <author> O. Goldreich, N. Nisan and A. Wigderson. </author> <title> On Yao's XOR lemma. Electronic Colloquim on Computational Complexity, </title> <address> TR95-050, </address> <year> 1995. </year>
Reference-contexts: A direct product conjecture asks whether, for some model, and a suitably hard computational problem for that model, several independent instances of the problem are harder than a single instance. A classical example of such a result is Yao's XOR lemma <ref> [31, 26, 20, 24, 25] </ref> which states that, if any feasible computation in a non-uniform model has a constant chance of failure at predicting a Boolean function, then the probability that a feasible computation could compute the of several strings becomes negligible.
Reference: [21] <author> O. Goldreich and Y. Oren. </author> <title> Definitions and properties of zero-knowledge proof systems. </title> <journal> Journal of Cryptology, </journal> <volume> Vol. 7, No. 1, </volume> <year> 1994, </year> <pages> pp. 1-32. </pages>
Reference-contexts: Sequential repetition, repeating the protocol several times, beginning the next run after the previous one terminates, reduces error in all important models. It also preserves desirable properties of the original protocol, such as zero-knowledge (see <ref> [21, 30] </ref>). However, this is an expensive solution, in that it increases the number of communication rounds of the protocol, which is undesirable for both practical and theoretical applications.
Reference: [22] <author> S. Goldwasser and S. Micali. </author> <title> Probabilistic Encryption. </title> <journal> J. Computer and System Sciences, </journal> <volume> Vol. 28, </volume> <year> 1984, </year> <pages> pp. 270-299. </pages>
Reference-contexts: This will be important for us. Suppose the adversary is given C R E pk (b) for some random bit b. According to the standard notion of semantic security <ref> [22] </ref> she cannot figure out b. We want a stronger property, namely that she cannot modify C to some different ciphertext C 0 whose corresponding plaintext is related to the plaintext of C.
Reference: [23] <author> S. Goldwasser, S. Micali and C. Rackoff. </author> <title> The knowledge complexity of interactive proof systems. </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 186-208, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Various notions of interactive protocols <ref> [23, 3, 6, 8] </ref> have found wide applicability over the last decade. They have turned out to be essential for cryptography but somewhat more surprisingly, they have also been key to complexity theory, in particular to the theory of hardness of approximation problems (see [1] for a survey).
Reference: [24] <author> R. Impagliazzo. </author> <title> Hard-core distributions for somewhat hard problems. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: A direct product conjecture asks whether, for some model, and a suitably hard computational problem for that model, several independent instances of the problem are harder than a single instance. A classical example of such a result is Yao's XOR lemma <ref> [31, 26, 20, 24, 25] </ref> which states that, if any feasible computation in a non-uniform model has a constant chance of failure at predicting a Boolean function, then the probability that a feasible computation could compute the of several strings becomes negligible.
Reference: [25] <author> R. Impagliazzo and A. Wigderson. </author> <title> P = BPP unless E has sub-exponential circuits. </title> <booktitle> Proceedings of the 29th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1997. </year>
Reference-contexts: rate we could expect: exponentially until it becomes negligible. (We cannot expect the error probability of a computationally sound proof to ever go below negligible, as this typically is as low as we assume the probability of breaking the underlying hard computational problem like factoring.) The proof exploits techniques from <ref> [25] </ref>. These results indicate that there is a fundamental difference about computational soundness and the kind of "statistical" soundness that is the property of interactive proofs (whether single or multiple prover ones) as far as composition is concerned. <p> A direct product conjecture asks whether, for some model, and a suitably hard computational problem for that model, several independent instances of the problem are harder than a single instance. A classical example of such a result is Yao's XOR lemma <ref> [31, 26, 20, 24, 25] </ref> which states that, if any feasible computation in a non-uniform model has a constant chance of failure at predicting a Boolean function, then the probability that a feasible computation could compute the of several strings becomes negligible. <p> just on the challenge, but on a random tape used to pick the challenge. (For example, the challenge could be a one-way function of the random tape.) This correspondence is the basic idea of the positive results for three round protocols, which uses a modified version of the proof in <ref> [25] </ref> of a direct product for Boolean functions. <p> The technique used is based on the XOR Casino game of <ref> [25] </ref>. Let V be a verifier defining a three message protocol. Thus V 's output is either 1 (accept) or 0 (reject). Say V 's random tape is of length r. Let k be a positive integer, The following theorem states a very general error-decreasing property for three-round protocols.
Reference: [26] <author> L. Levin. </author> <title> One-way functions and pseudorandom generators. </title> <journal> Combinatorica, </journal> <volume> Vol. 7, No. 4, </volume> <year> 1987, </year> <pages> pp. 357-363. </pages>
Reference-contexts: A direct product conjecture asks whether, for some model, and a suitably hard computational problem for that model, several independent instances of the problem are harder than a single instance. A classical example of such a result is Yao's XOR lemma <ref> [31, 26, 20, 24, 25] </ref> which states that, if any feasible computation in a non-uniform model has a constant chance of failure at predicting a Boolean function, then the probability that a feasible computation could compute the of several strings becomes negligible.
Reference: [27] <author> S. Micali. </author> <title> CS proofs. </title> <booktitle> Proceedings of the 35th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1994. </year>
Reference: [28] <author> M. Naor. </author> <title> Verification of a human in the loop or Identification via the Turing Test. </title> <type> Manuscript, </type> <year> 1996. </year>
Reference-contexts: The resulting protocol looks much like protocols obtained by a general technique to make trusted verifier zero-knowledge protocols truly zero-knowledge. 1 The basic protocol is usually cryptographic, but this idea also makes sense in non-traditional situations. For instance, in <ref> [28] </ref> there is a proposal to use "an automated Turing test" to make sure that a human is requesting to use a resource like an on-line database and for combating junk-mail.
Reference: [29] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the 27th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: However, the error can be reduced at an exponential rate depending on the communication complexity of the given protocol <ref> [29] </ref>, and this is the best possible [16]. Soon after the appearance of interactive proofs, the notion of arguments (also called computa-tionally convincing protocols) was put forth by [8, 9]. <p> In the last mentioned construction, the communication complexity of the original protocol depends linearly on k. Thus, these examples still hold out the possibility of a Raz-like <ref> [29] </ref> result in which the error does decrease but at a rate proportional to the communication complexity. However, we then present evidence that even this is unlikely to hold in the computational setting. <p> significantly: Claim 3.4 In the protocol consisting of k parallel repetitions of DD k , there is a polynomial time strategy for the prover to make the verifier accept with probability at least 1=2. 3.4 Failure of parallel error reduction with low communication In light of the results of Raz <ref> [29] </ref> and Feige and Verbitsky [16] a reasonable conjecture at this point is that the failure of error-reduction in protocol DD k is due to the fact that the communication complexity is proportional to k (the number times we are going to execute the protocol in parallel).
Reference: [30] <author> M. Tompa and H. Woll. </author> <title> Random Self-Reducibility and Zero-Knowledge Proofs of Possession of Information. </title> <booktitle> Proceedings of the 28th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1987. </year>
Reference-contexts: Sequential repetition, repeating the protocol several times, beginning the next run after the previous one terminates, reduces error in all important models. It also preserves desirable properties of the original protocol, such as zero-knowledge (see <ref> [21, 30] </ref>). However, this is an expensive solution, in that it increases the number of communication rounds of the protocol, which is undesirable for both practical and theoretical applications.
Reference: [31] <author> A. C. Yao. </author> <title> Theory and Applications of Trapdoor functions. </title> <booktitle> Proceedings of the 23rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1982. </year>
Reference-contexts: A direct product conjecture asks whether, for some model, and a suitably hard computational problem for that model, several independent instances of the problem are harder than a single instance. A classical example of such a result is Yao's XOR lemma <ref> [31, 26, 20, 24, 25] </ref> which states that, if any feasible computation in a non-uniform model has a constant chance of failure at predicting a Boolean function, then the probability that a feasible computation could compute the of several strings becomes negligible.
References-found: 31

