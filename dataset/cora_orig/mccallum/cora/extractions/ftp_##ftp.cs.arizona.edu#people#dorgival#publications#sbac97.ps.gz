URL: ftp://ftp.cs.arizona.edu/people/dorgival/publications/sbac97.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/dorgival/pub_english.html
Root-URL: http://www.cs.arizona.edu
Email: E-mail: fdorgival,llpg@cs.arizona.edu  
Title: Network Subsystems in MPPs: Where Did All the Performance Go?  
Author: Dorgival O. Guedes Larry L. Peterson 
Keyword: Resumo  
Note: Sponsored by Conselho Nacional de Desenvolvimento Cientfico e Tecnologico (CNPq), Brazil, Process no. 200861/93-0  
Date: June 1997  
Address: Tucson, AZ 85721, USA  
Affiliation: Department of Computer Science University of Arizona  
Abstract: In this work we describe our results on identifying the most important overheads in the network subsystem of massively parallel processors (MPPs), specially the Intel Paragon. We show how poor implementation techniques currently prevent performance of applications using TCP/IP protocols to communicate between supercomputers connected by high performance networks from achieving data rates close to the capacity of the medium. We identify some of the possible solutions to the problem, and discuss what can be expected in future systems. In particular, we present our results on evaluation of some of those solutions, including a user-level protocol implementation which scales with the number of concurrent connections and performance superior to the current system. Neste artigo apresenta-se resultados sobre a identifica~c~ao dos principais gargalos em subsistemas de rede de processadores massivamente paralelos, abordando particular-mente o Intel Paragon. Demonstra-se como tecnicas de implementa~c~ao correntes im-pedem que aplica~c~oes que utilizem protocolos como TCP/IP entre supercomputadores interligados por redes de alta velocidade alcancem taxas de transfer^encia proximas a capacidade do meio de transmiss~ao. Algumas possveis solu~c~oes para o problema s~ao sugeridas, e discute-se o que pode ser esperado de sistemas futuros. Em particular, resultados de avalia~c~ao dos benefcios de algumas tecnicas s~ao apresentados, destacando-se uma implementa~c~ao de protocolos no espa~co da aplica~c~ao que apresenta performance e capacidade de expans~ao para multiplas conex~oes superior aos sistemas atuais. 
Abstract-found: 1
Intro-found: 1
Reference: [Bar91] <author> Joseph S. Barrera. </author> <title> A fast Mach network IPC implementation. </title> <booktitle> In Proceedings of the Usenix Mach Symposium, 2560 Ninth Street, </booktitle> <address> Suite 215, Berkeley CA 94710, </address> <month> November </month> <year> 1991. </year> <institution> Usenix Association. </institution>
Reference-contexts: Here we see costs piling up. The raw HiPPI interface available to the application programmer is based on Mach NORMA IPC messages, messages used by Mach kernels in a cooperative network to communicate with each other <ref> [Bar91] </ref>. When the application in the compute node has to send a message, it issues a system call to deliver it to the kernel in its own node, which causes the first overhead.
Reference: [Com95] <author> Douglas E. Comer. </author> <title> Internetworking with TCP/IP, volume 1. </title> <publisher> Prentice Hall, 3rd edition, </publisher> <year> 1995. </year>
Reference-contexts: For example, the use of raw HiPPI connections is common in the supercomputer world, but such technique is limited to local HiPPI networks. When we intend to connect systems at different sites, separated by possibly multiple networks with different technologies, TCP/IP is still the easiest way to go <ref> [Com95] </ref>. 1 Although an implementation of PVM in the Paragon may use its internal IPC for communication between nodes, communication with other machines must rely on TCP/IP to cross the external network But where does the performance go? A TCP/IP connection between two Intel Paragon over a HiPPI switch, for example,
Reference: [Dun94] <author> Thomas H. Dunigan. </author> <title> Early experiences and performance of the Intel Paragon. </title> <type> Technical Report ORNL/TM-12194, </type> <institution> Oak Ridge National Laboratory, </institution> <month> oct </month> <year> 1994. </year>
Reference-contexts: All this limits the maximum throughput. It is easy to show that OSF/1 is the culprit here by comparing those numbers with measurements of the same transfer under SUNMOS as mentioned in <ref> [Dun94] </ref>. Under SUNMOS the transfer reaches up to 150 MB/s, very close to the link capacity. Obviously the overhead of OSF/1 limits the bandwidth between the application processor and the message processor.
Reference: [Dun96] <author> Thomas H. Dunigan. </author> <title> Performance of ATM/OC-12 on the Intel Paragon. </title> <type> Technical Report ORNL/TM-13239, </type> <institution> Oak Ridge National Laboratory, </institution> <month> may </month> <year> 1996. </year>
Reference-contexts: The problems begin when we try to use any TCP/IP based code to push data over a connection between two MPPs linked by some high speed network. The user would be shocked by the low bandwidth achieved in most cases. Many solutions today tend to adopt specialized approaches <ref> [Dun96] </ref>, which unfortunately do not scale well for a more general network. For example, the use of raw HiPPI connections is common in the supercomputer world, but such technique is limited to local HiPPI networks.
Reference: [FGB91] <author> Alessandro Forin, David Golub, and Brian Bershad. </author> <title> An I/O system for Mach 3.0. </title> <booktitle> In Proceedings of the Usenix Mach Symposium, 2560 Ninth Street, </booktitle> <address> Suite 215, Berkeley CA 94710, </address> <month> November </month> <year> 1991. </year> <institution> Usenix Association. </institution>
Reference-contexts: The throughput measured that way would be the actual maximum throughput available for an application using the HiPPI interfaces, and would identify the overhead due to remote device access <ref> [FGB91] </ref>. connection between generic compute nodes. In the I/O node case the maximum rate is 24 MB/s, already well below NX maximum throughput. In this case again the problem is due to operating system overhead. <p> Our implementation makes use of Intel p-threads library [Int93] and the Mach out-of-kernel device interface <ref> [FGB91] </ref> to access the HiPPI board. With each node being able to process its own TCP connections, the Unix server is removed from the data path completely. The server is accessed only during connection setup and tear down, when the host wide context of TCP has to be updated.
Reference: [Ger95] <author> Jerry Gerner. </author> <title> Input/output on the IBM SP2| an overview, </title> <note> 1995. Available at http://www.tc.cornell.edu/SmartNodes/Newsletters/IO.series/intro.html. </note>
Reference-contexts: Such machines are based on the use of high performance commodity processors connected by a high speed proprietary interconnection network. Examples of such systems are the IBM SP systems <ref> [Ger95] </ref> and the Intel Paragon [Int91]. What guarantees the high performance of such systems is the availability of interprocess communication primitives (IPC) which allow processes in different nodes of the machine to exchange information at very high rates with minimum delays.
Reference: [GLS95] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: In the application realm, one would expect to be able to extend a parallel application across multiple computers by using some already designed communication package, like PVM [Sun92] or MPI <ref> [GLS95] </ref>, and some stable protocol suite like TCP/IP 1 . But things are not that simple. The problems begin when we try to use any TCP/IP based code to push data over a connection between two MPPs linked by some high speed network.
Reference: [GP97] <author> Dorgival O. Guedes and Larry L. Peterson. </author> <title> Eliminating the network subsystem bottleneck in MPPs. </title> <note> To appear, </note> <year> 1997. </year>
Reference-contexts: This is possible by programming the packet filter at the I/O node with information to identify the end point of each TCP packet [YBMM94]. A more detailed description of our work can be found in <ref> [GP97] </ref>. At this point we make use of Mach NORMA IPC to reach the I/O node. As we have discussed, altering the kernel to accept NX requests directly might improve performance even further. We have performed the same connection scalability tests we used before for the OSF/1 protocol stack.
Reference: [Int91] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: Such machines are based on the use of high performance commodity processors connected by a high speed proprietary interconnection network. Examples of such systems are the IBM SP systems [Ger95] and the Intel Paragon <ref> [Int91] </ref>. What guarantees the high performance of such systems is the availability of interprocess communication primitives (IPC) which allow processes in different nodes of the machine to exchange information at very high rates with minimum delays.
Reference: [Int93] <institution> Paragon user's guide. Intel Corporation, </institution> <month> oct </month> <year> 1993. </year>
Reference-contexts: The remaining i860 (one or two, depending on the board model) are available for the application. Communication between the application processor (s) and the message processor is performed through shared memory structures, and is controlled by the interprocess communication (IPC) interface, called NX <ref> [Int93] </ref>. The operating system on each node can be either Intel's OSF/1, derived directly from the Mach operating system [RBG + 93, RBF + 89], or SUNMOS, a light weight kernel developed at Sandia Labs [SS94]. <p> We have ported the x-kernel [OP92] to the Intel Paragon application address space and used it as the implementation framework to build a user-level TCP/IP protocol stack, complete with the TCP extensions for high bandwidth networks such as HiPPI. Our implementation makes use of Intel p-threads library <ref> [Int93] </ref> and the Mach out-of-kernel device interface [FGB91] to access the HiPPI board. With each node being able to process its own TCP connections, the Unix server is removed from the data path completely.
Reference: [Int97] <institution> Intel TeraFLOPs supercomputer project home page. Intel Corporation, </institution> <year> 1997. </year> <note> http://www.ssd.intel.com/. </note>
Reference-contexts: There has been some research in the last few years pointing out that microkernel designs need not add such high overhead to system services [Lie95], and such results may eventually be used in MPP operating systems. For example, the next generation Intel supercomputer <ref> [Int97] </ref> does not use the Mach kernel in compute nodes, replacing it by a lighter, faster kernel, yet maintaining access to system resources for all nodes. 4.2 Integration of primitives Comparisons between NX bandwidth and Mach NORMA IPC show how performance can differ between restricted, application level limited communication and the
Reference: [Lie95] <author> Jochen Liedtke. </author> <title> On micro-kernel construction. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles. ACM, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: There has been some research in the last few years pointing out that microkernel designs need not add such high overhead to system services <ref> [Lie95] </ref>, and such results may eventually be used in MPP operating systems.
Reference: [LR94] <author> John LoVerso and Paul Roy. </author> <title> The network architecture of OSF/1 AD version 2. </title> <booktitle> In OSF/RI Operating Systems Collected Papers Vol. 3. OSF Research Institute, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: Next factor to justify such low numbers is the structure of the protocol stack implementation, represented in figure 4. As a Mach derivative, Intel OSF/1 implements all protocol processing functions inside the Unix server, a process running in one of the nodes of the MPP, called the service node <ref> [LR94] </ref>. The reason for that is to simplify the management of host wide protocol information, like ports and sequence numbers. Using a centralized server, all this information is kept in one place.
Reference: [MB92] <author> Chris Maeda and Brian N. Bershad. </author> <title> Networking performance for microkernels. </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Although it was an important project, with many contributions, Mach structure poses problems due to the high overhead it requires to allow applications to access systems resources, both on the local node and, more severely, on remote nodes <ref> [MB92] </ref>. SUNMOS can be pointed out as a possible solution when applications require just intra-MPP communication, but the lack of access to other system services like remote devices prevents it from being used when inter-MPP communication is a must.
Reference: [MB93] <author> Chris Maeda and Brian N. Bershad. </author> <title> Protocol service decomposition for high-performance network. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: We have focused on this problem in our research, where we have succeeded in avoiding the Unix server bottleneck. The idea we have implemented is illustrated in figure 6, and is based on the user level protocol implementation as proposed by Maeda and Bershad <ref> [MB93] </ref>. We have ported the x-kernel [OP92] to the Intel Paragon application address space and used it as the implementation framework to build a user-level TCP/IP protocol stack, complete with the TCP extensions for high bandwidth networks such as HiPPI.
Reference: [OP92] <author> S. W. O'Malley and L. L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The idea we have implemented is illustrated in figure 6, and is based on the user level protocol implementation as proposed by Maeda and Bershad [MB93]. We have ported the x-kernel <ref> [OP92] </ref> to the Intel Paragon application address space and used it as the implementation framework to build a user-level TCP/IP protocol stack, complete with the TCP extensions for high bandwidth networks such as HiPPI.
Reference: [RBF + 89] <author> Richard Rashid, Robert Baron, Alessandro Forin, David Golub, Michael Jones, Daniel Julin, Douglas Orr, and Richard Sanzi. </author> <title> Mach: A foundation for open systems. </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems(WWOS2), </booktitle> <month> sep </month> <year> 1989. </year>
Reference-contexts: Communication between the application processor (s) and the message processor is performed through shared memory structures, and is controlled by the interprocess communication (IPC) interface, called NX [Int93]. The operating system on each node can be either Intel's OSF/1, derived directly from the Mach operating system <ref> [RBG + 93, RBF + 89] </ref>, or SUNMOS, a light weight kernel developed at Sandia Labs [SS94].
Reference: [RBG + 93] <author> Paul Roy, David Black, Paulo Guedes, John LoVerso, Durriya Netterwala, Fara-marz Rabii, Michael Barnett, Bradford Kemp, Michael Leibensperger, Chris Peak, and Roman Zajcew. </author> <title> An OSF/1 unix for massively parallel multicom-puters. </title> <booktitle> In OSF/RI Operating Systems Collected Papers Vol. 2. OSF Research Institute, </booktitle> <address> Cambridge, MA, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Communication between the application processor (s) and the message processor is performed through shared memory structures, and is controlled by the interprocess communication (IPC) interface, called NX [Int93]. The operating system on each node can be either Intel's OSF/1, derived directly from the Mach operating system <ref> [RBG + 93, RBF + 89] </ref>, or SUNMOS, a light weight kernel developed at Sandia Labs [SS94].
Reference: [Ren97] <author> J. Renwick. </author> <title> IP over HIPPI. Request for Comments (Experimental) RFC 2067, </title> <institution> Internet Engineering Task Force, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: First of all, the standard for TCP/IP implementation over HiPPI networks states that the maximum segment size allowed in this case is a little under 64 KB <ref> [Ren97] </ref>. So we cannot benefit from sending data as huge 1 MB packets, as in our previous tests. This limitation makes the packetization overhead (headers, HiPPI burst transfer setup and tear down, etc.) much higher, reducing the maximum achievable rate.
Reference: [SS94] <author> Subhash Saini and Horst D. Simon. </author> <title> Applications performance under OSF/1 AD and SUNMOS on Intel Paragon XP/2-15. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <address> Washington, DC, </address> <month> nov </month> <year> 1994. </year>
Reference-contexts: The operating system on each node can be either Intel's OSF/1, derived directly from the Mach operating system [RBG + 93, RBF + 89], or SUNMOS, a light weight kernel developed at Sandia Labs <ref> [SS94] </ref>. OSF/1 provides the application running on any node with a complete interface to all system resources, while SUNMOS offers only a limited interface, allowing nodes in an application little more than message exchange, but with higher performance, due to the smaller overhead.
Reference: [Ste94] <author> Peter A. Steenkiste. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <journal> Computer, </journal> <volume> 27(3) </volume> <pages> 47-58, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: That confirms what has been suggested by others: TCP/IP implementations in high performance systems can benefit heavily from some kind of hardware assisted checksum computation associated with the network interface <ref> [Ste94] </ref>. One of the reasons such support is not present in the Paragon board is that its HiPPI interface was developed having in mind local access to disks and other storage devices, not internetworking.
Reference: [Sun92] <author> Vaidy Sunderam. </author> <title> Concurrent computing with PVM. </title> <booktitle> In Proceedings of the Workshop on Cluster Computing, </booktitle> <address> Tallahassee, FL, </address> <month> December </month> <year> 1992. </year> <institution> Supercomputing Computations Research Institute, Florida State University. </institution>
Reference-contexts: In the application realm, one would expect to be able to extend a parallel application across multiple computers by using some already designed communication package, like PVM <ref> [Sun92] </ref> or MPI [GLS95], and some stable protocol suite like TCP/IP 1 . But things are not that simple. The problems begin when we try to use any TCP/IP based code to push data over a connection between two MPPs linked by some high speed network.
Reference: [TB94] <author> John Michael Tracey and Arindam Banerji. </author> <title> Device driver issues in high-performance networking. </title> <booktitle> In Proceedings of the 1994 USENIX Symposium on High-Speed Networking, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Tuning the inter-kernel communication and device access primitives to the features of the proprietary interconnect (and extending the interconnect with features like hardware based authentication and access control) would certainly yield faster systems <ref> [TB94] </ref>.
Reference: [TR93] <author> Don Tolmie and John Renwick. </author> <title> Hippi: Simplicity yields success. </title> <journal> IEEE Network, </journal> <month> January </month> <year> 1993. </year>
Reference-contexts: The need for such connection comes from the ever increasing need to share information among different sites and different computers, as well as from the need to work on even larger problems. But although we have very high performance supercomputers and very fast network interconnects available today, like HiPPI <ref> [TR93] </ref> and ATM [Vet95], there has been only limited success on putting them together.
Reference: [Vet95] <author> Ronald J. Vetter. </author> <title> Atm concepts, architectures and protocols. </title> <journal> Communications of the ACM, </journal> <volume> 38(2), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: But although we have very high performance supercomputers and very fast network interconnects available today, like HiPPI [TR93] and ATM <ref> [Vet95] </ref>, there has been only limited success on putting them together.
Reference: [YBMM94] <author> Masanobu Yuhara, Brian N. Bershad, Chris Maeda, and J. Eliot B. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 153-165, </pages> <address> San Francisco, CA, </address> <booktitle> Winter 1994. USENIX. </booktitle>
Reference-contexts: Once the connection is set, data moves directly between the I/O node and the compute node holding the process using each connection. This is possible by programming the packet filter at the I/O node with information to identify the end point of each TCP packet <ref> [YBMM94] </ref>. A more detailed description of our work can be found in [GP97]. At this point we make use of Mach NORMA IPC to reach the I/O node. As we have discussed, altering the kernel to accept NX requests directly might improve performance even further.
References-found: 26

