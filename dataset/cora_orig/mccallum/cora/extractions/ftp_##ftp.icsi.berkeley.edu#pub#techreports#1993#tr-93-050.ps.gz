URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-050.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: Interior Point Methods in Semidefinite Programming with Applications to Combinatorial Optimization  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Farid Alizadeh 
Note: Research supported in part by NSF grant no. CDA-9211106, by the Air Force Office of Scientific Research grant AFOSR-87-0127, the National Science Foundation grant DCR-8420935 and the  
Date: September 1, 1993  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Minnesota Supercomputer Institute.  
Pubnum: TR-93-050  
Abstract: We study the semidefinite programming problem (SDP), i.e the problem of optimization of a linear function of a symmetric matrix subject to linear equality constraints and the additional condition that the matrix be positive semidefinite. First we review the classical cone duality as specialized to SDP. Next we present an interior point algorithm which converges to the optimal solution in polynomial time. The approach is a direct extension of Ye's projective method for linear programming. We also argue that most known interior point methods for linear programs can be transformed in a mechanical way to algorithms for SDP with proofs of convergence and polynomial time complexity also carrying over in a similar fashion. Finally we study the significance of these results in a variety of combinatorial optimization problems including the general 0-1 integer programs, the maximum clique and maximum stable set problems in perfect graphs, the maximum k-partite subgraph problem in graphs, and various graph partitioning and cut problems. As a result, we present barrier oracles for certain combinatorial optimization problems (in particular, clique and stable set problem for perfect graphs) whose linear programming formulation requires exponentially many inequalities. Existence of such barrier oracles refutes the commonly believed notion that in order to solve a combinatorial optimization problem with interior point methods, one needs its linear programming formulation explicitly. y A preliminary version of this paper appeared in the proceedings of the 2nd Integer Programming and Combinatorial Optimizations (IPCO) held at Carnegie-Mellon University, Pittsburgh, PA, 1992 under the title "Combinatorial Optimization with Semidefinite Matrices" The current version will appear in SIAM Journal on Optimization. z Currently at International Computer Science Institute, Berkeley CA 94704. E-mail: alizadeh@icsi.berkeley.edu 
Abstract-found: 1
Intro-found: 1
Reference: [Ali91] <author> F. Alizadeh. </author> <title> Combinatorial Optimization with Interior Point Methods and Semi-Definite Matrices. </title> <type> PhD thesis, </type> <institution> University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1991. </year>
Reference-contexts: Therefore, the authors extend the revolutionary result of Karmarkar [Kar84] to a rather general class of convex programs. In this article we study interior point methods for semidefinite programs from an alternative point of view. Our work <ref> [Ali91] </ref> started somewhat later than, and independent of, that of [NN90]. Nesterov and Nemirovskii obtain their complexity theorems by specializing their general results to SDP. <p> of the algebraic system of equations: XS = 0; AvecX = b and A T y + S = C, there are algebraic solutions among all optimal solutions of an SDP problem with integral input. 4 I am indebted to Joshi Ramana for bringing to my attention an error in <ref> [Ali91, Ali92] </ref> where I had claimed that the norm of the solution to any SDP problem is bounded by 2 L .
Reference: [Ali92] <author> F. Alizadeh. </author> <title> Optimization Over Positive Semi-Definite Cone; Interior-Point Methods and Combinatorial Applications. </title> <editor> In P. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing. </booktitle> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: of the algebraic system of equations: XS = 0; AvecX = b and A T y + S = C, there are algebraic solutions among all optimal solutions of an SDP problem with integral input. 4 I am indebted to Joshi Ramana for bringing to my attention an error in <ref> [Ali91, Ali92] </ref> where I had claimed that the norm of the solution to any SDP problem is bounded by 2 L . <p> Proofs of convergence or polynomial time complexity may also be extended mechanically in the same manner. We have already verified this claim on the approaches of Gonzaga [Gon89], Ye [Ye91] (see <ref> [Ali92] </ref>), and Monteiro and 18 LP SDP unknown vector: x unknown symmetric matrix: X inequality constraints: Lowner constraints: - dual variable: y dual variable: y dual slack vector: s dual slack symmetric matrix: S 1 I linear scaling: linear scaling: x ! (x i =(x 0 ) i ) n i=1
Reference: [AN87] <author> E. Anderson and P. Nash. </author> <title> Linear Programming in Infinite Dimensional Sapces. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash <ref> [AN87] </ref> and for alternative extensions refer to [BW81b, BW81a]. It is worth mentioning that [AN87] study the duality theory from the point of view of basic feasible solutions and extend the "tableau based" proofs of LP duality. <p> For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash <ref> [AN87] </ref> and for alternative extensions refer to [BW81b, BW81a]. It is worth mentioning that [AN87] study the duality theory from the point of view of basic feasible solutions and extend the "tableau based" proofs of LP duality. The latest version of Nesterov and Nemirovskii's text [NN92] also treats cone duality for the general convex programs. <p> We now state generalizations of Farkas' lemma. Such generalizations for arbitrary convex cones have been studied as early as 1958 by Hurwicz, [Hur58]. See <ref> [AN87] </ref> for references on the history and various extensions of Farkas' lemma to nonpolyhedral cones. Here we study the relevant forms of this lemma in the special case of SDP. It is not possible to generalize classical Farkas' lemma to nonpolyhedral cones without additional qualifications.
Reference: [Bar82a] <author> E. R. Barnes. </author> <title> An algorithm for partitioning the nodes of a graph. </title> <journal> SIAM J. Alg. and Disc. Meth., </journal> <volume> 3, </volume> <year> 1982. </year>
Reference-contexts: See also Barnes <ref> [Bar82a] </ref> and [Bar82b]. An important special case of the graph partitioning problem is the case when all m i 's are equal.
Reference: [Bar82b] <author> E. R. Barnes. </author> <title> Partitioning the nodes of a graph. </title> <editor> In L. Lesniak, D. R. Lick, and C. E. Wall, editors, </editor> <title> Graph Theory with Applications to Algorithms and Computer Science. </title> <publisher> John Wiley, </publisher> <year> 1982. </year>
Reference-contexts: See also Barnes [Bar82a] and <ref> [Bar82b] </ref>. An important special case of the graph partitioning problem is the case when all m i 's are equal.
Reference: [BH84] <author> E. Barnes and A. J. Hoffman. </author> <title> Partitioning, spectra, and linear programming. </title> <editor> In W. E. Pulleyblank, editor, </editor> <booktitle> Progress in Combinatorial Optimization. </booktitle> <publisher> Academic Press, </publisher> <year> 1984. </year>
Reference-contexts: Let us denote this minimum number by m (G). Computing m (G) is of course NP-hard. Hoffman and Donath in [DH72] and [DH73] derive the following lower bound on the size of the minimum partition (see also Barnes and Hoffman <ref> [BH84] </ref>). Let A be a matrix with A ij = w ij (A ii = 0). Then Donath and Hoffman prove the following relation [DH73]: m (G) 2 1 T x=a j=1 m j j (A + Diag x) (71) where a:= P w ij . <p> i+1 )A for i = 1; ; k V i - 0 for i = 1; ; k 32 and P k s:t: trace U i = i for i = 1; ; k P k 0 U i I for i = 1; ; k Barnes and Hoffman in <ref> [BH84] </ref> describe how to use the eigenvectors associated with the k largest eigenvalues of the optimal matrix A + Diag x fl to generate a partition of the nodes of the graph. See also Barnes [Bar82a] and [Bar82b].
Reference: [BICK69] <author> A. Ben-Israel, A. Charnes, and K. O. Kortanek. </author> <title> Duality and Asymptotic Solvability over Cones. </title> <journal> Bulletin of American Mathematical Society, </journal> <volume> 75(2) </volume> <pages> 318-324, </pages> <year> 1969. </year>
Reference-contexts: Duffin in [Duf56] was the first one to study such generalized duality theories. Later Hurwicz [Hur58], Ben-Israel, Charnes and Kortanek <ref> [BICK69] </ref>, Borwein and Wolkowicz [BW81b, BW81a], and Wolkowicz [Wol81] among others developed more general formulations of the duality theory.
Reference: [BJS90] <author> M. Bazaraa, J. Jarvis, and H. Sherali. </author> <title> Linear Programming and Network Flows. </title> <publisher> John Wiley, </publisher> <year> 1990. </year>
Reference-contexts: These rules are summarized in the table in figure 1; this table is a direct generalization of a similar table in the text of Bazaraa, Jarvis and Sherali <ref> [BJS90] </ref>. 3 An interior point algorithm. In this section we develop a potential reduction method for solving the primal problem so that, within O ( p nj log *j) iterations, we get an approximate solution with at least * relative accuracy, if * is sufficiently small.
Reference: [Bop87] <author> R. B. Boppana. </author> <title> Eigenvalues and graph bisection: an average case analysis. </title> <booktitle> In Proc. 28th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference-contexts: In that case the graph partitioning problem simplifies to: min (k=n)1 T x + trace V s:t: V + Diag x A V - 0 s:t: Y ii = k 0 Y I Boppana in <ref> [Bop87] </ref> considers the graph bisection problem (that is when k = 2 and m 1 = m 2 = n=2) and derives the following characterization which is always sharper than (5.74): 1 max [J * (A + Diag (x)) n 1 (P S (A + Diag (x)))] where P S :=(I
Reference: [BW81a] <author> J. Borwein and H. Wolkowicz. </author> <title> Characterization of optimality for the abstract convex program with finite dimensional range. </title> <journal> J. Austral. Math. Soc. series A, </journal> <volume> 30 </volume> <pages> 390-411, </pages> <year> 1981. </year>
Reference-contexts: Duffin in [Duf56] was the first one to study such generalized duality theories. Later Hurwicz [Hur58], Ben-Israel, Charnes and Kortanek [BICK69], Borwein and Wolkowicz <ref> [BW81b, BW81a] </ref>, and Wolkowicz [Wol81] among others developed more general formulations of the duality theory. For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash [AN87] and for alternative extensions refer to [BW81b, BW81a]. <p> Borwein and Wolkowicz <ref> [BW81b, BW81a] </ref>, and Wolkowicz [Wol81] among others developed more general formulations of the duality theory. For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash [AN87] and for alternative extensions refer to [BW81b, BW81a]. It is worth mentioning that [AN87] study the duality theory from the point of view of basic feasible solutions and extend the "tableau based" proofs of LP duality. The latest version of Nesterov and Nemirovskii's text [NN92] also treats cone duality for the general convex programs. <p> Now we state the most common form of Farkas' lemma as given in Schrijver's text [Sch86], and as extended to the positive semidefinite cone: 1 Alternative extensions without closedness assumption are treated in <ref> [BW81b, BW81a, Wol81] </ref> 5 Lemma 3 Extended Farkas' lemma: Let b 2 &lt; m and A 2 &lt; mfin 2 be a matrix such that its rows A T i: = vecA i where A i are symmetric for i = 1; ; m.
Reference: [BW81b] <author> J. Borwein and H. Wolkowicz. </author> <title> Facial reduction for a cone-convex programming problem. </title> <journal> J. Austral. Math. Soc. series A, </journal> <volume> 30 </volume> <pages> 369-380, </pages> <year> 1981. </year>
Reference-contexts: Duffin in [Duf56] was the first one to study such generalized duality theories. Later Hurwicz [Hur58], Ben-Israel, Charnes and Kortanek [BICK69], Borwein and Wolkowicz <ref> [BW81b, BW81a] </ref>, and Wolkowicz [Wol81] among others developed more general formulations of the duality theory. For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash [AN87] and for alternative extensions refer to [BW81b, BW81a]. <p> Borwein and Wolkowicz <ref> [BW81b, BW81a] </ref>, and Wolkowicz [Wol81] among others developed more general formulations of the duality theory. For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash [AN87] and for alternative extensions refer to [BW81b, BW81a]. It is worth mentioning that [AN87] study the duality theory from the point of view of basic feasible solutions and extend the "tableau based" proofs of LP duality. The latest version of Nesterov and Nemirovskii's text [NN92] also treats cone duality for the general convex programs. <p> Now we state the most common form of Farkas' lemma as given in Schrijver's text [Sch86], and as extended to the positive semidefinite cone: 1 Alternative extensions without closedness assumption are treated in <ref> [BW81b, BW81a, Wol81] </ref> 5 Lemma 3 Extended Farkas' lemma: Let b 2 &lt; m and A 2 &lt; mfin 2 be a matrix such that its rows A T i: = vecA i where A i are symmetric for i = 1; ; m.
Reference: [CDW75] <author> J. Cullum, W. E. Donath, and P. Wolfe. </author> <title> The minimization of certain nondifferentiable sums of eigenvalue problems. </title> <journal> Mathematical Programming Study, </journal> <volume> 3 </volume> <pages> 35-55, </pages> <year> 1975. </year> <month> 34 </month>
Reference-contexts: An early example of such problems were studied by Donath and Hoffman in connection with graph bisection and graph partitioning problems [DH72, DH73]; see section 5 below. Cullum, Donath and Wolfe studied the problem of minimizing the sum of the first few eigenvalues of a linearly constrained matrix in <ref> [CDW75] </ref>. They analyzed this problem from the nonsmooth optimization point of view. Also Fletcher studied a similar problem from the point of view of nondifferentiable optimization.
Reference: [CK77] <author> B. D. Craven and J. J. Koliha. </author> <title> Generalizations of Farkas' Theorem. </title> <journal> SIAM J. Math. Anal., </journal> <volume> 8(6) </volume> <pages> 983-997, </pages> <year> 1977. </year>
Reference-contexts: We may formulate and prove several other variants of Farkas' lemma in a similar vain, all of which are extensions of lemmas for the component-wise inequalities, as given for example in Schrijver's text [Sch86]. Related extensions for infinite programs have been studied in [Hur58] and <ref> [CK77] </ref>, and in the case of matrix variables in [CM81].
Reference: [CM81] <author> B. D. Craven and B. Mond. </author> <title> Linear Programming with Matrix Variables. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 38 </volume> <pages> 73-80, </pages> <year> 1981. </year>
Reference-contexts: Related extensions for infinite programs have been studied in [Hur58] and [CK77], and in the case of matrix variables in <ref> [CM81] </ref>.
Reference: [DH72] <author> W. E. Donath and A. J. Hoffman. </author> <title> Algorithms for partitioning graphs and computer logic based on eigenvectors of connection matrices. </title> <journal> IBM Technical Disclosures Bulletin, </journal> <volume> 15, </volume> <year> 1972. </year>
Reference-contexts: An early example of such problems were studied by Donath and Hoffman in connection with graph bisection and graph partitioning problems <ref> [DH72, DH73] </ref>; see section 5 below. Cullum, Donath and Wolfe studied the problem of minimizing the sum of the first few eigenvalues of a linearly constrained matrix in [CDW75]. They analyzed this problem from the nonsmooth optimization point of view. <p> Let us denote this minimum number by m (G). Computing m (G) is of course NP-hard. Hoffman and Donath in <ref> [DH72] </ref> and [DH73] derive the following lower bound on the size of the minimum partition (see also Barnes and Hoffman [BH84]). Let A be a matrix with A ij = w ij (A ii = 0).
Reference: [DH73] <author> W. E. Donath and A. J. Hoffman. </author> <title> Lower bounds for the partitioning of graphs. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 17, </volume> <year> 1973. </year>
Reference-contexts: An early example of such problems were studied by Donath and Hoffman in connection with graph bisection and graph partitioning problems <ref> [DH72, DH73] </ref>; see section 5 below. Cullum, Donath and Wolfe studied the problem of minimizing the sum of the first few eigenvalues of a linearly constrained matrix in [CDW75]. They analyzed this problem from the nonsmooth optimization point of view. <p> To formulate this problem as a semidefinite program, we use a technique originally employed by Donath and Hoffman in <ref> [DH73] </ref>. <p> Let us denote this minimum number by m (G). Computing m (G) is of course NP-hard. Hoffman and Donath in [DH72] and <ref> [DH73] </ref> derive the following lower bound on the size of the minimum partition (see also Barnes and Hoffman [BH84]). Let A be a matrix with A ij = w ij (A ii = 0). Then Donath and Hoffman prove the following relation [DH73]: m (G) 2 1 T x=a j=1 m <p> Hoffman and Donath in [DH72] and <ref> [DH73] </ref> derive the following lower bound on the size of the minimum partition (see also Barnes and Hoffman [BH84]). Let A be a matrix with A ij = w ij (A ii = 0). Then Donath and Hoffman prove the following relation [DH73]: m (G) 2 1 T x=a j=1 m j j (A + Diag x) (71) where a:= P w ij . Again it is clear that computing this bound is an SDP problem.
Reference: [DP90] <author> C. Delorme and S. Poljak. </author> <title> Laplacian eigenvalues and the maximum cut problem. </title> <type> Technical Report 599, </type> <institution> Universie de Paris-sud, Centre d'Orsay, </institution> <year> 1990. </year>
Reference-contexts: In <ref> [DP90, PR91] </ref> the following SDP bound is proposed: minf n 1 (A + Diag (x)) : 1 T x = ag MC (G) (76) where MC (G) is the size of maximum cut in G. (5.76) is equivalent to primal-dual pair: min z + (1=n)1 T x s.t. zI Diag (x)
Reference: [Duf56] <author> R. J. Duffin. </author> <title> Infinite Programs. </title> <editor> In H. W. Kuhn and A. W. Tucker, editor, </editor> <booktitle> Linear inequalities and Related Systems, </booktitle> <pages> pages 157-170. </pages> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1956. </year>
Reference-contexts: The duality theory in linear programming can be extended to generalized linear programming problems where " K " replaces "" in the primal problem and " K fl " replaces "" in the dual problem. Duffin in <ref> [Duf56] </ref> was the first one to study such generalized duality theories. Later Hurwicz [Hur58], Ben-Israel, Charnes and Kortanek [BICK69], Borwein and Wolkowicz [BW81b, BW81a], and Wolkowicz [Wol81] among others developed more general formulations of the duality theory.
Reference: [Fan93] <author> M. Fan. </author> <title> A quadratically convergent local algorithm on minimizing the largest eigenvalue of a symmetric matrix. Linear Algebra and its Applications, </title> <address> 188,189:207-230, </address> <year> 1993. </year>
Reference-contexts: Specifically if the condition that X is a diagonal matrix is added to the constraint set then (1.1) reduces to linear programming. Semidefinite programs arise in a wide variety of applications from control theory (see [VB93] and <ref> [Fan93] </ref>) to combinatorial optimization (see section 5 below) and even structural computational complexity theory (see [FL92]). The oldest form of semidefinite programming is the evaluation of eigenvalues of a symmetric matrix.
Reference: [FL92] <author> U. Feig and L. Lovasz. </author> <title> Two-server one-round proof systems: their power and their problems. </title> <booktitle> In Proc. 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 733-744, </pages> <year> 1992. </year>
Reference-contexts: Semidefinite programs arise in a wide variety of applications from control theory (see [VB93] and [Fan93]) to combinatorial optimization (see section 5 below) and even structural computational complexity theory (see <ref> [FL92] </ref>). The oldest form of semidefinite programming is the evaluation of eigenvalues of a symmetric matrix.
Reference: [Fle85] <author> R. Fletcher. </author> <title> Semi-definite matrix constraints in optimization. </title> <journal> SIAM J. Control Optim., </journal> <volume> 23 </volume> <pages> 493-513, </pages> <year> 1985. </year>
Reference-contexts: The latest version of Nesterov and Nemirovskii's text [NN92] also treats cone duality for the general convex programs. Papers of Overton and Womersley [OW92] and Fletcher <ref> [Fle85] </ref> treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. <p> Also, Fletcher in <ref> [Fle85] </ref> derives a closely related result to (4.40) but the result was incorrect (Fletcher had 0 S rather than 0 S I.) The min characterizations as well as the primal and dual formulation of the variants with equality constraints, we believe are new.
Reference: [FNO87] <author> S. Friedland, J. Nocedal, and M. L. Overton. </author> <title> The formulation and analysis of numerical methods for inverse eigenvalue problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 24(3), </volume> <year> 1987. </year>
Reference-contexts: In the same spirit as Fletcher, Overton [Ove88] studies the largest eigenvalue of a symmetric matrix as a convex, but nondifferentiable function. Based on earlier work <ref> [FNO87] </ref>, in [Ove88] he derives a quadratically convergent algorithm for the problem of minimizing the largest eigenvalue of an affinely constrained matrix.
Reference: [FW71] <author> P. A. Fillmore and J. P. Williams. </author> <title> Some convexity theorems for matrices. </title> <journal> Glasgow Mathematical Journal, </journal> <volume> 12 </volume> <pages> 110-117, </pages> <year> 1971. </year>
Reference-contexts: It is worth mentioning that this result is based on a beautiful convex hull characterization which was known at least as early as 1971, see <ref> [FW71] </ref>, but unfortunately has remained somewhat obscure.
Reference: [GLS81] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> The ellipsoid method and its consequences in combinatorial optimization. </title> <journal> Combinatorica, </journal> <volume> 1(2), </volume> <year> 1981. </year>
Reference-contexts: Semidefinite programs, however, are polynomial time solvable if an a priori bound on the size of their solution is known. This point was implicit in [Lov79] for a special instance of the SDP problem. It was proved in the work of Grotschel, Lovasz and Schrijver, <ref> [GLS81] </ref>. Polynomial time solvability of SDP is a direct consequence of the general results based on the ellipsoid method for convex programming. <p> Papers of Overton and Womersley [OW92] and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. Also Lovasz in [Lov79], Grotschel, Lovasz and Schrijver <ref> [GLS81, GLS84, GLS88] </ref>, and Shapiro in [Sha85] study more or less the same duality theory as we do, but their treatment is restricted to special forms of SDP. It is convenient to assume that C and A i in are symmetric. There is no loss of generality in this assumption.
Reference: [GLS84] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> Polynomial algorithms for perfect graphs. </title> <editor> In C. Berge and V. Chvatal, editors, </editor> <title> Perfect Graphs. </title> <publisher> North Holland, </publisher> <year> 1984. </year> <note> Annals of Discrete Mathematics, 21. </note>
Reference-contexts: Papers of Overton and Womersley [OW92] and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. Also Lovasz in [Lov79], Grotschel, Lovasz and Schrijver <ref> [GLS81, GLS84, GLS88] </ref>, and Shapiro in [Sha85] study more or less the same duality theory as we do, but their treatment is restricted to special forms of SDP. It is convenient to assume that C and A i in are symmetric. There is no loss of generality in this assumption. <p> Since by weak duality lemma we have that z 2 z 1 we conclude that z 2 = z 1 . It is also possible to derive a "complementary slackness" theorem. In fact, Grotschel, Lovasz and Schrijver in <ref> [GLS84] </ref> and Shapiro in [Sha85] mention the complementary slackness theorem for a more restricted form of SDP. Note that when the strong duality theorem is true and both primal and dual problems are bounded and feasible then the duality gap X * S vanishes. <p> Then ff k (G; w) #(G; w), where #(G; w) is defined as # k (G; w) = minf P k = maxfW * Y : Y 2 M and trace Y = k; 0 Y Ig (70) Proof: (This proof is essentially the same as the one given in <ref> [GLS84] </ref> for the case k = 1.) One can transform a weighted graph G into an unweighted one G w by replacing each vertex i with w i mutually nonadjacent vertices and then connecting all w i vertices arising from vertex i to all w j vertices arising from vertex j
Reference: [GLS88] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> Geometric Algorithms and Combinatorial Optimization. </title> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The main point is essentially that optimization of a linear function over a convex set endowed with a separation oracle and an a priori bound on the objective can be achieved in polynomial time using the ellipsoid method; see <ref> [GLS88] </ref> for a thorough treatment. The ellipsoid method, however, has not proven practical in most applications, including SDP. A more recent development is the possibility of using interior point methods to obtain polynomial time algorithms for semidefinite programs. <p> However, it is generally believed that in order to apply interior point methods to the same combinatorial optimization problem one needs to have the explicit listing of all of the inequalities in the LP formulation, see <ref> [GLS88] </ref> and [GT89]. For instance, Goldfarb and Todd in their survey article on linear programming write: ..., it appears that its [Karmarkar's new algorithm] theoretical implications are far more limited than those of the ellipsoid method. <p> Papers of Overton and Womersley [OW92] and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. Also Lovasz in [Lov79], Grotschel, Lovasz and Schrijver <ref> [GLS81, GLS84, GLS88] </ref>, and Shapiro in [Sha85] study more or less the same duality theory as we do, but their treatment is restricted to special forms of SDP. It is convenient to assume that C and A i in are symmetric. There is no loss of generality in this assumption. <p> and dual solutions X and S such that the duality gap X * S *. 3 If * is also a rational number, define L, the size of the SDP problem, as the number of bits in the binary representation of * and entries of C, A, and b, see <ref> [GLS88] </ref> for complete definition. One might expect that the interior point method developed in the previous sections leads to an algorithm which runs in time polynomial in m, n and L. However, this is not true in general as the solution itself may be exponentially large. <p> Odd wheel Constraints. Let W be a graph with 2k vertices such that vertices 1; 2; ; 2k 1 induce a cycle and vertex 2k is adjacent to all other vertices. Then W is called an odd wheel. It can be shown (see <ref> [GLS88] </ref>) that for all wheels W in G, the inequality 2k1 X x i + (k 1)x 2k k 1: (65) is valid for STAB G. set W -STAB G to the polytope defined by the set of all inequalities (5.65) and x i 0. <p> The stable set polytopes of such graphs have in general exponentially many facets. However, in <ref> [GLS88, LS91] </ref> it is shown that one can construct separation oracles for these polytopes and thus find the maximum stable set for the corresponding graphs in polynomial time. <p> It is common belief that in contrast to the ellipsoid method, interior point methods require explicit knowledge of the facets of the polytope on which we wish to optimize, see for instance <ref> [GLS88] </ref> and the quotation from [GT89] in the introduction. However, we can use polynomial time interior point 27 methods to optimize over STAB G in the special cases mentioned above, even though the number of facets in such polytopes may be exponentially large. <p> These results are equivalent to the following statement: Theorem 10 A graph G = (V; E) is perfect iff STAB G = Q-STAB G. (See <ref> [GLS88] </ref>.) Therefore, already the results of the preceding section imply that computing maximum cliques and maximum independent sets in perfect graphs can be accomplished in polynomial time by interior point methods. However, in this case one can derive a slightly stronger result. <p> This min-max equality is proved directly in <ref> [GLS88] </ref>, and also follows easily from the duality theory stated earlier, see (4.45). Lemma 13 For every vertex weighted graph G = (V; E), !(G; w) (G; w) (G; w) ff (G; w) #(G; w):=(G; w) (G; w) See [GLS88] chapter 9 for a thorough treatment of Lovasz number of graphs <p> This min-max equality is proved directly in <ref> [GLS88] </ref>, and also follows easily from the duality theory stated earlier, see (4.45). Lemma 13 For every vertex weighted graph G = (V; E), !(G; w) (G; w) (G; w) ff (G; w) #(G; w):=(G; w) (G; w) See [GLS88] chapter 9 for a thorough treatment of Lovasz number of graphs including several other characterization and many interesting properties. <p> Now our interior point algorithm can compute (G; w) in polynomial time; however in case of perfect graphs we have !(G; w) = (G; w) = (G; w) ff (G; w) = #(G; w) = (G; w) In <ref> [GLS88] </ref> the ellipsoid method was used to establish the polynomial time computability of maximum cliques in perfect graphs. We now show that interior point methods give us a slightly stronger result than the ellipsoid method.
Reference: [Gon89] <author> C. C. Gonzaga. </author> <title> An Algorithm for Solving Linear Programming in O(n 3 L) Operations. </title> <editor> In N. Megiddo, editor, </editor> <booktitle> Progress in Mathematical Programming, </booktitle> <pages> pages 1-28. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Proofs of convergence or polynomial time complexity may also be extended mechanically in the same manner. We have already verified this claim on the approaches of Gonzaga <ref> [Gon89] </ref>, Ye [Ye91] (see [Ali92]), and Monteiro and 18 LP SDP unknown vector: x unknown symmetric matrix: X inequality constraints: Lowner constraints: - dual variable: y dual variable: y dual slack vector: s dual slack symmetric matrix: S 1 I linear scaling: linear scaling: x ! (x i =(x 0 )
Reference: [GPST91] <author> A. V. Goldberg, S. A. Plotkin, D. Shmoys, and E. Tardos. </author> <title> Interior-Point Methods in Parallel Computation. </title> <journal> SIAM J. Comput., </journal> <volume> 21(1) </volume> <pages> 149-150, </pages> <year> 1991. </year>
Reference-contexts: This is particularly interesting because presently no linear programming formulation of the stable set and clique problems for perfect graphs with polynomially bounded number of facets is known. Linear programming interior point methods have been used by Goldberg et al <ref> [GPST91] </ref> to derive sublinear time parallel algorithms for the bounded weight assignment problem. We show that maximum stable sets for perfect graphs can be computed in randomized sublinear parallel time.
Reference: [Gra81] <author> A. Graham. </author> <title> Kronecker Products and Matrix Calculus: with Applications. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1981. </year> <month> 35 </month>
Reference-contexts: We use the following facts repeatedly: (A B)(C D) = AC BD vec (ABC) = (C T A)vec (B): See <ref> [Gra81] </ref>. If I and J are subsets of integers from 1 to p and from 1 to q, respectively, then A I;J is the submatrix of A whose rows are taken from those rows of A indexed by I, and whose columns are indexed by J.
Reference: [GT89] <author> D. Goldfarb and M. J. Todd. </author> <title> Linear Programming. </title> <editor> In G. L. Nemhauser, A. H. G. Rin--nooy Kan, and M. J. Todd, editors, </editor> <booktitle> Optimization, Handbooks in Operations Research and Management Sciences. </booktitle> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: However, it is generally believed that in order to apply interior point methods to the same combinatorial optimization problem one needs to have the explicit listing of all of the inequalities in the LP formulation, see [GLS88] and <ref> [GT89] </ref>. For instance, Goldfarb and Todd in their survey article on linear programming write: ..., it appears that its [Karmarkar's new algorithm] theoretical implications are far more limited than those of the ellipsoid method. <p> It is common belief that in contrast to the ellipsoid method, interior point methods require explicit knowledge of the facets of the polytope on which we wish to optimize, see for instance [GLS88] and the quotation from <ref> [GT89] </ref> in the introduction. However, we can use polynomial time interior point 27 methods to optimize over STAB G in the special cases mentioned above, even though the number of facets in such polytopes may be exponentially large.
Reference: [HJ85] <author> R. Horn and C. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: Note that P fl = P (this fact is direct consequence of Fejer's theorem in <ref> [HJ85] </ref>). G = (V; E) is a simple undirected graph without loops or multiple edges. A stable set S in G is a subset of vertices which are mutually nonadjacent. A clique K in G is a subset of vertices that are all mutually adjacent.
Reference: [Hur58] <author> L. Hurwicz. </author> <title> Programming in Linear Spaces. </title> <editor> In K. J. Arrow, L. Hurwicz, and H. Uzawa, editors, </editor> <title> Studies in Linear and Non-linear Programming, </title> <booktitle> II, </booktitle> <pages> pages 4-102. </pages> <publisher> Stanford University Press, Stanford, </publisher> <address> Ca, </address> <year> 1958. </year>
Reference-contexts: Duffin in [Duf56] was the first one to study such generalized duality theories. Later Hurwicz <ref> [Hur58] </ref>, Ben-Israel, Charnes and Kortanek [BICK69], Borwein and Wolkowicz [BW81b, BW81a], and Wolkowicz [Wol81] among others developed more general formulations of the duality theory. <p> We now state generalizations of Farkas' lemma. Such generalizations for arbitrary convex cones have been studied as early as 1958 by Hurwicz, <ref> [Hur58] </ref>. See [AN87] for references on the history and various extensions of Farkas' lemma to nonpolyhedral cones. Here we study the relevant forms of this lemma in the special case of SDP. It is not possible to generalize classical Farkas' lemma to nonpolyhedral cones without additional qualifications. <p> We may formulate and prove several other variants of Farkas' lemma in a similar vain, all of which are extensions of lemmas for the component-wise inequalities, as given for example in Schrijver's text [Sch86]. Related extensions for infinite programs have been studied in <ref> [Hur58] </ref> and [CK77], and in the case of matrix variables in [CM81].
Reference: [Jar91] <author> F. Jarre. </author> <title> An interior-point method for minimizing the maximum eigenvalue of a linear combination of matrices. </title> <type> Technical Report SOL-91-8, </type> <institution> Department of Operations Research, Stanford Univeristy, </institution> <month> June </month> <year> 1991. </year> <note> To appear in SIAM J. Control Optim. </note>
Reference-contexts: Furthermore, we argue that essentially any known interior point linear programming algorithm can also be transformed into an algorithm for SDP in a mechanical way; proofs of convergence and polynomial time computability extend in a similar fashion. Jarre in <ref> [Jar91] </ref> and Vandenberghe and Boyd in [VB93] later developed similar interior point algorithms for special forms of SDP. Polynomial time interior point methods for SDP have some interesting consequences for combinatorial optimization problems.
Reference: [Kar84] <author> N. Karmarkar. </author> <title> A New Polynomial-Time Algorithm for Linear Programming. </title> <journal> Combinator-ica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: As a special case, Nesterov and Nemirovskii show that linear programs with p inequality constraints, quadratic programs with p convex quadratic constraints and semidefinite programs over p fi p matrices all admit p-selfconcordant barriers. Therefore, the authors extend the revolutionary result of Karmarkar <ref> [Kar84] </ref> to a rather general class of convex programs. In this article we study interior point methods for semidefinite programs from an alternative point of view. Our work [Ali91] started somewhat later than, and independent of, that of [NN90]. <p> 2 2 [1 (X I)] Proof: In most interior-point linear programming algorithms it is shown that if kx 1k 1 &lt; 1 then n X ln x j (1 T x n) 2 (1 kx 1k 1 ) which is easily proved by expanding ln x, (see for example, Karmarkar <ref> [Kar84] </ref> or Ye [Ye91].) Now to prove the lemma simply substitute j (X) for x j . We use a projective transformation to bring the current iterate to the center, except that the center here is the identity matrix (in contrast with linear programming in which the center is 1). <p> It is not clear how factorization of A (X k X k )A T could be of any use in factoring A (X k+1 X k+1 )A T . 3. Karmarkar in <ref> [Kar84] </ref> gives a nice amortized method for updating factors of ADA T . He develops a technique where x k and x k+1 differ only in j k entries where P j k over all iterations is bounded by O ( n).
Reference: [KMY89] <author> M. Kojima, S. Mizuni, and A. Yoshise. </author> <title> A Primal-Dual Interior-Point Algorithm for Linear Programming. </title> <editor> In N. Megiddo, editor, </editor> <booktitle> Progress in Mathematical Programming. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Then, similar to linear programming, one can always transform the pair of primal and dual problems (2.2) to another pair for which initial interior feasible points are readily available. We extend the construction suggested by Kojima, Mizuno and Yoshise in <ref> [KMY89] </ref> which in turn is based on Megiddo's [Meg89]. 3 Since X, S and y are solution of the algebraic system of equations: XS = 0; AvecX = b and A T y + S = C, there are algebraic solutions among all optimal solutions of an SDP problem with integral <p> It is easy to see that if the optimal value of x 1 is not zero, then the original primal is infeasible (the proof is exactly like the one given in Kojima et al. in <ref> [KMY89] </ref>). Similarly if the optimal value of y 1 is not zero, then the original dual is infeasible. Otherwise, the optimal X fl and y fl are also optimal for the original primal and dual problems, respectively.
Reference: [Lov72] <author> L. Lovasz. </author> <title> Normal hypergraphs and the weak perfect graph conjecture. </title> <journal> Discrete Mathematics, </journal> <volume> 2, </volume> <year> 1972. </year>
Reference-contexts: Several interesting properties of perfect graphs should be noted. First, the perfect graph theorem of Lovasz indicates that a graph is perfect if and only if its complement is perfect, <ref> [Lov72] </ref>.
Reference: [Lov79] <author> L. Lovasz. </author> <title> On the shannon capacity of a graph. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 25(1), </volume> <month> January </month> <year> 1979. </year>
Reference-contexts: Semidefinite programs, however, are polynomial time solvable if an a priori bound on the size of their solution is known. This point was implicit in <ref> [Lov79] </ref> for a special instance of the SDP problem. It was proved in the work of Grotschel, Lovasz and Schrijver, [GLS81]. Polynomial time solvability of SDP is a direct consequence of the general results based on the ellipsoid method for convex programming. <p> Papers of Overton and Womersley [OW92] and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. Also Lovasz in <ref> [Lov79] </ref>, Grotschel, Lovasz and Schrijver [GLS81, GLS84, GLS88], and Shapiro in [Sha85] study more or less the same duality theory as we do, but their treatment is restricted to special forms of SDP. It is convenient to assume that C and A i in are symmetric. <p> However, in this case one can derive a slightly stronger result. Lovasz in <ref> [Lov79] </ref> discovered an invariant of graphs, (G; w), which has two desirable properties: first it is polynomial time computable, and second it is simultaneously an upper bound for !(G; w) and a lower bound for (G; w).
Reference: [LS91] <author> L. Lovasz and A. Schrijver. </author> <title> Cones of Matrices and Setfunctions, </title> <journal> and 0-1 Optimization. SIAM J. Optimization, </journal> <volume> 1(2), </volume> <year> 1991. </year>
Reference-contexts: We show that maximum stable sets for perfect graphs can be computed in randomized sublinear parallel time. Furthermore, based on the work of Lovasz and Schrijver <ref> [LS91] </ref>, we argue that in a branch and bound scheme for 2 0-1 programs interior point SDP algorithms may efficiently yield much sharper bounds than possible from linear programming relaxations of such problems. In section 2 we review the so called cone duality theory as specialized to semidefinite programs. <p> Generally such cuts may produce far better approximations than planar cuts. An ingenious approach for creating a class of nonlinear cuts has been proposed by Lovasz and Schrijver in <ref> [LS91] </ref>. The idea is to "lift" the space from vectors in &lt; n to n fi n symmetric matrices 5 . It is convenient to homogenize integer program by introducing a new variable x 0 as a multiple of b and then imposing the constraint x 0 = 1. <p> 2 ) is a mixed linear and semidefinite programming problem, and interior point techniques may be applied (as long as P is given by an explicit system of inequalities.) The process just described may be quite powerful in certain combinatorial 5 The presentation here is more restrictive than given in <ref> [LS91] </ref>. Lovasz and Schrijver do not assume that the matrix A is given explicitly. They only assume that the LP relaxation is endowed with a separation oracle. 25 optimization problems. <p> It turns out that (see <ref> [LS91] </ref>) STAB G N + (STAB G) Q-STAB G " C-STAB G " C-STAB G " W -STAB G E-STAB G and N + (STAB G) already provides sharper relaxation of STAB G than any of the polytopes defined above. <p> The stable set polytopes of such graphs have in general exponentially many facets. However, in <ref> [GLS88, LS91] </ref> it is shown that one can construct separation oracles for these polytopes and thus find the maximum stable set for the corresponding graphs in polynomial time. <p> m is an affine transformation mapping K on to A (K) then the following function is n-self-concordant for A (K): b + (y):= inffb (x) : x 2 A 1 (y) " Int Kg Now the theorem follows immediately from the definition of N + (STAB G) as given in <ref> [LS91] </ref> with the affine transformation A replaced by projection of elements of M + (STAB G) onto their diagonals. <p> For all such polytopes one can apply interior point methods and optimize over them in polynomial time. For a thorough discussion of liftings of polyhedra associated with combinatorial optimization problems consult <ref> [Yan88, LS91] </ref> and the references cited in them. It is an interesting problem to look for easily computable (for instance NC-computable or at least polynomial time computable) barriers for combinatorial optimization problems whose linear programming formulation contains exponentially many inequalities.
Reference: [MA88] <author> R. Monteiro and I. Adler. </author> <title> Polynomial-time primal-dual affine scaling algorithm for linear and convex quadratic programming and its power series extension. </title> <type> Technical Report ESRC 88-8, </type> <institution> Industrial Engineering and Operations Research Department, University of California-Berkeley, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: c 2 +1 T [Diag (x 0 )] 1 x X ! c 1 L 1 0 c 2 +trace L 1 0 XL T barrier function: barrier function: P ln x i ln det X norms: norms: kxk kXk kxk p ( j i (X)j p ) 1=p Adler <ref> [MA88] </ref>. This table itself may be summarized by the following rule: In any linear programming algorithm, replace any implicit or explicit reference to x i (or s i ) by a reference to i (X) (or i (S)).
Reference: [Meg89] <author> N. Megiddo. </author> <title> Pathways to the optimal set in linear programming. </title> <editor> In N. Megiddo, editor, </editor> <booktitle> Progress in Mathematical Programming. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Then, similar to linear programming, one can always transform the pair of primal and dual problems (2.2) to another pair for which initial interior feasible points are readily available. We extend the construction suggested by Kojima, Mizuno and Yoshise in [KMY89] which in turn is based on Megiddo's <ref> [Meg89] </ref>. 3 Since X, S and y are solution of the algebraic system of equations: XS = 0; AvecX = b and A T y + S = C, there are algebraic solutions among all optimal solutions of an SDP problem with integral input. 4 I am indebted to Joshi Ramana
Reference: [MVV87] <author> K. Mulmuley, U. V. Vazirani, and V. V. Vazirani. </author> <title> Matching is as Easy as Matrix Inversion. </title> <journal> Combinatorica, </journal> <pages> pages 105-131, </pages> <year> 1987. </year>
Reference-contexts: Therefore, testing this simultaneously for all vertices we get the set of vertices in the maximum clique. When we do not have uniqueness, we may use the randomized perturbation scheme of Mulmuley, Vazirani and Vazirani, <ref> [MVV87] </ref>. First recall their isolating lemma: Lemma 14 Let S = fx 1 ; ; x n g and F a family of subsets of S, that is F = fS 1 ; ; S N g. <p> Further, let elements of S be assigned integer weights chosen uniformly and independently at random from [1; 2n]. Then, Pr [There is a unique maximum weight set in F ] 1 : See <ref> [MVV87] </ref> for proof. To get a maximum clique in a perfect graph we follow a procedure similar to the one adopted by Mulmuley, Vazirani and Vazirani for constructing the minimum weighted perfect matching in graphs.
Reference: [NM90] <author> G. Narasimhan and R. Manber. </author> <title> A Generalization of Lovasz's # Function. </title> <editor> In W. Cook, and P. D. Seymour, editor, </editor> <booktitle> Polyhedral Combinatorics, volume 1 of DIMACS series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <pages> pages 19-27. </pages> <publisher> American Mathematical Society and Association for Computing Machinery, </publisher> <year> 1990. </year>
Reference-contexts: In fact, as mentioned in the last subsection, one can compute an n-self-concordant barrier for this polytope in polynomial time. 5.3 The maximum induced k-partite subgraph problem. In <ref> [NM90] </ref> G. Narasimhan and R. Manber generalized the concept of the Lovasz number of graphs as follows: Let ff k (G) be the size of the largest induced k-partite subgraph in G. Recall that (G) is the minimum number of cliques that can cover all vertices of G.
Reference: [NN90] <author> Y. Nesterov and A. Nemirovskii. </author> <title> Self-Concordant Functions and Polynomial Time Methods in Convex Programming. </title> <address> Moscow, </address> <year> 1990. </year>
Reference-contexts: The ellipsoid method, however, has not proven practical in most applications, including SDP. A more recent development is the possibility of using interior point methods to obtain polynomial time algorithms for semidefinite programs. The earliest work in this direction to our knowledge is that of 1 Nesterov and Nemirovskii <ref> [NN90] </ref>. In this important work the authors develop a general approach for using interior point methods for solving convex programming problems which is based on the concept of p-selfconcordant barrier functions. See the more recent [NN92] for a complete treatment of this subject. <p> Therefore, the authors extend the revolutionary result of Karmarkar [Kar84] to a rather general class of convex programs. In this article we study interior point methods for semidefinite programs from an alternative point of view. Our work [Ali91] started somewhat later than, and independent of, that of <ref> [NN90] </ref>. Nesterov and Nemirovskii obtain their complexity theorems by specializing their general results to SDP. We, on the other hand, take a specific interior point algorithm for linear programming (i.e Ye's projective potential reduction method [Ye90]) and extend it to SDP. <p> Proof: Nesterov and Nemirovskii prove that ln det X is n-selfconcordant for the cone of positive semidefinite n fi n matrices. (See <ref> [NN90] </ref> for definitions). They also show that existence of an n-self-concordant barrier for a convex set in general implies that one can optimize a linear function over that set with every O ( p n) iterations yielding a significant bit.
Reference: [NN92] <author> Y. Nesterov and A. Nemirovskii. </author> <title> Interior Point Polynomial Methods in Convex Programming: </title> <journal> Theory and Applications. </journal> <note> SIAM, 1992. To be published. </note>
Reference-contexts: In this important work the authors develop a general approach for using interior point methods for solving convex programming problems which is based on the concept of p-selfconcordant barrier functions. See the more recent <ref> [NN92] </ref> for a complete treatment of this subject. Nesterov and Nemirovskii show that for any convex set K that is endowed with a p-selfconcordant barrier function, there is an interior point algorithm which optimizes a linear function on K. <p> It turns out that at least for SDP, cone duality, which is a generalization of linear programming duality, is most appropriate for interior point methods (this point of view is also expressed in the latest edition of Nesterov and Nemirovskii <ref> [NN92] </ref>). In section 3 we develop an interior point algorithm which, as we mentioned, is a direct extension of Ye's projective potential reduction method. Furthermore, we propose a recipe to extend mechanically most known interior point algorithms for LP into similar algorithms for SDP. <p> It is worth mentioning that [AN87] study the duality theory from the point of view of basic feasible solutions and extend the "tableau based" proofs of LP duality. The latest version of Nesterov and Nemirovskii's text <ref> [NN92] </ref> also treats cone duality for the general convex programs. Papers of Overton and Womersley [OW92] and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. <p> They also show that existence of an n-self-concordant barrier for a convex set in general implies that one can optimize a linear function over that set with every O ( p n) iterations yielding a significant bit. Furthermore, in Proposition 1.5, pp. 121 of <ref> [NN92] </ref> they show that if a convex set K &lt; n is endowed with an n-self-concordant barrier b, and A : &lt; n ! &lt; m is an affine transformation mapping K on to A (K) then the following function is n-self-concordant for A (K): b + (y):= inffb (x) :
Reference: [Ove88] <author> M. L. Overton. </author> <title> On minimizing the maximum eigenvalue of a symmetric matrix. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 9(2), </volume> <year> 1988. </year> <month> 36 </month>
Reference-contexts: Also Fletcher studied a similar problem from the point of view of nondifferentiable optimization. In particular, he derives some expressions for the subgradients of the sum of the first few eigenvalues of a symmetric matrix and formulates optimality conditions for this problem. In the same spirit as Fletcher, Overton <ref> [Ove88] </ref> studies the largest eigenvalue of a symmetric matrix as a convex, but nondifferentiable function. Based on earlier work [FNO87], in [Ove88] he derives a quadratically convergent algorithm for the problem of minimizing the largest eigenvalue of an affinely constrained matrix. <p> In the same spirit as Fletcher, Overton <ref> [Ove88] </ref> studies the largest eigenvalue of a symmetric matrix as a convex, but nondifferentiable function. Based on earlier work [FNO87], in [Ove88] he derives a quadratically convergent algorithm for the problem of minimizing the largest eigenvalue of an affinely constrained matrix.
Reference: [Ove92] <author> M. L. Overton. </author> <title> Large-scale optimization of eigenvalues. </title> <journal> SIAM J. Optimization, </journal> <volume> 2(1) </volume> <pages> 88-120, </pages> <year> 1992. </year>
Reference-contexts: Based on earlier work [FNO87], in [Ove88] he derives a quadratically convergent algorithm for the problem of minimizing the largest eigenvalue of an affinely constrained matrix. This work is further extended in <ref> [Ove92] </ref> where both second order methods based on sequential quadratic programming, and first order methods based on sequential linear programming for large scale problems are developed.
Reference: [OW91] <author> M. L. Overton and R. S. Womersley. </author> <title> Optimality conditions and duality theory for minimizing sums of the largest eigenvalues of symmetric matrices. </title> <type> Report 556, </type> <institution> NYU Computer Science Department, </institution> <month> June </month> <year> 1991. </year> <note> To appear in Math. Prog. </note>
Reference-contexts: The oldest form of semidefinite programming is the evaluation of eigenvalues of a symmetric matrix. In fact, one can reformulate the classical theorems of Rayleigh-Ritz for the largest eigenvalue, and of Fan for the sum of the first few eigenvalues of a symmetric matrix, as semidefinite programs, see <ref> [OW91, OW92] </ref> and section 4 below. However, for these special cases, techniques of this paper do not seem to be appropriate as there exist better algorithms from both theoretical and practical points of view. <p> In this section we also go over some differences between SDP and LP as far as interior point methods and polynomial time algorithms in general are concerned. In section 4 we build on the results of Overton and Womersley <ref> [OW91, OW92] </ref> and derive semidefinite programming formulation for various eigenvalue optimization problems. We also state complementary slackness results for these problems. Finally, in section 5 we study some applications of SDP interior point methods to various combinatorial optimization problems. <p> (X) + + k (X) : AvecX = bg: (38) and k X i (A (x)) where A (x) = A 0 + m X x i A i (39) To show that these problems are indeed semidefinite programs, we use the following elegant charac terization by Overton and Womersley <ref> [OW91, OW92] </ref>. Theorem 5 For the sum of the first k eigenvalues of a symmetric matrix A the following semidefinite programming characterization holds: 1 (A) + + k (A) = max A * U s:t: trace U = k 0 U I Proof: See Overton and Womersley [OW91, OW92]. <p> Overton and Womersley <ref> [OW91, OW92] </ref>. Theorem 5 For the sum of the first k eigenvalues of a symmetric matrix A the following semidefinite programming characterization holds: 1 (A) + + k (A) = max A * U s:t: trace U = k 0 U I Proof: See Overton and Womersley [OW91, OW92]. It is worth mentioning that this result is based on a beautiful convex hull characterization which was known at least as early as 1971, see [FW71], but unfortunately has remained somewhat obscure. <p> if (z fl i (m i m i+1 )X fl i = (z fl i + (m i m i+1 )X fl i = (I Y fl i = (I W fl i = 0 The characterization (4.40), and the max part of (4.50) were given in Overton and Womersley <ref> [OW91] </ref>. Also, Fletcher in [Fle85] derives a closely related result to (4.40) but the result was incorrect (Fletcher had 0 S rather than 0 S I.) The min characterizations as well as the primal and dual formulation of the variants with equality constraints, we believe are new.
Reference: [OW92] <author> M. L. Overton and R. S. Womersley. </author> <title> On the sum of the largest eigenvalues of a symmetric matrix. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 41-45, </pages> <year> 1992. </year>
Reference-contexts: The oldest form of semidefinite programming is the evaluation of eigenvalues of a symmetric matrix. In fact, one can reformulate the classical theorems of Rayleigh-Ritz for the largest eigenvalue, and of Fan for the sum of the first few eigenvalues of a symmetric matrix, as semidefinite programs, see <ref> [OW91, OW92] </ref> and section 4 below. However, for these special cases, techniques of this paper do not seem to be appropriate as there exist better algorithms from both theoretical and practical points of view. <p> In this section we also go over some differences between SDP and LP as far as interior point methods and polynomial time algorithms in general are concerned. In section 4 we build on the results of Overton and Womersley <ref> [OW91, OW92] </ref> and derive semidefinite programming formulation for various eigenvalue optimization problems. We also state complementary slackness results for these problems. Finally, in section 5 we study some applications of SDP interior point methods to various combinatorial optimization problems. <p> The latest version of Nesterov and Nemirovskii's text [NN92] also treats cone duality for the general convex programs. Papers of Overton and Womersley <ref> [OW92] </ref> and Fletcher [Fle85] treat duality theory for the eigenvalue optimization problem from the subdifferential point of view. Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. <p> (X) + + k (X) : AvecX = bg: (38) and k X i (A (x)) where A (x) = A 0 + m X x i A i (39) To show that these problems are indeed semidefinite programs, we use the following elegant charac terization by Overton and Womersley <ref> [OW91, OW92] </ref>. Theorem 5 For the sum of the first k eigenvalues of a symmetric matrix A the following semidefinite programming characterization holds: 1 (A) + + k (A) = max A * U s:t: trace U = k 0 U I Proof: See Overton and Womersley [OW91, OW92]. <p> Overton and Womersley <ref> [OW91, OW92] </ref>. Theorem 5 For the sum of the first k eigenvalues of a symmetric matrix A the following semidefinite programming characterization holds: 1 (A) + + k (A) = max A * U s:t: trace U = k 0 U I Proof: See Overton and Womersley [OW91, OW92]. It is worth mentioning that this result is based on a beautiful convex hull characterization which was known at least as early as 1971, see [FW71], but unfortunately has remained somewhat obscure. <p> For an historical account of this result, its connection to the well-known, but computationally less useful theorem of Ky Fan, and interesting connections to the theorem of Birkhoff and Von Neumann concerning the convex hull of doubly stochastic matrices, refer to Overton and Womersley <ref> [OW92] </ref>. Now to express (4.38) as a semidefinite program we first derive another characterization of sum of the first k eigenvalues of A, by taking the dual of (4.40).
Reference: [PR91] <author> S. Poljak and F. Rendl. </author> <title> Solving the max-cut problem using eigenvalues. </title> <type> Technical Report 91735-OR, </type> <institution> Forschungsinstitut Fur Diskrete Mathematik, Institut Fur okonometrie und Operations Research, Rheinische Friedrich-Wilhelms-Universitat, Bonn, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: In <ref> [DP90, PR91] </ref> the following SDP bound is proposed: minf n 1 (A + Diag (x)) : 1 T x = ag MC (G) (76) where MC (G) is the size of maximum cut in G. (5.76) is equivalent to primal-dual pair: min z + (1=n)1 T x s.t. zI Diag (x)
Reference: [PR92] <author> S. Poljak and F. Rendl. </author> <title> Nonpolyhedral relaxations of graph-bisection problems. </title> <type> Technical Report 92-55, </type> <institution> DIMACS, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: For related treatment of maximum cut and graph bisection porblems see <ref> [PR92] </ref>. Acknowledgment Discussions with Stephen Boyd, Don Knuth, Laszlo Lovasz, Yuri Nesterov, Arkadii Nemirovskii, Joshi Ramana, Rob Womersley and particularly Michael Overton and Yinyu Ye, have been most useful in preparing this article and removing several errors in the preliminary versions.
Reference: [Roc70] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference-contexts: Taking the polar we have f0g = P " L ? Here L ? is the set fX : A vecX = 0g. Hence we have that X = 0 is the only solution of the system A vecX = 0, and X - 0 and <ref> [Roc70] </ref>, Theorem 9.1, p. 73 implies that K 1 is closed. <p> lemma 2.2 K 1 is a closed cone and thus there must exist a hyperplane, specifically a linear half-space, that separates b and K 1 , i.e. there exists some vector y such that b T y &lt; 0 and (AvecX) T y 0 for all X - 0, see <ref> [Roc70] </ref>, Theorem 11.7, pp.100. But this means that X * Mat (A T y) 0 for all X - 0, which is equivalent to Mat (A T y) - 0, and therefore the if part of the theorem is proved.
Reference: [RW93] <author> F. Rendl and H. Wolkowicz. </author> <title> A projection technique for partitioning the nodes of a graph. </title> <type> Technical Report CORR Report 90-20, </type> <institution> Department of Combinatorics and Optimization, University of Waterloo, Waterloo, </institution> <address> Ontario, N2L 3G1, Canada, </address> <year> 1993. </year>
Reference-contexts: Combining the SDP formulation of Hoffman and Donath, favorable average case analysis of Boppana, and the interior point technique developed in this paper may result in an effective and practical method for solving this problem. For generalizations of these ideas see <ref> [RW93] </ref>. Related to the graph bisection problem is the maximum cut problem: partition the nodes of the graph into two sets such that the number of edges with endpoints on different sets is maximum.
Reference: [Sch86] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <year> 1986. </year>
Reference-contexts: Hence we have that X = 0 is the only solution of the system A vecX = 0, and X - 0 and [Roc70], Theorem 9.1, p. 73 implies that K 1 is closed. Now we state the most common form of Farkas' lemma as given in Schrijver's text <ref> [Sch86] </ref>, and as extended to the positive semidefinite cone: 1 Alternative extensions without closedness assumption are treated in [BW81b, BW81a, Wol81] 5 Lemma 3 Extended Farkas' lemma: Let b 2 &lt; m and A 2 &lt; mfin 2 be a matrix such that its rows A T i: = vecA i <p> We may formulate and prove several other variants of Farkas' lemma in a similar vain, all of which are extensions of lemmas for the component-wise inequalities, as given for example in Schrijver's text <ref> [Sch86] </ref>. Related extensions for infinite programs have been studied in [Hur58] and [CK77], and in the case of matrix variables in [CM81].
Reference: [Sha85] <author> A. Shapiro. </author> <title> Extermal problems on the set of nonnegative definite matrices. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 67 </volume> <pages> 7-18, </pages> <year> 1985. </year>
Reference-contexts: Such an approach is related to the Kuhn-Tucker duality theory and relies on derivatives or subgradients. Also Lovasz in [Lov79], Grotschel, Lovasz and Schrijver [GLS81, GLS84, GLS88], and Shapiro in <ref> [Sha85] </ref> study more or less the same duality theory as we do, but their treatment is restricted to special forms of SDP. It is convenient to assume that C and A i in are symmetric. There is no loss of generality in this assumption. <p> Since by weak duality lemma we have that z 2 z 1 we conclude that z 2 = z 1 . It is also possible to derive a "complementary slackness" theorem. In fact, Grotschel, Lovasz and Schrijver in [GLS84] and Shapiro in <ref> [Sha85] </ref> mention the complementary slackness theorem for a more restricted form of SDP. Note that when the strong duality theorem is true and both primal and dual problems are bounded and feasible then the duality gap X * S vanishes.
Reference: [Sub93] <author> S. Subramani. </author> <title> Sums of Singular Values. </title> <type> Master's thesis, </type> <institution> School of Mathematic, University of New South Wales, </institution> <address> Kensignton, Australia, </address> <year> 1993. </year>
Reference-contexts: Similar formulations can be derived for maximizing (weighted) sums of the last few smallest eigen-values of symmetric matrices or the sum of the first few largest singular values of an arbitrary matrix; we omit these formulations here, see <ref> [Sub93] </ref>. However, maximizing the last few smallest eigenvalues of a symmetric matrix absolute-value-wise, or sum of the last few smallest singular values of an arbitrary matrix cannot be formulated as SDP because these problems are not convex programs. 5 Applications in combinatorial optimization.
Reference: [VB93] <author> L. Vandenberghe and S. Boyd. </author> <title> Primal-dual potential reduction method for problems involving matrix inequalities. </title> <type> Technical report, </type> <institution> Information Systems Laboratory, Department of Electrical Engineering, Stanford University, Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1993. </year> <note> To appear in Math. Prog. </note>
Reference-contexts: Specifically if the condition that X is a diagonal matrix is added to the constraint set then (1.1) reduces to linear programming. Semidefinite programs arise in a wide variety of applications from control theory (see <ref> [VB93] </ref> and [Fan93]) to combinatorial optimization (see section 5 below) and even structural computational complexity theory (see [FL92]). The oldest form of semidefinite programming is the evaluation of eigenvalues of a symmetric matrix. <p> Furthermore, we argue that essentially any known interior point linear programming algorithm can also be transformed into an algorithm for SDP in a mechanical way; proofs of convergence and polynomial time computability extend in a similar fashion. Jarre in [Jar91] and Vandenberghe and Boyd in <ref> [VB93] </ref> later developed similar interior point algorithms for special forms of SDP. Polynomial time interior point methods for SDP have some interesting consequences for combinatorial optimization problems.
Reference: [Wol81] <author> H. Wolkowicz. </author> <title> Some applications of optimization in matrix theory. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 40 </volume> <pages> 101-118, </pages> <year> 1981. </year>
Reference-contexts: Duffin in [Duf56] was the first one to study such generalized duality theories. Later Hurwicz [Hur58], Ben-Israel, Charnes and Kortanek [BICK69], Borwein and Wolkowicz [BW81b, BW81a], and Wolkowicz <ref> [Wol81] </ref> among others developed more general formulations of the duality theory. For a comprehensive treatment of generalized duality theory from the point of view of infinite dimensional linear programs, see the text of Anderson and Nash [AN87] and for alternative extensions refer to [BW81b, BW81a]. <p> Now we state the most common form of Farkas' lemma as given in Schrijver's text [Sch86], and as extended to the positive semidefinite cone: 1 Alternative extensions without closedness assumption are treated in <ref> [BW81b, BW81a, Wol81] </ref> 5 Lemma 3 Extended Farkas' lemma: Let b 2 &lt; m and A 2 &lt; mfin 2 be a matrix such that its rows A T i: = vecA i where A i are symmetric for i = 1; ; m. <p> Related extensions for infinite programs have been studied in [Hur58] and [CK77], and in the case of matrix variables in [CM81]. In all of these extensions we need to assume either some closedness criteria, or the dual problem must be modified by cones other than P (as in <ref> [Wol81] </ref>, for instance.) We mention a few more: Lemma 4 Let A 2 &lt; n 2 fim be a matrix whose columns are linearly independent and are of the form vecA i for symmetric A i , and B 2 &lt; nfin 2 .
Reference: [Yan88] <author> M. Yannakakis. </author> <title> Expressing combinatorial optimzation problems by linear programs. </title> <booktitle> In Proc. 29th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 223-228, </pages> <year> 1988. </year>
Reference-contexts: For all such polytopes one can apply interior point methods and optimize over them in polynomial time. For a thorough discussion of liftings of polyhedra associated with combinatorial optimization problems consult <ref> [Yan88, LS91] </ref> and the references cited in them. It is an interesting problem to look for easily computable (for instance NC-computable or at least polynomial time computable) barriers for combinatorial optimization problems whose linear programming formulation contains exponentially many inequalities. <p> This problem is especially interesting because Yannakakis shows that under certain symmetry preserving conditions on the lift operator it is impossible to lift the matching polytope to a higher dimensional polytope with polynomially many facets, <ref> [Yan88] </ref>. Whether the matching polytope can be lifted to a convex set endowed with an O (m)-self-concordant barrier remains open. 5.2 Maximum cliques in perfect graphs. A particularly nice application of semidefinite programming is to the solution of the maximum clique problem in perfect graphs.
Reference: [Ye90] <author> Y. Ye. </author> <title> A class of projective transformations for linear programming. </title> <journal> SIAM J. Comput., </journal> <volume> 19(3), </volume> <year> 1990. </year>
Reference-contexts: Our work [Ali91] started somewhat later than, and independent of, that of [NN90]. Nesterov and Nemirovskii obtain their complexity theorems by specializing their general results to SDP. We, on the other hand, take a specific interior point algorithm for linear programming (i.e Ye's projective potential reduction method <ref> [Ye90] </ref>) and extend it to SDP. Furthermore, we argue that essentially any known interior point linear programming algorithm can also be transformed into an algorithm for SDP in a mechanical way; proofs of convergence and polynomial time computability extend in a similar fashion. <p> Our development closely follows Ye's projective technique for linear programming <ref> [Ye90] </ref>. <p> The remarkable similarity between the algorithm presented here and Ye's LP algorithm in <ref> [Ye90] </ref> suggests that other LP interior point methods may also be extended to SDP problems. All proofs of convergence and polynomial-time complexity may be extended as well. The correspondence is summarized in figure 2.
Reference: [Ye91] <author> Y. Ye. </author> <title> An O(n 3 L) Potential Reduction Algorithm for Linear Programming. </title> <journal> Math. Prog., </journal> <volume> 50(2), </volume> <year> 1991. </year>
Reference-contexts: (X I)] Proof: In most interior-point linear programming algorithms it is shown that if kx 1k 1 &lt; 1 then n X ln x j (1 T x n) 2 (1 kx 1k 1 ) which is easily proved by expanding ln x, (see for example, Karmarkar [Kar84] or Ye <ref> [Ye91] </ref>.) Now to prove the lemma simply substitute j (X) for x j . We use a projective transformation to bring the current iterate to the center, except that the center here is the identity matrix (in contrast with linear programming in which the center is 1). <p> Proofs of convergence or polynomial time complexity may also be extended mechanically in the same manner. We have already verified this claim on the approaches of Gonzaga [Gon89], Ye <ref> [Ye91] </ref> (see [Ali92]), and Monteiro and 18 LP SDP unknown vector: x unknown symmetric matrix: X inequality constraints: Lowner constraints: - dual variable: y dual variable: y dual slack vector: s dual slack symmetric matrix: S 1 I linear scaling: linear scaling: x ! (x i =(x 0 ) i )
References-found: 60

