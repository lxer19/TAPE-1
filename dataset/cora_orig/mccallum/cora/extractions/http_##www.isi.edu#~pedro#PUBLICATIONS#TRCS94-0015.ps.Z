URL: http://www.isi.edu/~pedro/PUBLICATIONS/TRCS94-0015.ps.Z
Refering-URL: http://www.isi.edu/~pedro/PUBLICATIONS/SIAM95a.html
Root-URL: http://www.isi.edu
Email: fpedro,tyangg@cs.ucsb.edu.  
Title: Efficient Parallelization of Relaxation Iterative Methods for Banded Linear Systems  
Author: Pedro Diniz and Tao Yang 
Keyword: Partitioning, Scheduling, SOR, Banded Linear Systems.  
Note: AMS(MOS) subject classifications: 68M20; 65Y05, 65F10. The work of the first author was supported by JNICT Junta Nacional de Investiga~c~ao Cient ifica e Tecnologica and by the Fulbright program. The second author is supported by NSF-CCR-9409695 and a startup fund from UCSB.  
Date: December 1994  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California at Santa Barbara  
Abstract: In this paper we present an efficient parallel implementation of relaxation iterative methods, such as the Gauss-Seidel (GS) and Successive-Over-Relaxation (SOR), for solving banded linear systems on distributed memory machines. We introduce a novel partitioning and scheduling scheme in our implementation which allows perfect overlapping of computation with communication, hence minimizing latency effects. We provide analytic and experimental results to verify the effectiveness of our approach and discuss its incorporation in other iterative solvers. Experiments on nCUBE and Intel Paragon machines for several sample banded matrices show that the proposed approach yields good performance on these machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adams and H. Jordan, </author> <title> Is SOR color-blind?, </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 7 (1986), </volume> <pages> pp. 490-506. </pages>
Reference-contexts: A possible approach, as taken in the multi-coloring scheme, is to eliminate some of these dependencies, is to perform variable relabeling (coloring) [9]. This scheme increases the amount of exploitable parallelism since nodes assigned the same color can be computed concurrently. Adams and Jordan <ref> [1] </ref> have shown these methods to be numerically equivalent to performing several SOR iterations computed simultaneously in the absence of convergence tests. However, in order to preserve inter-color data dependencies, processors must communicate between each computation phase associated with each color. <p> The number of such variable per processor is about n (h+1)p . There are (h + 1) communication steps yielding a parallel time per iteration given by equation 4. T multicoloring par = p n ):(4) Adams and Jordan <ref> [1] </ref> have shown multicoloring methods to be equivalent to overlapping multiple SOR iterations. These methods are effective if the communication overhead is negligible. In the presence of non-zero communication costs, as is the case of current message-passing machines [3], multicoloring methods exhibit significant communication costs making it less effective.
Reference: [2] <author> R. Barrett et. al, </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, </title> <publisher> SIAM Publications. </publisher> <address> Philadelphia, PA, </address> <year> 1994 </year>
Reference-contexts: 1 Introduction Iterative methods can be used to solve large banded linear systems on concurrent processors. These methods range from the complex Conjugate-Gradient (CG) with preconditioning to the simple Successive-Over-Relaxation (SOR). Due to its simplicity SOR methods are still used in solving practical engineering problems <ref> [2, 5] </ref>. In addition, they can also be valuable in obtaining quick approximation of the solution in intermediate steps for more powerful methods (e.g. preconditioned Conjugate-Gradient) or even replace direct methods in applications parallelization [10]. In this paper we concentrate on the implementation of the SOR method on message-passing multiprocessors.
Reference: [3] <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: T multicoloring par = p n ):(4) Adams and Jordan [1] have shown multicoloring methods to be equivalent to overlapping multiple SOR iterations. These methods are effective if the communication overhead is negligible. In the presence of non-zero communication costs, as is the case of current message-passing machines <ref> [3] </ref>, multicoloring methods exhibit significant communication costs making it less effective. Experimental results presented in section 6 for tested matrix systems do show communication costs to be significant. 3.4 A New Pipelined Approach From the discussion in the previous two sections one can draw the following remarks.
Reference: [4] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon and D. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1988. </year>
Reference: [5] <author> G. Huang and W. Ongsakol, </author> <title> An Efficient Task Allocation Algorithm and its use to Paral-lelize Irregular Gauss-Seidel Type Algorithms, </title> <booktitle> In Proc. of the Eighth International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <year> (1994), </year> <pages> pp. 497-501. </pages>
Reference-contexts: 1 Introduction Iterative methods can be used to solve large banded linear systems on concurrent processors. These methods range from the complex Conjugate-Gradient (CG) with preconditioning to the simple Successive-Over-Relaxation (SOR). Due to its simplicity SOR methods are still used in solving practical engineering problems <ref> [2, 5] </ref>. In addition, they can also be valuable in obtaining quick approximation of the solution in intermediate steps for more powerful methods (e.g. preconditioned Conjugate-Gradient) or even replace direct methods in applications parallelization [10]. In this paper we concentrate on the implementation of the SOR method on message-passing multiprocessors.
Reference: [6] <author> V. Kumar, A. Grama, A. Gupta, G. Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <address> Benjamin-Cummings, </address> <year> 1994. </year>
Reference-contexts: + fih) = 2 (h + 1)(r + h)c + 2ff + 2fih:(6) Hence Speedup = (h + 2)n c + fi :(7) By approximating (h + 2) (h + 1) h we have Speedup = p n (h + ff c ) 10 Thus an overhead and isoefficiency functions <ref> [6] </ref> T overhead (W; p) = p (h + hc fi )(9) where W = n fi w stands for the total amount of work of each SOR iteration.
Reference: [7] <author> R. Leland and J. Rollett, </author> <title> Evaluation of a Parallel Conjugate Gradient Algorithm, In Numerical Methods in Fluid Dynamics III, </title> <editor> Ed. K. Morton and M. Baines, </editor> <publisher> Oxford Univ. Press, </publisher> <year> 1988. </year>
Reference-contexts: As an example, iterative methods like the CG cannot benefit from this approach. However it is possible to take advantage of overlapping several CG iterations as described by Leland <ref> [7, 8] </ref>. The main obstacle to parallelization in this method is due to the synchronization caused by the dot products.
Reference: [8] <author> Robert W. Leland, </author> <title> The Effectiveness of Parallel Iterative Algorithms for Solution of Large Sparse Linear Systems, </title> <type> PhD. Thesis, </type> <institution> Oxford University 1989. </institution>
Reference-contexts: As an example, iterative methods like the CG cannot benefit from this approach. However it is possible to take advantage of overlapping several CG iterations as described by Leland <ref> [7, 8] </ref>. The main obstacle to parallelization in this method is due to the synchronization caused by the dot products.
Reference: [9] <author> James M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <publisher> Plenum Press, </publisher> <year> 1988. </year>
Reference-contexts: Jacobi method), as iterates values depend on computed values of the same iteration. A possible approach, as taken in the multi-coloring scheme, is to eliminate some of these dependencies, is to perform variable relabeling (coloring) <ref> [9] </ref>. This scheme increases the amount of exploitable parallelism since nodes assigned the same color can be computed concurrently. Adams and Jordan [1] have shown these methods to be numerically equivalent to performing several SOR iterations computed simultaneously in the absence of convergence tests. <p> The labeling of the variables is the key point factor, yielding matrices with diagonal blocks <ref> [9] </ref>. The unknowns associated with these diagonal blocks are assigned the same color and can be computed in parallel. After global exchange of the values of variables of a given color, another swap for variable with another color can continue, implicitly overlapping multiple SOR iterations.
Reference: [10] <author> Jaswinder Singh and John Hennessy, </author> <title> Finding and Exploiting Parallelism in an Ocean Simulation Program: Experience, Results and Implications, </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <volume> vol. 15, no. 1, </volume> <year> (1992), </year> <pages> pp. 27-58. </pages>
Reference-contexts: Due to its simplicity SOR methods are still used in solving practical engineering problems [2, 5]. In addition, they can also be valuable in obtaining quick approximation of the solution in intermediate steps for more powerful methods (e.g. preconditioned Conjugate-Gradient) or even replace direct methods in applications parallelization <ref> [10] </ref>. In this paper we concentrate on the implementation of the SOR method on message-passing multiprocessors. The main difficulty in efficiently parallelizing the SOR methods stems from the intra-iteration data dependencies (cf. Jacobi method), as iterates values depend on computed values of the same iteration. <p> Under this scheme, scalar inversion would be replaced by a matrix solver and each x i would represent a set of unknowns by a diagonal matrix. This blocking scheme has been used in previous research (e.g. <ref> [10] </ref>) as a mean to improve the locality, both temporal and spatial, of memory accesses, in order to take advantage of memory hierarchy organization. Each SOR iteration can be viewed as a set of tasks to be executed with precedence constraints 3 arising from the underlying data dependences.
Reference: [11] <author> Xian-He Sun and Diane T. </author> <title> Rover, Scalability of Parallel Algorithm-Machine Combinations, </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 6, </volume> <year> (1994), </year> <pages> pp. 599-613. </pages>
Reference-contexts: We verify the scalability of our approach in two settings, namely for a fixed problem size and for constant amount of work per processor, i.e. r fi band = K. This is known as Scaled-Speedup <ref> [11] </ref>.
Reference: [12] <author> David Young, </author> <title> Iterative Solution of Large Linear Systems, </title> <publisher> Academic Press, </publisher> <year> 1971. </year> <month> 14 </month>
Reference-contexts: a ij = 0 for j &lt; 1 or j &gt; n. x k+1 i + a ii i1 X a ij x k+1 i+h X a ij x k Convergence conditions for the SOR as well as for the special case of Gauss-Seidel methods are extensively analyzed by Young <ref> [12] </ref> and thus omitted here. From the above formulation several remarks are in order: 1.
References-found: 12

