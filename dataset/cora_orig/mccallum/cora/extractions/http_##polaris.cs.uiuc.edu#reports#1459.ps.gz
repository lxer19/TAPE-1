URL: http://polaris.cs.uiuc.edu/reports/1459.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: lchoi@csrd.uiuc.edu yew@cs.umn.edu  
Phone: (217) 333-0969 (612) 625-7387  
Title: Compiler and Hardware Support for Cache Coherence in Large-Scale Multiprocessors: Design Considerations and Performance Evaluation  
Author: Lynn Choi Pen-Chung Yew 
Keyword: cache coherence, memory systems, performance evaluation, compilers, shared-memory multiprocessors  
Address: Urbana, IL 61801-1351 Minneapolis, MN 55455-0519  
Affiliation: Center for Supercomputing R D Department of Computer Science University of Illinois University of Minnesota  
Abstract: In this paper, we study a hardware-supported, compiler-directed (HSCD) cache coherence scheme which relies mostly on compiler analysis, but which also needs a reasonable amount of hardware support. Such a scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors such as the Cray T3D, and can be adapted to various cache organizations including multi-word cache lines and byte-addressable architectures. Several system related issues, including critical sections, threads with inter-thread communication, and task migration have also been addressed. The cost of the required hardware support is small and proportional to the cache size. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris paralleling compiler [31]. From our simulation study using the Perfect Club benchmarks [5], we found that in spite of conservative decisions made by the compiler, the performance of the proposed HSCD scheme can be comparable to that of a full-map hardware directory scheme. With its comparable performance and reduced hardware cost, the proposed scheme can be a viable alternative for large-scale multiprocessors such as the Cray T3D, which rely on users to maintain data coherence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, V. S. Adve, M. D. Hill, and M. K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Lilja [26] compared the performance of the version control scheme [14] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes. Agarwal and Adve <ref> [1] </ref> used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> Proceedings of Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <year> 1991. </year>
Reference-contexts: character, integer, or floating point data) is a distinct variable analyzed by the compiler. 6 Our experimental results in Section 4 show that a 4-bit or 8-bit timetag is large enough to achieve a very good performance. 9 Full-map Directory [8] LimitLess Directory (DIR NB i ) Two-phase Invalidation (bits) <ref> [2] </ref> (bits) (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead (DRAM) (P+2)*M*P (i+2)*M*P None Total (P = 1024, i = 10) 4MB SRAM / 64.5GB DRAM 4MB SRAM / 3GB DRAM 64MB SRAM only following parameters: P : number of processors, L : number of words in a memory block, <p> Figure 5 compares the storage overhead of our TPI scheme with a full-map directory scheme [8] as well as the LimitLess directory scheme <ref> [2] </ref>. It shows both the cache and the memory overhead in terms of the cache line size (L), the node cache size (C), the node memory size (M) and the number of processors (P).
Reference: [3] <author> J. Archibald and J. Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> Proceedings of The 11th Annual International Symposium on Computer Architectur, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Full-Map Directory Scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [8, 3] </ref>. It gives a performance comparison to the hardware directory protocols. and compiler support required, the caching strategy for shared data, and their performance limitations. 10 Miss rates Figure 11 shows the miss rates of each scheme for the six benchmarks with a 64KB direct-mapped cache.
Reference: [4] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Representation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We use the demand-driven symbolic analysis using the GSA form <ref> [4] </ref>. * Interprocedural analysis Previous HSCD schemes invalidate the entire cache at procedure boundaries to avoid side effects caused by procedure calls. We use a complete interprocedural analysis to avoid such invalidations and to exploit locality across proce dure boundaries. <p> First, we construct a procedure call graph. Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis is based on the following steps. First, we transform the source program into a GSA form <ref> [4] </ref>. Then, we construct a modified flow graph, called the epoch flow graph [21]. It contains the epoch boundary information as well as the control flows of the program.
Reference: [5] <author> M. Berry and others. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall, </month> <year> 1989. </year>
Reference-contexts: This difference will create less write traffic in the hardware directory schemes than in the TPI scheme. 4 Performance Evaluation 4.1 Experimental Methodology In this section, we describe our simulation study to evaluate our proposed scheme. We use six programs from the Perfect Club benchmark suite <ref> [5] </ref> as our target benchmarks. They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms [20], which are also implemented in the Polaris compiler. <p> The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the 22 Polaris paralleling compiler [31]. The result of our simulation study using the Perfect Club Benchmarks <ref> [5] </ref> shows that both hardware directory schemes and the TPI scheme have comparable amount of unnecessary cache misses. In hardware schemes, these misses result from the false-sharing effect while in our proposed scheme, they come from the conservative assumptions made by the compiler.
Reference: [6] <author> W. C. Brantley, K. P. McAuliffe, and J. Weiss. </author> <title> RP3 processor-memory element. </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 782-789, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: MIP 89-20891 and MIP 93-07910. such mechanisms [22, 16]. They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [36], the NYU Ultracom-puter [19], the IBM RP3 <ref> [6] </ref>, and the Illinois Cedar [25], compiler-directed techniques are used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. <p> In this paper, we investigated a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme which relies mostly on compiler analysis, but which also needs a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 <ref> [6] </ref>, Illinois Cedar [25], and several recently proposed schemes [11, 13, 14, 21, 17, 27, 28]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [7] <institution> IBM Inc. </institution> <note> Edited by C. May, </note> <author> E. Silha, R. Simpson, and H. Warren. </author> <title> The PowerPC Architecture: A Specification for a New Family of RISC Processors. </title> <month> March </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor <ref> [7] </ref>. 4 in epoch 3 are issued by the same processor due to unknown runtime scheduling information. Second, the read reference to X (f (i)) in epoch 4 cannot be analyzed precisely at compile time due to the unknown index value.
Reference: [8] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December, </month> <year> 1978. </year>
Reference-contexts: size of access units since each access unit (such as character, integer, or floating point data) is a distinct variable analyzed by the compiler. 6 Our experimental results in Section 4 show that a 4-bit or 8-bit timetag is large enough to achieve a very good performance. 9 Full-map Directory <ref> [8] </ref> LimitLess Directory (DIR NB i ) Two-phase Invalidation (bits) [2] (bits) (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead (DRAM) (P+2)*M*P (i+2)*M*P None Total (P = 1024, i = 10) 4MB SRAM / 64.5GB DRAM 4MB SRAM / 3GB DRAM 64MB SRAM only following parameters: P : number of <p> Figure 5 compares the storage overhead of our TPI scheme with a full-map directory scheme <ref> [8] </ref> as well as the LimitLess directory scheme [2]. It shows both the cache and the memory overhead in terms of the cache line size (L), the node cache size (C), the node memory size (M) and the number of processors (P). <p> Full-Map Directory Scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [8, 3] </ref>. It gives a performance comparison to the hardware directory protocols. and compiler support required, the caching strategy for shared data, and their performance limitations. 10 Miss rates Figure 11 shows the miss rates of each scheme for the six benchmarks with a 64KB direct-mapped cache.
Reference: [9] <author> Y.-C. Chen and A. Veidenbaum. </author> <title> An Effective Write Policy for Software Coherence Schemes. </title> <booktitle> Proceedings of Supercomputing '92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: This can be done by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor [15], such redundant write traffic can be effectively reduced <ref> [9] </ref>. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, all the writes should be written back to the main memory at epoch boundary. Therefore, the dirty cache copies cannot be kept in the cache across epoch boundaries.
Reference: [10] <author> Yung-Chin Chen. </author> <title> Cache Design and Performance in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Elec. Eng ., 1993. </institution> <type> Ph.D. Thesis. </type>
Reference-contexts: Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen <ref> [10, 35] </ref> showed that a simple invalidation scheme can achieve performance comparable to that of a directory scheme and discusses several different write policies. Most of those studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> A write-through write-allocate policy is used for both the TPI and the SC schemes, while a write-back cache is used for the hardware directory protocol. These write policies are chosen to deliver the best performance for each type of coherence scheme <ref> [10] </ref>. A weak consistency model is used for all the coherence schemes. It is assumed that each processor can handle basic arithmetic and logical operations in one cycle and has synchronization operations to support parallel language constructs. <p> In TRFD, there is a significant number of redundant writes which increases the overall network traffic of TPI substantially compared to HW. This additional write traffic can be effectively eliminated by organizing a write buffer as a cache <ref> [10] </ref>. Similar technique can also be employed to remove redundant write traffic for update-based coherence protocols. The third type of network traffic is for coherence transactions in the directory protocol. This extra traffic is relatively small compared to the read and write traffic for the benchmarks considered. <p> This problem would be much more significant in a sequential consistency model since both reads and writes are affected by the coherence transactions. 11 Although the compiler-directed schemes can employ write-back at task boundaries, it increases the latency of the invalidation, and results in more bursty traffic <ref> [10] </ref>. 16 Program Average Miss Latency Two-Phase Invalidation Hardware Directory 16 bytes 64 bytes 16 bytes 64 bytes SPEC77 136.2 356.3 136.4 355.5 OCEAN 136.2 354.3 136.4 353.6 FLO52 136.2 355.1 136.6 361.2 QCD2 136.0 354.7 145.5 405.4 TRFD 136.0 352.4 149.1 418.6 scheme. and FLO52, both schemes show a comparable
Reference: [11] <author> H. Cheong. </author> <title> Life Span Strategy A Compiler-Based Approach to Cache Coherence. </title> <booktitle> Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> Several compiler-directed cache coherence schemes [11, 13, 14, 21, 17, 27, 28] have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [11, 12, 21] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The guarded execution technique can be used to further optimize the code generation [20]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [11, 13] </ref>. 3 Hardware Implementation Issues Off-chip secondary cache implementation Since most of multiprocessors today use off-the-shelf microprocessors, it is more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. 7 necessary to determine a cache hit for a Time-Read operation. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [12] <author> H. Cheong and A. Veidenbaum. </author> <title> Stale Data Detection and Coherence Enforcement Using Flow Analysis. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:138-145, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Several compiler-directed cache coherence schemes [11, 13, 14, 21, 17, 27, 28] have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [11, 12, 21] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support.
Reference: [13] <author> H. Cheong and A. Veidenbaum. </author> <title> A Cache Coherence Scheme with Fast Selective Invalidation. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> The guarded execution technique can be used to further optimize the code generation [20]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [11, 13] </ref>. 3 Hardware Implementation Issues Off-chip secondary cache implementation Since most of multiprocessors today use off-the-shelf microprocessors, it is more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. 7 necessary to determine a cache hit for a Time-Read operation. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [14] <author> H. Cheong and A. Veidenbaum. </author> <title> A Version Control Approach To Cache Coherence. </title> <booktitle> Proceedings of the 1989 ACM/SIGARCH International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [29] compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja [26] compared the performance of the version control scheme <ref> [14] </ref> with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes. Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [15] <author> Digital Equipment Corp. </author> <title> Alpha 21164 Microprocessor: Hardware Reference Manual. </title> <year> 1994. </year>
Reference-contexts: This can be done by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor <ref> [15] </ref>, such redundant write traffic can be effectively reduced [9]. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, all the writes should be written back to the main memory at epoch boundary.
Reference: [16] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview. </title> <year> 1991. </year>
Reference-contexts: 1 Introduction Many commercially available large-scale multiprocessors, such as the Cray T3D and the Intel Paragon, do not provide hardware-coherent caches due to the expensive hardware required for fl This work is supported in part by the National Science Foundation under Grant No. MIP 89-20891 and MIP 93-07910. such mechanisms <ref> [22, 16] </ref>. They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms.
Reference: [17] <author> E. Darnell and K. Kennedy. </author> <title> Cache Coherence Using Local Knowledge. </title> <booktitle> Proceedings of the Supercomputing '93, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [18] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> Proceedings the 19th Annual International Symposium on Computer Archtecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Maintaining sharing information per cache word in directory schemes will increase directory storage significantly since the memory requirement is proportional to the total memory size instead of the total cache size. Optimizations to reduce the storage overhead may result in very complicated hardware protocols <ref> [18] </ref>. However, more fine-grained sharing information can be incorporated in this HSCD scheme more easily because the coherence enforcement is done locally. A cache memory has two components: data storage and address tag storage.
Reference: [19] <author> J. Edler, A. Gottlieb, C. P. Kruskal, K. P. McAuliffe, et al. </author> <title> Issues related to MIMD shared-memory computers: the NYU Ultracomputer approach. </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 126-135, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: MIP 89-20891 and MIP 93-07910. such mechanisms [22, 16]. They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [36], the NYU Ultracom-puter <ref> [19] </ref>, the IBM RP3 [6], and the Illinois Cedar [25], compiler-directed techniques are used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories.
Reference: [20] <author> Omitted for blind review. </author> <title> Hardware and Compiler Support for Cache Coherence in Large-Scale Multiprocessors. </title> <type> Ph.D. </type> <note> thesis in preparation, </note> <year> 1995. </year>
Reference-contexts: We call this sequence of events a stale reference sequence. This sequence can be easily detected by a compiler using a modified def-use chain analysis used in standard data-flow analysis techniques <ref> [20] </ref>. Software cache-bypass scheme (SC) Once all the potentially stale data references are identified by the compiler, cache coherence can be enforced if we guarantee that all such references access up-to-date data from main memory rather than from the stale cache copies. <p> The details of the compiler algorithms are beyond the scope of this paper and are described in <ref> [20] </ref>. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler [31]. Figure 3 shows the flowchart of our reference 4 These first read references are identified by finding the first occurrence of upwardly-exposed uses in an epoch in our algorithm. 6 marking algorithm. <p> We then transform the program in the GSA form back to the original program with the reference marking information, and generate appropriate cache and memory operations. The guarded execution technique can be used to further optimize the code generation <ref> [20] </ref>. <p> They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms <ref> [20] </ref>, which are also implemented in the Polaris compiler. After the compiler marking, we instrument the benchmarks to generate memory events, which are used for the simulation. Figure 7 shows the experimentation tools used in our simulations.
Reference: [21] <author> Omitted for blind review. </author> <title> A Compiler-Directed Cache Coherence Scheme with Improved Intertask Locality. </title> <booktitle> Proceedings of the ACM/IEEE Supercomputing'94, </booktitle> <pages> pages 773-782, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> Several compiler-directed cache coherence schemes [11, 13, 14, 21, 17, 27, 28] have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [11, 12, 21] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The issues of synchronization, such as lock variables and critical sections, have also rarely been addressed. 2 In this paper, we address these issues and demonstrate the feasibility and the performance of a HSCD scheme we previously proposed <ref> [21] </ref>. To study the compiler analysis techniques for the proposed scheme, we have implemented the compiler algorithms on the Polaris parallelizing compiler [31]. By performing execution-driven simulations on the Perfect Club Benchmarks, we evaluate the performance of our scheme compared to a hardware directory scheme. <p> Second, the read reference to X (f (i)) in epoch 4 cannot be analyzed precisely at compile time due to the unknown index value. To overcome these limitations, we propose a hardware scheme that keeps track of the local cache states at runtime <ref> [21] </ref>. 2.2 Two-phase invalidation scheme (TPI) In our proposed HSCD scheme, called the Two-Phase Invalidation (TPI) scheme 3 , each epoch is assigned a unique epoch number. <p> On a cache miss, the timetags of other words in the same cache line are assigned the value (R counter - 1) to handle the implicit RAW (read-after-write) or WAR (write-after-read) dependences between concurrent tasks in the same epoch <ref> [21] </ref> (see Section 2.1). Since the timetags are updated from the epoch counter by hardware at runtime, there is no need to store the timetags in main memory. <p> The per-procedure analysis is based on the following steps. First, we transform the source program into a GSA form [4]. Then, we construct a modified flow graph, called the epoch flow graph <ref> [21] </ref>. It contains the epoch boundary information as well as the control flows of the program. Given a source program unit and its epoch flow graph, G, we identify the target references in each epoch to utilize both spatial and temporal reuses inside a task. <p> However, a small timetag may lead 7 As noted by [30, 37], the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for large direct-mapped caches. 10 to frequent invalidations. In <ref> [21] </ref>, we propose a two-phase hardware reset mechanism that can simultaneously invalidate only those cache data whose timetags are out of phase. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [22] <author> Cray Research Inc. </author> <title> Cray T3D System Architecture Overview. </title> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many commercially available large-scale multiprocessors, such as the Cray T3D and the Intel Paragon, do not provide hardware-coherent caches due to the expensive hardware required for fl This work is supported in part by the National Science Foundation under Grant No. MIP 89-20891 and MIP 93-07910. such mechanisms <ref> [22, 16] </ref>. They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms.
Reference: [23] <institution> MIPS Technology Inc. </institution> <note> R10000 User's Manual, Alpha Revision 2.0. </note> <month> March </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: read 1 In Section 5, we generalize the execution model by considering more flexible task scheduling model that allows context switching and task migration. 2 This operation can be implemented in the MIPS R10000 processor by a cache block invalidate (Index Write Back Invalidate) followed by a regular load operation <ref> [23] </ref>. Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor [7]. 4 in epoch 3 are issued by the same processor due to unknown runtime scheduling information.
Reference: [24] <author> C. P. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1091-1098, </volume> <month> Sept., </month> <year> 1987. </year>
Reference-contexts: multiprocessor similar to the Cray T3D. 11 Cache and system organization Latency CPU single-issue processor ALU operations 1 CPU cycle cache size 64 KB, direct-mapped cache hit 1 CPU cycle line size 4 32-bit word cache line base miss latency 100 CPU cycles timetag size 8-bits network delay analytic model <ref> [24] </ref> number of processors 16 two-phase reset 128 cycles Each processor has an on-chip 64-KB direct-mapped lock-up free data cache with a 4-word cache line and an infinite size write buffer. 8 The default values of the parameters used for typical simulations are given in Figure 8. <p> The memory system provides a one-cycle cache hit latency and a 100-cycle miss latency assuming no network load. The network delays are simulated using an analytical delay model for indirect multistage networks <ref> [24] </ref>. The execution-driven simulator instruments the application codes to generate events that reflect the behavior of the codes executing on the target architecture. Simulated events include global and local memory accesses, parallel loop setup and scheduling operations, and synchronization operations.
Reference: [25] <author> D. Kuck, E. Davidson, et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Com puter Architecture, </booktitle> <pages> pages 213-223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp [36], the NYU Ultracom-puter [19], the IBM RP3 [6], and the Illinois Cedar <ref> [25] </ref>, compiler-directed techniques are used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. The C.mmp was the first to allow read-only shared data to be kept in private caches while leaving read-write data uncached. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar <ref> [25] </ref>, and several recently proposed schemes [11, 13, 14, 21, 17, 27, 28]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [26] <author> D. J. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [29] compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja <ref> [26] </ref> compared the performance of the version control scheme [14] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes.
Reference: [27] <author> A. Louri and H. Sung. </author> <title> A Compiler Directed Cache Coherence Scheme with Fast and Parallel Explicit Invalidation. </title> <booktitle> Proceedings of the 1992 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:2-9, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [28] <author> S. L. Min and J.-L. Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> I:23-32, </address> <year> 1989. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters using software. Each variable has a memory access attribute of either "private" or "global". Several compiler-directed cache coherence schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the the previous schemes. <p> This approach has a long history of predecessors, including C.mmp [36], IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes <ref> [11, 13, 14, 21, 17, 27, 28] </ref>. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [29] <author> S. L. Min and J.-L. Baer. </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(1) </volume> <pages> 25-44, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: It is a hardware approach with strong compiler support. We call them hardware-supported compiler-directed (HSCD) coherence schemes, which is distinctly different from a pure hardware directory scheme and a pure software scheme. Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer <ref> [29] </ref> compared the performance of a directory scheme and a timestamp-based scheme assuming infinite cache size and single-word cache lines. Lilja [26] compared the performance of the version control scheme [14] with directory schemes, and analyzed the directory overhead of several implementations.
Reference: [30] <author> J. M. Mulder, N. T. Quach, and M. J. Flynn. </author> <title> An area model for on-chip memories and its application. </title> <journal> Journal of Solid State Circuits, </journal> <volume> 26 </volume> <pages> 98-106, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Therefore, there must be a strategy to recycle the timetag values for a correct operation. A simple strategy is to invalidate the entire cache and reset the epoch counter when it overflows. However, a small timetag may lead 7 As noted by <ref> [30, 37] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for large direct-mapped caches. 10 to frequent invalidations.
Reference: [31] <author> D. A. Padua, R. Eigenmann, J. Hoeflinger, P. Peterson, P. Tu, S. Weatherford, and K. Faign. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPPs. In CSRD Rept. No. </title> <type> 1306. </type> <institution> Univ. of Illinois at Urbana-Champaign., </institution> <month> June, </month> <year> 1993. </year>
Reference-contexts: To study the compiler analysis techniques for the proposed scheme, we have implemented the compiler algorithms on the Polaris parallelizing compiler <ref> [31] </ref>. By performing execution-driven simulations on the Perfect Club Benchmarks, we evaluate the performance of our scheme compared to a hardware directory scheme. In Section 2, we describe an overview of our cache coherence scheme and discuss our compiler analysis techniques and their implementation. <p> The details of the compiler algorithms are beyond the scope of this paper and are described in [20]. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler <ref> [31] </ref>. Figure 3 shows the flowchart of our reference 4 These first read references are identified by finding the first occurrence of upwardly-exposed uses in an epoch in our algorithm. 6 marking algorithm. First, we construct a procedure call graph. <p> The cost is less than the Scalable Coherence Interface (SCI) because the tag size required is much smaller than that required in SCI. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the 22 Polaris paralleling compiler <ref> [31] </ref>. The result of our simulation study using the Perfect Club Benchmarks [5] shows that both hardware directory schemes and the TPI scheme have comparable amount of unnecessary cache misses.
Reference: [32] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution-Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> Proceedings of the Supercomputing 93, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: After the compiler marking, we instrument the benchmarks to generate memory events, which are used for the simulation. Figure 7 shows the experimentation tools used in our simulations. Simulation Execution-driven simulations <ref> [32] </ref> are used to verify the compiler algorithms and to evaluate the performance of our proposed coherence scheme.
Reference: [33] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> IEEE Transactions on Computers, C-43 No.6:651-663, </journal> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, code generation, such as address calculation, will increase the frequency of private references due to spill code. Shared data consistency prevents optimization of the references to the shared data <ref> [33] </ref>. 13 Coherence H/W support S/W support Sharing information Caching for Limitations Scheme shared data access BASE No No No No caching exclusively for shared data remote memory access SC No Stale reference Static local/global writes No intertask detection partial reuses locality within a task TPI Timetag storage Stale reference Static
Reference: [34] <author> D. M. Tullsen and S. J. Eggers. </author> <title> Limitations of Cache Prefetching on a Bus-Based Multiprocessors. </title> <booktitle> Proceedings of The 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The true sharing misses are necessary misses to satisfy cache coherence, while the false sharing misses are the unnecessary misses due to the lack of compile time information or the false-sharing effect of hardware protocols. The false sharing misses are identified during simulations using the method in <ref> [34] </ref>. If an invalidation is caused by an access to a word that the local processor had not used since getting the block into its cache, then it is a false sharing invalidation. Any subsequent invalidation miss on this block will also be counted as a false sharing miss.
Reference: [35] <author> A. V. Veidenbaum. </author> <title> A Compiler-Assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1035, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen <ref> [10, 35] </ref> showed that a simple invalidation scheme can achieve performance comparable to that of a directory scheme and discusses several different write policies. Most of those studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> These light threads usually consist of several iterations of a parallel loop and are supported by language extensions such as DOALL loops (see Figure 1 (a)). 1 Stale reference sequence The following sequence of events creates a stale reference at runtime <ref> [35] </ref>: (1) Processor P i reads or writes to memory location x at time T 1 , and brings a copy of x in its cache; (2) Another processor P j (j 6= i) writes to x later at time T 2 (&gt; T 1 ), and creates a new copy <p> In the hardware directory schemes, the coherence is guaranteed either by invalidation or update coherence transactions. For compiler-directed schemes, however, this might require careful handling for correct operations. For example, in the simple invalidation scheme <ref> [35] </ref>, assume that a task T 1 running on a processor P i in epoch E causes a cache miss on a read access to a variable A, and the task is migrated to a different processor P j in the same epoch.
Reference: [36] <author> C. G. Bell W. A. Wulf. </author> <title> C.mmp amulti-mini processor. </title> <booktitle> Proceedings of the Fall Joint Computer Conference, </booktitle> <pages> pages 765-777, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: MIP 89-20891 and MIP 93-07910. such mechanisms [22, 16]. They instead provide software mechanisms, while relying mostly on users to maintain data coherence either through language extensions or message-passing paradigms. In several early multiprocessor systems, such as the CMU C.mmp <ref> [36] </ref>, the NYU Ultracom-puter [19], the IBM RP3 [6], and the Illinois Cedar [25], compiler-directed techniques are used to solve the cache coherence problem. In this approach, cache coherence is maintained locally without the need for interprocessor communication or hardware directories. <p> In this paper, we investigated a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme which relies mostly on compiler analysis, but which also needs a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp <ref> [36] </ref>, IBM's RP3 [6], Illinois Cedar [25], and several recently proposed schemes [11, 13, 14, 21, 17, 27, 28]. The TPI scheme can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, and can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures.
Reference: [37] <author> H. Wang, T. Sun, and Q. Yang. </author> <title> CAT Caching Address Tags: A Technique for Reducing Area Cost of On-Chip Caches. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 381-390, </pages> <month> June </month> <year> 1995. </year> <month> 25 </month>
Reference-contexts: Therefore, there must be a strategy to recycle the timetag values for a correct operation. A simple strategy is to invalidate the entire cache and reset the epoch counter when it overflows. However, a small timetag may lead 7 As noted by <ref> [30, 37] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for large direct-mapped caches. 10 to frequent invalidations.
References-found: 37

