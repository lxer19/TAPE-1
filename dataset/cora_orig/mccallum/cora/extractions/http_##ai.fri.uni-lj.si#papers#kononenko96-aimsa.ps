URL: http://ai.fri.uni-lj.si/papers/kononenko96-aimsa.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: figor.kononenko, marko.robnik, uros.pompeg@fri.uni-lj.si  
Title: ReliefF for estimation and discretization of attributes in classification, regression, and ILP problems  
Author: Igor Kononenko, Marko Robnik- Sikonja, Uros Pompe 
Address: Trzaska 25, SI-1001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of computer and information science  
Abstract: Instead of myopic impurity functions, we propose the use of Reli-efF for heuristic guidance of inductive learning algorithms. The basic algoritm RELIEF, developed by Kira and Rendell (Kira and Rendell, 1992a;b), is able to efficiently solve classification problems involving highly dependent attributes, such as parity problems. However, it is sensitive to noise and is unable to deal with incomplete data, multi-class, and regression problems (continuous class). We have extended RELIEF in several directions. The extended algorithm ReliefF is able to deal with noisy and incomplete data, can be used for multiclass problems, and its regressional variant RReliefF can deal with regression problems. Another area of application is inductive logic programming (ILP) where, instead of myopic measures, ReliefF can be used to estimate the utility of literals during the theory construction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Breiman L., Friedman J.H., Olshen R.A. & Stone C.J. </author> <title> (1984) Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Function diff (Attribute,Instance1,Instance2) calculates the difference between the values of Attribute for two instances. For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval <ref> [0; 1] </ref>. Normalization with n guarantees all weights W [A] to be in the interval [1; 1]. The weights are estimates of the quality of attributes. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval [0; 1]. Normalization with n guarantees all weights W [A] to be in the interval <ref> [1; 1] </ref>. The weights are estimates of the quality of attributes.
Reference: [2] <author> Cestnik, B. </author> <title> (1989) Informativity-Based Splitting of Numerical Attributes into Intervals. </title> <booktitle> In Hamza, M.H.(ed):Expert Systems Theory & Applications, Proc. of the IASTED International Symposium, </booktitle> <publisher> Acta Press. </publisher>
Reference: [3] <author> Cestnik, B., Kononenko, I., Bratko, I. </author> <title> (1986) ASSISTANT 86: A Knowledge-Elicitation Tool for Sophisticated Users. </title> <editor> In: Ivan Bratko, Nada Lavrac (eds.): </editor> <booktitle> Progress in Machine Learning, Proceedings of EWSL 87. </booktitle> <address> Wilmslow, </address> <publisher> Sigma Press, </publisher> <year> 1987. </year>
Reference: [4] <author> Hunt E., Martin J & Stone P. </author> <title> (1966) Experiments in Induction, </title> <address> New York, </address> <publisher> Academic Press. </publisher>
Reference: [5] <author> Kira K. & Rendell L. </author> <title> (1992a) A practical approach to feature selection, </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <address> July 1992) D.Sleeman & P.Edwards (eds.), </address> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference: [6] <author> Kira K. & Rendell L. </author> <title> (1992b) The feature selection problem: traditional methods and new algorithm. </title> <booktitle> Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference: [7] <author> Kononenko I. </author> <title> (1994) Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. European Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994), </year> <editor> L. De Raedt & F.Bergadano (eds.), </editor> <publisher> Springer Verlag. </publisher>
Reference: [8] <author> Kononenko I. </author> <title> (1995) On biases when estimating multivalued attributes, </title> <booktitle> Proc. Intern. Joint Conf. on Artificial Intelligence, IJCAI-95, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference: [9] <author> Kononenko, I., Simec, E. </author> <title> (1995) Induction of decision trees using RELIEFF. </title> <editor> In: Della Riccia, G., Kruse, R., Viertl, R., (eds.): </editor> <booktitle> Mathematical and statistical methods in artificial intelligence, </booktitle> <publisher> Springer Verlag. </publisher> <pages> 14 </pages>
Reference: [10] <author> Lavrac N., Dzeroski S. </author> <title> (1994) Inductive logic programming, </title> <publisher> Ellis Horwood. </publisher>
Reference: [11] <author> Mantaras R.L. </author> <title> (1989) ID3 Revisited: A distance based criterion for attribute selection, </title> <booktitle> Proc. Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, U.S.A., </address> <month> Oct. </month> <year> 1989. </year>
Reference: [12] <editor> Muggleton S. (ed.) </editor> <booktitle> (1992) Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference: [13] <author> Pompe U. & Kononenko I. </author> <title> (1995) Linear space induction in first order logic with RELIEFF, </title> <editor> In: G.Della Riccia, R.Kruse, R.Viertl (eds.) </editor> <booktitle> Mathematical and statistical methods in artificial intelligence, </booktitle> <publisher> Springer Verlag. </publisher>
Reference: [14] <author> Quinlan J.R. </author> <title> (1986) Induction of decision trees, </title> <journal> Machine learning, </journal> <volume> Vol. </volume> <pages> 1. </pages>
Reference: [15] <author> Richeldi M., </author> <title> Rossotto M.(1995) Class-Driven Statistical Discretization of Continuous Attributes. </title> <editor> In Lavrac N., Wrobel S.(eds.) </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-95, </address> <publisher> Springer Verlag. </publisher>
Reference: [16] <author> Robnik M., Kononenko I. </author> <title> (1995) Discretization of continuous attributes using ReliefF, </title> <booktitle> Proc. </booktitle> <address> ERK-95, Portoroz, </address> <month> Sept. </month> <year> 1995. </year> <month> pp.149-152. </month>
Reference: [17] <author> Smyth P. & Goodman R.M. </author> <title> (1990) Rule induction using information theory. </title> <editor> In. G.Piatetsky-Shapiro & W.Frawley (eds.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press. </publisher> <pages> 15 </pages>
References-found: 17

