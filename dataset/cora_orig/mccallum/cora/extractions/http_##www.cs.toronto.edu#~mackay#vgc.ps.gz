URL: http://www.cs.toronto.edu/~mackay/vgc.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Title: Variational Gaussian Process Classifiers  
Author: Mark N. Gibbs David J.C. MacKay 
Note: Submitted to IEEE Transactions on Neural Networks  
Address: Cambridge CB3 0HE United Kingdom  Cambridge CB3 0HE United Kingdom  
Affiliation: Cavendish Laboratory  Cavendish Laboratory  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Barber, D., and Williams, C. K. I. </author> <title> (1997) Gaussian processes for Bayesian classification via hybrid Monte Carlo. In Neural Information Processing Systems 9 , ed. </title> <editor> by M. C. Mozer, M. I. Jordan, and T. </editor> <booktitle> Petsche, </booktitle> <pages> pp. 340-346. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: We would also like to set the hyperparameters of the covariance function to their most probable values given the data (a Monte Carlo approach could also be used <ref> (Barber and Williams 1997) </ref>). This is not possible as we do not have an analytic expression for P (Djfi). However we can maximize Z 0 and Z 00 with respect to fi to obtain approximations to the most probable fi given the data.
Reference: <author> Gibbs, M. N. </author> <title> (1997) Bayesian Gaussian Processes for Regression and Classification. </title> <institution> Cambridge University dissertation. </institution> <note> 11 Gibbs, </note> <author> M. N., and MacKay, D. J. C., </author> <title> (1996) Efficient implementation of Gaussian processes for inter-polation. </title> <note> Unpublished. </note>
Reference: <author> Hinton, G. E., and van Camp, D. </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 5-13. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Ichikawa, K., Bhadeshia, H. K. D. H., and MacKay, D. J. C. </author> <title> (1996) Model for hot cracking in low-alloy steel weld metals. </title> <booktitle> Science and Technology of Welding and Joining 1: </booktitle> <pages> 43-50. </pages>
Reference-contexts: The two results given for the VGC correspond to the approximations using the lower and upper bound respectively. weld. In a previous treatment of this problem using Bayesian neural networks <ref> (Ichikawa et al. 1996) </ref> the relationship between cracking and carbon content was highlighted and compared with experimental data. We performed a similar analysis using VGCs. An initial test was performed using a training set of 77 examples and a test set of 77 examples. <p> We performed a similar analysis using VGCs. An initial test was performed using a training set of 77 examples and a test set of 77 examples. The test error rates and test log likelihoods for the VGC and the Bayesian neural network approach <ref> (Ichikawa et al. 1996) </ref> can be seen in Table 2 where the test log likelihood is defined as test log likelihood = N test X n=1 where t n is the true test set classification (either 0 or 1) and ^ t n is the prediction P (t n = 1jD).
Reference: <author> Jaakkola, T. S., and Jordan, M. I. </author> <title> (1996) Computing upper and lower bounds on likelihoods in intractable networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in AI . Morgan Kaufman. </booktitle>
Reference-contexts: Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations. Neal (1997) has implemented a Monte Carlo approach to implementing a Gaussian process classifier. In this paper another approach is suggested based on the methods of Jaakkola and Jordan <ref> (Jaakkola and Jordan 1996) </ref>. We obtain tractable upper and lower bounds for the unnormalized posterior density P (ftgja)P (a). These bounds are parameterized by variational parameters which are adjusted in order to obtain the tightest possible fit. <p> We can define upper and lower bounds on the sigmoid, i.e. on P (t = 1ja (x)), <ref> (Jaakkola and Jordan 1996) </ref> as follows: P (t n = 1ja (x n )) Q (t n = 1ja n ; n ) = g ( n ) exp (a n n )=2 ( n )(a 2 n ) (8) where a n = a (x n ) and g (a
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) The evidence framework applied to classification networks. </title> <booktitle> Neural Computation 4 (5): </booktitle> <pages> 698-714. </pages>
Reference-contexts: Efficient methods for implementing Gaussian processes are described in Gibbs and MacKay (1996). fl Corresponding author 1 For classification models there are two well-established approaches to Bayesian inference: Gaussian approximations centred on the posterior modes <ref> (MacKay 1992a) </ref> and Monte Carlo methods (Neal 1995). Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations. Neal (1997) has implemented a Monte Carlo approach to implementing a Gaussian process classifier.
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: approximation P Q (a (x N+1 )jx N+1 ; D; f n g; fi) into equa tion 3 we find P (t N+1 = 1jx N+1 ; D) ' g (o (s l )a MP using the approximation for the integral of the product of a Gaussian and a sigmoid <ref> (MacKay 1992b) </ref>, Z dx g (x)Gaussian (a MP l ; s 2 l ) (23) where o (s) = 1= p We should note that, although we have been using the lower bound on P (t n = 1ja (x n )), we have not generated a lower bound on the
Reference: <author> MacKay, D. J. C. </author> <title> (1995a) Free energy minimization algorithm for decoding and cryptanalysis. </title> <journal> Electronics Letters 31 (6): </journal> <pages> 446-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1995b) Probable networks and plausible predictions | a review of practical Bayesian methods for supervised neural networks. Network: </title> <booktitle> Computation in Neural Systems 6: </booktitle> <pages> 469-505. </pages>
Reference-contexts: We place a prior probability distribution P (w) over the parameters which is traditionally taken to be Gaussian <ref> (MacKay 1995b) </ref>. In the alternative Gaussian process approach (Williams 1995; Williams and Rasmussen 1996; Williams 1998; Neal 1997), we model a (x) directly using a Gaussian process. This involves modelling the joint distribution of fa (x n )g with a Gaussian distribution.
Reference: <author> Neal, R. M. </author> <title> (1995) Bayesian Learning for Neural Networks. </title> <institution> Dept. of Computer Science, Univ. of Toronto dissertation. </institution>
Reference-contexts: Most parametric models are in fact special cases of Gaussian processes, with the covariance matrix depending on the details of the choice of basis functions OE h (x) and the prior P (w) <ref> (Neal 1995) </ref>. Efficient methods for implementing Gaussian processes are described in Gibbs and MacKay (1996). fl Corresponding author 1 For classification models there are two well-established approaches to Bayesian inference: Gaussian approximations centred on the posterior modes (MacKay 1992a) and Monte Carlo methods (Neal 1995). <p> h (x) and the prior P (w) <ref> (Neal 1995) </ref>. Efficient methods for implementing Gaussian processes are described in Gibbs and MacKay (1996). fl Corresponding author 1 For classification models there are two well-established approaches to Bayesian inference: Gaussian approximations centred on the posterior modes (MacKay 1992a) and Monte Carlo methods (Neal 1995). Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations. Neal (1997) has implemented a Monte Carlo approach to implementing a Gaussian process classifier. In this paper another approach is suggested based on the methods of Jaakkola and Jordan (Jaakkola and Jordan 1996).
Reference: <author> Neal, R. M. </author> <title> (1997) Monte Carlo implementation of Gaussian process models for Bayesian regression and classification. </title> <type> Technical Report CRG-TR-97-2, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: Unlike the regression covariance function (Gibbs and MacKay 1996) we assume a (x) to be noise 2 free. However we do introduce a "jitter" term ffi mn J <ref> (Neal 1997) </ref> to make the matrix computations well-conditioned. We choose the magnitude of J is small in comparison to 1 . The product of sigmoid functions in equation 5 generally makes the integral in equation 4 analytically intractable.
Reference: <author> Ripley, B. D. </author> <title> (1994) Flexible non-linear approaches to classification. In From Statistics to Neural Networks. Theory and Pattern Recognition Applications, </title> <editor> ed. by V. Cherkassky, J. H. Friedman, and H. Wechsler, </editor> <booktitle> number subseries F in ASI Proceedings. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Ripley, B. D. </author> <title> (1996) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Skilling, J. </author> <title> (1993) Bayesian numerical analysis. In Physics and Probability, </title> <editor> ed. by W. T. Grandy, Jr. and P. Milonni, </editor> <publisher> Cambridge. C.U.P. </publisher>
Reference-contexts: The cost of direct methods of inversion may become prohibitive when the number of data points N is greater than ' 1000. In Gibbs and MacKay (1996) efficient methods for matrix inversion <ref> (Skilling 1993) </ref> are developed that when applied to the Gaussian process framework allow large data sets to be tackled. Another problem with the variational approach is the profileration of variational parameters when dealing with large amounts of the data.
Reference: <author> Williams, C. K. I., </author> <title> (1995) Regression with Gaussian processes. </title> <note> To appear in Annals of Mathematics and Artificial Intelligence. </note>
Reference: <author> Williams, C. K. I. </author> <title> (1998) Computation with infinite neural networks. </title> <booktitle> Neural Computation 10 (5): </booktitle> <pages> 1203-1216. </pages>
Reference: <author> Williams, C. K. I., and Rasmussen, C. E. </author> <title> (1996) Gaussian processes for regression. </title> <booktitle> In Advances in Neural Information Processing Systems 8 , ed. </booktitle> <editor> by D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo. </editor> <publisher> MIT Press. </publisher> <pages> 12 </pages>
References-found: 17

